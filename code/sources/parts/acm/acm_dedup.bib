
@InProceedings{	  10.1145/3706598.3713633,
  author	= {Haghighi, Nava and Yu, Sunny and Landay, James A. and
		  Rosner, Daniela},
  title		= {Ontologies in Design: How Imagining a Tree Reveals
		  Possibilities and Assumptions in Large Language Models},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713633},
  doi		= {10.1145/3706598.3713633},
  abstract	= {Amid the recent uptake of Generative AI, sociotechnical
		  scholars and critics have traced a multitude of resulting
		  harms, with analyses largely focused on values and axiology
		  (e.g., bias). While value-based analyses are crucial, we
		  argue that ontologies—concerning what we allow ourselves
		  to think or talk about—is a vital but under-recognized
		  dimension in analyzing these systems. Proposing a need for
		  a practice-based engagement with ontologies, we offer four
		  orientations for considering ontologies in design:
		  pluralism, groundedness, liveliness, and enactment. We
		  share examples of potentialities that are opened up through
		  these orientations across the entire LLM development
		  pipeline by conducting two ontological analyses: examining
		  the responses of four LLM-based chatbots in a prompting
		  exercise, and analyzing the architecture of an LLM-based
		  agent simulation. We conclude by sharing opportunities and
		  limitations of working with ontologies in the design and
		  development of sociotechnical systems.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {244},
  numpages	= {20},
  keywords	= {ontological design, ontologies, generative AI, large
		  language models, foundation models, LLM agents},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3696410.3714816,
  author	= {Liu, Zhiqiang and Gan, Chengtao and Wang, Junjie and
		  Zhang, Yichi and Bo, Zhongpu and Sun, Mengshu and Chen,
		  Huajun and Zhang, Wen},
  title		= {OntoTune: Ontology-Driven Self-training for Aligning Large
		  Language Models},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696410.3714816},
  doi		= {10.1145/3696410.3714816},
  abstract	= {Existing domain-specific Large Language Models (LLMs) are
		  typically developed by fine-tuning general-purposed LLMs
		  with large-scale domain-specific corpora. However, training
		  on large-scale corpora often fails to effectively organize
		  domain knowledge of LLMs, leading to fragmented
		  understanding. Inspired by how humans connect concepts and
		  organize knowledge through mind maps, we aim to emulate
		  this approach by using ontology with hierarchical
		  conceptual knowledge to reorganize LLM's domain knowledge.
		  From this perspective, we propose an ontology-driven
		  self-training framework called OntoTune, which aims to
		  align LLMs with ontology through in-context learning,
		  enabling the generation of responses guided by the
		  ontology. We leverage in-context learning to identify
		  whether the LLM has acquired the specific concept's
		  ontology knowledge, and select the entries not yet mastered
		  by LLM as the training set to further align the LLM with
		  ontology. Compared to existing domain LLMs based on newly
		  collected large-scale domain-specific corpora, our
		  OntoTune, which relies on the existing, long-term developed
		  ontology and LLM itself, significantly reduces data
		  maintenance costs and offers improved generalization
		  ability. We conduct our study in the medical domain to
		  evaluate the effectiveness of OntoTune, utilizing a
		  standardized medical ontology, SNOMED CT as our ontology
		  source. Experimental results demonstrate that OntoTune
		  achieves state-of-the-art performance in both in-ontology
		  task hypernym discovery and out-of-ontology task medical
		  domain QA. Moreover, compared to the latest direct ontology
		  injection method TaxoLLaMA, our OntoTune better preserves
		  original knowledge of LLM. The code and data are available
		  at https://github.com/zjukg/OntoTune.},
  booktitle	= {Proceedings of the ACM on Web Conference 2025},
  pages		= {119–133},
  numpages	= {15},
  keywords	= {align with ontology, large language model, self-training},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InBook{	  10.1145/3730436.3730532,
  author	= {Zheng, Hanqi and Ouyang, Guige and Huang, Yongzhong},
  title		= {An Unsupervised Ontology Construction Method Based on
		  Pre-trained Language Model},
  year		= {2025},
  isbn		= {9798400713637},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3730436.3730532},
  abstract	= {With the fast growth of text data, the importance of
		  automatic ontology construction has grown significantly‎.
		  The proposed article provides a novel approach by applying
		  pre-trained ‎language models to automatically construct
		  the ontology. The framework consists of two sequential
		  phases: automatic concept discovery and automatic relation
		  discovery. In the context of automatic concept discovery,
		  instances and their embedding vectors ‎are extracted
		  first through Named Entity Recognition (NER). Then, the
		  unsupervised affinity ‎propagation (AP) clustering
		  algorithm is applied to classify these embedding vectors,
		  resulting in the discovery of the ‎concepts. A denoising
		  method is discussed to obtain higher accuracy with respect
		  to the concepts obtained to reduce noise caused by complete
		  clustering. Related to automatic relation discovery between
		  concepts (mentioned in the next section), the process
		  ‎generates inexplicable concepts that are contextually
		  similar based on the embedded vectors of instances mapping
		  ‎over the two entities. This can enable unsupervised
		  automatic discovery of relations between the contextually
		  related concepts. This method shows a certain feasibility
		  and achieves early effectiveness in unsupervised automatic
		  ontology construction with the experimental results. ‎},
  booktitle	= {Proceedings of the 2025 International Conference on
		  Artificial Intelligence and Computational Intelligence},
  pages		= {588–595},
  numpages	= {8}
}

@InProceedings{	  10.1145/3672608.3707840,
  author	= {Shin, Yong-Jun and Utz, Wilfrid},
  title		= {A Platform-Independent Software-Intensive Workflow
		  Modeling Language And An Open-Source Visual Programming
		  Tool: A Bottom-Up Approach Using Ontology Integration Of
		  Industrial Workflow Engines},
  year		= {2025},
  isbn		= {9798400706295},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672608.3707840},
  doi		= {10.1145/3672608.3707840},
  abstract	= {Many contemporary software-intensive services are
		  developed as workflows of collaborative and interdependent
		  tasks. Industrial workflow platforms (i.e., engines) such
		  as Airflow and Kubeflow automatically execute and monitor
		  the workflow specified in platform-specific code. The
		  code-based workflow specification becomes complex and
		  error-prone as services grow in complexity. Furthermore,
		  differences in platform-specific workflow specifications
		  cause inefficiencies when porting workflows between
		  platforms, even if the different platforms handle
		  semantically the same workflow.In this paper, we propose a
		  bottom-up approach for developing a platform-independent
		  software-intensive workflow modeling language. The approach
		  systematically extends the UML activity diagram by building
		  platform-independent ontologies of the workflow
		  specification from the given target industrial workflow
		  engines. Based on the approach, we develop a
		  platform-independent Workflow Modeling Language
		  (WorkflowML) that covers four famous workflow engines
		  (Airflow, Kubeflow, Argo workflow, and Metaflow).
		  Furthermore, we implement an open-source visual programming
		  tool for WorkflowML using the ADOxx metamodeling platform.
		  We validate our approach by evaluating the expressiveness
		  of WorkflowML based on modeling case studies of 42 simple
		  workflows and two real-case workflow-based services. The
		  evaluation results validate that WorkflowML serves as an
		  effective common visual language for target workflow
		  engines, supported by an open-source visual programming
		  tool.},
  booktitle	= {Proceedings of the 40th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1421–1430},
  numpages	= {10},
  keywords	= {workflow, domain-specific modeling language, metamodeling,
		  visual programming, tool, ADOxx, ontology},
  location	= {Catania International Airport, Catania, Italy},
  series	= {SAC '25}
}

@InProceedings{	  10.1145/3671127.3698792,
  author	= {Mulayim, Ozan Baris and Paul, Lazlo and Pritoni, Marco and
		  Prakash, Anand Krishnan and Sudarshan, Malavikha and
		  Fierro, Gabe},
  title		= {Large Language Models for the Creation and Use of Semantic
		  Ontologies in Buildings: Requirements and Challenges},
  year		= {2024},
  isbn		= {9798400707063},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3671127.3698792},
  doi		= {10.1145/3671127.3698792},
  abstract	= {Semantic ontologies offer a formalized, machine-readable
		  framework for representing knowledge, enabling the
		  structured description of complex systems. In the building
		  domain, the adoption of ontologies like the Brick schema
		  has transformed how buildings and their systems are modeled
		  by providing a standardized, interoperable language.
		  However, the complexity and the steep learning curve
		  involved in developing and querying semantic models present
		  substantial challenges, often requiring a workforce with
		  specialized expertise. This paper builds on our experience
		  in investigating how Large Language Models (LLMs) can help
		  address these challenges, focusing on their role in
		  constructing and querying of semantic models, particularly
		  using the Brick Schema. Our study outlines the requirements
		  and metrics for evaluating the scalability and
		  effectiveness of LLM-based tools, while also discussing the
		  current challenges and limitations in developing such
		  tools. Ultimately, this paper aims to orient research
		  efforts as various groups experiment with diverse
		  techniques, while enabling more effective comparison of
		  emerging solutions and fostering collaboration across the
		  field.},
  booktitle	= {Proceedings of the 11th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {312–317},
  numpages	= {6},
  keywords	= {Knowledge Graphs, Large Language Models, Semantic
		  Ontology},
  location	= {Hangzhou, China},
  series	= {BuildSys '24}
}

@Article{	  10.1145/3757923,
  author	= {Meloni, Antonello and Reforgiato Recupero, Diego and
		  Osborne, Francesco and salatino, angelo and Motta, Enrico
		  and Vahadati, Sahar and Lehmann, Jens},
  title		= {Exploring Large Language Models for Scientific Question
		  Answering via Natural Language to SPARQL Translation},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3757923},
  doi		= {10.1145/3757923},
  abstract	= {Translating scientific questions expressed in natural
		  language into SPARQL queries that can be executed over
		  knowledge graphs remains a significant challenge in the
		  field of question answering. Recently, several prominent
		  benchmarks, notably SciQA and DBLP-QuAD, have emerged to
		  evaluate performance in this domain. In this paper, we
		  provide a comprehensive analysis of the performance of
		  language models on these benchmarks, assessing various
		  optimization strategies. Our results indicate that the
		  combined use of fine-tuning and prompting techniques,
		  especially when incorporating strategic few-shot selection,
		  produces excellent results on both benchmarks. These
		  findings underscore an urgent need for more challenging
		  benchmarks to better assess model capabilities. We identify
		  key insights, common error patterns, and potential
		  opportunities for transfer learning, and we discuss their
		  implications for optimizing the performance of large
		  language models in knowledge graph-based question answering
		  tasks.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= aug,
  keywords	= {Knowledge graphs, Question answering, Language models,
		  Fine-tuning, Few-shot learning}
}

@InProceedings{	  10.1145/3637528.3671745,
  author	= {Komarlu, Tanay and Jiang, Minhao and Wang, Xuan and Han,
		  Jiawei},
  title		= {OntoType: Ontology-Guided and Pre-Trained Language Model
		  Assisted Fine-Grained Entity Typing},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671745},
  doi		= {10.1145/3637528.3671745},
  abstract	= {Fine-grained entity typing (FET), which assigns entities
		  in text with context-sensitive, fine-grained semantic
		  types, is a basic but important task for knowledge
		  extraction from unstructured text. FET has been studied
		  extensively in natural language processing and typically
		  relies on human-annotated corpora for training, which is
		  costly and difficult to scale. Recent studies explore the
		  utilization of pre-trained language models (PLMs) as a
		  knowledge base to generate rich and context-aware weak
		  supervision for FET. However, a PLM still requires
		  direction and guidance to serve as a knowledge base as they
		  often generate a mixture of rough and fine-grained types,
		  or tokens unsuitable for typing. In this study, we vision
		  that an ontology provides a semantics-rich, hierarchical
		  structure, which will help select the best results
		  generated by multiple PLM models and head words.
		  Specifically, we propose a novel annotation-free,
		  ontology-guided FET method, OntoType, which follows a type
		  ontological structure, from coarse to fine, ensembles
		  multiple PLM prompting results to generate a set of type
		  candidates, and refines its type resolution, under the
		  local context with a natural language inference model. Our
		  experiments on the Ontonotes, FIGER, and NYT datasets using
		  their associated ontological structures demonstrate that
		  our method outperforms the state-of-the-art zero-shot
		  fine-grained entity typing methods as well as a typical LLM
		  method, ChatGPT. Our error analysis shows that refinement
		  of the existing ontology structures will further improve
		  fine-grained entity typing.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1407–1417},
  numpages	= {11},
  keywords	= {fine-grained entity typing, masked language model
		  prompting, natural language understanding, zero-shot entity
		  typing},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.5555/3635637.3663238,
  author	= {Zhang, Shiyao and Dong, Yuji and Zhang, Yichuan and Payne,
		  Terry R. and Zhang, Jie},
  title		= {Large Language Model Assissted Multi-Agent Dialogue for
		  Ontology Alignment},
  year		= {2024},
  isbn		= {9798400704864},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Ontology alignment is critical in cross-domain
		  integration; however, it typically necessitates the
		  involvement of a human domain-expert, which can make the
		  task costly. Although a variety of machine-learning
		  approaches have been proposed that can simplify this task
		  by learning the patterns from experts, such techniques are
		  still susceptible to domain knowledge updates that could
		  potentially change the patterns and lead to extra expert
		  involvement. The use of Large Language Models (LLMs) has
		  demonstrated a general cognitive ability, which has the
		  potential to assist ontology alignment from the cognition
		  level, thus obviating the need for costly expert
		  involvement. However, the process by which the output of
		  LLMs is generated can be opaque and thus the reliability
		  and interpretability of such models is not always
		  predictable. This paper proposes a dialogue model, in which
		  multiple agents negotiate the correspondence between two
		  knowledge sets with the support from an LLM. We demonstrate
		  that this approach not only reduces the need for the
		  involvement of a domain expert for ontology alignment, but
		  that the results are interpretable despite the use of
		  LLMs.},
  booktitle	= {Proceedings of the 23rd International Conference on
		  Autonomous Agents and Multiagent Systems},
  pages		= {2594–2596},
  numpages	= {3},
  keywords	= {dialogue, large language model, multi-agent system,
		  negotiation, ontology alignment},
  location	= {Auckland, New Zealand},
  series	= {AAMAS '24}
}

@InProceedings{	  10.1145/3587259.3627571,
  author	= {Hertling, Sven and Paulheim, Heiko},
  title		= {OLaLa: Ontology Matching with Large Language Models},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627571},
  doi		= {10.1145/3587259.3627571},
  abstract	= {Ontology (and more generally: Knowledge Graph) Matching is
		  a challenging task where information in natural language is
		  one of the most important signals to process. With the rise
		  of Large Language Models, it is possible to incorporate
		  this knowledge in a better way into the matching pipeline.
		  A number of decisions still need to be taken, e.g., how to
		  generate a prompt that is useful to the model, how
		  information in the KG can be formulated in prompts, which
		  Large Language Model to choose, how to provide existing
		  correspondences to the model, how to generate candidates,
		  etc. In this paper, we present a prototype that explores
		  these questions by applying zero-shot and few-shot
		  prompting with multiple open Large Language Models to
		  different tasks of the Ontology Alignment Evaluation
		  Initiative (OAEI). We show that with only a handful of
		  examples and a well-designed prompt, it is possible to
		  achieve results that are en par with supervised matching
		  systems which use a much larger portion of the ground
		  truth.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {131–139},
  numpages	= {9},
  keywords	= {Entity Resolution, Large Language Model, Ontology
		  Matching},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@Article{	  10.1145/3676280,
  author	= {O'leary, Daniel E.},
  title		= {Using Large Language Models for Armchair Auditors},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {2},
  url		= {https://doi.org/10.1145/3676280},
  doi		= {10.1145/3676280},
  abstract	= {Armchair auditors are citizens who use open data to
		  investigate and monitor government activities, typically
		  using analytics and other approaches. Armchair auditors
		  provide a valuable role in holding governments and
		  organizations accountable. This paper investigates the
		  potential use of large language models (LLM) to support
		  armchair auditor analyses of different governmental
		  entities. Unfortunately, the literature, prior to the
		  development of LLM suggested several challenges for
		  armchair auditors. However, the analysis in this paper
		  suggests that LLM can provide substantial data and analytic
		  process support for armchair auditors mitigating issues
		  such as providing guidelines for analyses, guiding users to
		  appropriate communities, suggesting potential data
		  availability opportunities, doing analysis, and other
		  issues. As part of an approach to unifying armchair auditor
		  searches, this paper also suggests a prompt library
		  designed to support, standardize and promote best practice
		  analyzes among armchair auditors. In addition to these
		  issues, this paper also analyzes emerging ethical issues
		  associated with armchair auditors and their use of open
		  data and LLMs. Finally, this paper extends the activity
		  theory model to account for LLMs.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= jun,
  articleno	= {28},
  numpages	= {13},
  keywords	= {Activity theory, large language models, armchair auditor,
		  open data, ChatGPT, BARD}
}

@Article{	  10.1145/3735632,
  author	= {Li, Jiawei and Gao, Yang and Yang, Yizhe and Bai, Yu and
		  Zhou, Xiaofeng and Li, Yinghao and Sun, Huashan and Liu,
		  Yuhang and Si, Xingpeng and Ye, Yuhao and Wu, Yixiao and
		  Lin, Yiguan and Xu, Bin and Ren, Bowen and Feng, Chong and
		  Huang, Heyan},
  title		= {Fundamental Capabilities and Applications of Large
		  Language Models: A Survey},
  year		= {2025},
  issue_date	= {January 2026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {58},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3735632},
  doi		= {10.1145/3735632},
  abstract	= {Large Language Models (LLMs) have demonstrated remarkable
		  effectiveness across various domain-specific applications.
		  However, which fundamental capabilities most contribute to
		  their success in different domains remains unclear. This
		  uncertainty complicates LLM evaluation, as existing
		  benchmark-based assessments often fail to capture their
		  real-world performance, where the required capabilities may
		  differ from those measured in the benchmarks. In this
		  survey, we provide a systematic introduction to LLMs’
		  fundamental capabilities, encompassing their definitions,
		  formation mechanisms, and practical applications. We
		  further explore the relationships among these capabilities
		  and discuss how they collectively support complex
		  problem-solving in domain-specific applications. Building
		  on this foundation, we review recent advances in LLM-driven
		  applications across nine specific domains: medicine, law,
		  computational biology, finance, social sciences and
		  psychology, computer programming and software engineering,
		  robots and agents, AI for disciplines, and creative work.
		  We analyze how specific capabilities are leveraged for each
		  domain to address unique requirements. This perspective
		  enables us to establish connections between these
		  capabilities and domain requirements, and to evaluate the
		  varying importance of different capabilities across
		  different domains. Based on these insights, we propose
		  evaluation strategies tailored to the essential
		  capabilities required in each domain, offering practical
		  guidance for selecting suitable backbone LLMs in real-world
		  applications.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {38},
  numpages	= {42},
  keywords	= {Large language model, fundamental capabilities,
		  applications}
}

@Article{	  10.1145/3764579,
  author	= {Ling, Chen and Zhao, Xujiang and Lu, Jiaying and Deng,
		  Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury,
		  Tanmoy and Li, Yun and Cui, Hejie and Zhang, Xuchao and
		  Zhao, Tianjiao and Panalkar, Amit and Mehta, Dhagash and
		  Pasquali, Stefano and Cheng, Wei and Wang, Haoyu and Liu,
		  Yanchi and Chen, Zhengzhang and Chen, Haifeng and White,
		  Chris and Gu, Quanquan and Pei, Jian and Yang, Carl and
		  Zhao, Liang},
  title		= {Domain Specialization as the Key to Make Large Language
		  Models Disruptive: A Comprehensive Survey},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3764579},
  doi		= {10.1145/3764579},
  abstract	= {Large language models (LLMs) have significantly advanced
		  the field of natural language processing (NLP), providing a
		  highly useful, task-agnostic foundation for a wide range of
		  applications. However, directly applying LLMs to solve
		  sophisticated problems in specific domains meets many
		  hurdles, caused by the heterogeneity of domain data, the
		  sophistication of domain knowledge, the uniqueness of
		  domain objectives, and the diversity of the constraints
		  (e.g., various social norms, cultural conformity, religious
		  beliefs, and ethical standards in the domain applications).
		  Domain specification techniques are key to making large
		  language models disruptive in many applications.
		  Specifically, to solve these hurdles, there has been a
		  notable increase in research and practices conducted in
		  recent years on the domain specialization of LLMs. This
		  emerging field of study, with its substantial potential for
		  impact, necessitates a comprehensive and systematic review
		  to summarize better and guide ongoing work in this area. In
		  this article, we present a comprehensive survey on domain
		  specification techniques for large language models, an
		  emerging direction critical for large language model
		  applications. First, we propose a systematic taxonomy that
		  categorizes the LLM domain-specialization techniques based
		  on the accessibility to LLMs and summarizes the framework
		  for all the subcategories as well as their relations and
		  differences to each other. Second, we present an extensive
		  taxonomy of critical application domains that can benefit
		  dramatically from specialized LLMs, discussing their
		  practical significance and open challenges. Last, we offer
		  our insights into the current research status and future
		  trends in this area.},
  note		= {Just Accepted},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  keywords	= {Large Language Models, Natural Language Processing, Domain
		  Specialization}
}

@InProceedings{	  10.1145/3711896.3736557,
  author	= {Jiang, Pengcheng and Ouyang, Siru and Jiao, Yizhu and
		  Zhong, Ming and Tian, Runchu and Han, Jiawei},
  title		= {Retrieval And Structuring Augmented Generation with Large
		  Language Models},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3736557},
  doi		= {10.1145/3711896.3736557},
  abstract	= {Large Language Models (LLMs) have revolutionized natural
		  language processing with their remarkable capabilities in
		  text generation and reasoning. However, these models face
		  critical challenges when deployed in real-world
		  applications, including hallucination generation, outdated
		  knowledge, and limited domain expertise. Retrieval And
		  Structuring (RAS) Augmented Generation addresses these
		  limitations by integrating dynamic information retrieval
		  with structured knowledge representations. This survey (1)
		  examines retrieval mechanisms including sparse, dense, and
		  hybrid approaches for accessing external knowledge; (2)
		  explore text structuring techniques such as taxonomy
		  construction, hierarchical classification, and information
		  extraction that transform unstructured text into organized
		  representations; and (3) investigate how these structured
		  representations integrate with LLMs through prompt-based
		  methods, reasoning frameworks, and knowledge embedding
		  techniques. It also identifies technical challenges in
		  retrieval efficiency, structure quality, and knowledge
		  integration, while highlighting research opportunities in
		  multimodal retrieval, cross-lingual structures, and
		  interactive systems. This comprehensive overview provides
		  researchers and practitioners with insights into RAS
		  methods, applications, and future directions.},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {6032–6042},
  numpages	= {11},
  keywords	= {information retrieval, knowledge representation, large
		  language models},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@InProceedings{	  10.1145/3706468.3706566,
  author	= {Yeung, Steven},
  title		= {A comparative study of rule-based, machine learning and
		  large language model approaches in automated writing
		  evaluation (AWE)},
  year		= {2025},
  isbn		= {9798400707018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706468.3706566},
  doi		= {10.1145/3706468.3706566},
  abstract	= {Automated Writing Evaluation (AWE) tools have proved
		  beneficial to writing development. Research on AWE methods
		  is essential for improving tool performance and further
		  comparative studies are needed as new methods emerge. This
		  study examines the performance of several AWE approaches,
		  comparing rule-based and statistical methods, machine
		  learning (ML) models, and a large language model (LLM).
		  These three AWE methods were applied to a representative
		  sample of academic essays from the TOEFL11 dataset to
		  compare their assessment performance. Results show that the
		  selected LLM, GPT-4, outperformed the other two approaches
		  in terms of QWK and Pearson’s correlation coefficient,
		  while the Support Vector Machine (SVM) model in the ML
		  approach had the highest accuracy and the lowest mean
		  absolute error. This paper provides a detailed comparison
		  of these three approaches and discusses implications for
		  educational practice and future research around AWE.},
  booktitle	= {Proceedings of the 15th International Learning Analytics
		  and Knowledge Conference},
  pages		= {984–991},
  numpages	= {8},
  keywords	= {Rule-based method, machine learning, large language model,
		  automated writing evaluation, automated essay scoring,
		  generative AI},
  location	= { },
  series	= {LAK '25}
}

@Article{	  10.1145/3715318,
  author	= {Zhang, Qiang and Ding, Keyan and Lv, Tianwen and Wang,
		  Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and
		  Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and Zhuang,
		  Xiang and Wang, Zeyuan and Qin, Ming and Zhang, Mengyao and
		  Zhang, Jinlu and Cui, Jiyu and Xu, Renjun and Chen,
		  Hongyang and Fan, Xiaohui and Xing, Huabin and Chen,
		  Huajun},
  title		= {Scientific Large Language Models: A Survey on Biological
		  \&amp; Chemical Domains},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3715318},
  doi		= {10.1145/3715318},
  abstract	= {Large Language Models (LLMs) have emerged as a
		  transformative power in enhancing natural language
		  comprehension, representing a significant stride toward
		  artificial general intelligence. The application of LLMs
		  extends beyond conventional linguistic boundaries,
		  encompassing specialized linguistic systems developed
		  within various scientific disciplines. This growing
		  interest has led to the advent of scientific LLMs, a novel
		  subclass specifically engineered for facilitating
		  scientific discovery. As a burgeoning area in the community
		  of AI for Science, scientific LLMs warrant comprehensive
		  exploration. However, a systematic and up-to-date survey
		  introducing them is currently lacking. In this article, we
		  endeavor to methodically delineate the concept of
		  “scientific language,” whilst providing a thorough
		  review of the latest advancements in scientific LLMs. Given
		  the expansive realm of scientific disciplines, our analysis
		  adopts a focused lens, concentrating on the biological and
		  chemical domains. This includes an in-depth examination of
		  LLMs for textual knowledge, small molecules, macromolecular
		  proteins, genomic sequences, and their combinations,
		  analyzing them in terms of model architectures,
		  capabilities, datasets, and evaluation. Finally, we
		  critically examine the prevailing challenges and point out
		  promising research directions along with the advances of
		  LLMs. By offering a comprehensive overview of technical
		  developments in this field, this survey aspires to be an
		  invaluable resource for researchers navigating the
		  intricate landscape of scientific LLMs.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {161},
  numpages	= {38},
  keywords	= {Scientific domain, large language models, protein,
		  molecule, genome}
}

@Article{	  10.1145/3733719,
  author	= {Kreikemeyer, Justin Noah and Jankowski, Mi\l{}osz and
		  Wilsdorf, Pia and Uhrmacher, Adelinde M.},
  title		= {Using (Not-so) Large Language Models to Generate
		  Simulation Models in a Formal DSL: A Study on Reaction
		  Networks},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-3301},
  url		= {https://doi.org/10.1145/3733719},
  doi		= {10.1145/3733719},
  abstract	= {Formal languages are an integral part of modeling and
		  simulation. They allow the distillation of knowledge into
		  concise simulation models amenable to automatic execution,
		  interpretation, and analysis. However, the arguably most
		  humanly accessible means of expressing models is through
		  natural language, which is not easily interpretable by
		  computers. Here, we evaluate how a Large Language Model
		  (LLM) might be used for formalizing natural language into
		  simulation models. Existing studies only explored using
		  very large LLMs, like the commercial GPT models, without
		  fine-tuning model weights. To close this gap, we show how
		  an open-weights, 7B-parameter Mistral model can be
		  fine-tuned to translate natural language descriptions to
		  reaction network models in a domain-specific language,
		  offering a self-hostable, compute-, and memory efficient
		  alternative. To this end, we develop a synthetic data
		  generator to serve as the basis for fine-tuning and
		  evaluation. Our quantitative evaluation shows that our
		  fine-tuned Mistral model can recover the ground truth
		  simulation model in up to (84.5\% ) of cases. In addition,
		  our small-scale user study demonstrates the model’s
		  practical potential for one-time generation as well as
		  interactive modeling in various domains. While promising,
		  in its current form, the fine-tuned small LLM cannot catch
		  up with large LLMs. We conclude that higher-quality
		  training data are required, and expect future small and
		  open-source LLMs to offer new opportunities.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Model. Comput. Simul.},
  month		= may,
  keywords	= {simulation model generation, natural language processing,
		  language model, constrained decoding, knowledge
		  extraction}
}

@InProceedings{	  10.1145/3587259.3627560,
  author	= {Schneider, Florian and Dash, Sarthak and Bagchi, Sugato
		  and Mihindukulasooriya, Nandana and Gliozzo, Alfio
		  Massimiliano},
  title		= {NLFOA: Natural Language Focused Ontology Alignment},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627560},
  doi		= {10.1145/3587259.3627560},
  abstract	= {For Ontology Alignment (OA), the task is to align
		  semantically equivalent concepts and relations from
		  different ontologies. This task plays a crucial role in
		  many downstream tasks and applications in academia and
		  industry. Since manually aligning ontologies is inefficient
		  and costly, numerous approaches exist to do this
		  automatically. However, most approaches are tailored to
		  specific domains, are rule-based systems or based on
		  feature engineering, and require external knowledge. The
		  most recent advances in the field of OA rely on the widely
		  proven effectiveness of pre-trained language models to
		  represent the human-generated language that describes the
		  entities in an ontology. However, these approaches
		  additionally require sophisticated algorithms or Graph
		  Neural Networks to exploit an ontology’s graphical
		  structure to achieve state-of-the-art performance. In this
		  work, we present NLFOA, or Natural Language Focused
		  Ontology Alignment, which purely focuses on the natural
		  language contained in ontologies to process the
		  ontology’s semantics as well as graphical structure. An
		  evaluation of our approach on common OA datasets shows
		  superior results when finetuning with only a small number
		  of training samples. Additionally, it demonstrates strong
		  results in a zero-shot setting which could be employed in
		  an active learning setup to reduce human labor when
		  manually aligning ontologies significantly.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {114–121},
  numpages	= {8},
  keywords	= {Ontology Alignment Sentence Transformers Zero-Shot},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@Article{	  10.1145/3762669,
  author	= {Arazzi, Marco and Marconi Sciarroni, Monica and Nocera,
		  Antonino and Storti, Emanuele},
  title		= {RAG-IoE: IoT context-aware information retrieval with
		  Large Language Models in Industry 5.0},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3762669},
  doi		= {10.1145/3762669},
  abstract	= {Human-centric design, intelligence, and seamless
		  interconnectivity are key pillars of the Industry 5.0. A
		  critical challenge in these scenarios is the efficient
		  retrieval of relevant, context-aware information for
		  workers within Internet of Everything (IoE) networks.
		  Traditional information retrieval techniques struggle with
		  the heterogeneous, dynamic data generated in industrial
		  settings. To address this, we define a context-aware data
		  model for IoE scenarios, on top of which we propose
		  RAG-IoE, a novel Retrieval-Augmented Generation (RAG)
		  solution to enable adaptive, scalable, and context-based
		  information retrieval from both structured and unstructured
		  data sources. Our approach organizes IoE data within a
		  semantic framework, integrating hybrid retrieval methods.
		  It combines structured search on a Knowledge Graph with
		  unstructured data retrieval using embeddings stored in a
		  vector database, followed by LLM-driven reasoning to refine
		  results. This architecture enhances decision-making,
		  reduces cognitive overload, and ensures precise guidance
		  for industrial operators. We validate the efficiency and
		  effectiveness of RAG-IoE using a novel dataset through both
		  a user study and quantitative analysis, demonstrating its
		  potential to optimize human-machine collaboration in
		  Industry 5.0 environments.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Internet Things},
  month		= aug,
  keywords	= {Context-aware, Knowledge Graph, Large Language Model,
		  Retrieval-Augmented Generation, IoT, IoE, Industry 5.0}
}

@Article{	  10.1145/3735553,
  author	= {Li, Tao and Cui, Chenhui and Huang, Rubing and Towey, Dave
		  and Ma, Lei},
  title		= {Large Language Models for Automated Web-Form-Test
		  Generation: An Empirical Study},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3735553},
  doi		= {10.1145/3735553},
  abstract	= {Testing web forms is an essential activity for ensuring
		  the quality of web applications. It typically involves
		  evaluating the interactions between users and forms.
		  Automated test-case generation remains a challenge for
		  web-form testing: Due to the complex, multi-level structure
		  of web pages, it can be difficult to automatically capture
		  their inherent contextual information for inclusion in the
		  tests. Large Language Models (LLMs) have shown great
		  potential for contextual text generation. This motivated us
		  to explore how they could generate automated tests for web
		  forms, making use of the contextual information within form
		  elements. To the best of our knowledge, no comparative
		  study examining different LLMs has yet been reported for
		  web-form-test generation. To address this gap in the
		  literature, we conducted a comprehensive empirical study
		  investigating the effectiveness of 11 LLMs on 146 web forms
		  from 30 open-source Java web applications. In addition, we
		  propose three HTML-structure-pruning methods to extract key
		  contextual information. The experimental results show that
		  different LLMs can achieve different testing effectiveness,
		  with the GPT-4, GLM-4, and Baichuan2 LLMs generating the
		  best web-form tests. Compared with GPT-4, the other LLMs
		  had difficulty generating appropriate tests for the web
		  forms: Their successfully-submitted rates (SSRs) — the
		  proportions of the LLMs-generated web-form tests that could
		  be successfully inserted into the web forms and submitted
		  — decreased by 9.10\% to 74.15\%. Our findings also show
		  that, for all LLMs, when the designed prompts include
		  complete and clear contextual information about the web
		  forms, more effective web-form tests were generated.
		  Specifically, when using Parser-Processed HTML for Task
		  Prompt (PH-P), the SSR averaged 70.63\%, higher than the
		  60.21\% for Raw HTML for Task Prompt (RH-P) and 50.27\% for
		  LLM-Processed HTML for Task Prompt (LH-P). With RH-P,
		  GPT-4’s SSR was 98.86\%, outperforming models like LLaMa2
		  (7B) with 34.47\% and GLM-4V with 0\%. Similarly, with
		  PH-P, GPT-4 reached an SSR of 99.54\%, the highest among
		  all models and prompt types. Finally, this paper also
		  highlights strategies for selecting LLMs based on
		  performance metrics, and for optimizing the prompt design
		  to improve the quality of the web-form tests.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= may,
  keywords	= {Automated Web-Form Testing, Large Language Models (LLMs),
		  Web-Form-Test Generation, Java Web Applications, Empirical
		  Study}
}

@InProceedings{	  10.1145/3722565.3727197,
  author	= {Mulayim, Ozan Baris and Fierro, Gabe and Berg\'{e}s, Mario
		  and Pritoni, Marco},
  title		= {Towards Zero-shot Question Answering in CPS-IoT: Large
		  Language Models and Knowledge Graphs},
  year		= {2025},
  isbn		= {9798400716089},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3722565.3727197},
  doi		= {10.1145/3722565.3727197},
  abstract	= {Natural language provides an intuitive interface for
		  querying data, yet its unstructured nature often makes
		  precise retrieval of information challenging. Knowledge
		  graphs (KGs), with their structured and relational
		  representations, offer a powerful solution to structuring
		  knowledge, while large language models (LLMs) are capable
		  of interpreting user intent through language. This
		  combination of KGs and LLMs has been explored extensively
		  for Knowledge Graph Question Answering (KGQA), primarily
		  for open-domain or encyclopedic knowledge. Domain-specific
		  KGQA, instead, presents significant opportunities for
		  Cyber-Physical Systems (CPS) and the Internet of Things
		  (IoT), where the extraction of structured metadata is
		  essential for automation and scalability of control and
		  analytics applications.In this work, we evaluate and
		  improve AutoKGQA, a domain-independent KGQA framework that
		  utilizes LLMs to generate structured queries. Through a
		  case study on KGs of sensor data from buildings, we assess
		  its ability to retrieve time series identifiers, which are
		  a requirement for extracting time series data from large
		  sensory databases. Our results demonstrate that while
		  AutoKGQA performs well in certain cases, its
		  domain-agnostic approach leads to systematic failures
		  particularly in complex queries requiring implicit
		  knowledge. We show that domain-specific prompting
		  significantly enhances query accuracy, allowing even
		  smaller LLMs to perform on par with larger ones. These
		  findings highlight the impact of domain-adapted prompting
		  in KGQA (DA-KGQA) and suggest a path toward more efficient,
		  scalable, and interpretable AI-driven metadata retrieval
		  for CPS-IoT applications.},
  booktitle	= {Proceedings of the 2nd International Workshop on
		  Foundation Models for Cyber-Physical Systems \&amp;
		  Internet of Things},
  pages		= {7–12},
  numpages	= {6},
  keywords	= {Knowledge Graphs, Large Language Models, Time series
		  Extraction},
  location	= {Irvine, CA, USA},
  series	= {FMSys}
}

@InProceedings{	  10.1145/3701716.3717817,
  author	= {Jadhav, Suramya and Perumal, Suki and Tadavi, Yasmin and
		  Dash, Bikshita and Parthiban, Srinivasan},
  title		= {Leveraging Large Language Models for Biomedical Knowledge
		  Graph Construction and Querying: An Advanced NLP Approach},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3717817},
  doi		= {10.1145/3701716.3717817},
  abstract	= {This paper introduces a novel methodology for constructing
		  a comprehensive biomedical knowledge graph by applying
		  advanced Natural Language Processing (NLP) techniques. By
		  leveraging Large Language Models (LLMs) and a multifaceted
		  prompt engineering approach, we effectively perform Named
		  Entity Recognition (NER) and Relation Extraction (RE) on
		  biomedical literature, targeting entities such as diseases,
		  drugs, proteins, procedures, and symptoms. Our methodology
		  incorporates eight distinct prompt engineering strategies
		  for NER and a standardized approach for RE, facilitating
		  the extraction of intricate inter-entity relationships. The
		  resulting knowledge graph amalgamates diverse data sources
		  into a unified framework, enabling efficient querying,
		  visualization, and analysis of biomedical information.
		  Furthermore, we present an innovative query processing
		  pipeline that integrates GPT-3.5 turbo with the knowledge
		  graph, allowing users to interact with the graph through
		  natural language. This integrated system empowers the
		  discovery of novel correlations, accelerating scientific
		  research and fostering interdisciplinary collaboration.
		  This represents a substantial contribution to the field of
		  biomedical knowledge graph construction, offering a robust
		  platform for accelerating scientific discovery and
		  informing clinical decision-making.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2560–2566},
  numpages	= {7},
  keywords	= {knowledge graphs, large language models, named entity
		  recognition, prompt engineering, query processing,
		  relationship extraction},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3701716.3717820,
  author	= {Chen, Haoting and Rodr\'{\i}guez M\'{e}ndez, Sergio
		  Jos\'{e} and Omran, Pouya Ghiasnezhad},
  title		= {Open Local Knowledge Graph Construction from Academic
		  Papers Using Generative Large Language Models},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3717820},
  doi		= {10.1145/3701716.3717820},
  abstract	= {This manuscript introduces paper2lkg, a novel Local
		  Knowledge Graph Construction (KGC) pipeline designed to
		  transform individual academic papers into their structured
		  local Knowledge Graph (KG) representations. The pipeline
		  harnesses Large Language Models (LLMs), particularly
		  generative LLMs, to automate key Natural Language
		  Processing (NLP) tasks in KGC. The constructed local KGs
		  can potentially be used to enrich an existing academic KG
		  that lacks detailed local representations of individual
		  papers or further integrated into new academic KGs through
		  Knowledge Graph Alignment (KGA).},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2551–2559},
  numpages	= {9},
  keywords	= {knowledge graph construction, large language model,
		  natural language processing},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3711896.3737384,
  author	= {Huang, Tenghao and Lee, Dong Hee and Sweeney, John and
		  Shi, Jiatong and Steliotes, Emily and Lange, Matthew and
		  May, Jonathan and Chen, Muhao},
  title		= {FoodPuzzle: Toward Developing Large Language Model Agents
		  as Autonomous Flavor Scientists},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3737384},
  doi		= {10.1145/3711896.3737384},
  abstract	= {Flavor development in the food industry is increasingly
		  challenged by the need for rapid innovation and precise
		  flavor profile creation. Traditional flavor research
		  methods typically rely on iterative, subjective testing,
		  which lacks the efficiency and scalability required for
		  modern demands. This paper presents three contributions to
		  address these challenges. Firstly, we define a new problem
		  domain for scientific agents in flavor science,
		  conceptualized as the generation of hypotheses for flavor
		  profile sourcing and understanding. By leveraging their
		  capacity to identify relevant evidence and reason within
		  large context spaces, language model-backed agents can
		  perform the labor-intensive tasks of flavor sourcing and
		  understanding with enhanced efficiency and precision. To
		  facilitate research in this area, we introduce the
		  FoodPuzzle dataset, a challenging benchmark consisting of
		  978 food items and 1,766 flavor molecule profiles. We
		  propose a novel Scientific Agent approach, integrating
		  in-context learning and retrieval augmented techniques to
		  generate grounded hypotheses in the domain of food science.
		  Experimental results indicate that our model significantly
		  surpasses traditional methods in flavor profile prediction
		  tasks, demonstrating its potential to transform flavor
		  development practices.},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {5493–5504},
  numpages	= {12},
  keywords	= {agent, flavor science, in-context learning, large language
		  models, retrieval-augmented generation},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@Article{	  10.1145/3725856,
  author	= {Civitarese, Gabriele and Fiori, Michele and Choudhary,
		  Priyankar and Bettini, Claudio},
  title		= {Large Language Models Are Zero-Shot Recognizers for
		  Activities of Daily Living},
  year		= {2025},
  issue_date	= {August 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3725856},
  doi		= {10.1145/3725856},
  abstract	= {The sensor-based recognition of Activities of Daily Living
		  (ADLs) in smart home environments enables several
		  applications in the areas of energy management, safety,
		  well-being, and healthcare. ADL recognition is typically
		  based on deep learning methods requiring large datasets to
		  be trained. Recently, several studies proved that Large
		  Language Models (LLMs) effectively capture common-sense
		  knowledge about human activities. However, the
		  effectiveness of LLMs for ADL recognition in smart home
		  environments still deserves to be investigated. In this
		  work, we propose ADL-LLM, a novel LLM-based ADL recognition
		  system. ADL-LLM transforms raw sensor data into textual
		  representations, that are processed by an LLM to perform
		  zero-shot ADL recognition. Moreover, in the scenario where
		  a small labeled dataset is available, ADL-LLM can also be
		  empowered with few-shot prompting. We evaluated ADL-LLM on
		  two public datasets, showing its effectiveness in this
		  domain.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= jun,
  articleno	= {78},
  numpages	= {32},
  keywords	= {Human Activity Recognition, Large Language Models, Smart
		  Home, Activities of Daily Living}
}

@Article{	  10.1145/3756016,
  author	= {Vasic, Iva and Fill, Hans-Georg and Quattrini, Ramona and
		  Pierdicca, Roberto},
  title		= {Knowledge Graphs vs. Large Language Models: Competitors or
		  Partners in Supporting Virtual Museums},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3756016},
  doi		= {10.1145/3756016},
  abstract	= {Virtual museums are factual means for the dissemination
		  and documentation of Cultural Heritage (CH) content. They
		  are suitable environments for the semantic annotation of
		  artifacts and automatic virtual guides. To this end, we
		  identify and compare Traditional (ontology-based), Large
		  Language Model (LLM)-extended, and LLM-pure methods for the
		  semantic information strategies of digital CH. The
		  traditional method is described through an application
		  prototype, while the methods that involve LLM are tested
		  experimentally. To investigate the integral tasks related
		  to LLMs, our experiments include (i) semantic annotation
		  using the CIDOC Conceptual Reference Model (CRM) and
		  Knowledge Graph (KG) generation with LLMs for a painting
		  sample, and (ii) painting ranking relying solely on LLMs
		  using catalog descriptions as input. The experiments
		  demonstrate the potential of these methods to enhance
		  artwork interpretation, description, and refinement of the
		  results. Based on the relevant literature on traditional
		  semantic annotation and conducted experiments with LLMs, a
		  combination of ontologies and LLMs may provide an optimal
		  approach, as it offers the accuracy of structured knowledge
		  while providing a tool that interprets these elements into
		  natural language and vice versa. Relying solely on LLMs may
		  be risky due to the lack of domain-specific knowledge in
		  the training data of LLMs, whereas traditional methods
		  demand expertise in a specific domain and are more
		  time-consuming. Our approach shows potential in use cases
		  such as guiding museum visitors to artifacts that match
		  their interests, assisting museum curators with
		  documentation, or helping CH researchers identify
		  similarities in artifact collections.},
  note		= {Just Accepted},
  journal	= {J. Comput. Cult. Herit.},
  month		= jul,
  keywords	= {Large Language Model, Cultural Heritage, Semantic
		  Annotation, Knowledge Graphs}
}

@Article{	  10.1145/3682069,
  author	= {Ampel, Benjamin and Yang, Chi-Heng and Hu, James and Chen,
		  Hsinchun},
  title		= {Large Language Models for Conducting Advanced Text
		  Analytics Information Systems Research},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {1},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3682069},
  doi		= {10.1145/3682069},
  abstract	= {The exponential growth of digital content has generated
		  massive textual datasets, necessitating the use of advanced
		  analytical approaches. Large Language Models (LLMs) have
		  emerged as tools that are capable of processing and
		  extracting insights from massive unstructured textual
		  datasets. However, how to leverage LLMs for text analytics
		  Information Systems (IS) research is currently unclear. To
		  assist the IS community in understanding how to
		  operationalize LLMs, we propose a Text Analytics for
		  Information Systems Research (TAISR) framework. Our
		  proposed framework provides detailed recommendations
		  grounded in IS and LLM literature on how to conduct
		  meaningful text analytics IS research for design science,
		  behavioral, and econometric streams. We conducted three
		  business intelligence case studies using our TAISR
		  framework to demonstrate its application in several IS
		  research contexts. We also outline the potential challenges
		  and limitations of adopting LLMs for IS. By offering a
		  systematic approach and evidence of its utility, our TAISR
		  framework contributes to future IS research streams looking
		  to incorporate powerful LLMs for text analytics.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= feb,
  articleno	= {2},
  numpages	= {27},
  keywords	= {Large language models, information systems research, text
		  analytics}
}

@InBook{	  10.1145/3677389.3702562,
  author	= {Xilong, Hou and Junhan, Zang and Xiaoguang, Wang},
  title		= {Leveraging Large Language Models for Classification of
		  Cultural Heritage Domain Terms: A Case Study on CIDOC CRM},
  year		= {2025},
  isbn		= {9798400710933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677389.3702562},
  abstract	= {Large language models (LLMs) have recently revolutionized
		  human language understanding and generation. Ontology is
		  considered one of the primary cornerstones for representing
		  knowledge in a more meaningful way on the semantic web.
		  It's significant to explore whether LLMs know and
		  understand such ontological knowledge. In this paper, we
		  report an experiment to investigate the performance of LLMs
		  in the task of classifying cultural heritage domain terms
		  to upper-level ontology. We first probed the understanding
		  and memorization of CIDOC CRM ontological knowledge by
		  LLMs. Then, we further leverage LLMs to classify domain
		  terms into the structure of CRM, and compare the match type
		  with experts. Our initial findings indicate that LLMs
		  demonstrate a certain level of awareness and comprehension
		  of CIDOC CRM ontological knowledge. LLMs have shown
		  potential as valuable assistants in enhancing ontology
		  engineering and knowledge-intensive tasks.},
  booktitle	= {Proceedings of the 24th ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {59},
  numpages	= {5}
}

@InProceedings{	  10.1145/3635059.3635104,
  author	= {Karanikolas, Nikitas and Manga, Eirini and Samaridi,
		  Nikoletta and Tousidou, Eleni and Vassilakopoulos,
		  Michael},
  title		= {Large Language Models versus Natural Language
		  Understanding and Generation},
  year		= {2024},
  isbn		= {9798400716263},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3635059.3635104},
  doi		= {10.1145/3635059.3635104},
  abstract	= {In recent years, the process humans adopt to learn a
		  foreign language has moved from the strict "Grammar
		  –Translation" method, which is based mainly on grammar
		  and syntax rules, to more innovative processes, resulting
		  to the more modern "Communicative approach". As its name
		  states, this approach focuses on the coherent communication
		  with native speakers and the cultivation of oral skills,
		  without taking into consideration, at least at the first
		  stages, the rules that govern the language. The same trend
		  seems to have been applied to the way machinery can be
		  "educated" to comprehend and reproduce the unfamiliar,
		  human language. The "rule based" Natural Language
		  Generation (NLG) and Natural Language Understanding (NLU)
		  algorithms, on one hand, and the "text based" Large
		  Language Models (LLMs), on the other, are two, analogous to
		  the two human foreign language learning processes, subareas
		  of Natural Language Processing (NLP). This paper presents
		  these two alternative approaches, LLMs (a technology having
		  surfaced as an influential catalyst of NLP, during last
		  years) on the one hand and NLG/NLU on the other,
		  highlighting their applications, their technologies, their
		  capabilities, their differences, their strengths and
		  weaknesses and the challenges they present, contributing to
		  a deeper comprehension of the evolving landscape of
		  Artificial Intelligence and human-computer communication.},
  booktitle	= {Proceedings of the 27th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {278–290},
  numpages	= {13},
  keywords	= {Large Language Models, Natural Language Generation,
		  Natural Language Processing, Natural Language
		  Understanding},
  location	= {Lamia, Greece},
  series	= {PCI '23}
}

@InProceedings{	  10.1145/3726302.3729970,
  author	= {Xiong, Qiushi and Xu, Zhipeng and Liu, Zhenghao and Wang,
		  Mengjia and Chen, Zulong and Sun, Yue and Gu, Yu and Li,
		  Xiaohua and Yu, Ge},
  title		= {Enhancing the Patent Matching Capability of Large Language
		  Models via the Memory Graph},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3729970},
  doi		= {10.1145/3726302.3729970},
  abstract	= {Intellectual Property (IP) management involves
		  strategically protecting and utilizing intellectual assets
		  to enhance organizational innovation, competitiveness, and
		  value creation. Patent matching is a crucial task in
		  intellectual property management, which facilitates the
		  organization and utilization of patents. Existing models
		  often rely on the emergent capabilities of Large Language
		  Models (LLMs) and leverage them to identify related patents
		  directly. However, these methods usually depend on matching
		  keywords and overlook the hierarchical classification and
		  categorical relationships of patents. In this paper, we
		  propose MemGraph, a method that augments the patent
		  matching capabilities of LLMs by incorporating a memory
		  graph derived from their parametric memory. Specifically,
		  MemGraph prompts LLMs to traverse their memory to identify
		  relevant entities within patents, followed by attributing
		  these entities to corresponding ontologies. After
		  traversing the memory graph, we utilize extracted entities
		  and ontologies to improve the capability of LLM in
		  comprehending the semantics of patents. Experimental
		  results on the PatentMatch dataset demonstrate the
		  effectiveness of MemGraph, achieving a 17.68\% performance
		  improvement over baseline LLMs. The further analysis
		  highlights the generalization ability of MemGraph across
		  various LLMs, both in-domain and out-of-domain, and its
		  capacity to enhance the internal reasoning processes of
		  LLMs during patent matching. All data and codes are
		  available at https://github.com/NEUIR/MemGraph.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {337–347},
  numpages	= {11},
  keywords	= {large language models, memory graph, patent matching,
		  retrieval-augmented generation},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@InProceedings{	  10.1145/3688671.3688770,
  author	= {Saketos, Vasileios and Pantazi, Despina-Athanasia and
		  Koubarakis, Manolis},
  title		= {The Large Language Model GreekLegalRoBERTa},
  year		= {2024},
  isbn		= {9798400709821},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3688671.3688770},
  doi		= {10.1145/3688671.3688770},
  abstract	= {We develop four versions of GreekLegalRoBERTa, which are
		  four large language models trained on Greek legal and
		  nonlegal text. We show that our models surpass the
		  performance of GreekLegalBERT, Greek- LegalBERT-v2, and
		  GreekBERT in two tasks involving Greek legal documents:
		  named entity recognition and multi-class legal topic
		  classification. We view our work as a contribution to the
		  study of domain-specific NLP tasks in low-resource
		  languages, like Greek, using modern NLP techniques and
		  methodologies.},
  booktitle	= {Proceedings of the 13th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {18},
  numpages	= {7},
  keywords	= {Natural Language Processing, Pre-trained Language Models,
		  Greek NLP Resources, Greek Legislation, Classification,
		  Named Entity Recognition},
  location	= { },
  series	= {SETN '24}
}

@InProceedings{	  10.5555/3709347.3743843,
  author	= {Torshizi, Parisa Ghanad and Hensel, Laura B. and Shapiro,
		  Ari and Marsella, Stacy C.},
  title		= {Large Language Models for Virtual Human Gesture
		  Selection},
  year		= {2025},
  isbn		= {9798400714269},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Co-speech gestures convey a wide variety of meanings and
		  play an important role in face-to-face human interactions.
		  These gestures have been shown to significantly influence
		  the addressee's engagement, recall, comprehension, and
		  attitudes toward the speaker. Similarly, they have been
		  shown to impact human and embodied virtual agent
		  interaction. The process of selecting and animating
		  meaningful gestures has thus become a key focus in
		  designing embodied virtual agents. However, the automation
		  of this gesture selection process poses a significant
		  challenge. Prior gesture generation techniques have
		  attempted to address this challenge in varied ways from
		  fully automated, data-driven techniques -- which often
		  struggle to produce contextually meaningful gestures -- to
		  more manual approaches of crafting gesture expertise, which
		  are time-consuming and lack generalizability. In this
		  paper, we leverage the semantic capabilities of Large
		  Language Models to realize a gesture selection approach
		  that suggests meaningful, appropriate co-speech gestures.
		  We first illustrate the information on gestures encoded
		  into GPT4. Then we perform a study to specifically evaluate
		  alternative prompting approaches for their ability to
		  select meaningful, contextually relevant gestures and to
		  align them appropriately to the co-speech utterance.
		  Finally, we detail and demonstrate how this approach has
		  been implemented within a virtual agent system, automating
		  the selection and subsequent animation of the selected
		  gestures for human-agent interactions.},
  booktitle	= {Proceedings of the 24th International Conference on
		  Autonomous Agents and Multiagent Systems},
  pages		= {2051–2059},
  numpages	= {9},
  keywords	= {gesture selection, large language models, virtual humans},
  location	= {Detroit, MI, USA},
  series	= {AAMAS '25}
}

@Article{	  10.1145/3711012,
  author	= {Liu, Yiren and Li, Yerong and Mayfield, Ryan and Huang,
		  Yun},
  title		= {Improving Emotional Support Delivery in Text-Based
		  Community Safety Reporting Using Large Language Models},
  year		= {2025},
  issue_date	= {May 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {2},
  url		= {https://doi.org/10.1145/3711012},
  doi		= {10.1145/3711012},
  abstract	= {Emotional support is a crucial aspect of communication
		  between community members and police dispatchers during
		  incident reporting. However, there is a lack of
		  understanding about how emotional support is delivered
		  through text-based systems, especially in various
		  non-emergency contexts. In this study, we analyzed two
		  years of chat logs comprising 57,114 messages across 8,239
		  incidents from 130 higher education institutions. Our
		  empirical findings revealed significant variations in
		  emotional support provided by dispatchers, influenced by
		  the type of incident, service time, and a noticeable
		  decline in support over time across multiple organizations.
		  To improve the consistency and quality of emotional
		  support, we developed and implemented a fine-tuned Large
		  Language Model (LLM), named dispatcherLLM, designed to
		  suggest replies through simulating human dispatchers'
		  languages with appropriate emotional support. We evaluated
		  dispatcherLLM by comparing its generated responses to those
		  of human dispatchers and other off-the-shelf models using
		  real chat messages. Additionally, we conducted a human
		  evaluation to assess the perceived effectiveness of the
		  support provided by dispatcherLLM. This study not only
		  contributes new empirical understandings of emotional
		  support in text-based dispatch systems but also
		  demonstrates the significant potential of generative AI in
		  improving service delivery.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= may,
  articleno	= {CSCW114},
  numpages	= {31},
  keywords	= {emotion classification, event argument extraction, large
		  language models, live chat, safety reporting, text-based
		  reporting system}
}

@InProceedings{	  10.1145/3652620.3686246,
  author	= {Siddeshwar, Vaishali and Alwidian, Sanaa and Makrehchi,
		  Masoud},
  title		= {A Comparative Study of Large Language Models for Goal
		  Model Extraction},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3686246},
  doi		= {10.1145/3652620.3686246},
  abstract	= {User stories, expressed in snippets of natural language
		  text, are commonly used to elicit stakeholder's needs in
		  agile software development. Requirement engineers model
		  user stories to interpret the relations among goals and
		  requirements. Manual transformation of goal models has
		  challenges such as, difficulty of converting
		  lower-abstraction user stories into higher-level goals, and
		  extraction of goals embedded in user stories depends on the
		  skill of requirements engineers. In this paper we introduce
		  a technique that leverages Large Language Models (LLMs) to
		  automatically generate goal models from user stories. The
		  approach uses Iterative Prompt Engineering that guides LLM
		  to extract intentional elements and generate its
		  XML-compatible representation in Goal-oriented Requirements
		  Language (GRL). The generated models can be visualized
		  using jUCMNav tool. We evaluated our approach using three
		  LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation
		  indicates that GPT-4 or Llama can be used to assist
		  requirements engineers in modeling as they can produce GRL
		  goal models that are understandable. Additionally, these
		  LLMs are capable of exposing soft goals that are not
		  apparent to stakeholders who are new to the domain.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {253–263},
  numpages	= {11},
  keywords	= {goal-oriented requirement language (GRL), goal modeling,
		  user story, agile development, requirements engineering,
		  large language models (LLMS), GPT-4, llama, cohere},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.1145/3735645,
  author	= {Zhou, Yinghai and Wang, Ziyu and Jiang, Yunxin and Ma,
		  Bingqi and Wang, Rui and Liu, Yuan and Zhao, Yue and Tian,
		  Zhihong},
  title		= {AEKG4APT: An AI-Enhanced Knowledge Graph for Advanced
		  Persistent Threats with Large Language Model Analysis},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3735645},
  doi		= {10.1145/3735645},
  abstract	= {This paper introduces AEKG4APT, an APT Knowledge Graph
		  (KG) enhanced by Large Language Models (LLMs), as a way to
		  deal with the cybersecurity problems caused by Advanced
		  Persistent Threats (APTs). The core of AEKG4APT lies in the
		  combined application of LLMs, Cyber Threat Intelligence
		  (CTI), and KG. The first part of the paper goes into great
		  detail about how the AEKG4APT was constructed, including
		  its ontology schema, data sources, and dataset features.
		  There are also statistics on the AEKG4APT’s nodes,
		  relationships, and key attributes. Secondly, it was shown
		  how to utilize LLMs and public sandboxes for the collection
		  and analysis of CTI Additionally, tests that compare
		  traditional deep learning models to LLM methods show that
		  LLM is both more efficient and more accurate at extracting
		  information. Subsequently, the Decision Making Trial and
		  Evaluation Laboratory - Interpretive Structural Modeling
		  (DEMATEL-ISM) analytical method was introduced to identify
		  and analyse the factors and their interrelationships within
		  the AEKG4APT data, thereby revealing the key dependencies
		  and influence paths within the data structure. Experiments
		  were designed to demonstrate its applications in modeling,
		  computing, and obtaining interpretable computational
		  results on AEKG4APT. In addition, this paper also explores
		  the dynamic expansion capabilities of AEKG4APT, including
		  data expansion, schema expansion, and permanent maintenance
		  strategies, to address the evolving APT threats. Finally,
		  this paper summarizes the competitiveness and application
		  value of AEKG4APT by comparing it with other CTI KGs and
		  platforms in academia and industry, demonstrating its
		  extensive application potential in the field of
		  cybersecurity.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= may,
  keywords	= {Advanced Persistent Threat, Large Language Models,
		  Knowledge Graph, Cyber Threat Intelligence, Sandboxes,
		  DEMATEL-ISM}
}

@Article{	  10.1145/3725852,
  author	= {Bombieri, Marco and Fiorini, Paolo and Ponzetto, Simone
		  Paolo and Rospocher, Marco},
  title		= {Do LLMs Dream of Ontologies?},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3725852},
  doi		= {10.1145/3725852},
  abstract	= {Large Language Models (LLMs) have demonstrated remarkable
		  performance across diverse natural language processing
		  tasks, yet their ability to memorize structured knowledge
		  remains underexplored. In this paper, we investigate the
		  extent to which general-purpose pre-trained LLMs retain and
		  correctly reproduce concept identifier (ID)–label
		  associations from publicly available ontologies. We conduct
		  a systematic evaluation across multiple ontological
		  resources, including the Gene Ontology, Uberon, Wikidata,
		  and ICD-10, using LLMs such as Pythia-12B,
		  Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal
		  that only a small fraction of ontological concepts is
		  accurately memorized, with GPT-4 demonstrating the highest
		  performance. To understand why certain concepts are
		  memorized more effectively than others, we analyze the
		  relationship between memorization accuracy and concept
		  popularity on the Web. Our results indicate a strong
		  correlation between the frequency of a concept’s
		  occurrence online and the likelihood of accurately
		  retrieving its ID from the label. This suggests that LLMs
		  primarily acquire such knowledge through indirect textual
		  exposure rather than directly from structured ontological
		  resources. Furthermore, we introduce new metrics to
		  quantify prediction invariance, demonstrating that the
		  stability of model responses across variations in prompt
		  language and temperature settings can serve as a proxy for
		  estimating memorization robustness.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= mar,
  keywords	= {Large Language Models, Memorization, Ontologies}
}

@InProceedings{	  10.1145/3716554.3716558,
  author	= {Maslaris, Ioannis and Karamanou, Areti and Kalampokis,
		  Evangelos and Tarabanis, Konstantinos},
  title		= {Evaluating Large Language Models in Interaction with Open
		  Government Data},
  year		= {2025},
  isbn		= {9798400713170},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3716554.3716558},
  doi		= {10.1145/3716554.3716558},
  abstract	= {Large Language Models (LLMs) exhibit great abilities in
		  understanding and generating natural language. Open
		  Government Data (OGD) are datasets that while are available
		  to the public, their linked structure makes it difficult to
		  access. Large language models can significantly enhance
		  access to linked open government data by enabling users to
		  interact with OGD portals using natural language. This
		  study examines the use of LLMs to interact with open
		  government linked data effectively and efficiently. Based
		  on the QB vocabulary, we develop a framework that
		  formulates the task. A set of 20 questions is developed to
		  assess the capabilities of LLMs to execute OGD-related
		  tasks. We propose a simple system in which LLMs interact
		  semi-automatically with OGD. Our findings indicate that
		  smaller and quantized versions of popular LLMs are capable
		  of effectively managing these tasks, with
		  Llama-3.1-8B-Instruct-bnb-4bit identified as the most
		  effective model. This paper aims to promote further
		  interest in systems that enhance Open Government Data
		  portals and improve public access to open data.},
  booktitle	= {Proceedings of the 28th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {26–33},
  numpages	= {8},
  keywords	= {large language models, open government data, linked data,
		  natural language processing},
  location	= { },
  series	= {PCI '24}
}

@InProceedings{	  10.1145/3638584.3638635,
  author	= {Zhou, Yifan and Ding, Yizhou and Dong, Yuwu and He, Hao},
  title		= {Ontology-Semantic Alignment On Contrastive Video-Language
		  Model for Multimodel Video Retrieval Task},
  year		= {2024},
  isbn		= {9798400708688},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638584.3638635},
  doi		= {10.1145/3638584.3638635},
  abstract	= {Contrastive Learning-based models have shown impressive
		  performance in text-image retrieval tasks. However, when
		  applied in video retrieval, traditional contrastive
		  learning strategies have faced challenges in achieving
		  satisfactory results due to redundancy of video contents.
		  We discern several potential reasons: (1)Current
		  methodologies sometimes overlook the significant
		  information imbalance between videos and query text,
		  specifically neglecting the in-depth textual representation
		  of the content within the videos. (2) Current video
		  matching methodologies typically focus on cross-model
		  alignment at general entity similarity level, without
		  specific consideration for how entity pair preferences and
		  similarity properties affect the task at hand. (3) Previous
		  vectorized retrieval based on video content features have
		  been somewhat flawed. They primarily focused on aligning
		  overall features without having an video content tags
		  feature for meaningful feature discrimination. Considering
		  the shortcomings identified in the mentioned three aspects,
		  we propose an ontology semantic labels augments retrieval
		  model and introduce a method to integrate video ontology
		  semantic labels into the contrastive learning framework. In
		  particular, we have developed ontology semantic
		  descriptions about entities encompassing both human figures
		  and textual elements within the videos. Subsequently, we
		  conducted training and testing on the CMIVQA dataset to
		  assess the performance of our approach. The experimental
		  results show that employing fine-grained ontology labels as
		  sample pairs for contrastive learning leads to an increased
		  level of precision in video retrieval tasks.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {408–413},
  numpages	= {6},
  keywords	= {Multimodal alignment, Ontology description, Video content
		  understanding},
  location	= {Beijing, China},
  series	= {CSAI '23}
}

@Article{	  10.1145/3748302,
  author	= {Zhang, Zeyu and Dai, Quanyu and Bo, Xiaohe and Ma, Chen
		  and Li, Rui and Chen, Xu and Zhu, Jieming and Dong, Zhenhua
		  and Wen, Ji-Rong},
  title		= {A Survey on the Memory Mechanism of Large Language Model
		  based Agents},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3748302},
  doi		= {10.1145/3748302},
  abstract	= {Large language model (LLM) based agents have recently
		  attracted much attention from the research and industry
		  communities. Compared with original LLMs, LLM-based agents
		  are featured in their self-evolving capability, which is
		  the basis for solving real-world problems that need
		  long-term and complex agent-environment interactions. The
		  key component to support agent-environment interactions is
		  the memory of the agents. While previous studies have
		  proposed many promising memory mechanisms, they are
		  scattered in different papers, and there lacks a
		  systematical review to summarize and compare these works
		  from a holistic perspective, failing to abstract common and
		  effective designing patterns for inspiring future studies.
		  To bridge this gap, in this paper, we propose a
		  comprehensive survey on the memory mechanism of LLM-based
		  agents. In specific, we first discuss “what is” and
		  “why do we need” the memory in LLM-based agents. Then,
		  we systematically review previous studies on how to design
		  and evaluate the memory module. In addition, we also
		  present many agent applications, where the memory module
		  plays an important role. At last, we analyze the
		  limitations of existing work and show important future
		  directions. To keep up with the latest advances in this
		  field, we create a repository at .},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jul,
  keywords	= {Information Processing, Information System, Large Language
		  Model, Agent, Memory Mechanism}
}

@InProceedings{	  10.1145/3711896.3736556,
  author	= {Xu, Ran and Jiang, Patrick and Luo, Linhao and Xiao, Cao
		  and Cross, Adam and Pan, Shirui and Sun, Jimeng and Yang,
		  Carl},
  title		= {A Survey on Unifying Large Language Models and Knowledge
		  Graphs for Biomedicine and Healthcare},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3736556},
  doi		= {10.1145/3711896.3736556},
  abstract	= {In recent years, the landscape of digital biomedicine and
		  healthcare has been reshaped due to the disruptive
		  breakthroughs in AI-facilitated by tremendous data and
		  high-performance computers, large language models (LLMs)
		  have transformed information technology from accessing data
		  to performing analytical tasks. While demonstrating
		  unprecedented capabilities, LLMs have been found unreliable
		  in tasks requiring factual knowledge and rigorous
		  reasoning. Biomedicine and healthcare, as an important
		  vertical domain rapidly benefitting from progress in AI,
		  necessitates strict requirements on the accuracy,
		  controllability, and interpretability of analytical models,
		  posing critical challenges for LLMs. Despite recent studies
		  addressing the hallucination problem of LLMs, research on
		  empowering LLMs with the ability to plan, reason, and
		  ground with explicit knowledge has also started to prosper,
		  especially in the biomedicine and healthcare domain. On the
		  other hand, biomedical data are enormous and notoriously
		  complex, coming from various sources (e.g., biomedical
		  knowledge bases, online literature, and hospitals) and
		  bearing various modalities (e.g., tables, texts, images and
		  time-series). Healthcare professionals have spent decades
		  collecting, cleaning, and curating various types of data.
		  The processes are extremely costly, producing various
		  datasets with different data schemas, coding systems, and
		  quality standards, many privately owned by the creators,
		  making their integrative analysis and utilization through
		  unified AI techniques still rather challenging. The
		  generalizability of LLMs across different types of data
		  endow them strong promises in automating the processing of
		  large-scale complex healthcare data such as into unified
		  knowledge graphs (KGs). Our goal in this survey is to
		  systematically investigate and summarize recent studies on
		  the unification of LLMs and KGs, towards fully utilizing
		  the value of complex data, unleashing the power of
		  generative AI, and expediting next-generation AI for
		  biomedicine and healthcare applications.},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {6195–6205},
  numpages	= {11},
  keywords	= {biomedical sciences, health informatics, knowledge graph,
		  large language model},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@InProceedings{	  10.1145/3715275.3732129,
  author	= {Kocyigit, Emre and Rossi, Arianna and Sergeeva, Anastasia
		  and Negri Ribalta, Claudia and Farjami, Ali and Lenzini,
		  Gabriele},
  title		= {DeceptiLens: an Approach supporting Transparency in
		  Deceptive Pattern Detection based on a Multimodal Large
		  Language Model},
  year		= {2025},
  isbn		= {9798400714825},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715275.3732129},
  doi		= {10.1145/3715275.3732129},
  abstract	= {To detect deceptive design patterns on UIs, traditional
		  artificial intelligence models, such as machine learning,
		  have limited coverage and a lack of multimodality. In
		  contrast, the capabilities of Multimodal Large Language
		  Model (MM-LLM) can achieve wider coverage with superior
		  performance in the detection, while providing reasoning
		  behind each decision. We propose and implement an
		  MM-LLM-based approach (DeceptiLens) that analyzes UIs and
		  assesses the presence of deceptive design patterns. We
		  utilize Retrieval Augmented Generation (RAG) process in our
		  design and task the model with capturing the deceptive
		  patterns, classifying its category, e.g., false hierarchy,
		  confirmshaming, etc., and explaining the reasoning behind
		  the classifications by employing recent prompt engineering
		  techniques, such as Chain-of-Thought (CoT). We first create
		  a dataset by collecting UI screenshots from the literature
		  and web sources and quantify the agreement between the
		  model’s outputs and a few experts’ opinions. We
		  additionally ask experts to gauge the transparency of the
		  system’s explanations for its classifications in terms of
		  recognized metrics of clarity, correctness, completeness,
		  and verifiability. The results indicate that our approach
		  is capable of capturing the deceptive patterns in UIs with
		  high accuracy while providing clear, correct, complete, and
		  verifiable justifications for its decisions. We
		  additionally release two curated datasets, one with
		  expert-labeled UIs with deceptive design patterns, and one
		  with AI-based generated explanations. Lastly, we propose
		  recommendations for future improvement of the approach in
		  various contexts of use.},
  booktitle	= {Proceedings of the 2025 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {1942–1959},
  numpages	= {18},
  keywords	= {dark patterns, deceptive design patterns, LLMs, multimodal
		  LLMs},
  location	= { },
  series	= {FAccT '25}
}

@Article{	  10.1145/3639372,
  author	= {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu,
		  Ninghao and Deng, Huiqi and Cai, Hengyi and Wang,
		  Shuaiqiang and Yin, Dawei and Du, Mengnan},
  title		= {Explainability for Large Language Models: A Survey},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3639372},
  doi		= {10.1145/3639372},
  abstract	= {Large language models (LLMs) have demonstrated impressive
		  capabilities in natural language processing. However, their
		  internal mechanisms are still unclear and this lack of
		  transparency poses unwanted risks for downstream
		  applications. Therefore, understanding and explaining these
		  models is crucial for elucidating their behaviors,
		  limitations, and social impacts. In this article, we
		  introduce a taxonomy of explainability techniques and
		  provide a structured overview of methods for explaining
		  Transformer-based language models. We categorize techniques
		  based on the training paradigms of LLMs: traditional
		  fine-tuning-based paradigm and prompting-based paradigm.
		  For each paradigm, we summarize the goals and dominant
		  approaches for generating local explanations of individual
		  predictions and global explanations of overall model
		  knowledge. We also discuss metrics for evaluating generated
		  explanations and discuss how explanations can be leveraged
		  to debug models and improve performance. Lastly, we examine
		  key challenges and emerging opportunities for explanation
		  techniques in the era of LLMs in comparison to conventional
		  deep learning models.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= feb,
  articleno	= {20},
  numpages	= {38},
  keywords	= {Explainability, interpretability, large language models}
}

@Article{	  10.1145/3721128,
  author	= {Zhang, Quanjun and Fang, Chunrong and Zheng, Yi and Zhang,
		  Yaxin and Zhao, Yuan and Huang, Rubing and Zhou, Jianyi and
		  Yang, Yun and Zheng, Tao and Chen, Zhenyu},
  title		= {Improving Deep Assertion Generation via Fine-Tuning
		  Retrieval-Augmented Pre-Trained Language Models},
  year		= {2025},
  issue_date	= {September 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {34},
  number	= {7},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3721128},
  doi		= {10.1145/3721128},
  abstract	= {Unit testing validates the correctness of the units of the
		  software system under test and serves as the cornerstone in
		  improving software quality and reliability. To reduce
		  manual efforts in writing unit tests, some techniques have
		  been proposed to generate test assertions automatically,
		  including Deep Learning (DL)-based, retrieval-based, and
		  integration-based ones. Among them, recent
		  integration-based approaches inherit from both DL-based and
		  retrieval-based approaches and are considered
		  state-of-the-art. Despite being promising, such
		  integration-based approaches suffer from inherent
		  limitations, such as retrieving assertions with lexical
		  matching while ignoring meaningful code semantics and
		  generating assertions with a limited training corpus.In
		  this article, we propose a novel Retrieval-Augmented Deep
		  Assertion Generation (RetriGen) approach based on a hybrid
		  assertion retriever and a Pre-Trained Language Model
		  (PLM)-based assertion generator. Given a focal-test,
		  RetriGen first builds a hybrid assertion retriever to
		  search for the most relevant test–assert pair from
		  external codebases. The retrieval process takes both
		  lexical similarity and semantical similarity into account
		  via a token-based and an embedding-based retriever,
		  respectively. RetriGen then treats assertion generation as
		  a sequence-to-sequence task and designs a PLM-based
		  assertion generator to predict a correct assertion with
		  historical test–assert pairs and the retrieved external
		  assertion. Although our concept is general and can be
		  adapted to various off-the-shelf encoder–decoder PLMs, we
		  implement RetriGen to facilitate assertion generation based
		  on the recent CodeT5 model. We conduct extensive
		  experiments to evaluate RetriGen against six
		  state-of-the-art approaches across two large-scale datasets
		  and two metrics. The experimental results demonstrate that
		  RetriGen achieves 57.66\% and 73.24\% in terms of accuracy
		  and CodeBLEU, outperforming all baselines with an average
		  improvement of 50.66\% and 14.14\%, respectively.
		  Furthermore, RetriGen generates 1,598 and 1,818 unique
		  correct assertions that all baselines fail to produce,
		  3.71X and 4.58X more than the most recent approach EditAS.
		  We also demonstrate that adopting other PLMs can provide
		  substantial advancement, e.g., four additionally utilized
		  PLMs outperform EditAS by 7.91\%–12.70\% accuracy
		  improvement, indicating the generalizability of RetriGen.
		  Overall, our study highlights the promising future of
		  fine-tuning off-the-shelf PLMs to generate accurate
		  assertions by incorporating external knowledge sources.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= aug,
  articleno	= {209},
  numpages	= {23},
  keywords	= {Unit Testing, Assertion Generation, Pre-trained Language
		  Models, AI4SE}
}

@InProceedings{	  10.1145/3706598.3713307,
  author	= {Haag, David and Kumar, Devender and Gruber, Sebastian and
		  Hofer, Dominik P., MSc and Sareban, Mahdi and Treff, Gunnar
		  and Niebauer, Josef and Bull, Christopher N and Schmidt,
		  Albrecht and Smeddinck, Jan David},
  title		= {The Last JITAI? Exploring Large Language Models for
		  Issuing Just-in-Time Adaptive Interventions: Fostering
		  Physical Activity in a Prospective Cardiac Rehabilitation
		  Setting},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713307},
  doi		= {10.1145/3706598.3713307},
  abstract	= {We evaluated the viability of using Large Language Models
		  (LLMs) to trigger and personalize content in Just-in-Time
		  Adaptive Interventions (JITAIs) in digital health. As an
		  interaction pattern representative of context-aware
		  computing, JITAIs are being explored for their potential to
		  support sustainable behavior change, adapting interventions
		  to an individual's current context and needs. Challenging
		  traditional JITAI implementation models, which face severe
		  scalability and flexibility limitations, we tested GPT-4
		  for suggesting JITAIs in the use case of heart-healthy
		  activity in cardiac rehabilitation. Using three personas
		  representing patients affected by CVD with varying
		  severeness and five context sets per persona, we generated
		  450 JITAI decisions and messages. These were systematically
		  evaluated against those created by 10 laypersons (LayPs)
		  and 10 healthcare professionals (HCPs). GPT-4-generated
		  JITAIs surpassed human-generated intervention suggestions,
		  outperforming both LayPs and HCPs across all metrics (i.e.,
		  appropriateness, engagement, effectiveness, and
		  professionalism). These results highlight the potential of
		  LLMs to enhance JITAI implementations in personalized
		  health interventions, demonstrating how generative AI could
		  revolutionize context-aware computing.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {644},
  numpages	= {18},
  keywords	= {JITAIs, LLMs, adaptive interventions, context-aware
		  computing, digital health, generative AI, healthcare AI,
		  human-AI interaction, just-in-time adaptive interventions,
		  large language models},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3696410.3714720,
  author	= {Bouadi, Mohamed and Alavi, Arta and Benbernou, Salima and
		  Ouziri, Mourad},
  title		= {Synergizing Large Language Models and Knowledge-Based
		  Reasoning for Interpretable Feature Engineering},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696410.3714720},
  doi		= {10.1145/3696410.3714720},
  abstract	= {Feature engineering stands as a pivotal step in enhancing
		  the performance of machine learning (ML) models,
		  particularly with tabular data. However, traditional
		  feature engineering methods are often time-consuming and
		  requires case-by-case domain knowledge. In addition, as ML
		  systems become more common, interpretability becomes
		  increasingly important, especially among domain experts. To
		  this end, we propose ReaGen, an automated feature
		  engineering (AutoFE) approach that combines knowledge
		  graphs (KGs) with large language models (LLMs) to generate
		  interpretable features. ReaGen begins by symbolic REAsoning
		  over the KG to extract relevant information based on
		  datasets description. Then, it uses an LLM to iteratively
		  GENerate meaningful features. Finally, to overcome
		  challenges such as hallucinations and handling long
		  contexts typical in LLMs, our model performs logical
		  reasoning on the KG to ensure that the generated features
		  maintain interpretability. ReaGen provides Python code for
		  automatic feature generation and detailed explanations of
		  feature utility. It leverages both LLM's internal knowledge
		  and retrieved information from KGs. Experiments on public
		  datasets demonstrate that ReaGen significantly improves
		  prediction accuracy while ensuring high interpretability
		  through human-like explanations for each feature. This work
		  highlights the potential of integrating LLMs and KGs in
		  feature engineering, paving the way for interpretable ML
		  models.},
  booktitle	= {Proceedings of the ACM on Web Conference 2025},
  pages		= {2606–2620},
  numpages	= {15},
  keywords	= {automated feature engineering, interpretable ML, knowledge
		  graphs, large language models, symbolic reasoning},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@Article{	  10.1145/3700597,
  author	= {Ke, Ping Fan and Ng, Ka Chung},
  title		= {Human-AI Synergy in Survey Development: Implications from
		  Large Language Models in Business and Research},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {1},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3700597},
  doi		= {10.1145/3700597},
  abstract	= {This study examines the novel integration of Large
		  Language Models (LLMs) into the survey development process
		  in business and research through the development and
		  evaluation of the Behavioral Research Assistant (BRASS)
		  Bot. We first analyzed the traditional scale development
		  process to identify tasks suitable for LLM integration,
		  including both human-in-the-loop and automated LLM data
		  collection methods. Following this analysis, we developed
		  the details of BRASS Bot, incorporating design principles
		  of falsifiability and reproducibility. We then conducted a
		  comprehensive evaluation of the BRASS Bot across a diverse
		  set of LLMs, including GPT, Claude, Gemini, and Llama, to
		  assess its usability, validity, and reliability. We further
		  demonstrated the practical utility of the BRASS Bot by
		  conducting a user study and a predictive validity
		  simulation. Our research presents both theoretical and
		  practical implications. The augmentation approach of the
		  BRASS Bot enriches the theoretical foundations of
		  behavioral constructs by identifying previously overlooked
		  patterns. Additionally, the BRASS Bot offers significant
		  time and resource efficiency gains while enhancing scale
		  validity. Our work lays the foundation for future research
		  on the broader application of LLMs as both assistants and
		  collaborators in survey analysis and behavioral research
		  design and execution, highlighting their potential for a
		  transformative impact on the field.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= feb,
  articleno	= {9},
  numpages	= {39},
  keywords	= {Large Language Model, generative AI, scale development,
		  behavioral research}
}

@InProceedings{	  10.1145/3627673.3679156,
  author	= {Peng, Yiwen and Bonald, Thomas and Alam, Mehwish},
  title		= {Refining Wikidata Taxonomy using Large Language Models},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679156},
  doi		= {10.1145/3627673.3679156},
  abstract	= {Due to its collaborative nature, Wikidata is known to have
		  a complex taxonomy, with recurrent issues like the
		  ambiguity between instances and classes, the inaccuracy of
		  some taxonomic paths, the presence of cycles, and the high
		  level of redundancy across classes. Manual efforts to clean
		  up this taxonomy are time-consuming and prone to errors or
		  subjective decisions. We present WiKC, a new version of
		  Wikidata taxonomy cleaned automatically using a combination
		  of Large Language Models (LLMs) and graph mining
		  techniques. Operations on the taxonomy, such as cutting
		  links or merging classes, are performed with the help of
		  zero-shot prompting on an open-source LLM. The quality of
		  the refined taxonomy is evaluated from both intrinsic and
		  extrinsic perspectives, on a task of entity typing for the
		  latter, showing the practical interest of WiKC.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5395–5399},
  numpages	= {5},
  keywords	= {graph mining, knowledge graphs, large language model},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3696410.3714667,
  author	= {Barile, Roberto and d'Amato, Claudia and Fanizzi, Nicola},
  title		= {LP-DIXIT: Evaluating Explanations for Link Predictions on
		  Knowledge Graphs using Large Language Models},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696410.3714667},
  doi		= {10.1145/3696410.3714667},
  abstract	= {Knowledge Graphs provide a machine-readable representation
		  of knowledge conforming to graph-based data models. Link
		  prediction methods predict missing facts in incomplete
		  knowledge graphs, often using scalable embedding based
		  solutions that, however, lack comprehensibility which is
		  crucial in many domains. Filling this gap, explanation
		  methods identify supporting knowledge. For evaluating them,
		  user studies are the obvious choice as users are the main
		  recipients of explanations. However, finding domain experts
		  is often challenging. In contrast, an automated approach is
		  to measure the influence of explanations on the very same
		  link prediction task, thus disregarding the perspective of
		  users. Additionally, current evaluation methods vary across
		  different explanation approaches. We propose LP-DIXIT, the
		  first protocol to evaluate the utility of explanations of
		  link predictions. LP-DIXIT is user-aware, algorithmic and
		  unique for different explanation methods. It builds on a
		  typical setting of user studies, but adopts Large Language
		  Models (LLMs) to mimic users. Specifically, it measures how
		  explanations improve the user (LLM) ability to perform
		  predictions, which is key to trust. We experimentally
		  proved an overall agreement between LP-DIXIT and user
		  evaluations. Moreover, we adopted LP-DIXIT to conduct a
		  comparative study of state-of-the-art explanation methods.
		  The outcomes suggest that less is more: the most effective
		  explanations are those consisting of a single fact.},
  booktitle	= {Proceedings of the ACM on Web Conference 2025},
  pages		= {4034–4042},
  numpages	= {9},
  keywords	= {explanation, knowledge graphs, large language models, link
		  prediction},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3626772.3657848,
  author	= {Zhai, ChengXiang},
  title		= {Large Language Models and Future of Information Retrieval:
		  Opportunities and Challenges},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657848},
  doi		= {10.1145/3626772.3657848},
  abstract	= {Recent years have seen great success of large language
		  models (LLMs) in performing many natural language
		  processing tasks with impressive performance, including
		  tasks that directly serve users such as question answering
		  and text summarization. They open up unprecedented
		  opportunities for transforming information retrieval (IR)
		  research and applications. However, concerns such as
		  halluciation undermine their trustworthiness, limiting
		  their actual utility when deployed in real-world
		  applications, especially high-stake applications where
		  trust is vital. How can we both exploit the strengths of
		  LLMs and mitigate any risk caused by their weaknesses when
		  applying LLMs to IR? What are the best opportunities for us
		  to apply LLMs to IR? What are the major challenges that we
		  will need to address in the future to fully exploit such
		  opportunities? Given the anticipated growth of LLMs, what
		  will future information retrieval systems look like? Will
		  LLMs eventually replace an IR system? In this perspective
		  paper, we examine these questions and provide provisional
		  answers to them. We argue that LLMs will not be able to
		  replace search engines, and future LLMs would need to learn
		  how to use a search engine so that they can interact with a
		  search engine on behalf of users. We conclude with a set of
		  promising future research directions in applying LLMs to
		  IR.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {481–490},
  numpages	= {10},
  keywords	= {conversational information access, information retrieval
		  models, intelligent agent, large language models, search
		  engines},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3706598.3714069,
  author	= {Alvarado Garcia, Adriana and Candello, Heloisa and
		  Badillo-Urquiola, Karla and Wong-Villacres, Marisol},
  title		= {Emerging Data Practices: Data Work in the Era of Large
		  Language Models},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3714069},
  doi		= {10.1145/3706598.3714069},
  abstract	= {Data is one of the foundational aspects of making
		  Artificial Intelligence (AI) work as intended. As large
		  language models (LLMs) become the epicenter of AI, it is
		  crucial to understand better how the datasets that maintain
		  such models are created. The emergent nature of LLMs makes
		  it critical to understand the challenges practitioners
		  developing Gen AI technologies face to design alternatives
		  for better responding to Gen AI’s ethical issues. In this
		  paper, we provide such understanding by reporting on 25
		  interviews with practitioners who handle data in three
		  distinct development stages of different LLMs. Our
		  contributions are (1) empirical evidence of how
		  uncertainty, data practices, and reliance mechanisms change
		  across LLMs’ development cycle; (2) how the unique
		  qualities of LLMs impact data practices and their
		  implications for the future of Gen AI technologies; and (3)
		  provide three opportunities for HCI researchers interested
		  in supporting practitioners developing Gen AI
		  technologies.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {846},
  numpages	= {21},
  keywords	= {data work, data practices, AI, LLMs, synthetic data, data
		  governance, AI practitioners, GenAI, generative AI},
  location	= { },
  series	= {CHI '25}
}

@Article{	  10.1145/3736408,
  author	= {Koyuncu, Anil},
  title		= {Exploring Fine-Grained Bug Report Categorization with
		  Large Language Models and Prompt Engineering: An Empirical
		  Study},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3736408},
  doi		= {10.1145/3736408},
  abstract	= {Accurate classification of issues is essential for
		  effective project management and timely responses, as the
		  volume of issue reports continues to grow. Manual
		  classification is labor-intensive and error-prone,
		  necessitating automated solutions. While large language
		  models (LLMs) show promise in automated issue labeling,
		  most research focuses on broad categorization (e.g., bugs,
		  feature requests), with limited attention to fine-grained
		  categorization. Understanding specific bug types is
		  crucial, as different bugs require tailored resolution
		  strategies.This study addresses this gap by evaluating LLMs
		  and prompt engineering strategies for fine-grained bug
		  report categorization. We analyze 221,184 fine-grained bug
		  report category labels generated by selected LLMs using
		  various prompt engineering strategies for 1,024 bug
		  reports. We examine how LLMs and prompt engineering
		  influence output characteristics, control over outputs, and
		  categorization performance. Our findings highlight that
		  LLMs and prompt engineering significantly impact output
		  consistency and classification capability, with some
		  yielding consistent results and others introducing
		  variability. Based on these findings, we analyze the
		  agreements and disagreements between LLM-generated labels
		  and human annotations to assess category correctness. Our
		  results suggest that examining label consistency and
		  discrepancies can serve as a complementary method for
		  validating bug report categories, identifying unclear
		  reports, and detecting misclassifications in human
		  annotations.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= may,
  keywords	= {Prompt Engineering, Large Language Models, Automatic Bug
		  Report Classification, Label correctness}
}

@InProceedings{	  10.1145/3637528.3671469,
  author	= {Zhang, Yunyi and Zhong, Ming and Ouyang, Siru and Jiao,
		  Yizhu and Zhou, Sizhe and Ding, Linyi and Han, Jiawei},
  title		= {Automated Mining of Structured Knowledge from Text in the
		  Era of Large Language Models},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671469},
  doi		= {10.1145/3637528.3671469},
  abstract	= {Massive amount of unstructured text data are generated
		  daily, ranging from news articles to scientific papers. How
		  to mine structured knowledge from the text data remains a
		  crucial research question. Recently, large language models
		  (LLMs) have shed light on the text mining field with their
		  superior text understanding and instruction-following
		  ability. There are typically two ways of utilizing LLMs:
		  fine-tune the LLMs with human-annotated training data,
		  which is labor intensive and hard to scale; prompt the LLMs
		  in a zero-shot or few-shot way, which cannot take advantage
		  of the useful information in the massive text data.
		  Therefore, it remains a challenge on automated mining of
		  structured knowledge from massive text data in the era of
		  large language models. In this tutorial, we cover the
		  recent advancements in mining structured knowledge using
		  language models with very weak supervision. We will
		  introduce the following topics in this tutorial: (1)
		  introduction to large language models, which serves as the
		  foundation for recent text mining tasks, (2) ontology
		  construction, which automatically enriches an ontology from
		  a massive corpus, (3) weakly-supervised text classification
		  in flat and hierarchical label space, (4) weakly-supervised
		  information extraction, which extracts entity and relation
		  structures.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6644–6654},
  numpages	= {11},
  keywords	= {large language models, text mining, weak supervision},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3650400.3650478,
  author	= {Zhao, Wei and Chen, Qinghui and You, Junling},
  title		= {LlmRe: A zero-shot entity relation extraction method based
		  on the large language model},
  year		= {2024},
  isbn		= {9798400708305},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3650400.3650478},
  doi		= {10.1145/3650400.3650478},
  abstract	= {Entity relation extraction aims to extract knowledge
		  triples from unstructured or semi-structured text data and
		  can be applied to various fields, including medicine,
		  finance knowledge graph construction and intelligent
		  question-answering. Traditional entity relation extraction
		  requires a large amount of labeled data, consumes a lot of
		  labor and time, and the trained model lacks generalization
		  ability, which is difficult to migrate to other fields.
		  Zero-shot entity relation extraction relieves the
		  dependence on labeled data in traditional method. Based on
		  unlabeled text data, zero-shot entity relation extraction
		  has strong domain adaptability, which is a very challenging
		  and practical task. Recent work on large language models
		  shows that large models can effectively complete downstream
		  tasks through natural language instructions and have good
		  generalization ability. Inspired by this, we explore the
		  use of large models for information extraction. Due to the
		  randomness of large language model generation, we introduce
		  in-context learning in entity relation extraction task to
		  guide large language model to output data in a specified
		  format to help obtain structured data. At the same time, we
		  propose a three-stage extraction framework for decomposing
		  entity relation extraction tasks, and each stage is
		  conducted in the form of question and answer to reduce the
		  complexity of extraction. We evaluated the knowledge
		  triples extraction performance of the model on three
		  self-built test datasets in different fields, and the
		  experimental result showed that our proposed method
		  achieved impressive performance in the zero-shot entity
		  relation extraction task, surpassing the comparison model
		  on multiple metrics, proving the effectiveness and domain
		  adaptability of the proposed method.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {475–480},
  numpages	= {6},
  location	= {Xiamen, China},
  series	= {EITCE '23}
}

@InProceedings{	  10.1145/3605098.3635889,
  author	= {Arrieta, Kutz and Fillottrani, Pablo R and Keet, C.
		  Maria},
  title		= {CoSMo: A multilingual modular language for Content
		  Selection Modelling},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3635889},
  doi		= {10.1145/3605098.3635889},
  abstract	= {Representing snippets of information abstractly is a task
		  that needs to be performed for various purposes, such as
		  database view specification and the first stage in the
		  natural language generation pipeline for generative AI from
		  structured input, i.e., the content selection stage to
		  determine what needs to be verbalised. For the Abstract
		  Wikipedia project, requirements analysis revealed that such
		  an abstract representation requires multilingual modelling,
		  content selection covering declarative content and
		  functions, and both classes and instances. There is no
		  modelling language that meets either of the three features,
		  let alone a combination. Following a rigorous language
		  design process inclusive of broad stakeholder consultation,
		  we created CoSMo, a novel Content Selection Modeling
		  language that meets these and other requirements so that it
		  may be useful both in Abstract Wikipedia as well as other
		  contexts. We describe the design process, rationale and
		  choices, the specification, and preliminary evaluation of
		  the language.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {706–713},
  numpages	= {8},
  keywords	= {modeling language, query language, wikidata,
		  multilingualism},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@InBook{	  10.1145/3727648.3727757,
  author	= {Hu, Ruijuan and Yue, Zhihui and Zhou, Yuzhen and Zhou,
		  Huijuan},
  title		= {Research on the Representation and Construction Technique
		  of Event Causality Graph Based on Large Language Models},
  year		= {2025},
  isbn		= {9798400712647},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3727648.3727757},
  abstract	= {Event graph representation aims to construct a highly
		  accurate and comprehensive event knowledge representation
		  system. Existing event graph representation models lack
		  adaptability in representing dynamic knowledge, which
		  results in under utilization of their potential
		  performance. This paper proposes an event causality graph
		  representation model based on large language models(LLMs).
		  By integrating dynamic event knowledge with static data
		  knowledge, the model draws on the 5W1H theory and adopts a
		  top-down approach combined with a bottom-up approach using
		  LLMs retrieval augmented generation(RAG). This approach
		  designs and constructs an event semantic representation
		  model. Through matching the relevance of different argument
		  roles in scenario knowledge, it achieves adaptive
		  representation across different search spaces, thereby
		  realizing the construction of an event causality graph.
		  Experiments demonstrate the effectiveness of generating
		  event types based on RAG and the dynamic adaptability of
		  the constructed event causality graph.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Computer, Artificial Intelligence and Control Engineering},
  pages		= {672–679},
  numpages	= {8}
}

@Article{	  10.1145/3652028,
  author	= {Spinner, Thilo and Kehlbeck, Rebecca and Sevastjanova,
		  Rita and St\"{a}hle, Tobias and Keim, Daniel A. and
		  Deussen, Oliver and El-Assady, Mennatallah},
  title		= {-generAItor: Tree-in-the-loop Text Generation for Language
		  Model Explainability and Adaptation},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3652028},
  doi		= {10.1145/3652028},
  abstract	= {Large language models (LLMs) are widely deployed in
		  various downstream tasks, e.g., auto-completion, aided
		  writing, or chat-based text generation. However, the
		  considered output candidates of the underlying search
		  algorithm are under-explored and under-explained. We tackle
		  this shortcoming by proposing a tree-in-the-loop approach,
		  where a visual representation of the beam search tree is
		  the central component for analyzing, explaining, and
		  adapting the generated outputs. To support these tasks, we
		  present generAItor, a visual analytics technique,
		  augmenting the central beam search tree with various
		  task-specific widgets, providing targeted visualizations
		  and interaction possibilities. Our approach allows
		  interactions on multiple levels and offers an iterative
		  pipeline that encompasses generating, exploring, and
		  comparing output candidates, as well as fine-tuning the
		  model based on adapted data. Our case study shows that our
		  tool generates new insights in gender bias analysis beyond
		  state-of-the-art template-based methods. Additionally, we
		  demonstrate the applicability of our approach in a
		  qualitative user study. Finally, we quantitatively evaluate
		  the adaptability of the model to few samples, as occurring
		  in text-generation use cases.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= jun,
  articleno	= {14},
  numpages	= {32},
  keywords	= {Large language models, beam search tree, natural language
		  generation, explainability, language transformers, visual
		  analytics}
}

@InProceedings{	  10.1145/3716554.3716603,
  author	= {Zeginis, Dimitris and Kalampokis, Evangelos and Tarabanis,
		  Konstantinos},
  title		= {Applying an ontology-aware zero-shot LLM prompting
		  approach for information extraction in Greek: the case of
		  DIAVGEIA gov gr},
  year		= {2025},
  isbn		= {9798400713170},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3716554.3716603},
  doi		= {10.1145/3716554.3716603},
  abstract	= {Large Language Models (LLMs) have attracted considerable
		  attention, primarily due to their potential to
		  revolutionize sectors that heavily rely on textual
		  information. Governance is one such sector. Public
		  administrations around the globe produce millions of
		  documents including laws, administrative decisions and acts
		  (e.g., travel/budget approvals) that contain valuable
		  information in unstructured way. The documents are usually
		  stored at document-centered repositories. As a result the
		  actual data of the documents cannot be further searched or
		  processed. The availability of structured metadata of the
		  documents (e.g., who has traveled, where, when) could
		  further enhance the searching and processing of the
		  documents as well as enable data analytics. The
		  construction of metadata can be done through information
		  extraction approaches such as Named Entity Recognition
		  (NER), Relation Extraction (RE) and Event Extraction (EE)
		  on the documents. LLMs are recently used successfully for
		  information extraction tasks, while ontologies are
		  traditionally used for meaningful data modeling. The aim of
		  the paper is to apply and evaluate an ontology-aware
		  zero-shot LLM prompting approach for information extraction
		  in Greek language documents available in DIAVGEIA.gov.gr -
		  the Greek Open Government portal for administrative
		  documents. The evaluation assesses various LLM models/sizes
		  for various difficulties of information extraction tasks.
		  Overall the results are very promising, since most LLM
		  models, even smaller ones, performed very well for all
		  tasks in Greek.},
  booktitle	= {Proceedings of the 28th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {324–330},
  numpages	= {7},
  keywords	= {Information extraction, NER, LLM, Ontology, Greek, Public
		  administration},
  location	= { },
  series	= {PCI '24}
}

@Article{	  10.1145/3702234,
  author	= {Fang, Chen and Wang, Yidong and Song, Yunze and Long,
		  Qingqing and Lu, Wang and Chen, Linghui and Feng, Guihai
		  and Zhou, Yuanchun and Li, Xin},
  title		= {How do Large Language Models understand Genes and Cells},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3702234},
  doi		= {10.1145/3702234},
  abstract	= {Researching genes and their interactions is crucial for
		  deciphering the fundamental laws of cellular activity,
		  advancing disease treatment, drug discovery, and more.
		  Large language Models (LLMs), with their profound text
		  comprehension and generation capabilities, have made
		  significant strides across various natural science fields.
		  However, their application in cell biology remains limited
		  and a systematic evaluation of their performance is
		  lacking. To address this gap, in this paper, we select
		  seven mainstream LLMs and evaluate their performance across
		  nine gene-related problem scenarios. Our findings indicate
		  that LLMs possess a certain level of understanding of genes
		  and cells, but still lag behind domain-specific models in
		  comprehending transcriptional expression profiles.
		  Moreover, we have improved the current method of textual
		  representation of cells, enhancing the LLMs’ ability to
		  tackle cell annotation tasks. We encourage cell biology
		  researchers to leverage LLMs for problem-solving while
		  being mindful of the associated challenges. We release our
		  code and data at .},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  keywords	= {large language models, cell biology, gene gene
		  interaction, cell annotation}
}

@Article{	  10.1145/3678470,
  author	= {Swaileh A. Alzaidi, Muhammad and Alshammari, Alya and
		  Almanea, Manar and Al-khawaja, Haneen A. and Al Sultan,
		  Hanan and Alotaibi, Shoayee and Almukadi, Wafa},
  title		= {A Text-Inception-Based Natural Language Processing Model
		  for Sentiment Analysis of Drug Experiences},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3678470},
  doi		= {10.1145/3678470},
  abstract	= {The study of sentiment in Natural Language Processing
		  (NLP) is among the most successful research areas because
		  of the availability of millions of user opinions online
		  since the turn of the century. The economic, political, and
		  medical fields are just some of the many that have
		  benefited from studies of sentiment research. While
		  numerous studies have examined more mainstream topics like
		  consumer electronics, movies, and restaurants, relatively
		  few have examined health and medical concerns. Considerable
		  insight into where to direct efforts to improve public
		  health might be gained by a study of how people feel about
		  healthcare as a whole and of individual drug experiences in
		  particular. When it comes to medicine, automatic analysis
		  of online user evaluations paves the way for sifting
		  through massive amounts of user feedback to find
		  information regarding medications' efficacy and side
		  effects that might be used to enhance pharmacovigilance
		  programs. Simple rules-based methods have given way to more
		  complex machine learning approaches like deep learning,
		  which is developing as a technology for many natural
		  language processing jobs. The opensource datasets have been
		  analyzed with models that use word embeddings and term
		  frequency-inverse document frequency (TF-IDF). A
		  feature-enhanced text-inception model for sentiment
		  classification was presented to work in tandem with this
		  approach. The model first employed a cutting-edge
		  text-inception module to glean useful shallow features from
		  the text. K-MaxPooling was subsequently employed to reduce
		  the dimensionality of its shallow and deep includes as well
		  as enhance the generalization of characteristics, and a
		  deep feature extraction module was formed using the
		  bidirectional gated recurrent unit (Bi-GRU) and the capsule
		  neural network to comprehend the text's semantic data. By
		  combining traditional methods with cutting-edge artificial
		  intelligence techniques, this hybrid approach can
		  revolutionize public health initiatives, decision-making,
		  and pharmacovigilance in the healthcare industry. This
		  model achieved an exceptional accuracy rate of 99\%,
		  underscoring its effectiveness in sentiment classification
		  and demonstrating its potential to significantly contribute
		  to advancing healthcare and medical research.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  keywords	= {Natural Language Processing (NLP), User Opinions,
		  Healthcare, Medical Sentiment, Public Health, Deep
		  learning}
}

@Article{	  10.1145/3611651,
  author	= {Wang, Benyou and Xie, Qianqian and Pei, Jiahuan and Chen,
		  Zhihong and Tiwari, Prayag and Li, Zhao and Fu, Jie},
  title		= {Pre-trained Language Models in Biomedical Domain: A
		  Systematic Survey},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3611651},
  doi		= {10.1145/3611651},
  abstract	= {Pre-trained language models (PLMs) have been the de facto
		  paradigm for most natural language processing tasks. This
		  also benefits the biomedical domain: researchers from
		  informatics, medicine, and computer science communities
		  propose various PLMs trained on biomedical datasets, e.g.,
		  biomedical text, electronic health records, protein, and
		  DNA sequences for various biomedical tasks. However, the
		  cross-discipline characteristics of biomedical PLMs hinder
		  their spreading among communities; some existing works are
		  isolated from each other without comprehensive comparison
		  and discussions. It is nontrivial to make a survey that not
		  only systematically reviews recent advances in biomedical
		  PLMs and their applications but also standardizes
		  terminology and benchmarks. This article summarizes the
		  recent progress of pre-trained language models in the
		  biomedical domain and their applications in downstream
		  biomedical tasks. Particularly, we discuss the motivations
		  of PLMs in the biomedical domain and introduce the key
		  concepts of pre-trained language models. We then propose a
		  taxonomy of existing biomedical PLMs that categorizes them
		  from various perspectives systematically. Plus, their
		  applications in biomedical downstream tasks are
		  exhaustively discussed, respectively. Last, we illustrate
		  various limitations and future trends, which aims to
		  provide inspiration for the future research.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {55},
  numpages	= {52},
  keywords	= {Biomedical domain, pre-trained language models, natural
		  language processing}
}

@Article{	  10.1145/3605943,
  author	= {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh,
		  Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and
		  Agirre, Eneko and Heintz, Ilana and Roth, Dan},
  title		= {Recent Advances in Natural Language Processing via Large
		  Pre-trained Language Models: A Survey},
  year		= {2023},
  issue_date	= {February 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3605943},
  doi		= {10.1145/3605943},
  abstract	= {Large, pre-trained language models (PLMs) such as BERT and
		  GPT have drastically changed the Natural Language
		  Processing (NLP) field. For numerous NLP tasks, approaches
		  leveraging PLMs have achieved state-of-the-art performance.
		  The key idea is to learn a generic, latent representation
		  of language from a generic task once, then share it across
		  disparate NLP tasks. Language modeling serves as the
		  generic task, one with abundant self-supervised text
		  available for extensive training. This article presents the
		  key fundamental concepts of PLM architectures and a
		  comprehensive view of the shift to PLM-driven NLP
		  techniques. It surveys work applying the pre-training then
		  fine-tuning, prompting, and text generation approaches. In
		  addition, it discusses PLM limitations and suggested
		  directions for future research.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {30},
  numpages	= {40},
  keywords	= {Large language models, foundational models, generative AI,
		  neural networks}
}

@InProceedings{	  10.1145/3640457.3688171,
  author	= {Ali, Zafar and Qi, Guilin and Ullah, Irfan and Mohammed,
		  Adam A. Q. and Kefalas, Pavlos and Muhammad, Khan},
  title		= {GLAMOR: Graph-based LAnguage MOdel embedding for citation
		  Recommendation},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688171},
  doi		= {10.1145/3640457.3688171},
  abstract	= {Digital publishing’s exponential growth has created vast
		  scholarly collections. Guiding researchers to relevant
		  resources is crucial, and knowledge graphs (KGs) are key
		  tools for unlocking hidden knowledge. However, current
		  methods focus on external links between concepts, ignoring
		  the rich information within individual papers. Challenges
		  like insufficient multi-relational data, name ambiguity,
		  and cold-start issues further limit existing KG-based
		  methods, failing to capture the intricate attributes of
		  diverse entities. To solve these issues, we propose GLAMOR,
		  a robust KG framework encompassing entities e.g., authors,
		  papers, fields of study, and concepts, along with their
		  semantic interconnections. GLAMOR uses a novel random
		  walk-based KG text generation method and then fine-tunes
		  the language model using the generated text. Subsequently,
		  the acquired context-preserving embeddings facilitate
		  superior top@k predictions. Evaluation results on two
		  public benchmark datasets demonstrate our GLAMOR’s
		  superiority against state-of-the-art methods especially in
		  solving the cold-start problem.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {929–933},
  numpages	= {5},
  keywords	= {Attributed Graph Embedding, Citation Recommendation,
		  Cold-start, GLAMOR, Large Language Model, Recommender
		  Systems},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3630106.3658981,
  author	= {Kraft, Angelie and Soulier, Elo\"{\i}se},
  title		= {Knowledge-Enhanced Language Models Are Not Bias-Proof:
		  Situated Knowledge and Epistemic Injustice in AI},
  year		= {2024},
  isbn		= {9798400704505},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3630106.3658981},
  doi		= {10.1145/3630106.3658981},
  abstract	= {The factual inaccuracies ("hallucinations") of large
		  language models have recently inspired more research on
		  knowledge-enhanced language modeling approaches. These are
		  often assumed to enhance the overall trustworthiness and
		  objectivity of language models. Meanwhile, the issue of
		  bias is usually only mentioned as a limitation of
		  statistical representations. This dissociation of
		  knowledge-enhancement and bias is in line with previous
		  research on AI engineers’ assumptions about knowledge,
		  which indicate that knowledge is commonly understood as
		  objective and value-neutral by this community. We argue
		  that claims and practices by actors of the field still
		  reflect this underlying conception of knowledge. We
		  contrast this assumption with literature from social and,
		  in particular, feminist epistemology, which argues that the
		  idea of a universal disembodied knower is blind to the
		  reality of knowledge practices and seriously challenges
		  claims of "objective" or "neutral" knowledge. Knowledge
		  enhancement techniques commonly use Wikidata and Wikipedia
		  as their sources for knowledge, due to their large scales,
		  public accessibility, and assumed trustworthiness. In this
		  work, they serve as a case study for the influence of the
		  social setting and the identity of knowers on epistemic
		  processes. Indeed, the communities behind Wikidata and
		  Wikipedia are known to be male-dominated and many instances
		  of hostile behavior have been reported in the past decade.
		  In effect, the contents of these knowledge bases are highly
		  biased. It is therefore doubtful that these knowledge bases
		  would contribute to bias reduction. In fact, our empirical
		  evaluations of RoBERTa, KEPLER, and CoLAKE, demonstrate
		  that knowledge enhancement may not live up to the hopes of
		  increased objectivity. In our study, the average
		  probability for stereotypical associations was preserved on
		  two out of three metrics and performance-related gender
		  gaps on knowledge-driven task were also preserved. We build
		  on these results and critical literature to argue that the
		  label of "knowledge" and the commonly held beliefs about it
		  can obscure the harm that is still done to marginalized
		  groups. Knowledge enhancement is at risk of perpetuating
		  epistemic injustice, and AI engineers’ understanding of
		  knowledge as objective per se conceals this injustice.
		  Finally, to get closer to trustworthy language models, we
		  need to rethink knowledge in AI and aim for an agenda of
		  diversification and scrutiny from outgroup members.},
  booktitle	= {Proceedings of the 2024 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {1433–1445},
  numpages	= {13},
  keywords	= {bias, epistemology, fairness, feminism, knowledge
		  enhancement, knowledge graphs, language models, natural
		  language processing, representation},
  location	= {Rio de Janeiro, Brazil},
  series	= {FAccT '24}
}

@InProceedings{	  10.1145/3716895.3716900,
  author	= {Yang, Da and Wang, Hongbo and Shao, Shanzhong and Liu,
		  Shutian},
  title		= {Knowledge-Enhanced Large Language Model-Based Assistance
		  Training System for Subway Maintenance Personnel},
  year		= {2025},
  isbn		= {9798400718007},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3716895.3716900},
  doi		= {10.1145/3716895.3716900},
  abstract	= {To address the various challenges faced in training urban
		  rail transit system maintenance personnel, this paper
		  proposes a solution for developing a training system for
		  subway maintenance personnel using knowledge graphs and a
		  Retrieval-Augmented Generation (RAG)-enhanced large
		  language model. The approach involves first creating a
		  fine-tuning dataset from subway maintenance technical
		  documents to fine-tune the large language model. This
		  fine-tuned model then assists in constructing a subway
		  maintenance knowledge graph. Concurrently, a vector
		  database of subway maintenance knowledge is established.
		  Finally, a question-answering system leveraging both the
		  knowledge graph and the vector database as external
		  knowledge sources is developed to support the training of
		  subway maintenance personnel. Results demonstrate that this
		  system can effectively enhance the learning efficiency of
		  maintenance staff.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Artificial Intelligence and Computer Engineering},
  pages		= {25–29},
  numpages	= {5},
  keywords	= {Retrieval-Augmented Generation (RAG), knowledge graph
		  (KG), large language model (LLM), subway maintenance},
  location	= { },
  series	= {ICAICE '24}
}

@InProceedings{	  10.1145/3638530.3664163,
  author	= {Custode, Leonardo Lucio and Caraffini, Fabio and Yaman,
		  Anil and Iacca, Giovanni},
  title		= {An investigation on the use of Large Language Models for
		  hyperparameter tuning in Evolutionary Algorithms},
  year		= {2024},
  isbn		= {9798400704956},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638530.3664163},
  doi		= {10.1145/3638530.3664163},
  abstract	= {Hyperparameter optimization is a crucial problem in
		  Evolutionary Computation. In fact, the values of the
		  hyperparameters directly impact the trajectory taken by the
		  optimization process, and their choice requires extensive
		  reasoning by human operators. Although a variety of
		  self-adaptive Evolutionary Algorithms have been proposed in
		  the literature, no definitive solution has been found. In
		  this work, we perform a preliminary investigation to
		  automate the reasoning process that leads to the choice of
		  hyperparameter values. We employ two open-source Large
		  Language Models (LLMs), namely Llama2-70b and Mixtral, to
		  analyze the optimization logs online and provide novel
		  real-time hyperparameter recommendations. We study our
		  approach in the context of step-size adaptation for (1 +
		  1)-ES. The results suggest that LLMs can be an effective
		  method for optimizing hyperparameters in Evolution
		  Strategies, encouraging further research in this
		  direction.},
  booktitle	= {Proceedings of the Genetic and Evolutionary Computation
		  Conference Companion},
  pages		= {1838–1845},
  numpages	= {8},
  keywords	= {evolutionary algorithms, large language models, landscape
		  analysis, parameter tuning},
  location	= {Melbourne, VIC, Australia},
  series	= {GECCO '24 Companion}
}

@Article{	  10.14778/3742728.3742757,
  author	= {Liu, Yurong and Pena, Eduardo H. M. and Santos, A\'{e}cio
		  and Wu, Eden and Freire, Juliana},
  title		= {Magneto: Combining Small and Large Language Models for
		  Schema Matching},
  year		= {2025},
  issue_date	= {April 2025},
  publisher	= {VLDB Endowment},
  volume	= {18},
  number	= {8},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3742728.3742757},
  doi		= {10.14778/3742728.3742757},
  abstract	= {Recent advances in language models (LMs) open new
		  opportunities for schema matching (SM). Recent approaches
		  have shown their potential and key limitations: while small
		  LMs (SLMs) require costly, difficult-to-obtain training
		  data, large LMs (LLMs) demand significant computational
		  resources and face context window constraints. We present
		  Magneto, a cost-effective and accurate solution for SM that
		  combines the advantages of SLMs and LLMs to address their
		  limitations. By structuring the SM pipeline in two phases,
		  retrieval and reranking, Magneto can use computationally
		  efficient SLM-based strategies to derive candidate matches
		  which can then be reranked by LLMs, thus making it possible
		  to reduce runtime while improving matching accuracy. We
		  propose (1) a self-supervised approach to fine-tune SLMs
		  which uses LLMs to generate syntactically diverse training
		  data, and (2) prompting strategies that are effective for
		  reranking. We also introduce a new benchmark, developed in
		  collaboration with domain experts, which includes real
		  biomedical datasets and presents new challenges for SM
		  methods. Through a detailed experimental evaluation, using
		  both our new and existing benchmarks, we show that Magneto
		  is scalable and attains high accuracy for datasets from
		  different domains.},
  journal	= {Proc. VLDB Endow.},
  month		= sep,
  pages		= {2681–2694},
  numpages	= {14}
}

@InProceedings{	  10.1145/3626772.3657966,
  author	= {Zhang, Wenjia and Gui, Lin and Procter, Rob and He,
		  Yulan},
  title		= {Multi-Layer Ranking with Large Language Models for News
		  Source Recommendation},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657966},
  doi		= {10.1145/3626772.3657966},
  abstract	= {To seek reliable information sources for news events, we
		  introduce a novel task of expert recommendation, which aims
		  to identify trustworthy sources based on their previously
		  quoted statements. To achieve this, we built a novel
		  dataset, called NewsQuote, consisting of 23,571
		  quote-speaker pairs sourced from a collection of news
		  articles. We formulate the recommendation task as the
		  retrieval of experts based on their likelihood of being
		  associated with a given query. We also propose a
		  multi-layer ranking framework employing Large Language
		  Models to improve the recommendation performance. Our
		  results show that employing an in-context learning based
		  LLM ranker and a multi-layer ranking-based filter
		  significantly improve both the predictive quality and
		  behavioural quality of the recommender system.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2537–2542},
  numpages	= {6},
  keywords	= {in-context learning, large language model, recommender
		  system},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Proceedings{	  10.1145/3643795,
  title		= {LLM4Code '24: Proceedings of the 1st International
		  Workshop on Large Language Models for Code},
  year		= {2024},
  isbn		= {9798400705793},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the first edition of the InternationalWorkshop
		  on Large Language Models for Code (LLM4Code). Large
		  Language Models (LLMs), which are large-scale models being
		  trained on massive textual corpora, have achieved
		  significant advances in various domains, including Software
		  Engineering (SE). Recently, there has been a growing
		  interest in applying LLMs to assist software development
		  and maintenance, such as code generation and comprehension,
		  test generation, and program repair. Although the
		  application of LLMs on code-relevant tasks has shown very
		  promising performance, there is a huge potential to explore
		  this growing domain further. The motivation of the LLM4Code
		  workshop is to provide a platform for academics and
		  practitioners to discuss and share their ideas on applying
		  and developing LLMs to solve code-relevant problems in SE
		  activities.The LLM4Code workshop is concerned with the
		  research on how to better apply LLMs to solve code-relevant
		  tasks, how to design better LLMs for code-relevant tasks,
		  and how to better benchmark LLMs on code-relevant tasks.
		  The workshop aims to achieve multiple goals as follows.
		  Firstly, the workshop aims to provide an opportunity for
		  participants to discuss novel ideas and preliminary results
		  on LLMs for solving code-relevant SE problems, to exchange
		  the latest progress in this domain. Secondly, the workshop
		  aims to encourage participants to discuss the open
		  challenges and problems of LLM4code, to identify important
		  future directions in this domain. Finally, the workshop
		  aims to encourage participants to share infrastructures and
		  benchmarks that are foundational and beneficial for future
		  research in this domain.},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3627673.3679231,
  author	= {Afreen, Neda and Balloccu, Giacomo and Boratto, Ludovico
		  and Fenu, Gianni and Malloci, Francesca Maridina and
		  Marras, Mirko and Martis, Andrea Giovanni},
  title		= {EDGE: A Conversational Interface driven by Large Language
		  Models for Educational Knowledge Graphs Exploration},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679231},
  doi		= {10.1145/3627673.3679231},
  abstract	= {As education adopts digital platforms, the vast amount of
		  information from various sources, such as learning
		  management systems and learning object repositories,
		  presents challenges in navigation and elaboration.
		  Traditional interfaces involve a steep learning curve,
		  limited user accessibility, and lack flexibility. Language
		  models alone cannot address these issues as they do not
		  have access to structured information specific to the
		  educational organization. In this paper, we propose EDGE
		  (EDucational knowledge Graph Explorer), a natural language
		  interface that uses knowledge graphs to organize
		  educational information. EDGE translates natural language
		  requests into queries and converts the results back into
		  natural language responses. We show EDGE's versatility
		  using knowledge graphs built from public datasets,
		  providing example interactions of different stakeholders.
		  Demo video: https://u.garr.it/eYq63.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5159–5163},
  numpages	= {5},
  keywords	= {conversational interface, graph database, information
		  retrieval, knowledge graph, language model, learning
		  management},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3613905.3650844,
  author	= {Walker, Johanna and Koutsiana, Elisavet and Nwachukwu,
		  Michelle and Mero\~{n}o Pe\~{n}uela, Albert and Simperl,
		  Elena},
  title		= {The Promise and Challenge of Large Language Models for
		  Knowledge Engineering: Insights from a Hackathon},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3650844},
  doi		= {10.1145/3613905.3650844},
  abstract	= {Knowledge engineering (KE) is the process of building,
		  maintaining and using knowledge-based systems. This
		  recently takes the form of knowledge graphs (KGs). The
		  advent of new technologies like Large Language Models
		  (LLMs) has the potential to improve automation in KE work
		  due to the richness of their training data and their
		  performance at solving natural language processing tasks.
		  We conducted a multiple-methods study exploring user
		  opinions and needs regarding the use of LLMs in KE. We used
		  ethnographic techniques to observe KE workers using LLMs to
		  solve KE tasks during a hackathon, followed by interviews
		  with some of the participants. This interim study found
		  that despite LLMs’ promising capabilities for efficient
		  knowledge acquisition and requirements elicitation, their
		  effective deployment requires an extended set of
		  capabilities and training, particularly in prompting and
		  understanding data. LLMs can be useful for simple quality
		  assessment tasks, but in complex scenarios, the output is
		  hard to control and evaluation may require novel
		  approaches. With this study, we aim to evidence the
		  interaction of KE stakeholders with LLMs, identify areas of
		  potential, and understand the barriers to their effective
		  use. We find copilot approaches may be valuable in
		  developing processes where the human or a team of humans is
		  assisted by generative AI.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {318},
  numpages	= {9},
  keywords	= {Interviews, Knowledge Engineering, Knowledge Graph, Large
		  Language Models},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@Article{	  10.1145/3631392,
  author	= {Yang, Jian and Hu, Xinyu and Xiao, Gang and Shen, Yulong},
  title		= {A Survey of Knowledge Enhanced Pre-trained Language
		  Models},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3631392},
  doi		= {10.1145/3631392},
  abstract	= {Pre-trained language models learn informative word
		  representations on a large-scale text corpus through
		  self-supervised learning, which has achieved promising
		  performance in fields of natural language processing (NLP)
		  after fine-tuning. These models, however, suffer from poor
		  robustness and lack of interpretability. We refer to
		  pre-trained language models with knowledge injection as
		  knowledge-enhanced pre-trained language models (KEPLMs).
		  These models demonstrate deep understanding and logical
		  reasoning and introduce interpretability. In this survey,
		  we provide a comprehensive overview of KEPLMs in NLP. We
		  first discuss the advancements in pre-trained language
		  models and knowledge representation learning. Then we
		  systematically categorize existing KEPLMs from three
		  different perspectives. Finally, we outline some potential
		  directions of KEPLMs for future research.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= mar,
  keywords	= {natural language processing, pre-trained language models,
		  symbolic knowledge, knowledge enhanced pre-trained language
		  models}
}

@InProceedings{	  10.1145/3626772.3657904,
  author	= {Xie, Yuzhang and Lu, Jiaying and Ho, Joyce and Nahab, Fadi
		  and Hu, Xiao and Yang, Carl},
  title		= {PromptLink: Leveraging Large Language Models for
		  Cross-Source Biomedical Concept Linking},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657904},
  doi		= {10.1145/3626772.3657904},
  abstract	= {Linking (aligning) biomedical concepts across diverse data
		  sources enables various integrative analyses, but it is
		  challenging due to the discrepancies in concept naming
		  conventions. Various strategies have been developed to
		  overcome this challenge, such as those based on
		  string-matching rules, manually crafted thesauri, and
		  machine learning models. However, these methods are
		  constrained by limited prior biomedical knowledge and can
		  hardly generalize beyond the limited amounts of rules,
		  thesauri, or training samples. Recently, large language
		  models (LLMs) have exhibited impressive results in diverse
		  biomedical NLP tasks due to their unprecedentedly rich
		  prior knowledge and strong zero-shot prediction abilities.
		  However, LLMs suffer from issues including high costs,
		  limited context length, and unreliable predictions. In this
		  research, we propose PromptLink, a novel biomedical concept
		  linking framework that leverages LLMs. Empirical results on
		  the concept linking task between two EHR datasets and an
		  external biomedical KG demonstrate the effectiveness of
		  PromptLink. Furthermore, PromptLink is a generic framework
		  without reliance on additional prior knowledge, context, or
		  training data, making it well-suited for concept linking
		  across various types of data sources. The source code of
		  this study is available at
		  https://github.com/constantjxyz/PromptLink.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2589–2593},
  numpages	= {5},
  keywords	= {biomedical concept linking, few-shot prompting, large
		  language models for resource-constrained field, retrieve
		  \&amp; re-rank},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3705754.3705790,
  author	= {Zhu, Sitong and Xia, Baobing and Duan, Fangwei and Zhao,
		  Zhenyang and Zhao, Zhenxia and Xiao, Teliang and Liu,
		  Zhicheng and Liu, Xia},
  title		= {Automated Framework for Constructing Knowledge Graphs
		  Oriented for Standard Analysis Using Large Language
		  Models},
  year		= {2025},
  isbn		= {9798400710193},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3705754.3705790},
  doi		= {10.1145/3705754.3705790},
  abstract	= {In an era characterized by rapid technological growth and
		  digital transformation, the necessity for efficient and
		  structured knowledge representation has grown more crucial.
		  Standards serve as fundamental cornerstones that offer
		  guidelines, specifications, and frameworks to ensure the
		  quality and interoperability of products, services, as well
		  as systems. Nonetheless, the complexity and extensive
		  nature of standard documents present significant challenges
		  in extraction, alignment, and organization. Traditional
		  manual processing methods frequently prove labor-intensive
		  and susceptible to errors, impeding the capturing of
		  intricate relationships and hierarchies within these
		  standards. Knowledge Graphs (KGs) provide a robust
		  methodology for organizing information, facilitating
		  improved functionalities for advanced search, reasoning,
		  and analytics. Despite their potential, structuring KGs
		  from standard documents continues to be challenging because
		  of unstructured text, domain-specific terminology, and the
		  intricacies in accurate information extraction. Recent
		  advancements in Natural Language Processing (NLP),
		  particularly the emergence of Large Language Models (LLMs)
		  like GPT-3, have opened new avenues for automating the
		  extraction and structuring of information from unstructured
		  content. These models exhibit exceptional proficiency in
		  comprehending and producing human-like text, positioning
		  them as feasible solutions for addressing the complexities
		  associated with standard documents. This paper presents an
		  automated framework named StandardKG Builder, which
		  utilizes LLMs to construct knowledge graphs tailored for
		  standard analysis from multiple perspectives for complex
		  information extraction. Our evaluation on a comprehensive
		  dataset of standard documents highlights the framework’s
		  effectiveness and scalability. By merging sophisticated
		  knowledge representation with advanced NLP techniques, our
		  work significantly enhances the accessibility and analysis
		  of standard documents, paving the way for more efficient
		  and intelligent standard management systems.},
  booktitle	= {Proceedings of the 2024 2nd International Conference on
		  Electronics, Computers and Communication Technology},
  pages		= {183–189},
  numpages	= {7},
  keywords	= {Knowledge Graph, Large Language Models, Standard
		  Analysis},
  location	= { },
  series	= {CECCT '24}
}

@InProceedings{	  10.1145/3715014.3722067,
  author	= {Hu, Jiawei and Jia, Hong and Hassan, Mahbub and Yao, Lina
		  and Kusy, Brano and Hu, Wen},
  title		= {LightLLM: A Versatile Large Language Model for Predictive
		  Light Sensing},
  year		= {2025},
  isbn		= {9798400714795},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715014.3722067},
  doi		= {10.1145/3715014.3722067},
  abstract	= {We propose LightLLM, a model that fine tunes pre-trained
		  large language models (LLMs) for light-based sensing tasks.
		  It integrates a sensor data encoder to extract key
		  features, a contextual prompt to provide environmental
		  information, and a fusion layer to combine these inputs
		  into a unified representation. This combined input is then
		  processed by the pre-trained LLM, which remains frozen
		  while being fine-tuned through the addition of lightweight,
		  trainable components, allowing the model to adapt to new
		  tasks without altering its original parameters. This
		  approach enables flexible adaptation of LLM to specialized
		  light sensing tasks with minimal computational overhead and
		  retraining effort. We have implemented LightLLM for three
		  light sensing tasks: light-based localization, outdoor
		  solar forecasting, and indoor solar estimation. Using
		  real-world experimental datasets, we demonstrate that
		  LightLLM significantly outperforms state-of-the-art
		  methods, achieving 4.4x improvement in localization
		  accuracy and 3.4x improvement in indoor solar estimation
		  when tested in previously unseen environments. We further
		  demonstrate that LightLLM outperforms ChatGPT-4 with direct
		  prompting, highlighting the advantages of LightLLM's
		  specialized architecture for sensor data fusion with
		  textual prompts.},
  booktitle	= {Proceedings of the 23rd ACM Conference on Embedded
		  Networked Sensor Systems},
  pages		= {158–171},
  numpages	= {14},
  location	= {UC Irvine Student Center., Irvine, CA, USA},
  series	= {SenSys '25}
}

@InProceedings{	  10.1145/3589335.3653009,
  author	= {Poria, Soujanya},
  title		= {Understanding, Leveraging, and Improving Large Language
		  Models},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3653009},
  doi		= {10.1145/3589335.3653009},
  abstract	= {The emergence of Large Language Models (LLMs) has marked a
		  substantial advancement in Natural Language Processing
		  (NLP), contributing significantly to enhanced task
		  performance both within and outside specific domains.
		  However, amidst these achievements, three key questions
		  remain unanswered: 1) The mechanism through which LLMs
		  accomplish their tasks and their limitations, 2)
		  Effectively harnessing the power of LLMs across diverse
		  domains, and 3) Strategies for enhancing the performance of
		  LLMs. This talk aims to delve into our research group's
		  endeavors to address these pivotal questions. Firstly, I
		  will outline our approach, which involves utilizing
		  ontology-guided prompt perturbations to unravel the primary
		  limitations of LLMs in solving mathematical problems.
		  Moving on to the second question, we will explore the
		  utilization of synthetic data generated by LLMs to bolster
		  challenging downstream tasks, particularly focusing on
		  structured prediction where LLMs face persistent
		  challenges. I will elaborate on our initiatives aimed at
		  improving LLMs by incorporating highly effective retrieval
		  strategies, specifically addressing the prevalent challenge
		  of hallucinations that often plagues contemporary LLMs.
		  Finally, I will present a technique on LLM realignment to
		  restore safety lost during fine-tuning.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1805},
  numpages	= {1},
  keywords	= {keynote},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3613905.3650949,
  author	= {Oelen, Allard and Auer, S\"{o}ren},
  title		= {Leveraging Large Language Models for Realizing Truly
		  Intelligent User Interfaces},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3650949},
  doi		= {10.1145/3613905.3650949},
  abstract	= {The number of published scholarly articles is growing at a
		  significant rate, making scholarly knowledge organization
		  increasingly important. Various approaches have been
		  proposed to organize scholarly information, including
		  describing scholarly knowledge semantically leveraging
		  knowledge graphs. Transforming unstructured knowledge,
		  presented within articles, to structured and semantically
		  represented knowledge generally requires human intelligence
		  and labor since natural language processing methods alone
		  typically do not render sufficient precision and recall for
		  many applications. With the recent developments of Large
		  Language Models (LLMs), it becomes increasingly possible to
		  provide truly intelligent user interfaces guiding humans in
		  the transformation process. We present an approach to
		  integrate non-intrusive LLMs guidance into existing user
		  interfaces. More specifically, we integrate LLM-supported
		  user interface components into an existing scholarly
		  knowledge infrastructure. Additionally, we provide our
		  experiences with LLM integration, detailing best practices
		  and obstacles. Finally, we evaluate the approach using a
		  small-scale user evaluation with domain experts.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {222},
  numpages	= {8},
  keywords	= {Intelligent User Interface, LLM Interface, Scholarly
		  Knowledge Graphs},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@Article{	  10.1145/3696379,
  author	= {Bui, Minh-Thanh and Boffa, Matteo and Valentim, Rodolfo
		  Vieira and Navarro, Jose Manuel and Chen, Fuxing and Bao,
		  Xiaosheng and Houidi, Zied Ben and Rossi, Dario},
  title		= {A Systematic Comparison of Large Language Models
		  Performance for Intrusion Detection},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {CoNEXT4},
  url		= {https://doi.org/10.1145/3696379},
  doi		= {10.1145/3696379},
  abstract	= {We explore the capabilities of Large Language Models
		  (LLMs) to assist or substitute devices (i.e., firewalls)
		  and humans (i.e., security experts) respectively in the
		  detection and analysis of security incidents. We leverage
		  transformer-based technologies, from relatively small to
		  foundational sizes, to address the problem of correctly
		  identifying the attack severity (and accessorily
		  identifying and explaining the attack type). We contrast a
		  broad range of LLM techniques (prompting, retrieval
		  augmented generation, and fine-tuning of several models)
		  using state-of-the-art machine learning models as a
		  baseline. Using proprietary data from commercial
		  deployment, our study provides an unbiased picture of the
		  strengths and weaknesses of LLM for intrusion detection.},
  journal	= {Proc. ACM Netw.},
  month		= nov,
  articleno	= {22},
  numpages	= {23},
  keywords	= {computing methodologies, firewalls, intrusion detection
		  systems, machine learning, natural language processing,
		  security and privacy}
}

@InProceedings{	  10.1145/3637528.3671491,
  author	= {Alam, Mehwish and Buscaldi, Davide and Cochez, Michael and
		  Gesese, Genet Asefa and Osborne, Francesco and Reforgiato
		  Recupero, Diego},
  title		= {Workshop on Deep Learning and Large Language Models for
		  Knowledge Graphs (DL4KG)},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671491},
  doi		= {10.1145/3637528.3671491},
  abstract	= {The use of Knowledge Graphs (KGs) which constitute large
		  networks of real-world entities and their
		  interrelationships, has grown rapidly. A substantial body
		  of research has emerged, exploring the integration of deep
		  learning (DL) and large language models (LLMs) with KGs.
		  This workshop aims to bring together leading researchers in
		  the field to discuss and foster collaborations on the
		  intersection of KG and DL/LLMs.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6704–6705},
  numpages	= {2},
  keywords	= {artificial intelligence, deep learning, knowledge graphs,
		  large language models},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3624012,
  author	= {Jain, Deepak Kumar and Qamar, Shamimul and Sangwan,
		  Saurabh Raj and Ding, Weiping and Kulkarni, Anand J.},
  title		= {Ontology-Based Natural Language Processing for Sentimental
		  Knowledge Analysis Using Deep Learning Architectures},
  year		= {2024},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3624012},
  doi		= {10.1145/3624012},
  abstract	= {When tested with popular datasets, sentiment
		  categorization using deep learning (DL) algorithms will
		  produce positive results. Building a corpus on novel themes
		  to train machine learning methods in sentiment
		  classification with high assurance, however, will be
		  difficult. This study proposes a way for representing
		  efficient features of a dataset into a word embedding layer
		  of DL methods in sentiment classification known as KPRO
		  (knowledge processing and representation based on
		  ontology), a procedure to embed knowledge in the ontology
		  of opinion datasets. This research proposes novel methods
		  in ontology-based natural language processing utilizing
		  feature extraction as well as classification by a DL
		  technique. Here, input text has been taken as web ontology
		  based text and is processed for word embedding. Then the
		  feature mapping is carried out for this processed text
		  using least square mapping in which the sentiment-based
		  text has been mapped for feature extraction. The feature
		  extraction is carried out using a Markov model based
		  auto-feature encoder (MarMod_AuFeaEnCod). Extracted
		  features are classified by utilizing hierarchical
		  convolutional attention networks. Based on this classified
		  output, the sentiment of the text obtained from web data
		  has been analyzed. Results are carried out for Twitter and
		  Facebook ontology-based sentimental analysis datasets in
		  terms of accuracy, precision, recall, F-1 score, RMSE, and
		  loss curve analysis. For the Twitter dataset, the proposed
		  MarMod_AuFeaEnCod_HCAN attains an accuracy of 98\%,
		  precision of 95\%, recall of 93\%, F-1 score of 91\%, RMSE
		  of 88\%, and loss curve of 70.2\%. For Facebook, ontology
		  web dataset analysis is also carried out with the same
		  parameters in which the proposed MarMod_AuFeaEnCod_HCAN
		  acquires accuracy of 96\%, precision of 92\%, recall of
		  94\%, F-1 score of 91\%, RMSE of 77\%, and loss curve of 68.2\%.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  articleno	= {17},
  numpages	= {17},
  keywords	= {Ontology, NLP, KPRO, deep learning, feature extraction,
		  classification}
}

@InProceedings{	  10.1145/3627673.3680025,
  author	= {Huang, Jia-Hong and Yang, Chao-Chun and Shen, Yixian and
		  Pacces, Alessio M. and Kanoulas, Evangelos},
  title		= {Optimizing Numerical Estimation and Operational Efficiency
		  in the Legal Domain through Large Language Models},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680025},
  doi		= {10.1145/3627673.3680025},
  abstract	= {The legal landscape encompasses a wide array of lawsuit
		  types, presenting lawyers with challenges in delivering
		  timely and accurate information to clients, particularly
		  concerning critical aspects like potential imprisonment
		  duration or financial repercussions. Compounded by the
		  scarcity of legal experts, there's an urgent need to
		  enhance the efficiency of traditional legal workflows.
		  Recent advances in deep learning, especially Large Language
		  Models (LLMs), offer promising solutions to this challenge.
		  Leveraging LLMs' mathematical reasoning capabilities, we
		  propose a novel approach integrating LLM-based
		  methodologies with specially designed prompts to address
		  precision requirements in legal Artificial Intelligence
		  (LegalAI) applications. The proposed work seeks to bridge
		  the gap between traditional legal practices and modern
		  technological advancements, paving the way for a more
		  accessible, efficient, and equitable legal system. To
		  validate this method, we introduce a curated dataset
		  tailored to precision-oriented LegalAI tasks, serving as a
		  benchmark for evaluating LLM-based approaches. Extensive
		  experimentation confirms the efficacy of our methodology in
		  generating accurate numerical estimates within the legal
		  domain, emphasizing the role of LLMs in streamlining legal
		  processes and meeting the evolving demands of LegalAI.
		  Github:
		  https://github.com/Jhhuangkay/Optimizing-Numerical-Estimation-and-Operational-Efficiency-in-the-Legal-Domain.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4554–4562},
  numpages	= {9},
  keywords	= {large language models, precision-oriented legal artificial
		  intelligence, tailored prompt design},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3729236,
  author	= {Jin, Kebing and Zhuo, Hankz Hankui},
  title		= {Integrating AI Planning with Natural Language
		  Processing:&nbsp;A&nbsp;Combination of Explicit and Tacit
		  Knowledge},
  year		= {2025},
  issue_date	= {August 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3729236},
  doi		= {10.1145/3729236},
  abstract	= {Natural language processing (NLP) aims at investigating
		  the interactions between agents and humans, which processes
		  and analyzes large amounts of natural language data.
		  Large-scale language models play an important role in
		  current NLP. However, the challenges of explainability and
		  complexity come along with the development of language
		  models. One way is to introduce logical relations and rules
		  into NLP models, such as making use of Automated Planning.
		  Automated planning (AI planning) focuses on building
		  symbolic domain models and synthesizing plans to transit
		  initial states to goals based on domain models. Recently,
		  there have been plenty of works related to those two
		  fields, which have the abilities to generate explicit
		  knowledge, e.g., preconditions and effects of action
		  models, and learn from tacit knowledge, e.g., neural
		  models, respectively. Integrating AI planning and NLP
		  effectively improves the communication between human and
		  intelligent agents. This article outlines the commons and
		  relations between AI planning and NLP, and it argues that
		  each of them can effectively impact the other one in six
		  areas: (1) planning-based text understanding, (2)
		  planning-based NLP, (3) text-based human–robot
		  interaction, (4) planning-based explainability, (5)
		  evaluation metrics, and (6) applications. We also explore
		  some potential future issues between AI planning and NLP.
		  To the best of our knowledge, this survey is the first that
		  addresses the deep connections between AI planning and
		  NLP.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= aug,
  articleno	= {97},
  numpages	= {37},
  keywords	= {AI planning, Natural language processing, Natural language
		  understanding, Human-robot interaction, Explainability}
}

@InProceedings{	  10.1145/3631700.3665233,
  author	= {Biancini, Giorgio and Ferrato, Alessio and Limongelli,
		  Carla},
  title		= {Multiple-Choice Question Generation Using Large Language
		  Models: Methodology and Educator Insights},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3665233},
  doi		= {10.1145/3631700.3665233},
  abstract	= {Integrating Artificial Intelligence (AI) in educational
		  settings has brought new learning approaches, transforming
		  the practices of both students and educators. Among the
		  various technologies driving this transformation, Large
		  Language Models (LLMs) have emerged as powerful tools for
		  creating educational materials and question answering, but
		  there are still space for new applications. Educators
		  commonly use Multiple-Choice Questions (MCQs) to assess
		  student knowledge, but manually generating these questions
		  is resource-intensive and requires significant time and
		  cognitive effort. In our opinion, LLMs offer a promising
		  solution to these challenges. This paper presents a novel
		  comparative analysis of three widely known LLMs - Llama 2,
		  Mistral, and GPT-3.5 - to explore their potential for
		  creating informative and challenging MCQs. In our approach,
		  we do not rely on the knowledge of the LLM, but we inject
		  the knowledge into the prompt to contrast the
		  hallucinations, giving the educators control over the
		  test’s source text, too. Our experiment involving 21
		  educators shows that GPT-3.5 generates the most effective
		  MCQs across several known metrics. Additionally, it shows
		  that there is still some reluctance to adopt AI in the
		  educational field. This study sheds light on the potential
		  of LLMs to generate MCQs and improve the educational
		  experience, providing valuable insights for the future.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {584–590},
  numpages	= {7},
  keywords	= {Generative AI, LLMs, Multiple Choice Question},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@InProceedings{	  10.1145/3747227.3747264,
  author	= {Mao, Huijuan},
  title		= {Research on the Construction of Machine Translation Model
		  of Fuzzy Language in Ancient Chinese Medicine Books for the
		  Transmission of Chinese Medicine Culture},
  year		= {2025},
  isbn		= {9798400714382},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3747227.3747264},
  doi		= {10.1145/3747227.3747264},
  abstract	= {In order to improve the translation quality of fuzzy
		  language in ancient Chinese medical texts, a hybrid
		  translation model integrating deep learning and knowledge
		  graph is constructed to perform context modeling and
		  disambiguation optimization for the literary syntax,
		  semantic ambiguities, and terminology of the ancient texts.
		  The Bi-LSTM context modeling module is designed to enhance
		  semantic understanding with knowledge graph, and a
		  multi-strategy fusion translation decision mechanism is
		  used to improve translation accuracy. The results show that
		  the model performs well in the processing of ambiguities
		  such as disease dynamics and treatment principles, improves
		  terminological consistency and semantic retention, and
		  provides effective support for cross-lingual communication
		  of TCM culture.},
  booktitle	= {Proceedings of the 2025 International Conference on
		  Machine Learning and Neural Networks},
  pages		= {229–235},
  numpages	= {7},
  keywords	= {ancient Chinese medical books, fuzzy language, knowledge
		  map, machine translation},
  location	= { },
  series	= {MLNN '25}
}

@InProceedings{	  10.1145/3708036.3708272,
  author	= {Yang, Guangyuan and Xie, Quanying and Chen, Lei},
  title		= {A Scientometrics Analysis and Visualization of Large
		  Language Model in China's Library},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708272},
  doi		= {10.1145/3708036.3708272},
  abstract	= {Large Language Model has been researched in the field of
		  library from the following aspects:space reproduction,
		  service reform, library construction and so on. In order to
		  clarify the current research situation of Large Language
		  Model's application research in the field of library, and
		  provide some reference for the further development of
		  research fields related to Large Language Model empowering
		  library in the future. This paper utilizes two methods of
		  scientometrics and data visualization to analyze and study
		  the journal papers on the application of Large Language
		  Model in the field of Chinese libraries from the aspects of
		  the degree of academic focus, the way of creating academic
		  achievements and research topics of academic achievements,
		  and puts forward the research practice of strengthening the
		  application of Large Language Model in library from the
		  aspects of ’Strengthen the practical research of Large
		  Language Model empowering Chinese library’ and ‘Broaden
		  the field of research related to Large Language Model
		  empowering Chinese library’, in order to promote the
		  all-round development of Large Language Model in the field
		  of library.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {1403–1407},
  numpages	= {5},
  keywords	= {Chinese libraries, Data Visualization, Large Language
		  Model, Library Service, Scientometrics},
  location	= { },
  series	= {ICCSMT '24}
}

@Article{	  10.1145/3743144,
  author	= {Comuzzi, Marco and Ko, Jonghyeon and Maggi, Fabrizio},
  title		= {A Language to Model and Simulate Data Quality Issues in
		  Process Mining},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {2},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3743144},
  doi		= {10.1145/3743144},
  abstract	= {Real-life business process event logs may suffer from
		  significant data quality problems negatively influencing
		  process mining analysis. Over time, a range of approaches
		  has been developed to detect and repair these quality
		  problems. Validation of these approaches tends to be
		  challenging due to the lack of a ground truth. Moreover,
		  the identification and definition of event log quality
		  problems have been tackled mainly through a pattern-based
		  approach, with systematic and extensible methods currently
		  lacking. In this article, we present FLAWD, a formal
		  language for describing event log data quality issues that
		  enables solutions addressing the shortcomings of process
		  mining data quality research identified above. FLAWD can be
		  used to formally describe and possibly reason over event
		  log data quality errors, as well as to guide the
		  development of tools for controlled and sophisticated
		  “polluting” of event logs through which benchmark
		  datasets may be systematically created. We present the
		  abstract syntax grammar of FLAWD and an open-source
		  software tool based on it that allows for the insertion of
		  all so-called event log imperfection patterns in a
		  stochastic manner. We show how FLAWD has been used in our
		  research to generate benchmark datasets and how it can be
		  used to formally describe and replicate a range of errors
		  found in real-life event logs.},
  journal	= {J. Data and Information Quality},
  month		= jun,
  articleno	= {6},
  numpages	= {36},
  keywords	= {Event log, data quality, abstract syntax grammar, FLAWD,
		  error injection, error description}
}

@InProceedings{	  10.1145/3700297.3700331,
  author	= {Yang, Da and Liu, Shutian and Fu, Haoyang and Shen,
		  Jiayi},
  title		= {Research and Practice on the Construction of Course
		  Ideological and Political Education Based on Knowledge
		  Graphs and Large Language Models},
  year		= {2024},
  isbn		= {9798400707100},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3700297.3700331},
  doi		= {10.1145/3700297.3700331},
  abstract	= {Knowledge graphs and large language models (LLMs) have
		  become important tools for educational innovation. This
		  paper explores the application of these two technologies in
		  the construction of ideological and political education in
		  university courses. The paper begins by analyzing the
		  importance of course-based ideological and political
		  education and the challenges currently faced. It then
		  introduces the role of knowledge graphs in integrating
		  educational resources and constructing knowledge systems,
		  as well as the potential and current status of LLMs in
		  natural language processing and providing personalized
		  educational content. This study presents a method that
		  integrates the use of knowledge graphs and LLMs to
		  construct resources and application systems for
		  course-based ideological and political education. The
		  results of practical case studies demonstrate that the
		  proposed method improves the efficiency of constructing
		  ideological and political education content, enhances the
		  effectiveness of moral education within courses, and
		  contributes to the innovative development of ideological
		  and political education.},
  booktitle	= {Proceedings of the 2024 International Symposium on
		  Artificial Intelligence for Education},
  pages		= {193–198},
  numpages	= {6},
  keywords	= {Course Ideological and Political Education, Educational
		  Innovation, Knowledge Graph, Large Language Model (LLM)},
  location	= { },
  series	= {ISAIE '24}
}

@InProceedings{	  10.1145/3665601.3669844,
  author	= {Huang, Zezhou},
  title		= {Disambiguate Entity Matching using Large Language Models
		  through Relation Discovery},
  year		= {2024},
  isbn		= {9798400706943},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665601.3669844},
  doi		= {10.1145/3665601.3669844},
  abstract	= {Entity matching is a critical problem in data integration,
		  central to tasks like fuzzy joins for tuple enrichment.
		  Traditional approaches have focused on overcoming fuzzy
		  term representations through methods such as edit distance,
		  Jaccard similarity, and more recently, embeddings and deep
		  neural networks, including advancements from large language
		  models (LLMs) like GPT. However, when integrating with
		  external databases, the core challenge in entity matching
		  extends beyond term fuzziness to the ambiguity in defining
		  what constitutes a "match". This is because external
		  databases contain tuples with varying levels of detail and
		  granularity among entities, and an "exact match" in
		  traditional entity matching rarely happens. As a result,
		  understanding how entities are related and the potential
		  nuances is critical, especially for high-stake tasks for
		  responsible AI. In this work, we study a case problem of
		  entity matching for ESG reporting. We propose a novel
		  approach that shifts focus from purely identifying semantic
		  similarities to understanding and defining the "relations"
		  between entities for resolving ambiguities in matching,
		  with a human-in-the-loop process to make the final
		  decision. By pre-defining a set of relations relevant to
		  the task at hand, our method allows analysts to navigate
		  the spectrum of similarity more effectively, from exact
		  matches to conceptually related entities, and responsibly
		  perform downstream tasks.},
  booktitle	= {Proceedings of the Conference on Governance, Understanding
		  and Integration of Data for Effective and Responsible AI},
  pages		= {36–39},
  numpages	= {4},
  keywords	= {Data Integration, Entity Matching, Large Language Models},
  location	= {Santiago, AA, Chile},
  series	= {GUIDE-AI '24}
}

@Article{	  10.1145/3708326,
  author	= {Mountantonakis, Michalis and Tzitzikas, Yannis},
  title		= {Generating SPARQL Queries over CIDOC-CRM Using a Two-Stage
		  Ontology Path Patterns Method in LLM Prompts},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {1},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3708326},
  doi		= {10.1145/3708326},
  abstract	= {In this article, we focus on the task of exploiting the
		  capabilities of Large Language Models (LLMs) to generate
		  SPARQL Queries for answering natural questions over
		  cultural Knowledge Graphs (KGs) expressed according to the
		  ISO standard ontology CIDOC-CRM. Since CIDOC-CRM is an
		  event-based model, usually we have to follow long paths for
		  answering a question, thereby, the challenge is how to
		  construct the prompt for aiding the LLM to produce the
		  right SPARQL query. We propose and comparatively evaluate
		  methods based on the creation of ontology path patterns of
		  a configurable path radius (or length). Then, we construct
		  a new dedicated benchmark that includes 100 natural
		  questions and the corresponding SPARQL queries over two
		  real KGs from the cultural domain describing artworks.
		  Finally, we present comparative results about the
		  effectiveness and efficiency over the benchmark by using
		  ChatGPT-3.5. The most effective method follows a two-stage
		  process that predicts and uses the most appropriate path
		  patterns of (rleq 4) . This method achieves 3.5 (times)
		  higher accuracy than the baseline method (0.66 versus
		  0.19), that includes in the prompt only the list of
		  properties and classes of the KG.Benchmark:},
  journal	= {J. Comput. Cult. Herit.},
  month		= feb,
  articleno	= {12},
  numpages	= {20},
  keywords	= {Question Answering, CIDOC-CRM, Prompt Engineering,
		  Cultural Heritage, LLM}
}

@InProceedings{	  10.1145/3616855.3635772,
  author	= {Deng, Cheng and Zhang, Tianhang and He, Zhongmou and Chen,
		  Qiyuan and Shi, Yuanyuan and Xu, Yi and Fu, Luoyi and
		  Zhang, Weinan and Wang, Xinbing and Zhou, Chenghu and Lin,
		  Zhouhan and He, Junxian},
  title		= {K2: A Foundation Language Model for Geoscience Knowledge
		  Understanding and Utilization},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3635772},
  doi		= {10.1145/3616855.3635772},
  abstract	= {Large language models (LLMs) have achieved great success
		  in general domains of natural language processing. In this
		  paper, we bring LLMs to the realm of geoscience with the
		  objective of advancing research and applications in this
		  field. To this end, we present the first-ever LLM in
		  geoscience, K2, alongside a suite of resources developed to
		  further promote LLM research within geoscience. For
		  instance, we have curated the first geoscience instruction
		  tuning dataset, GeoSignal, which aims to align LLM
		  responses to geoscience-related user queries. Additionally,
		  we have established the first geoscience benchmark,
		  GeoBench, to evaluate LLMs in the context of geoscience. In
		  this work, we experiment with a complete recipe to adapt a
		  pre-trained general-domain LLM to the geoscience domain.
		  Specifically, we further train the LLaMA-7B model on 5.5B
		  tokens of geoscience text corpus, including over 1 million
		  pieces of geoscience literature, and utilize GeoSignal's
		  supervised data to fine-tune the model. Moreover, we share
		  a protocol that can efficiently gather domain-specific data
		  and construct domain-supervised data, even in situations
		  where manpower is scarce. Meanwhile, we equip K2 with the
		  abilities of using tools to be a naive geoscience aide.
		  Experiments conducted on the GeoBench demonstrate the
		  effectiveness of our approach and datasets on geoscience
		  knowledge understanding and utilization.We open-source all
		  the training data and K2 model checkpoints at
		  https://github.com/davendw49/k2},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {161–170},
  numpages	= {10},
  keywords	= {foundation model, geoscience knowledge mining, geoscience
		  large language model},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@InProceedings{	  10.1145/3539618.3591667,
  author	= {Li, Na and Kteich, Hanane and Bouraoui, Zied and
		  Schockaert, Steven},
  title		= {Distilling Semantic Concept Embeddings from Contrastively
		  Fine-Tuned Language Models},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591667},
  doi		= {10.1145/3539618.3591667},
  abstract	= {Learning vectors that capture the meaning of concepts
		  remains a fundamental challenge. Somewhat surprisingly,
		  perhaps, pre-trained language models have thus far only
		  enabled modest improvements to the quality of such concept
		  embeddings. Current strategies for using language models
		  typically represent a concept by averaging the
		  contextualised representations of its mentions in some
		  corpus. This is potentially sub-optimal for at least two
		  reasons. First, contextualised word vectors have an unusual
		  geometry, which hampers downstream tasks. Second, concept
		  embeddings should capture the semantic properties of
		  concepts, whereas contextualised word vectors are also
		  affected by other factors. To address these issues, we
		  propose two contrastive learning strategies, based on the
		  view that whenever two sentences reveal similar properties,
		  the corresponding contextualised vectors should also be
		  similar. One strategy is fully unsupervised, estimating the
		  properties which are expressed in a sentence from the
		  neighbourhood structure of the contextualised word
		  embeddings. The second strategy instead relies on a distant
		  supervision signal from ConceptNet. Our experimental
		  results show that the resulting vectors substantially
		  outperform existing concept embeddings in predicting the
		  semantic properties of concepts, with the ConceptNet-based
		  strategy achieving the best results. These findings are
		  furthermore confirmed in a clustering task and in the
		  downstream task of ontology completion.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {216–226},
  numpages	= {11},
  keywords	= {commonsense knowledge, contrastive learning, language
		  models, word embedding},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3727353.3727365,
  author	= {Zhai, Dongsheng and Du, Ruize and Liang, Guoqiang},
  title		= {Recommendation System Based on Heterogeneous Graph Neural
		  Network and Large Language Model},
  year		= {2025},
  isbn		= {9798400712425},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3727353.3727365},
  doi		= {10.1145/3727353.3727365},
  abstract	= {In order to break through technical problems and reduce
		  R&amp;D costs and risks, innovation subjects usually
		  establish cooperative relationships based on common
		  cooperation goals and technological similarities among
		  them. Therefore, based on the analysis of the common
		  cooperation goals of innovation subjects, this chapter
		  proposes a partner identification model for emerging
		  technological innovation, constructs a vector
		  representation of patented technologies obtained by patent
		  heterogeneous networks, and identifies potential partners
		  in the technology field on the basis of technological
		  similarity.},
  booktitle	= {Proceedings of the 2025 4th International Conference on
		  Big Data, Information and Computer Network},
  pages		= {67–72},
  numpages	= {6},
  keywords	= {Graph Neural Network, Heterogeneous Patent Network, Large
		  Language Model, Potential Partner Identification},
  location	= { },
  series	= {BDICN '25}
}

@InProceedings{	  10.1145/3716554.3716598,
  author	= {Antoniou, Christina and Bassiliades, Nick},
  title		= {Utilizing LLMs and ontologies to query educational
		  knowledge graphs},
  year		= {2025},
  isbn		= {9798400713170},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3716554.3716598},
  doi		= {10.1145/3716554.3716598},
  abstract	= {Knowledge Graphs (KGs) provide knowledge and data in a
		  structured format with content from various fields. But the
		  access to the knowledge graphs is done by experienced
		  users, that is, users who know the syntax of the SPARQL
		  language and the KG vocabulary (either in RDF Schema or in
		  OWL) in order to be able to ask questions to exploit the
		  knowledge graphs. However, this requires a lot of time and
		  effort for most of the users, which makes KGs inaccessible
		  to a large number of users. Large Language Models (LLMs)
		  that have appeared recently can provide an alternative way
		  to query knowledge graphs without the need to learn SPARQL
		  and/or know the schema and vocabulary of them, eliminating
		  the time and effort that ordinary users need to spend in
		  order to use them. In this article, we present some
		  experiments and their results illustrating how ChatGPT can
		  help ordinary users to generate SPARQL queries, without
		  knowing SPARQL, to effectively use knowledge graphs and
		  exploit their wealth of data. We experimented with ChatGPT
		  to explore whether it can generate SPARQL queries based on
		  user's natural language input and a given vocabulary
		  (ontology) about an educational knowledge graph. To this
		  end we have devised a specific prompt template. Results
		  indicate that LLMs can indeed help in this direction, given
		  the fact that they are prompted properly, using good
		  English language. We have also discussed some practical
		  lessons learned through this experiment.},
  booktitle	= {Proceedings of the 28th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {287–295},
  numpages	= {9},
  keywords	= {AI application, ChatGPT, RDF, knowledge graphs, large
		  language model use cases},
  location	= { },
  series	= {PCI '24}
}

@InProceedings{	  10.1145/3675094.3679000,
  author	= {Fiori, Michele and Civitarese, Gabriele and Bettini,
		  Claudio},
  title		= {Using Large Language Models to Compare Explainable Models
		  for Smart Home Human Activity Recognition},
  year		= {2024},
  isbn		= {9798400710582},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3675094.3679000},
  doi		= {10.1145/3675094.3679000},
  abstract	= {Recognizing daily activities with unobtrusive sensors in
		  smart environments enables various healthcare applications.
		  Monitoring how subjects perform activities at home and
		  their changes over time can reveal early symptoms of health
		  issues, such as cognitive decline. Most approaches in this
		  field use deep learning models, which are often seen as
		  black boxes mapping sensor data to activities. However,
		  non-expert users like clinicians need to trust and
		  understand these models' outputs. Thus, eXplainable AI
		  (XAI) methods for Human Activity Recognition have emerged
		  to provide intuitive natural language explanations from
		  these models. Different XAI methods generate different
		  explanations, and their effectiveness is typically
		  evaluated through user surveys, that are often challenging
		  in terms of costs and fairness. This paper proposes an
		  automatic evaluation method using Large Language Models
		  (LLMs) to identify, in a pool of candidates, the best XAI
		  approach for non-expert users. Our preliminary results
		  suggest that LLM evaluation aligns with user surveys.},
  booktitle	= {Companion of the 2024 on ACM International Joint
		  Conference on Pervasive and Ubiquitous Computing},
  pages		= {881–884},
  numpages	= {4},
  keywords	= {evaluation, human activity recognition, llms, xai},
  location	= {Melbourne VIC, Australia},
  series	= {UbiComp '24}
}

@InProceedings{	  10.1145/3640457.3688022,
  author	= {Afreen, Neda},
  title		= {Explainable and Faithful Educational Recommendations
		  through Causal Language Modelling via Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688022},
  doi		= {10.1145/3640457.3688022},
  abstract	= {The rapid expansion of digital education has significantly
		  increased the need for recommender systems to help learners
		  navigate the extensive variety of available learning
		  resources. Recent advancements in these systems have
		  notably improved the personalization of course
		  recommendations. However, many existing systems fail to
		  provide clear explanations for their recommendations,
		  making it difficult for learners to understand why a
		  particular suggestion was made. Researchers have emphasized
		  the importance of explanations in various domains such as
		  e-commerce, media, and entertainment, demonstrating how
		  explanations can enhance system transparency, foster user
		  trust, and improve decision-making processes. Despite these
		  insights, such approaches have been rarely applied to the
		  educational domain, and their effectiveness in practical
		  use remains largely unexamined. My research focuses on
		  developing explainable recommender systems for digital
		  education. First, I aim to design knowledge graphs that can
		  support high-quality recommendations in the educational
		  context. Second, I will create models backed by these
		  knowledge graphs that not only deliver accurate
		  recommendations but also provide faithful explanations for
		  each suggestion. Third, I will evaluate the effectiveness
		  of these explainable recommender systems in real-world
		  educational environments. Ultimately, this research aims to
		  advance the development of more transparent and
		  user-centric educational technologies.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {1358–1360},
  numpages	= {3},
  keywords	= {Explainability, Knowledge Graph, Language Modeling,
		  Personalization, Recommendation, Recommender Systems,
		  Transparency},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3664647.3681389,
  author	= {Zhao, Shengwei and Xu, Linhai and Liu, Yuying and Du,
		  Shaoyi},
  title		= {Multi-grained Correspondence Learning of Audio-language
		  Models for Few-shot Audio Recognition},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681389},
  doi		= {10.1145/3664647.3681389},
  abstract	= {Large-scale pre-trained audio-language models excel in
		  general multi-modal representation, facilitating their
		  adaptation to downstream audio recognition tasks in a
		  data-efficient manner. However, existing few-shot audio
		  recognition methods based on audio-language models
		  primarily focus on learning coarse-grained correlations,
		  which are not sufficient to capture the intricate matching
		  patterns between the multi-level information of audio and
		  the diverse characteristics of category concepts. To
		  address this gap, we propose multi-grained correspondence
		  learning for bootstrapping audio-language models to improve
		  audio recognition with few training samples. This approach
		  leverages generative models to enrich multi-modal
		  representation learning, mining the multi-level information
		  of audio alongside the diverse characteristics of category
		  concepts. Multi-grained matching patterns are then
		  established through multi-grained key-value cache and
		  multi-grained cross-modal contrast, enhancing the alignment
		  between audio and category concepts. Additionally, we
		  incorporate optimal transport to tackle temporal
		  misalignment and semantic intersection issues in
		  fine-grained correspondence learning, enabling flexible
		  fine-grained matching. Our method achieves state-of-the-art
		  results on multiple benchmark datasets for few-shot audio
		  recognition, with comprehensive ablation experiments
		  validating its effectiveness.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {9244–9252},
  numpages	= {9},
  keywords	= {audio-language models, few-shot audio recognition,
		  multi-grained correspondence learning, optimal transport},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Article{	  10.14778/3681954.3681973,
  author	= {Sun, Yushi and Xin, Hao and Sun, Kai and Xu, Yifan Ethan
		  and Yang, Xiao and Dong, Xin Luna and Tang, Nan and Chen,
		  Lei},
  title		= {Are Large Language Models a Good Replacement of
		  Taxonomies?},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {11},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3681954.3681973},
  doi		= {10.14778/3681954.3681973},
  abstract	= {Large language models (LLMs) demonstrate an impressive
		  ability to internalize knowledge and answer natural
		  language questions. Although previous studies validate that
		  LLMs perform well on general knowledge while presenting
		  poor performance on long-tail nuanced knowledge, the
		  community is still doubtful about whether the traditional
		  knowledge graphs should be replaced by LLMs. In this paper,
		  we ask if the schema of knowledge graph (i.e., taxonomy) is
		  made obsolete by LLMs. Intuitively, LLMs should perform
		  well on common taxonomies and at taxonomy levels that are
		  common to people. Unfortunately, there lacks a
		  comprehensive benchmark that evaluates the LLMs over a wide
		  range of taxonomies from common to specialized domains and
		  at levels from root to leaf so that we can draw a confident
		  conclusion. To narrow the research gap, we constructed a
		  novel taxonomy hierarchical structure discovery benchmark
		  named TaxoGlimpse to evaluate the performance of LLMs over
		  taxonomies. TaxoGlimpse covers ten representative
		  taxonomies from common to specialized domains with in-depth
		  experiments of different levels of entities in this
		  taxonomy from root to leaf. Our comprehensive experiments
		  of eighteen LLMs under three prompting settings validate
		  that LLMs perform miserably poorly in handling specialized
		  taxonomies and leaf-level entities. Specifically, the QA
		  accuracy of the best LLM drops by up to 30\% as we go from
		  common to specialized domains and from root to leaf levels
		  of taxonomies.},
  journal	= {Proc. VLDB Endow.},
  month		= jul,
  pages		= {2919–2932},
  numpages	= {14}
}

@InProceedings{	  10.1145/3663741.3664785,
  author	= {Barbon Junior, Sylvio and Ceravolo, Paolo and Groppe, Sven
		  and Jarrar, Mustafa and Maghool, Samira and S\`{e}des,
		  Florence and Sahri, Soror and Van Keulen, Maurice},
  title		= {Are Large Language Models the New Interface for Data
		  Pipelines?},
  year		= {2024},
  isbn		= {9798400706790},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3663741.3664785},
  doi		= {10.1145/3663741.3664785},
  abstract	= {A Language Model is a term that encompasses various types
		  of models designed to understand and generate human
		  communication. Large Language Models (LLMs) have gained
		  significant attention due to their ability to process text
		  with human-like fluency and coherence, making them valuable
		  for a wide range of data-related tasks fashioned as
		  pipelines. The capabilities of LLMs in natural language
		  understanding and generation, combined with their
		  scalability, versatility, and state-of-the-art performance,
		  enable innovative applications across various AI-related
		  fields, including eXplainable Artificial Intelligence
		  (XAI), Automated Machine Learning (AutoML), and Knowledge
		  Graphs (KG). Furthermore, we believe these models can
		  extract valuable insights and make data-driven decisions at
		  scale, a practice commonly referred to as Big Data
		  Analytics (BDA). In this position paper, we provide some
		  discussions in the direction of unlocking synergies among
		  these technologies, which can lead to more powerful and
		  intelligent AI solutions, driving improvements in data
		  pipelines across a wide range of applications and domains
		  integrating humans, computers, and knowledge.},
  booktitle	= {Proceedings of the International Workshop on Big Data in
		  Emergent Distributed Environments},
  articleno	= {6},
  numpages	= {6},
  keywords	= {Automated Machine Learning, Big Data Analytic,
		  Human-Computer Interaction, Knowledge Graphs, Natural
		  Language Understanding, eXplainable Artificial
		  Intelligence},
  location	= {Santiago, AA, Chile},
  series	= {BiDEDE '24}
}

@InProceedings{	  10.1145/3656650.3656688,
  author	= {Grigis, Paolo and De Angeli, Antonella},
  title		= {Playwriting with Large Language Models: Perceived
		  Features, Interaction Strategies and Outcomes},
  year		= {2024},
  isbn		= {9798400717642},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3656650.3656688},
  doi		= {10.1145/3656650.3656688},
  abstract	= {Large Language Models (LLMs) are sparking debates about
		  creativity, intellectual property, and artistic integrity.
		  This paper focuses on creativity, defined as consensual
		  agreement among domain experts. It presents an inductive
		  analysis of seven semi-structured interviews with
		  professional playwrights who engaged in a longitudinal
		  project with the aim of writing a theatre script using
		  commercial systems. Overall, participants regarded LLMs as
		  unsuitable for playwrighting. However, they enjoyed the
		  experience and identified utility for editorial tasks and
		  brainstorming. A significant obstacle was associated with
		  the politics embedded in LLMs. Not only did these systems
		  avoid a language that could offend sensibilities, but they
		  also refused to engage in taboos and conflicts, which are
		  the core of dramaturgy. Other system features (speed,
		  exploitation, and unpredictability) were sometimes
		  considered conducive and sometimes detrimental to
		  creativity. Participants experienced difficulties and tried
		  to build common ground by trial and error. Often, this
		  strategy evolved into role play: the playwright instructed
		  the LLM to enact characters. The interaction provided hints
		  of inspiration and fostered suspension of disbelief and
		  ontological reflection. However, it often led to technology
		  rejection. Comparing and contrasting our insights with
		  related work, we conclude by opening new directions for
		  research at the boundaries of HCI and AI.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Advanced Visual Interfaces},
  articleno	= {38},
  numpages	= {9},
  keywords	= {Creative AI, Creativity, Roleplay, Suspension of
		  Disbelief, Theatre, Unpredictability},
  location	= {Arenzano, Genoa, Italy},
  series	= {AVI '24}
}

@InProceedings{	  10.1145/3701716.3717822,
  author	= {Vakaj, Edlira and Mihindukulasooriya, Nandana and Tiwari,
		  Sanju and Rodriguez-M\'{e}ndez, Sergio J.},
  title		= {4th International Workshop on Natural Language Processing
		  for Knowledge Graph Construction},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3717822},
  doi		= {10.1145/3701716.3717822},
  abstract	= {Knowledge graphs (KG) are becoming increasingly popular,
		  at the heart of Gartner's emerging tech impact radar,
		  especially as a complementary theme for addressing the
		  challenges of recent advances in natural language
		  processing (NLP) with large language models related to
		  responsible AI such as fairness, transparency,
		  accountability, and explainability. Sir Tim Berners-Lee's
		  seminal work ''Weaving the Web: The Original Design and
		  Ultimate Destiny of the World Wide Web'', envisioned a
		  World Wide Web where information is not only accessible but
		  structured, allowing machines to interpret data
		  meaningfully. This vision laid the groundwork for
		  technologies such as RDF (Resource Description Framework)
		  and OWL (Web Ontology Language), which serve as
		  foundational components for modern KGs.However, the process
		  of building domain-specific KGs from extensive text corpora
		  is highly complex and resource-intensive, requiring careful
		  task design for entity recognition, disambiguation, and
		  relationship extraction, among others. These tasks are
		  essential to ensure accuracy and relevance in knowledge
		  representation, but they pose considerable challenges.
		  Addressing these complexities is crucial for the continued
		  advancement and application of KGs across domains.In this
		  context, the 4th NLP4KGC workshop is held to create a
		  collaborative platform for researchers, practitioners, and
		  industry experts in NLP and KG construction. Following the
		  success and growing community engagement in the previous
		  three editions, this year's workshop aims to deepen
		  collaboration and encourage innovative solutions in this
		  rapidly evolving field. The 4th NLP4KGC will continue to
		  bridge academia and industry, fostering the exchange of
		  insights, tools, and methodologies at the intersection of
		  NLP and KG development. The 4th NLP4KGC will consist of
		  five accepted papers and three keynotes from distinguished
		  speakers.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2545–2548},
  numpages	= {4},
  keywords	= {knowledge graph, large language models, natural language
		  understanding, responsible ai, semantic web, web of data},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3577190.3614158,
  author	= {Hensel, Laura Birka and Yongsatianchot, Nutchanon and
		  Torshizi, Parisa and Minucci, Elena and Marsella, Stacy},
  title		= {Large language models in textual analysis for gesture
		  selection},
  year		= {2023},
  isbn		= {9798400700552},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3577190.3614158},
  doi		= {10.1145/3577190.3614158},
  abstract	= {Gestures perform a variety of communicative functions that
		  powerfully influence human face-to-face interaction. How
		  this communicative function is achieved varies greatly
		  between individuals and depends on the role of the speaker
		  and the context of the interaction. Approaches to automatic
		  gesture generation vary not only in the degree to which
		  they rely on data-driven techniques but also the degree to
		  which they can produce context and speaker specific
		  gestures. However, these approaches face two major
		  challenges: The first is obtaining sufficient training data
		  that is appropriate for the context and the goal of the
		  application. The second is related to designer control to
		  realize their specific intent for the application. Here, we
		  approach these challenges by using large language models
		  (LLMs) to show that these powerful models of large amounts
		  of data can be adapted for gesture analysis and generation.
		  Specifically, we used ChatGPT as a tool for suggesting
		  context-specific gestures that can realize designer intent
		  based on minimal prompts. We also find that ChatGPT can
		  suggests novel yet appropriate gestures not present in the
		  minimal training data. The use of LLMs is a promising
		  avenue for gesture generation that reduce the need for
		  laborious annotations and has the potential to flexibly and
		  quickly adapt to different designer intents.},
  booktitle	= {Proceedings of the 25th International Conference on
		  Multimodal Interaction},
  pages		= {378–387},
  numpages	= {10},
  keywords	= {gesture analysis, gesture selection, large language
		  models},
  location	= {Paris, France},
  series	= {ICMI '23}
}

@InProceedings{	  10.1145/3696630.3730562,
  author	= {Klimek, Radoslaw},
  title		= {RE-oriented Model Development with LLM Support and
		  Deduction-based Verification},
  year		= {2025},
  isbn		= {9798400712760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696630.3730562},
  doi		= {10.1145/3696630.3730562},
  abstract	= {The requirements engineering (RE) phase is pivotal in
		  developing high-quality software. Integrating advanced
		  modelling techniques with large language models (LLMs) and
		  formal verification in a logical style can significantly
		  enhance this process. We propose a comprehensive framework
		  that focuses on specific Unified Modelling Language (UML)
		  diagrams for preliminary system development. This framework
		  offers visualisations at various modelling stages and
		  seamlessly integrates large language models and logical
		  reasoning engines. The behavioural models generated with
		  the assistance of LLMs are automatically translated into
		  formal logical specifications. Deductive formal
		  verification ensures that logical requirements and
		  interrelations between software artefacts are thoroughly
		  addressed. Ultimately, the framework facilitates the
		  automatic generation of program skeletons, streamlining the
		  transition from design to implementation.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  the Foundations of Software Engineering},
  pages		= {1297–1304},
  numpages	= {8},
  keywords	= {requirements engineering, formal IDE, UML modelling, large
		  language models, automated logical reasoning, temporal
		  logic},
  location	= {Clarion Hotel Trondheim, Trondheim, Norway},
  series	= {FSE Companion '25}
}

@InProceedings{	  10.1145/3653946.3653961,
  author	= {Jiang, Yingdi and Yao, Jiarui and Li, Fangfei and Zhang,
		  Yan},
  title		= {Research on Engineering Management Question-answering
		  System in the Communication Industry Based on Large
		  Language Models and Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400716553},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3653946.3653961},
  doi		= {10.1145/3653946.3653961},
  abstract	= {In the engineering management of the communication
		  industry, there are many issues, including low efficiency
		  in information acquisition and limitations in the level of
		  intelligence.Large language models, with their powerful
		  text comprehension and generation capabilities, offer new
		  perspectives for the development of this field.This study
		  constructed a question-answering system using a combined
		  approach of large language models and text knowledge bases.
		  The system dynamically leverages abundant external
		  knowledge and enhances the model's reasoning ability and
		  interpretability through knowledge graphs. In response to
		  five categories of issues in engineering management,
		  experiments and in-depth analysis revealed that although
		  large language models may lack granularity in addressing
		  some complex problems, the question-answering system
		  overall achieved intelligent assistance, improving the
		  efficiency of collaborative engineering management.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Machine Vision and Applications},
  pages		= {100–105},
  numpages	= {6},
  keywords	= {Engineering management, Keywords • Large language
		  models, Knowledge graphs, Question-answering},
  location	= {Singapore, Singapore},
  series	= {ICMVA '24}
}

@Article{	  10.1145/3596219,
  author	= {Zhao, Shuai and Li, Qing and Yang, Yuer and Wen, Jinming
		  and Luo, Weiqi},
  title		= {From Softmax to Nucleusmax: A Novel Sparse Language Model
		  for Chinese Radiology Report Summarization},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3596219},
  doi		= {10.1145/3596219},
  abstract	= {The Chinese radiology report summarization is a crucial
		  component in smart healthcare that employs language models
		  to summarize key findings in radiology reports and
		  communicate these findings to physicians. However, most
		  language models for radiology report summarization utilize
		  a softmax transformation in their output layer, leading to
		  dense alignments and strictly positive output
		  probabilities. This density is inefficient, reducing model
		  interpretability and giving probability mass to many
		  unrealistic outputs. To tackle this issue, we propose a
		  novel approach named nucleusmax. Nucleusmax is able to
		  mitigate dense outputs and improve model interpretability
		  by truncating the unreliable tail of the probability
		  distribution. In addition, we incorporate nucleusmax with a
		  copy mechanism, a useful technique to avoid professional
		  errors in the generated diagnostic opinions. To further
		  promote the research of radiology report summarization, we
		  also have created a Chinese radiology report summarization
		  dataset, which is freely available. Experimental results
		  showed via both automatic and human evaluation that the
		  proposed approach substantially improves the sparsity and
		  overall quality of outputs over competitive softmax models,
		  producing radiology summaries that approach the quality of
		  those authored by physicians. In general, our work
		  demonstrates the feasibility and prospect of the language
		  model to the domain of radiology and smart healthcare.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {180},
  numpages	= {21},
  keywords	= {Chinese radiology report summarization, language model,
		  softmax, abstractive summarization}
}

@InProceedings{	  10.1145/3689492.3690049,
  author	= {Thiede, Christoph and Taeumel, Marcel and B\"{o}hme, Lukas
		  and Hirschfeld, Robert},
  title		= {Talking to Objects in Natural Language: Toward Semantic
		  Tools for Exploratory Programming},
  year		= {2024},
  isbn		= {9798400712159},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689492.3690049},
  doi		= {10.1145/3689492.3690049},
  abstract	= {In exploratory programming, programmers often face a
		  semantic gap between their high-level understanding and the
		  low-level interfaces available for interacting with objects
		  in a system. That is, technical object structure and
		  behavior need to be interpreted as abstract domain
		  concepts, which then increases cognitive load and thus
		  impedes exploration progress. We propose semantic object
		  interfaces that bridge this gap by enabling contextual,
		  natural-language conversations with objects. Our approach
		  leverages an exploratory programming agent powered by a
		  large language model (LLM) to translate natural-language
		  questions into low-level experiments and provide high-level
		  answers. We describe a framework for integrating semantic
		  object interfaces into existing exploratory programming
		  systems, including a prototype implementation in
		  Squeak/Smalltalk using GPT-4o. We showcase the potential of
		  semantic object interfaces through case studies and discuss
		  their feasibility, limitations, and impact on the
		  programming experience. While challenges remain, our
		  approach promises to reduce mental effort and empower
		  programmers to explore and understand systems at a higher
		  level of abstraction for a better programming experience.},
  booktitle	= {Proceedings of the 2024 ACM SIGPLAN International
		  Symposium on New Ideas, New Paradigms, and Reflections on
		  Programming and Software},
  pages		= {68–84},
  numpages	= {17},
  keywords	= {ChatGPT, LLMs, Smalltalk, conversational agents,
		  exploratory programming, generative AI, natural-language
		  programming, object-oriented programming, semantic tools},
  location	= {Pasadena, CA, USA},
  series	= {Onward! '24}
}

@Article{	  10.1145/3596597,
  author	= {Hossain, Bayzid Ashik and Mukta, Md. Saddam Hossain and
		  Islam, Md Adnanul and Zaman, Akib and Schwitter, Rolf},
  title		= {Natural Language–Based Conceptual Modelling Frameworks:
		  State of the Art and Future Opportunities},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3596597},
  doi		= {10.1145/3596597},
  abstract	= {Identifying requirements for an information system is an
		  important task and conceptual modelling is the first step
		  in this process. Conceptual modelling plays a critical role
		  in the information system design process and usually
		  involves domain experts and knowledge engineers who
		  brainstorm together to identify the required knowledge to
		  build an information system. The conceptual modelling
		  process starts with the collection of necessary information
		  from the domain experts by the knowledge engineers.
		  Afterwards, the knowledge engineers use traditional model
		  driven engineering techniques to design the system based on
		  the collected information. Natural language–based
		  conceptual modelling frameworks or systems are used to help
		  domain experts and knowledge engineers in eliciting
		  requirements and building conceptual models from a natural
		  language text. In this article, we discuss the state of the
		  art of some recent conceptual modelling frameworks that are
		  based on natural language. We take a closer look at how
		  these frameworks are built, in particular at the underlying
		  motivation, architecture, types of natural language used
		  (e.g., restricted vs. unrestricted), types of the
		  conceptual model generated, verification support of the
		  requirements specifications as well as the conceptual
		  models, and underlying knowledge representation formalism.
		  We also discuss some future research opportunities that
		  these frameworks offer.},
  journal	= {ACM Comput. Surv.},
  month		= aug,
  articleno	= {12},
  numpages	= {26},
  keywords	= {Natural language processing, information extraction,
		  conceptual modelling, knowledge representation, semantic
		  round-tripping}
}

@InProceedings{	  10.1145/3624062.3624172,
  author	= {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao,
		  Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and
		  Xie, Zhen and Cerpa, Alberto and Du, Wan},
  title		= {HPC-GPT: Integrating Large Language Model for
		  High-Performance Computing},
  year		= {2023},
  isbn		= {9798400707858},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3624062.3624172},
  doi		= {10.1145/3624062.3624172},
  abstract	= {Large Language Models (LLMs), including the LLaMA model,
		  have exhibited their efficacy across various general-domain
		  natural language processing (NLP) tasks. However, their
		  performance in high-performance computing (HPC) domain
		  tasks has been less than optimal due to the specialized
		  expertise required to interpret the model’s responses. In
		  response to this challenge, we propose HPC-GPT, a novel
		  LLaMA-based model that has been supervised fine-tuning
		  using generated QA (Question-Answer) instances for the HPC
		  domain. To evaluate its effectiveness, we concentrate on
		  two HPC tasks: managing AI models and datasets for HPC, and
		  data race detection. By employing HPC-GPT, we demonstrate
		  comparable performance with existing methods on both tasks,
		  exemplifying its excellence in HPC-related scenarios. Our
		  experiments on open-source benchmarks yield extensive
		  results, underscoring HPC-GPT’s potential to bridge the
		  performance gap between LLMs and HPC-specific tasks. With
		  HPC-GPT, we aim to pave the way for LLMs to excel in HPC
		  domains, simplifying the utilization of language models in
		  complex computing applications.},
  booktitle	= {Proceedings of the SC '23 Workshops of the International
		  Conference on High Performance Computing, Network, Storage,
		  and Analysis},
  pages		= {951–960},
  numpages	= {10},
  keywords	= {Data Race Detection, High-performance Computing, Large
		  Language Model, Neural Network., OpenMP},
  location	= {Denver, CO, USA},
  series	= {SC-W '23}
}

@Article{	  10.1145/3715156,
  author	= {Varagnolo, Davide and Melo, Dora and Pimenta Rodrigues,
		  Irene},
  title		= {Translating Natural Language Questions into CIDOC-CRM
		  SPARQL Queries to Access Cultural Heritage Knowledge
		  Bases},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3715156},
  doi		= {10.1145/3715156},
  abstract	= {To explore information on the semantic web, SPARQL queries
		  or DL-queries are suitable tools. However, users interested
		  in exploring the content of such knowledge bases often find
		  it challenging to employ formal query languages, as this
		  requires familiarity with the target domain’s
		  representation model. To address these challenges, a
		  question-answering system that automatically translates
		  natural language questions into SPARQL queries, over the
		  Smithsonian American Art Museum CIDOC-CRM representation is
		  presented. The proposed approach uses an ontology, named
		  Query Ontology, defined to represent the natural language
		  concepts and relations specific to the question’s domain.
		  This system’s architecture uses a traditional natural
		  language processing symbolic approach, with a pipeline of
		  modules for the syntactic, semantic, and pragmatic
		  analysis. An evaluation of the proposed system is presented
		  and shows very promising results.},
  journal	= {J. Comput. Cult. Herit.},
  month		= apr,
  articleno	= {21},
  numpages	= {28},
  keywords	= {datasets, SPARQL queries, CIDOC-CRM representation,
		  SAAM’s knowledge base}
}

@Article{	  10.1145/3606699,
  author	= {Pich\'{e}, Dominique and Font, Ludovic and Zouaq, Amal and
		  Gagnon, Michel},
  title		= {Comparing Heuristic Rules and Masked Language Models for
		  Entity Alignment in the Literature Domain},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3606699},
  doi		= {10.1145/3606699},
  abstract	= {The cultural world offers a staggering amount of rich and
		  varied metadata on cultural heritage, accumulated by
		  governmental, academic, and commercial players. However,
		  the variety of involved institutions means that the data
		  are stored in as many complex and often incompatible models
		  and standards, which limits its availability and
		  explorability by the greater public. The adoption of Linked
		  Open Data technologies allows a strong interlinking of
		  these various databases as well as external connections
		  with existing knowledge bases. However, as they often
		  contain references to the same entities, the delicate issue
		  of entity alignment becomes the central challenge,
		  especially in the absence or scarcity of unique global
		  identifiers. To tackle this issue, we explored two
		  approaches, one based on a set of heuristic rules and one
		  based on masked language models, or masked language models
		  (MLMs). We compare these two approaches, as well as
		  different variations of MLMs, including some models trained
		  on a different language, and various levels of data
		  cleaning and labeling. Our results show that heuristics are
		  a solid approach but also that MLM-based entity alignment
		  obtains better performance coupled with the fact that it is
		  robust to the data format and does not require any form of
		  data preprocessing, which was not the case of the heuristic
		  approach in our experiments.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {62},
  numpages	= {18},
  keywords	= {Linked open data, entity matching, masked language models,
		  cultural heritage, literature}
}

@InProceedings{	  10.1145/3587259.3627572,
  author	= {Rula, Anisa and D'Souza, Jennifer},
  title		= {Procedural Text Mining with Large Language Models},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627572},
  doi		= {10.1145/3587259.3627572},
  abstract	= {Recent advancements in the field of Natural Language
		  Processing, particularly the development of large-scale
		  language models that are pretrained on vast amounts of
		  knowledge, are creating novel opportunities within the
		  realm of Knowledge Engineering. In this paper, we
		  investigate the usage of large language models (LLMs) in
		  both zero-shot and in-context learning settings to tackle
		  the problem of extracting procedures from unstructured PDF
		  text in an incremental question-answering fashion. In
		  particular, we leverage the current state-of-the-art GPT-4
		  (Generative Pre-trained Transformer 4) model, accompanied
		  by two variations of in-context learning that involve an
		  ontology with definitions of procedures and steps and a
		  limited number of samples of few-shot learning. The
		  findings highlight both the promise of this approach and
		  the value of the in-context learning customisations. These
		  modifications have the potential to significantly address
		  the challenge of obtaining sufficient training data, a
		  hurdle often encountered in deep learning-based Natural
		  Language Processing techniques for procedure extraction.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {9–16},
  numpages	= {8},
  keywords	= {knowledge capture, knowledge representation},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3608298.3608324,
  author	= {Schl\"{o}r, Daniel and Pfister, Jan and Hotho, Andreas},
  title		= {Optimizing Medical Service Request Processes through
		  Language Modeling and Semantic Search},
  year		= {2023},
  isbn		= {9798400700712},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3608298.3608324},
  doi		= {10.1145/3608298.3608324},
  abstract	= {Medical service requests are a crucial part of the
		  workflow in hospitals and healthcare organizations.
		  However, the process of requesting medical services can be
		  time consuming and can require physicians and medical
		  personnel to navigate complex interfaces and enter detailed
		  information about the requested service. In this paper, we
		  propose a system that uses machine learning techniques such
		  as large language models and semantic search to optimize
		  the process of requesting medical services. Our approach
		  enables physicians to request medical services using
		  natural language rather than navigating complex interfaces,
		  allowing for more efficient and flexible interactions with
		  hospital information systems. We evaluate our approach on
		  real-world data and discuss the implications of our work
		  for the future of digital health care. Our results suggest
		  that our approach has the potential to streamline the
		  process of requesting medical services and reduce the time
		  and manual effort required in the daily hospital routine.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Medical and Health Informatics},
  pages		= {136–141},
  numpages	= {6},
  keywords	= {language modeling, medical service optimization, semantic
		  search},
  location	= {Kyoto, Japan},
  series	= {ICMHI '23}
}

@InProceedings{	  10.1145/3661304.3661901,
  author	= {Sequeda, Juan and Allemang, Dean and Jacob, Bryon},
  title		= {A Benchmark to Understand the Role of Knowledge Graphs on
		  Large Language Model's Accuracy for Question Answering on
		  Enterprise SQL Databases},
  year		= {2024},
  isbn		= {9798400706530},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3661304.3661901},
  doi		= {10.1145/3661304.3661901},
  abstract	= {Enterprise applications of Large Language Models (LLMs)
		  hold promise for question answering on enterprise SQL
		  databases. However, the extent to which LLMs can accurately
		  respond to enterprise questions in such databases remains
		  unclear, given the absence of suitable Text-to-SQL
		  benchmarks tailored to enterprise settings. Additionally,
		  the potential of Knowledge Graphs (KGs) to enhance
		  LLM-based question answering by providing business context
		  is not well understood. This study aims to evaluate the
		  accuracy of LLM-powered question answering systems in the
		  context of enterprise questions and SQL databases, while
		  also exploring the role of knowledge graphs in improving
		  accuracy. To achieve this, we introduce a benchmark
		  comprising an enterprise SQL schema in the insurance
		  domain, a range of enterprise queries encompassing
		  reporting to metrics, and a contextual layer incorporating
		  an ontology and mappings that define a knowledge graph. Our
		  primary finding reveals that question answering using
		  GPT-4, with zero-shot prompts directly on SQL databases,
		  achieves an accuracy of 16\%. Notably, this accuracy
		  increases to 54\% when questions are posed over a Knowledge
		  Graph representation of the enterprise SQL database.
		  Therefore, investing in Knowledge Graph provides higher
		  accuracy for LLM powered question answering systems.},
  booktitle	= {Proceedings of the 7th Joint Workshop on Graph Data
		  Management Experiences \&amp; Systems (GRADES) and Network
		  Data Analytics (NDA)},
  articleno	= {5},
  numpages	= {12},
  location	= {Santiago, AA, Chile},
  series	= {GRADES-NDA '24}
}

@Proceedings{	  10.1145/3677779,
  title		= {CMNM '24: Proceedings of the International Conference on
		  Modeling, Natural Language Processing and Machine
		  Learning},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xi'an, China}
}

@InProceedings{	  10.1145/3652620.3688348,
  author	= {Gerhold, Marcus and Kouzel, Aliaksei and Mangal, Haroun
		  and Mehmed, Selin and Zaytsev, Vadim},
  title		= {Modelling of Cyber-Physical Systems through
		  Domain-Specific Languages: Decision, Analysis, Design},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688348},
  doi		= {10.1145/3652620.3688348},
  abstract	= {Cyber-Physical Systems (CPS) integrate computational
		  algorithms and physical components, requiring sophisticated
		  modelling techniques to address complex interactions and
		  dynamics. This paper explores the creation of
		  Domain-Specific Languages (DSLs) tailored for CPS, focusing
		  on the initial three critical phases: decision, analysis,
		  design. We present four key aspects to address in the
		  decision phase, design an ontology as a domain model for
		  the analysis phase, and collect some advice for the design
		  phase. By systematically addressing these phases, we
		  provide a comprehensive framework for developing DSLs that
		  can efficiently model CPS, facilitating improved design,
		  verification, and deployment of these intricate systems.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {1170–1179},
  numpages	= {10},
  keywords	= {cyber-physical systems, ontological analysis,
		  domain-specific languages},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3696500.3696523,
  author	= {He, Yudong and Tang, Yinqiu and Chen, Tianhong},
  title		= {A Study on Large Language Model-Based Approach for
		  Construction Contract Risk Detection},
  year		= {2024},
  isbn		= {9798400710278},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696500.3696523},
  doi		= {10.1145/3696500.3696523},
  abstract	= {Construction projects typically involve large-scale
		  operations and are subject to complex external conditions,
		  making it essential to safeguard the interests of
		  contractor enterprises through well-crafted contract
		  clauses. However, the current reliance on expert judgment
		  for identifying contract risks presents several challenges,
		  including lengthy processing times, heavy workloads, and
		  inconsistent results. To address these issues, this study
		  introduces a Large Language Model (LLM)-based approach for
		  automating the identification of risks in construction
		  contracts. The proposed method was rigorously validated on
		  26 actual contracts, achieving an average accuracy of
		  76.7\% across four state-of-the-art LLMs. This research
		  advances the application of LLMs in construction contract
		  management, providing practical solutions to existing
		  challenges and setting the stage for further exploration in
		  LLM-driven contract analysis.},
  booktitle	= {Proceedings of the 2024 International Conference on Big
		  Data and Digital Management},
  pages		= {136–141},
  numpages	= {6},
  location	= {Shanghai, China},
  series	= {ICBDDM '24}
}

@Article{	  10.1145/3589131,
  author	= {Van Thin, Dang and Hao, Duong Ngoc and Nguyen, Ngan
		  Luu-Thuy},
  title		= {Vietnamese Sentiment Analysis: An Overview and Comparative
		  Study of Fine-tuning Pretrained Language Models},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3589131},
  doi		= {10.1145/3589131},
  abstract	= {Sentiment Analysis (SA) is one of the most active research
		  areas in the Natural Language Processing (NLP) field due to
		  its potential for business and society. With the
		  development of language representation models, numerous
		  methods have shown promising efficiency in fine-tuning
		  pre-trained language models in NLP downstream tasks. For
		  Vietnamese, many available pre-trained language models were
		  also released, including the monolingual and multilingual
		  language models. Unfortunately, all of these models were
		  trained on different architectures, pre-trained data, and
		  pre-processing steps; consequently, fine-tuning these
		  models can be expected to yield different effectiveness. In
		  addition, there is no study focusing on evaluating the
		  performance of these models on the same datasets for the SA
		  task up to now. This article presents a fine-tuning
		  approach to investigate the performance of different
		  pre-trained language models for the Vietnamese SA task. The
		  experimental results show the superior performance of the
		  monolingual PhoBERT model and ViT5 model in comparison with
		  previous studies and provide new state-of-the-art
		  performances on five benchmark Vietnamese SA datasets. To
		  the best of our knowledge, our study is the first attempt
		  to investigate the performance of fine-tuning
		  Transformer-based models on five datasets with different
		  domains and sizes for the Vietnamese SA task.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {166},
  numpages	= {27},
  keywords	= {Vietnamese Sentiment Analysis, fine-tuning language
		  models, monolingual BERT model, multilingual BERT model, T5
		  architecture}
}

@Article{	  10.1145/3688089,
  author	= {Zhou, Kyrie Zhixuan and Kilhoffer, Zachary and Sanfilippo,
		  Madelyn Rose and Underwood, Ted and Gumusel, Ece and Wei,
		  Mengyi and Choudhry, Abhinav and Xiong, Jinjun},
  title		= {Ethics, Governance, and User Mental Models for Large
		  Language Models in Computing Education},
  year		= {2024},
  issue_date	= {Fall 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {31},
  number	= {1},
  issn		= {1528-4972},
  url		= {https://doi.org/10.1145/3688089},
  doi		= {10.1145/3688089},
  abstract	= {Large language models like ChatGPT are disrupting many
		  industries, including computing education. How should
		  policy evolve to improve learning outcomes?},
  journal	= {XRDS},
  month		= oct,
  pages		= {46–51},
  numpages	= {6}
}

@Article{	  10.1145/3529755,
  author	= {Zini, Julia El and Awad, Mariette},
  title		= {On the Explainability of Natural Language Processing Deep
		  Models},
  year		= {2022},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {5},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3529755},
  doi		= {10.1145/3529755},
  abstract	= {Despite their success, deep networks are used as black-box
		  models with outputs that are not easily explainable during
		  the learning and the prediction phases. This lack of
		  interpretability is significantly limiting the adoption of
		  such models in domains where decisions are critical such as
		  the medical and legal fields. Recently, researchers have
		  been interested in developing methods that help explain
		  individual decisions and decipher the hidden
		  representations of machine learning models in general and
		  deep networks specifically. While there has been a recent
		  explosion of work on Explainable Artificial Intelligence
		  (ExAI) on deep models that operate on imagery and tabular
		  data, textual datasets present new challenges to the ExAI
		  community. Such challenges can be attributed to the lack of
		  input structure in textual data, the use of word embeddings
		  that add to the opacity of the models and the difficulty of
		  the visualization of the inner workings of deep models when
		  they are trained on textual data.Lately, methods have been
		  developed to address the aforementioned challenges and
		  present satisfactory explanations on Natural Language
		  Processing (NLP) models. However, such methods are yet to
		  be studied in a comprehensive framework where common
		  challenges are properly stated and rigorous evaluation
		  practices and metrics are proposed.Motivated to democratize
		  ExAI methods in the NLP field, we present in this work a
		  survey that studies model-agnostic as well as
		  model-specific explainability methods on NLP models. Such
		  methods can either develop inherently interpretable NLP
		  models or operate on pre-trained models in a post hoc
		  manner. We make this distinction and we further decompose
		  the methods into three categories according to what they
		  explain: (1) word embeddings (input level), (2) inner
		  workings of NLP models (processing level), and (3)
		  models’ decisions (output level). We also detail the
		  different evaluation approaches interpretability methods in
		  the NLP field. Finally, we present a case-study on the
		  well-known neural machine translation in an appendix, and
		  we propose promising future research directions for ExAI in
		  the NLP field.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {103},
  numpages	= {31},
  keywords	= {ExAI, NLP, language models, transformers, neural machine
		  translation, transparent embedding models, explaining
		  decisions}
}

@InProceedings{	  10.1145/3635059.3635068,
  author	= {Mamalis, Marios Evangelos and Kalampokis, Evangelos and
		  Karamanou, Areti and Brimos, Petros and Tarabanis,
		  Konstantinos},
  title		= {Can Large Language Models Revolutionalize Open Government
		  Data Portals? A Case of Using ChatGPT in
		  statistics.gov.scot},
  year		= {2024},
  isbn		= {9798400716263},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3635059.3635068},
  doi		= {10.1145/3635059.3635068},
  abstract	= {Large language models possess tremendous natural language
		  understanding and generation abilities. However, they often
		  lack the ability to discern between fact and fiction,
		  leading to factually incorrect responses. Open Government
		  Data are repositories of, often times linked, information
		  that is freely available to everyone. By combining these
		  two technologies in a proof of concept designed application
		  utilizing the GPT3.5 OpenAI model and the Scottish open
		  statistics portal, we show that not only is it possible to
		  augment the large language model’s factuality of
		  responses, but also propose a novel way to effectively
		  access and retrieve statistical information from the data
		  portal just through natural language querying. We
		  anticipate that this paper will trigger a discussion
		  regarding the transformation of Open Government Portals
		  through large language models.},
  booktitle	= {Proceedings of the 27th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {53–59},
  numpages	= {7},
  keywords	= {chatgpt, large language model, linked data, natural
		  language processing, open government data},
  location	= {Lamia, Greece},
  series	= {PCI '23}
}

@InProceedings{	  10.1145/3584371.3612953,
  author	= {Quintana, Felix and Treangen, Todd and Kavraki, Lydia},
  title		= {Leveraging Large Language Models for Predicting Microbial
		  Virulence from Protein Structure and Sequence},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3612953},
  doi		= {10.1145/3584371.3612953},
  abstract	= {In the aftermath of COVID-19, screening for pathogens has
		  never been a more relevant problem. However, computational
		  screening for pathogens is challenging due to a variety of
		  factors, including (i) the complexity and role of the host,
		  (ii) virulence factor divergence and dynamics, and (iii)
		  population and community-level dynamics. Considering a
		  potential pathogen's molecular interactions, specifically
		  individual proteins and protein interactions can help
		  pinpoint a potential protein of a given microbe to cause
		  disease. However, existing tools for pathogen screening
		  rely on existing annotations (KEGG, GO, etc), making the
		  assessment of novel and unannotated proteins more
		  challenging. Here, we present an LLM-inspired approach that
		  considers protein sequence and structure to predict protein
		  virulence. We present a two-stage model incorporating
		  evolutionary features captured from the DistilProtBert
		  language model and protein structure in a graph
		  convolutional network. Our model performs better than
		  sequence alone for virulence function when high-quality
		  structures are present, thus representing a path forward
		  for virulence prediction of novel and unannotated
		  proteins.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {103},
  numpages	= {6},
  keywords	= {protein function, virulence prediction, graph-based
		  models, large language models},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@Article{	  10.14778/3712221.3712222,
  author	= {Qiang, Zhangcheng and Wang, Weiqing and Taylor, Kerry},
  title		= {Agent-OM: Leveraging LLM Agents for Ontology Matching},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {VLDB Endowment},
  volume	= {18},
  number	= {3},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3712221.3712222},
  doi		= {10.14778/3712221.3712222},
  abstract	= {Ontology matching (OM) enables semantic interoperability
		  between different ontologies and resolves their conceptual
		  heterogeneity by aligning related entities. OM systems
		  currently have two prevailing design paradigms:
		  conventional knowledge-based expert systems and newer
		  machine learning-based predictive systems. While large
		  language models (LLMs) and LLM agents have revolutionised
		  data engineering and have been applied creatively in many
		  domains, their potential for OM remains underexplored. This
		  study introduces a novel agent-powered LLM-based design
		  paradigm for OM systems. With consideration of several
		  specific challenges in leveraging LLM agents for OM, we
		  propose a generic framework, namely Agent-OM (Agent for
		  Ontology Matching), consisting of two Siamese agents for
		  retrieval and matching, with a set of OM tools. Our
		  framework is implemented in a proof-of-concept system.
		  Evaluations of three Ontology Alignment Evaluation
		  Initiative (OAEI) tracks over state-of-the-art OM systems
		  show that our system can achieve results very close to the
		  long-standing best performance on simple OM tasks and can
		  significantly improve the performance on complex and
		  few-shot OM tasks.},
  journal	= {Proc. VLDB Endow.},
  month		= nov,
  pages		= {516–529},
  numpages	= {14}
}

@Article{	  10.1145/3705617,
  author	= {Qu\'{e}r\'{e}, Marianne Aubin Le and Schroeder, Hope and
		  Randazzo, Casey and Gao, Jie},
  title		= {The State of Large Language Models in HCI Research:
		  Workshop Report},
  year		= {2025},
  issue_date	= {January - February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {32},
  number	= {1},
  issn		= {1072-5520},
  url		= {https://doi.org/10.1145/3705617},
  doi		= {10.1145/3705617},
  abstract	= {In this section, we feature reports from conferences,
		  symposia, workshops, and similar events, focusing on
		  discussions where the boundaries of HCI and UX are being
		  challenged and where debate is lively and ongoing.},
  journal	= {Interactions},
  month		= jan,
  pages		= {8–9},
  numpages	= {2}
}

@Article{	  10.1109/taslp.2024.3407575,
  author	= {Chen, Weize and Han, Xu and Lin, Yankai and He, Kaichen
		  and Xie, Ruobing and Zhou, Jie and Liu, Zhiyuan and Sun,
		  Maosong},
  title		= {Hyperbolic Pre-Trained Language Model},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3407575},
  doi		= {10.1109/TASLP.2024.3407575},
  abstract	= {In recent years, we have witnessed significant
		  improvements in pre-trained language models (PLM) brought
		  about by the scaling of parameter sizes and data amounts.
		  However, this also brings high computational and storage
		  costs. In this paper, we present a new direction to improve
		  PLMs without scaling parameters and data: adopting a
		  geometric feature space that is more suitable for encoding
		  the intrinsic structured features of text. Although text is
		  generally considered unstructured data, it possesses rich
		  intrinsic structured features that signify syntactic and
		  semantic relationships. Leveraging these structured
		  features is vital for text understanding. Given that
		  structured features are better encoded in hyperbolic spaces
		  than in the Euclidean spaces used by conventional PLMs, we
		  propose that PLMs should operate entirely within hyperbolic
		  spaces. Our experiments demonstrate the superiority of
		  hyperbolic PLMs over Euclidean PLMs across a wide variety
		  of tasks, using the same parameter and data settings. This
		  suggests that altering the geometry of model representation
		  is a promising direction for model enhancement.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= may,
  pages		= {3101–3112},
  numpages	= {12}
}

@Article{	  10.1145/3600230,
  author	= {Katwe, Praveen Kumar and Khamparia, Aditya and Gupta,
		  Deepak and Dutta, Ashit Kumar},
  title		= {Methodical Systematic Review of Abstractive Summarization
		  and Natural Language Processing Models for Biomedical
		  Health Informatics: Approaches, Metrics and Challenges},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3600230},
  doi		= {10.1145/3600230},
  abstract	= {Text summarization tasks are primarily very useful for
		  decision support systems and provide a source for useful
		  data for training of bots as they can reduce and retain the
		  useful information from the large corpus. This review
		  article is for studying the literature that already exists
		  in context of abstractive summarization and application of
		  NLP language models in biomedical and associated healthcare
		  applications. In past decade with trends like bigdata, IOT,
		  enormous amount of data is getting processed in all
		  structured, unstructured and semi structured formats. This
		  review provides a comprehensive literature survey in
		  research trends for abstractive summarization, foundations
		  of machine translation and evolution of language models.
		  This review identifies the potential of language model to
		  provide a possible methodology for improving the
		  performance and accuracy of various tasks in summarization.
		  Deep neural network-based language models have now been the
		  widely accepted state of art for various abstractive
		  summarization and there exists an enormous scope to
		  improvise and tune the language models for domain specific
		  use case. This study shows current systems lack in
		  faithfulness to original content and control of degree of
		  hallucination. This review also details on the evaluation
		  criteria and need for automated metrics and attempts to
		  provide guideline for evaluation for abstractive
		  summarization for health informatics.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  keywords	= {NLP, abstractive summarization, sentence compression,
		  sentence fusion, document summarization, language model,
		  ROUGE}
}

@InProceedings{	  10.1145/3544548.3581441,
  author	= {Ashby, Trevor and Webb, Braden K and Knapp, Gregory and
		  Searle, Jackson and Fulda, Nancy},
  title		= {Personalized Quest and Dialogue Generation in Role-Playing
		  Games: A Knowledge Graph- and Language Model-based
		  Approach},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3581441},
  doi		= {10.1145/3544548.3581441},
  abstract	= {Procedural content generation (PCG) in video games offers
		  unprecedented opportunities for customization and user
		  engagement. Working within the specialized context of
		  role-playing games (RPGs), we introduce a novel framework
		  for quest and dialogue generation that places the player at
		  the core of the generative process. Drawing on a
		  hand-crafted knowledge base, our method grounds generated
		  content with in-game context while simultaneously employing
		  a large-scale language model to create fluent, unique,
		  accompanying dialogue. Through human evaluation, we confirm
		  that quests generated using this method can approach the
		  performance of hand-crafted quests in terms of fluency,
		  coherence, novelty, and creativity; demonstrate the
		  enhancement to the player experience provided by greater
		  dynamism; and provide a novel, automated metric for the
		  relevance between quest and dialogue. We view our
		  contribution as a critical step toward dynamic, co-creative
		  narrative frameworks in which humans and AI systems jointly
		  collaborate to create unique and user-specific playable
		  experiences.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {290},
  numpages	= {20},
  keywords	= {English, GPT-2, MMORPG, NPC dialogue, RPG, World of
		  Warcraft, computational creativity, dynamic quest
		  generation, human-AI co-creativity, human-computer
		  interaction, knowledge graph, knowledge-grounded text
		  generation, language model, large-scale language models,
		  narrative, natural language processing, procedural content
		  generation, quest, quests, text generation, transformers,
		  video games},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@InProceedings{	  10.1145/3605098.3636026,
  author	= {Jamil, Hasan and Krawetz, Stephen and Gow, Alexander},
  title		= {Knowledge Synthesis using Large Language Models for a
		  Computational Biology Workflow Ecosystem},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3636026},
  doi		= {10.1145/3605098.3636026},
  abstract	= {An understanding of the molecular basis of musculoskeletal
		  pain is necessary for the development of therapeutics,
		  their management, and possible personalization.
		  One-in-three Americans use OTC pain killers, and one tenth
		  use prescription drugs to manage pain. The CDC also
		  estimates that about 20\% Americans suffer from chronic
		  pain. As the experience of acute or chronic pain varies due
		  to individual genetics and physiology, it is imperative
		  that researchers continue to find novel therapeutics to
		  treat or manage symptoms. In this paper, our goal is to
		  develop a seed knowledgebase computational platform, called
		  BioNursery, that will allow biologists to computationally
		  hypothesize, define and test molecular mechanisms
		  underlying pain. In our knowledge ecosystem, we accumulate
		  curated information from users about the relationships
		  among biological databases, analysis tools, and database
		  contents to generate biological analyses modules, called
		  π-graphs, or process graphs. We propose a mapping function
		  from a natural language description of a hypothesized
		  molecular model to a computational workflow for testing in
		  BioNursery. We use a crowd computing feedback and curation
		  system, called Explorer, to improve proposed computational
		  models for molecular mechanism discovery, and growing the
		  knowledge ecosystem. Since the pain knowledge ecosystem
		  does not yet exist, we validate our approach over a similar
		  application in fertility research.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {523–530},
  numpages	= {8},
  keywords	= {knowledge ecosystem, crowdsourcing, query reformulation},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@InProceedings{	  10.1145/3711954.3711961,
  author	= {Mukanova, Assel and Yergesh, Banu and Yelibayeva, Gaziza
		  and Bekmanova, Gulmira and Razakhova, Bibigul},
  title		= {Developing an Ontological Model for Pre-election
		  Advertising},
  year		= {2025},
  isbn		= {9798400717369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711954.3711961},
  doi		= {10.1145/3711954.3711961},
  abstract	= {The article discusses the process and methods of creating
		  ontological modeling in the field of “Pre-election
		  advertising”. The work includes a structural-semantic
		  analysis of the text corpus of political discourse and the
		  creation of an ontological model of this subject area. The
		  document discusses key concepts, their properties and
		  relationships necessary for building the model, as well as
		  the use of the formal language OWL to describe axioms and
		  relationships between concepts. The ontological model was
		  created using the Prot\'{e}g\'{e} platform and includes
		  various classes, properties, objects and individuals
		  specific to the subject area of pre-election advertising.
		  Methods of constructing logical rules for extracting
		  knowledge from a database, such as production rules and
		  logical programming, are also considered.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Information Systems Engineering},
  pages		= {23–28},
  numpages	= {6},
  keywords	= {Formal model, Knowledge base, Ontology, Political
		  discourse, Pre-election advertising},
  location	= { },
  series	= {ICISE '24}
}

@InProceedings{	  10.1145/3613904.3642026,
  author	= {Reitmaier, Thomas and Raju, Dani Kalarikalayil and Klejch,
		  Ondrej and Wallington, Electra and Markl, Nina and Pearson,
		  Jennifer and Jones, Matt and Bell, Peter and Robinson,
		  Simon},
  title		= {Cultivating Spoken Language Technologies for Unwritten
		  Languages},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642026},
  doi		= {10.1145/3613904.3642026},
  abstract	= {We report on community-centered, collaborative research
		  that weaves together HCI, natural language processing,
		  linguistic, and design insights to develop spoken language
		  technologies for unwritten languages. Across three visits
		  to a Banjara farming community in India, we use
		  participatory, technical, and creative methods to engage
		  community members, collect spoken language photo
		  annotations, and develop an information retrieval (IR)
		  system. Drawing on orality theory, we interrogate
		  assumptions and biases of current speech interfaces and
		  create a simple application that leverages our IR system to
		  match fluidly spoken queries with recorded annotations and
		  surface corresponding photos. In-situ evaluations show how
		  our novel approach returns reliable results and inspired
		  the co-creation of media retrieval use-cases that are more
		  appropriate in oral contexts. The very low (&lt; 4h) spoken
		  data requirements makes our approach adaptable to other
		  contexts where languages are unwritten or have no digital
		  language resources available.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {614},
  numpages	= {17},
  keywords	= {Speech/language, co-creation, field study, zero-resource
		  information retrieval},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3652620.3687809,
  author	= {G\"{o}bel, Susanne and L\"{a}mmel, Ralf},
  title		= {Model-Based Trust Analysis of LLM Conversations},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687809},
  doi		= {10.1145/3652620.3687809},
  abstract	= {LLM-based chatbots are routinely advertised as supporting
		  the collaboration of humans and AI. We study LLM
		  conversations from a knowledge elicitation perspective with
		  the objective of being able to understand and assess the
		  human's trust in knowledge elicited from the LLM and
		  complementary sources. Our approach is supported by the
		  DSML KEML, the Knowledge Elicitation Modeling Language,
		  subject to abstract and visual syntax as well as a model
		  transformation-based model semantics for trust analysis.
		  Conversations are modeled by a combination of sequence
		  diagrams and enhanced argumentation graphs --- the latter
		  for the purpose of relating information pieces (facts and
		  instructions) that are extracted from messages. The
		  analysis of the corresponding models entails trust scores
		  for gathered information (i.e., elicited knowledge).},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {602–610},
  numpages	= {9},
  keywords	= {MDE for AI, knowledge representation models, model-based
		  analysis of LLMS, dsmls for AI usage},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.5555/3635637.3662942,
  author	= {Ichida, Alexandre Yukio and Meneguzzi, Felipe and Cardoso,
		  Rafael C.},
  title		= {BDI Agents in Natural Language Environments},
  year		= {2024},
  isbn		= {9798400704864},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Developing autonomous agents to deal with real-world
		  problems is challenging, especially when developers are not
		  necessarily specialists in artificial intelligence. This
		  poses two key challenges regarding the interface of the
		  programming with the developer, and the efficiency of the
		  resulting agents. In this paper we tackle both challenges
		  in an efficient agent architecture that leverages recent
		  developments in natural language processing, and the
		  intuitive folk psychology abstraction of the beliefs,
		  desires, intentions (BDI) architecture. The resulting
		  architecture uses existing reinforcement learning
		  techniques to bootstrap the agent's reasoning capabilities
		  while allowing a developer to instruct the agent more
		  directly using natural language as its programming
		  interface. We empirically show the efficiency gains of
		  natural language plans over a pure machine learning
		  approach in the ScienceWorld environment.},
  booktitle	= {Proceedings of the 23rd International Conference on
		  Autonomous Agents and Multiagent Systems},
  pages		= {880–888},
  numpages	= {9},
  keywords	= {bdi agents, large language models, natural language,
		  reinforcement learning},
  location	= {Auckland, New Zealand},
  series	= {AAMAS '24}
}

@InProceedings{	  10.1145/3544548.3580907,
  author	= {Petridis, Savvas and Diakopoulos, Nicholas and Crowston,
		  Kevin and Hansen, Mark and Henderson, Keren and
		  Jastrzebski, Stan and Nickerson, Jeffrey V and Chilton,
		  Lydia B},
  title		= {AngleKindling: Supporting Journalistic Angle Ideation with
		  Large Language Models},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3580907},
  doi		= {10.1145/3544548.3580907},
  abstract	= {News media often leverage documents to find ideas for
		  stories, while being critical of the frames and narratives
		  present. Developing angles from a document such as a press
		  release is a cognitively taxing process, in which
		  journalists critically examine the implicit meaning of its
		  claims. Informed by interviews with journalists, we
		  developed AngleKindling, an interactive tool which employs
		  the common sense reasoning of large language models to help
		  journalists explore angles for reporting on a press
		  release. In a study with 12 professional journalists, we
		  show that participants found AngleKindling significantly
		  more helpful and less mentally demanding to use for
		  brainstorming ideas, compared to a prior journalistic angle
		  ideation tool. AngleKindling helped journalists deeply
		  engage with the press release and recognize angles that
		  were useful for multiple types of stories. From our
		  findings, we discuss how to help journalists customize and
		  identify promising angles, and extending AngleKindling to
		  other knowledge-work domains.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {225},
  numpages	= {16},
  keywords	= {Brainstorming, Generative AI, Ideation, Journalism, Large
		  Language Models},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@InProceedings{	  10.1145/3711896.3736879,
  author	= {Zhang, Yiqing and Liu, Xiaozhong and Murai, Fabricio},
  title		= {CLaDMoP: Learning Transferrable Models from Successful
		  Clinical Trials via LLMs},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3736879},
  doi		= {10.1145/3711896.3736879},
  abstract	= {Many existing models for clinical trial outcome prediction
		  are optimized using task-specific loss functions on trial
		  phase-specific data. While this scheme may boost prediction
		  for common diseases and drugs, it can hinder the learning
		  of generalizable representations, leading to more false
		  positives/negatives. To address this limitation, we
		  introduce CLaDMoP, a new pre-training approach for clinical
		  trial outcome prediction, alongside the Successful Clinical
		  Trials dataset (SCT), specifically designed for this task.
		  CLaDMoP leverages a Large Language Model-to encode trials'
		  eligibility criteria-linked to a lightweight Drug-Molecule
		  branch through a novel multi-level fusion technique. To
		  efficiently fuse long embeddings across levels, we
		  incorporate a grouping block, drastically reducing
		  computational overhead. CLaDMoP avoids reliance on
		  task-specific objectives by pre-training on a ''pair
		  matching'' proxy task. Compared to established zero-shot
		  and few-shot baselines, our method significantly improves
		  both PR-AUC and ROC-AUC, especially for phase I and phase
		  II trials. We further evaluate and perform ablation on
		  CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it
		  to state-of-the-art supervised baselines, including
		  MEXA-CTP, on the Trial Outcome Prediction (TOP) benchmark.
		  CLaDMoP achieves up to 10.5\% improvement in PR-AUC and
		  3.6\% in ROC-AUC, while attaining comparable F1 score to
		  MEXA-CTP, highlighting its potential for clinical trial
		  outcome prediction. Code and SCT dataset can be downloaded
		  from https://github.com/murai-lab/CLaDMoP.},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {3901–3911},
  numpages	= {11},
  keywords	= {clinical trial outcome prediction, llms, multi-modal data
		  fusion, representation learning, self-supervised
		  pre-training},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@InProceedings{	  10.1145/3584371.3612942,
  author	= {Kabir, Anowarul and Moldwin, Asher and Shehu, Amarda},
  title		= {A Comparative Analysis of Transformer-based Protein
		  Language Models for Remote Homology Prediction},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3612942},
  doi		= {10.1145/3584371.3612942},
  abstract	= {Protein language models based on the transformer
		  architecture are increasingly shown to learn rich
		  representations from protein sequences that improve
		  performance on a variety of downstream protein prediction
		  tasks. These tasks encompass a wide range of predictions,
		  including prediction of secondary structure, subcellular
		  localization, evolutionary relationships within protein
		  families, as well as superfamily and family membership.
		  There is recent evidence that such models also implicitly
		  learn structural information. In this paper we put this to
		  the test on a hallmark problem in computational biology,
		  remote homology prediction. We employ a rigorous setting,
		  where, by lowering sequence identity, we clarify whether
		  the problem of remote homology prediction has been solved.
		  Among various interesting findings, we report that current
		  state-of-the-art, large models are still underperforming in
		  the "twilight zone" of very low sequence identity.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {97},
  numpages	= {9},
  keywords	= {remote homology, transformer, large language model},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@Article{	  10.1109/taslp.2022.3153268,
  author	= {Wang, Chengyu and Dai, Suyang and Wang, Yipeng and Yang,
		  Fei and Qiu, Minghui and Chen, Kehan and Zhou, Wei and
		  Huang, Jun},
  title		= {ARoBERT: An ASR Robust Pre-Trained Language Model for
		  Spoken Language Understanding},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3153268},
  doi		= {10.1109/TASLP.2022.3153268},
  abstract	= {Spoken Language Understanding (SLU) aims to interpret the
		  meanings of human speeches in order to support various
		  human-machine interaction systems. A key technique for SLU
		  is Automatic Speech Recognition (ASR), which transcribes
		  speech signals into text contents. As the output texts of
		  modern ASR systems unavoidably contain errors, mainstream
		  SLU models either trained or tested on texts transcribed by
		  ASR systems would not be sufficiently error robust. We
		  present ARoBERT, an ASR Robust BERT model, which can be
		  fine-tuned to solve a variety of SLU tasks with noisy
		  inputs. To guarantee the robustness of ARoBERT, during
		  pretraining, we decrease the fluctuations of language
		  representations when some parts of the input texts are
		  replaced by homophones or synophones. Specifically, we
		  propose two novel self-supervised pre-training tasks for
		  ARoBERT, namely Phonetically-aware Masked Language Modeling
		  (PMLM) and ASR Model-adaptive Masked Language Modeling
		  (AMMLM). The PMLM task explicitly fuses the knowledge of
		  word phonetic similarities into the pre-training process,
		  which forces homophones and synophones to share similar
		  representations. In AMMLM, a data-driven algorithm is
		  further introduced to mine typical ASR errors such that
		  ARoBERT can tolerate ASR model errors. In the experiments,
		  we evaluate ARoBERT over multiple datasets. The results
		  show the superiority of ARoBERT, which consistently
		  outperforms strong baselines. We have also shown that
		  ARoBERT outperforms state-of-the-arts on a public
		  benchmark. Currently, ARoBERT has been deployed in an
		  online production system with significant improvements.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= feb,
  pages		= {1207–1218},
  numpages	= {12}
}

@InProceedings{	  10.1145/3573428.3573598,
  author	= {Yao, Shunyu and Hu, Jie and Sun, Chuxiong and Gao, Zhiqiao
		  and Liu, Ning},
  title		= {Key Phrase Extraction based on Pre-trained Language
		  Models},
  year		= {2023},
  isbn		= {9781450397148},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3573428.3573598},
  doi		= {10.1145/3573428.3573598},
  abstract	= {With the explosion of information and a large amount of
		  data appearing every moment, it is a meaningful task to
		  quickly find the information people want to know in a large
		  amount of text and to present long texts in a streamlined
		  form. Key phrase extraction, which aims to extract from
		  documents a collection of key phrases that express the
		  topic and content of the document, is important for text
		  processing tasks such as information retrieval and document
		  classification and can provide readers with a more
		  comprehensive overview of the topic. We use two types of
		  pre-trained language models for key phrase extraction,
		  namely DeBERTa and RoBERTa, which are first pre-trained on
		  the dataset and then fine-tuned, and the experimental
		  results of these models proved that DeBERTa-V3-Large has
		  reached an F1 score of 0.8925, which is the best result
		  among these models.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {941–945},
  numpages	= {5},
  keywords	= {Artificial Intelligence, Key Phrase Extraction, Natural
		  Language Processing, Neural Network},
  location	= {Xiamen, China},
  series	= {EITCE '22}
}

@InProceedings{	  10.1145/3664647.3681472,
  author	= {Sun, Luoyi and Xu, Xuenan and Wu, Mengyue and Xie, Weidi},
  title		= {Auto-ACD: A Large-scale Dataset for Audio-Language
		  Representation Learning},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681472},
  doi		= {10.1145/3664647.3681472},
  abstract	= {Recently, the AI community has made significant strides in
		  developing powerful foundation models, driven by
		  large-scale multimodal datasets. However, for audio
		  representation learning, existing datasets suffer from
		  limitations in the following aspects: insufficient volume,
		  simplistic content, and arduous collection procedures. To
		  establish an audio dataset with high-quality captions, we
		  propose an innovative, automatic approach leveraging
		  multimodal inputs, such as video frames, audio streams.
		  Specifically, we construct a large-scale, high-quality,
		  audio-language dataset, named as Auto-ACD, comprising over
		  1.5M audio-text pairs. We exploit a series of pre-trained
		  models or APIs, to determine audio-visual synchronisation,
		  generate image captions, object detection, or audio tags
		  for specific videos. Subsequently, we employ LLM to
		  paraphrase a congruent caption for each audio, guided by
		  the extracted multi-modality clues. To demonstrate the
		  effectiveness of the proposed dataset, we train widely used
		  models on our dataset and show performance improvement on
		  various downstream tasks, for example, audio-language
		  retrieval, audio captioning, zero-shot classification. In
		  addition, we establish a novel benchmark with environmental
		  information and provide a benchmark for audio-text tasks.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {5025–5034},
  numpages	= {10},
  keywords	= {audio captioning, audio-language dataset, audio-language
		  representation learning},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3591196.3593516,
  author	= {Ding, Zijian and Srinivasan, Arvind and Macneil, Stephen
		  and Chan, Joel},
  title		= {Fluid Transformers and Creative Analogies: Exploring Large
		  Language Models’ Capacity for Augmenting Cross-Domain
		  Analogical Creativity},
  year		= {2023},
  isbn		= {9798400701801},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3591196.3593516},
  doi		= {10.1145/3591196.3593516},
  abstract	= {Cross-domain analogical reasoning is a core creative
		  ability that can be challenging for humans. Recent work has
		  shown some proofs-of-concept of Large language Models’
		  (LLMs) ability to generate cross-domain analogies. However,
		  the reliability and potential usefulness of this capacity
		  for augmenting human creative work has received little
		  systematic exploration. In this paper, we systematically
		  explore LLMs capacity to augment cross-domain analogical
		  reasoning. Across three studies, we found: 1) LLM-generated
		  cross-domain analogies were frequently judged as helpful in
		  the context of a problem reformulation task (median 4 out
		  of 5 helpfulness rating), and frequently (∼ 80\% of
		  cases) led to observable changes in problem formulations,
		  and 2) there was an upper bound of ∼ 25\% of outputs
		  being rated as potentially harmful, with a majority due to
		  potentially upsetting content, rather than biased or toxic
		  content. These results demonstrate the potential utility
		  — and risks — of LLMs for augmenting cross-domain
		  analogical creativity.},
  booktitle	= {Proceedings of the 15th Conference on Creativity and
		  Cognition},
  pages		= {489–505},
  numpages	= {17},
  keywords	= {Analogy, Creativity Support Tools, Large Language Models},
  location	= {Virtual Event, USA},
  series	= {C&amp;C '23}
}

@InProceedings{	  10.1145/3650400.3650405,
  author	= {Wang, Chen and Hua, Min and Song, Jiale and Tang,
		  Xue-song},
  title		= {Knowledge Graphs Enhanced Large Language Model Prompt for
		  Electric Power Question Answering},
  year		= {2024},
  isbn		= {9798400708305},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3650400.3650405},
  doi		= {10.1145/3650400.3650405},
  abstract	= {With the continuous development and digital transformation
		  in the field of electric power, the application of large
		  language models in the electric power industry has become a
		  remarkable trend. The electric power industry is an
		  information-intensive domain involving extensive data
		  processing, predictive analysis, and decision-making.
		  Therefore, the application of large language models in the
		  electric power sector is of great significance. Current
		  large language models such as GPT3.5 and GLM can perform
		  well in tasks such as question answering dialogues.
		  However, these models still face challenges such as answer
		  hallucination and inaccurate responses. This paper proposes
		  a method to enhance question answering in large language
		  models using knowledge graphs, aiming to improve the
		  accuracy and reliability of these models in question
		  answering tasks in the electric power domain.The proposed
		  method first utilizes local electric power data to extract
		  triplets and generate a question answering dataset specific
		  to the electric power domain using a large language model.
		  Then, the relationships of the knowledge graph triplets are
		  incorporated into the question prompt to enhance the
		  quality of the model's answers. Furthermore, we fine-tune
		  the large language model using the expanded question set
		  derived from the triplets as knowledge enhanced data.
		  Subsequently, we conduct experiments on both an electric
		  power question answering dataset and a knowledge graph
		  question answering dataset. The experimental results
		  demonstrate that our method significantly improves various
		  metrics of the large language model in the electric power
		  question answering task. This research provides new
		  insights and approaches to enhance the effectiveness of
		  question answering systems in the electric power domain.
		  Future studies can further explore and optimize this prompt
		  expansion method for application in broader domains and
		  tasks.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {24–29},
  numpages	= {6},
  location	= {Xiamen, China},
  series	= {EITCE '23}
}

@InProceedings{	  10.1145/3722212.3725628,
  author	= {Setlur, Vidya},
  title		= {Supporting Human-Centric Data Exploration Through
		  Semantics and Natural Language Interaction},
  year		= {2025},
  isbn		= {9798400715648},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3722212.3725628},
  doi		= {10.1145/3722212.3725628},
  abstract	= {Data science plays an increasingly central role in
		  decision-making across domains, yet the effectiveness of
		  these decisions hinges not only on sophisticated algorithms
		  but also on how well systems support human interpretation,
		  exploration, and communication of data. This tutorial
		  explores the intersection of semantics, natural language
		  processing (NLP), and human-computer interaction in
		  creating human-centric data exploration tools that promote
		  accessibility, trust, and transparency. This includes
		  techniques for generating perceptually meaningful visual
		  encodings, using NLP for query interpretation and ambiguity
		  resolution, and designing conversational interfaces that
		  align with users' intent. The tutorial will highlight
		  research from a broad set of contributors in the
		  SIGMOD/VLDB, HCI, and visualization communities, spanning
		  natural language interfaces for databases, multimodal
		  interaction systems, semantic search for data repositories,
		  and the use of AI and large language models to augment
		  visual and textual analysis.As part of this 1.5-hour
		  session, we will present case studies and systems that
		  demonstrate how human-centric design can be integrated into
		  the data analysis pipeline, including tools that support
		  mixed-initiative interaction, adaptive defaults, and
		  subjective query interpretation. We will also discuss open
		  challenges and research opportunities, such as semantic
		  inferencing for unstructured data, retrieval-augmented
		  generation (RAG), and ethical considerations around
		  fairness, explainability, and user agency. Drawing from
		  principles in perception, linguistics, AI, and HCI, this
		  tutorial aims to equip attendees with both conceptual
		  frameworks and practical techniques to build more
		  inclusive, interpretable, and intelligent data systems. It
		  is intended for a broad audience in the SIGMOD/VLDB
		  community interested in designing the next generation of
		  data exploration tools that are aligned with human needs.},
  booktitle	= {Companion of the 2025 International Conference on
		  Management of Data},
  pages		= {851–854},
  numpages	= {4},
  keywords	= {conversational analytics, human-centered data exploration,
		  natural language interfaces, semantic enrichment},
  location	= {Berlin, Germany},
  series	= {SIGMOD/PODS '25}
}

@Article{	  10.1145/3609429.3609433,
  author	= {Bergami, Giacomo and Zegad\l{}o, Wiktor},
  title		= {Towards a Generalised Semistructured Data Model and Query
		  Language},
  year		= {2023},
  issue_date	= {Summer 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2023},
  number	= {Summer},
  issn		= {1931-1745},
  url		= {https://doi.org/10.1145/3609429.3609433},
  doi		= {10.1145/3609429.3609433},
  abstract	= {Although current efforts are all aimed at re-defining new
		  ways to harness old data representations, possibly with new
		  schema features, the challenges still open provide evidence
		  of the need for a "diametrically opposite" approach: in
		  fact, all information generated in real contexts is to be
		  understood lacking of any form of schema, where the schema
		  associated with such data is only determined a posteriori
		  based on either a specific application context, or from
		  some data's facets of interest. This solution should still
		  enable recommendation systems to manipulate the
		  aforementioned data semantically. After providing evidence
		  of these limitations from current literature, we propose a
		  new Generalized Semistructured data Model that makes
		  possible queries expressible in any data representation
		  through a Generalised Semistructured Query Language, both
		  relying upon script v2.0 as a MetaModel language
		  manipulating types as terms as well as allowing structural
		  aggregation functions.},
  journal	= {SIGWEB Newsl.},
  month		= aug,
  articleno	= {4},
  numpages	= {22}
}

@InProceedings{	  10.1145/3685650.3685667,
  author	= {Wanna, Selma and Solovyev, Nicholas and Barron, Ryan and
		  Eren, Maksim E. and Bhattarai, Manish and Rasmussen, Kim
		  \O{}. and Alexandrov, Boian S.},
  title		= {TopicTag: Automatic Annotation of NMF Topic Models Using
		  Chain of Thought and Prompt Tuning with LLMs},
  year		= {2024},
  isbn		= {9798400711695},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3685650.3685667},
  doi		= {10.1145/3685650.3685667},
  abstract	= {Topic modeling is a technique for organizing and
		  extracting themes from large collections of unstructured
		  text. Non-negative matrix factorization (NMF) is a common
		  unsupervised approach that decomposes a term
		  frequency-inverse document frequency (TF-IDF) matrix to
		  uncover latent topics and segment the dataset accordingly.
		  While useful for highlighting patterns and clustering
		  documents, NMF does not provide explicit topic labels,
		  necessitating subject matter experts (SMEs) to assign
		  labels manually. We present a methodology for automating
		  topic labeling in documents clustered via NMF with
		  automatic model determination (NMFk). By leveraging the
		  output of NMFk and employing prompt engineering, we utilize
		  large language models (LLMs) to generate accurate topic
		  labels. Our case study on over 34,000 scientific abstracts
		  on Knowledge Graphs demonstrates the effectiveness of our
		  method in enhancing knowledge management and document
		  organization.},
  booktitle	= {Proceedings of the ACM Symposium on Document Engineering
		  2024},
  articleno	= {8},
  numpages	= {4},
  keywords	= {chain of thought, llm, nmf, prompt tuning, topic
		  labeling},
  location	= {San Jose, CA, USA},
  series	= {DocEng '24}
}

@Article{	  10.14778/3665844.3665857,
  author	= {Feuer, Benjamin and Liu, Yurong and Hegde, Chinmay and
		  Freire, Juliana},
  title		= {ArcheType: A Novel Framework for Open-Source Column Type
		  Annotation Using Large Language Models},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {9},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3665844.3665857},
  doi		= {10.14778/3665844.3665857},
  abstract	= {Existing deep-learning approaches to semantic column type
		  annotation (CTA) have important shortcomings: they rely on
		  semantic types which are fixed at training time; require a
		  large number of training samples per type; incur high
		  run-time inference costs; and their performance can degrade
		  when evaluated on novel datasets, even when types remain
		  constant. Large language models have exhibited strong
		  zero-shot classification performance on a wide range of
		  tasks and in this paper we explore their use for CTA. We
		  introduce ArcheType, a simple, practical method for context
		  sampling, prompt serialization, model querying, and label
		  remapping, which enables large language models to solve CTA
		  problems in a fully zero-shot manner. We ablate each
		  component of our method separately, and establish that
		  improvements to context sampling and label remapping
		  provide the most consistent gains. ArcheType establishes a
		  new state-of-the-art performance on zero-shot CTA
		  benchmarks (including three new domain-specific benchmarks
		  which we release along with this paper), and when used in
		  conjunction with classical CTA techniques, it outperforms a
		  SOTA DoDuo model on the fine-tuned SOTAB benchmark.},
  journal	= {Proc. VLDB Endow.},
  month		= may,
  pages		= {2279–2292},
  numpages	= {14}
}

@InBook{	  10.1145/3674127.3674136,
  author	= {Galu\v{s}\v{c}\'{a}kov\'{a}, Petra and Oard, Douglas W.
		  and Nair, Suraj},
  title		= {Cross-language Retrieval},
  year		= {2024},
  isbn		= {9798400710506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  url		= {https://doi.org/10.1145/3674127.3674136},
  booktitle	= {Information Retrieval: Advanced Topics and Techniques},
  pages		= {321–357},
  numpages	= {37}
}

@Article{	  10.14778/3626292.3626294,
  author	= {Arora, Simran and Yang, Brandon and Eyuboglu, Sabri and
		  Narayan, Avanika and Hojel, Andrew and Trummer, Immanuel
		  and R\'{e}, Christopher},
  title		= {Language Models Enable Simple Systems for Generating
		  Structured Views of Heterogeneous Data Lakes},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {2},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3626292.3626294},
  doi		= {10.14778/3626292.3626294},
  abstract	= {A long standing goal in the data management community is
		  developing systems that input documents and output
		  queryable tables without user effort. Given the sheer
		  variety of potential documents, state-of-the art systems
		  make simplifying assumptions and use domain specific
		  training. In this work, we ask whether we can maintain
		  generality by using the in-context learning abilities of
		  large language models (LLMs). We propose and evaluate
		  Evaporate, a prototype system powered by LLMs. We identify
		  two strategies for implementing this system: prompt the LLM
		  to directly extract values from documents or prompt the LLM
		  to synthesize code that performs the extraction. Our
		  evaluations show a cost-quality tradeoff between these two
		  approaches. Code synthesis is cheap, but far less accurate
		  than directly processing each document with the LLM. To
		  improve quality while maintaining low cost, we propose an
		  extended implementation, Evaporate-Code+, which achieves
		  better quality than direct extraction. Our insight is to
		  generate many candidate functions and ensemble their
		  extractions using weak supervision. Evaporate-Code+
		  outperforms the state-of-the art systems using a sublinear
		  pass over the documents with the LLM. This equates to a
		  110X reduction in the number of documents the LLM needs to
		  process across our 16 real-world evaluation settings.},
  journal	= {Proc. VLDB Endow.},
  month		= oct,
  pages		= {92–105},
  numpages	= {14}
}

@InProceedings{	  10.1145/3528588.3528658,
  author	= {Alchokr, Rand and Borkar, Manoj and Thotadarya, Sharanya
		  and Saake, Gunter and Leich, Thomas},
  title		= {Supporting systematic literature reviews using
		  deep-learning-based language models},
  year		= {2023},
  isbn		= {9781450393430},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3528588.3528658},
  doi		= {10.1145/3528588.3528658},
  abstract	= {Background: Systematic Literature Reviews are an important
		  research method for gathering and evaluating the available
		  evidence regarding a specific research topic. However, the
		  process of conducting a Systematic Literature Review
		  manually can be difficult and time-consuming. For this
		  reason, researchers aim to semi-automate this process or
		  some of its phases. Aim: We aimed at using a deep-learning
		  based contextualized embeddings clustering technique
		  involving transformer-based language models and a weighted
		  scheme to accelerate the conduction phase of Systematic
		  Literature Reviews for efficiently scanning the initial set
		  of retrieved publications. Method: We performed an
		  experiment using two manually conducted SLRs to evaluate
		  the performance of two deep-learning-based clustering
		  models. These models build on transformer-based deep
		  language models (i.e., BERT and S-BERT) to extract
		  contextualized embeddings on different text levels along
		  with a weighted scheme to cluster similar publications.
		  Results: Our primary results show that clustering based on
		  embedding at paragraph-level using S-BERT-paragraph
		  represents the best performing model setting in terms of
		  optimizing the required parameters such as correctly
		  identifying primary studies, number of additional documents
		  identified as part of the relevant cluster and the
		  execution time of the experiments. Conclusions: The
		  findings indicate that using natural-language-based
		  deep-learning architectures for semi-automating the
		  selection of primary studies can accelerate the scanning
		  and identification process. While our results represent
		  first insights only, such a technique seems to enhance SLR
		  process, promising to help researchers identify the most
		  relevant publications more quickly and efficiently.},
  booktitle	= {Proceedings of the 1st International Workshop on Natural
		  Language-Based Software Engineering},
  pages		= {67–74},
  numpages	= {8},
  keywords	= {BERT, deep learning, language models, systematic
		  literature review},
  location	= {Pittsburgh, Pennsylvania},
  series	= {NLBSE '22}
}

@InProceedings{	  10.1145/3689492.3690054,
  author	= {Marron, Mark},
  title		= {A Programming Language for Data and Configuration!},
  year		= {2024},
  isbn		= {9798400712159},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689492.3690054},
  doi		= {10.1145/3689492.3690054},
  abstract	= {A day in the life of a developer often involves more time
		  working with schemas, configurations, and data description
		  systems than writing code and logic in a classical
		  programming language. As more systems move into distributed
		  worlds, e.g. cloud and microservices, and developers make
		  increasing use of libraries and frameworks, the need to
		  interact with a range of data formats and configuration
		  mechanisms is only increasing. This is a treacherous world,
		  where a misspelled property name or missing field can
		  render an entire service inoperable, a mistake that a
		  number in an API represents seconds instead of
		  milli-seconds can lead to a message being set for delivery
		  in several months instead of in an hour, misconfigured
		  schema can lead to public exposure of sensitive data, and
		  corrupt or erroneous results from a misunderstood data
		  format could result in massive financial and/or
		  reputational damage. To address these challenges this paper
		  casts the problems of data and configuration descriptions,
		  not as a problem of data representation, but as a type
		  system problem, that can be addressed with well understood
		  and highly effective programming language techniques! The
		  novel challenge is that data representation and
		  configuration are universal concerns in a system and,
		  particularly in modern cloud or micro-service systems,
		  these systems may involve many programming languages. In
		  the past this has led to specification systems that use a
		  least-common-denominator set of data types, often little
		  more than strings and numbers, and then rely on conventions
		  or (out-of-date) documentation to ensure that the data is
		  interpreted correctly. This paper shows that, with careful
		  design, it is possible to create a rich universal system
		  that can be used to express data and configuration
		  specifications in a way that is human readable/writable and
		  that can be produced/consumed, much like JSON, by a wide
		  range of programming languages and systems.},
  booktitle	= {Proceedings of the 2024 ACM SIGPLAN International
		  Symposium on New Ideas, New Paradigms, and Reflections on
		  Programming and Software},
  pages		= {147–161},
  numpages	= {15},
  keywords	= {Configuration, Data Specification, Programming Language},
  location	= {Pasadena, CA, USA},
  series	= {Onward! '24}
}

@InProceedings{	  10.1145/3723366.3723405,
  author	= {Qiu, Shunli and Jiang, Wenxia},
  title		= {Research on the Construction of Semantic Information
		  Retrieval Model Based on Logistics Ontology},
  year		= {2025},
  isbn		= {9798400718298},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3723366.3723405},
  doi		= {10.1145/3723366.3723405},
  abstract	= {Along with the popularity of the Internet and the increase
		  in the amount of information, how to quickly and
		  effectively retrieve the expected information in the
		  massive information has become the focus of information
		  retrieval concerns and research. The current retrieval
		  system is mainly based on the search keywords of the full
		  text matching query, the results often return a large
		  number of useless information, in the search rate and
		  accuracy rate can not meet the user's retrieval needs. In
		  this paper, based on the logistics field, we study the
		  construction method of domain ontology, semantic annotation
		  of knowledge document information based on ontology, and
		  semantic query expansion of user's query statement using
		  domain ontology, and propose a framework structure of
		  semantic information retrieval model based on domain
		  ontology, as well as designing and realising a prototype
		  system.},
  booktitle	= {Proceedings of the 2024 4th International Symposium on Big
		  Data and Artificial Intelligence},
  pages		= {247–252},
  numpages	= {6},
  keywords	= {Ontology, model building, semantic information retrieval},
  location	= { },
  series	= {ISBDAI '24}
}

@InProceedings{	  10.1145/3650400.3650526,
  author	= {Li, Wenqing and Qi, Xiaoman and Zhao, Qi and Wang, Chen
		  and Wu, Qiongyu and Tang, Xue-song},
  title		= {Knowledge Graph-Based Credibility Evaluation Method for
		  Electric Grid Large Language Model Knowledge
		  Question-Answering},
  year		= {2024},
  isbn		= {9798400708305},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3650400.3650526},
  doi		= {10.1145/3650400.3650526},
  abstract	= {In the field of electricity, specialized terminology is
		  often intricate and complex, making it challenging for
		  non-experts to comprehend. However, with the advancement of
		  artificial intelligence technology, the emergence of large
		  language models provides a new technological solution to
		  address this issue. Large language models, based on deep
		  learning techniques, have the capability to quickly
		  understand and interpret specialized terminology in the
		  electricity domain through learning from a vast corpus of
		  professional literature and data. They can then be applied
		  to various domains, including question-answering systems.
		  However, existing large language models still face issues
		  of unreliable outputs, necessitating a method to evaluate
		  their results and improve the quality of their
		  applications. We propose a knowledge graph-based
		  credibility evaluation method for electric grid large
		  language model knowledge question-answering. This method
		  aligns the answers generated by large language models with
		  the knowledge graph of a local knowledge base and
		  calculates their cosine similarity and Pearson correlation
		  coefficient. We batch-process the answers from the large
		  language model into an electricity dataset and validate
		  them using this method. Experimental results demonstrate
		  that this method can accurately and efficiently reflect the
		  relevance between texts, providing a reliable scoring basis
		  for question-answering by large models in vertical domains.
		  Future research can focus on exploring other embedding
		  methods that can better extract semantic relationships
		  between texts and validating the feasibility of this method
		  in vertical domains other than electricity.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {754–759},
  numpages	= {6},
  location	= {Xiamen, China},
  series	= {EITCE '23}
}

@InProceedings{	  10.1145/3625156.3625157,
  author	= {Liu, Gang and Jiang, Wenhua and Hu, Yulin and Zhan, Kai
		  and Wang, Tongli},
  title		= {CGIR: a Model of Cross Language Information Retrieval
		  based on Concept Graph by Fusing Attention Mechanism},
  year		= {2023},
  isbn		= {9798400708206},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625156.3625157},
  doi		= {10.1145/3625156.3625157},
  abstract	= {Cross language information retrieval faces challenges such
		  as language differences, data scarcity, contextual
		  disparities, and machine translation errors. To enhance
		  retrieval accuracy and effectiveness, this paper proposes a
		  similarity evaluation framework called Concept Graph
		  Information Retrieval (CGIR). The framework includes the
		  creation of concept graphs, quantized representations of
		  these graphs, and retrieval processes. The construction
		  process of CGIR incorporates an attention mechanism,
		  significantly boosting its performance and accuracy.
		  Through this fusion, concept graphs offer a comprehensive
		  representation of texts, capturing the essence of the
		  entire content while minimizing the displayed information,
		  all while preserving the original meaning of the text to
		  the fullest extent possible. The experimental results
		  clearly demonstrate that the generated concept graphs
		  effectively function as semantic representations of the
		  entire texts. In comparison to keyword-based,
		  ontology-based, and term-based retrieval methods, CGIR
		  exhibits a remarkable improvement in accuracy, surpassing
		  them by over 10\%.CCS CONCEPTS • Information
		  systems∼Information retrieval},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Information Science and Systems},
  pages		= {1–7},
  numpages	= {7},
  keywords	= {attention mechanism, concept graph, information
		  retrieval},
  location	= {Edinburgh, United Kingdom},
  series	= {ICISS '23}
}

@Article{	  10.1145/3575803,
  author	= {Di, Donglin and Song, Xianyang and Zhang, Weinan and
		  Zhang, Yue and Wang, Fanglin},
  title		= {Building Dialogue Understanding Models for Low-resource
		  Language Indonesian from Scratch},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3575803},
  doi		= {10.1145/3575803},
  abstract	= {Using off-the-shelf resources from resource-rich languages
		  to transfer knowledge to low-resource languages has
		  received a lot of attention. The requirements of enabling
		  the model to achieve the reliable performance, including
		  the scale of required annotated data and the effective
		  framework, are not well guided. To address the first
		  question, we empirically investigate the cost-effectiveness
		  of several methods for training intent classification and
		  slot-filling models from scratch in Indonesia (ID) using
		  English data. Confronting the second challenge, we propose
		  a Bi-Confidence-Frequency Cross-Lingual transfer framework
		  (BiCF), which consists of “BiCF Mixing”, “Latent
		  Space Refinement” and “Joint Decoder”, respectively,
		  to overcome the lack of low-resource language dialogue
		  data. BiCF Mixing based on the word-level alignment
		  strategy generates code-mixed data by utilizing the
		  importance-frequency and translating-confidence. Moreover,
		  Latent Space Refinement trains a new dialogue understanding
		  model using code-mixed data and word embedding models.
		  Joint Decoder based on Bidirectional LSTM (BiLSTM) and
		  Conditional Random Field (CRF) is used to obtain
		  experimental results of intent classification and
		  slot-filling. We also release a large-scale fine-labeled
		  Indonesia dialogue dataset (ID-WOZ1) and ID-BERT for
		  experiments. BiCF achieves 93.56\% and 85.17\% (F1 score)
		  on intent classification and slot filling, respectively.
		  Extensive experiments demonstrate that our framework
		  performs reliably and cost-efficiently on different scales
		  of manually annotated Indonesian data.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {105},
  numpages	= {20},
  keywords	= {Dialogue datasets, intent classification, slot-filling,
		  indonesian}
}

@InProceedings{	  10.1145/3709025.3712213,
  author	= {Sreedharan, Sreekant and Akda\u{g}, Melih and
		  Ramachandran, Muthu and R\o{}seag, Erik and Rokseth,
		  B\o{}rge},
  title		= {Legata - A domain language for maritime regulatory
		  compliance},
  year		= {2025},
  isbn		= {9798400714214},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3709025.3712213},
  doi		= {10.1145/3709025.3712213},
  abstract	= {The paper addresses the challenge of ensuring that
		  increasingly powerful autonomous maritime vessels operate
		  safely and conform to regulatory standards. We presents
		  Legata, a domain language designed to ensure regulatory
		  compliance in autonomous maritime vessels. By leveraging
		  large-scale simulations, Legata translates legal
		  regulations into computable terms, enabling precise
		  evaluation of vessel behavior across diverse scenarios. The
		  framework quantifies risk based on regulatory violations,
		  providing a structured method for assessing compliance. A
		  case study on the Istanbul Strait demonstrates Legata's
		  practical application.},
  booktitle	= {Proceedings of the 2025 Symposium on Computer Science and
		  Law},
  pages		= {89–107},
  numpages	= {19},
  keywords	= {AI \&amp; Law, Autonomous Vessels, Maritime Regulations,
		  Risk Evaluation, Safety Assurance},
  location	= {Munich, Germany},
  series	= {CSLAW '25}
}

@InProceedings{	  10.1145/3460620.3460631,
  author	= {Yelibayeva, Gaziza and Sharipbay, Altynbek and Bekmanova,
		  Gulmira and Omarbekova, Assel},
  title		= {Ontology-Based Extraction of Kazakh Language Word
		  Combinations in Natural Language Processing},
  year		= {2021},
  isbn		= {9781450388382},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460620.3460631},
  doi		= {10.1145/3460620.3460631},
  abstract	= {This article provides an ontological model of nominative
		  word combinations in the Kazakh language. It is necessary
		  for creation of the automated templates for search of
		  nominative word combinations of the Kazakh language in text
		  corpora. The presented model expands the theory of applied
		  linguistics in the field of extracting information from the
		  text during corpus studies. The results will be used in
		  semantic searches, Q&amp;A systems and in the development
		  of software applications for obtaining knowledge, as well
		  as for training and evaluation of knowledge on the syntax
		  of the Kazakh language in the system of e-learning.},
  booktitle	= {International Conference on Data Science, E-Learning and
		  Information Systems 2021},
  pages		= {58–59},
  numpages	= {2},
  location	= {Ma'an, Jordan},
  series	= {DATA'21}
}

@InProceedings{	  10.1145/3613905.3651051,
  author	= {Moore, Nicole and Amith, Muhammad and Neumann, Ana and
		  Hamilton, Jane and Tang, Lu and Savas, Lara and Tao, Cui},
  title		= {Translating motivational interviewing for the HPV vaccine
		  into a computable ontology model for automated AI
		  conversational interaction},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3651051},
  doi		= {10.1145/3613905.3651051},
  abstract	= {Human papillomavirus (HPV) vaccinations are lower than
		  expected. To protect the onset of head and neck cancers,
		  innovative strategies to improve the rates are needed.
		  Artificial intelligence may offer some solutions,
		  specifically conversational agents to perform counseling
		  methods. We present our efforts in developing a dialogue
		  model for automating motivational interviewing (MI) to
		  encourage HPV vaccination. We developed a formalized
		  dialogue model for MI using an existing ontology-based
		  framework to manifest a computable representation using
		  OWL2. New utterance classifications were identified along
		  with the ontology that encodes the dialogue model. Our work
		  is available on GitHub under the GPL v.3. We discuss how an
		  ontology-based model of MI can help standardize/formalize
		  MI counseling for HPV vaccine uptake. Our future steps will
		  involve assessing MI fidelity of the ontology model,
		  operationalization, and testing the dialogue model in a
		  simulation with live participants.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {341},
  numpages	= {12},
  keywords	= {cancer, chat bots, conversational agents, dialogue
		  systems, human papillomavirus, ontology, oral health,
		  patient-provider communication},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@InProceedings{	  10.1145/3696230.3696248,
  author	= {Asmawi, Adelina and Alam, Md. Saiful},
  title		= {Understanding the Digital Epistemologies of Chat GPT:
		  Towards a Decolonial Language Pedagogy},
  year		= {2024},
  isbn		= {9798400717574},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696230.3696248},
  doi		= {10.1145/3696230.3696248},
  abstract	= {Since its emergence, research around Chat GPT and language
		  teaching has trended into an asymmetry of opportunities and
		  challenges from both utopian and dystopian perspectives.
		  Chat GPT has Western data-based inherent coloniality and
		  thus carries invisible colonial perpetuation when used in
		  language education. However, Chat GPT has context-awareness
		  and personalization capacity and is open to user control.
		  Therefore, rather than decolonizing Chat GPT itself,
		  decolonizing by Chat GPT can be a flipped approach to
		  materialize decolonial persuasion in language pedagogy.
		  Grounded in Santos's epistemology of the south, this paper
		  attempts to conceptualize Chat GPT-assisted decolonial
		  pedagogy. Using the authors’ constructivist ideation, the
		  study employed simulated text data generated through a
		  series of Chat GPT-author conversations. The collected data
		  were analyzed by applying the educational data mining (EDM)
		  method to support the primary conceptualization of the
		  proposed decolonial pedagogy. The findings serve as a
		  breakthrough with a novelty discovered in Chat
		  GPT-facilitated decolonization of language pedagogy
		  empowering decolonially charged educators working in the
		  global south.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Digital Technology in Education (ICDTE)},
  pages		= {277–283},
  numpages	= {7},
  keywords	= {AI, Chat GPT, Chat GPT Epistemology, Decolonizing ELT,
		  Decolonizing Education},
  location	= { },
  series	= {ICDTE '24}
}

@InProceedings{	  10.1145/3744367.3744411,
  author	= {Yan, Chengmin},
  title		= {Innovation and Development of Chinese Language
		  International Education in the Digital Age},
  year		= {2025},
  isbn		= {9798400715068},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3744367.3744411},
  doi		= {10.1145/3744367.3744411},
  abstract	= {To study the development of China's digital applications
		  and international education, modern technological methods,
		  innovative education models and challenges were analysed.
		  The solution explains several examples of key technology
		  applications such as AI language assessment, internal
		  learning and blockchain, and shows that integrating
		  technology into an educational innovation in China offers
		  huge opportunities, but the supply of resources needs to be
		  further improved. Technical adaptation and ethical
		  issues.},
  booktitle	= {Proceedings of the 2025 International Conference on
		  Artificial Intelligence and Educational Systems},
  pages		= {272–277},
  numpages	= {6},
  keywords	= {Digital education, Chinese language international
		  education, technological empowerment, innovative teaching
		  and learning, ethical risks},
  location	= { },
  series	= {ICAIES '25}
}

@InProceedings{	  10.1145/3637528.3671742,
  author	= {Jiang, Wenyuan and Wu, Wenwei and Zhang, Le and Yuan,
		  Zixuan and Xiang, Jian and Zhou, Jingbo and Xiong, Hui},
  title		= {Killing Two Birds with One Stone: Cross-modal Reinforced
		  Prompting for Graph and Language Tasks},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671742},
  doi		= {10.1145/3637528.3671742},
  abstract	= {In recent years, Graph Neural Networks (GNNs) and Large
		  Language Models (LLMs) have exhibited remarkable capability
		  in addressing different graph learning and natural language
		  tasks, respectively. Motivated by this, integrating LLMs
		  with GNNs has been increasingly studied to acquire
		  transferable knowledge across modalities, which leads to
		  improved empirical performance in language and graph
		  domains. However, existing studies mainly focused on a
		  single-domain scenario by designing complicated integration
		  techniques to manage multimodal data effectively.
		  Therefore, a concise and generic learning framework for
		  multi-domain tasks, i.e., graph and language domains, is
		  highly desired yet remains under-exploited due to two major
		  challenges. First, the language corpus of downstream tasks
		  differs significantly from graph data, making it hard to
		  bridge the knowledge gap between modalities. Second, not
		  all knowledge demonstrates immediate benefits for
		  downstream tasks, potentially introducing disruptive noise
		  to context-sensitive models like LLMs. To tackle these
		  challenges, we propose a novel plug-and-play framework for
		  incorporating a lightweight cross-domain prompting method
		  into both language and graph learning tasks. Specifically,
		  we first convert the textual input into a domain-scalable
		  prompt, which not only preserves the semantic and logical
		  contents of the textual input, but also highlights related
		  graph information as external knowledge for different
		  domains. Then, we develop a reinforcement learning-based
		  method to learn the optimal edge selection strategy for
		  useful knowledge extraction, which profoundly sharpens the
		  multi-domain model capabilities. In addition, we introduce
		  a joint multi-view optimization module to regularize
		  agent-level collaborative learning across two domains.
		  Finally, extensive empirical justifications over 23 public
		  and synthetic datasets demonstrate that our approach can be
		  applied to diverse multi-domain tasks more accurately,
		  robustly, and reasonably, and improve the performances of
		  the state-of-the-art graph and language models in different
		  learning paradigms.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1301–1312},
  numpages	= {12},
  keywords	= {graph neural networks, large language models, prompt
		  learning, reinforcement learning},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3706598.3714073,
  author	= {Singh, Divyanshu Kumar and Das, Dipto and Semaan, Bryan},
  title		= {The Power of Language: Resisting Western Heteropatriarchal
		  Normative Writing Standards},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3714073},
  doi		= {10.1145/3706598.3714073},
  abstract	= {Language is more than communication; it is a form of
		  power. Whereas science has been scrutinized for privileging
		  Western values and norms, what has been less explored is
		  scientific linguistic performance (e.g. writing). The
		  enforcement of English as the “normative standard” has
		  prioritized hegemonic values and assumptions, thereby
		  shaping the expectations of scientific performance.
		  HCI/CSCW is dominated by heteropatriarchal Western
		  practices, overlooking entangled values and assumptions
		  impacting non-Western colleagues. Our work presents a
		  design fiction (fictitious case study) envisioning a
		  research contribution which embodies non-Western linguistic
		  nuances as an alternative “normative standard” for
		  scientific communication. Through this work, not only are
		  we championing care in developing responsible linguistic
		  practices in HCI/CSCW, but also epistemically challenging
		  readers with intentional confusion. We establish a call to
		  action for acknowledging and embracing different writing
		  practices that are more inclusive of the diverse
		  representation of scholars in HCI/CSCW.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {491},
  numpages	= {17},
  keywords	= {Language, Design Fiction, Coloniality, Human-Computer
		  Interaction, Decolonization, Feminism, People of Color,
		  Power, Justice},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3391274.3393639,
  author	= {Erekhinskaya, Tatiana and Strebkov, Dmitriy and Patel,
		  Sujal and Balakrishna, Mithun and Tatu, Marta and Moldovan,
		  Dan},
  title		= {Ten ways of leveraging ontologies for natural language
		  processing and its enterprise applications},
  year		= {2020},
  isbn		= {9781450379748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3391274.3393639},
  doi		= {10.1145/3391274.3393639},
  abstract	= {In the last years, Artificial Intelligence and Deep
		  Learning have matured from a facinating research area to
		  real-word applications across multiple domains. Enterprises
		  adopt data-driven approaches for various use cases. With
		  the increased adoption, such issues as governance of the
		  models, deployment, scalability, reusablity and maintenance
		  are widely addressed on the engineering side, but not so
		  much on the knowledge side. In this paper, we demonstrate
		  10 ways of leveraging ontology for Natural Language
		  Processing. Specifically, we explore the usage of
		  ontologies and related standards for labeling schema,
		  configuration, providing lexical data, powering rule engine
		  and automated generation of rules, as well as providing a
		  standard output format. Additionally, we discuss three
		  NLP-based applications: semantic search, question answering
		  and natural language querying and show how they can benefit
		  from ontology usage. The paper summarizes our experience of
		  using ontology in a number of projects for medical,
		  enterprise, financial, legal and security domains.},
  booktitle	= {Proceedings of The International Workshop on Semantic Big
		  Data},
  articleno	= {8},
  numpages	= {6},
  keywords	= {domain-specific knowledge, labeling, natural language
		  processing, natural language querying, ontologies, semantic
		  graph},
  location	= {Portland, Oregon},
  series	= {SBD '20}
}

@InProceedings{	  10.1145/3487553.3524923,
  author	= {Negreanu, Carina and Karaoglu, Alperen and Williams, Jack
		  and Chen, Shuang and Fabian, Daniel and Gordon, Andrew and
		  Lin, Chin-Yew},
  title		= {Rows from Many Sources: Enriching row completions from
		  Wikidata with a pre-trained Language Model},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524923},
  doi		= {10.1145/3487553.3524923},
  abstract	= {Row completion is the task of augmenting a given table of
		  text and numbers with additional, relevant rows. The task
		  divides into two steps: subject suggestion, the task of
		  populating the main column; and gap filling, the task of
		  populating the remaining columns. We present
		  state-of-the-art results for subject suggestion and gap
		  filling measured on a standard benchmark (WikiTables). Our
		  idea is to solve this task by harmoniously combining
		  knowledge base table interpretation and free text
		  generation. We interpret the table using the knowledge base
		  to suggest new rows and generate metadata like headers
		  through property linking. To improve candidate diversity,
		  we synthesize additional rows using free text generation
		  via GPT-3, and crucially, we exploit the metadata we
		  interpret to produce better prompts for text generation.
		  Finally, we verify that the additional synthesized content
		  can be linked to the knowledge base or a trusted web source
		  such as Wikipedia.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {1272–1280},
  numpages	= {9},
  keywords	= {free text generation, knowledge base linking, language
		  models, natural language applications, semantic knowledge,
		  tabular data},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3653081.3653117,
  author	= {Xing, Xueyang and Jia, Bo and Huang, Zhicheng and Chen,
		  Yongzhi and Wang, Junjie and Fan, Anfei and Chen, Xin and
		  Cao, Lei},
  title		= {A fusion inference method for large language models and
		  knowledge graphs based on structured injection and causal
		  inference},
  year		= {2024},
  isbn		= {9798400716485},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3653081.3653117},
  doi		= {10.1145/3653081.3653117},
  abstract	= {In this paper, we propose a large language model and
		  knowledge graph fusion reasoning method based on structured
		  injection and causal reasoning (LKFSC) to address the
		  limitations of existing large language models and knowledge
		  graphs in practical applications. The approach effectively
		  mitigates the problems of long-distance dependency and
		  limited contextual information, and improves the reasoning
		  capability of the large language model. Meanwhile, by
		  fusing the generative ability of the large language model
		  and the inference ability of the knowledge graph, the
		  method realizes intelligent reasoning for complex problems.
		  The main contributions of this paper include proposing a
		  structured injection method that introduces causality for
		  reasoning, and constructing a fusion reasoning framework
		  that effectively mitigates the illusory problem of large
		  language models and provides powerful and intelligent
		  decision support for practical applications.},
  booktitle	= {Proceedings of the 2023 5th International Conference on
		  Internet of Things, Automation and Artificial
		  Intelligence},
  pages		= {208–213},
  numpages	= {6},
  location	= {Nanchang, China},
  series	= {IoTAAI '23}
}

@InProceedings{	  10.1145/3717934.3718013,
  author	= {Wang, Gang and Wang, He and Xu, Min and Zhou, Aihua and
		  Yu, Hai and Qian, Zhonghao},
  title		= {Multi Domain Ontology Model Fusion and Interoperability
		  Method of Digital Twin in Distribution Network},
  year		= {2025},
  isbn		= {9798400707087},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3717934.3718013},
  doi		= {10.1145/3717934.3718013},
  abstract	= {The digital twin of distribution network covers models
		  including topology, geography, space, production,
		  operation, control, measurement, marketing, electric field,
		  etc. There is no unified rule framework for multi domain
		  ontology modeling methods, and there is an urgent need for
		  model fusion and comprehensive utilization. This article
		  focuses on the cross domain joint modeling problem of
		  distribution network ontology, conducts research on the
		  fusion method of multi domain ontology models of
		  distribution network digital twins, proposes a multi type
		  model resource fusion and comprehensive utilization method
		  with equipment and facilities as the core, integrates
		  business models and power grid models, studies multi domain
		  ontology model fusion interoperability technology for
		  digital twins, and realizes the comprehensive utilization
		  of distribution network digital twin model resources.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Information Technologies and Electrical Engineering},
  pages		= {516–522},
  numpages	= {7},
  keywords	= {Digital twin, Distribution network, Model fusion, Multi
		  domain ontology, Operation method},
  location	= { },
  series	= {ICITEE '24}
}

@InProceedings{	  10.1145/3701716.3715194,
  author	= {Frey, Johannes and Ferraz, Lucas and Hofer, Marvin},
  title		= {POTS - A Polyparadigmatic Ontology Term Search with
		  Fine-Grained Context Steering using Hyper-Level Vector
		  Spaces},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715194},
  doi		= {10.1145/3701716.3715194},
  abstract	= {We present a novel microservice-based system, that
		  facilitates a polyparadigmatic ontology term search
		  (leveraging semantic search via vector embeddings, keyword
		  search, and attribute filters). The search index strategy
		  intends to preserve important semantic aspects of the
		  ontological context of a term (selected attributes and term
		  relationships) using structured search fields and
		  multilevel vector spaces assembling hyper-level vector
		  spaces. The flexible, yet simple query API allows
		  fine-grained search requests based on a combination of
		  fuzzy and exact filters. The architecture is based on a
		  highly automatable and flexible Docker Compose setup
		  strategy. While deploying the system for a local ontology
		  is only one command away, the setup also allows ingesting a
		  configurable subset of over 1,800 published ontologies in
		  over 12,000 versions via DBpedia Archivo.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2831–2834},
  numpages	= {4},
  keywords	= {graph retrieval augmented generation, llms, ontology,
		  ontology retrieval, ontology terms embedding, owl, semantic
		  search, terminology lookup service},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@Proceedings{	  10.1145/3732771,
  title		= {SLE '25: Proceedings of the 18th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  year		= {2025},
  isbn		= {9798400718847},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Koblenz, Germany}
}

@InProceedings{	  10.5555/3586210.3586396,
  author	= {Shuttleworth, David and Padilla, Jose},
  title		= {From Narratives to Conceptual Models via Natural Language
		  Processing},
  year		= {2023},
  publisher	= {IEEE Press},
  abstract	= {This paper explores the use of natural language processing
		  (NLP) towards the semi-automatic generation of conceptual
		  models, and eventual simulation specifications, from
		  descriptions of a phenomenon. Narratives describing the
		  problem are transformed into a list of concepts and
		  relationships and visualized using a network graph. The
		  process relies on pattern-based grammatical rules and an
		  NLP dependency parser identifying important concept types,
		  namely actors, factors, and mechanisms. We use three
		  conceptualizations, created by potential users, to
		  understand how the NLP-generated model should and could be
		  adjusted. The objective of the research is to develop
		  potential standard approaches users can use to generate
		  conceptual models; develop a conceptual modeling assistant
		  that subject matter experts can use to make them
		  participant in the simulation creation process; and to
		  identify how narratives should be written so an NLP-based
		  conceptual modeling assistant may provide a thorough
		  description of a phenomenon.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2222–2233},
  numpages	= {12},
  location	= {Singapore, Singapore},
  series	= {WSC '22}
}

@InProceedings{	  10.1145/3733155.3733192,
  author	= {Pinna, Simone and Massa, Silvia Maria and Fenu, Matteo and
		  Casti, Giulio and Riboni, Daniele},
  title		= {Integration of Retrieval-Augmented Generation Technique
		  for LLM-based Differential Diagnosis Assistant},
  year		= {2025},
  isbn		= {9798400714023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3733155.3733192},
  doi		= {10.1145/3733155.3733192},
  abstract	= {Artificial Intelligence (AI) is increasingly transforming
		  the medical field, offering significant potential for
		  diagnosis, treatment, and patient care. However, its
		  successful integration relies on healthcare professionals,
		  such as doctors, psychologists, and nurses, trusting the
		  technology’s reliability and accuracy. For Large Language
		  Models (LLMs), this trust requires transparent, verifiable,
		  and rigorously reviewed information sources. This paper
		  presents an AI-powered tool for differential diagnosis and
		  disease comparison, utilizing an LLM enhanced by
		  Retrieval-Augmented Generation (RAG). RAG overcomes
		  traditional LLM limitations by enabling access to external,
		  domain-specific knowledge, ensuring accurate and
		  contextually relevant responses. The system leverages
		  PubMed, a biomedical article aggregator, to extract
		  symptom-related information from scientific literature on
		  various disorders. Evaluations involving
		  psychologist-administered questionnaires demonstrate that
		  combining a similarity score with detailed symptom
		  descriptions provides a clear understanding of
		  relationships between disorders. This approach may enhance
		  diagnostic precision and build trust in AI-driven tools,
		  encouraging their broader adoption in clinical practice.},
  booktitle	= {Proceedings of the 18th ACM International Conference on
		  PErvasive Technologies Related to Assistive Environments},
  pages		= {277–284},
  numpages	= {8},
  keywords	= {Large Language Models, Retrieval-Augmented Generation,
		  e-Health, Differential diagnosis},
  location	= { },
  series	= {PETRA '25}
}

@InProceedings{	  10.1145/3436829.3436833,
  author	= {Negm, Eman and Makady, Soha and Salah, Akram},
  title		= {Towards Ontology-based Domain Specific Language for
		  Internet of Things},
  year		= {2021},
  isbn		= {9781450377218},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3436829.3436833},
  doi		= {10.1145/3436829.3436833},
  abstract	= {Development of Internet of Things (IoT) applications is
		  considered as a complex task. It requires knowledge in the
		  different software layers starting from the low level
		  perception layer to the high level application layer. The
		  domain expert should be involved from the start of the
		  project to its end, to ensure that the delivered system
		  satisfies the user needs. Such involvement results from the
		  continuous need for the domain knowledge throughout the
		  software development lifecycle. Such long development time
		  along with the high cost of IoT applications, cause a slow
		  progress in the IoT development. In this paper, a Domain
		  Specific Language (DSL), called OntIoT, is proposed that
		  contributes in reducing the complexity of IoT application
		  development through providing the needed domain knowledge
		  in an automated manner. OntIoT is an ontology-based DSL
		  that utilizes the Semantic Sensor Network (SSN) ontology to
		  catch the IoT domain concepts and constraints.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Software and Information Engineering},
  pages		= {146–151},
  numpages	= {6},
  keywords	= {Domain Specific Language, Internet of Things, Ontology},
  location	= {Cairo, Egypt},
  series	= {ICSIE '20}
}

@InProceedings{	  10.1145/3652620.3687805,
  author	= {Netz, Lukas and Reimer, Jan and Rumpe, Bernhard},
  title		= {Using Grammar Masking to Ensure Syntactic Validity in
		  LLM-based Modeling Tasks},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687805},
  doi		= {10.1145/3652620.3687805},
  abstract	= {Low-code development platforms (LCDPs) are becoming
		  increasingly important in industry, which confronts us in
		  academic teaching with the challenge of educating students
		  in the basic principles, critical engagement, and
		  evaluation of LCDPs. This leads us to the question, how to
		  teach the usage of different LCDPs during an university
		  course. The short time frame of university-level courses
		  makes it challenging to teach more than only one LCDP. In
		  our teaching approach, students use two different LCDPs and
		  create a web-application with both of them. Firstly, we
		  require the students to define a target application with
		  common modeling languages, next they use the first LCDP, at
		  about half the time they switch to the second LCDP and
		  present their findings of the differences in methodology
		  and development processes at the end. We discuss this
		  approach, show survey results from the participants, and
		  explain lessons learned. This concept allows students
		  critical engagement with LCDPs and model-driven software
		  engineering. Supervisors get an insight into the
		  learnability of each LCDP and how novices adapt to
		  different domain-specific languages and their notations.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {115–122},
  numpages	= {8},
  keywords	= {low-code development platforms, education,
		  university-level courses, model-driven software
		  engineering, problem-based learning},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.1109/tcbb.2023.3248797,
  author	= {Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
  title		= {Prediction of Protein-Protein Interactions Using Vision
		  Transformer and Language Model},
  year		= {2023},
  issue_date	= {Sept.-Oct. 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {5},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2023.3248797},
  doi		= {10.1109/TCBB.2023.3248797},
  abstract	= {The knowledge of protein-protein interaction (PPI) helps
		  us to understand proteins’ functions, the causes and
		  growth of several diseases, and can aid in designing new
		  drugs. The majority of existing PPI research has relied
		  mainly on sequence-based approaches. With the availability
		  of multi-omics datasets (sequence, 3D structure) and
		  advancements in deep learning techniques, it is feasible to
		  develop a deep multi-modal framework that fuses the
		  features learned from different sources of information to
		  predict PPI. In this work, we propose a multi-modal
		  approach utilizing protein sequence and 3D structure. To
		  extract features from the 3D structure of proteins, we use
		  a pre-trained vision transformer model that has been
		  fine-tuned on the structural representation of proteins.
		  The protein sequence is encoded into a feature vector using
		  a pre-trained language model. The feature vectors extracted
		  from the two modalities are fused and then fed to the
		  neural network classifier to predict the protein
		  interactions. To showcase the effectiveness of the proposed
		  methodology, we conduct experiments on two popular PPI
		  datasets, namely, the human dataset and the
		  &lt;italic&gt;S. cerevisiae&lt;/italic&gt; dataset. Our
		  approach outperforms the existing methodologies to predict
		  PPI, including multi-modal approaches. We also evaluate the
		  contributions of each modality by designing uni-modal
		  baselines. We perform experiments with three modalities as
		  well, having gene ontology as the third modality.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= feb,
  pages		= {3215–3225},
  numpages	= {11}
}

@Article{	  10.1145/3606706,
  author	= {Mikhaylova, Daria and Metilli, Daniele},
  title		= {Extending RiC-O to Model Historical Architectural
		  Archives: The ITDT Ontology},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3606706},
  doi		= {10.1145/3606706},
  abstract	= {Historical architectural archives enjoy attention from
		  diverse audiences, acting as a primary source of
		  information for architects, historians, public authorities,
		  and common citizens alike. In Italy, the interest in
		  architectural archives has grown slowly but steadily for
		  the last 20 years. However, architectural archives do not
		  generally follow the trend common for museums and galleries
		  in publishing digitized materials and providing standard
		  metadata for individual records. The information that is
		  available online usually includes only an archival finding
		  aid, instead of metadata about the individual records, or
		  fully digital versions of the records. While cataloguing
		  standards for archival descriptions of architectural
		  records have existed at least since the 1980s, the rise of
		  Linked Open Data as a framework for publishing cultural
		  heritage data has allowed archivists to enhance these
		  archival descriptions with richer contextual information
		  and links to external knowledge bases. In this paper we
		  present the ITDT ontology, an extension of the Records in
		  Contexts Ontology that facilitates the representation of
		  architectural records and of the context related to
		  architectural projects, its process, and participating
		  entities. We discuss the application of the ontology to the
		  project files of Italian architect and engineer Dino
		  Tamburini (1924–2011), and the creation of a digital
		  archive offering multiple perspectives over the records.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {67},
  numpages	= {15},
  keywords	= {Historical archives, ontology, semantic annotations}
}

@InProceedings{	  10.1145/3297001.3297059,
  author	= {Deshpande, Ameet and Jegadeesan, Monisha},
  title		= {Leveraging Ontological Knowledge for Neural Language
		  Models},
  year		= {2019},
  isbn		= {9781450362078},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297001.3297059},
  doi		= {10.1145/3297001.3297059},
  abstract	= {Neural Language Models such as Word2Vec and GloVe have
		  been shown to encode semantic relatedness between words.
		  Improvements in unearthing these embeddings can ameliorate
		  performance in numerous downstream applications such as
		  sentiment analysis, question answering, and dialogue
		  generation. Lexical ontologies such as WordNet are known to
		  supply information about semantic similarity rather than
		  relatedness. Further, extracting word em-beddings from
		  small corpora is daunting for data-hungry neural networks.
		  This work shows how methods that conflate Word2Vec and
		  Ontologies can achieve better performance, reduce training
		  time and help adapt to domains with a minimum amount of
		  data.},
  booktitle	= {Proceedings of the ACM India Joint International
		  Conference on Data Science and Management of Data},
  pages		= {350–353},
  numpages	= {4},
  keywords	= {Domain-transfer, Hierarchy, Ontology, Word Vectors},
  location	= {Kolkata, India},
  series	= {CODS-COMAD '19}
}

@Proceedings{	  10.1145/3687997,
  title		= {SLE '24: Proceedings of the 17th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  year		= {2024},
  isbn		= {9798400711800},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 17th ACM SIGPLAN International Conference
		  on Software Language Engineering (SLE), held in Pasadena,
		  California, USA, October 20–21 2024, as part of SPLASH
		  2024. The SLE conference is devoted to the principles of
		  software languages: their design, their implementation, and
		  their evolution.},
  location	= {Pasadena, CA, USA}
}

@InProceedings{	  10.1145/3696410.3714672,
  author	= {Yang, Hui and Chen, Jiaoyan and Sattler, Uli},
  title		= {TransBox: EL++-closed Ontology Embedding},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696410.3714672},
  doi		= {10.1145/3696410.3714672},
  abstract	= {OWL (Web Ontology Language) ontologies, which are able to
		  represent both relational and type facts as standard
		  knowledge graphs and complex domain knowledge in
		  Description Logic (DL) axioms, are widely adopted in
		  domains such as healthcare and bioinformatics. Inspired by
		  the success of knowledge graph embeddings, embedding OWL
		  ontologies has gained significant attention in recent
		  years. Current methods primarily focus on learning
		  embeddings for atomic concepts and roles, enabling the
		  evaluation based on normalized axioms through specially
		  designed score functions. However, they often neglect the
		  embedding of complex concepts, making it difficult to infer
		  with more intricate axioms. This limitation reduces their
		  effectiveness in advanced reasoning tasks, such as Ontology
		  Learning and ontology-mediated Query Answering. In this
		  paper, we propose EL++-closed ontology embeddings which are
		  able to represent any logical expressions in DL EL++ via
		  composition. Furthermore, we develop TransBox, an effective
		  EL++ -closed ontology embedding method that can handle
		  many-to-one, one-to-many and many-to-many relations. Our
		  extensive experiments demonstrate that TransBox often
		  achieves state-of-the-art performance across various
		  real-world datasets for predicting complex axioms.},
  booktitle	= {Proceedings of the ACM on Web Conference 2025},
  pages		= {22–34},
  numpages	= {13},
  keywords	= {description logic, ontology completion, ontology
		  embedding, ontology learning, web ontology language},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3679431.3679514,
  author	= {Gao, Xiang and Zhou, Yuanchao},
  title		= {Exploring Computational Visual Interfaces for Artificial
		  Intelligence Language Modeling User Experience among
		  College Students: A Rooted Theoretical Approach},
  year		= {2024},
  isbn		= {9798400709951},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3679431.3679514},
  doi		= {10.1145/3679431.3679514},
  abstract	= {Artificial Intelligence (AI) is an emerging technology
		  with the aim of developing intelligent applications that
		  have broad applications in various fields such as
		  healthcare, education, and design. AI design research is
		  presently in an exploratory phase, yet its influence on
		  computer vision interfaces for subjective user experience
		  is becoming increasingly significant. Focused on Chinese
		  university students, the research delves into AI user
		  experience, emphasizing NLP, HCI, and SU. Data was
		  collected via surveys and interviews, with deep learning
		  techniques aiding data processing. A substantial volume of
		  user data was gathered through user surveys and in-depth
		  interviews, with deep learning techniques employed for data
		  preprocessing and feature extraction. Results show a
		  preference for personalized services and data mining in
		  interface design, while technical features and operational
		  fluency are priorities in programming. Enhancing HCI design
		  can improve operational efficiency and meet individual
		  needs, bolstered by clear visual interfaces and HCI
		  technologies, thus enhancing overall user experience
		  quality and effectiveness.},
  booktitle	= {Proceedings of the 2024 3rd International Symposium on
		  Control Engineering and Robotics},
  pages		= {516–522},
  numpages	= {7},
  location	= {Changsha, China},
  series	= {ISCER '24}
}

@Article{	  10.1145/3664609,
  author	= {Bhagwat, Suvarna Rajesh. and Bhavsar, R. P. and Pawar, B.
		  V.},
  title		= {Marathi to Indian Sign Language Machine Translation},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3664609},
  doi		= {10.1145/3664609},
  abstract	= {Machine translation has been a prominent field of
		  research, contributing significantly to human life
		  enhancement. Sign language machine translation, a subfield,
		  focuses on translating spoken language content into sign
		  language and vice versa, thereby facilitating communication
		  between the normal hearing and hard-of-hearing communities,
		  promoting inclusivity.This study presents the development
		  of a ‘sign language machine translation system’
		  converting simple Marathi sentences into Indian Sign
		  Language (ISL) glosses and animation. Given the
		  low-resource nature of both languages, a phrase-level
		  rule-based approach was employed for the translation.
		  Initial encoding of translation rules relied on basic
		  linguistic knowledge of Marathi and ISL, with subsequent
		  incorporation of rules to address 'simultaneous
		  morphological' features in ISL. These rules were applied
		  during the ‘generation phase’ of translation to
		  dynamically adjust phonological sign parameters, resulting
		  in improved target sentence fluency.The paper provides a
		  detailed description of the system architecture,
		  translation rules, and comprehensive experimentation.
		  Rigorous evaluation efforts were undertaken, encompassing
		  various linguistic features, and the findings are discussed
		  herein.The web-based version of the system serves as an
		  interpreter for brief communications and can support the
		  teaching and learning of sign language and its grammar in
		  schools for hard-of-hearing students.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  keywords	= {Marathi, Indian Sign Language, Phrase-level Translation,
		  Rule-based Translation, Inclusion of specially-abled
		  community, Sign languages’ simultaneous morphological
		  features}
}

@Article{	  10.1145/3593804,
  author	= {Huang, Tao and Hu, Shengze and Lin, Keke and Yang, Huali
		  and Zhang, Hao and Song, Houbing and Lv, Zhihan},
  title		= {Sequence Generation Model Integrating Domain Ontology for
		  Mathematical question tagging},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3593804},
  doi		= {10.1145/3593804},
  abstract	= {In online learning systems, tagging knowledge points for
		  questions is a fundamental task. Automatic tagging
		  technology uses intelligent algorithms to automatically tag
		  knowledge points for questions to reduce manpower and time
		  costs. However, the current knowledge point tagging
		  technology cannot satisfy the situation that mathematics
		  questions often involve a variable number of knowledge
		  points, lacks the consideration of the characteristics of
		  the mathematics field, and ignores the internal connection
		  between knowledge points. To address the above issues, we
		  propose a Sequence Generation Model Integrating Domain
		  Ontology for Mathematical question tagging (SOMPT). SOMPT
		  performs data augmentation for text and then obtains
		  intermediate text based on domain ontology replacement to
		  facilitate deep learning model to understand mathematical
		  question text. SOMPT is able to obtain dynamic word vector
		  embedding to optimize the textual representation for math
		  questions. What’s more, our model can capture the
		  relationship between tags to generate knowledge points more
		  accurately in the way of sequence generation. The
		  comparative experimental results show that our proposed
		  model has an excellent tagging ability for mathematical
		  questions. Moreover, the sequence generation module in
		  SOMPT can be applied on other multi-label classification
		  tasks and be on par with the state-of-the-art performance
		  models.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  keywords	= {Mathematical question tagging, Deep learning, Language
		  models, Sequence generation}
}

@Article{	  10.1145/3687306,
  author	= {Wu, Xiaobing},
  title		= {Unveiling Transformative Insights via Cross-Modal Learning
		  and Natural Language Processing for Enhanced Supply Chain
		  Intelligence},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3687306},
  doi		= {10.1145/3687306},
  abstract	= {This day's quickly developing business landscape, supply
		  chains have become more globalized, intricate, and
		  multi-covering, making them crucial for companies to
		  navigate through disruptions and unpredictability. The
		  major which are addressed in the supply chain process are
		  lack of transparency and visibility of the supply chain
		  network and that's leads to delay and inefficiency in the
		  process. In order to overcome those drawbacks in the supply
		  chain process, in this article an enhanced supply chain
		  intelligence is developed which performs Unveiling
		  Transformative Insights using the learning process like
		  Cross-Modal Learning (CML) and Natural Language Processing
		  (NLP). The implementation of these techniques is carried
		  out in the software Python. This analysis consists of
		  certain calculation called enhanced supply chain analysis,
		  sales revenue Vs SKU analysis, various modes cost analysis,
		  Lead time vs different supplier and location. The
		  comparative analysis is performed among the technique like
		  RF regression, SARIMA-LSTM-BP and BiLSTM model. The
		  parameters which are involved in this performance analysis
		  are MAE, MSE, RMSE and R^2.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= sep,
  keywords	= {Enhanced Supply Chain Intelligence, Unveiling
		  Transformative Insights, Cross-Modal Learning (CML) and
		  Natural Language Processing (NLP)}
}

@Article{	  10.1145/3709727,
  author	= {Luoma, Kyle and Kumar, Arun},
  title		= {SNAILS: Schema Naming Assessments for Improved LLM-Based
		  SQL Inference},
  year		= {2025},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {1},
  url		= {https://doi.org/10.1145/3709727},
  doi		= {10.1145/3709727},
  abstract	= {Large Language Models (LLMs) have revolutionized Natural
		  Language to SQL (NL-to-SQL), dominating most NL-to-SQL
		  benchmarks. But LLMs still face limitations due to
		  hallucinations, semantic ambiguity, and lexical mismatches
		  between an NL query and the database schema. Naturally, a
		  lot of work in the ML+DB intersection aims to mitigate such
		  LLM limitations. In this work, we shine the light on a
		  complementary data-centric question: How should DB schemas
		  evolve in this era of LLMs to boost NL-to-SQL? The
		  intuition is that more NL-friendly schema identifiers can
		  help LLMs work better with DBs. We dive deeper into this
		  seemingly obvious, but hitherto underexplored and
		  important, connection between schema identifier
		  ''naturalness'' and the behavior of LLM-based NL-to-SQL by
		  creating a new integrated benchmark suite we call SNAILS.
		  SNAILS has 4 novel artifacts: (1) A collection of
		  real-world DB schemas not present in prior NL-to-SQL
		  benchmarks; (2) A set of labeled NL-SQL query pairs on our
		  collection not seen before by public LLMs; (3) A notion of
		  naturalness level for schema identifiers and a novel
		  labeled dataset of modified identifiers; and (4) AI
		  artifacts to automatically modify identifier naturalness.
		  Using SNAILS, we perform a comprehensive empirical
		  evaluation of the impact of schema naturalness on LLM-based
		  NL-to-SQL accuracy, and present a method for improving
		  LLM-based NL-to-SQL with natural views. Our results reveal
		  statistically significant correlations across multiple
		  public LLMs from OpenAI, Meta, and Google on multiple
		  databases using both zero-shot prompting as well as more
		  complex NL-to-SQL workflows: DIN SQL, and CodeS. We present
		  several fine-grained insights and discuss pathways for DB
		  practitioners to better exploit LLMs for NL-to-SQL.},
  journal	= {Proc. ACM Manag. Data},
  month		= feb,
  articleno	= {77},
  numpages	= {26},
  keywords	= {benchmark, database, llm, natural language to sql,
		  relational database schema, schema design, schema linking,
		  schema naturalness}
}

@Article{	  10.1007/s00165-021-00554-3,
  author	= {de Lara, Juan and Guerra, Esther},
  title		= {Language Family Engineering with Product Lines of
		  Multi-level Models},
  year		= {2021},
  issue_date	= {Dec 2021},
  publisher	= {Springer-Verlag},
  address	= {Berlin, Heidelberg},
  volume	= {33},
  number	= {6},
  issn		= {0934-5043},
  url		= {https://doi.org/10.1007/s00165-021-00554-3},
  doi		= {10.1007/s00165-021-00554-3},
  abstract	= {Modelling is an essential activity in software
		  engineering. It typically involves two meta-levels: one
		  includes meta-models that describe modelling languages, and
		  the other contains models built by instantiating those
		  meta-models. Multi-level modelling generalizes this
		  approach by allowing models to span an arbitrary number of
		  meta-levels. A scenario that profits from multi-level
		  modelling is the definition of language families that can
		  be specialized (e.g., for different domains) by successive
		  refinements at subsequent meta-levels, hence promoting
		  language reuse. This enables an open set of variability
		  options given by all possible specializations of the
		  language family. However, multi-level modelling lacks the
		  ability to express closed variability regarding the
		  availability of language primitives or the possibility to
		  opt between alternative primitive realizations. This limits
		  the reuse opportunities of a language family. To improve
		  this situation, we propose a novel combination of product
		  lines with multi-level modelling to cover both open and
		  closed variability. Our proposal is backed by a formal
		  theory that guarantees correctness, enables top-down and
		  bottom-up language variability design, and is implemented
		  atop the MetaDepth multi-level modelling tool.},
  journal	= {Form. Asp. Comput.},
  month		= dec,
  pages		= {1173–1208},
  numpages	= {36},
  keywords	= {Meta-modelling, Multi-level modelling, Product lines,
		  Domain-specific languages, Software language engineering,
		  MetaDepth}
}

@InProceedings{	  10.1145/3575879.3576017,
  author	= {Fitsilis, Panos and Iatrellis, Omiros and Tsoutsa,
		  Paraskevi},
  title		= {Using TOSCA language to model personalized educational
		  content: Introducing eduTOSCA},
  year		= {2023},
  isbn		= {9781450398541},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3575879.3576017},
  doi		= {10.1145/3575879.3576017},
  abstract	= {Students attending Higher Education Institutions (HEIs) of
		  Vocational Educational and Training (VET) are faced with a
		  variety of complex decisions and procedures. To provide
		  students with more sustained and personalized advising,
		  many HEIs/VETs use academic advising systems and tools as a
		  way to minimize costs and streamline their advising
		  services. Furthermore, it is quite common for educational
		  programs to include and combine educational content from
		  different educational providers, while they are managed and
		  executed on different platforms. Therefore, the ability to
		  develop conceptual models for personalized learning based
		  on educational content produced by heterogeneous
		  educational service providers is a pressing need to
		  address. A similar issue is confronted when deploying
		  applications across diverse cloud computing platforms. A
		  solution that is provided in these situations is the
		  development of specialized languages for defining the
		  topology and the orchestration of applications such as
		  TOSCA, CAMP, Open-CSA, etc. In this paper, we propose to
		  use similar conceptual models for modelling heterogeneous
		  educational offerings toward personalized learning, which
		  are presented along with the overall architecture of a
		  system, named cc-coach, able to support these concepts.
		  Further, this paper is a proposal for the standardization
		  efforts needed for creating a multi-vendor educational
		  ecosystem with diverse stakeholders, able to support
		  personalized learning at various levels.},
  booktitle	= {Proceedings of the 26th Pan-Hellenic Conference on
		  Informatics},
  pages		= {355–360},
  numpages	= {6},
  keywords	= {TOSCA, curriculum modelling, e-learning, eduTOSCA,
		  orchestration, personalized learning},
  location	= {Athens, Greece},
  series	= {PCI '22}
}

@Article{	  10.1145/3737459,
  author	= {Ghimire, Sujan and Lin, Yu-Zheng and Mamun, Muntasir and
		  Chowdhury, Muhtasim Alam and Alemi, Farhad and Cai, Shuyu
		  and Guo, Jinduo and Zhu, Mingyu and Li, Honghui and Saber
		  Latibari, Banafsheh and Rafatirad, Setareh and Satam,
		  Pratik and Salehi, Soheil},
  title		= {HWREx: AI-enabled Hardware Weakness and Risk Exploration
		  and Storytelling Framework with LLM-assisted Mitigation
		  Suggestion},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1084-4309},
  url		= {https://doi.org/10.1145/3737459},
  doi		= {10.1145/3737459},
  abstract	= {Abstract:The growing complexity of modern computing
		  frameworks has led to an increase in cybersecurity
		  vulnerabilities reported to the National Vulnerability
		  Database (NVD). Extracting meaningful trends from this vast
		  amount of unstructured data is challenging without proper
		  tools and methodologies. Existing approaches lack a
		  holistic strategy for vulnerability mitigation and
		  prediction and effective knowledge extraction from the
		  Common Weakness Enumeration (CWE), Common Vulnerability
		  Exposure (CVE), and Common Attack Pattern Enumeration and
		  Classification (CAPEC) databases. We introduce the
		  AI-enabled Hardware Weakness and Risk Exploration and
		  Storytelling Framework with LLM-assisted Mitigation
		  Suggestion (HWREx), designed to address hardware
		  vulnerabilities and IoT security. Our architecture features
		  an Ontology-driven Storytelling capability that automates
		  ontology updates to track vulnerability patterns and
		  evolution over time, while offering mitigation strategies.
		  It also clarifies the complex interrelations among CVEs,
		  CWEs, and CAPECs through interactive visual knowledge
		  graphs. Our framework achieved accuracy rates of 62\% for
		  CWE-CWE, 83\% for CWE-CVE, and 77\% for CWE-CAPEC linkage
		  predictions. These graphs are instrumental for in-depth
		  hardware weakness analysis and enable HWREx to deliver
		  comprehensive assessments and actionable mitigation
		  strategies. Additionally, HWREx utilizes Generative
		  Pre-trained Transformers (GPT) to offer tailored mitigation
		  suggestions.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Des. Autom. Electron. Syst.},
  month		= may,
  keywords	= {Hardware Security, Electronic Design Automation (EDA),
		  Ontology Learning, Large Langauge Model (LLM), Natural
		  Language Processing (NLP), National Vulnerability Database
		  (NVD), Common Vulnerability and Exposure (CVE), Common
		  Weakness Enumeration (CWE), Common Attack Pattern
		  Enumeration and Classification (CAPEC), Internet of Things
		  (IoT)}
}

@Article{	  10.1145/3658451,
  author	= {Dou, Yutao and Huang, Yuwei and Zhao, Xiongjun and Zou,
		  Haitao and Shang, Jiandong and Lu, Ying and Yang, Xiaolin
		  and Xiao, Jian and Peng, Shaoliang},
  title		= {ShennongMGS: An LLM-based Chinese Medication Guidance
		  System},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {2},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3658451},
  doi		= {10.1145/3658451},
  abstract	= {The rapidly evolving field of Large Language Models (LLMs)
		  holds immense promise for healthcare, particularly in
		  medication guidance and adverse drug reaction prediction.
		  Despite their potential, existing LLMs face challenges in
		  dealing with complex polypharmacy scenarios and often
		  grapple with data lag issues. To address these limitations,
		  we introduce an LLM-based Chinese medication guidance
		  system, called ShennongMGS, specifically tailored for
		  robust medication guidance and adverse drug reaction
		  predictions. Our system transforms multi-source
		  heterogeneous medication information into a knowledge graph
		  and employs a two-stage training strategy to construct a
		  specialized LLM (ShennongGPT). This method enables the
		  simulation of professional pharmacists’ decision-making
		  processes and incorporates the capability for knowledge
		  self-updating, thereby significantly enhancing drug safety
		  and the overall quality of medical services. Rigorously
		  evaluated by medical professionals and artificial
		  intelligence experts, our method demonstrates superiority,
		  outperforming existing general and specialized LLMs in
		  performance.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= mar,
  articleno	= {15},
  numpages	= {14},
  keywords	= {Large language model, model fine-tuning, medication
		  guidance, Chinese medical system, natural language
		  processing, software system}
}

@Article{	  10.1145/3706057,
  author	= {Jayasundara, Sakuna Harinda and Gamagedara Arachchilage,
		  Nalin Asanka and Russello, Giovanni},
  title		= {SoK: Access Control Policy Generation from High-level
		  Natural Language Requirements},
  year		= {2024},
  issue_date	= {April 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3706057},
  doi		= {10.1145/3706057},
  abstract	= {Administrator-centered access control failures can cause
		  data breaches, putting organizations at risk of financial
		  loss and reputation damage. Existing graphical policy
		  configuration tools and automated policy generation
		  frameworks attempt to help administrators configure and
		  generate access control policies by avoiding such failures.
		  However, graphical policy configuration tools are prone to
		  human errors, making them unusable. On the other hand,
		  automated policy generation frameworks are prone to
		  erroneous predictions, making them unreliable. Therefore,
		  to find ways to improve their usability and reliability, we
		  conducted a Systematic Literature Review analyzing 49
		  publications. The thematic analysis of the publications
		  revealed that graphical policy configuration tools are
		  developed to write and visualize policies manually.
		  Moreover, automated policy generation frameworks are
		  developed using machine learning (ML) and natural language
		  processing (NLP) techniques to automatically generate
		  access control policies from high-level requirement
		  specifications. Despite their utility in the access control
		  domain, limitations of these tools, such as the lack of
		  flexibility, and limitations of frameworks, such as the
		  lack of domain adaptation, negatively affect their
		  usability and reliability, respectively. Our study offers
		  recommendations to address these limitations through
		  real-world applications and recent advancements in the NLP
		  domain, paving the way for future research.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {102},
  numpages	= {37},
  keywords	= {Access control, policy engineering, system administrator,
		  user interfaces, frameworks, usability, reliability}
}

@InProceedings{	  10.1145/3716554.3716557,
  author	= {Ali, Mohsan and Giallousi, Nina and Melidis, Alexandros
		  and Alexopoulos, Charalampos and Charalabidis, Yannis},
  title		= {GlossAPI: Architecturing the Greek Data Pile for LLM
		  development},
  year		= {2025},
  isbn		= {9798400713170},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3716554.3716557},
  doi		= {10.1145/3716554.3716557},
  abstract	= {With the release of large language models such as ChatGPT,
		  there has been a surge in demand for national language data
		  sources. Greek language resources, in particular, have
		  become widely available and are frequently discussed in the
		  context of LLM development for tailored or custom use. The
		  development of large language models presents several
		  challenges, one of which is data preparation, which must be
		  of high quality and grounded in linguistic, conceptual, and
		  historical foundations. Creating data sets of this nature
		  is a highly complex task. This study focuses on the
		  collection, curation, and management of data sets for the
		  Greek language, as well as making the data provision for
		  use in training custom Greek LLMs smoother, not only for
		  custom LLM development but also for fine-tuning existing
		  LLM models. Immediately after the ChatGPT invention, the
		  GlossAPI project initiative started in 2023. This research
		  encourages the need for a Greek data pile creation, which
		  is further dependent upon the data collection, acquisition,
		  data cleaning, annotation, and classification steps. Upon
		  cleaning the data pile, storage of the data pile and
		  serving it to the LLMs or other users is the main concern.
		  This concern is resolved by the use of an integrative,
		  capable database solution that can facilitate in-database
		  inference, including data collection, annotation,
		  classification, and storage. For each of these stages, a
		  special protocol requires, for instance, the use of
		  existing LLMs or other artificial intelligence models for
		  data preprocessing, annotation, and classification. To
		  store data, we need a database that can seamlessly
		  integrate with other data resources. Our study aim at the
		  development of an architecture to unite Greek data pile and
		  intelligent AI models at a one place using MindsDB
		  integrative capabilities. MindsDB is an emerging database
		  with an integration functionality to wide variety of AI
		  models which can facilitate the data processing,
		  annotation, classification through the integration of
		  Supervised, unsupervised, and even though LLMs models.},
  booktitle	= {Proceedings of the 28th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {16–25},
  numpages	= {10},
  keywords	= {Generative Pre-Trained Transformers, Large Language
		  Models, Large Scale Language Corpora, Natural Language
		  Processing, Pre-Trained Language Models},
  location	= { },
  series	= {PCI '24}
}

@InProceedings{	  10.1145/3605098.3636053,
  author	= {Alharbi, Reham and Tamma, Valentina and Grasso, Floriana
		  and Payne, Terry},
  title		= {An Experiment in Retrofitting Competency Questions for
		  Existing Ontologies},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3636053},
  doi		= {10.1145/3605098.3636053},
  abstract	= {Competency Questions (CQs) are a form of ontology
		  functional requirements expressed as natural language
		  questions. Inspecting CQs together with the axioms in an
		  ontology provides critical insights into the intended scope
		  and applicability of the ontology. CQs also underpin a
		  number of tasks in the development of ontologies e.g.
		  ontology reuse, ontology testing, requirement
		  specification, and the definition of patterns that
		  implement such requirements. Although CQs are integral to
		  the majority of ontology engineering methodologies, the
		  practice of publishing CQs alongside the ontological
		  artefacts is not widely observed by the community.In this
		  context, we present an experiment in retrofitting CQs from
		  existing ontologies. We propose RETROFIT-CQs, a method to
		  extract candidate CQs directly from ontologies using
		  Generative AI. In the paper we present the pipeline that
		  facilitates the extraction of CQs by leveraging Large
		  Language Models (LLMs) and we discuss its application to a
		  number of existing ontologies.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1650–1658},
  numpages	= {9},
  keywords	= {ontology engineering, competency questions, large language
		  models},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@InProceedings{	  10.1145/3726302.3730148,
  author	= {Kantz, Benedikt and Innerebner, Kevin and Waldert, Peter
		  and Lengauer, Stefan and Lex, Elisabeth and Schreck,
		  Tobias},
  title		= {OnSET: Ontology and Semantic Exploration Toolkit},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3730148},
  doi		= {10.1145/3726302.3730148},
  abstract	= {Retrieval over knowledge graphs is typically performed
		  using specialized, complex query languages such as SPARQL.
		  We propose a novel system, Ontology and Semantic
		  Exploration Toolkit (OnSET), that allows novice users to
		  quickly build queries with visual user guidance provided by
		  topic modeling and semantic search throughout the
		  application. OnSET enables users without prior knowledge of
		  the ontology or networked knowledge to start exploring
		  topics of interest over knowledge graphs, including the
		  retrieval and detailed exploration of prototypical
		  sub-graphs and their instances. Existing systems either
		  focus on direct graph exploration or do not foster further
		  exploration of the result set. We, however, provide a
		  node-based editor that can extend these missing properties
		  of existing systems to support search over large ontologies
		  with sub-graph instances. Furthermore, OnSET combines
		  efficient and open platforms to deploy the system on
		  commodity hardware.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3980–3984},
  numpages	= {5},
  keywords	= {graph retrieval, natural language, ontology, user
		  guidance, visualization},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@InProceedings{	  10.1145/3582768.3582778,
  author	= {Chowdhury, Md Towhidul Absar and Sharma, Naveen},
  title		= {Community Asset Ontology for Modeling Community Data using
		  Information Extraction},
  year		= {2023},
  isbn		= {9781450397629},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3582768.3582778},
  doi		= {10.1145/3582768.3582778},
  abstract	= {In this paper, we analyze some data-related challenges to
		  building resilient and sustainable communities,
		  particularly how to computationally model the social and
		  economical dynamic that exists within a community. To that
		  end, we propose the Community Asset Ontology (CAO) for a
		  knowledge graph that can encapsulate community data as
		  modeled in existing social science literature. We utilize
		  existing information extraction paradigms to map natural
		  language community data to CAO and evaluate the usefulness
		  of such an ontology-based approach compared to a baseline
		  open information extraction approach.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {195–199},
  numpages	= {5},
  keywords	= {information extraction, knowledge graph, ontologies},
  location	= {Bangkok, Thailand},
  series	= {NLPIR '22}
}

@InProceedings{	  10.1145/3696630.3728697,
  author	= {Chom\k{a}tek, \L{}ukasz and S\l{}abosz, Wojciech and
		  Poniszewska-Mara\'{n}da, Aneta},
  title		= {From Words to Wisdom: LLMs Summarizing Instructional
		  Content},
  year		= {2025},
  isbn		= {9798400712760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696630.3728697},
  doi		= {10.1145/3696630.3728697},
  abstract	= {This study explores the effectiveness of large language
		  models (LLMs) in summarizing instructional video
		  transcriptions, a key application in educational
		  technology. We assessed nine LLMs using two prompts—a
		  simple base prompt and an enhanced, structured
		  prompt—across 62 instructional videos. Two evaluating
		  models, gpt-4o-mini and gemini-1.5-flash, scored the
		  summaries based on seven criteria tailored to instructional
		  content: overall structure, presence of examples,
		  availability of sources, relevance, coherence, narration,
		  and ACCURACY. Results showed notable performance
		  differences, with models like Mistral Large and Claude 3.5
		  Sonnet performing best, especially with the enhanced
		  prompt. However, the enhanced prompt improved narrative
		  quality at the expense of structural clarity in some cases.
		  Evaluator bias was also observed, with gpt-4o-mini
		  assigning higher scores than gemini-1.5-flash, highlighting
		  the need for multiple evaluators. These findings underscore
		  the role of prompt design and model choice in educational
		  LLM applications and suggest future research into
		  optimizing prompts and standardizing evaluation methods.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  the Foundations of Software Engineering},
  pages		= {1623–1630},
  numpages	= {8},
  keywords	= {experience, instructional materials, summarization, large
		  language models},
  location	= {Clarion Hotel Trondheim, Trondheim, Norway},
  series	= {FSE Companion '25}
}

@InProceedings{	  10.1145/3643795.3648384,
  author	= {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and
		  Ashiwal, Virendra and Linsbauer, Sofia and Eskandani,
		  Nafise},
  title		= {LLM-based and Retrieval-Augmented Control Code
		  Generation},
  year		= {2024},
  isbn		= {9798400705793},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643795.3648384},
  doi		= {10.1145/3643795.3648384},
  abstract	= {Control code is designed and implemented for industrial
		  automation applications that manage power plants,
		  petrochemical processes, or steel production. Popular large
		  language models (LLM) can synthesize low-level control code
		  in the Structured Text programming notation according to
		  the standard IEC 61131-3, but are not aware of proprietary
		  control code function block libraries, which are often used
		  in practice. To automate control logic implementation
		  tasks, we proposed a retrieval-augmented control code
		  generation method that can integrate such function blocks
		  into the generated code. With this method control engineers
		  can benefit from the code generation capabilities of LLMs,
		  re-use proprietary and well-tested function blocks, and
		  speed up typical programming tasks significantly. We have
		  evaluated the method using a prototypical implementation
		  based on GPT-4, LangChain, Open-PLC, and the open-source
		  OSCAT function block library. In several spot sample tests,
		  we successfully generated IEC 61131-3 ST code that
		  integrated the desired function blocks, could be compiled,
		  and validated through simulations.},
  booktitle	= {Proceedings of the 1st International Workshop on Large
		  Language Models for Code},
  pages		= {22–29},
  numpages	= {8},
  keywords	= {large language models, code generation, IEC 61131-3,
		  industrial automation, PLC, DCS, ChatGPT, GPT-4},
  location	= {Lisbon, Portugal},
  series	= {LLM4Code '24}
}

@InProceedings{	  10.1145/3486001.3486240,
  author	= {Hagen, Morten and Arora, Piyush and Ghosh, Rahul and
		  Thomas, Dawn and Joshi, Salil R},
  title		= {Class-Based Order-Independent Models of Natural Language
		  for Bayesian Auto-Complete Inference},
  year		= {2021},
  isbn		= {9781450385947},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486001.3486240},
  doi		= {10.1145/3486001.3486240},
  abstract	= {We introduce a model for auto-complete of general queries
		  via Bayesian inference. To that end, we address three
		  issues: First, the problem of predicting a word given
		  previous words in a text. Usually, the context words are
		  treated as a directional sequence. In our approach, we
		  introduce a set-based class language model with
		  order-independence, modeling the context words as a set of
		  classes. Second, towards the task of predicting the next
		  word’s class based on the classes of previous words plus
		  an incomplete word prefix, we present a Bayesian framework
		  that incorporates the set-based class language model in
		  conjunction with an ontology. Third, regarding the
		  auto-complete problem, we provide complete query
		  suggestions via abstract class-space search which
		  determines similar historical queries that contain the
		  classes of previous words plus the next word’s predicted
		  class. Subsequently, we apply the model to auto-complete
		  inference in a system setting, in which users can access
		  data via natural language queries.},
  booktitle	= {Proceedings of the First International Conference on AI-ML
		  Systems},
  articleno	= {20},
  numpages	= {7},
  keywords	= {Bayesian inference, Class-based language model,
		  auto-complete, order-independence},
  location	= {Bangalore, India},
  series	= {AIMLSystems '21}
}

@Article{	  10.1145/3617371,
  author	= {Banerjee, Anasua and Kumar, Vinay and Shankar, Achyut and
		  Jhaveri, Rutvij H. and Banik, Debajyoty},
  title		= {Automatic Resource Augmentation for Machine Translation in
		  Low Resource Language: EnIndic Corpus},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3617371},
  doi		= {10.1145/3617371},
  abstract	= {Parallel corpus is the primary ingredient of machine
		  translation. It is required to train the statistical
		  machine translation (SMT) and neural machine translation
		  (NMT) systems. There is a lack of good quality parallel
		  corpus for Hindi to English. Comparable corpora for a given
		  language pair are comparatively easy to find, but this
		  cannot be used directly in SMT or NMT systems. As a result,
		  we generate a parallel corpus from the comparable corpus.
		  For this purpose, the sentences (which are translations of
		  each other) are mined from the comparable corpus to prepare
		  the parallel corpus. The proposed algorithm uses the length
		  of the sentence and word translation model to align
		  sentence pairs that are translations of each other. Then,
		  the sentence pairs that are poor translations of each other
		  (measured by a similarity score based on IBM model 1
		  translation probability) are filtered out. We apply this
		  algorithm to comparable corpora, which are crawled from
		  speeches of the President and Vice-President of India, and
		  mined parallel corpora out of them. The prepared parallel
		  corpus contains good quality aligned sentences (with
		  96.338\% f-score). Subsequently, incorrect sentence pairs
		  are filtered out manually to make the corpus in qualitative
		  practical use. Finally, we gather various sentences from
		  different sources to prepare the EnIndic corpus, which
		  comprises 1,656,207 English-Hindi sentence pairs
		  (miscellaneous domain). We have deployed this prepared
		  largest English-Hindi parallel corpus at
		  https://github.com/debajyoty/EnIndic.git and the source
		  code at
		  https://github.com/debajyoty/EnIndicSourceCode.git.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  keywords	= {Parallel Corpus, Comparable Corpus, Machine Translation,
		  Linguistic Resources and Natural Language Processing}
}

@Article{	  10.1145/3586080,
  author	= {Fafalios, Pavlos and Kritsotaki, Athina and Doerr,
		  Martin},
  title		= {The SeaLiT Ontology – An Extension of CIDOC-CRM for the
		  Modeling and Integration of Maritime History Information},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3586080},
  doi		= {10.1145/3586080},
  abstract	= {We describe the construction and use of the SeaLiT
		  Ontology, an extension of the ISO standard CIDOC-CRM for
		  the modelling and integration of maritime history
		  information. The ontology has been developed gradually,
		  following a bottom-up approach that required the analysis
		  of large amounts of real primary data (archival material)
		  as well as knowledge and validation by domain experts
		  (maritime historians). We present the specification of the
		  ontology, RDFS and OWL implementations, as well as
		  knowledge graphs that make use of this data model for
		  integrating information originating from a large and
		  diverse set of archival documents, such as crew lists,
		  sailors registers, naval ship registers, and payrolls. We
		  also describe an application that operates over these
		  knowledge graphs and which supports historians in exploring
		  and quantitatively analysing the integrated data through a
		  user-friendly interface. Finally, we discuss aspects
		  related to the use, evolution, and sustainability of the
		  ontology.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {60},
  numpages	= {21},
  keywords	= {Ontologies, maritime history, CIDOC-CRM, data integration,
		  semantic interoperability}
}

@InProceedings{	  10.1145/3627050.3630732,
  author	= {Gui, Zhou and Freund, Michael and Harth, Andreas},
  title		= {A Natural Language Interface for IoT Systems Using the Web
		  of Things Abstraction},
  year		= {2024},
  isbn		= {9798400708541},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627050.3630732},
  doi		= {10.1145/3627050.3630732},
  abstract	= {We present a demo of a Natural Language Interface (NLI)
		  for controlling Internet of Things (IoT) devices using the
		  Web of Things (WoT) specification as an intermediate
		  abstraction layer. All interaction information of a device
		  is stored in a Knowledge Graph using the thing description
		  ontology. The central component of the NLI is a
		  sequence-to-sequence neural network model for text to code
		  translation. We build a data corpus based on the
		  functionalities of a Philips Hue smart lamp and use the
		  corpus to train the text to code model. Our demonstration
		  illustrates how to control the power state, the light
		  colour, and the brightness of a Philips Hue smart lamp
		  using natural language commands. The implementation of an
		  NLI system based on the WoT specification represents an
		  approach towards the development of easy-to-use and
		  interoperable IoT systems.},
  booktitle	= {Proceedings of the 13th International Conference on the
		  Internet of Things},
  pages		= {186–188},
  numpages	= {3},
  keywords	= {Knowledge Graphs, Natural Language Understanding, Text to
		  Code, Web of Things},
  location	= {Nagoya, Japan},
  series	= {IoT '23}
}

@InProceedings{	  10.1145/3708557.3716342,
  author	= {Westh\"{a}u\ss{}er, Rebecca and Zepf, Sebastian and
		  Minker, Wolfgang},
  title		= {CAIM: A Cognitive AI Memory Framework for Long-term
		  Interaction with LLMs},
  year		= {2025},
  isbn		= {9798400714092},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708557.3716342},
  doi		= {10.1145/3708557.3716342},
  abstract	= {The concept of cognitive artificial intelligence (AI)
		  intends to simulate the human thought process in a
		  computerized model. When applied as an extension to large
		  language models (LLMs), cognitive AI has the potential to
		  support fostering long-term relationships by improving the
		  contextual relevance of generated responses. In this
		  context, we propose CAIM, a framework inspired by cognitive
		  AI principles. CAIM aims to enhance the memory capabilities
		  of LLMs by integrating aspects of cognitive AI, such as
		  thoughts, memory mechanisms, and decision-making. CAIM
		  consists of three modules: 1.) The Memory Controller as
		  central decision unit 2.) the Memory Retrieval, which
		  filters relevant data for an interaction upon request, and
		  3.) the Post-Thinking, which maintains the memory storage.
		  In this paper, we describe the core architecture of CAIM
		  and outline potential extensions aiming to stimulate
		  discussions around holistic memory modeling for LLMs
		  inspired by cognitive AI.},
  booktitle	= {Companion Proceedings of the 30th International Conference
		  on Intelligent User Interfaces},
  pages		= {22–25},
  numpages	= {4},
  keywords	= {Large Language Models, Long-term Memory, Cognitive AI},
  location	= { },
  series	= {IUI '25 Companion}
}

@InProceedings{	  10.1145/3711896.3737138,
  author	= {Wu, Wei and Wang, Chao and Chen, Liyi and Yin, Mingze and
		  Zhu, Yiheng and Fu, Kun and Ye, Jieping and Xiong, Hui and
		  Wang, Zheng},
  title		= {Structure-Enhanced Protein Instruction Tuning: Towards
		  General-Purpose Protein Understanding with LLMs},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3737138},
  doi		= {10.1145/3711896.3737138},
  abstract	= {Proteins, as essential biomolecules, play a central role
		  in biological processes, including metabolic reactions and
		  DNA replication. Accurate prediction of their properties
		  and functions is crucial in biological applications. Recent
		  development of protein language models (pLMs) with
		  supervised fine tuning provides a promising solution to
		  this problem. However, the fine-tuned model is tailored for
		  particular downstream prediction task, and achieving
		  general-purpose protein understanding remains a challenge.
		  In this paper, we introduce Structure-Enhanced Protein
		  Instruction Tuning (SEPIT) framework to bridge this gap.
		  Our approach incorporates a novel structure-aware module
		  into pLMs to enrich their structural knowledge, and
		  subsequently integrates these enhanced pLMs with large
		  language models (LLMs) to advance protein understanding. In
		  this framework, we propose a novel instruction tuning
		  pipeline. First, we warm up the enhanced pLMs using
		  contrastive learning and structure denoising. Then,
		  caption-based instructions are used to establish a basic
		  understanding of proteins. Finally, we refine this
		  understanding by employing a mixture of experts (MoEs) to
		  capture more complex properties and functional information
		  with the same number of activated parameters. Moreover, we
		  construct the largest and most comprehensive protein
		  instruction dataset to date, which allows us to train and
		  evaluate the general-purpose protein understanding model.
		  Extensive experiments on both open-ended generation and
		  closed-set answer tasks demonstrate the superior
		  performance of SEPIT over both closed-source general LLMs
		  and open-source LLMs trained with protein knowledge.},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {3216–3227},
  numpages	= {12},
  keywords	= {insturction tuning, large language models, protein},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@InProceedings{	  10.1145/3625704.3625720,
  author	= {Bekmanova, Gulmira and Omarbekova, Assel and Mukanova,
		  Assel and Zulkhazhav, Altanbek and Zakirova, Alma and
		  Ongarbayev, Yerkin},
  title		= {Development of an Ontological Model of Words in Public
		  Political Discourse},
  year		= {2023},
  isbn		= {9798400709142},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625704.3625720},
  doi		= {10.1145/3625704.3625720},
  abstract	= {The aim of the research is to develop methods for
		  analyzing political discourse in social networks in the
		  Kazakh language in order to identify official and
		  unofficial information sources of political discourse, as
		  well as to determine the mood of the discussion in these
		  sources. The article presents an ontological model of the
		  subject area of elections, a referendum.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Education and Multimedia Technology},
  pages		= {362–367},
  numpages	= {6},
  keywords	= {Artificial intelligence, knowledge base, discourse,
		  ontology, formalization},
  location	= {Tokyo, Japan},
  series	= {ICEMT '23}
}

@InProceedings{	  10.1145/3701716.3715870,
  author	= {Jiao, Yizhu and Ouyang, Siru and Zhong, Ming and Zhang,
		  Yunyi and Ding, Linyi and Zhou, Sizhe and Han, Jiawei},
  title		= {Retrieval and Structuring Augmented Generation with LLMs
		  for Web Applications},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715870},
  doi		= {10.1145/3701716.3715870},
  abstract	= {Although Large Language Models (LLMs) have revolutionized
		  natural language processing, they face significant
		  challenges in web applications, including hallucinations,
		  outdated knowledge, and limited specialization in niche
		  domains. To address these issues, this tutorial explores
		  how integrating retrieval mechanisms and structured
		  knowledge can enhance LLM performance for web use. By
		  leveraging Retrieval-Augmented Generation (RAG), we can
		  ground LLM outputs with relevant external data, mitigating
		  limitations in applications like search engines, chatbots,
		  and recommendation systems. We delve into text structuring
		  techniques-such as taxonomy construction, multi-level text
		  classification, and taxonomy-guided information
		  retrieval-that improve the effectiveness of information
		  retrieval processes. Furthermore, we examine how
		  structure-guided augmented generation through information
		  extraction and knowledge graph construction can reduce
		  hallucinations and enhance factual accuracy. By bridging
		  the gap between unstructured language models and structured
		  knowledge, we aim to unlock new potentials for dynamic web
		  content generation and personalized user experiences.
		  Finally, we highlight future directions for seamlessly
		  integrating retrieval and generation, enhancing
		  personalization, and incorporating multimodal data to
		  expand the capabilities of LLMs in web applications.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {25–28},
  numpages	= {4},
  keywords	= {large language models, retrieval augmented generation,
		  text mining, text structuring},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3550356.3561606,
  author	= {Huang, Yining and Dhouib, Saadia and Medinacelli, Luis
		  Palacios and Malenfant, Jacques},
  title		= {Enabling semantic interoperability of asset administration
		  shells through an ontology-based modeling method},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561606},
  doi		= {10.1145/3550356.3561606},
  abstract	= {Digital twin technology establishes the future development
		  vision for Industry 4.0, and is also an important
		  exploration direction for the Model-Driven Engineering
		  (MDE) paradigm. Because it builds a more flexible and
		  communicative production system through models that spans
		  life cycle, hierarchy and architecture. The standard
		  proposed under the concept of Industry 4.0, the Asset
		  Administration Shell (AAS), provides a syntactic
		  interoperability interface for all assets involved in smart
		  factories. However, there is still a need to fill the gap
		  regarding semantic interoperability, in order to allow
		  efficient interactions between Industry 4.0 components.
		  Ontologies are a good candidate because they provide formal
		  semantics expressed using a knowledge representation
		  language, and in addition, there are many associated mature
		  tools for reasoning and inference. Therefore, we propose a
		  modeling approach that provides semantic interoperability
		  for AAS-based digital twins using ontologies.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {497–502},
  numpages	= {6},
  keywords	= {asset administration shell, digital twins, industry 4.0,
		  model-driven engineering, ontology, semantic
		  interoperability, smart manufacturing},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@Article{	  10.1145/3554727,
  author	= {Dong, Chenhe and Li, Yinghui and Gong, Haifan and Chen,
		  Miaoxin and Li, Junxin and Shen, Ying and Yang, Min},
  title		= {A Survey of Natural Language Generation},
  year		= {2022},
  issue_date	= {August 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {8},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3554727},
  doi		= {10.1145/3554727},
  abstract	= {This article offers a comprehensive review of the research
		  on Natural Language Generation (NLG) over the past two
		  decades, especially in relation to data-to-text generation
		  and text-to-text generation deep learning methods, as well
		  as new applications of NLG technology. This survey aims to
		  (a) give the latest synthesis of deep learning research on
		  the NLG core tasks, as well as the architectures adopted in
		  the field; (b) detail meticulously and comprehensively
		  various NLG tasks and datasets, and draw attention to the
		  challenges in NLG evaluation, focusing on different
		  evaluation methods and their relationships; (c) highlight
		  some future emphasis and relatively recent research issues
		  that arise due to the increasing synergy between NLG and
		  other artificial intelligence areas, such as computer
		  vision, text, and computational creativity.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {173},
  numpages	= {38},
  keywords	= {Natural language generation, data-to-text generation,
		  text-to-text generation, deep learning, evaluation}
}

@InProceedings{	  10.1145/3583780.3615126,
  author	= {Dong, Hang and Chen, Jiaoyan and He, Yuan and Horrocks,
		  Ian},
  title		= {Ontology Enrichment from Texts: A Biomedical Dataset for
		  Concept Discovery and Placement},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615126},
  doi		= {10.1145/3583780.3615126},
  abstract	= {Mentions of new concepts appear regularly in texts and
		  require automated approaches to harvest and place them into
		  Knowledge Bases (KB), e.g., ontologies and taxonomies.
		  Existing datasets suffer from three issues, (i) mostly
		  assuming that a new concept is pre-discovered and cannot
		  support out-of-KB mention discovery; (ii) only using the
		  concept label as the input along with the KB and thus
		  lacking the contexts of a concept label; and (iii) mostly
		  focusing on concept placement w.r.t a taxonomy of atomic
		  concepts, instead of complex concepts, i.e., with logical
		  operators. To address these issues, we propose a new
		  benchmark, adapting MedMentions dataset (PubMed abstracts)
		  with SNOMED CT versions in 2014 and 2017 under the Diseases
		  sub-category and the broader categories of Clinical
		  finding, Procedure, and Pharmaceutical / biologic product.
		  We provide usage on the evaluation with the dataset for
		  out-of-KB mention discovery and concept placement, adapting
		  recent Large Language Model based methods.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5316–5320},
  numpages	= {5},
  keywords	= {SNOMED CT, biomedical ontologies, concept placement,
		  entity linking, language models, ontology enrichment, text
		  mining},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Article{	  10.1145/3652952,
  author	= {Degbelo, Auriol},
  title		= {Prolegomena to a Description Language for GenAI Tools in
		  Cities},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {1},
  url		= {https://doi.org/10.1145/3652952},
  doi		= {10.1145/3652952},
  abstract	= {The potential of generative AI has been recently
		  demonstrated through different applications. The open
		  government and smart city initiatives can leverage this
		  potential to produce innovations that improve government
		  workflows and the lives of citizens. This commentary makes
		  the case for a description language enabling the structured
		  documentation of these upcoming innovations. The
		  description language would facilitate the communication
		  between governments, citizens, and innovators. The key
		  elements of the description language are briefly sketched
		  and its usefulness is shown by the generation of ideas for
		  GenAI tools related to interactive maps in cities.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= feb,
  articleno	= {8},
  numpages	= {8},
  keywords	= {Smart cities, open government, GenAI tools, metadata
		  generation, interactive maps, human-computer interaction}
}

@Article{	  10.1145/3548457,
  author	= {Lahoti, Pawan and Mittal, Namita and Singh, Girdhari},
  title		= {A Survey on NLP Resources, Tools, and Techniques for
		  Marathi Language Processing},
  year		= {2022},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3548457},
  doi		= {10.1145/3548457},
  abstract	= {Natural Language Processing (NLP) has been in practice for
		  the past couple of decades, and extensive work has been
		  done for the Western languages, particularly the English
		  language. The Eastern counterpart, especially the languages
		  of the Indian subcontinent, needs attention as not much
		  language processing work has been done on these languages.
		  Western languages are rich in dictionaries, WordNet, and
		  associated tools, while Indian languages are lagging behind
		  in this segment. Marathi is the third most spoken language
		  in India and the 15th most spoken language worldwide. Lack
		  of resources, complex linguistic facts, and the inclusion
		  of prevalent dialects of neighbors have resulted in limited
		  work for Marathi. The aim of this study is to provide an
		  insight into the various linguistic resources, tools, and
		  state-of-the-art techniques applied to the processing of
		  the Marathi language. Initially, morphological descriptions
		  of the Marathi language are provided, followed by a
		  discussion on the characteristics of the Marathi language.
		  Thereafter, for Marathi language, the availability of
		  corpus, tools, and techniques to be used to develop NLP
		  tasks is reviewed. Finally, gap analysis is discussed in
		  current research and future directions for this new and
		  dynamic area of research are listed that will benefit the
		  Marathi Language Processing research community.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= dec,
  articleno	= {47},
  numpages	= {34},
  keywords	= {Marathi language, Marathi morphology, Marathi resources,
		  Part-of-Speech (POS) tagging, Named Entity Recognition
		  (NER), Word Sense Disambiguation (WSD)}
}

@Proceedings{	  10.1145/3711542,
  title		= {NLPIR '24: Proceedings of the 2024 8th International
		  Conference on Natural Language Processing and Information
		  Retrieval},
  year		= {2024},
  isbn		= {9798400717383},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3519298,
  author	= {Xue, Xingsi and Liu, Wenyu},
  title		= {Integrating Heterogeneous Ontologies in Asian Languages
		  Through Compact Genetic Algorithm&nbsp;with Annealing
		  Re-sample Inheritance Mechanism},
  year		= {2023},
  issue_date	= {March 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3519298},
  doi		= {10.1145/3519298},
  abstract	= {An ontology is a state-of-the-art knowledge modeling
		  technique in the natural language domain, which has been
		  widely used to overcome the linguistic barriers in Asian
		  and European countries’ intelligent applications.
		  However, due to the different knowledge backgrounds of
		  ontology developers, the entities in the ontologies could
		  be defined in different ways, which hamper the
		  communications among the intelligent applications built on
		  them. How to find the semantic relationships among the
		  entities that are lexicalized in different languages is
		  called the Cross-lingual Ontology Matching problem (COM),
		  which is a challenge problem in the ontology matching
		  domain. To face this challenge, being inspired by the
		  success of the Genetic Algorithm&nbsp;(GA) in the ontology
		  matching domain, this work proposes a Compact GA with
		  Annealing Re-sample Inheritance mechanism (CGA-ARI) to
		  efficiently address the COM problem. In particular, a
		  Cross-lingual Similarity Metric (CSM) is presented to
		  distinguish two cross-lingual entities, a discrete optimal
		  model is built to define the COM problem, and the compact
		  encoding mechanism and the Annealing Re-sample Inheritance
		  mechanism (ARI) are introduced to improve CGA’s searching
		  performance. The experiment uses Multifarm track to test
		  CGA-ARI’s performance, which includes 45 ontology pairs
		  in different languages. The experimental results show that
		  CGA-ARI is able to significantly improve the performance of
		  GA and CGA and determine better alignments than
		  state-of-the-art ontology matching systems.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= mar,
  articleno	= {73},
  numpages	= {21},
  keywords	= {Cross-lingual ontology alignment, compact genetic
		  algorithm, Annealing Re-sample Inheritance Mechanism}
}

@InProceedings{	  10.1145/3724504.3724548,
  author	= {Yang, Xiaoqing and Liang, Xiaobo},
  title		= {A Visual Analysis of the Hotspots of the National Security
		  and Language Studies in the Context of Big Data},
  year		= {2025},
  isbn		= {9798400711732},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3724504.3724548},
  doi		= {10.1145/3724504.3724548},
  abstract	= {This paper explores the convergence of national security
		  and language, a critical but under-explored area of
		  contemporary scholarship. It adopts a visual analytical
		  approach, using bibliometric techniques and CiteSpace
		  software to analyse 2,039 journal articles from the Web of
		  Science Core Collection. All of these articles have been
		  published over the past 38 years (1986-2024) on the topic
		  of national security and language. The aim of this paper is
		  to map knowledge using big data mining, to identify
		  research hotspots in the field of language and national
		  security, and to visualize the results using knowledge
		  mapping software. The analysis focuses on publication time,
		  keyword co-occurrence, citation bursts and their clustering
		  relationships. The findings reveal that: (1)there has been
		  a significant rise in academic interest in the field of
		  language and national security since 1986, with a notable
		  increase after 2014, which underscores the growing
		  awareness of the critical role of language in national
		  security discourse and practice; (2) the research hotspots
		  of national security and language research are divided into
		  nine main clusters, namely, #0 food security, #1 European
		  Union, #2politics, #3 discourse analysis, #4 national
		  language processing, #5 security, #6 national security, #7
		  national identity and #8 globalization. Future research
		  should aim to further explore the intersection of language
		  and national security by incorporating a wider range of
		  sources, including non-English literature, in order to
		  enrich the understanding of this field and to examine the
		  potential impact of emerging technologies such as
		  artificial intelligence and machine learning on the
		  dynamics of language in the security contexts.},
  booktitle	= {Proceedings of the 2024 2nd International Conference on
		  Information Education and Artificial Intelligence},
  pages		= {267–272},
  numpages	= {6},
  keywords	= {Bibliometric Analysis, CiteSpace, Language, National
		  Security},
  location	= { },
  series	= {ICIEAI '24}
}

@InProceedings{	  10.1145/3701716.3717818,
  author	= {Yang, Can and Pereira Nunes, Bernardo and Rodr\'{\i}guez
		  M\'{e}ndez, Sergio},
  title		= {LLM as Auto-Prompt Engineer: Automated NER Prompt
		  Optimisation},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3717818},
  doi		= {10.1145/3701716.3717818},
  abstract	= {The emergence of Large Language Models (LLMs) has
		  revolutionised natural language processing capabilities.
		  However, despite these advances, effectively optimising
		  prompts for knowledge extraction tasks like Named Entity
		  Recognition (NER) remains challenging. This paper presents
		  a zero-shot automated prompt engineering approach that
		  decomposes the NER task into two phases: entity boundary
		  detection and entity classification. Our method
		  incorporates structured task analysis, automated prompt
		  generation, test case generation, and iterative
		  optimisation, requiring no labelled training examples. This
		  decomposition allows for more precise entity recognition
		  while maintaining the efficiency. Through experimentation
		  on the CoNLL-2003 dataset using standard exact-match
		  evaluation metrics, our approach demonstrates improvements
		  over unified methods, achieving a 75.39\% F1 score compared
		  to baseline approaches (72.90\%). The key contributions
		  include: (1) A structured pipeline for zero-shot automated
		  prompt engineering in NER tasks that addresses the
		  challenges of prompt design and optimisation; (2) A
		  two-phase approach to NER tasks that separates boundary
		  detection from entity classification; and (3) Experimental
		  results demonstrating the effectiveness of our approach
		  compared to existing zero-shot approaches in NER tasks.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2574–2578},
  numpages	= {5},
  keywords	= {automated prompt engineering, large language models, named
		  entity recognition, prompt optimisation},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@Article{	  10.1145/3709681,
  author	= {Omar, Reham and Mangukiya, Omij and Mansour, Essam},
  title		= {Dialogue Benchmark Generation from Knowledge Graphs with
		  Cost-Effective Retrieval-Augmented LLMs},
  year		= {2025},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {1},
  url		= {https://doi.org/10.1145/3709681},
  doi		= {10.1145/3709681},
  abstract	= {Dialogue benchmarks are crucial in training and evaluating
		  chatbots engaging in domain-specific conversations.
		  Knowledge graphs (KGs) represent semantically rich and
		  well-organized data spanning various domains, such as DBLP,
		  DBpedia, and YAGO. Traditionally, dialogue benchmarks have
		  been manually created from documents, neglecting the
		  potential of KGs in automating this process. Some
		  question-answering benchmarks are automatically generated
		  using extensive preprocessing from KGs, but they do not
		  support dialogue generation. This paper introduces
		  Chatty-Gen, a novel multi-stage retrieval-augmented
		  generation platform for automatically generating
		  high-quality dialogue benchmarks tailored to a specific
		  domain using a KG. Chatty-Gen decomposes the generation
		  process into manageable stages and uses assertion rules for
		  automatic validation between stages. Our approach enables
		  control over intermediate results to prevent time-consuming
		  restarts due to hallucinations. It also reduces reliance on
		  costly and more powerful commercial LLMs. Chatty-Gen
		  eliminates upfront processing of the entire KG using
		  efficient query-based retrieval to find representative
		  subgraphs based on the dialogue context. Our experiments
		  with several real and large KGs demonstrate that Chatty-Gen
		  significantly outperforms state-of-the-art systems and
		  ensures consistent model and system performance across
		  multiple LLMs of diverse capabilities, such as GPT-4o,
		  Gemini 1.5, Llama 3, and Mistral.},
  journal	= {Proc. ACM Manag. Data},
  month		= feb,
  articleno	= {31},
  numpages	= {26},
  keywords	= {assertion-based validation, benchmarking, conversational
		  question answering, cost-effecive inference, graph
		  serialization, knowledge graphs (kgs), large language
		  models (llms), retrieval-augumented generation (rag)}
}

@InProceedings{	  10.1145/3587281.3587700,
  author	= {Deng, Xiang},
  title		= {A More Accessible Web with Natural Language Interface},
  year		= {2023},
  isbn		= {9798400707483},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587281.3587700},
  doi		= {10.1145/3587281.3587700},
  abstract	= {The past decade has witnessed the rapid growth and
		  evolution of the Web. Today, people can perform a multitude
		  of tasks through the use of a single browser. Despite the
		  immense power and capabilities of the Web, its increasing
		  complexity and the overwhelming amount of information pose
		  significant challenges for users to easily access the
		  information they need and achieve their goals. Especially
		  for those who are less technologically proficient or have
		  disabilities. In this work, we propose to tackle this issue
		  by building a general natural language interface for the
		  Web, which enables users to express their needs in natural
		  language and have the system carry out the arduous actions.
		  We examine the key components crucial for building the
		  natural language interface. On top of that, we present our
		  ongoing efforts in curating a new benchmark dataset
		  covering a diverse range of websites and tasks, and
		  establishing baselines to demonstrate the feasibility of
		  building such a system.},
  booktitle	= {Proceedings of the 20th International Web for All
		  Conference},
  pages		= {153–155},
  numpages	= {3},
  keywords	= {Natural Language Interface, Web Accessibility},
  location	= {Austin, TX, USA},
  series	= {W4A '23}
}

@InProceedings{	  10.1145/3735654.3735942,
  author	= {Hoseini, Sayed and Herrmann, Vincent and Quix, Christoph},
  title		= {End-To-End ML with LLMs and Semantic Data Management:
		  Experiences from Chemistry 4.0},
  year		= {2025},
  isbn		= {9798400719240},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3735654.3735942},
  doi		= {10.1145/3735654.3735942},
  abstract	= {Machine Learning (ML) in industrial chemistry is often
		  hindered by the complexity of preprocessing heterogeneous
		  datasets. In this proof-of-concept study, we explore the
		  use of semantic data management to support LLM-driven
		  automation of end-to-end ML pipelines in a real-world
		  Chemistry 4.0 setting. A semantic model is used to capture
		  domain knowledge and metadata in a machine-readable form,
		  guiding LLMs through natural language prompts to generate
		  complete data wrangling and ML modeling code. We evaluate
		  several state-of-the-art LLMs on their ability to
		  autonomously produce functionally correct Python code for
		  preprocessing and Gaussian Process modeling. Our results
		  show that, when guided by structured semantic context,
		  larger LLMs can reliably generate accurate pipelines,
		  significantly reducing the need for manual intervention.
		  These findings provide an encouraging starting point for
		  further exploration toward leveraging the semantic model to
		  improve the robustness of code generation by systematically
		  integrating relevant information into the generation
		  process, rather than relying solely on the raw intelligence
		  of the LLM.},
  booktitle	= {Proceedings of the Workshop on Data Management for
		  End-to-End Machine Learning},
  articleno	= {6},
  numpages	= {10},
  keywords	= {AutoML, Data Wrangling, LLMs, Semantic Data Management},
  location	= {Berlin, Germany},
  series	= {DEEM '25}
}

@InProceedings{	  10.1145/3652620.3687806,
  author	= {Burgue\~{n}o, Lola and Keet, Maria and Kienzle, J\"{o}rg
		  and Michael, Judith and Babur, \"{O}nder},
  title		= {A Human Behavior Exploration Approach Using LLMs for
		  Cyber-Physical Systems},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687806},
  doi		= {10.1145/3652620.3687806},
  abstract	= {In the early phases of Cyber-Physical Systems (CPS)
		  development, scoping human behavior plays a significant
		  role, especially when interactions extend beyond expected
		  behavior. Here, it is especially challenging to develop
		  cases that capture the full spectrum of human behavior. Up
		  to now, identifying such behavior of humans remains a task
		  for domain experts. We explore how one can use Large
		  Languages Models (LLMs) in the design phase of systems to
		  provide additional information about human-CPS interaction.
		  Our approach proposes a preliminary ontology describing a
		  hierarchy of types of behavior and relevant CPS components
		  as input for prompt templates. It uses them to generate
		  parts of human behavior descriptions, as well as a canned
		  prompt with one variable about behavior. For demonstration,
		  we take a smart building with a Home Energy System as the
		  use case.An initial user evaluation shows that the behavior
		  descriptions generated with standard and ontology-driven
		  prompts complement each other and are useful when assisting
		  humans. The discovered uncommon behaviors can be used to
		  complete interaction scenarios that eventually result in a
		  more robust CPS implementation.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {578–586},
  numpages	= {9},
  keywords	= {human behavior, large language models, cyber-physical
		  systems, user scenario, digital twin},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3727582.3728689,
  author	= {Yamani, Asma and Baslyman, Malak and Ahmed, Moataz},
  title		= {Leveraging LLMs for User Stories in AI Systems: UStAI
		  Dataset},
  year		= {2025},
  isbn		= {9798400715945},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3727582.3728689},
  doi		= {10.1145/3727582.3728689},
  abstract	= {AI systems are gaining widespread adoption across various
		  sectors and domains. Creating high-quality AI system
		  requirements is crucial for aligning the AI system with
		  business goals and consumer values and for social
		  responsibility. However, with the uncertain nature of AI
		  systems and the heavy reliance on sensitive data, more
		  research is needed to address the elicitation and analysis
		  of AI systems requirements. With the proprietary nature of
		  many AI systems, there is a lack of open-source
		  requirements artifacts and technical requirements documents
		  for AI systems, limiting broader research and
		  investigation. With Large Language Models (LLMs) emerging
		  as a promising alternative to human-generated text, this
		  paper investigates the potential use of LLMs to generate
		  user stories for AI systems based on abstracts from
		  scholarly papers. We conducted an empirical evaluation
		  using three LLMs and generated 1260 user stories from 42
		  abstracts from 26 domains. We assess their quality using
		  the Quality User Story (QUS) framework. Moreover, we
		  identify relevant non-functional requirements (NFRs) and
		  ethical principles. Our analysis demonstrates that the
		  investigated LLMs can generate user stories inspired by the
		  needs of various stakeholders, offering a promising
		  approach for generating user stories for research purposes
		  and for aiding in the early requirements elicitation phase
		  of AI systems. We have compiled and curated a collection of
		  stories generated by various LLMs into a dataset (UStAI),
		  which is now publicly available for use.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Predictive Models and Data Analytics in Software
		  Engineering},
  pages		= {21–30},
  numpages	= {10},
  keywords	= {User stories, large language models, quality requirements,
		  requirements elicitation, requirements generation},
  location	= {Trondheim, Norway},
  series	= {PROMISE '25}
}

@InProceedings{	  10.1145/3568364.3568388,
  author	= {Zeng, Jinghan},
  title		= {Cross-Language Vocabulary Teaching Based on Information
		  Network Technology: The Example of Loanwords},
  year		= {2022},
  isbn		= {9781450396950},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3568364.3568388},
  doi		= {10.1145/3568364.3568388},
  abstract	= {Loanwords have always been a hot topic in the study and
		  teaching of Chinese as a foreign language, but there are
		  also many different views and debates. This article
		  summarises and outlines the hotspots of research into and
		  teaching of loanwords in linguistics in recent decades and
		  discusses the current issues of language teaching and
		  cross-linguistic education in the context of the new
		  COVID-19 pandemic in relation to information network
		  technology. First, the concept and scope of loanwords are
		  defined, and the rationale of loanwords is discussed from
		  the perspective of word formation. On this basis, the
		  characteristics of loanwords are summarised, the online
		  teaching of loanwords in the context of the new pandemic is
		  discussed from the perspective of information technology
		  and the Sinicization of loanwords is discussed from the
		  perspective of language development.},
  booktitle	= {Proceedings of the 4th World Symposium on Software
		  Engineering},
  pages		= {155–160},
  numpages	= {6},
  keywords	= {information-based teaching, language education, loanwords,
		  word-building rationale},
  location	= {Xiamen, China},
  series	= {WSSE '22}
}

@InProceedings{	  10.1145/3701716.3717556,
  author	= {Ma, Ao and Li, Zhiyuan and Liang, Zhuonan and Gu,
		  Tiancheng and Fan, Jianan and Long, Jieting and M\"{u}ller,
		  Henning and Cai, Weidong},
  title		= {Seek Inner: LLM-Enhanced Information Mining for Medical
		  Visual Question Answering},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3717556},
  doi		= {10.1145/3701716.3717556},
  abstract	= {Medical visual question answering (Med-VQA) focuses on
		  analyzing medical images to accurately respond to
		  clinicians' specific questions. Although integrating prior
		  knowledge can enhance VQA reasoning, current methods often
		  struggle to extract relevant information from the vast and
		  complex medical knowledge base, thereby limiting the
		  models' ability to learn domain-specific features. To
		  overcome this limitation, our study presents a novel
		  information mining approach that leverages large language
		  models (LLMs) to efficiently retrieve pertinent data.
		  Specifically, we design a latent knowledge generation
		  module that employs LLMs to separately extract and filter
		  information from questions and answers, enhancing the
		  model's inference capabilities. Furthermore, we propose a
		  multi-level prompt fusion module in which an initial prompt
		  interacts with the extracted latent knowledge to draw
		  clinically relevant details from both unimodal and
		  multimodal features. Experimental results demonstrate that
		  our approach outperforms current state-of-the-art models on
		  multiple Med-VQA benchmark datasets.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2297–2305},
  numpages	= {9},
  keywords	= {knowledge injection, large language models, medical visual
		  question answering, multimodal learning},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@Proceedings{	  10.1145/3640310,
  title		= {MODELS '24: Proceedings of the ACM/IEEE 27th International
		  Conference on Model Driven Engineering Languages and
		  Systems},
  year		= {2024},
  isbn		= {9798400705045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Linz, Austria}
}

@InProceedings{	  10.1145/3706599.3720126,
  author	= {Zhang, Shengchen and Guo, Weiwei and Sun, Xiaohua},
  title		= {Align with Me, Not TO Me: How People Perceive Concept
		  Alignment with LLM-Powered Conversational Agents},
  year		= {2025},
  isbn		= {9798400713958},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706599.3720126},
  doi		= {10.1145/3706599.3720126},
  abstract	= {Concept alignment—building a shared understanding of
		  concepts—is essential for human and human-agent
		  communication. While large language models (LLMs) promise
		  human-like dialogue capabilities for conversational agents,
		  the lack of studies to understand people’s perceptions
		  and expectations of concept alignment hinders the design of
		  effective LLM agents. This paper presents results from two
		  lab studies with human-human and human-agent pairs using a
		  concept alignment task. Quantitative and qualitative
		  analysis reveals and contextualizes potentially (un)helpful
		  dialogue behaviors, how people perceived and adapted to the
		  agent, as well as their preconceptions and expectations.
		  Through this work, we demonstrate the co-adaptive and
		  collaborative nature of concept alignment and identify
		  potential design factors and their trade-offs, sketching
		  the design space of concept alignment dialogues. We
		  conclude by calling for designerly endeavors on
		  understanding concept alignment with LLMs in context, as
		  well as technical efforts to combine theory-informed and
		  LLM-driven approaches.},
  booktitle	= {Proceedings of the Extended Abstracts of the CHI
		  Conference on Human Factors in Computing Systems},
  articleno	= {67},
  numpages	= {10},
  keywords	= {Concept Alignment, Grounding, Conversational Agents, Large
		  Language Models, Human-Agent Interaction},
  location	= { },
  series	= {CHI EA '25}
}

@InProceedings{	  10.1145/3705328.3748751,
  author	= {Ferrero, Fabio},
  title		= {Narrative-Driven Itinerary Recommendation: LLM Integration
		  for Immersive Urban Walking},
  year		= {2025},
  isbn		= {9798400713644},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3705328.3748751},
  doi		= {10.1145/3705328.3748751},
  abstract	= {Sedentary behavior, dubbed the disease of the 21st
		  century, is a ubiquitous force driving chronic illness.
		  Yet, traditional itinerary and Point-of-Interest (POI)
		  Recommender Systems (RSs) lack engaging elements that
		  motivate routine urban walking. This research proposes a
		  novel framework combining narrative-driven storytelling
		  with location-based RSs to promote physical activity and
		  immersive urban exploration. This approach introduces a
		  bidirectional alignment between POI and itinerary
		  recommendations and LLM-generated narratives, transforming
		  routine urban walks into dynamic journeys where
		  contextually relevant stories unfold across city locations.
		  Unlike sequential POI recommendations, this framework
		  embeds location suggestions within contextually relevant
		  narratives of various genres, simultaneously promoting
		  health benefits and deeper city exploration. The research
		  addresses three research questions using a method that
		  builds a structured knowledge base by extracting entities
		  (e.g., POIs, and characters) and semantic links from
		  narrative corpora, enabling semantic alignment between
		  recommended physical locations and story elements. The core
		  aspects of this work are: (i) context-aware itinerary
		  recommendations and personalized story generation, (ii)
		  bidirectional mapping between RSs and story generation, and
		  (iii) systems design bridging user’s needs to promote
		  urban walking as a health activity. Evaluation employs
		  comparative user studies measuring quality and engagement,
		  route-narrative semantic alignment, and narrative analysis
		  to validate the integrated proposed approach.},
  booktitle	= {Proceedings of the Nineteenth ACM Conference on
		  Recommender Systems},
  pages		= {1485–1491},
  numpages	= {7},
  keywords	= {Walking Route Recommender Systems, Large Language Models,
		  Geographically Anchored Interactive Narratives.},
  location	= { },
  series	= {RecSys '25}
}

@Article{	  10.1109/taslp.2022.3181350,
  author	= {An, Jinwon and Cho, Sungzoon and Bang, Junseong and Kim,
		  Misuk},
  title		= {Domain-Slot Relationship Modeling Using a Pre-Trained
		  Language Encoder for Multi-Domain Dialogue State Tracking},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3181350},
  doi		= {10.1109/TASLP.2022.3181350},
  abstract	= {Dialogue state tracking for multi-domain dialogues is
		  challenging because the model should be able to track
		  dialogue states across multiple domains and slots. As using
		  pre-trained language models is the de facto standard for
		  natural language processing tasks, many recent studies use
		  them to encode the dialogue context for predicting the
		  dialogue states. Model architectures that have certain
		  inductive biases for modeling the relationship among
		  different domain-slot pairs are also emerging. Our work is
		  based on these research approaches on multi-domain dialogue
		  state tracking. We propose a model architecture that
		  effectively models the relationship among domain-slot pairs
		  using a pre-trained language encoder. Inspired by the way
		  the special &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$[CLS]$&lt;/tex-math&gt;&lt;/inline-formula&gt;
		  token in BERT is used to aggregate the information of the
		  whole sequence, we use multiple special tokens for each
		  domain-slot pair that encodes information corresponding to
		  its domain and slot. The special tokens are run together
		  with the dialogue context through the pre-trained language
		  encoder, which effectively models the relationship among
		  different domain-slot pairs. Our experimental results on
		  the datasets MultiWOZ-2.0 and MultiWOZ-2.1 show that our
		  model outperforms other models with the same setting. Our
		  ablation studies incorporate three main parts. The first
		  component shows the effectiveness of our approach
		  exploiting the relationship modeling. The second component
		  compares the effect of using different pre-trained language
		  encoders. The final component involves comparing different
		  initialization methods that could be used for the special
		  tokens. Qualitative analysis of the attention map of the
		  pre-trained language encoder shows that our special tokens
		  encode relevant information through the encoding process by
		  attending to each other.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jun,
  pages		= {2091–2102},
  numpages	= {12}
}

@InProceedings{	  10.1145/3726302.3731964,
  author	= {Zhao, Mengyue and Nokleby, Matthew and Shen, Bo and Dong,
		  Wenbo and Pachauri, Deepti and Yang, Andrew},
  title		= {Ontology-Guided Knowledge Graph Retrieval for Multi-Hop
		  and Cross-Granularity Store Fulfillment Queries},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3731964},
  doi		= {10.1145/3726302.3731964},
  abstract	= {Answering complex queries in store fulfillment, such as
		  ''What percentage of employee-assigned actions remain
		  unresolved?'' or ''How many worklists for a specific
		  product type were completed within a timeframe at each
		  location?'' requires precise, multi-hop reasoning across
		  datasets with varying granularities. This paper introduces
		  an ontology-based knowledge graph (KG) approach integrated
		  with a structured text-to-Cypher generation pipeline,
		  enabling accurate retrieval for such queries. Benchmarking
		  against a robust hybrid search baseline combining BM25 and
		  semantic search, our method demonstrates superior
		  performance in addressing multi-hop and cross-granularity
		  questions. Leveraging a KG schema designed to capture
		  intricate relationships (e.g.
		  (OrderLineItem)-[:INVOLVES_ACTION]-&gt;(Action)-[:INVOLVES]-&gt;(BatchProcess)-[:IS_COMPLETED_AT]-&gt;(Location)),
		  we reveal universal patterns for constructing and querying
		  highly relational data. This work highlights the
		  transformative potential of ontology-driven KGs to improve
		  reasoning, data aggregation, and decision-making, with
		  broader implications for any domain requiring structured,
		  multi-relational data analysis.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {4360–4364},
  numpages	= {5},
  keywords	= {hybrid search, knowledge graph, large language model,
		  multi-hop reasoning, natural language processing,
		  ontology-driven retrieval, store fulfillment.,
		  text-to-cypher},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@InProceedings{	  10.1145/3639233.3639234,
  author	= {Okgetheng, Boago and Malema, Gabofetswe},
  title		= {Named Entity Recognition for Setswana Language: A
		  conditional Random Fields (CRF) Approach},
  year		= {2024},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639233.3639234},
  doi		= {10.1145/3639233.3639234},
  abstract	= {Named Entity Recognition (NER) is a fundamental task in
		  Natural Language Processing (NLP) focused on identifying
		  entities like individuals, organizations, and locations
		  within text. Locating these entities can present initial
		  challenges, and subsequent classification can be equally
		  daunting. This complexity is exemplified in Setswana, where
		  shared naming between locations and personal names adds an
		  extra layer of intricacy. This study introduces a Setswana
		  NER approach, featuring a Setswana Regex Annotator (SERxA)
		  for preliminary entity classification, followed by BRAT
		  tool annotation. Employing the Conditional Random Fields
		  (CRF) algorithm, we establish a supervised statistical
		  machine learning NER model for Setswana. Evaluation using
		  standard metrics on a held-out test set attains impressive
		  F1-scores of 0.94 for person entities and 0.79 for location
		  entities. Our findings underscore the viability of NER in
		  Setswana and emphasize the necessity of nurturing NLP
		  resources for less-resourced languages.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {240–244},
  numpages	= {5},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '23}
}

@InProceedings{	  10.1145/3555776.3578739,
  author	= {Ihsan, Ahmad Zainul and Fathalla, Said and Sandfeld,
		  Stefan},
  title		= {DISO: A Domain Ontology for Modeling Dislocations in
		  Crystalline Materials},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3578739},
  doi		= {10.1145/3555776.3578739},
  abstract	= {Crystalline materials, such as metals and semiconductors,
		  nearly always contain a special defect type called
		  dislocation. This defect decisively determines many
		  important material properties, e.g., strength, fracture
		  toughness, or ductility. Over the past years, significant
		  effort has been put into understanding dislocation behavior
		  across different length scales via experimental
		  characterization techniques and simulations. This paper
		  introduces the dislocation ontology (DISO), which defines
		  the concepts and relationships related to linear defects in
		  crystalline materials. We developed DISO using a top-down
		  approach in which we start defining the most general
		  concepts in the dislocation domain and subsequent
		  specialization of them. DISO is published through a
		  persistent URL following W3C best practices for publishing
		  Linked Data. Two potential use cases for DISO are presented
		  to illustrate its usefulness in the dislocation dynamics
		  domain. The evaluation of the ontology is performed in two
		  directions, evaluating the success of the ontology in
		  modeling a real-world domain and the richness of the
		  ontology.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1746–1753},
  numpages	= {8},
  keywords	= {ontology, dislocation, crystallographic defects, linked
		  data, materials science and engineering},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@InProceedings{	  10.1145/3698205.3733952,
  author	= {Priyanka Rani, FNU and Alomair, Maryam and Pan, Shimei and
		  Chen, Lujie K.},
  title		= {Systematically Identifying, Defining and Organizing
		  Knowledge Components for Data Science Problem Solving
		  through Human-LLM Collaboration},
  year		= {2025},
  isbn		= {9798400712913},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698205.3733952},
  doi		= {10.1145/3698205.3733952},
  abstract	= {As demand grows for job-ready data science professionals,
		  there is increasing recognition that traditional training
		  often falls short in cultivating the higher-order reasoning
		  and real-world problem-solving skills essential to the
		  field. A foundational step toward addressing this gap is
		  the identification and organization of knowledge components
		  (KCs) that underlie data science problem solving (DSPS).
		  KCs represent conditional knowledge-knowing about
		  appropriate actions given particular contexts or
		  conditions-and correspond to the critical decisions data
		  scientists must make throughout the problem-solving
		  process. While existing taxonomies in data science
		  education support curriculum development, they often lack
		  the granularity and focus needed to support the assessment
		  and development of DSPS skills. In this paper, we present a
		  novel framework that combines the strengths of large
		  language models (LLMs) and human expertise to identify,
		  define, and organize KCs specific to DSPS. We treat LLMs as
		  ''knowledge engineering assistants'' capable of generating
		  candidate KCs by drawing on their extensive training data,
		  which includes a vast amount of domain knowledge and
		  diverse sets of real-world DSPS cases. Our process involves
		  prompting multiple LLMs to generate decision points,
		  synthesizing and refining KC definitions across models, and
		  using sentence-embedding models to infer the underlying
		  structure of the resulting taxonomy. Human experts then
		  review and iteratively refine the taxonomy to ensure
		  validity. This human-AI collaborative workflow offers a
		  scalable and efficient proof-of-concept for LLM-assisted
		  knowledge engineering. The resulting KC taxonomy lays the
		  groundwork for developing fine-grained assessment tools and
		  adaptive learning systems that support deliberate practice
		  in DSPS. Furthermore, the framework illustrates the
		  potential of LLMs not just as content generators but as
		  partners in structuring domain knowledge to inform
		  instructional design. Future work will involve extending
		  the framework by generating a directed graph of KCs based
		  on their input-output dependencies and validating the
		  taxonomy through expert consensus and learner studies. This
		  approach contributes to both the practical advancement of
		  DSPS coaching in data science education and the broader
		  methodological toolkit for AI-supported knowledge
		  engineering.},
  booktitle	= {Proceedings of the Twelfth ACM Conference on Learning @
		  Scale},
  pages		= {341–345},
  numpages	= {5},
  keywords	= {data science education, data science problem solving,
		  domain analysis, knowledge components, knowledge
		  engineering, large language models, problem solving},
  location	= {Palermo, Italy},
  series	= {L@S '25}
}

@InProceedings{	  10.1145/3589335.3651557,
  author	= {Venkatakrishnan, Radhakrishnan and Tanyildizi, Emrah and
		  Canbaz, M. Abdullah},
  title		= {Semantic interlinking of Immigration Data using LLMs for
		  Knowledge Graph Construction},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651557},
  doi		= {10.1145/3589335.3651557},
  abstract	= {The challenge of managing immigration data is exacerbated
		  by its reliance on paper-based, evidence-driven records
		  maintained by legal professionals, creating obstacles for
		  efficient processing and analysis due to inherent trust
		  issues with AI-based systems. This paper introduces a
		  cutting-edge framework to surmount these hurdles by
		  synergizing Large Language Models (LLMs) with Knowledge
		  Graphs (KGs), revolutionizing traditional data handling
		  methods. Our method transforms archaic, paper-based
		  immigration records into a structured, interconnected
		  knowledge network that intricately mirrors the legal and
		  procedural nuances of immigration, ensuring a dynamic and
		  trustworthy platform for data analysis. Utilizing LLMs, we
		  extract vital entities and relationships from diverse legal
		  documents to forge a comprehensive knowledge graph,
		  encapsulating the complex legalities and procedural
		  disparities in immigration processes and mapping the
		  multifaceted interactions among stakeholders like
		  applicants, sponsors, and legal experts. This graph not
		  only facilitates a deep dive into the legal stipulations
		  but also incorporates them, significantly boosting the
		  system's reliability and precision. With the integration of
		  Retrieval Augmented Generation (RAG) for exact,
		  context-aware data retrieval and Augmented Knowledge
		  Creation for developing a conversational interface via
		  LLMs, our framework offers a scalable, adaptable solution
		  to immigration data management. This innovative
		  amalgamation of LLMs, KGs, and RAG techniques marks a
		  paradigm shift towards more informed, efficient, and
		  trustworthy decision-making in the sphere of global
		  migration, setting a new benchmark for legal technology and
		  data source management.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {605–608},
  numpages	= {4},
  keywords	= {data restructuring, document processing, information
		  retrieval, knowledge graphs, large language models, legal
		  tech},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1145/3610582,
  author	= {Sethi, Nandini and Dev, Amita and Bansal, Poonam and
		  Sharma, Deepak Kumar and Gupta, Deepak},
  title		= {A Pragmatic Analysis of Machine Translation Techniques for
		  Preserving the Authenticity of the Sanskrit Language},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3610582},
  doi		= {10.1145/3610582},
  abstract	= {Machine Translation has been a field of study for over six
		  decades, but it has acquired substantial prominence in the
		  last decade as processing capacity in personal computers
		  has increased. The purpose of this paper is to discuss the
		  usage of Sanskrit as a source, target, or supporting
		  language in various Machine Translation systems. To
		  investigate Machine Translation, researchers use a variety
		  of strategies, including corpus-based, direct, and
		  rule-based approaches. The primary goal of employing
		  Sanskrit in Machine Translation is to evaluate its
		  appropriateness, lexicon, and performance when proper
		  Machine Translation methods are used. The research examines
		  various modelling strategies for developing a machine
		  translation system, specifically Statistical and Neural
		  Machine Translation, in order to bridge the gap between
		  Sanskrit and its current successor, Hindi. Interpretations
		  are formed in Statistical Machine Translation by matching
		  words from the source and target languages with statistical
		  models and bilingual text corpora to learn parameters.
		  Neural Machine Translation, on the other hand, uses an
		  artificial neural network to predict the likelihood of a
		  word sequence, frequently modelling entire phrases within a
		  single integrated model. Neural Machine Translation is
		  implemented using an encoder-decoder architecture with an
		  attention mechanism. One of the most significant
		  contributions of this paper is the use of different data
		  sources, data collecting, and scraping to create a complete
		  dataset. According to the study's findings, Neural Machine
		  Translation outperforms the Statistical Machine Translation
		  modelling technique. Furthermore, the paper examines the
		  distinctive qualities of the Sanskrit language as well as
		  the difficulties encountered by researchers in digesting
		  Sanskrit while constructing the machine translation system.
		  This study investigates the use of Sanskrit in Machine
		  Translation and analyses several modelling methods, such as
		  Statistical and Neural Machine Translation. The paper
		  emphasizes the advantages of Neural Machine Translation and
		  discusses the unique characteristics and challenges of the
		  Sanskrit language in machine translation development.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  keywords	= {Sanskrit, Corpus-Based Machine Translation (CBMT),
		  Bilingual Dictionary, Interlingua, Parallel Corpora,
		  Natural Language Processing (NLP), part of speech (POS)}
}

@InProceedings{	  10.1145/3640310.3674093,
  author	= {Morales, Sergio and Claris\'{o}, Robert and Cabot, Jordi},
  title		= {A DSL for Testing LLMs for Fairness and Bias},
  year		= {2024},
  isbn		= {9798400705045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640310.3674093},
  doi		= {10.1145/3640310.3674093},
  abstract	= {Large language models (LLMs) are increasingly integrated
		  into software systems to enhance them with generative AI
		  capabilities. But LLMs may reflect a biased behavior,
		  resulting in systems that could discriminate against
		  gender, age or ethnicity, among other ethical concerns.
		  Society and upcoming regulations will force companies and
		  development teams to ensure their AI-enhanced software is
		  ethically fair. To facilitate such ethical assessment, we
		  propose LangBiTe, a model-driven solution to specify
		  ethical requirements, and customize and automate the
		  testing of ethical biases in LLMs. The evaluation can raise
		  awareness on the biases of the LLM-based components of the
		  system and/or trigger a change in the LLM of choice based
		  on the requirements of that particular application. The
		  model-driven approach makes both the requirements
		  specification and the test generation platform-independent,
		  and provides end-to-end traceability between the
		  requirements and their assessment. We have implemented an
		  open-source tool set, available on GitHub, to support the
		  application of our approach.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {203–213},
  numpages	= {11},
  keywords	= {Bias, Domain-Specific Language, Ethics, Large Language
		  Models, Model-Driven Engineering, Red Teaming, Testing},
  location	= {Linz, Austria},
  series	= {MODELS '24}
}

@Article{	  10.14778/3636218.3636225,
  author	= {Zhang, Yi and Deriu, Jan and Katsogiannis-Meimarakis,
		  George and Kosten, Catherine and Koutrika, Georgia and
		  Stockinger, Kurt},
  title		= {ScienceBenchmark: A Complex Real-World Benchmark for
		  Evaluating Natural Language to SQL Systems},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {4},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3636218.3636225},
  doi		= {10.14778/3636218.3636225},
  abstract	= {Natural Language to SQL systems (NL-to-SQL) have recently
		  shown improved accuracy (exceeding 80\%) for natural
		  language to SQL query translation due to the emergence of
		  transformer-based language models, and the popularity of
		  the Spider benchmark. However, Spider mainly contains
		  simple databases with few tables, columns, and entries,
		  which do not reflect a realistic setting. Moreover, complex
		  real-world databases with domain-specific content have
		  little to no training data available in the form of
		  NL/SQL-pairs leading to poor performance of existing
		  NL-to-SQL systems.In this paper, we introduce
		  ScienceBenchmark, a new complex NL-to-SQL benchmark for
		  three real-world, highly domain-specific databases. For
		  this new benchmark, SQL experts and domain experts created
		  high-quality NL/SQL-pairs for each domain. To garner more
		  data, we extended the small amount of human-generated data
		  with synthetic data generated using GPT-3. We show that our
		  benchmark is highly challenging, as the top performing
		  systems on Spider achieve a very low performance on our
		  benchmark. Thus, the challenge is many-fold: creating
		  NL-to-SQL systems for highly complex domains with a small
		  amount of hand-made training data augmented with synthetic
		  data. To our knowledge, ScienceBenchmark is the first
		  NL-to-SQL benchmark designed with complex real-world
		  scientific databases, containing challenging training and
		  test data carefully validated by domain experts.},
  journal	= {Proc. VLDB Endow.},
  month		= dec,
  pages		= {685–698},
  numpages	= {14}
}

@InProceedings{	  10.1145/3701716.3715558,
  author	= {Khurana, Udayan and Suneja, Sahil and Samulowitz, Horst},
  title		= {Table Retrieval using LLMs and Semantic Table Similarity},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715558},
  doi		= {10.1145/3701716.3715558},
  abstract	= {Searching for relevant tables in response to a textual
		  phrase or a question is an important task for large tabular
		  data repositories, such as relational databases, CSV files
		  in data lakes, etc. It is somewhat different from the
		  problem of web document search because the subjects of
		  search are tables rather than documents, while the query
		  remains textual. In this paper, we explore a novel
		  technique for table search on large repositories using
		  natural language queries. It is based on a generative
		  methodology that aims to maximize the semantic connection
		  between the query and the resulting tables. Unlike
		  traditional keyword search approaches, our technique can
		  find the needed tables more effectively through deeper
		  semantic concept discovery rather than simply searching for
		  exact keyword matches. Additionally, our technique supports
		  natural language queries rather than plain keyword queries.
		  In this paper, we describe the core ideas, implementation,
		  and effectiveness of our method using two different
		  benchmarks with diverse queries.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {1072–1076},
  numpages	= {5},
  keywords	= {generative ai, large language models, semantic similarity,
		  structured data search, table retrieval},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3705328.3748100,
  author	= {Zhu, Sinan and Simonovikj, Sanja and Edmonds, Darren and
		  Sun, Yang},
  title		= {Metadata Generation and Evaluation using LLMs - Case Study
		  on Canonical Titles},
  year		= {2025},
  isbn		= {9798400713644},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3705328.3748100},
  doi		= {10.1145/3705328.3748100},
  abstract	= {In online job search platforms, autocomplete plays a
		  crucial role in providing immediate, structured suggestions
		  that guide users through their query process. However,
		  inconsistencies in job title expressions, such as ’sr
		  data scientist’ versus ’data scientist senior’, or
		  embellished forms such as ’superstar software
		  engineer’, can undermine the quality of autocomplete
		  suggestions and diminish user satisfaction. Traditional
		  normalization methods rely on manually curated
		  vocabularies, which are labor intensive and often
		  insufficient to capture the diverse variations in raw job
		  titles.We present an automated and scalable framework for
		  canonical title generation that leverages large language
		  models (LLMs) alongside embedding-based similarity measures
		  to derive normalized job titles directly from raw data. Our
		  approach generalizes to domains with unstructured or
		  inconsistently formatted titles (e.g. product catalogs or
		  course titles): we systematically remove irrelevant
		  information, enforce a consistent format, and eliminate
		  overly generic or redundant titles by combining
		  LLM-generated normalization with a two-stage deduplication
		  process. Our method demonstrates significant improvements
		  in normalization quality, with offline accuracy gains of
		  18.6\% over baseline methods and online A/B tests showing
		  an improvement of 160\% in user engagement metrics.},
  booktitle	= {Proceedings of the Nineteenth ACM Conference on
		  Recommender Systems},
  pages		= {1010–1013},
  numpages	= {4},
  keywords	= {canonical job titles, occupation taxonomy, data cleaning
		  with LLMs},
  location	= { },
  series	= {RecSys '25}
}

@InProceedings{	  10.1145/3637528.3671857,
  author	= {Ouyang, Siru and Huang, Jiaxin and Pillai, Pranav and
		  Zhang, Yunyi and Zhang, Yu and Han, Jiawei},
  title		= {Ontology Enrichment for Effective Fine-grained Entity
		  Typing},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671857},
  doi		= {10.1145/3637528.3671857},
  abstract	= {Fine-grained entity typing (FET) is the task of
		  identifying specific entity types at a fine-grained level
		  for entity mentions based on their contextual information.
		  Conventional methods for FET require extensive human
		  annotation, which is time-consuming and costly given the
		  massive scale of data. Recent studies have been developing
		  weakly supervised or zero-shot approaches. We study the
		  setting of zero-shot FET where only an ontology is
		  provided. However, most existing ontology structures lack
		  rich supporting information and even contain ambiguous
		  relations, making them ineffective in guiding FET. Recently
		  developed language models, though promising in various
		  few-shot and zero-shot NLP tasks, may face challenges in
		  zero-shot FET due to their lack of interaction with
		  task-specific ontology. In this study, we propose \o{}urs,
		  where we (1) enrich each node in the ontology structure
		  with two categories of extra information:instance
		  information for training sample augmentation andtopic
		  information to relate types with contexts, and (2) develop
		  a coarse-to-fine typing algorithm that exploits the
		  enriched information by training an entailment model with
		  contrasting topics and instance-based augmented training
		  samples. Our experiments show that \o{}urs achieves
		  high-quality fine-grained entity typing without human
		  annotation, outperforming existing zero-shot methods by a
		  large margin and rivaling supervised methods. \o{}urs also
		  enjoys strong transferability to unseen and finer-grained
		  types. We will open source this work upon acceptance.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2318–2327},
  numpages	= {10},
  keywords	= {fine-grained entity typing, language models, natural
		  language inference, zero-shot learning},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3736229.3736261,
  author	= {Almuntashiri, Abdullah Hamed and Ib\'{a}\~{n}ez,
		  Luis-Daniel and Chapman, Adriane},
  title		= {Using LLMs to infer provenance information},
  year		= {2025},
  isbn		= {9798400719417},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3736229.3736261},
  doi		= {10.1145/3736229.3736261},
  abstract	= {Having a provenance record facilitates data reuse and
		  experimental reuse. However, provenance capture requires
		  either: specific provenance-enabled systems to be used or
		  human documentation. While there have been many examples of
		  provenance-enabled systems for scientific usage, they are
		  still the exception, not the norm. The one, standard place
		  for provenance information of scientific experiments
		  remains the scientific publication. Unfortunately,
		  provenance buried in text is not immediately useful for
		  computational purposes. Large Language Models (LLMs) have
		  demonstrated exceptional capability across various tasks,
		  particularly in information extraction. In this paper, we
		  explore the potential of LLMs to infer a provenance record
		  for scientific experiments from scientific papers. We
		  develop an extractor, identify the most effective prompt
		  for provenance extraction. Our results emphasise the
		  capability of ChatGPT-4o in accessing and extracting
		  provenance information from biomedical research papers.
		  Additionally, we assess the scalability of the extractor
		  for use in extracting provenance information across a set
		  of biomedical research papers.},
  booktitle	= {Proceedings of the ProvenanceWeek 2025},
  pages		= {1–10},
  numpages	= {10},
  keywords	= {Dataset Search, Provenance, Information Extraction, LLMs},
  location	= { },
  series	= {PW' 25}
}

@Article{	  10.1145/3748323,
  author	= {Al-Thubaity, AbdulMohsen},
  title		= {A Novel Dataset for Arabic Domain Specific Term Extraction
		  and Comparative Evaluation of BERT-Based Models for Arabic
		  Term Extraction},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3748323},
  doi		= {10.1145/3748323},
  abstract	= {Automatic term extraction from domain-specific corpora is
		  a well-known challenge in natural language processing, with
		  applications in machine translation, information retrieval,
		  text classification, ontology building, and thesaurus
		  construction. Unlike English, where various approaches have
		  been explored, Arabic automatic term extraction has relied
		  heavily on rule-based or statistical methods due to the
		  lack of annotated datasets. This paper introduces the first
		  annotated dataset for Arabic automatic term extraction
		  (AraATE) in the field of Arabic linguistics. AraATE
		  comprises 4,148 sentences and 155,502 tokens, annotated
		  with 4,362 single and multi-word Arabic linguistic terms.
		  The dataset covers diverse areas of Arabic linguistics,
		  including lexicography, semantics, pragmatics, phonetics,
		  and semiotics. Additionally, this paper presents the
		  results of fine-tuning five BERT-based models using AraATE.
		  The findings indicate that AraBERTv0.2-base, CAMeLBERT-MSA,
		  and AraBERTv0.2-large exhibit comparable F1 scores (0.82,
		  0.81, and 0.81). However, no statistically significant
		  difference was observed in the performance of these models.
		  The availability of AraATE will facilitate Arabic term
		  extraction by serving as a benchmarking dataset for
		  different approaches. Nevertheless, the field still
		  requires additional benchmarking datasets that cover other
		  domains.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  keywords	= {Arabic term extraction, BERT, fine-tuning language models,
		  information extraction, and natural language processing}
}

@InProceedings{	  10.1145/3648050.3648051,
  author	= {Wang, Shengyao},
  title		= {Research on Natural Language Recognition Processing System
		  in Computer Intelligent Graphics},
  year		= {2024},
  isbn		= {9798400708251},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3648050.3648051},
  doi		= {10.1145/3648050.3648051},
  abstract	= {This paper proposes a natural language processing method
		  based on grammar rule matching. Then the construction model
		  of NLP system based on this algorithm is given. Set up an
		  interactive editing environment to compress and encode the
		  recorded information and upload it to the server. After
		  cross-editing, it is input into the embedded system to
		  complete the identification process and then achieve the
		  identification of the session semantic block, the
		  identification of the session topic, and the extraction of
		  the session content. The proposed method is tested
		  systematically, and the results show that the accuracy of
		  the proposed method reaches 93\%.},
  booktitle	= {Proceedings of the 2023 International Conference on AI and
		  Metaverse in Supply Chain Management},
  articleno	= {24},
  numpages	= {6},
  keywords	= {Embedded system, Grammar rule matching, Natural language,
		  Parameter extraction, Speech recognition},
  location	= {Bhubaneswar, India},
  series	= {AIMSCM '23}
}

@Article{	  10.1145/3617993,
  author	= {Xu, Jin},
  title		= {Research on the Diversification of Language Ability in
		  Applied Linguistics and Foreign Linguistics Based on New
		  Media},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3617993},
  doi		= {10.1145/3617993},
  abstract	= {In order to improve the research effect of the
		  diversification of language ability in applied linguistics
		  and foreign linguistics, this paper conducts research on
		  the diversification of language ability in applied
		  linguistics and foreign linguistics based on new media and
		  intelligent data processing technology. In the process of
		  language learning data processing, combining the adjacency
		  degree and the network structure entropy, this paper
		  proposes the adjacency structure entropy based on the super
		  network to identify the key nodes in the super network.
		  Moreover, this paper applies this indicator to the
		  competitive hypernetwork, and accurately obtains the key
		  nodes of the hypernetwork. In addition, this paper
		  constructs a scientific research cooperation hypernetwork,
		  and applies the adjacency structure entropy in the
		  hypernetwork to this dataset. From the experimental
		  research, it can be seen that the algorithm proposed in
		  this paper can play a certain role in the diversification
		  analysis of linguistic ability in linguistics and foreign
		  linguistics. At the same time, the model proposed in this
		  paper has a certain effect on the diversification of
		  language ability in applied linguistics and foreign
		  linguistics.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= sep,
  keywords	= {new media, applied linguistics, foreign linguistics,
		  language ability, diversification}
}

@InProceedings{	  10.1145/3589132.3625600,
  author	= {Liu, Mengyi and Wang, Xieyang and Xu, Jianqiu and Lu,
		  Hua},
  title		= {NALSpatial: An Effective Natural Language Transformation
		  Framework for Queries over Spatial Data},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589132.3625600},
  doi		= {10.1145/3589132.3625600},
  abstract	= {Spatial databases play a vital role in many applications
		  that access spatial data via appropriate queries. However,
		  most application users lack the expertise necessary for
		  formulating spatial queries. To fill in this gap, we
		  propose an effective framework called NALSpatial that
		  translates natural language queries over spatial data into
		  executable database queries. NALSpatial consists of two
		  core phases. The natural language understanding phase
		  extracts key entity information, comprehends the query
		  intent and determines the query type. The key entities and
		  query type are passed to the subsequent natural language
		  translation phase, which employs entity mapping rules and
		  structured language models to construct executable database
		  queries accordingly. We implement NALSpatial on the
		  open-source extensible database system SECONDO to support
		  range queries, nearest neighbor queries, spatial joins and
		  aggregation queries. Extensive experiments show that
		  NALSpatial on average achieves response time of about 2.5
		  seconds, translatability of 95\% and translation precision
		  of 92\%, outperforming state-of-the-art natural language
		  transformation methods.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {57},
  numpages	= {4},
  keywords	= {spatial database, natural language interface, semantic
		  parsing, query processing},
  location	= {Hamburg, Germany},
  series	= {SIGSPATIAL '23}
}

@InProceedings{	  10.1145/3587828.3587840,
  author	= {Nordin, Noratikah and Zainol, Zurinahni and Mohd Noor,
		  Mohd Halim and Chan, Lai Fong},
  title		= {An Ontology-based Modeling for Classifying Risk of
		  Suicidal Behavior},
  year		= {2023},
  isbn		= {9781450398589},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587828.3587840},
  doi		= {10.1145/3587828.3587840},
  abstract	= {Classifying an individual with suicidal behavior is a
		  complex problem. A clinical decision support system (CDSS)
		  helps medical experts in their daily work and supports them
		  in effective decision-making. The huge amount of medical
		  information and the complex correlation between the risk
		  factors and the level of risk for suicidal behavior makes
		  the representation of data is challenging. Therefore, this
		  paper proposes an ontology-based modeling to classify an
		  individual with at-risk of suicidal behavior for effective
		  clinical decision support system. The case study is
		  conducted to evaluate the ontology model and provides a
		  general approach to knowledge sharing and reusing knowledge
		  for suicide risk prevention and management. The finding
		  shows that the ontology model can be used as a knowledge
		  base for classification, and it is suitable to capture
		  medical knowledge, detailed concepts, and relationships in
		  a formal way using Web Ontology Language (OWL). The results
		  of the proposed ontology model in terms of accuracy,
		  specificity, and sensitivity are 83\%, 84\%, and 82\%
		  respectively.},
  booktitle	= {Proceedings of the 2023 12th International Conference on
		  Software and Computer Applications},
  pages		= {71–76},
  numpages	= {6},
  keywords	= {Clinical Decision Support System, Data Mining, Knowledge
		  Base, Ontological Model, Suicidal Behavior},
  location	= {Kuantan, Malaysia},
  series	= {ICSCA '23}
}

@InProceedings{	  10.1145/3492547.3492612,
  author	= {A. Abdelnabi, Esra and M. Maatuk, Abdelsalam and M.
		  Abdelaziz, Tawfig},
  title		= {An Algorithmic Approach for Generating Behavioral UML
		  Models Using Natural Language Processing},
  year		= {2021},
  isbn		= {9781450390446},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3492547.3492612},
  doi		= {10.1145/3492547.3492612},
  abstract	= {The process of transformation from informal requirements
		  stated in natural language into a formal specification such
		  as Unified Modeling Language (UML) is an important
		  challenge. User requirements that are expressed in natural
		  language can be very problematic, which makes the
		  requirements analysis a difficult task. In this paper, we
		  propose a method to analyze the natural language
		  requirements and generate sequence and collaboration
		  diagrams from these requirements, which are commonly used
		  to describe the behavior of software systems. A case study
		  was accomplished to compare the diagrams generated by the
		  proposed approach to the diagrams produced by other
		  approaches. The results showed that the elements of the
		  sequence and collaboration diagrams extracted through our
		  approach are very satisfactory and they would be acceptable
		  as initial analysis models.},
  booktitle	= {The 7th International Conference on Engineering \&amp; MIS
		  2021},
  articleno	= {36},
  numpages	= {6},
  keywords	= {NLP tools, Requirement analysis, Sequence and
		  Collaboration diagrams, UML diagrams},
  location	= {Almaty, Kazakhstan},
  series	= {ICEMIS'21}
}

@InProceedings{	  10.1145/3652988.3673932,
  author	= {Steenstra, Ian and Nouraei, Farnaz and Arjmand, Mehdi and
		  Bickmore, Timothy},
  title		= {Virtual Agents for Alcohol Use Counseling: Exploring
		  LLM-Powered Motivational Interviewing},
  year		= {2024},
  isbn		= {9798400706257},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652988.3673932},
  doi		= {10.1145/3652988.3673932},
  abstract	= {We introduce a novel application of large language models
		  (LLMs) in developing a virtual counselor capable of
		  conducting motivational interviewing (MI) for alcohol use
		  counseling. Access to effective counseling remains limited,
		  particularly for substance abuse, and virtual agents offer
		  a promising solution by leveraging LLM capabilities to
		  simulate nuanced communication techniques inherent in MI.
		  Our approach combines prompt engineering and integration
		  into a user-friendly virtual platform to facilitate
		  realistic, empathetic interactions. We evaluate the
		  effectiveness of our virtual agent through a series of
		  studies focusing on replicating MI techniques and human
		  counselor dialog. Initial findings suggest that our
		  LLM-powered virtual agent matches human counselors’
		  empathetic and adaptive conversational skills, presenting a
		  significant step forward in virtual health counseling and
		  providing insights into the design and implementation of
		  LLM-based therapeutic interactions.},
  booktitle	= {Proceedings of the 24th ACM International Conference on
		  Intelligent Virtual Agents},
  articleno	= {20},
  numpages	= {10},
  keywords	= {Alcohol Use Counseling, Embodied Conversational Agents,
		  Intelligent Virtual Agents, Large Language Models,
		  Motivational Interviewing, Persuasive Technology},
  location	= {GLASGOW, United Kingdom},
  series	= {IVA '24}
}

@InProceedings{	  10.1145/3711403.3711485,
  author	= {Zhao, Wei and Nie, Zhenhua and Li, Xiaoming},
  title		= {Teaching Practice of C Language Programming under the
		  Constructivist Learning Paradigm},
  year		= {2025},
  isbn		= {9798400717468},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711403.3711485},
  doi		= {10.1145/3711403.3711485},
  abstract	= {The construction of new engineering disciplines and the
		  “Double Tops” initiative have put forward new
		  requirements for the training of engineering professionals.
		  Taking the course of C language programming as an example,
		  this paper analyzes the pain points in current course
		  teaching, combines the characteristics of students’
		  learning situation, takes students as the center, and based
		  on the knowledge graph, an effective tool for organizing
		  resources, carries out a modular teaching practice of C
		  language programming design under the constructivist
		  learning paradigm. Integrating the OBE (Outcomes-Based
		  Education) concept into teaching, it enhances students’
		  practical ability, cultivates their computational thinking,
		  and highlights the training of students’ ability to
		  analyze and solve complex engineering problems and creative
		  thinking. The exploration and practice of this course
		  teaching can provide reference and reference for the
		  teaching design of other programming courses.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Educational Technology Management},
  pages		= {504–508},
  numpages	= {5},
  keywords	= {Program design, constructivist learning, teaching mode,
		  teaching reform and practice},
  location	= { },
  series	= {ICETM '24}
}

@InProceedings{	  10.1145/3613905.3644062,
  author	= {Gould, Sandy J. J. and Brumby, Duncan P. and Cox, Anna
		  L.},
  title		= {ChatTL;DR – You Really Ought to Check What the LLM Said
		  on Your Behalf},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3644062},
  doi		= {10.1145/3613905.3644062},
  abstract	= {Interactive large language models (LLMs) are so hot right
		  now, and are probably going to be hot for a while. There
		  are lots of problems exciting challenges created by mass
		  use of LLMs. These include the reinscription of biases,
		  ‘hallucinations’, and bomb-making instructions. Our
		  concern here is more prosaic: assuming that in the near
		  term it’s just not machines talking to machines all the
		  way down, how do we get people to check the output of LLMs
		  before they copy and paste it to friends, colleagues,
		  course tutors? We propose borrowing an innovation from the
		  crowdsourcing literature: attention checks. These checks
		  (e.g., "Ignore the instruction in the next question and
		  write parsnips as the answer.") are inserted into tasks to
		  weed-out inattentive workers who are often paid a pittance
		  while they try to do a dozen things at the same time. We
		  propose ChatTL;DR1, an interactive LLM that inserts
		  attention checks into its outputs. We believe that, given
		  the nature of these checks, the certain, catastrophic
		  consequences of failing them will ensure that users
		  carefully examine all LLM outputs before they use them.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {552},
  numpages	= {7},
  keywords	= {LLMs, Large Language Models, academics being hilarious,
		  attention checks, checking behaviour,
		  computers-talking-to-computers-all-the-way-down-circlejerk,
		  error detection, human factors, instructional manipulation
		  checks, that-bloody-automatic-lane-assist-ffs},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@InProceedings{	  10.1145/3643479.3662055,
  author	= {Bui, Tuan and Tran, Oanh and Nguyen, Phuong and Ho, Bao
		  and Nguyen, Long and Bui, Thang and Quan, Tho},
  title		= {Cross-Data Knowledge Graph Construction for LLM-enabled
		  Educational Question-Answering System: A Case Study at
		  HCMUT},
  year		= {2024},
  isbn		= {9798400705472},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643479.3662055},
  doi		= {10.1145/3643479.3662055},
  abstract	= {In today's rapidly evolving landscape of Artificial
		  Intelligence, large language models (LLMs) have emerged as
		  a vibrant research topic. LLMs find applications in various
		  fields and contribute significantly. Despite their powerful
		  language capabilities, similar to pre-trained language
		  models (PLMs), LLMs still face challenges in remembering
		  events, incorporating new information, and addressing
		  domain-specific issues or hallucinations. To overcome these
		  limitations, researchers have proposed Retrieval-Augmented
		  Generation (RAG) techniques, some others have proposed the
		  integration of LLMs with Knowledge Graphs (KGs) to provide
		  factual context, thereby improving performance and
		  delivering more accurate feedback to user queries.Education
		  plays a crucial role in human development and progress.
		  With the technology transformation, traditional education
		  is being replaced by digital or blended education.
		  Therefore, educational data in the digital environment is
		  increasing day by day. Data in higher education
		  institutions are diverse, comprising various sources such
		  as unstructured/structured text, relational databases,
		  web/app-based API access, etc. Constructing a Knowledge
		  Graph from these cross-data sources is not a simple task.
		  This article proposes a method for automatically
		  constructing a Knowledge Graph from multiple data sources
		  and discusses some initial applications (experimental
		  trials) of KG in conjunction with LLMs for
		  question-answering tasks.},
  booktitle	= {Proceedings of the 1st ACM Workshop on AI-Powered Q&amp;A
		  Systems for Multimedia},
  pages		= {36–43},
  numpages	= {8},
  keywords	= {Education, Knowledge Graph, Large language model, Open
		  Intent Discovery, Question-Answering System},
  location	= {Phuket, Thailand},
  series	= {AIQAM '24}
}

@InProceedings{	  10.1145/3726302.3731950,
  author	= {Zheng, Qi and Zhong, Mingjie and Gong, Saisai and Jiang,
		  Huimin and Wu, Kaixin and Liu, Hong and Xu, Jia and Mo,
		  Linjian},
  title		= {MAAQR: An LLM-based Multi-Agent Framework for Adaptive
		  Query Rewriting in Alipay Search},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3731950},
  doi		= {10.1145/3726302.3731950},
  abstract	= {Query rewriting is essential in e-commerce search, as it
		  bridges the lexical gap between user queries and item
		  descriptions, thereby enhancing search performance. Despite
		  recent advancements, current rewriting approaches are still
		  limited by an inadequate comprehension of domain-specific
		  knowledge and a lack of mechanisms for adaptive refinement
		  in response to new or changing query-item relationships. To
		  overcome these limitations, we propose a large language
		  model (LLM) based Multi-Agent Framework for Adaptive Query
		  Rewriting (MAAQR) in Alipay Search. Initially, we perform
		  knowledge-enhanced fine-tuning to improve the LLM's
		  understanding of query and item semantics. Subsequently, a
		  multi-agent collaborative rewriting architecture is
		  employed to enhance rewrite quality and adaptability. MAAQR
		  has been successfully deployed to serve Alipay's mini-app
		  search since December 2024. Through offline experiments and
		  online A/B testing, MAAQR significantly improves
		  click-through rates (CTR) and the number of transactions
		  for target queries, while substantially reducing the
		  zero-results rate (ZRR).},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {4289–4293},
  numpages	= {5},
  keywords	= {information retrieval, large language models, multi-agent,
		  query rewriting},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@Article{	  10.1145/3732784,
  author	= {Xie, Shuyi and Yao, Wenlin and Dai, Yong and Wang, Shaobo
		  and Xu, Zishan and Lin, Fan and Zhou, Donglin and Jin,
		  Lifeng and Feng, Xinhua and Wei, Pengzhi and Lin, Yujie and
		  Hu, Zhichao and Yu, Dong and Zhang, Zhengyou and Nie, Jing
		  and Liu, Yuhong},
  title		= {TencentLLMEval: A Hierarchical Evaluation of Real-World
		  Capabilities for Human-Aligned LLMs},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3732784},
  doi		= {10.1145/3732784},
  abstract	= {Large language models (LLMs) have shown impressive
		  capabilities across various natural language tasks.
		  However, evaluating their alignment with human preferences
		  remains a challenge. To this end, we propose a
		  comprehensive human evaluation framework to assess LLMs’
		  proficiency in following instructions on diverse real-world
		  tasks. We construct a hierarchical task tree encompassing 7
		  major areas covering over 200 categories and over 800
		  tasks, which covers diverse capabilities such as question
		  answering, reasoning, multiturn dialogue, and text
		  generation, to evaluate LLMs in a comprehensive and
		  in-depth manner. We also design detailed evaluation
		  standards and processes to facilitate consistent, unbiased
		  judgments from human evaluators. A test set of over 3,000
		  instances is released, spanning different difficulty levels
		  and knowledge domains. Our work provides a standardized
		  methodology to evaluate human alignment in LLMs for both
		  English and Chinese. We also analyze the feasibility of
		  automating parts of evaluation with a strong LLM (GPT-4).
		  Our framework supports a thorough assessment of LLMs as
		  they are integrated into real-world applications. We have
		  made publicly available the task tree, TencentLLMEval
		  dataset, and evaluation methodology which have been
		  demonstrated as effective in assessing the performance of
		  Tencent Hunyuan LLMs 1. By doing so, we aim to facilitate
		  the benchmarking of advances in the development of safe and
		  human-aligned LLMs.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= apr,
  keywords	= {human-aligned, LLMs, Evaluation}
}

@InProceedings{	  10.1145/3625007.3631503,
  author	= {Abrouk, Lylia and Chergui, Hamza and Ahaggach, Hamid},
  title		= {OntoFiC : an ontology for financial fraud detection and
		  customer behavior modeling},
  year		= {2024},
  isbn		= {9798400704093},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625007.3631503},
  doi		= {10.1145/3625007.3631503},
  abstract	= {Fraud detection is a complex issue for financial
		  institutions. They must have tools for the prevention and
		  detection of fraud. In this article, we present our
		  approach to detect fraudulent transactions in SWIFT network
		  based on the domain ontology. Firstly, we present the
		  OntoFiC ontology constructed for the modeling of SWIFT
		  transactions and actors. This ontology is populated with a
		  real dataset. We developed our rules-based approach with
		  rules associated to fraud scenarios to label our
		  transactions as legitimate or fraudulent. Finally, we made
		  SPARQL requests to visualize these transactions through
		  graphs. Our work is part of a collaboration project with a
		  financial company, SKAIZen Group.},
  booktitle	= {Proceedings of the 2023 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {509–513},
  numpages	= {5},
  keywords	= {fraud detection, SWIFT network, domain ontology, ontology
		  population, rules-based approach, SPARQL},
  location	= {Kusadasi, Turkiye},
  series	= {ASONAM '23}
}

@InProceedings{	  10.1145/3600160.3605069,
  author	= {Castiglione, Gianpietro and Bella, Giampaolo and
		  Santamaria, Daniele Francesco},
  title		= {Towards Grammatical Tagging for the Legal Language of
		  Cybersecurity},
  year		= {2023},
  isbn		= {9798400707728},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600160.3605069},
  doi		= {10.1145/3600160.3605069},
  abstract	= {Legal language can be understood as the language typically
		  used by those engaged in the legal profession and, as such,
		  it may come both in spoken or written form. Recent
		  legislation on cybersecurity obviously uses legal language
		  in writing, thus inheriting all its interpretative
		  complications due to the typical abundance of cases and
		  sub-cases as well as to the general richness in detail.
		  This paper faces the challenge of the essential
		  interpretation of the legal language of cybersecurity,
		  namely of the extraction of the essential Parts of Speech
		  (POS) from the legal documents concerning cybersecurity.
		  The challenge is overcome by our methodology for POS
		  tagging of legal language. It leverages state-of-the-art
		  open-source tools for Natural Language Processing (NLP) as
		  well as manual analysis to validate the outcomes of the
		  tools. As a result, the methodology is automated and,
		  arguably, general for any legal language following minor
		  tailoring of the preprocessing step. It is demonstrated
		  over the most relevant EU legislation on cybersecurity,
		  namely on the NIS 2 directive, producing the first, albeit
		  essential, structured interpretation of such a relevant
		  document. Moreover, our findings indicate that tools such
		  as SpaCy and ClausIE reach their limits over the legal
		  language of the NIS 2.},
  booktitle	= {Proceedings of the 18th International Conference on
		  Availability, Reliability and Security},
  articleno	= {82},
  numpages	= {9},
  keywords	= {Act, Data Protection, NLP, POS tagging, Privacy,
		  Pronouncement},
  location	= {Benevento, Italy},
  series	= {ARES '23}
}

@Article{	  10.1145/3643505,
  author	= {King, Evan and Yu, Haoxiang and Lee, Sangsu and Julien,
		  Christine},
  title		= {Sasha: Creative Goal-Oriented Reasoning in Smart Homes
		  with Large Language Models},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {1},
  url		= {https://doi.org/10.1145/3643505},
  doi		= {10.1145/3643505},
  abstract	= {Smart home assistants function best when user commands are
		  direct and well-specified---e.g., "turn on the kitchen
		  light"---or when a hard-coded routine specifies the
		  response. In more natural communication, however, human
		  speech is unconstrained, often describing goals (e.g.,
		  "make it cozy in here" or "help me save energy") rather
		  than indicating specific target devices and actions to take
		  on those devices. Current systems fail to understand these
		  under-specified commands since they cannot reason about
		  devices and settings as they relate to human situations. We
		  introduce large language models (LLMs) to this problem
		  space, exploring their use for controlling devices and
		  creating automation routines in response to under-specified
		  user commands in smart homes. We empirically study the
		  baseline quality and failure modes of LLM-created action
		  plans with a survey of age-diverse users. We find that LLMs
		  can reason creatively to achieve challenging goals, but
		  they experience patterns of failure that diminish their
		  usefulness. We address these gaps with Sasha, a smarter
		  smart home assistant. Sasha responds to loosely-constrained
		  commands like "make it cozy" or "help me sleep better" by
		  executing plans to achieve user goals---e.g., setting a
		  mood with available devices, or devising automation
		  routines. We implement and evaluate Sasha in a hands-on
		  user study, showing the capabilities and limitations of
		  LLM-driven smart homes when faced with unconstrained
		  user-generated scenarios.},
  journal	= {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month		= mar,
  articleno	= {12},
  numpages	= {38},
  keywords	= {ambient intelligence, large language models, pervasive
		  computing, smart environments}
}

@InProceedings{	  10.1145/3744915.3748460,
  author	= {Zine, Nada and Quinton, Cl\'{e}ment and Rouvoy, Romain},
  title		= {LLM-based Co-Evolution of Configurable Software Systems},
  year		= {2025},
  isbn		= {9798400720246},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3744915.3748460},
  doi		= {10.1145/3744915.3748460},
  abstract	= {Software Product Lines (SPLs) and s are de&nbsp;facto
		  standards for managing variability in software systems.
		  However, maintaining an up-to-date during software
		  evolution is particularly challenging. Ensuring its
		  consistency with the artifacts of an SPL requires
		  co-evolving them alongside the developed system. When
		  performed manually, this co-evolution process is tedious
		  and error-prone, highlighting the need for automated
		  support. Yet, little attention has been given to the
		  automation of co-evolution between and source code. In this
		  paper, we explore the potential of open-source s to fill
		  this gap. Specifically, we investigate the extent to which
		  s can support bidirectional co-evolution: from to source
		  code—where modifications in the drive changes in the
		  code—and from source code to —where updates in the code
		  are reflected back into the. We evaluate our -based
		  approach on a real-world configurable system. Our results
		  demonstrate that co-evolution from source code to achieves
		  F1 scores ranging from 0.93 to 1.0, while co-evolution from
		  to source code achieves F1 scores between 0.41 and 0.99.
		  These findings highlight the potential of s to support this
		  co-evolution process, while also showing limitations and
		  suggesting areas for improvement, particularly for the
		  co-evolution from to code. Additionally, we conduct a
		  comparative study across various s, revealing how choice
		  affects co-evolution and, incidentally, how it affects
		  model and code generation. Up to a certain size limit,
		  larger s tend to produce more accurate and stable outputs
		  than smaller ones, however, this influence is less
		  pronounced in the code generation task. Overall, our work
		  opens a new research avenue where s are leveraged for
		  automating the co-evolution between configurable software
		  systems and variability models.},
  booktitle	= {Proceedings of the 29th ACM International Systems and
		  Software Product Line Conference - Volume A},
  pages		= {27–38},
  numpages	= {12},
  keywords	= {Software Product Lines, Large Language Models, Feature
		  Models, Co-evolution},
  location	= { },
  series	= {SPLC-A '25}
}

@InProceedings{	  10.1145/3486608.3486907,
  author	= {Barash, Mikhail},
  title		= {Vision: the next 700 language workbenches},
  year		= {2021},
  isbn		= {9781450391115},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486608.3486907},
  doi		= {10.1145/3486608.3486907},
  abstract	= {Language workbenches (LWBs) are tools to define software
		  languages together with tailored Integrated Development
		  Environments for them. A comprehensive review of language
		  workbenches by Erdweg et al. (Comput. Lang. Syst. Struct.
		  44, 2015) presented a feature model of functionality of
		  LWBs from the point of view of "languages that can be
		  defined with a LWB, and not the definition mechanism of the
		  LWB itself". This vision paper discusses possible
		  functionality of LWBs with regard to language definition
		  mechanisms. We have identified five groups of such
		  functionality, related to: metadefinitions,
		  metamodifications, metaprocess, LWB itself, and programs
		  written in languages defined in a LWB. We design one of the
		  features ("ability to define dependencies between language
		  concerns") based on our vision.},
  booktitle	= {Proceedings of the 14th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  pages		= {16–21},
  numpages	= {6},
  keywords	= {Language workbenches, algebraic specifications,
		  metaprogramming, software languages},
  location	= {Chicago, IL, USA},
  series	= {SLE 2021}
}

@Article{	  10.14778/3632093.3632111,
  author	= {Singh, Mukul and Cambronero, Jos\'{e} and Gulwani, Sumit
		  and Le, Vu and Negreanu, Carina and Nouri, Elnaz and Raza,
		  Mohammad and Verbruggen, Gust},
  title		= {FormaT5: Abstention and Examples for Conditional Table
		  Formatting with Natural Language},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {3},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3632093.3632111},
  doi		= {10.14778/3632093.3632111},
  abstract	= {Formatting is an important property in tables for
		  visualization, presentation, and analysis. Spreadsheet
		  software allows users to automatically format their tables
		  by writing data-dependent conditional formatting (CF)
		  rules. Writing such rules is often challenging for users as
		  it requires understanding and implementing the underlying
		  logic. We present FormaT5, a transformer-based model that
		  can generate a CF rule given the target table and a natural
		  language description of the desired formatting logic. We
		  find that user descriptions for these tasks are often
		  under-specified or ambiguous, making it harder for code
		  generation systems to accurately learn the desired rule in
		  a single step. To tackle this problem of
		  under-specification and minimise argument errors, FormaT5
		  learns to predict placeholders though an abstention
		  objective. These placeholders can then be filled by a
		  second model or, when examples of rows that should be
		  formatted are available, by a programming-by-example
		  system. To evaluate FormaT5 on diverse and real scenarios,
		  we create an extensive benchmark of 1053 CF tasks,
		  containing real-world descriptions collected from four
		  different sources. We release our benchmarks to encourage
		  research in this area. Abstention and filling allow FormaT5
		  to outperform 8 different neural approaches on our
		  benchmarks, both with and without examples. Our results
		  illustrate the value of building domain-specific learning
		  systems.},
  journal	= {Proc. VLDB Endow.},
  month		= nov,
  pages		= {497–510},
  numpages	= {14}
}

@InProceedings{	  10.1145/3543873.3587601,
  author	= {Amith, Muhammad and Cui, Licong and Roberts, Kirk and Tao,
		  Cui},
  title		= {Application of an ontology for model cards to generate
		  computable artifacts for linking machine learning
		  information from biomedical research},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587601},
  doi		= {10.1145/3543873.3587601},
  abstract	= {Model card reports provide a transparent description of
		  machine learning models which includes information about
		  their evaluation, limitations, intended use, etc. Federal
		  health agencies have expressed an interest in model cards
		  report for research studies using machine-learning based
		  AI. Previously, we have developed an ontology model for
		  model card reports to structure and formalize these
		  reports. In this paper, we demonstrate a Java-based library
		  (OWL API, FaCT++) that leverages our ontology to publish
		  computable model card reports. We discuss future directions
		  and other use cases that highlight applicability and
		  feasibility of ontology-driven systems to support FAIR
		  challenges.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {820–825},
  numpages	= {6},
  keywords	= {FAIR, artificial intelligence, description logic, document
		  engineering, inference, machine learning, model card
		  reports, ontology, semantic web, transparency},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@Article{	  10.1145/3749116.3749132,
  author	= {Khan, Arijit and Wu, Tianxing and Chen, Xi},
  title		= {LLM+KG@VLDB 24 Workshop Summary},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {2},
  issn		= {0163-5808},
  url		= {https://doi.org/10.1145/3749116.3749132},
  doi		= {10.1145/3749116.3749132},
  abstract	= {The unification of large language models (LLMs) and
		  knowledge graphs (KGs) has emerged as a hot topic. At the
		  LLM+KG'24 workshop, co-located with VLDB 2024 in Guangzhou,
		  China, the key theme explored was important data management
		  challenges and opportunities due to the effective
		  interaction between LLMs and KGs. The report outlines major
		  directions and approaches presented by various speakers
		  during the workshop.},
  journal	= {SIGMOD Rec.},
  month		= jul,
  pages		= {60–65},
  numpages	= {6}
}

@InProceedings{	  10.1145/3690712.3690723,
  author	= {Wasi, Azmine Toushik and Islam, Mst Rafia and Islam,
		  Raima},
  title		= {LLMs as Writing Assistants: Exploring Perspectives on
		  Sense of Ownership and Reasoning},
  year		= {2024},
  isbn		= {9798400710315},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3690712.3690723},
  doi		= {10.1145/3690712.3690723},
  abstract	= {Sense of ownership in writing confines our investment of
		  thoughts, time, and contribution, leading to attachment to
		  the output. However, using writing assistants introduces a
		  mental dilemma, as some content isn’t directly our
		  creation. For instance, we tend to credit Large Language
		  Models (LLMs) more in creative tasks, even though all tasks
		  are equal for them. Additionally, while we may not claim
		  complete ownership of LLM-generated content, we freely
		  claim authorship. We conduct a short survey to examine
		  these issues and understand underlying cognitive processes
		  in order to gain a better knowledge of human-centered
		  aspects in writing and improve writing aid systems.},
  booktitle	= {Proceedings of the Third Workshop on Intelligent and
		  Interactive Writing Assistants},
  pages		= {38–42},
  numpages	= {5},
  keywords	= {Human computer interaction, Large Language Models, Sense
		  of Ownership, Writing Assistants},
  location	= {Honolulu, HI, USA},
  series	= {In2Writing '24}
}

@InProceedings{	  10.1145/3723498.3723840,
  author	= {Xu, Kaijie and Verbrugge, Clark},
  title		= {Constraint Is All You Need: Optimization-Based 3D Level
		  Generation with LLMs},
  year		= {2025},
  isbn		= {9798400718564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3723498.3723840},
  doi		= {10.1145/3723498.3723840},
  abstract	= {Procedural Content Generation (PCG) has long enabled
		  efficient and varied game level creation. However,
		  integrating high-level design intentions and game mechanics
		  into complex 3D environments remains challenging. This
		  paper introduces a comprehensive framework that transforms
		  narrative-level descriptions into playable 3D game levels.
		  First, Large Language Models (LLMs) parse natural language
		  descriptions of game environments into a structured Game
		  Level Description Language (GLDL), capturing essential
		  spatial constraints. Next, we model level generation as a
		  Facility Layout Optimization problem, ensuring that
		  facility placements and configurations adhere to specified
		  design criteria. Through comprehensive experiments,
		  including automated constraint evaluations and agent-based
		  simulations, our approach ensures both the feasibility and
		  stability of the constraints extracted from textual
		  descriptions. We confirm that the resulting game levels
		  remain interactive, reasonable, and controllable to their
		  original specifications.},
  booktitle	= {Proceedings of the 20th International Conference on the
		  Foundations of Digital Games},
  articleno	= {66},
  numpages	= {13},
  keywords	= {Procedural Content Generation, Facility Layout Problem,
		  Level Generation, Large Language Models, Game Design},
  location	= { },
  series	= {FDG '25}
}

@Article{	  10.1145/3626960,
  author	= {Tsaneva, Stefani and Sabou, Marta},
  title		= {Enhancing Human-in-the-Loop Ontology Curation Results
		  through Task Design},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {1},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3626960},
  doi		= {10.1145/3626960},
  abstract	= {The success of artificial intelligence (AI) applications
		  is heavily dependent on the quality of data they rely on.
		  Thus, data curation, dealing with cleaning, organising, and
		  managing data, has become a significant research area to be
		  addressed. Increasingly, semantic data structures such as
		  ontologies and knowledge graphs empower the new generation
		  of AI systems. In this article, we focus on ontologies as a
		  special type of data. Ontologies are conceptual data
		  structures representing a domain of interest and are often
		  used as a backbone to knowledge-based intelligent systems
		  or as an additional input for machine learning algorithms.
		  Low-quality ontologies, containing incorrectly represented
		  information or controversial concepts modelled from a
		  single viewpoint, can lead to invalid application outputs
		  and biased systems. Thus, we focus on the curation of
		  ontologies as a crucial factor for ensuring trust in the
		  enabled AI systems. While some ontology quality aspects can
		  be automatically evaluated, others require a
		  human-in-the-loop evaluation. Yet, despite the importance
		  of the field, several ontology quality aspects have not yet
		  been addressed and there is a lack of guidelines for
		  optimal design of human computation tasks to perform such
		  evaluations. In this article, we advance the
		  state-of-the-art by making two novel contributions. First,
		  we propose a human computation (HC)–based approach for
		  the verification of ontology restrictions —an ontology
		  evaluation aspect that has not yet been addressed with HC
		  techniques. Second, by performing two controlled
		  experiments with a junior expert crowd, we empirically
		  derive task design guidelines for achieving high-quality
		  evaluation results related to (i) the formalism for
		  representing ontology axioms and (ii) crowd qualification
		  testing. We find that the representation format of the
		  ontology does not significantly influence the campaign
		  results. Nevertheless, contributors expressed a preference
		  in working with a graphical ontology representation.
		  Additionally, we show that an objective qualification test
		  is better fitted at assessing contributors’ prior
		  knowledge rather than a subjective self-assessment and that
		  prior modelling knowledge of the contributors had a
		  positive effect on their judgements. We make all artefacts
		  designed and used in the experimental campaign publicly
		  available.},
  journal	= {J. Data and Information Quality},
  month		= mar,
  articleno	= {4},
  numpages	= {25},
  keywords	= {Ontology evaluation, human-in-the-loop, human
		  computation}
}

@InProceedings{	  10.1145/3627508.3638340,
  author	= {Gohsen, Marcel and Stein, Benno},
  title		= {Assisted Knowledge Graph Authoring: Human-Supervised
		  Knowledge Graph Construction from Natural Language},
  year		= {2024},
  isbn		= {9798400704345},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627508.3638340},
  doi		= {10.1145/3627508.3638340},
  abstract	= {Encyclopedic knowledge graphs, such as Wikidata, host an
		  extensive repository of millions of knowledge statements.
		  However, domain-specific knowledge from fields such as
		  history, physics, or medicine is significantly
		  underrepresented in those graphs. Although few
		  domain-specific knowledge graphs exist (e.g., Pubmed for
		  medicine), developing specialized retrieval applications
		  for many domains still requires constructing knowledge
		  graphs from scratch. To facilitate knowledge graph
		  construction, we introduce WAKA: a Web application that
		  allows domain experts to create knowledge graphs through
		  the medium with which they are most familiar: natural
		  language.},
  booktitle	= {Proceedings of the 2024 Conference on Human Information
		  Interaction and Retrieval},
  pages		= {376–380},
  numpages	= {5},
  keywords	= {Information Extraction, Knowledge Graph Construction,
		  Semantic Web},
  location	= {Sheffield, United Kingdom},
  series	= {CHIIR '24}
}

@InProceedings{	  10.1145/3706599.3719683,
  author	= {Sch\"{a}fer, Ren\'{e} and Preuschoff, Paul Miles and
		  Niewianda, Rene and Hahn, Sophie and Fiedler, Kevin and
		  Borchers, Jan},
  title		= {Don't Detect, Just Correct: Can LLMs Defuse Deceptive
		  Patterns Directly?},
  year		= {2025},
  isbn		= {9798400713958},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706599.3719683},
  doi		= {10.1145/3706599.3719683},
  abstract	= {Deceptive (or dark) patterns, UI design strategies
		  manipulating users against their best interests, have
		  become widespread. We introduce an idea for technical
		  countermeasures against such patterns. It feeds the HTML
		  code of web elements that may contain deceptive patterns
		  into a large language model (LLM) and iteratively prompts
		  it to make these elements less manipulative. We evaluated
		  our approach with GPT-4o and self-created web elements. The
		  most consistent results appeared after three iterations,
		  with 91\% of deceptive elements being less manipulative and
		  96\% not more manipulative than originally. We contribute
		  our minimal and improved prompts and a labeled dataset of
		  all 2,600 redesigns with the LLM’s justifications for its
		  changes. We also performed preliminary tests on real
		  websites to show and discuss the feasibility of our
		  approach in the field. Our findings suggest that LLMs can
		  defuse certain deceptive patterns without prior model
		  training, promising a major advance in fighting these
		  manipulations.},
  booktitle	= {Proceedings of the Extended Abstracts of the CHI
		  Conference on Human Factors in Computing Systems},
  articleno	= {199},
  numpages	= {11},
  keywords	= {deceptive patterns, dark patterns, large language models,
		  countermeasures},
  location	= { },
  series	= {CHI EA '25}
}

@Article{	  10.1145/3707637,
  author	= {Hamed, Naeima and Rana, Omer and Orozco Ter Wengel, Pablo
		  and Goossens, Benoit and Perera, Charith},
  title		= {FooDS: Ontology-based Knowledge Graphs for Forest
		  Observatories},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {1},
  url		= {https://doi.org/10.1145/3707637},
  doi		= {10.1145/3707637},
  abstract	= {Wildlife research activities generate data on ecosystems
		  and species interactions from varied independent projects.
		  Forest Observatories are online platforms that curate,
		  integrate, and analyze wildlife research data for forest
		  monitoring. However, integrating data from disparate
		  sources can be challenging due to data heterogeneity. This
		  study, in collaboration with a research facility in the
		  forest of Sabah, Malaysian Borneo, proposes a novel
		  approach to integrate heterogeneous wildlife data for
		  Forest Observatories. We used the Forest Observatory
		  Ontology (FOO) to standardize wildlife data entities
		  generated by sensors. Four semantically modeled wildlife
		  datasets populated FOO, resulting in an ontology-based
		  knowledge graph named FooDS (Forest Observatory Ontology
		  Data Store). We evaluated FOO and FooDS using specialized
		  open-source ontology scanners, domain experts’ feedback,
		  and applied use cases. This study contributes FooDS, the
		  first ontology-based knowledge graph for Forest
		  Observatories, which provides accurate query responses,
		  reasoning about data, and granular data acquisition from
		  diverse datasets. FOO in turtle format, FOO’s
		  documentation and FooDS in turtle format and their resource
		  website are published at , , , and .},
  journal	= {ACM J. Comput. Sustain. Soc.},
  month		= jan,
  articleno	= {2},
  numpages	= {42},
  keywords	= {Wildlife data, Internet of Things, ontology, knowledge
		  graph}
}

@InProceedings{	  10.1145/3652620.3688197,
  author	= {Silva Mercado, Jonathan},
  title		= {AI Assisted Domain Modeling Explainability and
		  Traceability},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688197},
  doi		= {10.1145/3652620.3688197},
  abstract	= {Domain Models are abstract representations of selected
		  elements in a domain that is created in a collaborative
		  process between domain and modeler experts. The
		  participants share domain knowledge to conceptualize and
		  reason about the elements that will create the domain
		  models. Through this exchange, a comprehensive and accurate
		  representation of the domain is achieved, ensuring that the
		  model captures the relevant aspects and relationships in
		  the domain. Research in Artificial Intelligence (AI) has
		  explored various methods to assist in the creation of
		  domain models from text using Natural Language Processing
		  (NLP) and Machine Learning (ML). Recent advancements with
		  Large Language Models (LLMs) have shown that it is possible
		  to create domain models using prompting techniques;
		  however, the generated domain models contain errors and
		  remain constrained by the performance of the LLM
		  used.Despite the impressive capabilities of LLMs to create
		  domain models, it is evident that it does not address the
		  needs of domain and modelers experts that participate in
		  the creation of domain models. Every AI technique has its
		  advantages and limitations that must be integrated with
		  human feedback in a collaboration process. Therefore, we
		  propose an approach that incorporates human-AI
		  collaboration supported by AI assistants that follows a
		  dialogue approach to understand the users needs and purpose
		  to suggest relevant models. Our proposal combines symbolic
		  and subsymbolic AI techniques with explainability and
		  traceability of the decisions that assist to create domain
		  models that are relevant for the users.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {130–135},
  numpages	= {6},
  keywords	= {domain modeling, large language models, uncertainty,
		  explainability, traceability},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3701716.3715531,
  author	= {Frey, Johannes and Ciuciu-Kiss, Jenifer Tabita and Arndt,
		  Natanael},
  title		= {Breaking Accessibility Barriers: An Ontology Proxy with
		  Failure Recovery and Time Travel Capabilities},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715531},
  doi		= {10.1145/3701716.3715531},
  abstract	= {This paper introduces a novel concept and implementation
		  of an ontology proxy designed to seamlessly enhance
		  accessibility and reliability of the Web of Ontologies by
		  addressing challenges such as link rot, evolution
		  inconsistencies, and communication failures. The proxy
		  features a time travel mode, powered by DBpedia Archivo,
		  that provides access to archived and versioned snapshots of
		  ontologies. This enables failure recovery and the emulation
		  of a consistent state in time, supporting reproducible
		  research and enhancing the FAIRness of ontologies and
		  associated (meta)data in a plug-and-play manner. Initial
		  evaluations show significant improvements in ontology
		  retrieval success rates, underscoring the proxy's potential
		  as a viable interface for breaking accessibility
		  barriers.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {966–970},
  numpages	= {5},
  keywords	= {accessibility issues, fair data, linked data, ontology
		  wayback machine},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3570991.3571051,
  author	= {Ghosh, Sohom and Naskar, Sudip Kumar},
  title		= {Using Natural Language Processing to Enhance
		  Understandability of Financial Texts},
  year		= {2023},
  isbn		= {9781450397971},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570991.3571051},
  doi		= {10.1145/3570991.3571051},
  abstract	= {Dealing with money has always been one of the basic skills
		  one needs to live a comfortable life. However, financial
		  literacy rates across the nations are extremely low.
		  Furthermore, over the years the returns from traditional
		  investment avenues like bank fixed deposits (FD), real
		  estate, etc. have been diminishing. This entices new-age
		  investors to trade and reap profits from the ever-growing
		  stock markets. Nevertheless, in reality, only a handful of
		  active traders are able to earn more than the FD rates.
		  This is due to the lack of financial knowledge. The
		  presence of complex concepts and jargons further reduces
		  comprehensibility. In this paper, we present how financial
		  texts can be demystified using Natural Language Processing
		  (NLP). It consists of neural-based readability assessment
		  and hypernym extraction tools to improve the readability of
		  financial texts. Other modules include financial domain
		  specific systems for automated claim detection,
		  sustainability assessment, etc.},
  booktitle	= {Proceedings of the 6th Joint International Conference on
		  Data Science \&amp; Management of Data (10th ACM IKDD CODS
		  and 28th COMAD)},
  pages		= {301–302},
  numpages	= {2},
  keywords	= {claim detection, financial text processing, hypernym
		  detection, natural language processing, readability},
  location	= {Mumbai, India},
  series	= {CODS-COMAD '23}
}

@InProceedings{	  10.1145/3681772.3698217,
  author	= {Tupayachi, Jose and Li, Xueping},
  title		= {Conversational Geographic Question Answering for Route
		  Optimization: An LLM and Continuous Retrieval-Augmented
		  Generation Approach},
  year		= {2025},
  isbn		= {9798400711510},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3681772.3698217},
  doi		= {10.1145/3681772.3698217},
  abstract	= {We present a pilot study exploring the potential of Large
		  Language Models (LLMs) to interface with application
		  programming interfaces through logical instructions,
		  specifically within the domain of Geographic Question
		  Answering for route optimization. This study employs a
		  Continuous Retrieval-Augmented Generation approach combined
		  with fine-tuned LLMs, featuring customized node-based
		  storage and vector search retrieval. We also provide a
		  comparative analysis of the method's effectiveness and
		  adaptability in handling diverse textual queries.},
  booktitle	= {Proceedings of the 17th ACM SIGSPATIAL International
		  Workshop on Computational Transportation Science GenAI and
		  Smart Mobility Session},
  pages		= {56–59},
  numpages	= {4},
  keywords	= {Geographical Information, Large Language Models, Question
		  Answering, Retrieval Augmented Generation},
  location	= {Atlanta, GA, USA},
  series	= {IWCTS'24}
}

@InProceedings{	  10.1145/3472307.3484649,
  author	= {Kim, Sujeong and Tamrakar, Amir},
  title		= {“How to best say it?” : Translating Directives in
		  Machine Language into Natural Language in the Blocks
		  World},
  year		= {2021},
  isbn		= {9781450386203},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3472307.3484649},
  doi		= {10.1145/3472307.3484649},
  abstract	= {We propose a method to generate optimal natural language
		  for block placement directives generated by a machine’s
		  planner during human-agent interactions in the blocks
		  world. A non user-friendly machine directive, e.g.,
		  move(ObjId, toPos), is transformed into visually and
		  contextually grounded referring expressions that are much
		  easier for the user to comprehend. We describe an algorithm
		  that progressively and generatively transforms the
		  machine’s directive in ECI (Elementary Composable
		  Ideas)-space, generating many alternative versions of the
		  directive. We then define a cost function to evaluate the
		  ease of comprehension of these alternatives and select the
		  best option. The parameters for this cost function were
		  derived empirically from a user study that measured
		  utterance-to-action timings.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Human-Agent Interaction},
  pages		= {289–293},
  numpages	= {5},
  keywords	= {cost, natural language generation, optimization,
		  perception},
  location	= {Virtual Event, Japan},
  series	= {HAI '21}
}

@InProceedings{	  10.1145/3582437.3587185,
  author	= {van Rozen, Riemer and Bouwer, Anders and Millenaar,
		  Karel},
  title		= {Towards a Unified Language for Card Game Design},
  year		= {2023},
  isbn		= {9781450398558},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3582437.3587185},
  doi		= {10.1145/3582437.3587185},
  abstract	= {Card game creation is a powerful tool for
		  game&nbsp;design. Using&nbsp;playing cards, game designers
		  can rapidly prototype and iteratively playtest a game’s
		  core mechanisms to explore alternatives and improve the
		  gameplay. However, this process is time-consuming,
		  imprecise and challenging to steer and focus. We aim to
		  empower designers with solutions that automate game design
		  processes. In particular, we study to what extent a unified
		  game design language can offer theoretical foundations,
		  systematic techniques and practical solutions. We propose a
		  novel approach towards a solution that leverages the
		  expressive power of playing cards. Initially focusing on
		  well-known card games, we illustrate the steps for creating
		  CardScript, a formal language and toolkit that supports
		  game design processes. The approach also has the potential
		  to impact a wider research area. When fully developed, a
		  unified language with a common tool set can enable reuse,
		  and eventually support joint research agendas. We start the
		  discussion by highlighting perspectives that relate open
		  challenges to opportunities for future collaboration.},
  booktitle	= {Proceedings of the 18th International Conference on the
		  Foundations of Digital Games},
  articleno	= {47},
  numpages	= {4},
  keywords	= {design tools, domain-specific languages, game design},
  location	= {Lisbon, Portugal},
  series	= {FDG '23}
}

@InProceedings{	  10.1145/3340555.3356099,
  author	= {Tan, Chaohong and Ling, Zhenhua},
  title		= {Multi-Classification Model for Spoken Language
		  Understanding},
  year		= {2019},
  isbn		= {9781450368605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340555.3356099},
  doi		= {10.1145/3340555.3356099},
  abstract	= {The spoken language understanding (SLU) is an important
		  part of spoken dialogue system (SDS). In the paper, we
		  focus on how to extract a set of act-slot-value tuples from
		  users’ utterances in the 1st Chinese Audio-Textual Spoken
		  Language Understanding Challenge (CATSLU). This paper
		  adopts the pretrained BERT model to encode users’
		  utterances and builds multiple classifiers to get the
		  required tuples. In our framework, finding acts and values
		  of slots are recognized as classification tasks
		  respectively. Such multi-task training is expected to help
		  the encoder to get better understanding of the utterance.
		  Since the system is built on the transcriptions given by
		  automatic speech recognition (ASR), some tricks are applied
		  to correct the errors of the tuples. We also found that
		  using the minimum edit distance (MED) between results and
		  candidates to rebuild the tuples was beneficial in our
		  experiments.},
  booktitle	= {2019 International Conference on Multimodal Interaction},
  pages		= {526–530},
  numpages	= {5},
  keywords	= {BERT, Spoken language understanding, classification,
		  multi-task learning, text tagging},
  location	= {Suzhou, China},
  series	= {ICMI '19}
}

@InProceedings{	  10.1145/3719160.3728624,
  author	= {Desai, Smit and Dubiel, Mateusz and Zargham, Nima and
		  Mildner, Thomas and Spillner, Laura},
  title		= {Personas Evolved: Designing Ethical LLM-Based
		  Conversational Agent Personalities},
  year		= {2025},
  isbn		= {9798400715273},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3719160.3728624},
  doi		= {10.1145/3719160.3728624},
  abstract	= {The emergence of Large Language Models (LLMs) has
		  revolutionized Conversational User Interfaces (CUIs),
		  enabling more dynamic, context-aware, and human-like
		  interactions across diverse domains, from social sciences
		  to healthcare. However, the rapid adoption of LLM-based
		  personas raises critical ethical and practical concerns,
		  including bias, manipulation, and unforeseen social
		  consequences. Unlike traditional CUIs, where personas are
		  carefully designed with clear intent, LLM-based personas
		  generate responses dynamically from vast datasets, making
		  their behavior less predictable and harder to govern. This
		  workshop aims to bridge the gap between CUI and broader AI
		  communities by fostering a cross-disciplinary dialogue on
		  the responsible design and evaluation of LLM-based
		  personas. Bringing together researchers, designers, and
		  practitioners, we will explore best practices, develop
		  ethical guidelines, and promote frameworks that ensure
		  transparency, inclusivity, and user-centered interactions.
		  By addressing these challenges collaboratively, we seek to
		  shape the future of LLM-driven CUIs in ways that align with
		  societal values and expectations.},
  booktitle	= {Proceedings of the 7th ACM Conference on Conversational
		  User Interfaces},
  articleno	= {32},
  numpages	= {4},
  keywords	= {Conversational User Interfaces, Interaction Design,
		  Personas, LLMs},
  location	= { },
  series	= {CUI '25}
}

@InProceedings{	  10.1145/3656156.3665133,
  author	= {Haghighi, Nava},
  title		= {Ontological Breakdown: Toward a World of Many Worlds},
  year		= {2024},
  isbn		= {9798400706325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3656156.3665133},
  doi		= {10.1145/3656156.3665133},
  abstract	= {Examining the taken-for-granted assumptions and views of
		  the world underlying the design of technological artifacts,
		  this work posits that a lack of ontological self-reflection
		  can constrain imagination, impeding movement toward a world
		  of many worlds. I propose ontological breakdown as an
		  analytic lens for interrogating the default assumptions
		  underlying the design of technology, using LLMs as a
		  case-study and drawing parallels to the discourse on values
		  in design. Then, I share three ways in which I have used
		  ontological breakdowns generatively to (1) surface
		  ontological difference and create spaces for experiencing
		  ontological alternatives to our defaults, (2) explore
		  ontological alternatives in the design of artifacts and
		  enable the end-users to notice their own ontological
		  defaults, and (3) expand ontological diversity by
		  empowering the end-users to move beyond the prescribed
		  defaults. I demonstrate the generative potential of
		  ontological breakdowns by providing examples of my work in
		  personal informatics.},
  booktitle	= {Companion Publication of the 2024 ACM Designing
		  Interactive Systems Conference},
  pages		= {70–73},
  numpages	= {4},
  keywords	= {LLM, generative AI, ontological breakdown, ontological
		  design, ontologies, personal informatics},
  location	= {IT University of Copenhagen, Denmark},
  series	= {DIS '24 Companion}
}

@Article{	  10.1145/3717067,
  author	= {Jiang, Shuyu and Chen, Xingshu and Tang, Rui},
  title		= {Deceiving LLM through Compositional Instruction with
		  Hidden Attacks},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1556-4665},
  url		= {https://doi.org/10.1145/3717067},
  doi		= {10.1145/3717067},
  abstract	= {Recently, large language models (LLMs) have demonstrated
		  promising applications in the autonomous driving (AD)
		  domain, including language-based interactions and
		  decision-making. Ensuring they safely handle harmful inputs
		  is crucial before formal deployment. However, research
		  reveals emerging hand-crafted jailbreak attacks, which pack
		  harmful prompts into harmless instructions, can bypass
		  LLMs’ security mechanisms and elicit harmful responses.
		  To deeply understand such jailbreaks, this paper introduces
		  a Compositional Instruction Attack (CIA) framework to
		  generalize them, and develop two CIA jailbreaking methods
		  to automatically generate tailored jailbreak prompts for
		  each harmful prompt. Then, this paper builds the first CIA
		  question-answering (CIAQA) dataset with 2.7K
		  multiple-choice questions of 900 successful jailbreaks, for
		  assessing LLMs’ ability to identify underlying harmful
		  intents, harmfulness, and task priority in CIA jailbreaks.
		  Combined with experimental analysis on CIAQA and other
		  datasets, this paper concludes three possible reasons for
		  the failure of LLM defenses against CIAs. Finally, we
		  propose an intent-based defense paradigm (IBD), enabling
		  LLMs to defend against CIA by leveraging its capability to
		  identify intents. Experimental results show CIA can achieve
		  attack success rates (ASR) of 95\%+ and 85\%+ in AD and
		  common harmful scenarios for three well-known LLMs (GPT-4,
		  GPT-3.5, and Llama2-70b-chat), and IBD reduces ASR by 74\%+.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Auton. Adapt. Syst.},
  month		= feb,
  keywords	= {Large language model, autonomous driving, adversarial
		  attack, harmful prompt}
}

@Article{	  10.1145/3564769,
  author	= {Kumar Attar, Rakesh and Goyal, Vishal and Goyal, Lalit},
  title		= {State of the Art of Automation in Sign Language: A
		  Systematic Review},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3564769},
  doi		= {10.1145/3564769},
  abstract	= {Sign language is the fundamental communication language of
		  deaf people. Efforts to develop sign language generation
		  systems can make the life of these people smooth and
		  effortless. Despite the importance of sign language
		  generation systems, there is a paucity of a systematic
		  literature review. This is the foremost recognizable
		  scholastic literature review of sign language generation
		  systems. It presents a scholastic database of the
		  literature between 1998 and 2020 and suggests
		  classification criteria to systematize research studies.
		  Four hundred fourteen research studies were recognized and
		  reviewed for their direct pertinence to sign language
		  generation systems. One hundred sixty-two research studies
		  were subsequently chosen, examined, and classified. Each of
		  the 162 chosen research papers was categorized based on 30
		  sign languages and was further comparatively analyzed based
		  on seven comparison parameters (input form, translation
		  technologies, application domain, use of parsers/grammars,
		  manual/non-manual features, accuracy, and output form). It
		  is evident from our research findings that the majority of
		  research on sign language generation was carried out using
		  data-driven approaches in the absence of proper grammar
		  rules and generated only manual signs. This research study
		  may provide researchers a roadmap toward future research
		  directions and facilitate the compilation of information in
		  the field of sign language generation.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {94},
  numpages	= {80},
  keywords	= {Machine translation, Interlingua, virtual avatar, SiGML,
		  HamNoSys}
}

@InProceedings{	  10.1145/3307630.3342417,
  author	= {Achtaich, Asmaa and Roudies, Ounsa and Souissi, Nissrine
		  and Salinesi, Camille and Mazo, Ra\'{u}l},
  title		= {Evaluation of the State-Constraint Transition Modelling
		  Language: A Goal Question Metric Approach},
  year		= {2019},
  isbn		= {9781450366687},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3307630.3342417},
  doi		= {10.1145/3307630.3342417},
  abstract	= {Self-adaptive systems (SAS) are exceptional systems, on
		  account of their versatile composition, dynamic behavior
		  and evolutive nature. Existing formal languages for the
		  specification of SAS focus on adapting system elements to
		  achieve a target goal, following specific rules, without
		  much attention on the adaptation of requirements
		  themselves. The State-Constraint Transition (SCT) modeling
		  language enables the specification of dynamic requirements,
		  both at the domain and application level, as a result of
		  space or time variability. This language, evaluated in this
		  paper, enables the specification of a variety of
		  requirement types, for SASs from different domains, while
		  generating a configuration, all configurations, and number
		  of possible configurations, in milliseconds. This paper
		  presents these results, namely; expressiveness, domain
		  independence and scalability, from the viewpoint of
		  designers and domain engineers, following a
		  goal-question-metric approach. However, being primarily
		  based on constraint programming (CP), the language suffers
		  from drawbacks inherited from this paradigm, specifically
		  time related requirements, like (e.g. order, frequency and
		  staged requirements).},
  booktitle	= {Proceedings of the 23rd International Systems and Software
		  Product Line Conference - Volume B},
  pages		= {106–113},
  numpages	= {8},
  keywords	= {IoT, constraint programming, dynamic software product
		  lines, modeling language, state machine},
  location	= {Paris, France},
  series	= {SPLC '19}
}

@InBook{	  10.1145/3677389.3702541,
  author	= {Wang, Chunying and Ji, Heng and Han, Wenying},
  title		= {Construction and Application of Emotion Ontology for
		  Narrative Text},
  year		= {2025},
  isbn		= {9798400710933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677389.3702541},
  abstract	= {Emotional information in narrative texts is an important
		  resource for deepening text interpretation and promoting
		  literature utilization, but there is still a lack of
		  dedicated information organization schemes. Based on the
		  systematic analysis of related theories, this study
		  constructed the narrative text emotion ontology (NTEO),
		  which contains three core components of event, appraisal,
		  and emotion, as well as three related components of emotion
		  category, emotion expression, and action tendency, for
		  structuring and organizing the emotion information.
		  Meanwhile, six relations of temporal, cause, circumstance,
		  regulation, change, and trigger were summarized for
		  embedding emotional units into narrative sequences.
		  Furthermore, taking Tsen Shui-fang's Diary, an archive
		  related to the Nanjing Massacre in China, as an example, we
		  explored the innovative applications of fine-grained
		  emotion classification, fine-grained emotion visualization,
		  and emotion attribution analysis supported by this emotion
		  ontology.},
  booktitle	= {Proceedings of the 24th ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {61},
  numpages	= {5}
}

@InProceedings{	  10.1145/3307630.3342401,
  author	= {Villota, Angela and Mazo, Ra\'{u}l and Salinesi, Camille},
  title		= {The High-Level Variability Language: An Ontological
		  Approach},
  year		= {2019},
  isbn		= {9781450366687},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3307630.3342401},
  doi		= {10.1145/3307630.3342401},
  abstract	= {Given its relevance, there is an extensive body of
		  research for modeling variability in diverse domains.
		  Regretfully, the community still faces issues and
		  challenges to port or share variability models among tools
		  and methodological approaches. There are researchers, for
		  instance, implementing the same algorithms and analyses
		  again because they use a specific modeling language and
		  cannot use some existing tool. This paper introduces the
		  High-Level Variability Language (HLVL), an expressive and
		  extensible textual language that can be used as a modeling
		  and an intermediate language for variability. HLVL was
		  designed following an ontological approach, i.e., by
		  defining their elements considering the meaning of the
		  concepts existing on different variability languages. Our
		  proposal not only provides a unified language based on a
		  comprehensive analysis of the existing ones but also sets
		  foundations to build tools that support different notations
		  and their combination.},
  booktitle	= {Proceedings of the 23rd International Systems and Software
		  Product Line Conference - Volume B},
  pages		= {162–169},
  numpages	= {8},
  keywords	= {domain specific language, variability language,
		  variability specification},
  location	= {Paris, France},
  series	= {SPLC '19}
}

@InProceedings{	  10.5555/3643142.3643333,
  author	= {Tu, Ming-Yu and Ehm, Hans and Ismail, Abdelgafar and
		  Ulrich, Philipp},
  title		= {Reusable Ontology Generation and Matching from Simulation
		  Models},
  year		= {2024},
  isbn		= {9798350369663},
  publisher	= {IEEE Press},
  abstract	= {As simulating semiconductor manufacturing grows complex,
		  model reuse becomes appealing since it can reduce the time
		  incurred in developing future models. Also, considering a
		  large network of the semiconductor supply chain, knowledge
		  sharing can enable the efficient development of simulation
		  models in a collaborative organization. Such necessity of
		  reusability and interoperability of simulation models
		  motivates this paper. We will address these challenges
		  through ontological modeling and linking of the simulation
		  components. The first application is generating reusable
		  ontologies from simulation models. Another discussed
		  application is ontology matching for knowledge sharing
		  between simulation components and a meta-model of the
		  semiconductor supply chain. The proposed approach succeeds
		  in automatically transforming simulation into reusable
		  knowledge and identifying interconnection in a
		  semiconductor manufacturing system.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2298–2309},
  numpages	= {12},
  location	= {San Antonio, Texas, USA},
  series	= {WSC '23}
}

@InProceedings{	  10.1145/3544548.3580964,
  author	= {Ruoff, Marcel and Myers, Brad A and Maedche, Alexander},
  title		= {ONYX: Assisting Users in Teaching Natural Language
		  Interfaces Through Multi-Modal Interactive Task Learning},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3580964},
  doi		= {10.1145/3544548.3580964},
  abstract	= {Users are increasingly empowered to personalize natural
		  language interfaces (NLIs) by teaching how to handle new
		  natural language (NL) inputs. However, our formative study
		  found that when teaching new NL inputs, users require
		  assistance in clarifying ambiguities that arise and want
		  insight into which parts of the input the NLI understands.
		  In this paper we introduce ONYX, an intelligent agent that
		  interactively learns new NL inputs by combining NL
		  programming and programming-by-demonstration, also known as
		  multi-modal interactive task learning. To address the
		  aforementioned challenges, ONYX provides suggestions on how
		  ONYX could handle new NL inputs based on previously learned
		  concepts or user-defined procedures, and poses follow-up
		  questions to clarify ambiguities in user demonstrations,
		  using visual and textual aids to clarify the connections.
		  Our evaluation shows that users provided with ONYX’s new
		  features achieved significantly higher accuracy in teaching
		  new NL inputs (median: 93.3\%) in contrast to those without
		  (median: 73.3\%).},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {417},
  numpages	= {16},
  keywords	= {Data Visualization Tools, End User Development,
		  Interactive Task Learning, Natural Language Interfaces},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@InProceedings{	  10.1145/3670474.3685948,
  author	= {Batten, Christopher and Pinckney, Nathaniel and Liu,
		  Mingjie and Ren, Haoxing and Khailany, Brucek},
  title		= {PyHDL-Eval: An LLM Evaluation Framework for Hardware
		  Design Using Python-Embedded DSLs},
  year		= {2024},
  isbn		= {9798400706998},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670474.3685948},
  doi		= {10.1145/3670474.3685948},
  abstract	= {Embedding hardware design frameworks within Python is a
		  promising technique to improve the productivity of hardware
		  engineers. At the same time, there is significant interest
		  in using large-language models (LLMs) to improve key chip
		  design tasks. This paper describes PyHDL-Eval, a new
		  framework for evaluating LLMs on specification-to-RTL tasks
		  in the context of Python-embedded domain-specific languages
		  (DSLs). The framework includes 168 problems, Verilog
		  reference solutions, Verilog test benches, Python test
		  scripts, and workflow orchestration scripts. We use the
		  framework to conduct a detailed case study comparing five
		  LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo)
		  targeting Verilog and five Python-embedded DSLs (PyMTL3,
		  PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate
		  the promise of in-context learning when applied to smaller
		  models (e.g., pass rate for CodeGemma 7B improves from
		  14.9\% to 32.7\% on Verilog) and Python-embedded DSLs
		  (e.g., pass rate for LLama3 70B improves from 0.6\% to
		  33.0\% on PyMTL3). We find LLMs perform better when
		  targeting Verilog as compared to Python-embedded DSLs
		  (e.g., pass rate for GPT4 Turbo is 72.2\% on Verilog and
		  29.8-62.0\% on the Python-embedded DSLs) despite using a
		  popular general-purpose host language. PyHDL-Eval will
		  serve as a useful framework for future research at the
		  intersection of Python-embedded DSLs and LLMs.},
  booktitle	= {Proceedings of the 2024 ACM/IEEE International Symposium
		  on Machine Learning for CAD},
  articleno	= {10},
  numpages	= {17},
  keywords	= {Python-embedded domain-specific languages, hardware
		  description languages, large language models},
  location	= {Salt Lake City, UT, USA},
  series	= {MLCAD '24}
}

@Article{	  10.1145/3374217,
  author	= {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and
		  Li, Chenliang},
  title		= {Adversarial Attacks on Deep-learning Models in Natural
		  Language Processing: A Survey},
  year		= {2020},
  issue_date	= {June 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {11},
  number	= {3},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3374217},
  doi		= {10.1145/3374217},
  abstract	= {With the development of high computational devices, deep
		  neural networks (DNNs), in recent years, have gained
		  significant popularity in many Artificial Intelligence (AI)
		  applications. However, previous efforts have shown that
		  DNNs are vulnerable to strategically modified samples,
		  named adversarial examples. These samples are generated
		  with some imperceptible perturbations, but can fool the
		  DNNs to give false predictions. Inspired by the popularity
		  of generating adversarial examples against DNNs in Computer
		  Vision (CV), research efforts on attacking DNNs for Natural
		  Language Processing (NLP) applications have emerged in
		  recent years. However, the intrinsic difference between
		  image (CV) and text (NLP) renders challenges to directly
		  apply attacking methods in CV to NLP. Various methods are
		  proposed addressing this difference and attack a wide range
		  of NLP applications. In this article, we present a
		  systematic survey on these works. We collect all related
		  academic works since the first appearance in 2017. We then
		  select, summarize, discuss, and analyze 40 representative
		  works in a comprehensive way. To make the article
		  self-contained, we cover preliminary knowledge of NLP and
		  discuss related seminal works in computer vision. We
		  conclude our survey with a discussion on open issues to
		  bridge the gap between the existing progress and more
		  robust adversarial attacks on NLP DNNs.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= apr,
  articleno	= {24},
  numpages	= {41},
  keywords	= {Deep neural networks, adversarial examples, natural
		  language processing, textual data}
}

@InProceedings{	  10.1145/3307630.3342403,
  author	= {Berger, Thorsten and Collet, Philippe},
  title		= {Usage Scenarios for a Common Feature Modeling Language},
  year		= {2019},
  isbn		= {9781450366687},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3307630.3342403},
  doi		= {10.1145/3307630.3342403},
  abstract	= {Feature models are recognized as a de facto standard for
		  variability modeling. Presented almost three decades ago,
		  dozens of different variations and extensions to the
		  original feature-modeling notation have been proposed,
		  together with hundreds of variability management techniques
		  building upon feature models. Unfortunately, despite
		  several attempts to establish a unified language, there is
		  still no emerging consensus on a feature-modeling language
		  that is both intuitive and simple, but also expressive
		  enough to cover a range of important usage scenarios. There
		  is not even a documented and commonly agreed set of such
		  scenarios.Following an initiative among product-line
		  engineering researchers in September 2018, we present 14
		  usage scenarios together with examples and requirements
		  detailing each scenario. The scenario descriptions are the
		  result of a systematic process, where members of the
		  initiative authored original descriptions, which received
		  feedback via a survey, and which we then refined and
		  extended based on the survey results, reviewers' comments,
		  and our own expertise. We also report the relevance of
		  supporting each usage scenario for the language, as
		  perceived by the initiative's members, prioritizing each
		  scenario. We present a roadmap to build and implement a
		  first version of the envisaged common language.},
  booktitle	= {Proceedings of the 23rd International Systems and Software
		  Product Line Conference - Volume B},
  pages		= {174–181},
  numpages	= {8},
  keywords	= {feature models, software product lines, unified language},
  location	= {Paris, France},
  series	= {SPLC '19}
}

@Article{	  10.1145/3745022,
  author	= {Lu, Kezhi and Lu, Jie and Xu, Hanshi and Guo, Kairui and
		  Zhang, Qian and Lin, Hua and Grosser, Mark and Zhang, Yi
		  and Zhang, Guangquan},
  title		= {Genomics-Enhanced Cancer Risk Prediction for Personalized
		  LLMs-Driven Healthcare Recommender Systems},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3745022},
  doi		= {10.1145/3745022},
  abstract	= {Cancer risk prediction is a cornerstone of personalized
		  medicine that offers opportunities for early detection and
		  preventive interventions. However, the current models are
		  designed to predict cancer risk face several challenges.
		  First, most rely on traditional statistical methods, which
		  struggle to capture the complexity of genetic, family
		  medical history, and lifestyle factors. Hence, the accuracy
		  of these models is limited. Additionally, the models
		  neglect to integrate multidimensional data sources,
		  particularly genetic information like single nucleotide
		  polymorphisms (SNPs), which could enhance prediction
		  accuracy. Third, while the system might effectively predict
		  risk, it cannot translate those predictions into actionable
		  healthcare recommendations to reduce cancer risk.In this
		  study, we address all three of these limitations. With a
		  focus on six prevalent cancers – we extracted SNPs data
		  from the UK Biobank and designed a novel risk prediction
		  model for cancer and personalized healthcare
		  recommendations based upon the mixture of experts (MoE)
		  paradigm and large language models (LLMs) respectively.
		  Named MoE-HRS, experts based two router networks for
		  separate processing by the Transformer and the
		  convolutional neural network (CNN). Experiments on UK
		  Biobank data show that our model outperforms
		  state-of-the-art cancer risk prediction models. To bridge
		  the gap between risk prediction and practical healthcare
		  applications, we devised a healthcare recommender system
		  powered by LLMs. This approach holds promise for enhancing
		  early detection rates and promoting preventive healthcare
		  management1.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jun,
  keywords	= {Healthcare Recommender Systems, LLMs-Driven Recommender
		  Systems, Genetic Risk Prediction, Recommendation}
}

@InProceedings{	  10.5555/3712729.3712892,
  author	= {Dimitrakopoulos, George and Ehm, Hans and Tsaousi, Eleni},
  title		= {Enhanced Ontology Extraction: Integrating GPT AI with
		  Human Knowledge on the Example of EU Standards Related to
		  Semiconductor Supply Chains},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {This paper addresses challenges in creating ontologies for
		  the semiconductor supply chain. Ontologies are crucial for
		  seamless data exchange within the semantic web, enabling
		  initiatives like GAIA-X and CatenA-X. Traditionally,
		  ontology creation is complex. Here, we propose a novel
		  AI-assisted method using large language models (LLMs) like
		  ChatGPT 4 Turbo to support human experts. This
		  collaboration aims to expedite ontology generation while
		  maintaining quality. While initial tests show promise,
		  refining the human-AI interface for clear content
		  generation remains a focus. By improving this
		  collaboration, we expect to create more accurate and
		  complete ontologies, fostering efficient information
		  sharing and strengthening the meaningfulness of standards
		  within the semiconductor supply chain.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1955–1965},
  numpages	= {11},
  location	= {Orlando, Florida, USA},
  series	= {WSC '24}
}

@InProceedings{	  10.1145/3708635.3708655,
  author	= {Yu, Hong Qing and Sutton, Jack and O'Neill, Sam and
		  Reiff-Marganiec, Stephan},
  title		= {Case Studies on LLM Centric and Services Oriented Data
		  Analytics Agent Development},
  year		= {2025},
  isbn		= {9798400717765},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708635.3708655},
  doi		= {10.1145/3708635.3708655},
  abstract	= {This paper presents a novel service orchestration
		  framework for a chatbot application focused on data
		  analytics questions. The framework integrates Large
		  Language Models (LLMs) with service-oriented computing to
		  transform data analytics into a dynamic, conversational
		  experience. The approach leverages advancements in LLM
		  technology to enable real-time, automated data insights via
		  chatbot interfaces, making complex data analytics
		  accessible across various industries. In addition, the data
		  will be processed and analysis at edge-machine rather than
		  post all the data directly to the LLMs on the cloud.
		  Therefore, the Central to the framework is the local Micro
		  Analytics Service (MAS) and a dynamic service-data
		  coordination framework, which together facilitate the
		  decoupling of data from business logic, allowing for
		  intuitive engagement with analytics processes. Through two
		  case studies, retail data analysis and regional healthcare
		  planning, the ability of the framework to provide
		  actionable insights through natural language prompts is
		  demonstrated, showcasing its potential to significantly
		  reduce barriers to sophisticated data analytics. The
		  evaluation reveals strong performance in data connection
		  and code generation, with identified areas for improvement
		  in visualizations and handling complex data scenarios.},
  booktitle	= {Proceedings of the 2024 13th International Conference on
		  Software and Information Engineering},
  pages		= {69–76},
  numpages	= {8},
  keywords	= {LLM-driven service orchestration, Dynamic data analytics
		  services, Services Computing},
  location	= { },
  series	= {ICSIE '24}
}

@InProceedings{	  10.1145/3543873.3587617,
  author	= {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
  title		= {Automated Ontology Evaluation: Evaluating Coverage and
		  Correctness using a Domain Corpus},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587617},
  doi		= {10.1145/3543873.3587617},
  abstract	= {Ontologies conceptualize domains and are a crucial part of
		  web semantics and information systems. However, re-using an
		  existing ontology for a new task requires a detailed
		  evaluation of the candidate ontology as it may cover only a
		  subset of the domain concepts, contain information that is
		  redundant or misleading, and have inaccurate relations and
		  hierarchies between concepts. Manual evaluation of large
		  and complex ontologies is a tedious task. Thus, a few
		  approaches have been proposed for automated evaluation,
		  ranging from concept coverage to ontology generation from a
		  corpus. Existing approaches, however, are limited by their
		  dependence on external structured knowledge sources, such
		  as a thesaurus, as well as by their inability to evaluate
		  semantic relationships. In this paper, we propose a novel
		  framework to automatically evaluate the domain coverage and
		  semantic correctness of existing ontologies based on domain
		  information derived from text. The approach uses a
		  domain-tuned named-entity-recognition model to extract
		  phrasal concepts. The extracted concepts are then used as a
		  representation of the domain against which we evaluate the
		  candidate ontology’s concepts. We further employ a
		  domain-tuned language model to determine the semantic
		  correctness of the candidate ontology’s relations. We
		  demonstrate our automated approach on several large
		  ontologies from the oceanographic domain and show its
		  agreement with a manual evaluation by domain experts and
		  its superiority over the state-of-the-art.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {1127–1137},
  numpages	= {11},
  keywords	= {BERT, knowledge engineering, natural language processing,
		  ontology},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3672608.3707960,
  author	= {Malandri, Lorenzo and Mercorio, Fabio and Serino,
		  Antonio},
  title		= {SkiLLMo: Normalized ESCO Skill Extraction through
		  Transformer Models},
  year		= {2025},
  isbn		= {9798400706295},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672608.3707960},
  doi		= {10.1145/3672608.3707960},
  abstract	= {In recent years, natural language processing (NLP)
		  technologies have made a significant contribution in
		  addressing a number of labour market tasks. One of the most
		  interesting challenges is the automatic extraction of
		  competences from unstructured texts.This paper presents a
		  pipeline for efficiently extracting and standardizing
		  skills from job advertisements using NLP techniques. The
		  proposed methodology leverages open-source Transformer and
		  Large Language Models to extract skills and map them to the
		  European labour market taxonomy, ESCO.To address the
		  computational challenges of processing lengthy job
		  advertisements, a BERT model was fine-tuned to identify
		  text segments likely containing skills. This filtering step
		  reduces noise and ensures that only relevant content is
		  processed further. The filtered text is then passed to an
		  LLM, which extracts implicit and explicit hard and soft
		  skills through prompt engineering. The extracted skills are
		  subsequently matched with entries in a vector store
		  containing the ESCO taxonomy to achieve
		  standardization.Evaluation by domain experts shows that the
		  pipeline achieves a precision of 91\% for skill extraction,
		  80\% for skill standardization and a combined overall
		  precision of 79\%. These results demonstrate the
		  effectiveness of the proposed approach in facilitating
		  structured and standardized skill extraction from job
		  postings.},
  booktitle	= {Proceedings of the 40th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1969–1978},
  numpages	= {10},
  keywords	= {skill extraction, large language models, transformer
		  models, information extraction, labor market},
  location	= {Catania International Airport, Catania, Italy},
  series	= {SAC '25}
}

@Article{	  10.1613/jair.1.19388,
  author	= {Russo, Mayra and Vidal, Maria-Esther},
  title		= {Towards an Ontology-Driven Approach to Document Bias},
  year		= {2025},
  issue_date	= {Aug 2025},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {83},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.19388},
  doi		= {10.1613/jair.1.19388},
  abstract	= {Machine learning (ML)-powered systems are capable of
		  reproducing and often amplifying undesired biases embedded
		  in society, emphasizing the importance of operating under
		  practices that enable the study and understanding of the
		  intrinsic characteristics of ML pipelines. This supports
		  the emergence of documentation frameworks with the idea
		  that “any remedy for bias starts with awareness of its
		  existence.” However, a resource that can formally
		  describe ML pipelines in terms of detected biases is still
		  missing. To address this gap, we present the Doc-BiasO
		  ontology, a resource that sets out to create an integrated
		  vocabulary of biases defined in the Trustworthy AI
		  literature and their measures, as well as to incorporate
		  relevant domain terminology and relationships between them.
		  Overseeing ontology engineering best practices, we reuse
		  existing vocabularies on machine learning and AI to foster
		  knowledge sharing and interoperability between the actors
		  concerned with its research, development, regulation, and
		  others. In addition, we demonstrate the potential of
		  Doc-BiasO with an experiment on an existing benchmark and
		  as part of a neuro-symbolic system. Overall, our main
		  objective is to contribute towards clarifying existing
		  terminology on bias research as it rapidly expands to all
		  areas of AI and to improve the interpretation of bias in
		  data and downstream impact through its documentation.},
  journal	= {J. Artif. Int. Res.},
  month		= aug,
  numpages	= {35},
  keywords	= {Explainable AI, AI in Healthcare, Trustworthy AI}
}

@InProceedings{	  10.1145/3719384.3719446,
  author	= {Lima Carneiro Alves De, Bruno Rucy and Kramer, Merlin and
		  Pinheiro, Victor Henrique Cabral},
  title		= {Distributed Incremental Ontology Reasoning over Dynamic
		  T-boxes},
  year		= {2025},
  isbn		= {9798400717925},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3719384.3719446},
  doi		= {10.1145/3719384.3719446},
  abstract	= {With the advent of Retrieval Augmented Generation (RAG),
		  Knowledge Graphs (KGs) have yet again had a surge in
		  interest in both Academia and Industry, as their use allows
		  for extending the context of Large Language Models (LLMs)
		  by combining traditional vector search with reasoning over
		  Ontologies or Property Graphs encoded as KGs. RAG is a
		  highly dynamic scenario, where the LLM agent might not only
		  retrieve information from a KG or vector store but mutate
		  it as well. This implies eventually there being a greater
		  demand for equally-dynamic KG reasoning systems. We provide
		  a solution to this for the popular ontology language
		  RDF-Schema (RDFS) by showing that computing entailment as a
		  bottom-up query over RDFS graphs with dynamic
		  Terminological Boxes (TBox) and Assertional Boxes (ABox),
		  those where edges and nodes belonging to both boxes can be
		  freely added and removed, can be expressed as an
		  incremental DBSP computation. This computation is then
		  implemented with the distributed computation framework
		  Differential Dataflow (DD), that subsumes DBSP, and
		  compared with a state-of-the-art commercial ontology
		  reasoner. We find that our approach provides more even
		  performance across additions and deletions and a higher
		  potential for scalability across benchmarks with up to 250
		  GBs of data.},
  booktitle	= {Proceedings of the 2024 7th Artificial Intelligence and
		  Cloud Computing Conference},
  pages		= {423–428},
  numpages	= {6},
  keywords	= {ontology reasoning, rdfs, stream processing},
  location	= { },
  series	= {AICCC '24}
}

@InProceedings{	  10.1109/ase56229.2023.00075,
  author	= {Huang, Qing and Wan, Zhenyu and Xing, Zhenchang and Wang,
		  Changjing and Chen, Jieshan and Xu, Xiwei and Lu, Qinghua},
  title		= {Let's Chat to Find the APIs: Connecting Human, LLM and
		  Knowledge Graph through AI Chain},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00075},
  doi		= {10.1109/ASE56229.2023.00075},
  abstract	= {API recommendation methods have evolved from literal and
		  semantic keyword matching to query expansion and query
		  clarification. The latest query clarification method is
		  knowledge graph (KG)-based, but limitations include
		  out-of-vocabulary (OOV) failures and rigid question
		  templates. To address these limitations, we propose a novel
		  knowledge-guided query clarification approach for API
		  recommendation that leverages a large language model (LLM)
		  guided by KG. We utilize the LLM as a neural knowledge base
		  to overcome OOV failures, generating fluent and appropriate
		  clarification questions and options. We also leverage the
		  structured API knowledge and entity relationships stored in
		  the KG to filter out noise, and transfer the optimal
		  clarification path from KG to the LLM, increasing the
		  efficiency of the clarification process. Our approach is
		  designed as an AI chain that consists of five steps, each
		  handled by a separate LLM call, to improve accuracy,
		  efficiency, and fluency for query clarification in API
		  recommendation. We verify the usefulness of each unit in
		  our AI chain, which all received high scores close to a
		  perfect 5. When compared to the baselines, our approach
		  shows a significant improvement in MRR, with a maximum
		  increase of 63.9\% higher when the query statement is
		  covered in KG and 37.2\% when it is not. Ablation
		  experiments reveal that the guidance of knowledge in the KG
		  and the knowledge-guided pathfinding strategy are crucial
		  for our approach's performance, resulting in a 19.0\% and
		  22.2\% increase in MAP, respectively. Our approach
		  demonstrates a way to bridge the gap between KG and LLM,
		  effectively compensating for the strengths and weaknesses
		  of both.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {471–483},
  numpages	= {13},
  keywords	= {API recommendation, query clarification, knowledge graph,
		  large language model, out-of-vocabulary},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@InProceedings{	  10.1145/3733155.3736601,
  author	= {Bhattacharya, Sukriti and Naudet, Yannick},
  title		= {Integrating Ontology with Deep Reinforcement Learning: A
		  Formal Framework for Explainability in Robotic
		  Applications},
  year		= {2025},
  isbn		= {9798400714023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3733155.3736601},
  doi		= {10.1145/3733155.3736601},
  abstract	= {This paper presents a formal framework that integrates an
		  ontology with Deep Reinforcement Learning (DRL) to enhance
		  explainability in AI-driven robotic applications. By
		  mapping DRL components to ontological concepts, our
		  approach generates human-readable explanations for complex
		  model decisions. We demonstrate the framework through a
		  robotic arm pick-and-place task, illustrating improved
		  transparency and interpretability. The integration
		  facilitates domain knowledge incorporation while addressing
		  the critical need for explainable AI systems. Despite
		  challenges in computational overhead and ontological
		  alignment, our contribution advances trustworthy AI systems
		  that support effective human-AI collaboration.},
  booktitle	= {Proceedings of the 18th ACM International Conference on
		  PErvasive Technologies Related to Assistive Environments},
  pages		= {60–63},
  numpages	= {4},
  keywords	= {Ontology, Deep Reinforcement Learning, Explainability,
		  Transparency, Interpretability, Human-AI Collaboration},
  location	= { },
  series	= {PETRA '25}
}

@Article{	  10.1145/3719206,
  author	= {Cremaschi, Marco and D'Adda, Fabio and Maurino, Andrea},
  title		= {stEELlm: An LLM for Generating Semantic Annotations of
		  Tabular Data},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3719206},
  doi		= {10.1145/3719206},
  abstract	= {The capabilities of LLMs represent a pivotal step in
		  transforming how we manage and interact with information
		  and data. We witness an increasingly pervasive use of such
		  models in various computational tasks. In some preliminary
		  works, attempts to integrate Knowledge Graphs and Large
		  Language Models (LLMs) can be identified, in particular, to
		  perform the classic tasks related to the construction of
		  Knowledge Graphs through semantic annotation of texts.
		  Nowadays, tables are widely used and play a crucial role in
		  creating, organising, and sharing information that could be
		  used to produce factual knowledge to be integrated into a
		  Knowledge Graph. However, table-to-KG techniques through
		  LLM have not been extensively investigated. This paper
		  presents stEELlm, an innovative Semantic Table
		  Interpretation approach obtained by fine-tuning the Mixtral
		  8x7B model. Conducted experiments demonstrate the
		  capabilities of our model to successfully create semantic
		  annotations of heterogeneous datasets, a scenario where
		  classic approaches based on heuristics tend to fail.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= feb,
  keywords	= {Large Language Models, Knowledge Graphs, Pre-training,
		  Fine-tuning, Prompt Engineering, Semantic Table
		  Interpretation}
}

@Article{	  10.1145/3582496,
  author	= {Shafi, Jawad and Adeel Nawab, Rao Muhammad and Rayson,
		  Paul},
  title		= {Semantic Tagging for the Urdu Language: Annotated Corpus
		  and Multi-Target Classification Methods},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3582496},
  doi		= {10.1145/3582496},
  abstract	= {Extracting and analysing meaning-related information from
		  natural language data has attracted the attention of
		  researchers in various fields, such as natural language
		  processing, corpus linguistics, information retrieval, and
		  data science. An important aspect of such automatic
		  information extraction and analysis is the annotation of
		  language data using semantic tagging tools. Different
		  semantic tagging tools have been designed to carry out
		  various levels of semantic analysis, for instance, named
		  entity recognition and disambiguation, sentiment analysis,
		  word sense disambiguation, content analysis, and semantic
		  role labelling. Common to all of these tasks, in the
		  supervised setting, is the requirement for a manually
		  semantically annotated corpus, which acts as a knowledge
		  base from which to train and test potential word and
		  phrase-level sense annotations. Many benchmark corpora have
		  been developed for various semantic tagging tasks, but most
		  are for English and other European languages. There is a
		  dearth of semantically annotated corpora for the Urdu
		  language, which is widely spoken and used around the world.
		  To fill this gap, this study presents a large benchmark
		  corpus and methods for the semantic tagging task for the
		  Urdu language. The proposed corpus contains 8,000 tokens in
		  the following domains or genres: news, social media,
		  Wikipedia, and historical text (each domain having 2K
		  tokens). The corpus has been manually annotated with 21
		  major semantic fields and 232 sub-fields with the USAS
		  (UCREL Semantic Analysis System) semantic taxonomy which
		  provides a comprehensive set of semantic fields for
		  coarse-grained annotation. Each word in our proposed corpus
		  has been annotated with at least one and up to nine
		  semantic field tags to provide a detailed semantic analysis
		  of the language data, which allowed us to treat the problem
		  of semantic tagging as a supervised multi-target
		  classification task. To demonstrate how our proposed corpus
		  can be used for the development and evaluation of Urdu
		  semantic tagging methods, we extracted local, topical and
		  semantic features from the proposed corpus and applied
		  seven different supervised multi-target classifiers to
		  them. Results show an accuracy of 94\% on our proposed
		  corpus which is free and publicly available to download.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {175},
  numpages	= {32},
  keywords	= {Urdu corpus annotation, multi-target classifiers, semantic
		  annotation, semantic tagger}
}

@Proceedings{	  10.1145/3639233,
  title		= {NLPIR '23: Proceedings of the 2023 7th International
		  Conference on Natural Language Processing and Information
		  Retrieval},
  year		= {2023},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Seoul, Republic of Korea}
}

@InProceedings{	  10.1145/3704440.3704788,
  author	= {Lehmann, Ren\'{e}},
  title		= {Towards Interoperability of APIs - an LLM-based approach},
  year		= {2024},
  isbn		= {9798400713545},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704440.3704788},
  doi		= {10.1145/3704440.3704788},
  abstract	= {Applications that integrate multiple Application
		  Programming Interfaces (APIs) often face challenges due to
		  data and application heterogeneity, complicating the
		  integration process. Our proposed approach leverages Large
		  Language Models (LLMs) to translate API function calls and
		  responses into natural language, enabling communication
		  between API consumers and providers without requiring them
		  to adhere to the same communication protocols or data
		  formats. By abstracting technical complexities, this
		  approach addresses both syntactic and semantic differences.
		  We outline the potential of LLMs to improve API
		  interoperability and discuss strategies for optimizing
		  performance, reducing overhead, and ensuring system
		  reliability through error detection and correction.},
  booktitle	= {Proceedings of the 25th International Middleware
		  Conference: Demos, Posters and Doctoral Symposium},
  pages		= {29–30},
  numpages	= {2},
  keywords	= {Application Heterogeneity, Application Programming
		  Interface, Data Heterogeneity, Interoperability, Large
		  Language Model},
  location	= {Hong Kong, Hong Kong},
  series	= {Middleware '24}
}

@Article{	  10.1145/3729218,
  author	= {Sun, Jiankai and Zheng, Chuanyang and Xie, Enze and Liu,
		  Zhengying and Chu, Ruihang and Qiu, Jianing and Xu, Jiaqi
		  and Ding, Mingyu and Li, Hongyang and Geng, Mengzhe and Wu,
		  Yue and Wang, Wenhai and Chen, Junsong and Yin, Zhangyue
		  and Ren, Xiaozhe and Fu, Jie and He, Junxian and Wu, Yuan
		  and Liu, Qi and Liu, Xihui and Li, Yu and Dong, Hao and
		  Cheng, Yu and Zhang, Ming and Heng, Pheng Ann and Dai,
		  Jifeng and Luo, Ping and Wang, Jingdong and Wen, Ji-Rong
		  and Qiu, Xipeng and Guo, Yike and Xiong, Hui and Liu, Qun
		  and Li, Zhenguo},
  title		= {A Survey of Reasoning with Foundation Models: Concepts,
		  Methodologies, and Outlook},
  year		= {2025},
  issue_date	= {November 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {11},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3729218},
  doi		= {10.1145/3729218},
  abstract	= {Reasoning, a crucial ability for complex problem-solving,
		  plays a pivotal role in various real-world settings such as
		  negotiation, medical diagnosis, and criminal investigation.
		  It serves as a fundamental methodology in the field of
		  Artificial General Intelligence (AGI). With the ongoing
		  development of foundation models, there is a growing
		  interest in exploring their abilities in reasoning tasks.
		  In this article, we introduce seminal foundation models
		  proposed or adaptable for reasoning, highlighting the
		  latest advancements in various reasoning tasks, methods,
		  and benchmarks. We then delve into the potential future
		  directions behind the emergence of reasoning abilities
		  within foundation models. We also discuss the relevance of
		  multimodal learning, autonomous agents, and super alignment
		  in the context of reasoning. By discussing these future
		  research directions, we hope to inspire researchers in
		  their exploration of this field, stimulate further
		  advancements in reasoning with foundation models, e.g.,
		  Large Language Models (LLMs), and contribute to the
		  development of AGI.},
  journal	= {ACM Comput. Surv.},
  month		= jun,
  articleno	= {278},
  numpages	= {43},
  keywords	= {Reasoning, foundation models, multimodal, AI agent,
		  artificial general intelligence}
}

@InProceedings{	  10.1145/3318236.3318246,
  author	= {Cristea, Daniela-Maria and Trofin, Bogdan-Gabriel},
  title		= {A Historical Ontology of Semi-automatic Specification
		  Extraction from Romanian Language},
  year		= {2019},
  isbn		= {9781450362450},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318236.3318246},
  doi		= {10.1145/3318236.3318246},
  abstract	= {The increasing need for ontologies with practical results
		  and the difficulties of manual construction guide us
		  towards methods that propose automatic and semi-automated
		  techniques for creating ontologies. The article presents a
		  conceptual method for building ontology for historical
		  documents aiming to support historians and researchers to
		  organize metadata, search for relevant factors, extract
		  information and derive new knowledge for localized metadata
		  from documents.We envision its use for educational purposes
		  and for the benefit of the community through several
		  aspects, such as scientific utility, accessibility to
		  documents from the late medieval period to explore events
		  or personal histories of 14-16 centuries.As a case study,
		  we implemented a semi-automatic method for extracting
		  instances from a category of Romanian documents that
		  generate a historical ontology. The results are evaluated
		  on the basis of the need for an automated tool comparable
		  to those found in the literature for other languages.},
  booktitle	= {Proceedings of the 2019 2nd International Conference on
		  Geoinformatics and Data Analysis},
  pages		= {125–129},
  numpages	= {5},
  keywords	= {Historical document, historical domain, ontology
		  population, semi-automatic ontology extraction},
  location	= {Prague, Czech Republic},
  series	= {ICGDA '19}
}

@Article{	  10.1145/3569927,
  author	= {Broy, Manfred and Rumpe, Bernhard},
  title		= {Development Use Cases for Semantics-Driven Modeling
		  Languages},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {66},
  number	= {5},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3569927},
  doi		= {10.1145/3569927},
  abstract	= {Choosing underlying semantic theories and definition
		  techniques must closely follow intended use cases for the
		  modeling language.},
  journal	= {Commun. ACM},
  month		= apr,
  pages		= {62–71},
  numpages	= {10}
}

@Article{	  10.1145/3676956,
  author	= {Pei, Jiahuan and Yan, Guojun and De Rijke, Maarten and
		  Ren, Pengjie},
  title		= {Mixture-of-Languages Routing for Multilingual Dialogues},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3676956},
  doi		= {10.1145/3676956},
  abstract	= {We consider multilingual dialogue systems and ask how the
		  performance of a dialogue system can be improved by using
		  information that is available in other languages than the
		  language in which a conversation is being conducted. We
		  adopt a collaborative chair-experts framework, where each
		  expert agent can be either monolingual or cross-lingual,
		  and a chair agent follows a mixture-of-experts procedure
		  for globally optimizing multilingual task-oriented dialogue
		  systems. We propose a mixture-of-languages routing
		  framework that includes four functional components, i.e.,
		  input embeddings of multilingual dialogues, language model,
		  pairwise alignment between the representation of every two
		  languages, and mixture-of-languages. We quantify language
		  characteristics of unity and diversity using a number of
		  similarity metrics, i.e., genetic similarity and word and
		  sentence similarity based on embeddings. Our main finding
		  is that the performance of multilingual task-oriented
		  dialogue systems can be greatly impacted by three key
		  aspects, i.e., data sufficiency, language characteristics,
		  and model design in a mixture-of-languages routing
		  framework.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= oct,
  articleno	= {165},
  numpages	= {33},
  keywords	= {multilingual systems, task-oriented dialogue systems,
		  collaborative agents, mixture-of-experts}
}

@InProceedings{	  10.1145/3550356.3561534,
  author	= {Cornelis, Milan and Vanommeslaeghe, Yon and Van Acker,
		  Bert and De Meulenaere, Paul},
  title		= {An ontology DSL for the co-design of mechatronic systems},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561534},
  doi		= {10.1145/3550356.3561534},
  abstract	= {The complexity of mechatronic systems is vastly
		  increasing. Therefore, the design of these systems requires
		  different engineering domains, e.g., the mechanical,
		  electrical, and control domains, to work together. The
		  different domains often work in parallel to gain efficiency
		  in this so-called co-design process. However, the design
		  choices made by engineers in one domain can influence
		  parameters in another domain. Too little or even no
		  knowledge about these cross-domain influences may later
		  lead to system integration problems or to degraded system
		  performance. Solving these problems requires taking steps
		  back in the development process, causing a higher design
		  cost. In order to improve this cross-domain collaboration,
		  we propose using ontologies to assist the co-design process
		  by explicitly capturing the design dependencies, both
		  within and across the engineering domains. However,
		  designing ontologies can be complex and is labor-intensive,
		  especially if one relies on generic ontology languages like
		  the Web Ontology Language 2 (OWL 2). Therefore, we created
		  a Domain Specific Language (DSL) focusing on the essential
		  complexity, which enables engineers to design a
		  cross-domain system ontology in a consistent and
		  straightforward way. We elaborate on the metamodel for this
		  DSL, discuss the realization of a prototype tool, and
		  demonstrate how one can then reason on this ontology to
		  derive new information about the various cross-domain
		  design relationships.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {633–642},
  numpages	= {10},
  keywords	= {co-design, domain-specific language, mechatronics,
		  metamodel, ontology},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3742876.3742882,
  author	= {Esterhuyse, Christopher A. and M\"{u}ller, Tim and van
		  Binsbergen, L. Thomas},
  title		= {A Stable Model Semantics for eFLINT Norm Specifications
		  and Model Checking Scenarios},
  year		= {2025},
  isbn		= {9798400719950},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3742876.3742882},
  doi		= {10.1145/3742876.3742882},
  abstract	= {Since its introduction at GPCE2020, the eFLINT norm
		  specification language has been used in academic and
		  industrial applications to specify and automate compliance
		  for various norms, such as privacy regulations and data
		  processing agreements. The eFLINT interpreter has been used
		  to automate the analysis of real-time or historical cases
		  by computing logical consequences and reporting normative
		  violations. To support future language and tooling
		  developments, we contribute a formal definition of the
		  language as a translation to first-order logic programming
		  with stable model semantics. The described semantics aligns
		  with the previous semi-formal descriptions of the language,
		  but resolves issues relating to logical inference with
		  negative antecedent and aggregation operators.
		  Specifically, we formalise the connection between eFLINT's
		  derivation rules and Horn clauses under the stable model
		  semantics. Secondly, by repurposing the Clingo answer-set
		  solver as a highly-optimised eFLINT interpreter, we extend
		  the toolset for eFLINT with model-checking abstract
		  properties in addition to case analysis. We evaluate the
		  new semantics and interpreter via an empirical comparison
		  of the existing implementation to our prototype
		  implementation. We observe that the expected subset of our
		  tests have the equivalent behaviours.},
  booktitle	= {Proceedings of the 24th ACM SIGPLAN International
		  Conference on Generative Programming: Concepts and
		  Experiences},
  pages		= {80–93},
  numpages	= {14},
  keywords	= {answer set solving, dynamic semantics, logic programming,
		  model checking, norm, specification languages},
  location	= {Bergen, Norway},
  series	= {GPCE '25}
}

@InProceedings{	  10.5555/3511065.3511090,
  author	= {Finidori, Helene},
  title		= {From pattern language to pattern literacy: the biosemiotic
		  underpinnings of "patterning" and "languaging"},
  year		= {2022},
  isbn		= {9781941652169},
  publisher	= {The Hillside Group},
  address	= {USA},
  abstract	= {The key inquiry in this paper, to move the discussion
		  forward on innovation and the future of Pattern Language,
		  is about the relationship between patterns (and more
		  precisely our capacities as humans to recognize and use
		  patterns, aka 'patterning') and language (both in its form
		  and in the processes of 'languaging' involved), in order to
		  assess how each can be leveraged in understanding and
		  communication, within and across domains. I dive here deep
		  into the biological and bio-semiotic underpinnings of
		  patterning and languaging, seeking to make a clear
		  distinction between them. I explore the nature and
		  "timeless properties" of patterns as signs and their role
		  in the emergence of human cognition and language from an
		  evolutionary perspective. In particular I examine their
		  involvement in 'habit taking' and in the coordination of
		  unselfconscious action and creative processes, such as
		  evoked by Christopher Alexander.This paper does not provide
		  solutions or answers, it sets a foundation to show how the
		  development of a pattern literacy around patterns seen as
		  basic units for the coordination of action and the
		  understanding of the world, beyond domain knowledge and
		  linguistic divides, could bring new possibilities for the
		  study and orientation of socio-ecological and
		  socio-technological systems. This will open up
		  opportunities to further explore how pattern languages
		  could be understood and applied towards this objective, in
		  order to actually realize their potential as lingua
		  franca.},
  booktitle	= {Proceedings of the 27th Conference on Pattern Languages of
		  Programs},
  articleno	= {18},
  numpages	= {34},
  keywords	= {action research, boundary objects, complex adaptive
		  modeling, complex systems, participatory inquiry, pattern
		  language, pattern literacy, semiotics},
  location	= {Virtual Event},
  series	= {PLoP '20}
}

@InProceedings{	  10.1145/3516492.3558811,
  author	= {Ong, Ethel},
  title		= {How HCI can Inform the Design of Human Language
		  Technologies},
  year		= {2023},
  isbn		= {9781450392501},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3516492.3558811},
  doi		= {10.1145/3516492.3558811},
  abstract	= {In this paper, we present how practices in human-computer
		  interaction can be used to inform the design of human
		  language technologies. We begin with a description of human
		  language technologies, paying particular attention to the
		  collaborative nature of dialogue systems that necessitates
		  a strong interaction between man and machine. We then share
		  our insights from our application of principles and
		  practices of human computer interaction in the design and
		  evaluation of these conversational interfaces.},
  booktitle	= {Proceedings of the Asian HCI Symposium 2022},
  pages		= {44–47},
  numpages	= {4},
  keywords	= {Conversational interfaces, Dialogue systems,
		  Human-language technologies},
  location	= {New Orleans, LA, USA},
  series	= {Asian HCI '22}
}

@Article{	  10.1145/3592599,
  author	= {Jin, Dawei and Hu, Yiyi and Chen, Jingyu and Xia,
		  Mengran},
  title		= {Stock Price Trends Prediction Based on the Classical
		  Models with Key Information Fusion of Ontologies},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3592599},
  doi		= {10.1145/3592599},
  abstract	= {An ontology of the financial field can support effective
		  association and integration of financial knowledge. Based
		  on behavioral finance, social media is increasingly applied
		  as one of the data sources for information fusion in stock
		  forecasting to approximate the patterns of market changes.
		  By predicting Tesla (TSLA) stock price trends, this study
		  finds that satisfactory forecasting results can be achieved
		  using classical models and incorporating key information
		  features from the technical indicator ontology class and
		  the investor behavior ontology class, even in the face of
		  the impact of the COVID-19 epidemic. In the post-epidemic
		  period, the back propagation neural network (BPNN) model is
		  used to predict the price trend of TSLA for the next five
		  trading days with an accuracy of up to 91.34\%, an F1 score
		  of 0.91, and a return of up to 268.42\% obtained from
		  simulated trading. This study extends the research on stock
		  forecasting using fused information in the ontology of the
		  financial field, providing a new basis for general
		  investors in the selection of fusion information and the
		  application of trading strategies and providing effective
		  support for organizations to make intelligent financial
		  decisions under uncertainty.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {154},
  numpages	= {22},
  keywords	= {Financial ontology, feature fusion, trends prediction,
		  social media, sentiment analysis}
}

@Article{	  10.14778/3742728.3742761,
  author	= {Chung, Yeounoh and Kakkar, Gaurav T. and Gan, Yu and
		  Milne, Brenton and \"{O}zcan, Fatma},
  title		= {Is Long Context All You Need? Leveraging LLM's Extended
		  Context for NL2SQL},
  year		= {2025},
  issue_date	= {April 2025},
  publisher	= {VLDB Endowment},
  volume	= {18},
  number	= {8},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3742728.3742761},
  doi		= {10.14778/3742728.3742761},
  abstract	= {Large Language Models (LLMs) have demonstrated impressive
		  capabilities across a range of natural language processing
		  tasks. In particular, improvements in reasoning abilities
		  and the expansion of context windows have opened new
		  avenues for leveraging these powerful models. NL2SQL is
		  challenging in that the natural language question is
		  inherently ambiguous, while the SQL generation requires a
		  precise understanding of complex data schema and semantics.
		  One approach to this semantic ambiguous problem is to
		  provide more and sufficient contextual information.In this
		  work, we explore the performance and the latency tradeoffs
		  of the extended context window (a.k.a., long context)
		  offered by Google's state-of-the-art LLM (gemini-1.5-pro).
		  We study the impact of various contextual information,
		  including column example values, question and SQL query
		  pairs, user-provided hints, SQL documentation, and schema.
		  To the best of our knowledge, this is the first work to
		  study how the extended context window and extra contextual
		  information can help NL2SQL generation with respect to both
		  accuracy and latency cost. We show that long context LLMs
		  are robust and do not get lost in the extended contextual
		  information. Additionally, our long-context NL2SQL pipeline
		  based on Google's Gemini-pro-1.5 achieves strong
		  performance across multiple benchmark datasets without
		  fine-tuning or expensive self-consistency based
		  techniques.},
  journal	= {Proc. VLDB Endow.},
  month		= sep,
  pages		= {2735–2747},
  numpages	= {13}
}

@Article{	  10.1145/3528576,
  author	= {Bashir, Muhammad Farrukh and Javed, Abdul Rehman and
		  Arshad, Muhammad Umair and Gadekallu, Thippa Reddy and
		  Shahzad, Waseem and Beg, Mirza Omer},
  title		= {Context-aware Emotion Detection from Low-resource Urdu
		  Language Using Deep Neural Network},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3528576},
  doi		= {10.1145/3528576},
  abstract	= {Emotion detection (ED) plays a vital role in determining
		  individual interest in any field. Humans use gestures,
		  facial expressions, and voice pitch and choose words to
		  describe their emotions. Significant work has been done to
		  detect emotions from the textual data in English, French,
		  Chinese, and other high-resource languages. However,
		  emotion classification has not been well studied in
		  low-resource languages (i.e., Urdu) due to the lack of
		  labeled corpora. This article presents a publicly available
		  Urdu Nastalique Emotions Dataset (UNED) of sentences and
		  paragraphs annotated with different emotions and proposes a
		  deep learning (DL)-based technique for classifying emotions
		  in the UNED corpus. Our annotated UNED corpus has six
		  emotions for both paragraphs and sentences. We perform
		  extensive experimentation to evaluate the quality of the
		  corpus and further classify it using machine learning and
		  DL approaches. Experimental results show that the developed
		  DL-based model performs better than generic machine
		  learning approaches with an F1 score of 85\% on the UNED
		  sentence-based corpus and 50\% on the UNED paragraph-based
		  corpus.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {131},
  numpages	= {30},
  keywords	= {Emotion detection (ED), Urdu Nastalique Emotions Dataset
		  (UNED), annotated UNED corpus}
}

@InProceedings{	  10.1145/3627673.3680009,
  author	= {Agrawal, Sanjay and Merugu, Srujana and Sembium, Vivek},
  title		= {Boosting Entity Recognition by leveraging Cross-task
		  Domain Models for Weak Supervision},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680009},
  doi		= {10.1145/3627673.3680009},
  abstract	= {Entity Recognition (ER) is a common natural language
		  processing task encountered in a number of real-world
		  applications. For common domains and named entities such as
		  places and organisations, there exists sufficient high
		  quality annotated data and foundational models such as T5
		  and GPT-3.5 also provide highly accurate predictions.
		  However, for niche domains such as e-commerce and medicine
		  with specialized entity types, there is a paucity of
		  labeled data since manual labeling of tokens is often
		  time-consuming and expensive, which makes entity
		  recognition challenging for such domains. Recent works such
		  as NEEDLE [48] propose hybrid solutions to efficiently
		  combine a small amount of strongly labeled
		  (human-annotated) with a large amount of weakly labeled
		  (distant supervision) data to yield superior performance
		  relative to supervised training. The extensive noise in the
		  weakly labeled data, however, remains a challenge. In this
		  paper, we propose WeSDoM (Weak Supervision with Domain
		  Models), which leverages pretrained encoder models from the
		  same domain but different tasks to create domain ontologies
		  that can enable the creation of less noisy weakly labeled
		  data. Experiments on internal e-commerce and public
		  biomedical NER datasets demonstrate that WeSDoM outperforms
		  existing SOTA baselines by a significant margin. We achieve
		  new SOTA F1 scores on two popular Biomedical NER datasets,
		  BC5CDR-chem 94.27, BC5CDR-disease 91.23.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4324–4331},
  numpages	= {8},
  keywords	= {cross-task domain encoder, entity recognition, ontologies,
		  weak supervision},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3663529.3663861,
  author	= {Goel, Drishti and Husain, Fiza and Singh, Aditya and
		  Ghosh, Supriyo and Parayil, Anjaly and Bansal, Chetan and
		  Zhang, Xuchao and Rajmohan, Saravan},
  title		= {X-Lifecycle Learning for Cloud Incident Management using
		  LLMs},
  year		= {2024},
  isbn		= {9798400706585},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3663529.3663861},
  doi		= {10.1145/3663529.3663861},
  abstract	= {Incident management for large cloud services is a complex
		  and tedious process that requires a significant amount of
		  manual effort from on-call engineers (OCEs). OCEs typically
		  leverage data from different stages of the software
		  development lifecycle [SDLC] (e.g., codes, configuration,
		  monitor data, service properties, service dependencies,
		  trouble-shooting documents, etc.) to generate insights for
		  detection, root cause analysis and mitigation of incidents.
		  Recent advancements in large language models [LLMs] (e.g.,
		  ChatGPT, GPT-4, Gemini) have created opportunities to
		  automatically generate contextual recommendations for the
		  OCEs, assisting them in quickly identifying and mitigating
		  critical issues. However, existing research typically takes
		  a silo-ed view of solving a certain task in incident
		  management by leveraging data from a single stage of the
		  SDLC. In this paper, we demonstrate that augmenting
		  additional contextual data from different stages of the
		  SDLC improves the performance of two critically important
		  and practically challenging tasks: (1) automatically
		  generating root cause recommendations for dependency
		  failure related incidents, and (2) identifying the ontology
		  of service monitors used for automatically detecting
		  incidents. By leveraging a dataset of 353 incidents and 260
		  monitors from Microsoft, we demonstrate that augmenting
		  contextual information from different stages of the SDLC
		  improves the performance over state-of-the-art methods.},
  booktitle	= {Companion Proceedings of the 32nd ACM International
		  Conference on the Foundations of Software Engineering},
  pages		= {417–428},
  numpages	= {12},
  keywords	= {Cloud Services, Large language models, Monitor management,
		  Reliability, Root-cause analysis},
  location	= {Porto de Galinhas, Brazil},
  series	= {FSE 2024}
}

@Article{	  10.1613/jair.1.14061,
  author	= {Kurucz, Agi and Ryzhikov, Vladislav and Savateev, Yury and
		  Zakharyaschev, Michael},
  title		= {Deciding FO-rewritability of Regular Languages and
		  Ontology-Mediated Queries in Linear Temporal Logic},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {76},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.14061},
  doi		= {10.1613/jair.1.14061},
  abstract	= {Our concern is the problem of determining the data
		  complexity of answering an ontology-mediated query (OMQ)
		  formulated in linear temporal logic LTL over (Z,&lt;) and
		  deciding whether it is rewritable to an FO(&lt;)-query,
		  possibly with some extra predicates. First, we observe
		  that, in line with the circuit complexity and
		  FO-definability of regular languages, OMQ answering in AC0,
		  ACC0&nbsp;and NC1&nbsp;coincides with
		  FO(&lt;,≡)-rewritability using unary
		  predicates&nbsp;x&nbsp;≡ 0 (mod&nbsp;n),
		  FO(&lt;,MOD)-rewritability, and FO(RPR)-rewritability using
		  relational primitive recursion, respectively. We prove
		  that, similarly to known PSᴘᴀᴄᴇ-completeness of
		  recognising FO(&lt;)-definability of regular languages,
		  deciding FO(&lt;,≡)- and FO(&lt;,MOD)-definability is
		  also PSᴘᴀᴄᴇ-complete (unless ACC0&nbsp;= NC1). We
		  then use this result to show that deciding FO(&lt;)-,
		  FO(&lt;,≡)- and FO(&lt;,MOD)-rewritability of LTL OMQs is
		  ExᴘSᴘᴀᴄᴇ-complete, and that these problems become
		  PSᴘᴀᴄᴇ-complete for OMQs with a linear Horn
		  ontology and an atomic query, and also a positive query in
		  the cases of FO(&lt;)- and FO(&lt;,≡)-rewritability.
		  Further, we consider FO(&lt;)-rewritability of OMQs with a
		  binary-clause ontology and identify OMQ classes, for which
		  deciding it is PSᴘᴀᴄᴇ-, Π2p- and coNP-complete.},
  journal	= {J. Artif. Int. Res.},
  month		= may,
  numpages	= {59}
}

@InProceedings{	  10.1145/3600046.3600050,
  author	= {Zhang, Wei and Chen, Perry and Yang, Jian and Tang,
		  Yongping and Su, Jianwen},
  title		= {A Capability Description Language Design for Data
		  Products},
  year		= {2023},
  isbn		= {9798400708466},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600046.3600050},
  doi		= {10.1145/3600046.3600050},
  abstract	= {Data has become a crucial factor driving the global
		  economy, from various perspectives, including economics,
		  technological advancements, digitization, and data
		  analytics technologies. Data products are specific,
		  ready-made data sets that have been enriched with metadata
		  and designed to achieve particular outcomes. However, there
		  is currently limited research on how to provide efficient
		  and automated data product generation and sharing services.
		  This paper proposes an approach to extract descriptive data
		  capabilities to automate these processes. We examine
		  real-world datasets from the Internet of Things (IoT) and
		  user scenarios to extract selection capabilities and
		  retrieval capabilities from the working data. A data access
		  model is presented based on these capabilities. These
		  preliminary results form the foundation for automating data
		  product generation.},
  booktitle	= {Proceedings of the Second ACM Data Economy Workshop},
  pages		= {21–26},
  numpages	= {6},
  keywords	= {Data Capability, Data Economy, Data Product, Data
		  Service},
  location	= {Seattle, WA, USA},
  series	= {DEC '23}
}

@InProceedings{	  10.1145/3613904.3642698,
  author	= {Liu, Yiren and Chen, Si and Cheng, Haocong and Yu, Mengxia
		  and Ran, Xiao and Mo, Andrew and Tang, Yiliu and Huang,
		  Yun},
  title		= {How AI Processing Delays Foster Creativity: Exploring
		  Research Question Co-Creation with an LLM-based Agent},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642698},
  doi		= {10.1145/3613904.3642698},
  abstract	= {Developing novel research questions (RQs) often requires
		  extensive literature reviews, especially in
		  interdisciplinary fields. To support RQ development through
		  human-AI co-creation, we leveraged Large Language Models
		  (LLMs) to build an LLM-based agent system named CoQuest. We
		  conducted an experiment with 20 HCI researchers to examine
		  the impact of two interaction designs: breadth-first and
		  depth-first RQ generation. The findings revealed that
		  participants perceived the breadth-first approach as more
		  creative and trustworthy upon task completion. Conversely,
		  during the task, participants considered the depth-first
		  generated RQs as more creative. Additionally, we discovered
		  that AI processing delays allowed users to reflect on
		  multiple RQs simultaneously, leading to a higher quantity
		  of generated RQs and an enhanced sense of control. Our work
		  makes both theoretical and practical contributions by
		  proposing and evaluating a mental model for human-AI
		  co-creation of RQs. We also address potential ethical
		  issues, such as biases and over-reliance on AI, advocating
		  for using the system to improve human research creativity
		  rather than automating scientific inquiry. The system’s
		  source is available at:
		  https://github.com/yiren-liu/coquest.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {17},
  numpages	= {25},
  keywords	= {Co-creation Systems, Large Language Models,
		  Mixed-initiative Design, Scientifc Discovery},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@Article{	  10.1145/3708478,
  author	= {Meng, Zhixin and Zhan, Shaoxiong and Xu, Ruiqing and
		  Mayer, Wolfgang and Zhu, Ye and Zhang, Hong-Yu and He,
		  Chuan and He, Keqing and Cheng, Debo and Feng, Zaiwen},
  title		= {Domain Ontology-Driven Knowledge Graph Generation from
		  Text},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708478},
  doi		= {10.1145/3708478},
  abstract	= {A knowledge graph serves as a unified and standardized
		  representation for extracting and representing textual
		  information. In the field of knowledge extraction and
		  representation research, named entity recognition and
		  relation extraction provide effective solutions for
		  knowledge graph generation tasks. However, it is a
		  challenge that lies in extracting domain-specific knowledge
		  from the rich and general textual corpora and generating
		  corresponding domain knowledge graphs to support
		  domain-specific reasoning, question-answering, and
		  decision-making tasks. The hierarchical domain knowledge
		  representation model (i.e. domain ontology) provides a
		  solution for this problem. Therefore, we propose an
		  end-to-end approach based on domain ontology embedding and
		  pre-trained language models for domain knowledge graph
		  generation from text, which incorporates domain node
		  recognition and domain relation extraction phases. We
		  evaluated our domain ontology-driven model on the
		  Wikidata-TekGen dataset and the DBpedia-WebNLG dataset, and
		  the results indicate that our approach based on the
		  pre-trained language models with fewer parameters compared
		  with the baseline models has significantly contributed to
		  the domain knowledge graph generation without prompts.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Probab. Mach. Learn.},
  month		= dec,
  keywords	= {Domain Knowledge Graph, Domain Ontology, Ontology
		  Embedding, Domain Node Recognition, Domain Relation
		  Extraction}
}

@Article{	  10.1145/3468889,
  author	= {Zhang, Ruqing and Guo, Jiafeng and Chen, Lu and Fan,
		  Yixing and Cheng, Xueqi},
  title		= {A Review on Question Generation from Natural Language
		  Text},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3468889},
  doi		= {10.1145/3468889},
  abstract	= {Question generation is an important yet challenging
		  problem in Artificial Intelligence (AI), which aims to
		  generate natural and relevant questions from various input
		  formats, e.g., natural language text, structure database,
		  knowledge base, and image. In this article, we focus on
		  question generation from natural language text, which has
		  received tremendous interest in recent years due to the
		  widespread applications such as data augmentation for
		  question answering systems. During the past decades, many
		  different question generation models have been proposed,
		  from traditional rule-based methods to advanced neural
		  network-based methods. Since there have been a large
		  variety of research works proposed, we believe it is the
		  right time to summarize the current status, learn from
		  existing methodologies, and gain some insights for future
		  development. In contrast to existing reviews, in this
		  survey, we try to provide a more comprehensive taxonomy of
		  question generation tasks from three different
		  perspectives, i.e., the types of the input context text,
		  the target answer, and the generated question. We take a
		  deep look into existing models from different dimensions to
		  analyze their underlying ideas, major design principles,
		  and training strategies We compare these models through
		  benchmark tasks to obtain an empirical understanding of the
		  existing techniques. Moreover, we discuss what is missing
		  in the current literature and what are the promising and
		  desired future directions.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= sep,
  articleno	= {14},
  numpages	= {43},
  keywords	= {Question generation, natural language generation, survey}
}

@InProceedings{	  10.5555/3400397.3400443,
  author	= {Padilla, Jose J and Shuttleworth, David and O'Brien,
		  Kevin},
  title		= {Agent-based model characterization using natural language
		  processing},
  year		= {2020},
  isbn		= {9781728132839},
  publisher	= {IEEE Press},
  abstract	= {This paper reports on Natural Language Processing (NLP) as
		  a technique to analyze phenomena towards specifying
		  agent-based models (ABM). The objective of the ABM NLP
		  Analyzer is to facilitate non-simulationists to actively
		  engage in the learning and collaborative designing of ABMs.
		  The NLP model identifies candidate agents, candidate agent
		  attributes, and candidate rules all of which
		  non-simulationists can later evaluate for feasibility.
		  IBM's Watson Natural Language Understanding (NLU) and
		  Knowledge Studio were used in order to annotate, evaluate,
		  extract agents, agent attributes, and agent rules from
		  unstructured descriptions of phenomena. The software, and
		  related agent-attribute-rule characterization, provides
		  insight into a simple but useful means of conceptualizing
		  and specifying baseline ABMs. Further, it emphasizes on how
		  to approach the design of ABMs without the use of NLP by
		  focusing on the identification of agent, attributes and
		  rules.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {560–571},
  numpages	= {12},
  location	= {National Harbor, Maryland},
  series	= {WSC '19}
}

@Article{	  10.1145/3705322,
  author	= {Zhang, Jianrong and Fan, Hehe and Yang, Yi},
  title		= {Protein Captioning: Bridging the Gap between Protein
		  Sequences and Natural Languages},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3705322},
  doi		= {10.1145/3705322},
  abstract	= {We introduce the multimodal task of Protein Captioning,
		  which is an easy-to-understand and flexible way for protein
		  analysis. Compared to specific protein recognition or
		  classification tasks, such as enzyme reaction
		  classification and gene ontology term prediction, protein
		  captioning provides comprehensive textural descriptions for
		  proteins, thus playing a key role in bridging the gap
		  between protein sequences and natural languages. To address
		  the problem, we propose a simple yet effective method,
		  Protein-to-Text Generative Pre-trained Transformer
		  (P2T-GPT), to fuse multimodal embeddings and translate the
		  chain of amino acid residues in a protein to a sequence of
		  natural language words, i.e., text. For the evaluation of
		  protein captioning, we collect the ProteinCap dataset that
		  contains 94,454 protein-text pairs. Experiments on
		  ProteinCap demonstrate the effectiveness of the proposed
		  P2T-GPT on protein captioning. For example, our method
		  obtains improvements of 8.74, 10.03, and 11.05 in the
		  BERTScore compared to the baseline model on ProteinCap-
		  (alpha,beta,gamma) , respectively. As minor contributions,
		  first, P2T-GPT provides a way to connect protein science
		  and Large Language Models (LLMs). By appending ChatGPT, our
		  method can interact in a conversational way to answer
		  questions given a protein. Second, we show that protein
		  captioning can be treated as a pre-trained task that can
		  benefit a range of downstream tasks, to a certain extent.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= nov,
  keywords	= {Protein captioning, Natural language processing,
		  Multimodal learning}
}

@Article{	  10.14778/3746405.3746410,
  author	= {Sun, Ye and Shi, Lei and Tong, Yongxin},
  title		= {eXpath: Explaining Knowledge Graph Link Prediction with
		  Ontological Closed Path Rules},
  year		= {2025},
  issue_date	= {May 2025},
  publisher	= {VLDB Endowment},
  volume	= {18},
  number	= {9},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3746405.3746410},
  doi		= {10.14778/3746405.3746410},
  abstract	= {Link prediction (LP) is crucial for Knowledge Graphs (KG)
		  completion but commonly suffers from interpretability
		  issues. While several methods have been proposed to explain
		  embedding-based LP models, they are generally limited to
		  local explanations on KG and are deficient in providing
		  human interpretable semantics. Based on real-world
		  observations of the characteristics of KGs from multiple
		  domains, we propose to explain LP models in KG with
		  path-based explanations. An integrated framework, namely
		  eXpath, is introduced which incorporates the concept of
		  relation path with ontological closed path rules to enhance
		  both the efficiency and effectiveness of LP interpretation.
		  Notably, the eXpath explanations can be fused with other
		  single-link explanation approaches to achieve a better
		  overall solution. Extensive experiments across benchmark
		  datasets and LP models demonstrate that introducing eXpath
		  can boost the quality of resulting explanations by about
		  20\% on two key metrics and reduce the required explanation
		  time by 61.4\%, in comparison to the best existing method.
		  Case studies further highlight eXpath's ability to provide
		  more semantically meaningful explanations through
		  path-based evidence.},
  journal	= {Proc. VLDB Endow.},
  month		= sep,
  pages		= {2818–2830},
  numpages	= {13}
}

@InProceedings{	  10.1145/3649158.3657036,
  author	= {Ahmed, Mohiuddin and Wei, Jinpeng and Al-Shaer, Ehab},
  title		= {Prompting LLM to Enforce and Validate CIS Critical
		  Security Control},
  year		= {2024},
  isbn		= {9798400704918},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3649158.3657036},
  doi		= {10.1145/3649158.3657036},
  abstract	= {Proper security control enforcement reduces the attack
		  surface and protects the organizations against attacks.
		  Organizations like NIST and CIS (Center for Internet
		  Security) provide critical security controls (CSCs) as a
		  guideline to enforce cyber security. Automated enforcement
		  and measurability mechanisms for these CSCs still need to
		  be developed. Analyzing the implementations of security
		  products to validate security control enforcement is
		  non-trivial. Moreover, manually analyzing and developing
		  measures and metrics to monitor, and implementing those
		  monitoring mechanisms are resource-intensive tasks and
		  massively dependent on the security analyst's expertise and
		  knowledge. To tackle those problems, we use large language
		  models (LLMs) as a knowledge base and reasoner to extract
		  measures, metrics, and monitoring mechanism implementation
		  steps from security control descriptions to reduce the
		  dependency on security analysts. Our approach used few-shot
		  learning with chain-of-thought (CoT) prompting to generate
		  measures and metrics and generated knowledge prompting for
		  metrics implementation. Our evaluation shows that prompt
		  engineering to extract measures, metrics, and monitoring
		  implementation mechanisms can reduce dependency on humans
		  and semi-automate the extraction process. We also
		  demonstrate metric implementation steps using generated
		  knowledge prompting with LLMs.},
  booktitle	= {Proceedings of the 29th ACM Symposium on Access Control
		  Models and Technologies},
  pages		= {93–104},
  numpages	= {12},
  keywords	= {account management., critical security control, llm,
		  prompt engineering},
  location	= {San Antonio, TX, USA},
  series	= {SACMAT 2024}
}

@InProceedings{	  10.1145/3670474.3685974,
  author	= {Wang, Li-C.},
  title		= {LLM-Assisted Analytics in Semiconductor Test (Invited)},
  year		= {2024},
  isbn		= {9798400706998},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670474.3685974},
  doi		= {10.1145/3670474.3685974},
  abstract	= {The emergence of Large Language Models (LLMs) has impacted
		  our perspective on applying Machine Learning (ML) in
		  semiconductor test. This paper shares our experience in
		  leveraging the power of LLMs to build an AI agent for test
		  data analytics. We advocate for an end-to-end approach
		  where the Knowledge Graph (KG) plays a central role. Using
		  wafermap analytics as an example, we highlight the key
		  ideas behind developing the LLM-assisted AI agent named
		  IEA-Plot, and discuss its practical applications.},
  booktitle	= {Proceedings of the 2024 ACM/IEEE International Symposium
		  on Machine Learning for CAD},
  articleno	= {38},
  numpages	= {7},
  keywords	= {Knowledge Graph, Large Language Model, Machine Learning,
		  Test Data Analytics},
  location	= {Salt Lake City, UT, USA},
  series	= {MLCAD '24}
}

@Article{	  10.1145/3690767,
  author	= {Paul, Soumen and Das, Partha Pratim and Rao, K.
		  Sreenivas},
  title		= {Ontology in Dance Domain—A Survey},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {1},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3690767},
  doi		= {10.1145/3690767},
  abstract	= {This article presents a literature review on the domain of
		  dance research that exploits ontology artifacts to manage
		  their domain knowledge. Any dance form around the world is
		  rich in knowledge because of its historical and
		  geographical diversity, movement rules, and interpretive
		  aspects. Researchers found various approaches to manage
		  this knowledge base, among which the ontology development
		  process is preferred by most people owing to its
		  superficial and easy-to-manage characteristics. However,
		  the heterogenic use of ontology in different dance research
		  aspects demands an organized study to understand its
		  contributions to the domain. Our survey approach toward
		  this objective starts with a systematic literature
		  selection and further grouping them into four categories
		  based on ontology involvement. Second, we discuss each
		  group of articles by their contributions and the level of
		  ontology involvement. Third, a novel evaluation framework
		  is proposed, which assesses each selected article based on
		  19 attributes from ontology quality, development, and
		  applications perspectives. We rank each article into three
		  qualitative measures, i.e., High (H), Medium (M), and Low
		  (L), for our attribute set based on our understanding.
		  Finally, we comprehensively analyze the outcomes of our
		  qualitative assessment to present the current research
		  status and their limitations in the candidate domain. This
		  review aspires to be a cornerstone resource, enlightening
		  researchers about the current landscape and future
		  prospects of ontological involvement in dance research.},
  journal	= {J. Comput. Cult. Herit.},
  month		= feb,
  articleno	= {16},
  numpages	= {32},
  keywords	= {Ontology, Domain knowledge, Dance domain, Multimedia,
		  Quality model, Evaluation framework}
}

@Article{	  10.1145/3592601,
  author	= {Gharagozlou, Hamid and Mohammadzadeh, Javad and
		  Bastanfard, Azam and Ghidary, Saeed Shiry},
  title		= {Semantic Relation Extraction: A Review of Approaches,
		  Datasets, and Evaluation Methods With Looking at the
		  Methods and Datasets in the Persian Language},
  year		= {2023},
  issue_date	= {July 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {7},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3592601},
  doi		= {10.1145/3592601},
  abstract	= {A large volume of unstructured data, especially text data,
		  is generated and exchanged daily. Consequently, the
		  importance of extracting patterns and discovering knowledge
		  from textual data is significantly increasing. As the task
		  of automatically recognizing the relations between two or
		  more entities, semantic relation extraction has a prominent
		  role in the exploitation of raw text. This article surveys
		  different approaches and types of relation extraction in
		  English and the most prominent proposed methods in Persian.
		  We also introduce, analyze, and compare the most important
		  datasets available for relation extraction in Persian and
		  English. Furthermore, traditional and emerging evaluation
		  metrics for supervised, semi-supervised, and unsupervised
		  methods are described, along with pointers to commonly used
		  performance evaluation datasets. Finally, we briefly
		  describe challenges in extracting relationships in Persian
		  and English and dataset creation challenges.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  articleno	= {189},
  numpages	= {29},
  keywords	= {Semantic relations, relation extraction, Natural Language
		  Processing (NLP), automatic extraction, Persian text
		  processing, linguistics, dataset, evaluation methods,
		  information extraction}
}

@Article{	  10.1007/s00165-021-00549-0,
  author	= {Dubslaff, Clemens and Koopmann, Patrick and Turhan,
		  Anni-Yasmin},
  title		= {Enhancing Probabilistic Model Checking with Ontologies},
  year		= {2021},
  issue_date	= {Dec 2021},
  publisher	= {Springer-Verlag},
  address	= {Berlin, Heidelberg},
  volume	= {33},
  number	= {6},
  issn		= {0934-5043},
  url		= {https://doi.org/10.1007/s00165-021-00549-0},
  doi		= {10.1007/s00165-021-00549-0},
  abstract	= {Probabilistic model checking (PMC) is a well-established
		  method for the quantitative analysis of state based
		  operational models such as Markov decision processes.
		  Description logics (DLs) provide a well-suited formalism to
		  describe and reason about knowledge and are used as basis
		  for the web ontology language (OWL). We investigate how
		  such knowledge described by DLs can be integrated into the
		  PMC process, introducing ontology-mediated PMC.
		  Specifically, we propose ontologized programs as a
		  formalism that links ontologies to behaviors specified by
		  probabilistic guarded commands, the de-facto standard input
		  formalism for PMC tools such as Prism. Through DL
		  reasoning, inconsistent states in the modeled system can be
		  detected. We present three ways to resolve these
		  inconsistencies, leading to different Markov decision
		  process semantics. We analyze the computational complexity
		  of checking whether an ontologized program is consistent
		  under these semantics. Further, we present and implement a
		  technique for the quantitative analysis of ontologized
		  programs relying on standard DL reasoning and PMC tools.
		  This way, we enable the application of PMC techniques to
		  analyze knowledge-intensive systems.We evaluate our
		  approach and implementation on amulti-server systemcase
		  study,where different DL ontologies are used to provide
		  specifications of different server platforms and situations
		  the system is executed in.},
  journal	= {Form. Asp. Comput.},
  month		= dec,
  pages		= {885–921},
  numpages	= {37},
  keywords	= {Probabilisticmodel checking, Ontologies, Description
		  logics, Ontology-mediated verification, Context dependent
		  systems analysis}
}

@Proceedings{	  10.1145/3567512,
  title		= {SLE 2022: Proceedings of the 15th ACM SIGPLAN
		  International Conference on Software Language Engineering},
  year		= {2022},
  isbn		= {9781450399197},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 15th ACM SIGPLAN International Conference
		  on Software Language Engineering (SLE), co-located with the
		  ACM SIGPLAN conference on Systems, Programming, Languages,
		  and Applications (SPLASH) in Auckland, a vibrant port city
		  in northern New Zealand, from December 5th to December 10th
		  2022. Like its predecessors, the this edition of the SLE
		  conference, SLE 2022, is devoted to the principles of
		  software languages: their design, their implementation, and
		  their evolution. As such, SLE brings together researchers
		  united by their common interest in the creation, capture,
		  and tooling of software languages.},
  location	= {Auckland, New Zealand}
}

@InProceedings{	  10.1145/3599957.3606210,
  author	= {Garcia, Rebecca and Harris, Hunter and Beach, Matthew and
		  Couch, Dylan and Khan, Samee U.},
  title		= {UAS Integration Safety and Security Technology Ontology},
  year		= {2023},
  isbn		= {9798400702280},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3599957.3606210},
  doi		= {10.1145/3599957.3606210},
  abstract	= {Unmanned Aerial Systems (UAS) are a versatile and
		  essential tool for law enforcement, first responders,
		  utility providers, and the public. Integrating the UAS into
		  the National Airspace System (NAS) poses a significant
		  challenge to policymakers and manufacturers. A UAS
		  Integration Safety and Security Technology Ontology (ISSTO)
		  has been developed in the Web Ontology Language (OWL) to
		  aid this integration. ISSTO is a domain ontology covering
		  aviation topics corresponding to flights, aircraft types,
		  manufacturers, temporal/spatial, waivers and
		  authorizations, track data, NAS facilities, air traffic
		  control advisories, weather phenomena, surveillance and
		  security equipment, and events, sensor types, radio
		  frequency ranges, actions, and outcomes. As ISSTO is a
		  domain ontology, it models the current state of UAS
		  integration into the NAS and provides a comprehensive view
		  of every aspect of UAS.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Research in Adaptive and Convergent Systems},
  articleno	= {9},
  numpages	= {6},
  keywords	= {Aviation, Ontology, Unmanned Aerial Systems},
  location	= {Gdansk, Poland},
  series	= {RACS '23}
}

@InProceedings{	  10.5555/3716662.3716812,
  author	= {Zhang, Richard and van Liemt, Erin and Fischella, Tyler},
  title		= {Ontology of Belief Diversity: A Community-Based
		  Epistemological Approach},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {AI applications across classification, fairness, and human
		  interaction often implicitly require ontologies of social
		  concepts. Constructing these well, especially when there
		  are many relevant categories, is a controversial task but
		  is crucial for achieving meaningful inclusivity. Here, we
		  focus on developing a pragmatic ontology of belief systems,
		  which is a complex and often controversial space. By
		  iterating on our community-based design until mutual
		  agreement is reached, we found that epistemological methods
		  were best for categorizing the fundamental ways beliefs
		  differ, maximally respecting our principles of inclusivity
		  and brevity. We demonstrate our methodology's utility and
		  interpretability via user studies in term annotation and
		  sentiment analysis experiments for belief fairness in
		  language models.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {1735–1743},
  numpages	= {9},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@InProceedings{	  10.1145/3701716.3715483,
  author	= {Chhetri, Tek Raj and Halchenko, Yaroslav O. and Jarecka,
		  Dorota and Trivedi, Puja and Ghosh, Satrajit S. and Ray,
		  Patrick and Ng, Lydia},
  title		= {Bridging the Scientific Knowledge Gap and Reproducibility:
		  A Survey of Provenance, Assertion and Evidence Ontologies},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715483},
  doi		= {10.1145/3701716.3715483},
  abstract	= {The rapid growth of scientific publications and evolving
		  experimental paradigms create significant challenges in
		  staying up-to-date with current advances. Assertions are
		  often unstructured and have limited provenance, which
		  hinders reproducibility. Ontologies and knowledge graphs
		  (KGs) offer structured solutions by capturing assertions,
		  evidence, and provenance to support reproducibility. This
		  paper reviews 23 ontologies -- 13 focused on assertions and
		  evidence and 10 on provenance -- providing an overview of
		  the current landscape while highlighting key challenges and
		  opportunities for improvement.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {924–928},
  numpages	= {5},
  keywords	= {assertion and evidence ontology, knowledge discovery and
		  integration, knowledge graphs, ontology survey, provenance,
		  reproducibility},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3230833.3232799,
  author	= {Johnson, Pontus and Lagerstr\"{o}m, Robert and Ekstedt,
		  Mathias},
  title		= {A Meta Language for Threat Modeling and Attack
		  Simulations},
  year		= {2018},
  isbn		= {9781450364485},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3230833.3232799},
  doi		= {10.1145/3230833.3232799},
  abstract	= {Attack simulations may be used to assess the cyber
		  security of systems. In such simulations, the steps taken
		  by an attacker in order to compromise sensitive system
		  assets are traced, and a time estimate may be computed from
		  the initial step to the compromise of assets of interest.
		  Attack graphs constitute a suitable formalism for the
		  modeling of attack steps and their dependencies, allowing
		  the subsequent simulation.To avoid the costly proposition
		  of building new attack graphs for each system of a given
		  type, domain-specific attack languages may be used. These
		  languages codify the generic attack logic of the considered
		  domain, thus facilitating the modeling, or instantiation,
		  of a specific system in the domain. Examples of possible
		  cyber security domains suitable for domain-specific attack
		  languages are generic types such as cloud systems or
		  embedded systems but may also be highly specialized kinds,
		  e.g. Ubuntu installations; the objects of interest as well
		  as the attack logic will differ significantly between such
		  domains.In this paper, we present the Meta Attack Language
		  (MAL), which may be used to design domain-specific attack
		  languages such as the aforementioned. The MAL provides a
		  formalism that allows the semi-automated generation as well
		  as the efficient computation of very large attack graphs.
		  We declare the formal background to MAL, define its syntax
		  and semantics, exemplify its use with a small
		  domain-specific language and instance model, and report on
		  the computational performance.},
  booktitle	= {Proceedings of the 13th International Conference on
		  Availability, Reliability and Security},
  articleno	= {38},
  numpages	= {8},
  keywords	= {Attack Graphs, Cyber Security, Domain Specific Language,
		  Threat Modeling},
  location	= {Hamburg, Germany},
  series	= {ARES '18}
}

@InProceedings{	  10.1145/3627673.3679915,
  author	= {Abdi, Samireh},
  title		= {Enhancing Event Detection with Inter-Event Dependencies in
		  Large Ontologies},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679915},
  doi		= {10.1145/3627673.3679915},
  abstract	= {Event Detection (ED), a crucial component of comprehensive
		  text analysis tools, is a well-established task within the
		  fields of Natural Language Processing (NLP) and Information
		  Extraction (IE). Current state-of-the-art models for ED
		  primarily focus on identifying a limited set of predefined
		  event types. Recently, the challenge of detecting a broad
		  array of predefined event types has garnered increasing
		  interest within the IE community. However, a significant
		  gap in existing research on ED with extensive ontologies is
		  the inadequate exploration of how interactions between
		  event types affect ED model performance. One of the
		  hindrances for this purpose is the lack of resources to
		  encode event-event dependencies for large ontologies. This
		  study introduces a novel approach that leverages existing
		  inter-event dependency resources to provide this
		  information for extensive ontologies. Specifically, a
		  solution based on Optimal Transport is proposed to map
		  event-event dependency from existing resources to a large
		  ontology. We conduct extensive experiments on multiple
		  benchmark datasets to assess the effectiveness of our
		  approach. Our findings, supported by a thorough analysis,
		  demonstrate that this innovative technique significantly
		  enhances the performance of ED models, especially for
		  ontologies with a large number of event types.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3612–3616},
  numpages	= {5},
  keywords	= {event detection, large ontology, optimal transport},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3711403.3711488,
  author	= {Lin, Jian and Mai, Shanyin and Bu, Bingqian and He,
		  Musheng and Wang, Xiaoyi},
  title		= {Research on the Application of STEM Practical Teaching
		  Based on RAG Knowledge Graph and Large Models},
  year		= {2025},
  isbn		= {9798400717468},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711403.3711488},
  doi		= {10.1145/3711403.3711488},
  abstract	= {Practical experience plays a pivotal role in STEM
		  education, effectively cultivating students' practical
		  skills, innovation capabilities, and critical thinking.
		  However, the scarcity of domain-specific practical
		  experience data within Large Language Models (LLMs) has not
		  fully met the deep-level practical knowledge demands of
		  STEM education, impacting the learners' application
		  outcomes. This paper proposes a STEM practical teaching and
		  inquiry system based on Retrieval-Augmented Generation
		  (RAG) technology and knowledge graphs, aiming to enhance
		  learners' learning experiences and interdisciplinary
		  learning abilities, achieving an intelligent upgrade of
		  STEM practical teaching.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Educational Technology Management},
  pages		= {520–527},
  numpages	= {8},
  keywords	= {Knowledge Graph, Large Language Models (LLMs), RAG, STEM
		  Practical Teaching},
  location	= { },
  series	= {ICETM '24}
}

@InProceedings{	  10.1145/3503229.3547044,
  author	= {Abbasi, Ebrahim Khalil and Leclercq, Tony and Heymans,
		  Patrick},
  title		= {A meta-model for product configuration ontologies},
  year		= {2022},
  isbn		= {9781450392068},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503229.3547044},
  doi		= {10.1145/3503229.3547044},
  abstract	= {Conceptual modelling of product configuration is an
		  essential step towards improving reuse and configuration
		  knowledge sharing, systems interoperability, and people
		  communication. Among several approaches proposed,
		  ontology-based approaches are known to provide better
		  support for the conceptualization of product configuration
		  knowledge in terms of precision and expressiveness of
		  reasoning and representation. This paper studies product
		  configuration ontologies and presents a meta-model of
		  concepts and their relationships that are used in those
		  ontologies. The proposed meta-model consists of a
		  generalisation hierarchy of configuration types, the
		  compositional structure of a configurable product, the
		  generalisation hierarchy of a component, and constraint
		  types. The meta-model has been designed with the aim of
		  first understanding the current state of product
		  configuration ontologies and then extending it for
		  integrating new concepts.},
  booktitle	= {Proceedings of the 26th ACM International Systems and
		  Software Product Line Conference - Volume B},
  pages		= {166–173},
  numpages	= {8},
  keywords	= {knowledge representation, ontology, product
		  configuration},
  location	= {Graz, Austria},
  series	= {SPLC '22}
}

@InProceedings{	  10.1145/3178876.3186029,
  author	= {Cannaviccio, Matteo and Barbosa, Denilson and Merialdo,
		  Paolo},
  title		= {Towards Annotating Relational Data on the Web with
		  Language Models},
  year		= {2018},
  isbn		= {9781450356398},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3178876.3186029},
  doi		= {10.1145/3178876.3186029},
  abstract	= {Tables and structured lists on Web pages are a potential
		  source of valuable information, and several methods have
		  been proposed to annotate them with semantics that can be
		  leveraged for search, question answering and information
		  extraction. This paper is concerned with the specific
		  problem of finding and ranking relations from a given
		  Knowledge Graph (KG) that hold over pairs of entities
		  juxtaposed in a table or structured list. The
		  state-of-the-art for this task is to attempt to link the
		  entities mentioned in the table cells to objects in the KG
		  and rank the relations that hold for those linked objects.
		  As a result, these methods are hampered by the
		  incompleteness and uneven coverage in even the best
		  knowledge graphs available today. The alternative described
		  here does not require entity linking, relying instead on
		  ranking relations using generative language models derived
		  from Web-scale corpora. As such, it can produce quality
		  results even when the entities in the table are missing in
		  the KG. The experimental validation, designed to expose the
		  challenges posed by KG incompleteness, shows that our
		  approach is robust and effective in practice.},
  booktitle	= {Proceedings of the 2018 World Wide Web Conference},
  pages		= {1307–1316},
  numpages	= {10},
  keywords	= {generative language models, knowledge graphs, web table
		  understanding},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3503823.3503898,
  author	= {Krasadakis, Panteleimon and Sakkopoulos, Evangelos and
		  Verykios, Vassilios S.},
  title		= {A Natural Language Processing Survey on Legislative and
		  Greek Documents},
  year		= {2022},
  isbn		= {9781450395557},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503823.3503898},
  doi		= {10.1145/3503823.3503898},
  abstract	= {Natural Language Processing is developing rapidly
		  alongside the various complex applications that make use of
		  it and they will depend on it even further in the future.
		  It has many challenges that require the attention of both
		  researchers and businesses. The state-of-the-art approaches
		  usually involve the implementation of Deep Learning Neural
		  Networks. Our work serves as a rigorous research of the
		  bibliography on the field focusing on Legal and Greek
		  documents. We also present the current challenges of the
		  field and some future considerations.},
  booktitle	= {Proceedings of the 25th Pan-Hellenic Conference on
		  Informatics},
  pages		= {407–412},
  numpages	= {6},
  keywords	= {Deep Learning, Information Extraction, Natural Language
		  Processing},
  location	= {Volos, Greece},
  series	= {PCI '21}
}

@InProceedings{	  10.1145/3587259.3627568,
  author	= {Dobriy, Daniil and Polleres, Axel},
  title		= {O2WB: A tool enabling ontology reuse in Wikibase},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627568},
  doi		= {10.1145/3587259.3627568},
  abstract	= {The Semantic Web initiative has established standards and
		  practices for publishing interconnected knowledge, where
		  RDF Schema and OWL shall enable the reuse of ontologies as
		  one of these established practices. However, Wikibase, the
		  software behind Wikidata, which is increasingly gaining
		  popularity among data publishers, lacks the functionality
		  to import and reuse existing RDF Schema and OWL ontologies.
		  To facilitate ontology reuse, FAIR data publishing and
		  encourage a tighter connection of existing Linked Data
		  resources with Wikibase instances, we align the Wikibase
		  data model with RDF and present O2WB, a tool for ontology
		  import and export within Wikibase.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {101–104},
  numpages	= {4},
  keywords	= {FAIR, Interoperability, Linked Data, Ontology Reuse,
		  Wikibase},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@Article{	  10.1145/3185046,
  author	= {Purao, Sandeep and Bolloju, Narasimha and Tan, Chuan-Hoo},
  title		= {A Modeling Language for Conceptual Design of Systems
		  Integration Solutions},
  year		= {2018},
  issue_date	= {June 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {2},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3185046},
  doi		= {10.1145/3185046},
  abstract	= {Systems integration—connecting software systems for
		  cross-functional work—is a significant concern in many
		  large organizations, which continue to maintain hundreds,
		  if not thousands, of independently evolving software
		  systems. Current approaches in this space remain ad hoc,
		  and closely tied to technology platforms. Following a
		  design science approach, and via multiple design-evaluate
		  cycles, we develop Systems Integration Requirements
		  Engineering Modeling Language (SIRE-ML) to address this
		  problem. SIRE-ML builds on the foundation of coordination
		  theory, and incorporates important semantic information
		  about the systems integration domain. The article develops
		  constructs in SIRE-ML, and a merge algorithm that allows
		  both functional managers and integration professionals to
		  contribute to building a systems integration solution.
		  Integration models built with SIRE-ML provide benefits such
		  as ensuring coverage and minimizing ambiguity, and can be
		  used to drive implementation with different platforms such
		  as middleware, services, and distributed objects. We
		  evaluate SIRE-ML for ontological expressiveness and report
		  findings about applicability check with an expert panel.
		  The article discusses implications for future research such
		  as tool building and empirical evaluation, as well as
		  implications for practice.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= sep,
  articleno	= {8},
  numpages	= {25},
  keywords	= {Design science, SIRE-ML, conceptual modeling}
}

@Article{	  10.1145/3615864,
  author	= {Mesmia, Fatma Ben and Mouhoub, Malek},
  title		= {Semi-Automatic Building and Learning of a Multilingual
		  Ontology},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {11},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3615864},
  doi		= {10.1145/3615864},
  abstract	= {Most online platforms, applications, and Websites use a
		  massive amount of heterogeneous evolving data. These data
		  must be structured and normalized before integration to
		  improve the search and increase the relevance of results.
		  An ontology can address this critical task by efficiently
		  managing data and providing structured formats through
		  techniques such as the Web Ontology Language (OWL).
		  However, building an ontology can be costly, primarily if
		  conducted manually. In this context, we propose a new
		  methodology for automatically building and learning a
		  multilingual ontology using Arabic as the base language via
		  a corpus collected from Wikipedia. Our proposed methodology
		  relies on Finite-state transducers (FSTs). FSTs are
		  regrouped into a cascade to reduce errors and minimize
		  ambiguity. The produced ontology is extended to English and
		  French and independent language images via a translator we
		  developed using APIs. The rationale for starting with the
		  Arabic corpus to extract terms is that entity linking is
		  more convenient from Arabic to other languages. In
		  addition, many Wikipedia articles in English and French
		  (for instance) do not have associated Arabic articles, but
		  the opposite is true. In addition, dealing with Arabic
		  terms permits us to enrich the Arabic module of the free
		  linguistic platform we use in dictionaries and graphs. To
		  assess the efficiency of our proposed methodology, we
		  conducted performance metrics. The reported results are
		  encouraging and promising.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {242},
  numpages	= {19},
  keywords	= {Ontology building and learning, finite sate transducer,
		  transducer cascade, API, Arabic NLP}
}

@Proceedings{	  10.1145/3582768,
  title		= {NLPIR '22: Proceedings of the 2022 6th International
		  Conference on Natural Language Processing and Information
		  Retrieval},
  year		= {2022},
  isbn		= {9781450397629},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bangkok, Thailand}
}

@InProceedings{	  10.1145/3613904.3642436,
  author	= {Gray, Colin M. and Santos, Cristiana Teixeira and Bielova,
		  Nataliia and Mildner, Thomas},
  title		= {An Ontology of Dark Patterns Knowledge: Foundations,
		  Definitions, and a Pathway for Shared Knowledge-Building},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642436},
  doi		= {10.1145/3613904.3642436},
  abstract	= {Deceptive and coercive design practices are increasingly
		  used by companies to extract profit, harvest data, and
		  limit consumer choice. Dark patterns represent the most
		  common contemporary amalgamation of these problematic
		  practices, connecting designers, technologists, scholars,
		  regulators, and legal professionals in transdisciplinary
		  dialogue. However, a lack of universally accepted
		  definitions across the academic, legislative, practitioner,
		  and regulatory space has likely limited the impact that
		  scholarship on dark patterns might have in supporting
		  sanctions and evolved design practices. In this paper, we
		  seek to support the development of a shared language of
		  dark patterns, harmonizing ten existing regulatory and
		  academic taxonomies of dark patterns and proposing a
		  three-level ontology with standardized definitions for 64
		  synthesized dark pattern types across low-, meso-, and
		  high-level patterns. We illustrate how this ontology can
		  support translational research and regulatory action,
		  including transdisciplinary pathways to extend our initial
		  types through new empirical work across application and
		  technology domains.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {289},
  numpages	= {22},
  keywords	= {dark patterns, deceptive design, ontology, regulation},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3444757.3485108,
  author	= {Morais, Gabriel and Bork, Dominik and Adda, Mehdi},
  title		= {Towards an Ontology-driven Approach to Model and Analyze
		  Microservices Architectures},
  year		= {2021},
  isbn		= {9781450383141},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3444757.3485108},
  doi		= {10.1145/3444757.3485108},
  abstract	= {Microservices Architectures (MSAs) are continuously
		  replacing monolithic systems toward achieving more flexible
		  and maintainable service-oriented software systems.
		  However, the shift toward an MSA also requires a
		  technological and managerial shift for its adopters.
		  Architecting and managing MSAs represent unique challenges,
		  including microservices' identification, interoperability,
		  and reuse. To handle these challenges, we propose an
		  Ontology-driven Conceptual Modelling approach, based on the
		  Ontology of Microservices Architecture Concepts (OMSAC),
		  for modelling and analyzing microservices-based systems. We
		  show, how OMSAC-based conceptual models, stocked in a
		  Stardog triple store, support Stakeholder-specific
		  communication, documentation, and reuse. This paper reports
		  on the application of our approach in three open-source MSA
		  systems with a focus on microservices' discovery based on
		  similarity metrics. Eventually, we compare the extracted
		  similarity metrics derived from the application of machine
		  learning techniques to the OMSAC models with a manual
		  analysis performed by experts.},
  booktitle	= {Proceedings of the 13th International Conference on
		  Management of Digital EcoSystems},
  pages		= {79–86},
  numpages	= {8},
  keywords	= {Microservices, OMSAC, Stardog, machine learning,
		  ontology},
  location	= {Virtual Event, Tunisia},
  series	= {MEDES '21}
}

@InProceedings{	  10.1145/3613904.3642669,
  author	= {Das, Dipto and Guha, Shion and Brubaker, Jed R. and
		  Semaan, Bryan},
  title		= {The ``Colonial Impulse" of Natural Language Processing: An
		  Audit of Bengali Sentiment Analysis Tools and Their
		  Identity-based Biases},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642669},
  doi		= {10.1145/3613904.3642669},
  abstract	= {While colonization has sociohistorically impacted
		  people’s identities across various dimensions, those
		  colonial values and biases continue to be perpetuated by
		  sociotechnical systems. One category of sociotechnical
		  systems–sentiment analysis tools–can also perpetuate
		  colonial values and bias, yet less attention has been paid
		  to how such tools may be complicit in perpetuating
		  coloniality, although they are often used to guide various
		  practices (e.g., content moderation). In this paper, we
		  explore potential bias in sentiment analysis tools in the
		  context of Bengali communities who have experienced and
		  continue to experience the impacts of colonialism. Drawing
		  on identity categories most impacted by colonialism amongst
		  local Bengali communities, we focused our analytic
		  attention on gender, religion, and nationality. We
		  conducted an algorithmic audit of all sentiment analysis
		  tools for Bengali, available on the Python package index
		  (PyPI) and GitHub. Despite similar semantic content and
		  structure, our analyses showed that in addition to
		  inconsistencies in output from different tools, Bengali
		  sentiment analysis tools exhibit bias between different
		  identity categories and respond differently to different
		  ways of identity expression. Connecting our findings with
		  colonially shaped sociocultural structures of Bengali
		  communities, we discuss the implications of downstream bias
		  of sentiment analysis tools.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {769},
  numpages	= {18},
  keywords	= {Algorithmic audit, Bias, Colonial, Identity, Sentiment
		  analysis tools},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3732771.3742719,
  author	= {Fr\"{o}lich, Damian and Pacciani, Tommaso and van
		  Binsbergen, L. Thomas},
  title		= {Exploratory, Omniscient, and Multiverse Diagnostics in
		  Debuggers for Non-Deterministic Languages},
  year		= {2025},
  isbn		= {9798400718847},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3732771.3742719},
  doi		= {10.1145/3732771.3742719},
  abstract	= {Debugging non-deterministic programs is inherently
		  difficult as the compound effects of non-deterministic
		  execution steps is hard to predict and gives rise to a
		  (potentially) vast space of reachable program states such
		  that manual exploration of all reachable states is
		  infeasible.Multiverse debugging addresses these problems by
		  realising a fine-grained, exhaustive and interactive
		  process for state space exploration. At SLE2023, Pasquier
		  et al. presented a generic framework that makes exploration
		  practical through user-defined reductions on program states
		  and by proposing expressive logics for defining and
		  searching for states and traces of interest, generalising
		  the concept of breakpoint. The framework has been validated
		  through the case study language AnimUML designed to make
		  non-deterministic UML specifications executable.In this
		  paper, we perform additional case studies to evaluate the
		  applicability of the framework. We analyse three
		  non-deterministic, domain-specific languages representing
		  three different domains: grammar engineering, formal
		  operational semantics, and norm engineering. The framework
		  is evaluated against requirements extracted from these
		  domains, resulting in the identification of several
		  limitations of the framework. We then propose a modified
		  and extended framework and apply it to develop multiverse
		  debuggers for the case study languages. The result
		  demonstrates a multiverse debugging framework with more
		  general applicability.},
  booktitle	= {Proceedings of the 18th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  pages		= {134–147},
  numpages	= {14},
  keywords	= {debuggers, domain-specific languages, exploratory
		  programming, multiverse debuggers, parsing},
  location	= {Koblenz, Germany},
  series	= {SLE '25}
}

@InProceedings{	  10.1145/3652620.3687793,
  author	= {Hou-Liu, Jerry and Jiang, Zhekai and Babikian, Aren A.},
  title		= {Concretize: A Model-Driven Tool for Scenario-Based
		  Autonomous Vehicle Testing},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687793},
  doi		= {10.1145/3652620.3687793},
  abstract	= {To achieve rigorous certification of autonomous vehicles
		  (AVs), testing approaches must handle all possible,
		  practically relevant traffic scenarios. This is achievable
		  through the handling of relevant abstractions within the
		  scenario specification language and throughout the scenario
		  generation process. While many scenario generation
		  approaches exist, they are often limited to generating
		  instances of a fixed (pre-defined) scenario and lack tool
		  support. In this paper, we introduce Concretize, a
		  model-driven AV testing framework. It (1) allows users to
		  define scenario specifications using an abstract
		  domain-specific language, and (2) generates conforming
		  concrete (exact) scenarios, which are (3) visualized via a
		  user-friendly web interface. Scenarios are also (4)
		  executed in simulation, in which case Concretize (5)
		  auto-generates figures depicting the monitored safety
		  behavior of the AV-under-test wrt. scenario components at
		  various abstraction levels.Video demonstration:
		  https://youtu.be/inaD8jd7YxI.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {66–70},
  numpages	= {5},
  keywords	= {model-driven engineering, scenario-based autonomous
		  vehicle testing, traffic scenario generation,
		  domain-specific language},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.1145/3578708,
  author	= {Belhadi, Asma and Djenouri, Youcef and Srivastava, Gautam
		  and Lin, Jerry Chun-Wei},
  title		= {Fast and Accurate Framework for Ontology Matching in Web
		  of Things},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3578708},
  doi		= {10.1145/3578708},
  abstract	= {The Web of Things (WoT) can help with knowledge discovery
		  and interoperability issues in many Internet of Things
		  (IoT) applications. This article focuses on semantic
		  modeling of WoT and proposes a new approach called
		  Decomposition for Ontology Matching (DOM) to discover
		  relevant knowledge by exploring correlations between WoT
		  data using decomposition strategies. The DOM technique
		  adopts several decomposition techniques to order highly
		  linked ontologies of WoT data into similar groups. The main
		  idea is to decompose the instances of each ontology into
		  similar groups and then match instances of similar groups
		  instead of entire instances of two ontologies. Three main
		  algorithms for decomposition have been developed. The first
		  algorithm is based on radar scanning, which determines the
		  distribution of distances between each instance and all
		  other instances to determine the cluster centroid. The
		  second algorithm is based on adaptive grid clustering,
		  where it focuses on distribution information and the
		  construction of spanning trees. The third algorithm is
		  based on split index clustering, where instances are
		  divided into groups of cells from which noise is removed
		  during the merging process. Several studies were conducted
		  with different ontology databases to illustrate the use of
		  the DOM technique. The results show that DOM outperforms
		  state-of-the-art ontology matching models in terms of
		  computational cost while maintaining the quality of the
		  matching. Moreover, these results demonstrate that DOM is
		  capable of handling various large datasets in WoT
		  contexts.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {147},
  numpages	= {19},
  keywords	= {Ontology matching, Web of Things, decomposition}
}

@InProceedings{	  10.1145/3546118.3546147,
  author	= {Lekova, Anna and Andreeva, Anna and Tanev, Tanio and
		  Simonska, Miglena and Kostova, Snezhana},
  title		= {A system for speech and language therapy with a potential
		  to work in the IoT},
  year		= {2022},
  isbn		= {9781450396448},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3546118.3546147},
  doi		= {10.1145/3546118.3546147},
  abstract	= {In this study we propose a designed, developed and
		  validated by experiments Speech and Language Therapy (SLT)
		  system to provide SLT intervention for children with
		  communication disorders. In order to help the SLT services
		  in different educational and social context, the system has
		  a potential to work in the Internet of Things (IoT). It can
		  wire different assistive devices, APIs, online services and
		  agents in order to meet the child individual needs for the
		  intervention. Humanoid NAO-type robot, Emotiv EPOC+ brain
		  headset, emotionally-expressive robot EmoSan and Kinect
		  depth sensor are the devices connected using Node-RED. It
		  is a flow-based tool designed for visual programming
		  without a need to write any code and it can run locally or
		  in the IoT. The proposed system is sufficiently general to
		  be applied for other kind of therapy and to support other
		  assistive technologies, cloud services and people in the
		  remote areas.},
  booktitle	= {Proceedings of the 23rd International Conference on
		  Computer Systems and Technologies},
  pages		= {119–124},
  numpages	= {6},
  keywords	= {Brain Computer Interface, Node-RED, Socially Assistive
		  Robots, Speech Language Therapy},
  location	= {University of Ruse, Ruse, Bulgaria},
  series	= {CompSysTech '22}
}

@InProceedings{	  10.1145/3627673.3680094,
  author	= {Gubanov, Michael and Pyayt, Anna and Karolak, Aleksandra},
  title		= {CancerKG.ORG - A Web-scale, Interactive, Verifiable
		  Knowledge Graph-LLM Hybrid for Assisting with Optimal
		  Cancer Treatment and Care},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680094},
  doi		= {10.1145/3627673.3680094},
  abstract	= {Here, we describe one of the first Web-scale hybrid
		  Knowledge Graph (KG)-Large Language Model (LLM), populated
		  with the latest peer-reviewed medical knowledge on
		  colorectal Cancer. It is currently being evaluated to
		  assist with both medical research and clinical information
		  retrieval tasks at Moffitt Cancer Center and Research
		  Institute, which is one of the top Cancer centers in the
		  U.S. and in the world. Our hybrid is remarkable as it
		  serves the user needs better than just an LLM, KG or a
		  search-engine in isolation. LLMs as is are known to exhibit
		  hallucinations and catastrophic forgetting as well as are
		  trained on outdated corpora. The state of the art KGs, such
		  as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require
		  manual curation, hence are quickly getting stale. CancerKG
		  is unsupervised and is capable of automatically ingesting
		  and organizing the latest medical findings. To alleviate
		  the LLMs shortcomings, the verified KG serves as a
		  Retrieval Augmented Generation (RAG) guardrail. CancerKG
		  exhibits 5 different advanced user interfaces, each
		  tailored to serve different data modalities better and more
		  convenient for the user. We evaluated CancerKG on real user
		  queries and report a high NDCG score on a large-scale
		  corpora of approximately 44K publications.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4497–4505},
  numpages	= {9},
  keywords	= {LLM, artificial intelligence (AI), cancer, data
		  management},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3627673.3680090,
  author	= {Wu, Hao and Cho, Hyunji and Davies, Anna R. and Jones,
		  Gareth J. F.},
  title		= {LLM-based Automated Web Retrieval and Text Classification
		  of Food Sharing Initiatives},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680090},
  doi		= {10.1145/3627673.3680090},
  abstract	= {Urban and peri-urban (UPU) food systems encounter
		  challenges in sustainability and are fragile and vulnerable
		  to shocks. Addressing these issues is one of the key
		  drivers of food sharing initiatives (FSIs) which focus on
		  collective acts around food across the food system. FSIs
		  range from seed sharing and surplus food redistribution to
		  community composting. We describe our development and
		  deployment of web retrieval and content classification
		  tools designed to provide automated mapping of FSIs at
		  scale to populate databases of FSIs within cities. We
		  present our novel automated system tailored for retrieving,
		  identifying, categorizing and real-time monitoring of FSIs
		  in over 200 European cities. Developed within the European
		  CULTIVATE project, this system not only aids in
		  comprehending the complex dynamics of the food sharing
		  economy, but also enhances its visibility and operational
		  efficiency. The automation of these processes plays a vital
		  role in supporting the goals of the CULTIVATE project,
		  notably in promoting sustainable food practices and
		  resilient local food networks. Our system integrates web
		  search using queries constructed automatically using
		  domain-specific vocabulary resources with Large Language
		  Model (LLM) query writing and classification methods.
		  Experimental results using a collection of data derived
		  from real online FSI content underscore the potential of
		  digital automation to make significant contributions to
		  innovative digital solutions to contemporary sustainability
		  challenges. As such, the findings of this work pave the way
		  for future research and implementation in similar
		  contexts.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4983–4990},
  numpages	= {8},
  keywords	= {automatic query writing, content discovery and
		  classification, food sharing, llm-based retrieval, web
		  search},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3567445.3571103,
  author	= {Jarwar, Muhammad Aslam and Watson CBE FREng, Jeremy and
		  Ani, Uchenna P Daniel and Chalmers, Stuart},
  title		= {Industrial Internet of Things Security Modelling using
		  Ontological Methods},
  year		= {2023},
  isbn		= {9781450396653},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3567445.3571103},
  doi		= {10.1145/3567445.3571103},
  abstract	= {The Industrial Internet of Things (IIoT) trend presents
		  many significant benefits for improving industrial
		  operations. However, its emergence from the convergence of
		  legacy Industrial Control Systems (ICS) and Information and
		  Communication Technologies (ICT) has introduced newer
		  security issues such as weak or lack of end-to-end
		  security. These challenges have weakened the interest of
		  many critical infrastructure industries in adopting
		  IIoT-enabled systems. Implementing security in IIoT is
		  challenging because this involves many heterogeneous
		  Information Technology (IT) and Operational Technology (OT)
		  devices and complex interactions with humans, and the
		  environments in which these are operated and monitored.
		  This article presents the initial results of the PETRAS
		  Secure Ontologies for Internet of Things Systems (SOfIoTS)
		  project, which consists of key security concepts and a
		  modular design of a base security ontology, which supports
		  security knowledge representation and analysis of IIoT
		  security.},
  booktitle	= {Proceedings of the 12th International Conference on the
		  Internet of Things},
  pages		= {163–170},
  numpages	= {8},
  keywords	= {Cyber physical systems, Industrial Internet of Things,
		  Knowledge modelling, Security attributes, Security
		  ontology},
  location	= {Delft, Netherlands},
  series	= {IoT '22}
}

@Article{	  10.1145/3511097,
  author	= {Nasim, Zarmeen and Haider, Sajjad},
  title		= {Automatic Labeling of Clusters for a Low-Resource Urdu
		  Language},
  year		= {2022},
  issue_date	= {September 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3511097},
  doi		= {10.1145/3511097},
  abstract	= {Document clustering techniques often produce clusters that
		  require human intervention to interpret the meaning of such
		  clusters. Automatic cluster labeling refers to the process
		  of assigning a meaningful phrase to a cluster as a label.
		  This article proposes an unsupervised method for cluster
		  labeling that is based on noun phrase chunking. The
		  proposed method is compared with four other
		  statistical-based methods, including Z-Order, M-Order,
		  T-Order, and YAKE. In addition to the statistical measures
		  based labeling schemes, the approach is also compared with
		  two graph-based techniques: TextRank and PositionRank. The
		  experiments were performed on the low-resource Urdu
		  language corpus of News Headlines. The proposed approach's
		  effectiveness was evaluated using cosine similarity, the
		  Jaccard index, and feedback received from human evaluators.
		  The results show that the proposed method outperforms other
		  methods. It was found that the labels produced were more
		  relevant and semantically rich in contrast to other
		  approaches.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {93},
  numpages	= {22},
  keywords	= {Cluster labeling, low-resource language, urdu language
		  processing}
}

@InProceedings{	  10.1145/3704137.3704180,
  author	= {Rzepka, Rafal and Obayashi, Akihiko},
  title		= {Effectiveness of Security Export Control Ontology for
		  Predicting Answer Type and Regulation Categories},
  year		= {2025},
  isbn		= {9798400718014},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704137.3704180},
  doi		= {10.1145/3704137.3704180},
  abstract	= {In this paper we present results of our experiments
		  investigating if an expert knowledge graph can improve
		  Large Language Models accuracy in predicting correct answer
		  labels and regulations related to the topic of security
		  export control. As the lack of related data prevents
		  machine-learning or fine-tuning approaches, we implement
		  prompt expansion by searching most relevant nodes of the
		  graph and adding the expanded context to the prompt.
		  Results of our experiments show that the addition improved
		  answer type selection but clearly hamper the capability of
		  finding a correct regulation category.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Advances in Artificial Intelligence},
  pages		= {156–161},
  numpages	= {6},
  keywords	= {Expert Systems, GraphRAG, Large Language Models, Knowledge
		  Graph, Security Export Control},
  location	= { },
  series	= {ICAAI '24}
}

@InProceedings{	  10.1145/3487553.3524199,
  author	= {Kuculo, Tin},
  title		= {Comprehensive Event Representations using Event Knowledge
		  Graphs and Natural Language Processing},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524199},
  doi		= {10.1145/3487553.3524199},
  abstract	= {Recent work has utilised knowledge-aware approaches to
		  natural language understanding, question answering,
		  recommendation systems, and other tasks. These approaches
		  rely on well-constructed and large-scale knowledge graphs
		  that can be useful for many downstream applications and
		  empower knowledge-aware models with commonsense reasoning.
		  Such knowledge graphs are constructed through knowledge
		  acquisition tasks such as relation extraction and knowledge
		  graph completion. This work seeks to utilise and build on
		  the growing body of work that uses findings from the field
		  of natural language processing (NLP) to extract knowledge
		  from text and build knowledge graphs. The focus of this
		  research project is on how we can use transformer-based
		  approaches to extract and contextualise event information,
		  matching it to existing ontologies, to build comprehensive
		  knowledge graph-based event representations. Specifically,
		  sub-event extraction is used as a way of creating
		  sub-event-aware event representations. These event
		  representations are then further enriched through
		  fine-grained location extraction and contextualised through
		  the alignment of historically relevant quotes.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {359–363},
  numpages	= {5},
  keywords	= {event extraction, event geotagging, event representation,
		  knowledge graph, quotes},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3652620.3688261,
  author	= {Oakes, Bentley and Gomes, Claudio and Kamburjan, Eduard
		  and Abbiati, Giuseppe and Ecem Bas, Elif and Engelsgaard,
		  Sebastian},
  title		= {Towards Ontological Service-Driven Engineering of Digital
		  Twins},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688261},
  doi		= {10.1145/3652620.3688261},
  abstract	= {The systematic engineering of Digital Twins (DTs) requires
		  the establishment of clear methodologies supported by
		  intelligent tooling. We propose an approach to guide the
		  user in the creation and deployment of services for DTs
		  utilizing ontologies and workflows. In our approach, the
		  user selects a desired DT service from an array of options.
		  This selection is then used to suggest a) enablers and
		  models to place in the DT, and b) development and
		  deployment workflows for the DT service. The aim is to
		  provide DT engineering guidance to assist non-software
		  engineering experts to develop DT services more rapidly
		  with less effort. We describe our initial work on applying
		  this approach to a derived version of an industrial wind
		  turbine generator case study, utilizing openCAESAR for
		  ontology definition and enacting the workflows with Jupyter
		  notebooks.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {464–469},
  numpages	= {6},
  keywords	= {digital twins, ontologies, DT services, wind turbine
		  testing, guided software engineering, recommendation,
		  workflows},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3685243.3685291,
  author	= {Moret, Julian Gutierrez and Samper-Zapater, Jose Javier
		  and Delgado, Ana M. and Rocha, Jose Macario de S and Tena,
		  Gustavo and Soriano, Francisco R.},
  title		= {A Comprehensive Analysis of Ontologies Related to Safety
		  Related Traffic Information for Road Traffic},
  year		= {2025},
  isbn		= {9798400717338},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3685243.3685291},
  doi		= {10.1145/3685243.3685291},
  abstract	= {Ontologies within the Semantic Web represent a
		  transformative advancement, offering standardized data
		  schemes that seamlessly interconnect with diverse
		  vocabularies and facilitate efficient information exchange
		  globally. This paper explores the intersection of
		  ontologies and road safety, a domain critical to public
		  well-being. The analysis focuses on existing ontologies
		  related to Safety Related Traffic Information, delving into
		  their implications for enhanced road safety, optimized
		  traffic management, and streamlined safety-related data
		  exchange. The study discerns common patterns, best
		  practices, and areas for improvement across various
		  ontological models, thereby contributing to the development
		  of standardized solutions for managing road safety
		  information. Using an ad hoc metrics system for this type
		  of information will assist us in determining which ontology
		  is most suitable and/or the need for its adaptation or
		  utilization. Acknowledging the vital role of ontologies in
		  promoting data interoperability, knowledge sharing, and
		  informed decision-making, this research underscores the
		  potential of integrating Semantic Web technologies and
		  ontological models to revolutionize safety-related
		  information management. The findings advocate for the
		  transformative impact of ontologies in reducing road
		  accidents and safeguarding human lives, emphasizing their
		  significance in advancing the evolution of safer and more
		  efficient road networks.},
  booktitle	= {Proceedings of the 12th Euro American Conference on
		  Telematics and Information Systems},
  articleno	= {10},
  numpages	= {1},
  keywords	= {Intelligent Transport., Knowledge Representation, Linked
		  Data, Safety Road, Semantic Web, Traffic Ontology},
  location	= {Praia, Cape Verde},
  series	= {EATIS 2024}
}

@InProceedings{	  10.1145/3233027.3233029,
  author	= {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o},
		  Robert},
  title		= {Extracting software product line feature models from
		  natural language specifications},
  year		= {2018},
  isbn		= {9781450364645},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3233027.3233029},
  doi		= {10.1145/3233027.3233029},
  abstract	= {The specification of a family of software products may
		  include documents written in natural language.
		  Automatically extracting knowledge from these documents is
		  a challenging problem that requires using Natural Language
		  Processing (NLP) techniques. This knowledge can be
		  formalized as a Feature Model (FM), a diagram capturing the
		  key features and the relationships among them.In this
		  paper, we first review previous works that have presented
		  tools for extracting FMs from textual specifications and
		  compare their strengths and limitations. Then, we propose a
		  framework for feature and relationship extraction, which
		  overcomes the identified limitations and is built upon
		  state-of-the-art open-source NLP tools. This framework is
		  evaluated against previous works using several case
		  studies, showing improved results.},
  booktitle	= {Proceedings of the 22nd International Systems and Software
		  Product Line Conference - Volume 1},
  pages		= {43–53},
  numpages	= {11},
  keywords	= {NLTK, feature model extraction, natural language
		  processing, requirements engineering, software product
		  line},
  location	= {Gothenburg, Sweden},
  series	= {SPLC '18}
}

@InProceedings{	  10.1145/3532213.3532240,
  author	= {Liu, Yixun and Zou, Chuyi and Wang, Yongheng},
  title		= {Distil Knowledge from Natural Language},
  year		= {2022},
  isbn		= {9781450396110},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3532213.3532240},
  doi		= {10.1145/3532213.3532240},
  abstract	= {Knowledge Distillation (KD) is a machine learning approach
		  for model compression and acceleration, which is suitable
		  for applications with limited computational resources. KD
		  is typically performed by distilling and transferring the
		  knowledge of large teacher models into smaller student
		  models, to enhance the performance of the latter. Current
		  KD models require supervision by a pretrained teacher
		  model, whose training requires additional computational
		  cost. In this work, we first analyze the output of the
		  teacher, then propose a method to distill knowledge from
		  natural language. On this basis, we propose
		  Semantic-knowledge-based Teacher-free KD (ST-KD) to further
		  advance model compression and acceleration methods. Our
		  model was tested on the CIFAR10 and CIFAR100 image
		  classification datasets, and it was experimentally
		  demonstrated that it improved the performance of a variety
		  of deep neural networks with virtually no additional
		  computational cost.},
  booktitle	= {Proceedings of the 8th International Conference on
		  Computing and Artificial Intelligence},
  pages		= {181–186},
  numpages	= {6},
  keywords	= {Image classification, Knowledge distillation},
  location	= {Tianjin, China},
  series	= {ICCAI '22}
}

@InProceedings{	  10.1145/3589335.3651945,
  author	= {Ayoub, Michael Antonios Kruse and Su, Zhan and Li,
		  Qiuchi},
  title		= {A Case Study of Enhancing Sparse Retrieval using LLMs},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651945},
  doi		= {10.1145/3589335.3651945},
  abstract	= {While dense retrieval methods have made significant
		  advancements, sparse retrieval techniques continue to offer
		  advantages in terms of interpretability and
		  generalizability. However, query-document term mismatch in
		  sparse retrieval persists, rendering it infeasible for many
		  practical applications. Recent research has shown that
		  Large Language Models (LLMs) hold relevant information that
		  can enhance sparse retrieval through the application of
		  prompt engineering. In this paper, we build upon this
		  concept to explore various strategies employing LLMs for
		  information retrieval purposes. Specifically, we utilize
		  LLMs to enhance sparse retrieval by query rewriting and
		  query expansion. In query rewriting, the original query is
		  refined by creating several new queries. For query
		  expansion, LLMs are employed to generate extra terms,
		  thereby enriching the original query. We conduct
		  experiments on a range of well-known information retrieval
		  datasets, including MSMARCO-passage, TREC2019, TREC2020,
		  Natural Questions, SCIFACT. The experiments show that LLMs
		  can be beneficial for sparse methods since the added
		  information provided by the LLMs can help diminish the
		  discrepancy between the term frequencies of the important
		  terms in a query and the relevant document. In certain
		  domains, we demonstrate that the effectiveness of LLMs is
		  constrained, indicating that they may not consistently
		  perform optimally, which will be explored in future
		  research.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1609–1615},
  numpages	= {7},
  keywords	= {information retrieval, large language models, query
		  expansion, query writing},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3452021.3458310,
  author	= {Console, Marco and Kolaitis, Phokion G. and Pieris,
		  Andreas},
  title		= {Model-theoretic Characterizations of Rule-based
		  Ontologies},
  year		= {2021},
  isbn		= {9781450383813},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3452021.3458310},
  doi		= {10.1145/3452021.3458310},
  abstract	= {An ontology specifies an abstract model of a domain of
		  interest via a formal language that is typically based on
		  logic. Although description logics are popular formalisms
		  for modeling ontologies, tuple-generating dependencies
		  (tgds), originally introduced as a unifying framework for
		  database integrity constraints, and later on used in data
		  exchange and integration, are also well suited for modeling
		  ontologies that are intended for data-intensive tasks. The
		  reason is that, unlike description logics, tgds can easily
		  handle higher-arity relations that naturally occur in
		  relational databases. In recent years, there has been an
		  extensive study of tgd-ontologies and of their applications
		  to several different data-intensive tasks. However, the
		  fundamental question of whether the expressive power of
		  tgd-ontologies can be characterized in terms of
		  model-theoretic properties remains largely unexplored. We
		  establish several characterizations of tgd-ontologies,
		  including characterizations of ontologies specified by such
		  central classes of tgds as full, linear, guarded, and
		  frontier-guarded tgds. Our characterizations use the
		  well-known notions of critical instance and direct product,
		  as well as a novel locality property for tgd-ontologies. We
		  further use this locality property to decide whether an
		  ontology expressed by frontier-guarded (respectively,
		  guarded) tgds can be expressed by tgds in the weaker class
		  of guarded (respectively, linear) tgds, and effectively
		  construct such an equivalent ontology if one exists.},
  booktitle	= {Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium
		  on Principles of Database Systems},
  pages		= {416–428},
  numpages	= {13},
  keywords	= {finite axiomatizability, guardedness, model theory,
		  ontologies, tuple-generating dependencies},
  location	= {Virtual Event, China},
  series	= {PODS'21}
}

@InProceedings{	  10.1145/3469830.3470894,
  author	= {Wang, Xieyang and Xu, Jianqiu and Lu, Hua},
  title		= {NALMO: A Natural Language Interface for Moving Objects
		  Databases},
  year		= {2021},
  isbn		= {9781450384254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3469830.3470894},
  doi		= {10.1145/3469830.3470894},
  abstract	= {Moving objects databases (MODs) have been extensively
		  studied due to their wide variety of applications including
		  traffic management, tourist service and mobile commerce.
		  However, queries in natural languages are still not
		  supported in MODs. Since most users are not familiar with
		  structured query languages, it is essentially important to
		  bridge the gap between natural languages and the underlying
		  MODs system commands. Motivated by this, we design a
		  natural language interface for moving objects, named NALMO.
		  In general, we use semantic parsing in combination with a
		  location knowledge base and domain-specific rules to
		  interpret natural language queries. We design a corpus of
		  moving objects queries for model training, which is later
		  used to determine the query type. Extracted entities from
		  parsing are mapped through deterministic rules to perform
		  query composition. NALMO is able to well translate moving
		  objects queries into structured (executable) languages. We
		  support four kinds of queries including time interval
		  queries, range queries, nearest neighbor queries and
		  trajectory similarity queries. We develop the system in a
		  prototype system SECONDO and evaluate our approach using
		  240 natural language queries extracted from popular
		  conference and journal papers in the domain of moving
		  objects. Experimental results show that (i) NALMO achieves
		  accuracy and precision 98.1 and 88.1, respectively, and
		  (ii) the average time cost of translating a query is
		  1.47s.},
  booktitle	= {Proceedings of the 17th International Symposium on Spatial
		  and Temporal Databases},
  pages		= {1–11},
  numpages	= {11},
  keywords	= {moving objects database, natural language interface, query
		  processing, semantic parsing, structured language},
  location	= {virtual, USA},
  series	= {SSTD '21}
}

@InProceedings{	  10.1145/3543873.3587318,
  author	= {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
  title		= {OntoEval: an Automated Ontology Evaluation System},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587318},
  doi		= {10.1145/3543873.3587318},
  abstract	= {Developing semantically-aware web services requires
		  comprehensive and accurate ontologies. Evaluating an
		  existing ontology or adapting it is a labor-intensive and
		  complex task for which no automated tools exist.
		  Nevertheless, in this paper we propose a tool that aims at
		  making this vision come true, i.e., we present a tool for
		  the automated evaluation of ontologies that allows one to
		  rapidly assess an ontology’s coverage of a domain and
		  identify specific problems in the ontology’s structure.
		  The tool evaluates the domain coverage and correctness of
		  parent-child relations of a given ontology based on domain
		  information derived from a text corpus representing the
		  domain. The tool provides both overall statistics and
		  detailed analysis of sub-graphs of the ontology. In the
		  demo, we show how these features can be used for the
		  iterative improvement of an ontology.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {82–85},
  numpages	= {4},
  keywords	= {BERT, knowledge engineering, natural language processing,
		  ontology},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@Proceedings{	  10.1145/3528588,
  title		= {NLBSE '22: Proceedings of the 1st International Workshop
		  on Natural Language-based Software Engineering},
  year		= {2022},
  isbn		= {9781450393430},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 1st edition of the International Workshop
		  on Natural Language-Based Software Engineering (NLBSE). The
		  potential of Natural Language Processing (NLP) and Natural
		  Language Generation (NLG) to support developers and
		  engineers in a wide number of software engineering-related
		  tasks (e.g., requirements engineering, extraction of
		  knowledge and patterns from the software artifacts,
		  summarization and prioritization of development and
		  maintenance activities, etc.) is increasingly evident.
		  Furthermore, the current availability of libraries (e.g.,
		  NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that
		  allow efficiently and easily dealing with low-level aspects
		  of natural language processing and representation, pushed
		  more and more researchers to closely work with industry to
		  attempt to solve software engineers' real-world problems.},
  location	= {Pittsburgh, Pennsylvania}
}

@InProceedings{	  10.1145/3704137.3704176,
  author	= {Dash, Sheetal and Seker, Huseyin and Shahpasand, Maryam},
  title		= {From Data to Defense: How Ontology Fuels AI in Cyber
		  Threat Detection},
  year		= {2025},
  isbn		= {9798400718014},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704137.3704176},
  doi		= {10.1145/3704137.3704176},
  abstract	= {In today's evolving digital landscape, cybersecurity
		  threats [1] have become increasingly complex and
		  persistent, with attacks like data breaches and ransomware
		  exploiting vulnerabilities in digital systems. As
		  organizations handle growing amounts of data, robust
		  defense strategies depend on comprehensive datasets for
		  detecting and preventing threats. This paper addresses data
		  scarcity in cybersecurity by proposing the development of a
		  dataset ontology tailored to the domain. In data science,
		  ontologies are structured frameworks that define
		  relationships between different concepts within a specific
		  field. A dataset ontology for cybersecurity serves as a
		  cohesive framework for categorizing, organizing, and
		  interconnecting datasets, making it easier for
		  professionals to access and analyze threat-relevant data.
		  The objective is to fill the gap in existing datasets and
		  enable more precise, data-driven cybersecurity strategies.
		  Every cyber-attack generates data, such as network logs,
		  malware metadata, or phishing records. Professionals use
		  this data to analyze threats, understand attack methods,
		  and devise preventive strategies.AI models rely on large
		  volumes of high-quality data to train algorithms, using
		  historical patterns to detect real-time threats and respond
		  before significant damage occurs. Unfortunately, the lack
		  of structured and comprehensive datasets [2] in
		  cybersecurity presents a significant challenge to building
		  these AI models. The data generated by cyber-attacks is
		  often fragmented, inconsistent, or incomplete, making it
		  difficult to develop accurate threat detection systems.
		  Without access to well-organized datasets, AI models are
		  less effective, prone to producing false positives, and
		  unable to adapt to new types of attacks [3]. As a result,
		  cybersecurity defenses are weakened, leaving organizations
		  vulnerable to increasingly sophisticated threats. The
		  dataset ontology proposed in this paper seeks to address
		  the challenge of data scarcity by providing a structured
		  framework for organizing cybersecurity datasets. By
		  categorizing and systematizing data related to various
		  types of cyber-attacks, this ontology facilitates a more
		  comprehensive and detailed understanding of the threat
		  landscape. In the case of cybersecurity, the ontology
		  organizes data based on categories such as attack types,
		  threat actors, vulnerabilities, and attack methods.This
		  proposed well-designed dataset ontology offers several key
		  benefits. First, it makes it easier to identify gaps in
		  existing datasets and create new, targeted datasets that
		  capture the intricacies of real-world cyber-attacks. These
		  datasets can then be used to train AI models, ensuring that
		  they are better equipped to detect and respond to emerging
		  threats. Second, this ontology establishes relationships
		  between different types of cyber-attacks, providing a more
		  holistic view of how these attacks are interconnected. For
		  example, it can link phishing attacks to ransomware
		  infections, allowing AI models to recognize patterns that
		  may otherwise go unnoticed. The ontology-driven approach is
		  particularly valuable because it enables continuous updates
		  and improvements to cybersecurity datasets. As cyber
		  threats evolve, so must the data used to train AI models. A
		  dataset ontology provides the flexibility needed to
		  incorporate new data as it becomes available, ensuring that
		  cybersecurity models remain relevant and effective in the
		  face of changing attack patterns.The research involved the
		  creation of a preliminary draft taxonomy followed by the
		  categorization and classification of the taxonomy into
		  distinct clusters, representing different facets within the
		  realm of cybersecurity. The dataset ontology introduced in
		  this paper offers a solution by ensuring that AI models
		  have access to well-organized, comprehensive datasets that
		  reflect the full spectrum of cyber threats. By
		  systematically categorizing different types of
		  cyber-attacks and the data associated with them, the
		  ontology provides AI models with the information they need
		  to detect both common and emerging threats. This, in turn,
		  enhances the accuracy and reliability of these models,
		  enabling them to detect real-time cyber-attacks with
		  greater precision and fewer false positives.Moreover, the
		  ontology-driven approach ensures that AI models remain
		  adaptable in the face of evolving threats [4].
		  Cyber-attacks are not static; they continuously evolve as
		  malicious actors develop new techniques to bypass existing
		  defenses. Without access to updated datasets, AI models may
		  become outdated and ineffective. The dataset ontology
		  addresses this issue by allowing for continuous updates to
		  the data used in training, ensuring that AI models remain
		  agile and responsive to new challenges. The practical
		  applications of the proposed dataset ontology extend beyond
		  the realm of theoretical research. By providing a
		  structured and comprehensive framework for organizing
		  cybersecurity data, the ontology enables organizations to
		  develop more effective defense strategies and respond more
		  quickly to emerging threats. For instance, organizations
		  can use dataset ontology to create AI models capable of
		  detecting insider threats, which are particularly difficult
		  to identify due to the complexity of monitoring behavior
		  [5] within trusted environments. These models can also be
		  used to detect zero-day vulnerabilities, which are
		  exploited by attackers before security teams have the
		  chance to patch them.Additionally, dataset ontology
		  facilitates collaboration across the cybersecurity industry
		  by providing a common framework for sharing data.
		  Currently, many organizations struggle to share
		  cybersecurity data due to the lack of standardized formats
		  and structures. The proposed ontology provides a solution
		  to this issue by offering a standardized framework that can
		  be adopted across the industry, making it easier for
		  organizations to collaborate and share data in a meaningful
		  way.In conclusion, the creation of dataset ontology
		  represents a significant advancement in the field of
		  cybersecurity. By addressing the problem of data scarcity
		  and fragmentation [5], the ontology provides a structured
		  framework for organizing and interconnecting diverse
		  datasets, allowing for more effective data analysis and
		  threat detection. This ontology-driven approach enhances
		  the accuracy and adaptability of AI models, enabling them
		  to detect and neutralize cyber threats in real time.
		  Furthermore, the practical applications of dataset ontology
		  extend beyond academic research, empowering organizations
		  to protect their data, infrastructure, and services from a
		  wide range of cyber-attacks.As cyber threats continue to
		  evolve, the need for structured and comprehensive datasets
		  will only grow. The dataset ontology provides a much-needed
		  solution to this challenge, ensuring that AI models remain
		  effective in detecting and responding to new and emerging
		  threats. By improving the way cybersecurity data is
		  organized and analyzed, ontology helps safeguard the
		  interconnected digital ecosystem that underpins modern
		  society.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Advances in Artificial Intelligence},
  pages		= {121–133},
  numpages	= {13},
  keywords	= {DDoS, Taxonomy, cyber security, datasets, metrics,
		  ontology, real-time cyber threats, type},
  location	= { },
  series	= {ICAAI '24}
}

@InProceedings{	  10.1145/3711542.3711607,
  author	= {Al Subhi, Sundos Nasser Said and Sassani, Ardavan and
		  Mikler, Armin Robert},
  title		= {Navigating Complexity: A Multi-Domain Ontology Evaluation
		  with Cluster Centroids as Hierarchical Representatives},
  year		= {2025},
  isbn		= {9798400717383},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711542.3711607},
  doi		= {10.1145/3711542.3711607},
  abstract	= {Building on prior research, this study extends and
		  improves an algorithm originally designed to identify
		  flood-related ontology concepts, evaluating its
		  applicability across multiple fields. The primary
		  objectives are to: (1) demonstrate the algorithm’s
		  effectiveness in various domains (urban system, flood with
		  urban system, and flood with fashion) and (2) illustrate
		  how community detection enables each cluster’s
		  “centroid” to anchor the ontology hierarchy and
		  represent core topics. The improved algorithm shows greater
		  efficiency, completing in 0.35 seconds compared to 0.6
		  seconds for the original algorithm on datasets with up to
		  1,000 rows, with both algorithms delivering consistent
		  results. The findings confirm the algorithm’s
		  adaptability across different domains and demonstrate the
		  use of cluster centroids, derived through community
		  detection, as representatives of key topics in
		  domain-specific ontologies. This approach enhances
		  information retrieval, supports coherent data integration
		  across systems, and provides a framework to aid
		  decision-making and analysis in complex domains.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {192–198},
  numpages	= {7},
  keywords	= {Multi-Domain Ontology, Community Detection, Co-Occurrence
		  Graphs},
  location	= { },
  series	= {NLPIR '24}
}

@InProceedings{	  10.1145/3701716.3715309,
  author	= {Sun, Qiang and Luo, Yuanyi and Zhang, Wenxiao and Li,
		  Sirui and Li, Jichunyang and Niu, Kai and Kong, Xiangrui
		  and Liu, Wei},
  title		= {Docs2KG: A Human-LLM Collaborative Approach to Unified
		  Knowledge Graph Construction from Heterogeneous Documents},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715309},
  doi		= {10.1145/3701716.3715309},
  abstract	= {Enterprises generate vast amounts of unstructured
		  documents, posing challenges for knowledge extraction and
		  representation. Large language models (LLMs) offer strong
		  potential for processing such data but struggle with
		  factual accuracy and provenance. Knowledge graphs (KGs)
		  provide a structured framework to address these limitations
		  [6], yet constructing high-quality KGs from heterogeneous
		  data remains a challenge. To address this issue, we present
		  Docs2KG, a modular framework to build high-quality KGs from
		  diverse unstructured documents. We first employs
		  state-of-the-art document processing techniques to extract
		  textual content, tabular data, and figures. The extracted
		  information is then unified into a multifaceted KG with
		  three aspects: (1) a Layout KG capturing document
		  structural hierarchies, (2) a Metadata KG preserving
		  document properties, and (3) a Semantic KG representing
		  domain-specific entities and relationships. Docs2KG
		  supports multiple construction paradigms for Semantic KG:
		  ontology-based approaches, hybrid NLP pipelines with LLM
		  verification, LLM-guided ontology generation, and
		  specialized models for named entity recognition, event
		  extraction, and causal relationship identification to
		  enhance semantic coverage and accuracy. A key feature of
		  Docs2KG is its human-in-the-loop verification interface,
		  enabling iterative quality assessment and refinement of the
		  resulting KGs. Docs2KG is openly available at
		  https://docs2kg.ai4wa.com, with the aim of advancing
		  knowledge graph construction research and accelerating
		  enterprise applications through high-quality knowledge
		  graph construction.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {801–804},
  numpages	= {4},
  keywords	= {heterogeneous data, knowledge graph, unstructured data},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3229345.3229373,
  author	= {de Almeida Bordignon, Ana Cl\'{a}udia and Thom,
		  Lucin\'{e}ia Heloisa and Silva, Thanner Soares and Dani,
		  Vinicius Stein and Fantinato, Marcelo and Ferreira, Renato
		  Cesar Borges},
  title		= {Natural Language Processing in Business Process
		  Identification and Modeling: A Systematic Literature
		  Review},
  year		= {2018},
  isbn		= {9781450365598},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3229345.3229373},
  doi		= {10.1145/3229345.3229373},
  abstract	= {Business Process Management (BPM) has been receiving
		  increasing attention in recent years. Many organizations
		  have been adapting their business to a process-centered
		  view since they started noticing its potential to reduce
		  costs, improve productivity and achieve higher levels of
		  quality. However, implementing BPM in organizations
		  requires time, making the automation of process
		  identification and discovery highly desirable. To achieve
		  this expectation, the application of Natural Language
		  Processing (NLP) techniques and tools has emerged to
		  generate process models from unstructured text. In this
		  paper, we provide the results of a systematic literature
		  review conducted in preparation and processing of natural
		  language text aiming the extraction of business processes
		  and process quality assurance. The study presents
		  techniques applied to the BPM life-cycle phases of process
		  identification, process discovery and process analysis as
		  well as tools to support process discovery. This review
		  covered papers from 2009 up to 2016 and identifies 518
		  articles of which 33 were selected as relevant to our work.
		  The results of the present study may be valuable to support
		  research in extraction of business process models from
		  natural language text.},
  booktitle	= {Proceedings of the XIV Brazilian Symposium on Information
		  Systems},
  articleno	= {25},
  numpages	= {8},
  keywords	= {Business Process Management, Natural Language Processing,
		  Process Analysis, Process Discovery, Systematic Literature
		  Review},
  location	= {Caxias do Sul, Brazil},
  series	= {SBSI '18}
}

@Article{	  10.1145/3722233,
  author	= {Novak, Justin and Hueca, Angel and Rodman, Christopher and
		  Perl, Samuel and Breaux, Travis and Valdengo, Justin},
  title		= {Building a Better SOC: Towards the Ontology for Security
		  Operations Center Assistance and Replication (OSCAR)},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {1},
  url		= {https://doi.org/10.1145/3722233},
  doi		= {10.1145/3722233},
  abstract	= {There are many methods for developing a Security
		  Operations Center (SOC) or SOC capability. However, there
		  currently exists no unified approach which comprehensively
		  outlines the people, processes, and technology required for
		  developing a SOC, or how an organization might implement
		  those into an effective SOC capability. This article
		  outlines a data gathering process used to compile knowledge
		  necessary for a proposed Ontology for SOC Creation
		  Assistance and Replication, which can serve as a solution
		  to the gap in the current body of knowledge. An ontology
		  such as the one proposed here would leverage the collective
		  experience of a large cadre of cybersecurity experts with
		  deep knowledge in fields related to Security Operations and
		  the development of SOCs. Using interview methods and
		  analysis, the knowledge of how these experts approach the
		  problem of creating new SOC capabilities within a set of
		  known constraints can be captured and codified. The result
		  is a comprehensive body of structured knowledge outlining
		  what critical decisions are made during the process, and
		  how those decisions affect the implementation of People,
		  Processes, and Technology which become part of a SOC. It is
		  this body of knowledge which can be organized and presented
		  as a formal ontology.},
  journal	= {Digital Threats},
  month		= mar,
  articleno	= {6},
  numpages	= {22},
  keywords	= {SOC, Security Operations, Security Operations Center,
		  People, Process, Technology, Incident Response, Ontology}
}

@InProceedings{	  10.1145/3657054.3657077,
  author	= {Maratsi, Maria Ioanna and Ahmed, Umair and Alexopoulos,
		  Charalampos and Charalabidis, Yannis and Polini, Andrea},
  title		= {Towards Cross-Domain Linking of Data: A Semantic Mapping
		  of Cultural Heritage Ontologies},
  year		= {2024},
  isbn		= {9798400709883},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3657054.3657077},
  doi		= {10.1145/3657054.3657077},
  abstract	= {The Linked Open Vocabularies (LOV) registry, designed with
		  the Linked Data principles at core, provides an environment
		  suitable for research which targets domain-specific, but
		  also potentially reusable, information representation. The
		  main purpose of this study is to follow the recommendations
		  pertaining to the utilisation of LOV as a basis for
		  experimentation in order to examine how information within
		  the Cultural Heritage (CH) domain can be improved in terms
		  of reusability and interoperability. The present lack of
		  cross-domain knowledge transfer forms the motivation behind
		  this study, with the aim of facilitating the transition
		  from conventional, domain-specific knowledge representation
		  to reusable and semantically interoperable information. The
		  methodology of this study involves the manual semantic
		  mapping of elements from 12 vocabularies in the LOV
		  registry, reinforced by a small-scale experiment using
		  contemporary large language models (LLMs), particularly
		  GPT, for a preliminary assessment of the mapping process.
		  The findings revealed several key aspects to consider
		  regarding the alignment of semantically adjacent vocabulary
		  elements in the CH domain and beyond, emphasising the
		  potential unveiled by linking domain-focused schemata to
		  standardised, established ones while preserving the
		  conceptual hierarchies inherent to each individual
		  knowledge domain. The contribution of this research
		  pertains to the vision of linking data across different
		  domains by initiating the alignment among representation
		  schemata in CH, with the ultimate aim to expand beyond the
		  boundaries of the in-word knowledge domain, while employing
		  combinatory methodological approaches of technological
		  means and human expertise to facilitate this process.},
  booktitle	= {Proceedings of the 25th Annual International Conference on
		  Digital Government Research},
  pages		= {165–176},
  numpages	= {12},
  location	= {Taipei, Taiwan},
  series	= {dg.o '24}
}

@InProceedings{	  10.1145/3180374.3181333,
  author	= {Hamdani, Maryum and Butt, Wasi Haider and Anwar, Muhammad
		  Waseem and Azam, Farooque},
  title		= {A Systematic Literature Review on Interaction Flow
		  Modeling Language (IFML)},
  year		= {2018},
  isbn		= {9781450354318},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3180374.3181333},
  doi		= {10.1145/3180374.3181333},
  abstract	= {Design of front-end interfaces in software applications is
		  a complex process. In this context, Interaction Flow
		  Modeling Language (IFML) is an emerging standard introduced
		  in 2013 by Object Management Group (OMG). In this article,
		  a Systematic Literature Review (SLR) is performed to
		  examine the applications of IFML. Particularly, 22 research
		  studies (2014-2017) are identified and analyzed.
		  Consequently, four major areas are recognized where IFML is
		  frequently applied i.e. mobile applications (9 studies),
		  web applications (8 studies), others (4 studies) and
		  desktop applications (1 study). Furthermore, 9 leading IFML
		  tools are presented i.e. Modeling (3) and model
		  transformation (6). It has been concluded that IFML
		  certainly simplifies the design and implementation of
		  front-end interfaces. However, the existing IFML tools are
		  not mature enough to be utilized for complex and large
		  software applications.},
  booktitle	= {Proceedings of the 2018 2nd International Conference on
		  Management Engineering, Software Engineering and Service
		  Sciences},
  pages		= {134–138},
  numpages	= {5},
  keywords	= {IFML, Interaction flow modeling language, MDA, SLR},
  location	= {Wuhan, China},
  series	= {ICMSS 2018}
}

@InProceedings{	  10.1145/3489088.3489089,
  author	= {A. Setiawan, Foni and Murdani, Dendy and Riana, Freza and
		  Dwimawati, Eny},
  title		= {COPOMBOCY: A COVID-19 Pandemic Ontology Model of Bogor
		  City},
  year		= {2022},
  isbn		= {9781450385244},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3489088.3489089},
  doi		= {10.1145/3489088.3489089},
  abstract	= {Coronavirus Disease of 2019 (COVID-19) has become a global
		  health problem along with a declaration by the World Health
		  Organization (WHO) on March 11, 2020, which declared it a
		  pandemic. COVID-19 has spread to almost all countries,
		  including Indonesia. Data related to the COVID-19 pandemic
		  is complex and heterogeneous. To generate and maintain
		  knowledge that is semantically stored in it, knowledge
		  modeling is necessary to be done. This study aims to
		  develop a model of knowledge about the COVID-19 pandemic in
		  Bogor City in the form of ontology. The scope of knowledge
		  includes the distribution of agents, close contact tracing,
		  COVID-19 transmission, diagnosis, disease, type of illness,
		  location, Large-Scale Social Restrictions (Pembatasan
		  Sosial Berskala Besar [PSBB]) implementation, PSBB
		  sanctions, statistics, status, symptoms, test processes,
		  test results, and color zones. Data were obtained from the
		  Bogor City COVID-19 Task Force, COVID-19 Information \&amp;
		  Coordination Center of West Java Province (Pusat Informasi
		  \&amp; Koordinasi COVID-19 Provinsi Jawa Barat [PIKOBAR]),
		  Covid19.go.id, literature review, interview with
		  epidemiologist, and reuse of knowledge from existing
		  ontologies. The result of this study is an ontology in the
		  Web Ontology Language (OWL) format consisting of 88
		  classes, 29 object properties, 48 property data, 2103
		  axioms, and 229 individuals. The test results on the built
		  ontology were successful in answering 85.7\% of non-expert
		  questions and 71.4\% of expert questions. Overall, the
		  ontology built successfully answered 78.6\% of the
		  questions about the COVID-19 pandemic in Bogor City. The
		  ontology has been published so that it is publicly
		  available and is still being developed to accommodate the
		  latest data.},
  booktitle	= {Proceedings of the 2021 International Conference on
		  Computer, Control, Informatics and Its Applications},
  pages		= {86–90},
  numpages	= {5},
  keywords	= {Bogor City, COVID-19, Ontology, Pandemic},
  location	= {Virtual/online conference, Indonesia},
  series	= {IC3INA '21}
}

@Article{	  10.1145/3604612,
  author	= {Dave, Nakul R. and Mehta, Mayuri A. and Kotecha, Ketan},
  title		= {A Systematic Review of Stemmers of Indian and Non-Indian
		  Vernacular Languages},
  year		= {2024},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3604612},
  doi		= {10.1145/3604612},
  abstract	= {The stemming process is crucial and significant in the
		  pre-processing step of natural language processing. The
		  stemmer oversees the stemming process. It facilitates the
		  extraction of morphological variants of a root or base word
		  from the provided word. Over the period, several stemmers
		  for various vernacular languages have been proposed.
		  However, very few research studies have comprehensively
		  investigated these available stemmers. This article makes
		  multifold contributions. First, we discuss the various
		  stemmers of 15 Indian and 17 non-Indian languages
		  describing their key points, benefits, and drawbacks. All
		  the Indian languages for which stemmers have been built are
		  covered in this study. For the non-Indian languages,
		  stemmers of commonly spoken languages have been covered.
		  Second, we present a language-wise comparative analysis of
		  stemmers based on our identified parameters. Third, we
		  discuss the wordnets and dictionaries available for
		  different languages. Fourth, we provide details of the
		  datasets available for various languages. Fifth, we also
		  provide challenges in existing stemmers and future
		  directions for future researchers. The study presented in
		  this article reveals that significant research has been
		  carried out for the stemmers of influential languages such
		  as English, Arabic, and Urdu. On the other hand, languages
		  with d resources, such as Farsi, Polish, Odia, Amharic, and
		  others, have received the least attention for research.
		  Moreover, rigorous analysis reveals that most of the
		  stemmers suffer from over-stemming errors. With a complete
		  catalogue of available stemmers, this study aims at
		  assisting the researchers and professionals working in the
		  areas such as information retrieval, semantic annotation,
		  word meaning disambiguation, and ontology learning.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  articleno	= {18},
  numpages	= {51},
  keywords	= {Natural Language Processing (NLP), stemming, rule-based
		  stemmer, dictionary-based stemmer, hybrid stemmer,
		  over-stemming error, under-stemming error}
}

@InProceedings{	  10.1145/3442442.3452347,
  author	= {Johnson, Isaac and Gerlach, Martin and S\'{a}ez-Trumper,
		  Diego},
  title		= {Language-agnostic Topic Classification for Wikipedia},
  year		= {2021},
  isbn		= {9781450383134},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442442.3452347},
  doi		= {10.1145/3442442.3452347},
  abstract	= {A major challenge for many analyses of Wikipedia
		  dynamics—e.g., imbalances in content quality, geographic
		  differences in what content is popular, what types of
		  articles attract more editor discussion—is grouping the
		  very diverse range of Wikipedia articles into coherent,
		  consistent topics. This problem has been addressed using
		  various approaches based on Wikipedia’s category network,
		  WikiProjects, and external taxonomies. However, these
		  approaches have always been limited in their coverage:
		  typically, only a small subset of articles can be
		  classified, or the method cannot be applied across (the
		  more than 300) languages on Wikipedia. In this paper, we
		  propose a language-agnostic approach based on the links in
		  an article for classifying articles into a taxonomy of
		  topics that can be easily applied to (almost) any language
		  and article on Wikipedia. We show that it matches the
		  performance of a language-dependent approach while being
		  simpler and having much greater coverage.},
  booktitle	= {Companion Proceedings of the Web Conference 2021},
  pages		= {594–601},
  numpages	= {8},
  keywords	= {Wikipedia, language-agnostic, topic classification},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3708468.3711892,
  author	= {Post, Kevin and Kuchida, Reo and Olapade, Mayowa and Yin,
		  Zhigang and Nurmi, Petteri and Flores, Huber},
  title		= {ContextLLM: Meaningful Context Reasoning from Multi-Sensor
		  and Multi-Device Data Using LLMs},
  year		= {2025},
  isbn		= {9798400714030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708468.3711892},
  doi		= {10.1145/3708468.3711892},
  abstract	= {Conventional context awareness and activity recognition
		  models produce abstract outputs that offer limited insights
		  into user behavior and situational context. These can be
		  significantly enhanced by leveraging multi-sensor and
		  multi-device data streams. However, the aggregation and
		  modelling of context sensor data presents complex
		  challenges that require advanced inference capabilities. We
		  introduce ContextLLM, a context-driven solution powered by
		  Large Language Models (LLMs), designed to transform sparse,
		  abstract insights from various sensors and devices into a
		  detailed, descriptive context. Through rigorous experiments
		  using a well-established benchmark dataset for activity
		  recognition, we demonstrate that ContextLLM can
		  significantly enhance context understanding. However, our
		  analysis also highlights how the quality and complexity of
		  sensor data representations impact the LLM's ability to
		  accurately deduce context. Building on these findings, we
		  develop a research agenda that outlines key challenges, and
		  conclude with a discussion on the limitations and practical
		  considerations of LLM-based reasoning in context-aware
		  applications.},
  booktitle	= {Proceedings of the 26th International Workshop on Mobile
		  Computing Systems and Applications},
  pages		= {13–18},
  numpages	= {6},
  keywords	= {Context modelling, Sensor data assistant, Wearable data},
  location	= {La Quinta, CA, USA},
  series	= {HotMobile '25}
}

@Article{	  10.1145/3594724,
  author	= {Sartini, Bruno and Baroncini, Sofia and van Erp, Marieke
		  and Tomasi, Francesca and Gangemi, Aldo},
  title		= {ICON: An Ontology for Comprehensive Artistic
		  Interpretations},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3594724},
  doi		= {10.1145/3594724},
  abstract	= {In this work, we introduce ICON, an ontology that models
		  artistic interpretations of artworks’ subject matter
		  (i.e., iconographies) and meanings (i.e., symbols,
		  iconological aspects). Developed by conceptualizing
		  authoritative knowledge and notions taken from Panofsky’s
		  levels of interpretation theory, ICON ontology focuses on
		  the granularity of interpretations. It can be used to
		  describe an interpretation of an artwork from the
		  pre-iconographical, icongraphical, and iconological levels.
		  Its main classes have been aligned to ontologies that come
		  from the domains of cultural descriptions (ArCo, CIDOC-CRM,
		  VIR), semiotics (DOLCE), bibliometrics (CITO), and
		  symbolism (Simulation Ontology), to grant a robust schema
		  that can be extendable using additional classes and
		  properties coming from these ontologies. The ontology was
		  evaluated through competency questions that range from
		  simple recognition on a specific level of interpretation to
		  complex scenarios. Data written using this model was
		  compared to state-of-the-art ontologies and schemas to both
		  highlight the current lack of a domain-specific ontology on
		  art interpretation and show how our work fills some of the
		  current gaps. The ontology is openly available and
		  compliant with FAIR principles. With our ontology, we hope
		  to encourage digital art historians working for cultural
		  institutions in making more detailed linked open data about
		  the content of their artifacts, to exploit the full
		  potential of Semantic Web in linking artworks through not
		  only subjects and common metadata but also specific
		  symbolic interpretations, intrinsic meanings, and the
		  motifs through which their subjects are represented.
		  Additionally, by basing our work on theories made by
		  different art history scholars in the last century, we make
		  sure that their knowledge and studies will not be lost in
		  the transition to the digital, linked open data era.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {59},
  numpages	= {38},
  keywords	= {Iconology, iconography, art interpretation, ontology,
		  cultural heritage, Semantic Web}
}

@InProceedings{	  10.1145/3686397.3686420,
  author	= {Sun, Yi and Yang, Wanru and Liu, Yin},
  title		= {The Application of Constructing Knowledge Graph of Oral
		  Historical Archives Resources Based on LLM-RAG},
  year		= {2024},
  isbn		= {9798400717345},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3686397.3686420},
  doi		= {10.1145/3686397.3686420},
  abstract	= {Oral historical archive resources are an emerging archive
		  resource with the rapid development of modern technology.
		  Its "bottom-up" approach to historical research has
		  received widespread attention in the fields of history,
		  archives, and libraries. Under the common knowledge
		  discovery mode, oral historical archives resources are
		  showing a dispersed state. Information technology
		  represented by knowledge graphs can break through the data
		  solidification of oral historical archives, reshape the
		  information stack of oral historical archives, and achieve
		  knowledge association and aggregation of oral historical
		  archive resources. The article attempts to construct a
		  knowledge graph of the oral historical archives resources
		  on the theme of "science and art" in the collection of T.D.
		  Lee Library of Shanghai Jiao Tong University. It uses Large
		  Language Model - Retrieval Augmented Generation (LLM-RAG)
		  for knowledge extraction, and then uses a semantic model
		  for knowledge organization and management. The article
		  attempts to empower humanities with technology, exploring
		  the possibility of combining "digital technology" and
		  "humanities research", extending traditional humanities
		  research methods, breaking down barriers between technology
		  and humanities resources, and providing a new path
		  reference for revealing resource content characteristics,
		  semantic deep correlation, and multi-dimensional knowledge
		  discovery.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Information System and Data Mining},
  pages		= {142–149},
  numpages	= {8},
  keywords	= {Knowledge Graph, LLM-RAG, Oral History Archives},
  location	= { },
  series	= {ICISDM '24}
}

@InProceedings{	  10.1145/3672608.3707818,
  author	= {Mariani, Elisa and Seghouani, Nac\'{e}ra and Ma, Yue},
  title		= {A Hybrid Self-Correcting Approach for Embedding
		  Ontology-based Knowledge Graphs},
  year		= {2025},
  isbn		= {9798400706295},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672608.3707818},
  doi		= {10.1145/3672608.3707818},
  abstract	= {One significant challenge in Knowledge Graph (KG)
		  Embedding is the generation of negative triples. Negative
		  triples are essential as they enable a training model to
		  distinguish between relationships that exist within the KG
		  and those that do not. In this paper, we propose
		  TransHySeCo approach: a Hybrid and Self-Correcting approach
		  for embedding knowledge graphs. TransHySeCo is based on a
		  hybrid training using both the domain semantics provided in
		  the ontology related to the KG and the topology underlying
		  the graph structure. Moreover, it is self-correcting. It
		  generates new negative triples by leveraging the embeddings
		  from previous training iterations and the (quasi-)true
		  negatives obtained with the ontology-based negative
		  generation method proposed in this paper. The
		  self-correction terminates when no new (quasi-)true
		  negative triple is generated. To evaluate TransHySeCo, we
		  conducted experiments on different benchmark datasets and
		  assessed the embeddings' effectiveness for the link
		  prediction task. The results show that TransHySeCo provides
		  KG embeddings of promising quality for link prediction.},
  booktitle	= {Proceedings of the 40th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1156–1163},
  numpages	= {8},
  keywords	= {knowledge graph, ontology, translational embedding, link
		  prediction},
  location	= {Catania International Airport, Catania, Italy},
  series	= {SAC '25}
}

@Article{	  10.1145/3708520,
  author	= {Cederbladh, Johan and Cicchetti, Antonio and Jongeling,
		  Robbert},
  title		= {A Road-Map to Readily Available Early Validation and
		  Verification of System Behaviour in Model-Based Systems
		  Engineering using Software Engineering Best Practices},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {34},
  number	= {5},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3708520},
  doi		= {10.1145/3708520},
  abstract	= {In this article, we discuss how we can facilitate the
		  growing need for early validation and verification
		  (V&amp;V) of system behaviour in Model-Based Systems
		  Engineering (MBSyE). Several aspects, such as reducing cost
		  and time to market, push companies towards integration of
		  V&amp;V methods earlier in development to support effective
		  decision-making. One foundational methodology seeing
		  increased attention in industry is the use of MBSyE, which
		  brings benefits of models with well-defined syntax and
		  semantics to support V&amp;V activities, rather than
		  relying on natural language text documentation. Despite
		  their promise, industrial adoption of these practices is
		  still challenging.This article presents a vision for
		  readily available early V&amp;V. We present a summary of
		  the literature on early V&amp;V in MBSyE and position
		  existing challenges regarding potential solutions and
		  future investigations towards this vision. We elaborate our
		  vision by means of challenges with a specific emphasis on
		  early V&amp;V of system behaviour. We identify three
		  specific challenge areas: Creating and managing Models,
		  Organisational systems engineering aspects, and early
		  V&amp;V Methods. Finally, we outline a road-map to address
		  these categories of challenges, in which we propose the
		  transfer of established best practices from the software
		  engineering domain to support emerging technologies in the
		  systems engineering domain.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= may,
  articleno	= {151},
  numpages	= {30},
  keywords	= {Validation, Verification, Models, Early, Systems,
		  Behaviour}
}

@InProceedings{	  10.1145/3631700.3665226,
  author	= {Afreen, Neda and Balloccu, Giacomo and Boratto, Ludovico
		  and Fenu, Gianni and Malloci, Francesca Maridina and
		  Marras, Mirko and Martis, Andrea Giovanni},
  title		= {Learner-centered Ontology for Explainable Educational
		  Recommendation},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3665226},
  doi		= {10.1145/3631700.3665226},
  abstract	= {Ontologies form the core of knowledge graphs, which act as
		  faithful, semantic-rich sources for training models in
		  delivering explainable recommendations. These models learn
		  to extract logical paths between learners and resources to
		  be recommended within the knowledge graph, according to
		  behavior- and content-based patterns. Extracted paths are
		  then used not only to provide recommendations, but also to
		  generate accompanying textual explanations. Despite the
		  potential of this approach, current ontologies derived from
		  the traditional learner-resource interaction data fall
		  short in terms of richness from an educational perspective.
		  Conversely, general-purpose ontologies, while comprehensive
		  in educational aspects, are overly complex for
		  recommendation tasks. Unfortunately, a suboptimal ontology
		  might prevent to articulate reasoning paths, and thus
		  explanations, relevant for learners within the knowledge
		  graph. To counter this limitation, in this paper, we
		  propose LOXER, a novel ontology designed to unlock
		  learner-centered logical paths for explainable educational
		  recommendation. Our design integrates insights from diverse
		  sources, including feedback from a local co-design group of
		  learners, observations from specialized traditional
		  large-scale educational recommendation datasets, and
		  connections with well-known vocabularies of other existing
		  ontologies. To validate our ontology, we conducted an
		  evaluation of the explanation types it enables, involving
		  university and lifelong learners and assessing explanation
		  properties like effectiveness, decision-making speed,
		  motivation, satisfaction, and confidence. Results show our
		  ontology’s ability to foster diverse considerations
		  during the learners’ decision-making process and to
		  establish a semantic structure for knowledge graphs for
		  explainable recommendation.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {567–575},
  numpages	= {9},
  keywords	= {Explainability., Ontology, Recommendation},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@InProceedings{	  10.1145/3425898.3426958,
  author	= {van Binsbergen, L. Thomas and Liu, Lu-Chi and van
		  Doesburg, Robert and van Engers, Tom},
  title		= {eFLINT: a domain-specific language for executable norm
		  specifications},
  year		= {2020},
  isbn		= {9781450381741},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3425898.3426958},
  doi		= {10.1145/3425898.3426958},
  abstract	= {Software systems that share potentially sensitive data are
		  subjected to laws, regulations, policies and/or contracts.
		  The monitoring, control and enforcement processes applied
		  to these systems are currently to a large extent manual,
		  which we rather automate by embedding the processes as
		  dedicated and adaptable software services in order to
		  improve efficiency and effectiveness. This approach
		  requires such regulatory services to be closely aligned
		  with a formal description of the relevant norms. This paper
		  presents eFLINT, a domain-specific language developed for
		  formalizing norms. The theoretical foundations of the
		  language are found in transition systems and in Hohfeld's
		  framework of legal fundamental conceptions. The language
		  can be used to formalize norms from a large variety of
		  sources. The resulting specifications are executable and
		  support several forms of reasoning such as automatic case
		  assessment, manual exploration and simulation. Moreover,
		  the specifications can be used to develop regulatory
		  services for several types of monitoring, control and
		  enforcement. The language is evaluated through a case study
		  formalizing articles 6(1)(a) and 16 of the General Data
		  Protection Regulation (GDPR). A prototype implementation of
		  eFLINT is discussed and is available online.},
  booktitle	= {Proceedings of the 19th ACM SIGPLAN International
		  Conference on Generative Programming: Concepts and
		  Experiences},
  pages		= {124–136},
  numpages	= {13},
  keywords	= {GDPR, domain-specific language, executable specifications,
		  normative modeling, policy enforcement},
  location	= {Virtual, USA},
  series	= {GPCE 2020}
}

@Article{	  10.1145/3418208,
  author	= {Ni, Pin and Li, Yuming and Li, Gangmin and Chang, Victor},
  title		= {A Hybrid Siamese Neural Network for Natural Language
		  Inference in Cyber-Physical Systems},
  year		= {2021},
  issue_date	= {June 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {2},
  issn		= {1533-5399},
  url		= {https://doi.org/10.1145/3418208},
  doi		= {10.1145/3418208},
  abstract	= {Cyber-Physical Systems (CPS), as a multi-dimensional
		  complex system that connects the physical world and the
		  cyber world, has a strong demand for processing large
		  amounts of heterogeneous data. These tasks also include
		  Natural Language Inference (NLI) tasks based on text from
		  different sources. However, the current research on natural
		  language processing in CPS does not involve exploration in
		  this field. Therefore, this study proposes a Siamese
		  Network structure that combines Stacked Residual Long
		  Short-Term Memory (bidirectional) with the Attention
		  mechanism and Capsule Network for the NLI module in CPS,
		  which is used to infer the relationship between
		  text/language data from different sources. This model is
		  mainly used to implement NLI tasks and conduct a detailed
		  evaluation in three main NLI benchmarks as the basic
		  semantic understanding module in CPS. Comparative
		  experiments prove that the proposed method achieves
		  competitive performance, has a certain generalization
		  ability, and can balance the performance and the number of
		  trained parameters.},
  journal	= {ACM Trans. Internet Technol.},
  month		= mar,
  articleno	= {33},
  numpages	= {25},
  keywords	= {Cyber-physical systems, Natural language inference,
		  Siamese neural networks}
}

@InProceedings{	  10.1145/3383583.3398545,
  author	= {Qin, Jian and \v{Z}umer, Maja and Wang, Xiaoguang and Fan,
		  Wei},
  title		= {Conceptual Models and Ontological Schemas for Semantically
		  Sustainable Digital Libraries},
  year		= {2020},
  isbn		= {9781450375856},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383583.3398545},
  doi		= {10.1145/3383583.3398545},
  abstract	= {Semantic frameworks build foundations for digital
		  libraries and repositories to enable structured data and
		  information representation and interoperability in today's
		  interlinked information systems. Conceptual modeling and
		  ontological schemas provide effective communication and
		  powerful tools for creating shared understanding and
		  sustainable systems in various digital libraries. This
		  panel will present cases in which conceptual modeling and
		  ontologies are used to enrich content representation and
		  reach consensus among communities of practice, especially
		  in fast changing digital society and emerging application
		  domains. Four experts in knowledge organization will first
		  give a brief introduction for their research in conceptual
		  modeling and ontology building and then engage the audience
		  with question answering interactions.},
  booktitle	= {Proceedings of the ACM/IEEE Joint Conference on Digital
		  Libraries in 2020},
  pages		= {441–442},
  numpages	= {2},
  keywords	= {conceptual modeling, cultural heritage resource
		  description, knowledge representation, library reference
		  model, ontological modeling, semantic enrichment},
  location	= {Virtual Event, China},
  series	= {JCDL '20}
}

@InProceedings{	  10.1145/3428658.3430973,
  author	= {Rocha, Bartira Dantas and Silva, Larysse and Batista,
		  Thais and Cavalcante, Everton and Gomes, Porf\'{\i}rio},
  title		= {An Ontology-based Information Model for Multi-Domain
		  Semantic Modeling and Analysis of Smart City Data},
  year		= {2020},
  isbn		= {9781450381963},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3428658.3430973},
  doi		= {10.1145/3428658.3430973},
  abstract	= {Smart city services are typically defined according to
		  domains (e.g., health, education, safety) and supported by
		  different systems. Consequently, the analysis of smart city
		  data is often domain-specific, thus limiting the
		  capabilities of the offered services and hampering
		  decision-making that relies on isolated domain information.
		  To support a suitable analysis across multiple domains, it
		  is necessary having a unified data model able to handle the
		  inherent heterogeneity of smart city data and take into
		  account both geographic and citizen information. This paper
		  presents an ontology-based information model to support
		  multi-domain analysis in smart cities to foster
		  interoperability and powerful automated reasoning upon
		  unambiguous information. The proposed information model
		  follows Linked Data principles and takes advantage of
		  ontologies to define information semantically. The semantic
		  relationships and properties defined in the model also
		  allow inferring new pieces of information that improve
		  accuracy when analyzing multiple city domains. This paper
		  reports an evaluation of the information model through
		  ontological metrics and competence questions.},
  booktitle	= {Proceedings of the Brazilian Symposium on Multimedia and
		  the Web},
  pages		= {73–80},
  numpages	= {8},
  keywords	= {Inference, Information model, Linked Data, Ontologies,
		  Semantic search, Smart cities},
  location	= {S\~{a}o Lu\'{\i}s, Brazil},
  series	= {WebMedia '20}
}

@InProceedings{	  10.1145/3607827.3616842,
  author	= {Rossetto, Federico and Dalton, Jeffrey and Murray-Smith,
		  Roderick},
  title		= {Generating Multimodal Augmentations with LLMs from Song
		  Metadata for Music Information Retrieval},
  year		= {2023},
  isbn		= {9798400702839},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3607827.3616842},
  doi		= {10.1145/3607827.3616842},
  abstract	= {In this work we propose a set of new automatic text
		  augmentations that leverage Large Language Models from song
		  metadata to improve on music information retrieval tasks.
		  Compared to recent works, our proposed methods leverage
		  large language models and copyright-free corpora from web
		  sources, enabling us to release the knowledge sources
		  collected. We show how combining these representations with
		  the audio signal provides a 21\% relative improvement on
		  five of six datasets on genre classification, emotion
		  recognition and music tagging, achieving state-of-the-art
		  in three (GTZAN, FMA-Small and Deezer). We demonstrate the
		  benefit of injecting external knowledge sources by
		  comparing them withintrinsic text representation methods
		  that rely only on the sample's information.},
  booktitle	= {Proceedings of the 1st Workshop on Large Generative Models
		  Meet Multimodal Applications},
  pages		= {51–59},
  numpages	= {9},
  keywords	= {large language models application, multimodal learning,
		  music information retrieval},
  location	= {Ottawa ON, Canada},
  series	= {LGM3A '23}
}

@InProceedings{	  10.1145/3711542.3711583,
  author	= {Tan, Tee Hean},
  title		= {Rule-Based vs. AI-Driven: Comparing PolyAQG Framework and
		  Generative AI Models},
  year		= {2025},
  isbn		= {9798400717383},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711542.3711583},
  doi		= {10.1145/3711542.3711583},
  abstract	= {This comparative analysis examines the PolyAQG framework
		  and Generative AI models (e.g., ChatGPT, Gemini) across ten
		  key criteria for question generation. The PolyAQG
		  framework, a rule-based approach, is well-suited for
		  structured content and excels in generating consistent
		  questions for educational purposes. However, it may be
		  limited in creativity and depth. Generative AI models,
		  while capable of covering broader topics and interpreting
		  complex contexts, require more computational resources and
		  may introduce inaccuracies in specialized domains. The
		  PolyAQG framework offers scalability within specific
		  domains and predictable error handling. Generative AI
		  models, although scalable across topics, may require
		  fine-tuning for accuracy. Furthermore, Generative AI
		  enables dynamic user interaction and fosters critical
		  thinking, while the PolyAQG framework provides a more
		  limited user interface. The choice between PolyAQG and
		  generative AI depends on application needs. PolyAQG is
		  ideal for structured questions and consistency, while
		  generative AI excels in creativity, adaptability, and user
		  interaction.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {298–303},
  numpages	= {6},
  keywords	= {Generative AI model, PolyAQG framework, contextual
		  understanding, domain-specific, questions generation,
		  rule-based, scalability},
  location	= { },
  series	= {NLPIR '24}
}

@InProceedings{	  10.1145/3437120.3437310,
  author	= {E. Samaridi, Nikoletta and N. Karanikolas, Nikitas and C.
		  Papakitsos, Evangelos},
  title		= {Lexicographic Environments in Natural Language Processing
		  (NLP)},
  year		= {2021},
  isbn		= {9781450388979},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3437120.3437310},
  doi		= {10.1145/3437120.3437310},
  abstract	= {In this paper, a literature review is presented in
		  reference to the most important lexicographical
		  environments that have been developed in the last decades,
		  with the aim of utilizing them in various knowledge
		  management applications, in the wider area of ​​the
		  Semantic Web (SW). This paper focuses on the semantic
		  networks of lexical information, in order to highlight
		  their value but also the urgent need to design and
		  implement an electronic conceptual dictionary with
		  ontological structure for the modern Greek language,
		  according to international dictionary standards.},
  booktitle	= {Proceedings of the 24th Pan-Hellenic Conference on
		  Informatics},
  pages		= {219–222},
  numpages	= {4},
  keywords	= {Computational Lexicography, conceptual dictionary, corpus,
		  ontology, semantic networks},
  location	= {Athens, Greece},
  series	= {PCI '20}
}

@Article{	  10.1145/3704729,
  author	= {Asprino, Luigi and Damiano, Rossana and Daquino, Marilena
		  and De Giorgis, Stefano and Gangemi, Aldo and Lieto,
		  Antonio and Sartini, Bruno and Striani, Manuel},
  title		= {An Ontology Network for Citizen Curation},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3704729},
  doi		= {10.1145/3704729},
  abstract	= {Citizen curation is gaining momentum as a new form of
		  engagement with cultural heritage. Citizen curatorial
		  activities require and produce a wealth of information,
		  ranging from descriptions of the artefacts to visitor
		  experience feedback. Although formalising and integrating
		  such various data is of paramount importance, the domain
		  lacks comprehensive ontologies to enable querying,
		  interpreting and reasoning over the collected data. Social
		  Participation, Cohesion and Inclusion through Cultural
		  Engagement (SPICE) is an EU project dedicated to
		  experimenting with citizen curation activities to foster
		  cultural engagement. SPICE develops technologies that help
		  communities to create and share their own interpretation of
		  cultural artefacts, hence developing a better understanding
		  of, and empathy for, themselves and other communities. Part
		  of the SPICE ecosystem of technologies is the SPICE
		  Ontology Network (SON), which empowers applications with
		  knowledge-level reasoning abilities and supports both
		  applications and users interacting with data involved in
		  citizen curation activities. This article provides an
		  overview of the SON and outlines its main use cases.},
  journal	= {J. Comput. Cult. Herit.},
  month		= dec,
  articleno	= {72},
  numpages	= {30},
  keywords	= {citizen curation, ontologies, cultural heritage, semantic
		  web}
}

@Article{	  10.1145/3397874,
  author	= {Moldovan, Alex and Nicula, Vlad and Pasca, Ionut and Popa,
		  Mihai and Namburu, Jaya Krishna and Oros, Anamaria and
		  Brie, Paul},
  title		= {OpenUIDL, A User Interface Description Language for
		  Runtime Omni-Channel User Interfaces},
  year		= {2020},
  issue_date	= {June 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {EICS},
  url		= {https://doi.org/10.1145/3397874},
  doi		= {10.1145/3397874},
  abstract	= {We extend the concept of cross-device user interfaces into
		  the new, more general, concept of omni-channel user
		  interfaces to better reflect the technological variety
		  offered for developing multi-target user interfaces for
		  interactive applications. We present a model-based approach
		  for developing runtime omni-channel user interfaces for
		  multi-target applications, which consists of: (1) OpenUIDL,
		  a user interface description language for describing
		  omni-channel user interfaces with its semantics by a
		  meta-model and its syntax based on JSON, (2) the definition
		  of a step-wise approach for producing runtime interactive
		  applications based onOpenUIDLwith integration into the
		  development life cycle, (3) the development of a
		  cloud-based, OpenUIDL compliant, Interactive Development
		  Environment that supports the application and the enactment
		  of the step-wise approach and its illustration on several
		  multi-target user interfaces.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jun,
  articleno	= {86},
  numpages	= {52},
  keywords	= {model-based user interface, multi-target user interfaces,
		  omni-channel user interfaces, open source, user interface
		  description language}
}

@InProceedings{	  10.5555/3374138.3374175,
  author	= {Wagner, Gerd},
  title		= {Towards a non-proprietary modeling language for processing
		  network simulation},
  year		= {2019},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {Processing networks have been investigated in the
		  mathematical theory of queueing and have been the
		  application focus of most industrial simulation software
		  products, starting with GPSS and SIMAN/Arena. They allow
		  modeling many forms of discrete processing processes, and
		  are mainly used in simulation projects for the
		  manufacturing and services industries. However, there is
		  still no proper vendor-neutral language definition for this
		  paradigm, e.g., in the form of a meta-model defining an
		  abstract syntax for specifying the structure and dynamics
		  of processing networks. We reconstruct the core of this
		  paradigm in the form of a UML-based meta-model and show how
		  to map a processing network specified with this metamodel
		  to an Object Event Simulation model providing its
		  operational semantics.},
  booktitle	= {Proceedings of the 2019 Summer Simulation Conference},
  articleno	= {37},
  numpages	= {12},
  keywords	= {GPSS, SIMAN, arena, processing networks, queuing
		  networks},
  location	= {Berlin, Germany},
  series	= {SummerSim '19}
}

@Article{	  10.1109/taslp.2018.2852492,
  author	= {Jang, Youngsoo and Ham, Jiyeon and Lee, Byung-Jun and Kim,
		  Kee-Eung},
  title		= {Cross-Language Neural Dialog State Tracker for Large
		  Ontologies Using Hierarchical Attention},
  year		= {2018},
  issue_date	= {November 2018},
  publisher	= {IEEE Press},
  volume	= {26},
  number	= {11},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2018.2852492},
  doi		= {10.1109/TASLP.2018.2852492},
  abstract	= {Dialog state tracking, which refers to identifying the
		  user intent from utterances, is one of the most important
		  tasks in dialog management. In this paper, we present our
		  dialog state tracker developed for the fifth dialog state
		  tracking challenge, which focused on cross-language
		  adaptation using a very scarce machine-translated training
		  data when compared to the size of the ontology. Our dialog
		  state tracker is based on the bi-directional long
		  short-term memory network with a hierarchical attention
		  mechanism in order to spot important words in user
		  utterances. The user intent is predicted by finding the
		  closest keyword in the ontology to the attention-weighted
		  word vector. With the suggested methodology, our tracker
		  can overcome various difficulties due to the scarce
		  training data that existing machine learning-based trackers
		  had, such as predicting user intents they have not seen
		  before. We show that our tracker outperforms other trackers
		  submitted to the challenge with respect to most of the
		  performance measures.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {2072–2082},
  numpages	= {11}
}

@InProceedings{	  10.1145/3605098.3636153,
  author	= {Ehrlinger, Lisa and Holzner, Harald and Hemelmayr, Nora
		  and W\"{o}\ss{}, Wolfram},
  title		= {A News Article Tag Categorization Ontology},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3636153},
  doi		= {10.1145/3605098.3636153},
  abstract	= {In recent years, the demand for personalized and
		  subject-specific news has been growing. Users and
		  organizations alike are requesting the ability to curate
		  individual timelines based on their topics of interest. To
		  enable the easy curation and discovery of such timelines, a
		  suitable news tag categorization is required that is both,
		  news-domain-oriented, and fine-grained enough to cover
		  specific (e.g., regional) use cases. Since existing
		  categorization systems do not sufficiently fulfill both
		  requirements, we developed an ontology that contains news
		  article tags based on media topics by the International
		  Press Telecommunications Council and Wikipedia categories.
		  The ontology has been implemented within the Newsadoo
		  platform, where it improves the topic curation and
		  exploration process.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1665–1667},
  numpages	= {3},
  keywords	= {news articles, tag categorization, ontologies, wikipedia
		  categories},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@Article{	  10.1145/3539732,
  author	= {Singh, Shashank Sheshar and Srivastava, Vishal and Kumar,
		  Ajay and Tiwari, Shailendra and Singh, Dilbag and Lee,
		  Heung-No},
  title		= {Social Network Analysis: A Survey on Measure, Structure,
		  Language Information Analysis, Privacy, and Applications},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3539732},
  doi		= {10.1145/3539732},
  abstract	= {The rapid growth in popularity of online social networks
		  provides new opportunities in computer science, sociology,
		  math, information studies, biology, business, and more.
		  Social network analysis (SNA) is a paramount technique
		  supporting understanding social relationships and networks.
		  Accordingly, certain studies and reviews have been
		  presented focusing on information dissemination, influence
		  analysis, link prediction, and more. However, the ultimate
		  aim is for social network background knowledge and analysis
		  to solve real-world social network problems. SNA still has
		  several research challenges in this context, including
		  users’ privacy in online social networks. Inspired by
		  these facts, we have presented a survey on social network
		  analysis techniques, visualization, structure, privacy, and
		  applications. This detailed study has started with the
		  basics of network representation, structure, and measures.
		  Our primary focus is on SNA applications with
		  state-of-the-art techniques. We further provide a
		  comparative analysis of recent developments on SNA problems
		  in the sequel. The privacy preservation with SNA is also
		  surveyed. In the end, research challenges and future
		  directions are discussed to suggest to researchers a
		  starting point for their research.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {137},
  numpages	= {47},
  keywords	= {Information diffusion, influence maximization, link
		  prediction, community detection, social network analysis}
}

@InProceedings{	  10.1109/ase56229.2023.00018,
  author	= {Morales, Sergio and Claris\'{o}, Robert and Cabot, Jordi},
  title		= {Automating Bias Testing of LLMs},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00018},
  doi		= {10.1109/ASE56229.2023.00018},
  abstract	= {Large Language Models (LLMs) are being quickly integrated
		  in a myriad of software applications. This may introduce a
		  number of biases, such as gender, age or ethnicity, in the
		  behavior of such applications. To face this challenge, we
		  explore the automatic generation of tests suites to assess
		  the potential biases of an LLM. Each test is defined as a
		  prompt used as input to the LLM and a test oracle that
		  analyses the LLM output to detect the presence of biases.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {1705–1707},
  numpages	= {3},
  keywords	= {testing, ethics, bias, fairness, large language models},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@Article{	  10.1613/jair.1.16401,
  author	= {Dimartino, Mirko M. and Wood, Peter T. and Cali, Andrea
		  and Poulovassilis, Alexandra},
  title		= {Efficient Ontology-Mediated Query Answering: Extending
		  DL-liteR and Linear ELH},
  year		= {2025},
  issue_date	= {Jun 2025},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {82},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.16401},
  doi		= {10.1613/jair.1.16401},
  abstract	= {The OWL 2 QL profile of the OWL 2 Web Ontology Language,
		  based on the family of description logics called DL-Lite,
		  is designed so that data stored in a standard relational
		  database system (RDBMS) can be queried through an ontology
		  via a rewriting mechanism, i.e., by rewriting the query
		  into an SQL query that is then answered by the RDBMS
		  system, without any changes to the data. In this paper we
		  propose a language whose expressive power goes beyond that
		  of DL-Lite while still allowing query answering via
		  rewriting of queries into unions of conjunctive two-way
		  regular path queries (UC2RPQs) instead of SQL queries. Our
		  language is an extension of both OWL 2 QL and linear ELH:
		  OWL 2 QL is extended by allowing qualified existential
		  quantification on the left-hand side of concept inclusion
		  axioms, and linear ELH by allowing inverses in role
		  inclusion axioms. We identify a syntactic property of the
		  extended language that guarantees UC2RPQ-rewritability. We
		  propose a novel rewriting technique for conjunctive queries
		  (CQs) under our ontology language that makes use of
		  nondeterministic finite state automata. We show that CQ
		  answering in our setting is NLOGSPACE-complete with respect
		  to data complexity and NP-complete for combined complexity;
		  we also show that answering instance queries is
		  NLOGSPACE-complete for data complexity and in PTIME for
		  combined complexity.},
  journal	= {J. Artif. Int. Res.},
  month		= apr,
  numpages	= {49}
}

@Article{	  10.1145/3564604,
  author	= {Bibi, Nazia and Rana, Tauseef and Maqbool, Ayesha and
		  Alkhalifah, Tamim and Khan, Wazir Zada and Bashir, Ali
		  Kashif and Zikria, Yousaf Bin},
  title		= {Reusable Component Retrieval: A Semantic Search Approach
		  for Low-Resource Languages},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3564604},
  doi		= {10.1145/3564604},
  abstract	= {A common practice among programmers is to reuse existing
		  code, accomplished by performing natural language queries
		  through search engines. The main aim of code retrieval is
		  to search for the most relevant snippet from a corpus of
		  code snippets. However, code retrieval frameworks for
		  low-resource languages are insufficient. Retrieving the
		  most relevant code snippet efficiently can be accomplished
		  only by eliminating the semantic gap between the code
		  snippets residing in the repository and the user’s query
		  (natural language description). The primary objective of
		  the research is to contribute to this field by providing a
		  code search framework that can be extended for low-resource
		  languages. The secondary objective is to provide a code
		  retrieval mechanism that is semantically relevant to the
		  user query and provide programmers with the ability to
		  locate source code that they want to use when developing
		  new applications. The proposed approach is implemented
		  using a web platform to search for source code. As code
		  retrieval is a sophisticated task, the proposed approach
		  incorporates a semantic search mechanism. This research
		  uses a semantic model for code retrieval, which generates
		  meanings or synonyms of words. The proposed model
		  integrates ontologies and Natural Language Processing.
		  System performance measures and classification accuracy are
		  computed using precision, recall, and F1-score. We also
		  compare the proposed approach with state-of-the-art
		  baseline models. The retrieved results are ranked, showing
		  that our approach significantly outperforms robust code
		  matching. Our evaluation shows that semantic matching leads
		  to improved source code retrieval. This study marks a
		  substantial advancement in integrating programming
		  expertise with code retrieval techniques. Moreover, our
		  system lets users know when and how it is used for
		  successful semantic searching.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {141},
  numpages	= {31},
  keywords	= {Ontologies, web semantics, source code retrieval, source
		  code search, information retrieval}
}

@InBook{	  10.1145/3672608.3707870,
  author	= {Adu-Duodu, Kwabena and Wilson, Stanly and Li, Yinhao and
		  Oladimeji, Aanuoluwapo and Huraysi, Talea and Barati,
		  Masoud and Perera, Charith and Solaiman, Ellis and Rana,
		  Omer and Ranjan, Rajiv and Shah, Tejal},
  title		= {A Circular Construction Product Ontology for End-of-Life
		  Decision-Making},
  year		= {2025},
  isbn		= {9798400706295},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672608.3707870},
  abstract	= {Efficient management of end-of-life (EoL) products is
		  critical for advancing circularity in supply chains,
		  particularly within the construction industry where EoL
		  strategies are hindered by heterogenous lifecycle data and
		  data silos. Current tools like Environmental Product
		  Declarations (EPDs) and Digital Product Passports (DPPs)
		  are limited by their dependency on seamless data
		  integration and interoperability which remain significant
		  challenges. To address these, we present the Circular
		  Construction Product Ontology (CCPO), an applied framework
		  designed to overcome semantic and data heterogeneity
		  challenges in EoL decision-making for construction
		  products. CCPO standardises vocabulary and facilitates data
		  integration across supply chain stakeholders enabling
		  lifecycle assessments (LCA) and robust decision-making. By
		  aggregating disparate data into a unified product
		  provenance, CCPO enables automated EoL recommendations
		  through customisable SWRL rules aligned with European
		  standards and stakeholder-specific circularity SLAs,
		  demonstrating its scalability and integration capabilities.
		  The adopted circular product scenario depicts CCPO's
		  application while competency question evaluations show its
		  superior performance in generating accurate EoL suggestions
		  highlighting its potential to greatly improve
		  decision-making in circular supply chains and its
		  applicability in real-world construction environments.},
  booktitle	= {Proceedings of the 40th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1943–1952},
  numpages	= {10}
}

@Article{	  10.1145/3611306,
  author	= {Vats, Preeti and Sharma, Nonita and Sharma, Deepak Kumar},
  title		= {HKG: A Novel Approach for Low Resource Indic Languages to
		  Automatic Knowledge Graph Construction},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3611306},
  doi		= {10.1145/3611306},
  abstract	= {Knowledge graph (KG), a visual representation of text data
		  as a semantic network, holds enormous promise for the
		  development of more intelligent robots. It leads to
		  significant potential solutions for many tasks like
		  question answering, recommendation, and information
		  retrieval. However, this area is confined to using English
		  text only. Since low-resource languages are now being used
		  in the world of AI, it is necessary to develop a semantic
		  network for them as well. In this research work, the
		  authors provide state-of-the-art techniques for automatic
		  knowledge graph construction for the Hindi language, which
		  is still unexplored in ontology. Constructing a knowledge
		  graph faces several hurdles and obstacles in the linguistic
		  domain, primarily when it deals with the Hindi language.
		  With an emphasis on the Indian perspective, this research
		  intends to introduce a novel approach ‘HKG’ for
		  knowledge graph construction framework for Hindi. It also
		  implements the LSTM model to evaluate the accuracy of newly
		  constructed knowledge graphs and compute different
		  evaluation metrics such as accuracy and F1-score. This
		  knowledge graph evaluates the accuracy of 87.50 using
		  Doc2Vec word embedding with a train-test split of 7:3.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  keywords	= {Knowledge graph Construction, Natural Language Processing,
		  Low Resource Indian Languages, Stanza, Link Analysis and
		  Neighbor Nodes, LSTM}
}

@InProceedings{	  10.1145/3638067.3638080,
  author	= {Costa, Simone Dornelas and Manso, Carolina De Freitas and
		  Marques, Leonardo Carneiro and Gadelha, Bruno Freitas and
		  Conte, Tayana Uch\^{o}a and Barcellos, Monalessa Perini},
  title		= {Using Networked Ontologies to support UX Evaluation in
		  Immersive Context},
  year		= {2024},
  isbn		= {9798400717154},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638067.3638080},
  doi		= {10.1145/3638067.3638080},
  abstract	= {Over the past few years, new interactive systems such as
		  immersive technologies have gradually permeated our daily
		  lives and found adoption across various fields. Immersive
		  technologies provide users with immersive experiences.
		  Assessing and modeling the quality of such experiences has
		  become a trending topic in HCI, and UX is a key quality
		  attribute in this context. When it comes to immersive
		  experiences, evaluating UX is particularly challenging
		  because the user should not be interrupted to provide
		  feedback. In this paper, we propose using networked
		  ontologies to support evaluating immersive experiences. We
		  have explored using ontologies from an ontology network
		  addressing the HCI domain to develop a tool that supports
		  UX experts evaluating such experiences based on data
		  recorded in interaction logs. We used the ontology-based
		  tool to evaluate the UX of an immersive application that
		  supports collaborative music composition. The tool
		  extracted data from the application interaction logs
		  applied UX metrics, and provided consolidated data and
		  information in graphs and tables. We conducted a study and
		  collected feedback from the tool developer and three UX
		  experts who used the tool. Results showed that using
		  networked ontologies to develop a tool to support UX
		  evaluation is feasible and valuable. In summary, the
		  ontologies helped at the conceptual level by offering a
		  basis to define the system’s structural model and at the
		  implementation level by assigning semantics to data to make
		  inferences about UX. Based on the UX experts’
		  perceptions, the tool was considered a promising system,
		  beneficial, helpful, and easy to use.},
  booktitle	= {Proceedings of the XXII Brazilian Symposium on Human
		  Factors in Computing Systems},
  articleno	= {69},
  numpages	= {12},
  keywords	= {Immersive Experience, Ontology, Ontology Network, UX
		  Evaluation, User Experience},
  location	= {Macei\'{o}, Brazil},
  series	= {IHC '23}
}

@InProceedings{	  10.1145/3652620.3687820,
  author	= {Moln\'{a}r, Vince and Graics, Bence and V\"{o}r\"{o}s,
		  Andr\'{a}s and Tonetta, Stefano and Cristoforetti, Luca and
		  Kimberly, Greg and Dyer, Pamela and Giammarco, Kristin and
		  Koethe, Manfred and Hester, John and Smith, Jamie and
		  Grimm, Christoph},
  title		= {Towards the Formal Verification of SysML v2 Models},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687820},
  doi		= {10.1145/3652620.3687820},
  abstract	= {Systems Modeling Language (SysML) is the de facto standard
		  in the industry for modeling complex systems. SysML v2 is
		  the new version of the language with reworked fundamentals.
		  In this paper, we explore how the new formal semantics of
		  SysML v2 can enable formal verification and various forms
		  of automated reasoning. Formal verification involves
		  mathematically proving the correctness of a system's design
		  with respect to certain specifications or properties. This
		  rigorous approach ensures that models behave as intended
		  under all possible conditions. Through a detailed
		  examination, we demonstrate how five specific tools -
		  Gamma, MP-Firebird, Imandra, SAVVS, and SysMD - can
		  formally analyze SysML v2 models. We show how these tools
		  support the different concepts in the language, as well as
		  the set of features and technologies they provide to users
		  of SysML v2, such as model checking, theorem proving,
		  contract-based design, or automatic fault injections. We
		  propose a workflow for applying formal methods on SysML v2
		  models, illustrated by example models and artifacts
		  generated by the above tools.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {1086–1095},
  numpages	= {10},
  keywords	= {SysML V2, systems modeling, formal methods, verification
		  and validation, automated reasoning, tools},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3627673.3679582,
  author	= {Zhu, Yinghao and Ren, Changyu and Wang, Zixiang and Zheng,
		  Xiaochen and Xie, Shiyun and Feng, Junlan and Zhu, Xi and
		  Li, Zhoujun and Ma, Liantao and Pan, Chengwei},
  title		= {EMERGE: Enhancing Multimodal Electronic Health Records
		  Predictive Modeling with Retrieval-Augmented Generation},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679582},
  doi		= {10.1145/3627673.3679582},
  abstract	= {The integration of multimodal Electronic Health Records
		  (EHR) data has significantly advanced clinical predictive
		  capabilities. Existing models, which utilize clinical notes
		  and multivariate time-series EHR data, often fall short of
		  incorporating the necessary medical context for accurate
		  clinical tasks, while previous approaches with knowledge
		  graphs (KGs) primarily focus on structured knowledge
		  extraction. In response, we propose EMERGE, a
		  Retrieval-Augmented Generation (RAG) driven framework to
		  enhance multimodal EHR predictive modeling. We extract
		  entities from both time-series data and clinical notes by
		  prompting Large Language Models (LLMs) and align them with
		  professional PrimeKG, ensuring consistency. In addition to
		  triplet relationships, we incorporate entities' definitions
		  and descriptions for richer semantics. The extracted
		  knowledge is then used to generate task-relevant summaries
		  of patients' health statuses. Finally, we fuse the summary
		  with other modalities using an adaptive multimodal fusion
		  network with cross-attention. Extensive experiments on the
		  MIMIC-III and MIMIC-IV datasets' in-hospital mortality and
		  30-day readmission tasks demonstrate the superior
		  performance of the EMERGE framework over baseline models.
		  Comprehensive ablation studies and analysis highlight the
		  efficacy of each designed module and robustness to data
		  sparsity. EMERGE contributes to refining the utilization of
		  multimodal EHR data in healthcare, bridging the gap with
		  nuanced medical contexts essential for informed clinical
		  predictions. We have publicly released the code at
		  https://github.com/yhzhu99/EMERGE.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3549–3559},
  numpages	= {11},
  keywords	= {electronic health record, large language model, multimodal
		  learning, retrieval-augmented generation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3341105.3373977,
  author	= {Chang, Doo Soo and Cho, Gun Hee and Choi, Yong Suk},
  title		= {Ontology-based knowledge model for human-robot interactive
		  services},
  year		= {2020},
  isbn		= {9781450368667},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341105.3373977},
  doi		= {10.1145/3341105.3373977},
  abstract	= {Recently, increasing interest in service robots with
		  artificial intelligence has triggered several studies on
		  developing service robots as human assistants. To perform
		  automated tasks, service robots are not only required to
		  recognize various attributes of the service environment,
		  but they must also determine, which tasks and behaviors to
		  perform, according to their internal system specifications
		  and those of the individual situation. To perform tasks in
		  such a generalized manner efficiently, the service robot
		  must abstractly understand the recognition data obtained
		  from the external environment and plan its tasks by
		  combining the data and existing knowledge. Thus, an
		  abstract and integrated knowledge management of low-level
		  external recognition data and symbolic-level knowledge is
		  indispensable. This study proposes a knowledge model for an
		  integrated robot framework, which is used to provide
		  human-robot interactive services. Our knowledge model is
		  based on an ontology that contains general knowledge which
		  provides clear definitions of common concepts and domain
		  knowledge which provides specific concepts required to
		  understand the information about agents(users and robots),
		  and the environments about human-robot interactive
		  services. An exemplary experiment is given in the context
		  of a social robot service, which shows the usability of our
		  knowledge model.},
  booktitle	= {Proceedings of the 35th Annual ACM Symposium on Applied
		  Computing},
  pages		= {2029–2038},
  numpages	= {10},
  keywords	= {human-robot interaction, knowledge framework,
		  ontology-based knowledge processing, social robot service},
  location	= {Brno, Czech Republic},
  series	= {SAC '20}
}

@Article{	  10.1145/3444689,
  author	= {Zhao, Liping and Alhoshan, Waad and Ferrari, Alessio and
		  Letsholo, Keletso J. and Ajagbe, Muideen A. and Chioasca,
		  Erol-Valeriu and Batista-Navarro, Riza T.},
  title		= {Natural Language Processing for Requirements Engineering:
		  A Systematic Mapping Study},
  year		= {2021},
  issue_date	= {April 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3444689},
  doi		= {10.1145/3444689},
  abstract	= {Natural Language Processing for Requirements Engineering
		  (NLP4RE) is an area of research and development that seeks
		  to apply natural language processing (NLP) techniques,
		  tools, and resources to the requirements engineering (RE)
		  process, to support human analysts to carry out various
		  linguistic analysis tasks on textual requirements
		  documents, such as detecting language issues, identifying
		  key domain concepts, and establishing requirements
		  traceability links. This article reports on a mapping study
		  that surveys the landscape of NLP4RE research to provide a
		  holistic understanding of the field. Following the guidance
		  of systematic review, the mapping study is directed by five
		  research questions, cutting across five aspects of NLP4RE
		  research, concerning the state of the literature, the state
		  of empirical research, the research focus, the state of
		  tool development, and the usage of NLP technologies. Our
		  main results are as follows: (i) we identify a total of 404
		  primary studies relevant to NLP4RE, which were published
		  over the past 36 years and from 170 different venues; (ii)
		  most of these studies (67.08\%) are solution proposals,
		  assessed by a laboratory experiment or an example
		  application, while only a small percentage (7\%) are
		  assessed in industrial settings; (iii) a large proportion
		  of the studies (42.70\%) focus on the requirements analysis
		  phase, with quality defect detection as their central task
		  and requirements specification as their commonly processed
		  document type; (iv) 130 NLP4RE tools (i.e., RE specific NLP
		  tools) are extracted from these studies, but only 17 of
		  them (13.08\%) are available for download; (v) 231
		  different NLP technologies are also identified, comprising
		  140 NLP techniques, 66 NLP tools, and 25 NLP resources, but
		  most of them—particularly those novel NLP techniques and
		  specialized tools—are used infrequently; by contrast,
		  commonly used NLP technologies are traditional analysis
		  techniques (e.g., POS tagging and tokenization),
		  general-purpose tools (e.g., Stanford CoreNLP and GATE) and
		  generic language lexicons (WordNet and British National
		  Corpus). The mapping study not only provides a collection
		  of the literature in NLP4RE but also, more importantly,
		  establishes a structure to frame the existing
		  literature&nbsp;through categorization, synthesis and
		  conceptualization of the main theoretical concepts and
		  relationships that encompass&nbsp;both RE and NLP aspects.
		  Our work thus produces a conceptual framework of NLP4RE.
		  The framework is used to identify research gaps and
		  directions, highlight technology transfer needs, and
		  encourage more synergies between the RE community, the NLP
		  one, and the software&nbsp;and systems&nbsp;practitioners.
		  Our results can be used as a starting point to frame future
		  studies according to a well-defined terminology and can be
		  expanded as new technologies and novel solutions emerge.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {55},
  numpages	= {41},
  keywords	= {Requirements engineering (RE), natural language processing
		  (NLP), software engineering (SE), systematic mapping study,
		  systematic review}
}

@InProceedings{	  10.1145/3460210.3493540,
  author	= {Pernisch, Romana and Dell'Aglio, Daniele and Bernstein,
		  Abraham},
  title		= {Toward Measuring the Resemblance of Embedding Models for
		  Evolving Ontologies},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493540},
  doi		= {10.1145/3460210.3493540},
  abstract	= {Updates on ontologies affect the operations built on top
		  of them. But not all changes are equal: some updates
		  drastically change the result of operations; others lead to
		  minor variations, if any. Hence, estimating the impact of a
		  change ex-ante is highly important, as it might make
		  ontology engineers aware of the consequences of their
		  action during editing. However, in order to estimate the
		  impact of changes, we need to understand how to measure
		  them. To address this gap for embeddings, we propose a new
		  measure called Embedding Resemblance Indicator (ERI), which
		  takes into account both the stochasticity of learning
		  embeddings as well as the shortcomings of established
		  comparison methods. We base ERI on (i) a similarity score,
		  (ii) a robustness factor $hatμ $ (based on the embedding
		  method, similarity measure, and dataset), and (iii) the
		  number of added or deleted entities to the embedding
		  computed with the Jaccard index.To evaluate ERI, we
		  investigate its usage in the context of two biomedical
		  ontologies and three embedding methods---GraRep, LINE, and
		  DeepWalk---as well as the two standard benchmark
		  datasets---FB15k-237 and Wordnet-18-RR---with TransE and
		  RESCAL embeddings. To study different aspects of ERI, we
		  introduce synthetic changes in the knowledge graphs,
		  generating two test-cases with five versions each and
		  compare their impact with the expected behaviour. Our
		  studies suggests that ERI behaves as expected and captures
		  the similarity of embeddings based on the severity of
		  changes. ERI is crucial for enabling further studies into
		  impact of changes on embeddings.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {177–184},
  numpages	= {8},
  keywords	= {embedding similarity, knowledge graph embeddings, ontology
		  evolution},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@InProceedings{	  10.1145/3706598.3713932,
  author	= {Kretzer, Felix and Kolthoff, Kristian and Bartelt,
		  Christian and Ponzetto, Simone Paolo and Maedche,
		  Alexander},
  title		= {Closing the Loop between User Stories and GUI Prototypes:
		  An LLM-Based Assistant for Cross-Functional Integration in
		  Software Development},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713932},
  doi		= {10.1145/3706598.3713932},
  abstract	= {Graphical user interfaces (GUIs) are at the heart of
		  almost every software we encounter. GUIs are often created
		  through a collaborative effort involving UX designers,
		  product owners, and software developers, constantly facing
		  changing requirements. Historically, problems in GUI
		  development include a fragmented, poorly integrated tool
		  landscape and high synchronization efforts between
		  stakeholders. Recent approaches suggest using large
		  language models (LLMs) to recognize requirements
		  fulfillment in GUIs and automatically propose new GUI
		  components. Based on ten interviews with practitioners,
		  this paper proposes an LLM-based assistant as a Figma
		  plug-in that bridges the gap between user stories and GUI
		  prototyping. We evaluated the prototype with 40 users and
		  40 crowd-workers, showing that the effectiveness of GUI
		  creation is improved by using LLMs to detect
		  requirements’ completion and generate new GUI components.
		  We derive design rationales to support cross-functional
		  integration in software development, ensuring that our
		  plug-in integrates well into established processes.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {879},
  numpages	= {19},
  keywords	= {GUI Prototypes; User Stories; Requirements; Assistance},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3686169.3686200,
  author	= {Dunbar, Michael and Speed, Chris},
  title		= {Data Sets for Regenerative Futures: Cultivating Relational
		  Ontologies},
  year		= {2024},
  isbn		= {9798400710421},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3686169.3686200},
  doi		= {10.1145/3686169.3686200},
  abstract	= {This short paper examines the limitations of current urban
		  data infrastructures in representing more-than-human
		  perspectives and explores pathways toward more inclusive,
		  regenerative data practices. It argues that prevalent data
		  sets and knowledge repositories reflect anthropocentric
		  ontologies, perpetuating the erasure of non-human subjects
		  and diverse epistemologies. The paper identifies key
		  challenges in developing more inclusive data
		  infrastructures, including ontological incommensurability,
		  the risk of cognitive injustice, and the need for expanded
		  representational repertoires. Drawing on examples from
		  Indigenous knowledge systems, multispecies ethnographies,
		  and arts-based practices, the authors propose strategies
		  for enriching urban datasets. These include centering
		  marginalized perspectives, expanding participatory data
		  governance models, and reimagining urban infrastructure as
		  multi-species sensing apparatuses. The paper concludes by
		  calling for the cultivation of relational data ontologies
		  that can better capture the complex interdependencies
		  between human and more-than-human worlds, essential for
		  envisioning and realizing regenerative urban futures.},
  booktitle	= {Proceedings of the Halfway to the Future Symposium},
  articleno	= {39},
  numpages	= {4},
  keywords	= {Data sets, More-than-human, Regenerative Futures},
  location	= {Santa Cruz, CA, USA},
  series	= {HttF '24}
}

@InProceedings{	  10.1145/3387940.3392162,
  author	= {Schlutter, Aaron and Vogelsang, Andreas},
  title		= {Knowledge Extraction from Natural Language Requirements
		  into a Semantic Relation Graph},
  year		= {2020},
  isbn		= {9781450379632},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3387940.3392162},
  doi		= {10.1145/3387940.3392162},
  abstract	= {Knowledge extraction and representation aims to identify
		  information and to transform it into a machine-readable
		  format. Knowledge representations support Information
		  Retrieval tasks such as searching for single statements,
		  documents, or metadata. Requirements specifications of
		  complex systems such as automotive software systems are
		  usually divided into different subsystem specifications.
		  Nevertheless, there are semantic relations between
		  individual documents of the separated subsystems, which
		  have to be considered in further processes (e.g.
		  dependencies). If requirements engineers or other
		  developers are not aware of these relations, this can lead
		  to inconsistencies or malfunctions of the overall system.
		  Therefore, there is a strong need for tool support in order
		  to detects semantic relations in a set of large natural
		  language requirements specifications. In this work we
		  present a knowledge extraction approach based on an
		  explicit knowledge representation of the content of natural
		  language requirements as a semantic relation graph. Our
		  approach is fully automated and includes an NLP pipeline to
		  transform unrestricted natural language requirements into a
		  graph. We split the natural language into different parts
		  and relate them to each other based on their semantic
		  relation. In addition to semantic relations, other
		  relationships can also be included in the graph. We
		  envision to use a semantic search algorithm like spreading
		  activation to allow users to search different semantic
		  relations in the graph.},
  booktitle	= {Proceedings of the IEEE/ACM 42nd International Conference
		  on Software Engineering Workshops},
  pages		= {373–379},
  numpages	= {7},
  keywords	= {knowledge extraction, natural language processing,
		  requirement engineering, semantic relation graph, spreading
		  activation},
  location	= {Seoul, Republic of Korea},
  series	= {ICSEW'20}
}

@InProceedings{	  10.1145/3637528.3671576,
  author	= {Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang,
		  Lun and Zhang, Shuyang and Chen, Ting},
  title		= {RareBench: Can LLMs Serve as Rare Diseases Specialists?},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671576},
  doi		= {10.1145/3637528.3671576},
  abstract	= {Generalist Large Language Models (LLMs), such as GPT-4,
		  have shown considerable promise in various domains,
		  including medical diagnosis. Rare diseases, affecting
		  approximately 300 million people worldwide, often have
		  unsatisfactory clinical diagnosis rates primarily due to a
		  lack of experienced physicians and the complexity of
		  differentiating among many rare diseases. In this context,
		  recent news such as "ChatGPT correctly diagnosed a
		  4-year-old's rare disease after 17 doctors failed"
		  underscore LLMs' potential, yet underexplored, role in
		  clinically diagnosing rare diseases. To bridge this
		  research gap, we introduce RareBench, a pioneering
		  benchmark designed to systematically evaluate the
		  capabilities of LLMs on 4 critical dimensions within the
		  realm of rare diseases. Meanwhile, we have compiled the
		  largest open-source dataset on rare disease patients,
		  establishing a benchmark for future studies in this domain.
		  To facilitate differential diagnosis of rare diseases, we
		  develop a dynamic few-shot prompt methodology, leveraging a
		  comprehensive rare disease knowledge graph synthesized from
		  multiple knowledge bases, significantly enhancing LLMs'
		  diagnostic performance. Moreover, we present an exhaustive
		  comparative study of GPT-4's diagnostic capabilities
		  against those of specialist physicians. Our experimental
		  findings underscore the promising potential of integrating
		  LLMs into the clinical diagnostic process for rare
		  diseases. This paves the way for exciting possibilities in
		  future advancements in this field.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4850–4861},
  numpages	= {12},
  keywords	= {benchmark for llms, evaluation, rare disease diagnosis},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3717413.3717417,
  author	= {Steinert, Alexandro and Ferenz, Stephan and Niesse,
		  Astrid},
  title		= {Choosing the Right Ontology to Describe Research Data in
		  the Energy Domain},
  year		= {2025},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {4},
  url		= {https://doi.org/10.1145/3717413.3717417},
  doi		= {10.1145/3717413.3717417},
  abstract	= {As in all disciplines, increasing the FAIRness
		  (findability, accessibility, interoperability, and
		  reusability) of research data and software is a goal in
		  energy research. In order to achieve this, it is important
		  to identify the most appropriate ontology for the
		  description of research data and software. However, despite
		  the importance of this task, it still presents a
		  significant challenge. While there are some comparisons of
		  ontologies, a gap exists in assessing their usefulness
		  according to ontology metadata. This paper fills this gap
		  by defining 21 criteria sorted into four categories to help
		  researchers choose ontologies in the energy domain. The
		  criteria are used to compare eight ontologies for energy
		  research to showcase their use and analyze the ontologies.
		  The analysis reveals the Open Energy Ontology (OEO) as the
		  top-ranked ontology. This underscores the importance of
		  metadata comparison in ontology selection and highlights
		  the benefits of incorporating metadata criteria into
		  ontology terminology services to support researchers.},
  journal	= {SIGENERGY Energy Inform. Rev.},
  month		= feb,
  pages		= {33–48},
  numpages	= {16},
  keywords	= {FAIR principles, energy research, energy research data
		  management, ontology, semantic web}
}

@Article{	  10.1145/3533428,
  author	= {Mundotiya, Rajesh and Kumar, Shantanu and Kumar, Ajeet and
		  Chaudhary, Umesh and Chauhan, Supriya and Mishra, Swasti
		  and Gatla, Praveen and Singh, Anil Kumar},
  title		= {Development of a Dataset and a Deep Learning Baseline
		  Named Entity Recognizer for Three Low Resource Languages:
		  Bhojpuri, Maithili, and Magahi},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3533428},
  doi		= {10.1145/3533428},
  abstract	= {In Natural Language Processing (NLP) pipelines, Named
		  Entity Recognition (NER) is one of the preliminary
		  problems, which marks proper nouns and other named entities
		  such as Location, Person, Organization, Disease and so on.
		  Such entities, without an NER module, adversely affect the
		  performance of a machine translation system. NER helps in
		  overcoming this problem by recognizing and handling such
		  entities separately, although it can be useful in
		  Information Extraction systems also. Bhojpuri, Maithili,
		  and Magahi are low resource languages, usually known as
		  Purvanchal languages. This article focuses on the
		  development of an NER benchmark dataset for Machine
		  Translation systems developed to translate from these
		  languages to Hindi by annotating parts of the available
		  corpora with named entities. Bhojpuri, Maithili, and Magahi
		  corpora of sizes 228,373, 157,468, and 56,190 tokens,
		  respectively, were annotated using 22 entity labels. The
		  annotation considers coarse-grained annotation labels
		  followed by the tagset used in one of the Hindi NER
		  datasets. We also report a Deep Learning baseline that uses
		  an LSTM-CNNs-CRF model. The lower baseline F1-scores from
		  the NER tool obtained by using Conditional Random Fields
		  models are 70.56\% for Bhojpuri, 73.19\% for Maithili, and
		  84.18\% for Magahi. The Deep Learning-based technique
		  (LSTM-CNNs-CRF) achieved 61.41\% for Bhojpuri, 71.38\% for
		  Maithili, and 86.39\% for Magahi. As the results show,
		  LSTM-CNNs-CRF fails to outperform the lower baseline in the
		  case of Bhojpuri and Maithili, which have more data in
		  terms of the number of tokens, but not in terms of the
		  number of named entities. However, the cross-lingual model
		  training of LSTM-CNNs-CRF for Bhojpuri and Maithili
		  performed better than the CRF.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {29},
  numpages	= {20},
  keywords	= {Indo-Aryan languages, low resource languages, Purvanchal
		  languages, Bhojpuri, Maithili, Magahi, named entity
		  recognition, Conditional Random Fields, Deep Learning}
}

@InProceedings{	  10.1145/3549737.3549765,
  author	= {Masa, Panagiota and Meditskos, Georgios and Kintzios,
		  Spyridon and Vrochidis, Stefanos and Kompatsiaris,
		  Ioannis},
  title		= {Ontology-based Modelling and Reasoning for Forest Fire
		  Emergencies in Resilient Societies},
  year		= {2022},
  isbn		= {9781450395977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3549737.3549765},
  doi		= {10.1145/3549737.3549765},
  abstract	= {Every year, thousands of forest fires throughout the world
		  cause disasters. One of the most critical challenges during
		  a wildfire disaster is the effective management of
		  heterogeneous information relative to the crisis to support
		  human operators and authorities. Towards addressing this
		  challenge, this paper presents an ontology-based framework
		  for data representation and interlinking of wildfire events
		  that are being used to foster advanced reasoning,
		  situational awareness and interpretation for decision
		  support. More specifically, we illustrate the capabilities
		  of the ONTO-SAFE ontology to symbolically model contextual
		  information in the domain, addressing application and user
		  requirements promoting the creation of interoperable
		  knowledge graphs. On top of the symbolic knowledge graphs,
		  we define a rule-based framework for infusing expert
		  knowledge in the form of constraints and rules to recognize
		  patterns and situations of interest based on domain
		  knowledge, assisting end-users in taking informed decisions
		  and facilitating advanced decision-making.},
  booktitle	= {Proceedings of the 12th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {24},
  numpages	= {9},
  location	= {Corfu, Greece},
  series	= {SETN '22}
}

@InProceedings{	  10.1145/3411408.3411410,
  author	= {Papantoniou, Katerina and Tzitzikas, Yannis},
  title		= {NLP for the Greek Language: A Brief Survey},
  year		= {2020},
  isbn		= {9781450388788},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411408.3411410},
  doi		= {10.1145/3411408.3411410},
  abstract	= {There is a plethora of methods, tools and resources for
		  processing text in the English language, however this is
		  not the case for other languages, like Greek. Due to the
		  increasing interest in NLP, and since there is a noteworthy
		  number of works related to the processing of the Greek
		  language, in this paper we survey the work related to the
		  processing of Greek language. In particular, we list and
		  briefly discuss related works, resources and tools,
		  categorized according to various processing layers and
		  contexts. This survey can be useful for researchers and
		  students interested in NLP tasks, Information Retrieval and
		  Knowledge Management for the Greek language.},
  booktitle	= {11th Hellenic Conference on Artificial Intelligence},
  pages		= {101–109},
  numpages	= {9},
  keywords	= {Greek language, Survey, natural language processing},
  location	= {Athens, Greece},
  series	= {SETN 2020}
}

@InProceedings{	  10.1145/3487664.3487700,
  author	= {de Almeida, Melonie and Samarawickrama, Chamodi and de
		  Silva, Nisansa and Ratnayaka, Gathika and Perera, Shehan},
  title		= {Identifying Legal Party Members from Legal Opinion
		  Documents using Natural Language Processing},
  year		= {2022},
  isbn		= {9781450395564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487664.3487700},
  doi		= {10.1145/3487664.3487700},
  abstract	= {Law and order is a field that can highly benefit from the
		  contribution of Natural Language Processing (NLP) to its
		  betterment. An area in which NLP can be of immense help is,
		  information retrieval from legal documents which function
		  as legal databases. The extraction of legal parties from
		  the aforementioned legal documents can be identified as a
		  task of high importance since it has a significant impact
		  on the proceedings of contemporary legal cases. This study
		  proposes a novel deep learning methodology which can be
		  effectively used to find a solution to the problem of
		  identifying legal party members in legal documents. In
		  addition to that, in this paper, we introduce a novel data
		  set which is annotated with legal party information by an
		  expert in the legal domain. The deep learning model
		  proposed in this study provides a benchmark for the legal
		  party identification task on this data set. Evaluations for
		  the solution presented in the paper show that our system
		  has 90.89\% precision and 91.69\% recall for an unseen
		  paragraph from a legal document, thus conforming the
		  success of our attempt.},
  booktitle	= {The 23rd International Conference on Information
		  Integration and Web Intelligence},
  pages		= {259–266},
  numpages	= {8},
  keywords	= {Legal party identification, NER, Recurrent Neural
		  Networks, coreference resolution},
  location	= {Linz, Austria},
  series	= {iiWAS2021}
}

@InProceedings{	  10.1145/3589334.3645317,
  author	= {Zhao, Yizheng},
  title		= {Efficient Computation of Signature-Restricted Views for
		  Semantic Web Ontologies},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645317},
  doi		= {10.1145/3589334.3645317},
  abstract	= {Uniform Interpolation (UI) is an advanced reasoning
		  service used to narrow down an ontology to a restricted
		  view. This new ontology, known as a uniform interpolant,
		  will only consist of the ''relevant names'', yet it will
		  retain their original meanings. UI is immensely promising
		  due to its applicability across various domains where
		  custom views of ontologies are essential. Nonetheless, to
		  unlock its full potential, we need optimized techniques to
		  generate these tailored views. Previous studies suggest
		  that creating uniform interpolants for EL-ontologies is
		  notably challenging. In some instances, it is not even
		  feasible to compute a uniform interpolant; when feasible,
		  the size of the uniform interpolant can be up to triple
		  exponentially larger than the source ontology. Despite
		  these challenges, our paper introduces an improved
		  ''forgetting'' technique specifically designed for
		  computing uniform interpolants of ELI-ontologies. We
		  demonstrate that, with good normalization and inference
		  strategies, such uniform interpolants can be efficiently
		  computed, just as quickly as computing ''modules''. A
		  comprehensive evaluation with a prototypical implementation
		  of the method shows superb success rates over two popular
		  benchmark datasets, demonstrating a clear computational
		  advantage over state-of-the-art approaches.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {1945–1953},
  numpages	= {9},
  keywords	= {forgetting, module extraction, ontology, uniform
		  interpolation},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3266237.3266244,
  author	= {da Silva, Jo\~{a}o Pablo S. and Ecar, Miguel and Pimenta,
		  Marcelo S. and Guedes, Gilleanes T. A. and Rodrigues, Elder
		  M.},
  title		= {Towards a domain-specific modeling language for
		  self-adaptive systems conceptual modeling},
  year		= {2018},
  isbn		= {9781450365031},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3266237.3266244},
  doi		= {10.1145/3266237.3266244},
  abstract	= {Self-adaptive Systems (SaSs) are able to adapt their
		  behavior at runtime in response to contextual changes. In
		  this work, we are interested in SaSs conceptual modeling,
		  which is the act of creating models that describe aspects
		  of the world. SaSs modeling is a non-trivial activity
		  because it deals with requirements uncertainty, contextual
		  changes, and behavior adaptation. This complexity can be
		  minimized by using Domain-Specific Modeling Languages
		  (DSMLs), which may be created by extending Unified Modeling
		  Language (UML). In this paper, we propose a UML profile
		  that represents the higher-level abstractions required to
		  provide support for SaSs conceptual modeling. We developed
		  the UML profile by modeling the domain of interest and
		  extending the UML class metaclass. The UML profile was
		  evaluated through the focus group technique, which was
		  performed by software engineering professors. As the
		  outcome, the focus group participants considered the UML
		  profile able to produce SaSs conceptual models with more
		  expressiveness than UML standard.},
  booktitle	= {Proceedings of the XXXII Brazilian Symposium on Software
		  Engineering},
  pages		= {208–213},
  numpages	= {6},
  keywords	= {UML profiling, conceptual modeling, self-adaptive system},
  location	= {Sao Carlos, Brazil},
  series	= {SBES '18}
}

@InProceedings{	  10.1145/3688671.3688737,
  author	= {Iacovou, Marios and Kontogiannis, Stelios and Avgerinakis,
		  Konstantinos},
  title		= {Ontology Data Insights and Alerts for Wildfire
		  Protection},
  year		= {2024},
  isbn		= {9798400709821},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3688671.3688737},
  doi		= {10.1145/3688671.3688737},
  abstract	= {This paper presents an improved approach to wildfire
		  protection by integrating heterogeneous data sources under
		  a novel fire detection ontology. The introduced technique
		  automatically unifies data from IoT devices into a single
		  Semantic Knowledge Graph (SemKG), improving user experience
		  by removing the need for users to examine several data
		  sources. To make easier quick and well-informed
		  decision-making, the system generates real-time
		  notifications with different degrees of severity. The
		  validation of the introduced technique has already taken
		  place in various pilot sites of the SILVANUS H2020 project,
		  which focus on developing a climate-resilient forest
		  management platform to prevent and suppress wildfires,
		  involving 49 partners from the European Union, Brazil,
		  Indonesia, and Australia. In this paper, we demonstrate the
		  effectiveness of this approach through scenarios of active
		  fire detection and gas leakage.},
  booktitle	= {Proceedings of the 13th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {41},
  numpages	= {6},
  keywords	= {Fire Detection, Health Impact Monitoring, IoT, Ontology,
		  SILVANUS, Semantic Knowledge Graph, Wildfire Protection},
  location	= { },
  series	= {SETN '24}
}

@InProceedings{	  10.1145/3689492.3689812,
  author	= {Aish, Robert and Fisher, Al and Orchard, Dominic and
		  Torry, Jay},
  title		= {Programming Languages for the Future of Design
		  Computation},
  year		= {2024},
  isbn		= {9798400712159},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689492.3689812},
  doi		= {10.1145/3689492.3689812},
  abstract	= {Design Computation is the use of programming in the design
		  of physical systems such as buildings and infrastructure.
		  This involves embedding both general-purpose textual
		  languages and domain-specific visual languages within
		  geometry modelling and engineering applications in the
		  construction industry. A unique form of entry-level
		  end-user programming has emerged in Design Computation.
		  However, there are significant usability and
		  representational issues; general-purpose languages present
		  barriers to adoption, whilst visual languages do not scale
		  to complex design problems. In this essay, we explore how
		  advances in programming language research could be
		  harnessed in future Design Computation languages to address
		  these pedagogic, representational and scaling issues so as
		  to improve human readable program structure and semantics
		  and to enable machine-readable program verification.},
  booktitle	= {Proceedings of the 2024 ACM SIGPLAN International
		  Symposium on New Ideas, New Paradigms, and Reflections on
		  Programming and Software},
  pages		= {241–265},
  numpages	= {25},
  keywords	= {Cognitive Dimensions, Collaborative Coding, Collection
		  Types, Design Computation, End-User Programming, Program
		  Verification, Programming Languages, Type Systems, Units of
		  Measure, Usability, Visual Languages},
  location	= {Pasadena, CA, USA},
  series	= {Onward! '24}
}

@InProceedings{	  10.1145/3270112.3270124,
  author	= {Gilson, Fabian},
  title		= {Teaching software language engineering and usability
		  through students peer reviews},
  year		= {2018},
  isbn		= {9781450359658},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3270112.3270124},
  doi		= {10.1145/3270112.3270124},
  abstract	= {Modelling techniques have been part of the software
		  engineering practice for decades. Despite Model-Driven
		  Software Engineering (Mde) being spread in the industry,
		  effective teaching through meaningful use cases remains a
		  challenge. Some guidelines and teaching experiences have
		  been reported that either focus on the content of an Mde
		  course, or on tooling environments designed for students.
		  Furthermore, little emphasis has been put on the usability
		  in the context of Software Language Engineering (Sle),
		  which may lead to technically valid languages, but which
		  are difficult to use. In this paper, we report on our
		  attempt to combine a practical use case for Sle and a
		  structured peer review focusing on the language usability
		  and its tooling environment. By putting students in both
		  the seats of language designers and users, we believe to
		  empower students with suitable technical Sle skills as well
		  as making them aware that a good software language requires
		  more than theoretical validity to be adopted.},
  booktitle	= {Proceedings of the 21st ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  pages		= {98–105},
  numpages	= {8},
  keywords	= {domain specific language, language usability, model-driven
		  software engineering, software language engineering},
  location	= {Copenhagen, Denmark},
  series	= {MODELS '18}
}

@Article{	  10.1145/3586078,
  author	= {Antonini, Alessio and Adamou, Alessandro and
		  Su\'{a}rez-Figueroa, Mari Carmen and Benatti, Francesca},
  title		= {Experiential Observations: An Ontology Pattern-Based Study
		  on Capturing the Potential Content within Evidences of
		  Experiences},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3586078},
  doi		= {10.1145/3586078},
  abstract	= {Modelling the knowledge behind human experiences is a
		  complex process: it should take into account, among others,
		  the activities performed, human observations and the
		  documentation of the evidence. To represent this knowledge
		  in a declarative way means to support data interoperability
		  in the context of cultural heritage artefacts, as linked
		  datasets on experience documentation have started to
		  appear. With this objective in mind, we describe a study
		  based on an ontology design pattern for modelling
		  experiences through observations, which are considered
		  indirect evidence of a mental process (i.e., the
		  experience). This pattern highlights the structural
		  differences between types of experiential documentation,
		  such as diaries and social media, providing a guideline for
		  the comparability between different domains and for
		  supporting the construction of heterogeneous datasets based
		  on an epistemic compatibility. We have performed not only a
		  formal evaluation over the pattern but also an assessment
		  through a series of case studies. This approach includes
		  (a) the analysis of interoperability among two case studies
		  (reading through social media and historical sources); (b)
		  the development of an ontology for collecting evidences of
		  reading, which reuses the proposed pattern; and (c) the
		  inspection of experience in humanities datasets.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {58},
  numpages	= {30},
  keywords	= {ICT technologies in support of creating new cultural
		  experiences or digital artefacts; metadata, classification
		  schema, ontologies and semantic processing for CH
		  multimedia repositories; knowledge patterns; digital
		  humanities; intangible cultural heritage; human experience
		  studies}
}

@InProceedings{	  10.1145/3708036.3708046,
  author	= {Liao, Fei},
  title		= {Detecting Public Perceptions of Apollo Go on Chinese
		  Social Media Based on Topic Modeling and Sentiment
		  analysis},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708046},
  doi		= {10.1145/3708036.3708046},
  abstract	= {As one representative of autonomous vehicles (AVs), Apollo
		  Go, Baidu's self-driving cab service, has been operated in
		  11 cities in China and aroused heated discussion on Chinese
		  social media after its crash with one pedestrian in Wuhan
		  in July, 2024. It's significant to explore the public's
		  opinions and sentiment for a company to promote its
		  products and expand its market. Chinese social media,
		  Weibo, provides a vast amount of data to dig out the public
		  attitudes and perspectives. In this paper, 11,882 Weibo
		  posts from May 29th to August 15th 2024 were crawled for
		  topic modelling and sentiment analysis by using LDA
		  algorithm and sentiment dictionaries. It is found that the
		  public mainly focus on five topics related with Apollo Go,
		  including unemployment, accident and privacy, rumor
		  response and market expansion, industry development and
		  riding experience as well as price. While some people
		  embrace Apollo Go which, they think, stands for the future
		  means of transportation, some of them show their concern
		  about the liability for the accident, riding safety,
		  possible privacy invasion as well as the potential
		  unemployment. At the end this study, several measures are
		  put forward to tackle those negative sentiments to promote
		  the popularity of AVs. Meanwhile, the limitation of this
		  study were also proposed at the conclusion of this paper.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {61–65},
  numpages	= {5},
  keywords	= {Apollo Go, Autonomous vehicles, Sentiment analysis, Topic
		  modelling, Weibo},
  location	= { },
  series	= {ICCSMT '24}
}

@Article{	  10.1145/3715069,
  author	= {Varshney, Deeksha and Behera, Niranshu and Katari, Prajeet
		  and Ekbal, Asif},
  title		= {MedProm: Bridging Dialogue Gaps in Healthcare with
		  Knowledge-Enhanced Generative Models},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715069},
  doi		= {10.1145/3715069},
  abstract	= {In medical dialogue systems, recent advancements
		  underscore the critical role of incorporating relevant
		  medical knowledge to enhance performance. However, existing
		  knowledge bases often lack completeness, posing a challenge
		  in sourcing pertinent information. We present MedProm, a
		  novel generative model tailored for medical dialogue
		  generation to address this gap. Motivated by the need for
		  comprehensive and contextually relevant responses, MedProm
		  leverages state-of-the-art language models such as BioGPT.
		  Our model is designed to integrate extensive medical
		  knowledge into conversations, facilitating effective
		  communication between patients and healthcare providers. At
		  the core of MedProm lies the MediConnect Graph, a
		  meticulously constructed knowledge graph capturing
		  intricate relationships among medical entities extracted
		  from dialogue contexts. By employing a KnowFusion encoder
		  with a pretraining objective and masked multi-head
		  self-attention, MedProm effectively processes the
		  MediConnect graph, enabling precise control over
		  information flow to capture its underlying structure.
		  Furthermore, MedProm incorporates a sophisticated
		  Curriculum Knowledge Decoder, leveraging transformer-based
		  decoding to generate response utterances conditioned on
		  input representations from the KnowFusion Encoder. The
		  training process is guided through curriculum learning,
		  gradually increasing optimization difficulty based on a
		  coherence-based criterion. Experimental results on two
		  datasets demonstrate the efficacy of MedProm in generating
		  accurate and contextually relevant responses compared to
		  state-of-the-art models.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= jan,
  keywords	= {Medical Dialogue Systems (MDS), Generative Neural Model,
		  MediConnect Graph, KnowFusion Encoder, Curriculum Knowledge
		  Decoder}
}

@InProceedings{	  10.1145/3173574.3173851,
  author	= {Huber, Bernd and McDuff, Daniel and Brockett, Chris and
		  Galley, Michel and Dolan, Bill},
  title		= {Emotional Dialogue Generation using Image-Grounded
		  Language Models},
  year		= {2018},
  isbn		= {9781450356206},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3173574.3173851},
  doi		= {10.1145/3173574.3173851},
  abstract	= {Computer-based conversational agents are becoming
		  ubiquitous. However, for these systems to be engaging and
		  valuable to the user, they must be able to express emotion,
		  in addition to providing informative responses. Humans rely
		  on much more than language during conversations; visual
		  information is key to providing context. We present the
		  first example of an image-grounded conversational agent
		  using visual sentiment, facial expression and scene
		  features. We show that key qualities of the generated
		  dialogue can be manipulated by the features used for
		  training the agent. We evaluate our model on a large and
		  very challenging real-world dataset of conversations from
		  social media (Twitter). The image-grounding leads to
		  significantly more informative, emotional and specific
		  responses, and the exact qualities can be tuned depending
		  on the image features used. Furthermore, our model improves
		  the objective quality of dialogue responses when evaluated
		  on standard natural language metrics.},
  booktitle	= {Proceedings of the 2018 CHI Conference on Human Factors in
		  Computing Systems},
  pages		= {1–12},
  numpages	= {12},
  keywords	= {computer vision, conversation, conversational agents,
		  dialogue, emotion},
  location	= {Montreal QC, Canada},
  series	= {CHI '18}
}

@InProceedings{	  10.1145/3685243.3685295,
  author	= {Samper-Zapater, J. Javier and Martinez-Dura, Juan J. and
		  Martinez-Plume, Javier and Rodr\'{\i}guez, David
		  Garc\'{\i}a and Moret, Julian Guti\'{e}rrez and Baeza,
		  Ernesto L\'{o}pez},
  title		= {VAS. A Semantic Model for Earth Observation data},
  year		= {2025},
  isbn		= {9798400717338},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3685243.3685295},
  doi		= {10.1145/3685243.3685295},
  abstract	= {This paper discusses a segment of the "Artificial
		  Intelligence and Semantics of Earth Observation (EO) data
		  for the establishment of the Valencia Anchor Station as a
		  supersite of the CEOS LPV Program (ASOTVAS)." The ASOTVAS
		  project aims to apply AI and data semantics algorithms to
		  the Valencia Anchor Station’s control zone (10 km x 10
		  km). A data semantics analysis explores concepts such as
		  vocabularies, ontologies, and knowledge maps, with an
		  emphasis on the reuse of existing models. Crucially, an
		  ontological model with clear data definitions is developed
		  to correlate Copernicus Program data with station
		  parameters (e.g., FAPAR, LST, soil moisture, etc.). Using
		  the ontology, the actual data are aggregated to create a
		  knowledge map, which includes both an assertion model
		  (concepts and properties) and an instance model (specific
		  instances for ontological relationships). The main task is
		  to develop an ontology-based representation scheme for
		  terms of the application domain, which facilitates
		  knowledge capture, distribution, and maintenance. The
		  scheme enables computer processing for tasks related to
		  information handling and supporting information exchange.
		  Widely accepted ontology specification methodologies are
		  employed. To avoid unnecessary efforts, emphasis is put on
		  the reuse of existing ontologies or vocabularies, expanded,
		  and adapted to the described case study. The optimal
		  granularity is chosen for efficient ontology operation. The
		  conceptual and technological requirements of the
		  representation scheme are discussed. The methodology is
		  based on software quality standards.},
  booktitle	= {Proceedings of the 12th Euro American Conference on
		  Telematics and Information Systems},
  articleno	= {9},
  numpages	= {8},
  keywords	= {earth observation data, ontology, remote sensing,
		  semantic.},
  location	= {Praia, Cape Verde},
  series	= {EATIS 2024}
}

@InProceedings{	  10.1145/3340555.3356100,
  author	= {Li, Hao and Liu, Chen and Zhu, Su and Yu, Kai},
  title		= {Robust Spoken Language Understanding with Acoustic and
		  Domain Knowledge},
  year		= {2019},
  isbn		= {9781450368605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340555.3356100},
  doi		= {10.1145/3340555.3356100},
  abstract	= {Spoken language understanding (SLU) converts user
		  utterances into structured semantic forms. There are still
		  two main issues for SLU: robustness to ASR-errors and the
		  data sparsity of new and extended domains. In this paper,
		  we propose a robust SLU system by leveraging both acoustic
		  and domain knowledge. We extract audio features by training
		  ASR models on a large number of utterances without semantic
		  annotations. For exploiting domain knowledge, we design
		  lexicon features from the domain ontology and propose an
		  error elimination algorithm to help predicted values
		  recovered from ASR-errors. The results of CATSLU challenge
		  show that our systems can outperform all of the other teams
		  across four domains.},
  booktitle	= {2019 International Conference on Multimodal Interaction},
  pages		= {531–535},
  numpages	= {5},
  keywords	= {Robustness, Spoken Language Understanding},
  location	= {Suzhou, China},
  series	= {ICMI '19}
}

@Article{	  10.1145/3583592,
  author	= {Clavaud, Florence and Francart, Thomas and Charbonnier,
		  Pauline},
  title		= {RiC-O Converter: A Software to Convert EAC-CPF and EAD
		  2002 XML Files to RDF Datasets Conforming to Records in
		  Contexts Ontology},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3583592},
  doi		= {10.1145/3583592},
  abstract	= {RiC-O Converter is an open source command-line tool to
		  convert EAD finding aids and EAC-CPF authority records to
		  RDF files conforming to ICA Records in Contexts ontology
		  (RiC-O) in a robust manner. It was developed for the
		  Archives nationales of France but is aimed to be reused by
		  other archival institutions, and to this aim is fully
		  documented in English. It is based on XSLT stylesheets that
		  consider the variability of EAD content. It enabled the
		  Archives nationales of France to convert 15,400 EAC-CPF
		  files and 31,000 EAD files into a homogeneous knowledge
		  graph, and to start a more specific project aiming to
		  provide end users with an intuitive search interface for a
		  significant subset of this graph, opening new perspectives
		  for navigating and linking from/to archival metadata.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {42},
  numpages	= {13},
  keywords	= {Records in Contexts (RiC), RiC Ontology (RiC-O), RDF, XML
		  EAD, XML EAC-CPF, open source software, archival finding
		  aids, archival authority records}
}

@InProceedings{	  10.1145/3428502.3428638,
  author	= {Chichkova, Natalia and Begler, Alena and Vlasov, Vitaly},
  title		= {Modeling city land use with an ontology},
  year		= {2020},
  isbn		= {9781450376747},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3428502.3428638},
  doi		= {10.1145/3428502.3428638},
  abstract	= {Urban planning developers, city government and citizens
		  need a digital city model for sustainable development and
		  city planning. Such models are based on main regulatory
		  documents that differ by the city. In the current project,
		  we aimed to develop a Land Use Ontology (LUO) for St.
		  Petersburg as well as to expand general approaches to
		  create such ontologies. The project is at its initial
		  stage, i.e. development of the Land Use and Development
		  Rules model that is a crucial document for the city land
		  use system. We propose the ontological model of the city
		  land use. The model includes classes for city land sites
		  description, their coordinates, and permissions for
		  construction works. The model is implemented in Web
		  Ontology Language that assures the five-star rating of open
		  data. The model source code can be found at
		  https://github.com/inxaoc/cityont-public.},
  booktitle	= {Proceedings of the 13th International Conference on Theory
		  and Practice of Electronic Governance},
  pages		= {851–854},
  numpages	= {4},
  keywords	= {City digital model, city open data, land use ontology,
		  smart city},
  location	= {Athens, Greece},
  series	= {ICEGOV '20}
}

@InProceedings{	  10.1145/3330204.3330253,
  author	= {Chagas, Michael William and Farias, Kleinner and
		  Gon\c{c}ales, Lucian and Kupssinsk\"{u}, Lucas and Gluz,
		  Jo\~{a}o Carlos},
  title		= {Hermes: A Natural Language Interface Model for Software
		  Transformation},
  year		= {2019},
  isbn		= {9781450372374},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3330204.3330253},
  doi		= {10.1145/3330204.3330253},
  abstract	= {Software maintenance is a costly task and error-prone for
		  both software developers and users as well. By knowing how
		  and what software requirements need to be changed, end
		  users could perform maintenance assisted by tools. However,
		  current literature lacks for tools that support automated
		  maintenance in real-world scenarios and allow users
		  interaction via natural language. Even worse, the current
		  tools are unable to understand the semantic of requests, as
		  well as perform the necessary transformations in the
		  maintenance software. This paper, therefore, proposes
		  Hermes, a natural language interface model for software
		  transformation. It combines computational linguistics
		  techniques and logic programming to perform automated
		  maintenance requests in software. Hermes interacts with end
		  user through state of the art language parsers and domain
		  ontologies by interpreting the semantics of changes
		  requests to build a typed graph that change the software.
		  Hermes was evaluated through an empirical study with 8
		  participants to investigate its performance, the level of
		  acceptance, and usability. The collected data show that
		  Hermes was accurate, producing a high elevated correctness
		  number of hits by finding correct transformations and has
		  been highly accepted by the users. The results are
		  encouraging and show the potential for using Hermes to
		  properly produce software maintenance requests.},
  booktitle	= {Proceedings of the XV Brazilian Symposium on Information
		  Systems},
  articleno	= {43},
  numpages	= {8},
  keywords	= {Natural Language Processing, Ontologies, Parsing},
  location	= {Aracaju, Brazil},
  series	= {SBSI '19}
}

@InProceedings{	  10.1145/3360901.3364431,
  author	= {Wiens, Vitalis and Lohmann, Steffen and Auer, S\"{o}ren},
  title		= {GizMO -- A Customizable Representation Model for
		  Graph-Based Visualizations of Ontologies},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364431},
  doi		= {10.1145/3360901.3364431},
  abstract	= {Visualizations can support the development, exploration,
		  communication, and sense-making of ontologies. Suitable
		  visualizations, however, are highly dependent on individual
		  use cases and targeted user groups. In this article, we
		  present a methodology that enables customizable definitions
		  for the visual representation of ontologies.The methodology
		  describes visual representations using the OWL annotation
		  mechanisms and separates the visual abstraction into two
		  information layers. The first layer describes the graphical
		  appearance of OWL constructs. The second layer addresses
		  visual properties for conceptual elements from the
		  ontology. Annotation ontologies and a modular architecture
		  enable separation of concerns for individual information
		  layers. Furthermore, the methodology ensures the separation
		  between the ontology and its visualization.We showcase the
		  applicability of the methodology by introducing GizMO, a
		  representation model for graph-based visualizations in the
		  form of node-link diagrams. The graph visualization meta
		  ontology (GizMO) provides five annotation object types that
		  address various aspects of the visualization (e.g., spatial
		  positions, viewport zoom factor, and canvas background
		  color). The practical use of the methodology and GizMO is
		  shown using two applications that indicate the variety of
		  achievable ontology visualizations.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {163–170},
  numpages	= {8},
  keywords	= {annotation ontology, customization, ontology
		  visualization, visual notation, visual representation,
		  visualization framework},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.1145/3701716.3715240,
  author	= {Liang, Lei and Bo, Zhongpu and Gui, Zhengke and Zhu,
		  Zhongshu and Zhong, Ling and Zhao, Peilong and Sun, Mengshu
		  and Zhang, Zhiqiang and Zhou, Jun and Chen, Wenguang and
		  Zhang, Wen and Chen, Huajun},
  title		= {KAG: Boosting LLMs in Professional Domains via Knowledge
		  Augmented Generation},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715240},
  doi		= {10.1145/3701716.3715240},
  abstract	= {The recently developed Retrieval-Augmented Generation
		  (RAG) technology has enabled the efficient construction of
		  domain-specific applications. The key technologies of RAG
		  are retrieval based on similarity and reasoning based on
		  next-token prediction. However, this approach differs
		  significantly from how humans solve problems. Humans
		  typically follow certain analytical logic, reasoning while
		  retrieving relevant information, and then connecting the
		  clues to serve as references, ultimately generating an
		  answer. In this process, the focus is on the semantic type
		  and clear relationships between the keywords rather than
		  similarity and co-occurrence. This difference in
		  methodology results in the answers generated by RAG
		  technology being insufficiently accurate or valuable.In
		  this work, we concentrate on establishing semantic
		  relationships between keywords to enable a more precise
		  expression of knowledge and propose the Knowledge Augmented
		  Generation(KAG) framework. KAG performs semantic parsing
		  and reasoning on both documents and questions, involving
		  three specific strategies: In the indexing phase, we
		  complete the semantic information of keywords and the
		  semantic relationships between them through information
		  extraction and semantic reasoning; in the reasoning phase
		  of question answering, we leverage semantic parsing to
		  transform questions into Logical Forms with clear semantic
		  types and relationships; in the retrieval phase, we predict
		  the semantic relationships between Logical Form elements
		  and structured index, thereby obtaining the required
		  references.We compared KAG with existing RAG methods in
		  three multi-hop QA datasets and the results show that KAG
		  significantly outperforms existing methods, achieving a new
		  state-of-the-art. We also applied KAG to real E-Government
		  Q&amp;A business scenario, and achieving significant
		  improvements in professionalism compared to traditional RAG
		  methods. Meanwhile, to help developers easily build
		  accurate and efficient domain knowledge QA services, our
		  KAG natively supports the open-source KG engine OpenSPG.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {334–343},
  numpages	= {10},
  keywords	= {information retrieval, kbqa, knowledge graph, knowledge
		  reasoning, rag},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3358501.3361235,
  author	= {Roy Chaudhuri, Subhrojyoti and Natarajan, Swaminathan and
		  Banerjee, Amar and Choppella, Venkatesh},
  title		= {Methodology to develop domain specific modeling
		  languages},
  year		= {2019},
  isbn		= {9781450369848},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3358501.3361235},
  doi		= {10.1145/3358501.3361235},
  abstract	= {Domain Specific Modeling Languages (DSML) significantly
		  improve productivity in designing Computer Based System
		  (CBS), by enabling them to be modeled at higher levels of
		  abstraction. It is common for large and complex systems
		  with distributed teams, to use DSMLs, to express and
		  communicate designs of such systems uniformly, using a
		  common language. DSMLs enable domain experts, with no or
		  minimal software development background, to model
		  solutions, using the language and terminologies used in
		  their respective domains. Although, there are already a
		  number of DSMLs available for modeling CBSs, their need is
		  felt strongly across multiple domains, which still are not
		  well supported with DSMLs. Developing a new DSML, however,
		  is non trivial, as it requires (a) significant knowledge
		  about the domain for which the DSML needs to be developed,
		  as well as (b) skills to create new languages. In the
		  current practice, DSMLs are developed by experts, who have
		  substantial understanding of the domain of interest and
		  strong background in computer science. One of the many
		  challenges in the development of DSMLs, is the collection
		  of domain knowledge and its utilization, based on which the
		  abstract syntax, the backbone of the DSML is defined. There
		  is a clear gap in the current state of art and practice,
		  with respect to overcoming this challenge. We propose a
		  methodology, which makes it easier for people with
		  different backgrounds such as domain experts, solution
		  architects, to contribute towards defining the abstract
		  syntax of the DSML. The methodology outlines a set of steps
		  to systematically capture knowledge about the domain of
		  interest, and use that to arrive at the abstract syntax of
		  the DSML. The key contribution of our work is in
		  abstracting a CBS from a domain into a Domain Specific
		  Machine, embodied in domain specific concepts. The
		  methodology outlines, how the Domain Specific Machine, when
		  coupled with guidelines from current practices of
		  developing DSMLs, results in the definition of the abstract
		  syntax of the intended DSML. We discuss our methodology in
		  detail, in this paper.},
  booktitle	= {Proceedings of the 17th ACM SIGPLAN International Workshop
		  on Domain-Specific Modeling},
  pages		= {1–10},
  numpages	= {10},
  keywords	= {domain specific language, domain specific modeling
		  language, language engineering, modeling},
  location	= {Athens, Greece},
  series	= {DSM 2019}
}

@InProceedings{	  10.1145/3732945.3733004,
  author	= {Yang, Cuicui and Zhu, Min},
  title		= {Research on Robotics Diffusion Transformer in Robot
		  Foundation Models Based on Diffusion Model},
  year		= {2025},
  isbn		= {9798400715204},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3732945.3733004},
  doi		= {10.1145/3732945.3733004},
  abstract	= {In recent years, diffusion models have shown great
		  potential in the field of robotics. This paper focuses on
		  the research of robot foundation models based on diffusion
		  models, with a particular emphasis on the Robotics
		  Diffusion Transformer (RDT). First, it reviews the
		  technological frontier of robot basic action models,
		  including visual language models and robot action learning.
		  Then, it details the RDT model, covering its design for an
		  ALOHA dual - arm robot, core methods such as diffusion
		  model design, feature coding, regularization, non - linear
		  action output, and Alternating Condition Injection (ACI).
		  The RDT model is trained on a large number of datasets and
		  fine - tuned on specific data. Experimental results
		  demonstrate that RDT outperforms previous models in tasks
		  such as generalization to unseen objects and scenes, few -
		  shot learning, and dexterous control. Ablation experiments
		  further verify the effectiveness of its key design
		  elements. Overall, RDT proves the significance of
		  generative models in embodied intelligence, showing that
		  appropriate model design and data - driven training can
		  achieve excellent results.},
  booktitle	= {Proceedings of the 2025 4th International Conference on
		  Intelligent Systems, Communications and Computer Networks},
  pages		= {402–409},
  numpages	= {8},
  keywords	= {Diffusion model, Robot action learning, Robot foundation
		  model, Robotics Diffusion Transformer (RDT), Visual
		  language model},
  location	= { },
  series	= {ISCCN '25}
}

@InProceedings{	  10.1145/3716554.3716555,
  author	= {Mendes, Mara and Ali, Mohsan and Charalabidis, Yannis},
  title		= {Analyzing the Freedom of Information Requests Using Topic
		  Modeling: Towards User-driven Open Government Data
		  Ecosystems},
  year		= {2025},
  isbn		= {9798400713170},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3716554.3716555},
  doi		= {10.1145/3716554.3716555},
  abstract	= {Open data has become an important player in promoting
		  government transparency and accountability around the
		  globe. Governments have established policies to provide
		  citizens or users with access to data. One way to obtain
		  such data is through Freedom of Information Act (FOIA)
		  requests. Considering users' requests for data and
		  responding with the required information makes open data
		  ecosystems user-driven. This paper explores freedom of
		  information (FOI) requests received by Germany's Ministry
		  of Health as a user-driven approach to open government data
		  (OGD). By applying advanced natural language processing
		  (NLP) techniques, specifically Latent Dirichlet Allocation
		  (LDA) and BERTopic, we analyze a large corpus of FOI
		  requests to identify key topics and trends in public
		  inquiries. The findings reveal valuable insights into
		  citizens' information demands and demonstrate the strengths
		  of these NLP methods in extracting actionable patterns. In
		  this way, governments can easily understand user needs in
		  health-related datasets (this study, in particular, focuses
		  on requests made by citizens related to COVID-19 data).
		  This study lays the groundwork in two ways: first, by
		  understanding and thematically categorizing user needs
		  related to open data, and second, by improving government
		  data transparency, informing the prioritization of dataset
		  releases, and supporting evidence-based policymaking.},
  booktitle	= {Proceedings of the 28th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {1–10},
  numpages	= {10},
  keywords	= {Freedom of Information, Open Data, Open Data Ecosystems,
		  Open Government Data, Topic Modeling},
  location	= { },
  series	= {PCI '24}
}

@InProceedings{	  10.1145/3610978.3640642,
  author	= {Saracco, Alessandro and Lillo, Alberto and Stranisci,
		  Marco and Gena, Cristina},
  title		= {Human Robot Interaction through an Ontology-based Dialogue
		  Engine},
  year		= {2024},
  isbn		= {9798400703232},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3610978.3640642},
  doi		= {10.1145/3610978.3640642},
  abstract	= {This paper outlines the evolution of the Sugar, Salt,
		  \&amp; Pepper project for high level functioning children
		  affected by autism, focusing on the development of a
		  dialogue system that relies on an ontology-based knowledge
		  base. The ontology offers a formal representation of
		  knowledge and interrelationships within the movie domain.
		  The dialogue system addresses issues related to predefined
		  answers, emphasizing adaptability for multi-platform use,
		  particularly in the context of the social robot Pepper. The
		  research covers detailed phases of construction and
		  development, highlighting implementation choices and
		  challenges faced.},
  booktitle	= {Companion of the 2024 ACM/IEEE International Conference on
		  Human-Robot Interaction},
  pages		= {940–944},
  numpages	= {5},
  keywords	= {dialog, human-robot interaction, machine learning,
		  ontology},
  location	= {Boulder, CO, USA},
  series	= {HRI '24}
}

@InProceedings{	  10.1145/3592813.3592893,
  author	= {Campos, J\'{u}lio Gon\c{c}alves and De Almeida, Vitor
		  Pinheiro and De Armas, Elvismary Molina and Da Silva, Geiza
		  Maria Hamazaki and Corseuil, Eduardo Thadeu and Gonzalez,
		  Fernando Rodrigues},
  title		= {INSIDE: an Ontology-based Data Integration System Applied
		  to the Oil and Gas Sector},
  year		= {2023},
  isbn		= {9798400707599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3592813.3592893},
  doi		= {10.1145/3592813.3592893},
  abstract	= {Context: Data integration remains a major challenge facing
		  organizations in the information age. Despite the advances
		  made in recent decades, new approaches have become
		  necessary to deal with new challenges such as Big Data.
		  Problem: Semantic heterogeneity is a significant problem
		  faced by companies in the oil and gas sector, as it makes
		  it difficult to exchange information with other companies.
		  Furthermore, there is a shortage of data integration
		  systems that use open source technologies to deal with
		  semantics, interoperability and scalability. Solution:
		  INSIDE - Semantic Interoperability for Engineering Data
		  Integration - an information system based on ontologies for
		  data integration developed for an oil and gas company. SI
		  Theory: This work is influenced by Representation Theory,
		  based on the idea that an information system is a faithful
		  representation of certain phenomena in the real world.
		  Method: Review of state of the art on system architectures
		  for data integration and use of methodologies for
		  elaborating ontologies that represent the knowledge base of
		  the information system. Summary of Results: Implementation
		  of a prototype that allows querying heterogeneous data
		  sources using a vocabulary familiar to the user, removing
		  ambiguities from data with semantics. Contributions and
		  Impact in IS area: The development of a solution for data
		  integration using open source technologies tested with
		  real-world data from a company in the oil and gas sector
		  that can serve as a reference for developing new applied
		  systems to other sectors.},
  booktitle	= {Proceedings of the XIX Brazilian Symposium on Information
		  Systems},
  pages		= {94–101},
  numpages	= {8},
  keywords	= {Data Integration, Information Systems, Ontology., Semantic
		  Web},
  location	= {Macei\'{o}, Brazil},
  series	= {SBSI '23}
}

@InProceedings{	  10.1145/3652620.3688208,
  author	= {Weber, Thomas and Ojha, Monalisha and Sadeghi, Mohammad
		  and K\"{o}nig, Lars and Armbruster, Martin and Lange, Arne
		  and Burger, Erik and Atkinson, Colin},
  title		= {Towards Deep Reactions in Multi-Level, Multi-View
		  Modeling},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688208},
  doi		= {10.1145/3652620.3688208},
  abstract	= {As the scale, complexity, and scope of software-intensive
		  systems continue to grow, so does the importance of
		  synergistically integrating two important emerging
		  paradigms in software engineering - multi-level modeling
		  and multi-view modeling. While stable tooling for both has
		  been developed by research institutions in recent years, to
		  date no tool has attempted to integrate the two at a
		  fundamental level. In this paper, we describe some first
		  steps we have taken in this direction by integrating the
		  Vitruvius V-SUM-based multi-view environment with the
		  Melanee multi-level modeling environment. In particular, we
		  show how Vitruvius's Reactions language, which allows
		  different models in Vitruvius V-SUMs to be kept consistent,
		  can be extended to support multi-level V-SUMs and views
		  represented in Melanee's dialect of multi-level modeling.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {760–769},
  numpages	= {10},
  keywords	= {multi-level modeling, V-SUM, view-based modeling,
		  vitruvius, consistency},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.1145/3744754,
  author	= {Ahmad, Pir Noman and Ullah, Inam and M. Salim, Nagwa and
		  Kumar Singh, Sushil and Jiang, Weiwei and Al-Khasawneh,
		  Mahmoud Ahmad and Daradkeh, Yousef Ibrahim},
  title		= {Deep Neural Network-Based Feature Encoding for Automated
		  Health Monitoring Using Large AI Models in Online
		  Communication Systems},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3744754},
  doi		= {10.1145/3744754},
  abstract	= {The hybrid model combines deep neural networks (DNN) and
		  large AI models, such as large language models (LLM), for
		  enhanced clinical information retrieval (CIR) from
		  electronic clinical records (ECR). While LLMs show promise
		  for encoding complex medical data, they face challenges in
		  user-dependent information, such as patient reports with
		  encoded knowledge, accessing real-time data, and requiring
		  extensive fine-tuning for clinical decision-making in
		  online communication systems. To overcome these
		  limitations, we introduce a Transformer-based Sequence
		  (TBS) multimodal method that integrates representation
		  learning with human expertise to encode and analyze
		  intricate relationships within clinical data. This model
		  improves predictive tasks and medical search accuracy,
		  achieving F1-scores of 0.83-0.80, and outperforms baseline
		  methods. Integrating AI-driven methodologies in healthcare
		  has the potential to transform medical record analysis and
		  utilization, resulting in enhanced patient outcomes and
		  more personalized healthcare solutions.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Internet Things},
  month		= jun,
  keywords	= {Deep neural network, communication systems, healthcare,
		  large AI models, transformer-based sequence}
}

@InProceedings{	  10.1145/3723366.3723370,
  author	= {Wang, Yukang and Yang, Mengyu and Chen, Yu and Yu,
		  Mingjie},
  title		= {Research on Intelligent Course Q&amp;A Systems Based on
		  NLP Models, Knowledge Graphs, and Deep Learning Methods},
  year		= {2025},
  isbn		= {9798400718298},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3723366.3723370},
  doi		= {10.1145/3723366.3723370},
  abstract	= {With the increasing demand for personalized and
		  interactive learning in online education environments.
		  intelligent question-answering (Q&amp;A) systems have
		  emerged as powerful tools to enhance student engagement and
		  support independent learning. However, traditional
		  educational Q&amp;A systems face significant limitations,
		  such as reliance on simple keyword matching, inability to
		  provide contextually rich answers, and challenges in
		  handling complex, procedural queries. These limitations
		  often result in incomplete or irrelevant responses,
		  highlighting the need for more sophisticated solutions that
		  can understand and interpret diverse student queries
		  accurately. This research presents the design and
		  implementation of an intelligent course Q&amp;A system that
		  integrates Natural Language Processing (NLP), semantic
		  analysis, and knowledge graph traversal to deliver
		  context-aware, real-time responses to student queries. The
		  system is built upon advanced deep learning techniques,
		  leveraging BERT for answer retrieval, a Bi-LSTM + CRF model
		  for Named Entity Recognition (NER), and a dynamically
		  constructed knowledge graph for multi-step reasoning. The
		  system was evaluated on a dataset of student queries and
		  course content, demonstrating strong performance with an
		  overall accuracy of 88.1\%. The results highlight the
		  system's effectiveness in handling factual, conceptual, and
		  procedural questions, offering personalized and relevant
		  answers to enhance the learning experience. However,
		  challenges remain in addressing ambiguous queries, scaling
		  the knowledge graph, and providing detailed procedural
		  explanations. The paper concludes by discussing potential
		  future improvements, such as the integration of interactive
		  dialogue mechanisms, optimized knowledge graphs, and
		  enhanced explanatory models. This intelligent Q&amp;A
		  system represents a step forward in educational technology,
		  offering scalable and adaptable solutions for modern
		  digital learning environments.},
  booktitle	= {Proceedings of the 2024 4th International Symposium on Big
		  Data and Artificial Intelligence},
  pages		= {21–28},
  numpages	= {8},
  keywords	= {Deep Learning, Educational Technology, Knowledge Graph,
		  Natural Language Processing, Question-Answering System},
  location	= { },
  series	= {ISBDAI '24}
}

@InProceedings{	  10.1145/3318464.3383128,
  author	= {undefinedzcan, Fatma and Quamar, Abdul and Sen, Jaydeep
		  and Lei, Chuan and Efthymiou, Vasilis},
  title		= {State of the Art and Open Challenges in Natural Language
		  Interfaces to Data},
  year		= {2020},
  isbn		= {9781450367356},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318464.3383128},
  doi		= {10.1145/3318464.3383128},
  abstract	= {Recent advances in natural language understanding and
		  processing resulted in renewed interest in natural language
		  based interfaces to data, which provide an easy mechanism
		  for non-technical users to access and query the data. While
		  early systems only allowed simple selection queries over a
		  single table, some recent work supports complex BI queries,
		  with many joins and aggregation, and even nested queries.
		  There are various approaches in the literature for
		  interpreting user's natural language query. Rule-based
		  systems try to identify the entities in the query, and
		  understand the intended relationships between those
		  entities. Recent years have seen the emergence and
		  popularity of neural network based approaches which try to
		  interpret the query holistically, by learning the patterns.
		  In this tutorial, we will review these natural language
		  interface solutions in terms of their interpretation
		  approach, as well as the complexity of the queries they can
		  generate. We will also discuss open research challenges.},
  booktitle	= {Proceedings of the 2020 ACM SIGMOD International
		  Conference on Management of Data},
  pages		= {2629–2636},
  numpages	= {8},
  keywords	= {conversation systems, natural language interfaces, natural
		  language query},
  location	= {Portland, OR, USA},
  series	= {SIGMOD '20}
}

@InProceedings{	  10.1145/3383313.3418478,
  author	= {Twomey, Niall and Fain, Mikhail and Ponikar, Andrey and
		  Sarraf, Nadine},
  title		= {Towards Multi-Language Recipe Personalisation and
		  Recommendation},
  year		= {2020},
  isbn		= {9781450375832},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383313.3418478},
  doi		= {10.1145/3383313.3418478},
  abstract	= {Multi-language recipe personalisation and recommendation
		  is an under-explored field of information retrieval in
		  academic and production systems. The existing gaps in our
		  current understanding are numerous, even on fundamental
		  questions such as whether consistent and high-quality
		  recipe recommendation can be delivered across languages.
		  Motivated by this need, we consider the multi-language
		  recipe recommendation setting and present grounding results
		  that will help to establish the potential and absolute
		  value of future work in this area. Our work draws on
		  several billion events from millions of recipes, with
		  published recipes and users incorporating several
		  languages, including Arabic, English, Indonesian, Russian,
		  and Spanish. We represent recipes using a combination of
		  normalised ingredients, standardised skills and image
		  embeddings obtained without human intervention. In
		  modelling, we take a classical approach based on optimising
		  an embedded bi-linear user-item metric space towards the
		  interactions that most strongly elicit cooking intent. For
		  users without interaction histories, a bespoke
		  content-based cold-start model that predicts context and
		  recipe affinity is introduced. We show that our approach to
		  personalisation is stable and scales well to new languages.
		  A robust cross-validation campaign is employed and
		  consistently rejects baseline models and representations,
		  strongly favouring those we propose. Our results are
		  presented in a language-oriented (as opposed to
		  model-oriented) fashion to emphasise the language-based
		  goals of this work. We believe that this is the first
		  large-scale work that evaluates the value and potential of
		  multi-language recipe recommendation and personalisation.},
  booktitle	= {Proceedings of the 14th ACM Conference on Recommender
		  Systems},
  pages		= {708–713},
  numpages	= {6},
  keywords	= {information retrieval, personalisation, recipes and food
		  modelling, recommendation},
  location	= {Virtual Event, Brazil},
  series	= {RecSys '20}
}

@InProceedings{	  10.5555/3716662.3716767,
  author	= {Rathje, William},
  title		= {Learning When Not to Measure: Theorizing Ethical Alignment
		  in LLMs},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {LLMs and other forms of generative AI have shown immense
		  promise in producing highly accurate epistemic judgements
		  in domains as varied as law, education, and medicine - with
		  GPT notably passing the legal Bar exam and various medical
		  licensing exams. The safe extension of LLMs into
		  safety-critical professional domains requires assurance not
		  only of epistemic but ethical alignment. This paper adopts
		  a theoretical and philosophical approach, drawing from
		  metaethical theories to argue for a distinction hinging
		  around quantitative, axiological comparability that
		  separates Kantian ethics from not only the utilitarianism
		  it is well-known to oppose, but from just distribution
		  theories as well, which are key to debiasing LLM models. It
		  presents the novel hypothesis that LLM ethical acquisition
		  from both corpus induction and RLHF may encounter value
		  conflicts between Kantian and just distribution principles
		  that intensify as they come into improved alignment with
		  both theories, hinging around the variability by which
		  self-attention may statistically attend to the same
		  characterizations as more person-like or more resource-like
		  under distinct prompting strategies.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {1190–1199},
  numpages	= {10},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@Article{	  10.1145/3555719,
  author	= {Park, Eun Hee and Storey, Veda C.},
  title		= {Emotion Ontology Studies: A Framework for Expressing
		  Feelings Digitally and its Application to Sentiment
		  Analysis},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3555719},
  doi		= {10.1145/3555719},
  abstract	= {Emotion ontologies have been developed to capture affect,
		  a concept that encompasses discrete emotions and feelings,
		  especially for research on sentiment analysis, which
		  analyzes a customer's attitude towards a company or a
		  product. However, there have been limited efforts to adapt
		  and employ these ontologies. This research surveys and
		  synthesizes emotion ontology studies to develop a Framework
		  of Emotion Ontologies that can be used to help a user
		  select or design an appropriate emotion ontology to support
		  sentiment analysis and increase the user's understanding of
		  the roles of affect, context, and behavioral information
		  with respect to sentiment. The framework, which is derived
		  from research on emotion ontologies, psychology, and
		  sentiment analysis, classifies emotion ontologies as
		  discrete emotion or one of two hybrid ontologies that are
		  combinations of the discrete, dimensional, or componential
		  process emotion paradigms. To illustrate its usefulness,
		  the framework is applied to the development of an emotion
		  ontology for a sentiment analysis application.},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  articleno	= {181},
  numpages	= {38},
  keywords	= {Ontology, sentiment analysis, affect, emotion, Framework
		  of Emotion Ontologies, discrete emotion ontology,
		  dimensional emotion ontology, componential process
		  ontology}
}

@Article{	  10.1145/3594719,
  author	= {Chakraborty, Chinmay and Wan, Shaohua and Khosravi,
		  Mohammad R.},
  title		= {Editorial: Ontology-based Knowledge Presentation and
		  Computational Linguistics for Semantic Big Social Data
		  Analytics in Asian Social Networks},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3594719},
  doi		= {10.1145/3594719},
  abstract	= {Data-driven ontology-based knowledge (OK) presentation and
		  computational linguistics for evolving semantic Asian
		  social networks (ASNs) can make one of the most important
		  platforms that provide robust and real-time data mapping in
		  massive access across the heterogeneous big data sources in
		  the web that is named OK-ASN. It benefits from
		  computational intelligence, web-of-things (WoT)
		  architecture, semantic features, statistical learning and
		  pattern recognition, database management, computer vision,
		  cyber-security, and language processing. OK-ASN is a
		  critical strategy for WoT big data mining and enterprises
		  from social media to medical and industrial sectors.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {136},
  numpages	= {3},
  keywords	= {Social data, ontology, knowledge graph, computational
		  linguistics, Asian social networks}
}

@InProceedings{	  10.1145/3401895.3402064,
  author	= {Henarejos-Blasco, Jos\'{e} and Garc\'{\i}a-D\'{\i}az,
		  Jos\'{e} Antonio and Apolinario-Arzube, \'{O}scar and
		  Valencia-Garc\'{\i}a, Rafael},
  title		= {CNL-RDF-query: a controlled natural language interface for
		  querying ontologies and relational databases},
  year		= {2021},
  isbn		= {9781450377119},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3401895.3402064},
  doi		= {10.1145/3401895.3402064},
  abstract	= {The most commonly used search engines do not always show
		  the information that the user needs, because they do not
		  take into account factors such as the context or natural
		  language ambiguity. Therefore, other types of search
		  engines that considered these factors emerged in last few
		  years, such as question-answering systems or semantic
		  web-based search engines. In this work, we present
		  CNL-RDF-Query, a controlled natural language interface for
		  querying rdf-based ontologies and relational databases. The
		  system guides users in the construction of queries with the
		  knowledge of domain ontology. Our proposal has been tested
		  in the domain of the IMDb movie repository.},
  booktitle	= {Proceedings of the 10th Euro-American Conference on
		  Telematics and Information Systems},
  articleno	= {35},
  numpages	= {5},
  keywords	= {SPARQL, natural language interface, ontologies, user
		  interfaces},
  location	= {Aveiro, Portugal},
  series	= {EATIS '20}
}

@InProceedings{	  10.1145/3723178.3723230,
  author	= {Arnob, Arjun Kumar Bose and Naim, Mostaque Ahammed and
		  Rezwan, Md Tahmid and Hasan, Mohammad Mahmudul},
  title		= {Utilizing Kidney Ontology for Data-Driven Exploration of
		  Potential Biomarkers in Kidney Diseases: Introducing the
		  Kidney Diseases Biomarker Ontology (KDBO)},
  year		= {2025},
  isbn		= {9798400713828},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3723178.3723230},
  doi		= {10.1145/3723178.3723230},
  abstract	= {This study proposes a new method of identifying possible
		  kidney disease biomarkers using Kidney Diseases Biomarker
		  Ontology (KDBO). This study classically merges clinical
		  imaging, biopsy data, proteomics, and genomic data derived
		  from various sources through the Kidney Development
		  Subontology (KDSO) of Gene Ontology (GO). Machine learning
		  algorithms, network analysis, and statistical approaches
		  are used to identify novel biomarkers that have
		  implications for renal impairment at different stages of
		  the disease have been employed here. A comparison between
		  the new upstart markers found and the available right
		  markers was made in patient cohorts to confirm their
		  accuracy. The study showed that employing kidney ontology
		  improves diagnosis, prognosis, and treatment by
		  facilitating earlier identification of dysfunction, precise
		  evaluation of disease severity levels, tailored choice
		  therapy, addressing issues related to renal ontologies, and
		  presenting suggestions for future studies.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Computing Advancements},
  pages		= {391–398},
  numpages	= {8},
  keywords	= {Kidney Ontology, Biomarkers, Data Integration, KDBO, Renal
		  Disease, Chronic Kidney Disease (CKD)},
  location	= { },
  series	= {ICCA '24}
}

@InProceedings{	  10.1145/3569902.3569947,
  author	= {Cerqueira, Jorsiele},
  title		= {An Ontology for Context-aware Middleware for Dependable
		  Medical Systems},
  year		= {2023},
  isbn		= {9781450397377},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3569902.3569947},
  doi		= {10.1145/3569902.3569947},
  abstract	= {In healthcare systems, there is an ecosystem of
		  heterogeneous biosensors. A middleware is required for
		  transmitting and establishing dependable communication with
		  multiple integrations and information exchange through
		  messages. However, in environments in which a distributed
		  and dynamic network exists, a purely traditional middleware
		  would not minimize the effect failures at data transmission
		  and network congestion can cause. To preserve data
		  integrity and provide relevant services as the environment
		  changes, the use of context-aware middleware is
		  recommended. This article describes an ontology for
		  context-aware middleware to handle challenges faced by
		  medical system networks and environment changes.},
  booktitle	= {Proceedings of the 11th Latin-American Symposium on
		  Dependable Computing},
  pages		= {79–83},
  numpages	= {5},
  keywords	= {context aware middleware, dependable, modelling,
		  ontology},
  location	= {Fortaleza/CE, Brazil},
  series	= {LADC '22}
}

@InProceedings{	  10.1145/3191697.3214330,
  author	= {Sharpe, Oli},
  title		= {Semprola: a semiotic programming language},
  year		= {2018},
  isbn		= {9781450355131},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3191697.3214330},
  doi		= {10.1145/3191697.3214330},
  abstract	= {Most people interested in developing new programming
		  languages or programming environments are looking at how to
		  improve the syntax and semantics of the program text or at
		  tools that help make programmers more productive at
		  crafting the program text. What we need is a more
		  fundamental change to the conception of what a program is.
		  This paper introduces a new, Semiotic Programming
		  environment in which we program with signs in a context,
		  rather than with symbols in a text file and where we treat
		  dialogue rather than functions as the dominant organising
		  principle of our code. All of the information held in this
		  environment is managed in a distributed, semiotic graph
		  that is organized into multiple ontological spaces. Taken
		  together these enable our programs and data to have greater
		  semantic depth. Finally the paper gives a brief
		  introduction to Semprola, a Semiotic Programming Language
		  that can be used in this Semiotic Programming
		  environment.},
  booktitle	= {Companion Proceedings of the 2nd International Conference
		  on the Art, Science, and Engineering of Programming},
  pages		= {202–213},
  numpages	= {12},
  keywords	= {Compile time semantics, computational referent, context,
		  dialogue, distributed graph, messaging, multiple
		  ontologies, nodedge, programming languages, referent,
		  semantic depth, semantics, semiotic programming, semiotics,
		  semprola, sign, signified, signifier, spuid},
  location	= {Nice, France},
  series	= {Programming '18}
}

@InProceedings{	  10.1145/3459637.3482491,
  author	= {Xie, Chenhao and Huang, Wenhao and Liang, Jiaqing and
		  Huang, Chengsong and Xiao, Yanghua},
  title		= {WebKE: Knowledge Extraction from Semi-structured Web with
		  Pre-trained Markup Language Model},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482491},
  doi		= {10.1145/3459637.3482491},
  abstract	= {The World Wide Web contains rich up-to-date information
		  for knowledge graph construction. However, most current
		  relation extraction techniques are designed for free text
		  and thus do not handle well semi-structured web content. In
		  this paper, we propose a novel multi-phase machine reading
		  framework, called WebKE. It processes the web content on
		  different granularity by first detecting areas of interest
		  at DOM tree node level and then extracting relational
		  triples for each area. We also propose HTMLBERT as an
		  encoder the web content. It is a pre-trained markup
		  language model that fully leverages the visual layout
		  information and DOM-tree structure, without the need of
		  hand engineered features. Experimental results show that
		  the proposed approach outperforms state-of- the-art methods
		  by a considerable gain. The source code is available at
		  https://github.com/redreamality/webke.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {2211–2220},
  numpages	= {10},
  keywords	= {htmlbert, knowledge extraction, knowledge graph
		  construction, pre-trained markup language model, relation
		  extraction, semi-structured web extraction, webke},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3340555.3356097,
  author	= {Huang, Heyan and Mao, Xianling and Yang, Puhai},
  title		= {Streamlined Decoder for Chinese Spoken Language
		  Understanding},
  year		= {2019},
  isbn		= {9781450368605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340555.3356097},
  doi		= {10.1145/3340555.3356097},
  abstract	= {As a critical component of Spoken Dialog System (SDS),
		  spoken language understanding (SLU) attracts a lot of
		  attention, especially for methods based on unaligned data.
		  Recently, a new approach has been proposed that utilizes
		  the hierarchical relationship between act-slot-value
		  triples. However, it ignores the transfer of internal
		  information which may record the intermediate information
		  of the upper level and contribute to the prediction of the
		  lower level. So, we propose a novel streamlined decoding
		  structure with attention mechanism, which uses three
		  successively connected RNN to decode act, slot and value
		  respectively. On the first Chinese Audio-Textual Spoken
		  Language Understanding Challenge (CATSLU), our model
		  exceeds state-of-the-art model on an unaligned multi-turn
		  task-oriented Chinese spoken dialogue dataset provided by
		  the contest.},
  booktitle	= {2019 International Conference on Multimodal Interaction},
  pages		= {516–520},
  numpages	= {5},
  keywords	= {attention mechanisms, long short term memory networks,
		  pointer network, spoken dialog system, spoken language
		  understanding, streamlined decoder},
  location	= {Suzhou, China},
  series	= {ICMI '19}
}

@InProceedings{	  10.1145/3368691.3368700,
  author	= {Joy, Jeevamol and Raj, Nisha S and G, Renumol V},
  title		= {An ontology model for content recommendation in
		  personalized learning environment},
  year		= {2019},
  isbn		= {9781450372848},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368691.3368700},
  doi		= {10.1145/3368691.3368700},
  abstract	= {Personalized Learning Environments (PLEs) are expected to
		  enhance the learning experience by providing tailor-made
		  services based on learner preferences. It is of utmost
		  importance to provide a personalized system which can
		  automatically adapt to learners' learning styles, knowledge
		  level and intelligently recommend resources that would
		  favor and improve the learning. The existing PLEs still
		  exhibit cold-start problems and other issues related with
		  mapping of learning style and learning object. To solve
		  these issues and to improve the dynamicity of the PLEs, an
		  appropriate learner/learning object model is very
		  essential. In this paper, we introduce an ontology model
		  which encapsulates both learner profile and learning object
		  attributes, which can be used for the content
		  recommendation in an e-learning platform. Learner profile
		  is the representation of learner data which includes both
		  static and dynamic characteristics of the learner. The
		  static data is gathered directly from the learner using
		  forms and questionnaires and dynamic data is collected by
		  tracking the behavior of learners, while interacting
		  through a learning management system. The proposed ontology
		  also holds space for learning topics and their
		  corresponding Learning Object (LO) characteristics. The
		  elements which come under the educational category of IEEE
		  LOM standard, is considered for tagging the selected
		  learning objects in the ontology. The JENA API of Java
		  programming language is used for developing the ontology.
		  The data is described using Resource Description Framework
		  (RDF) tools. We have developed an ontology model consisting
		  of an adaptive learner profile and standard LO
		  characteristics which can be used in content recommender
		  systems of e-learning environment.},
  booktitle	= {Proceedings of the Second International Conference on Data
		  Science, E-Learning and Information Systems},
  articleno	= {9},
  numpages	= {6},
  keywords	= {content recommendation, learner profile, learning object,
		  ontology, personalized learning environment},
  location	= {Dubai, United Arab Emirates},
  series	= {DATA '19}
}

@Article{	  10.1145/3712008,
  author	= {Burgue\~{n}o, Lola and Di Ruscio, Davide and Sahraoui,
		  Houari and Wimmer, Manuel},
  title		= {Automation in Model-Driven Engineering: A Look Back, and
		  Ahead},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {34},
  number	= {5},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3712008},
  doi		= {10.1145/3712008},
  abstract	= {Model-Driven Engineering (MDE) provides a huge body of
		  knowledge of automation for many different engineering
		  tasks, especially those involving transitioning from design
		  to implementation. With the huge progress made in AI,
		  questions arise about the future of MDE, such as how
		  existing MDE techniques and technologies can be improved or
		  how other activities that currently lack dedicated support
		  can also be automated. However, at the same time, it has to
		  be revisited where and how models should be used to keep
		  the engineers in the loop for creating, operating, and
		  maintaining complex systems. To trigger dedicated research
		  on these open points, we discuss the history of automation
		  in MDE and present perspectives on how automation in MDE
		  can be further improved and which obstacles have to be
		  overcome in both the medium and long-term.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= may,
  articleno	= {122},
  numpages	= {25},
  keywords	= {Model-Driven Engineering (MDE), automation}
}

@InProceedings{	  10.1145/3374587.3374603,
  author	= {Xiaohui, Chen and Yinzhen, Liu and Li, Xu and Lei, Ge and
		  Yiwei, Ma},
  title		= {The Construction Method of Geographic Knowledge Graph
		  Ontology Model Based on GML},
  year		= {2020},
  isbn		= {9781450376273},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3374587.3374603},
  doi		= {10.1145/3374587.3374603},
  abstract	= {Geographic ontology model is the conceptual model of
		  geographic knowledge graph and the logical basis for
		  constructing the pattern layer of geographic knowledge
		  graph. In the classification of geographic ontology
		  research, geographic ontology model is in the category of
		  domain ontology. It is a set of abstract structures to
		  express ontology according to the spatial location,
		  attribute characteristics and relational characteristics of
		  geographic data. This paper discussed the logical
		  components and architecture of geographic ontology,
		  designed the geographic ontology model reference to GML,
		  described the model using OWL language, and constructed the
		  geographic ontology model based on GML. The geographic
		  ontology model comprises three sub-models: element model,
		  geometric model and spatial relation model. Finally, based
		  on Prot\'{e}g\'{e} ontology construction tool, this paper
		  designed the semantic description of geographic entity and
		  realized the construction of geographic ontology system.},
  booktitle	= {Proceedings of the 2019 3rd International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {138–143},
  numpages	= {6},
  keywords	= {GML, Geographic knowledge graph, geographic ontology,
		  logical composition, ontology model construction},
  location	= {Normal, IL, USA},
  series	= {CSAI '19}
}

@InProceedings{	  10.1145/3410992.3410996,
  author	= {Noura, Mahda and Heil, Sebastian and Gaedke, Martin},
  title		= {Natural language goal understanding for smart home
		  environments},
  year		= {2020},
  isbn		= {9781450387583},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3410992.3410996},
  doi		= {10.1145/3410992.3410996},
  abstract	= {One of the main challenges of the Internet of Things (IoT)
		  is to enable end-users without technical experience to use,
		  control or monitor smart devices. However, enabling
		  end-users to interact with these smart devices in an
		  intuitive and natural way becomes increasingly important as
		  they become more pervasive in our homes, workplaces and
		  public environments. Voice-based interfaces are the
		  emerging trend to provide a more natural human-device
		  interaction in smart environments. Such interfaces require
		  Natural Language Understanding (NLU) approaches to identify
		  the meaning of end-users' voice inputs. Designing voice
		  interfaces that are not limited to a small, fixed set of
		  pre-defined commands is far from trivial. Existing
		  voice-based solutions in the smart home domain either
		  restrict the end-users to follow a strict language pattern,
		  do not support indirect goals, require a large training
		  dataset, or need a voice assistant located in the cloud. In
		  this paper, we propose an approach for understanding
		  end-users goals from voice inputs in smart homes. Our
		  approach alleviates the need for end-users to learn or
		  remember concrete operations of the devices and specific
		  words/pattern structures rather it enables them to control
		  their smart homes based on the desired goals (effects). We
		  evaluate the approach through application to a collection
		  of 253 goals from real end-users and report on quality
		  metrics. The results demonstrate that our solution provides
		  a good accuracy, high precision and acceptable recall for
		  understanding end-users goals in the smart home domain.},
  booktitle	= {Proceedings of the 10th International Conference on the
		  Internet of Things},
  articleno	= {1},
  numpages	= {8},
  keywords	= {goal recognition, internet of things, natural language
		  understanding, smart home, voice interface},
  location	= {Malm\"{o}, Sweden},
  series	= {IoT '20}
}

@InProceedings{	  10.1145/3613372.3613380,
  author	= {Santos J\'{u}nior, Paulo S\'{e}rgio and Almeida, Jo\~{a}o
		  Paulo A. and Barcellos, Monalessa},
  title		= {Towards Federated Ontology-Driven Data Integration in
		  Continuous Software Engineering},
  year		= {2023},
  isbn		= {9798400707872},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613372.3613380},
  doi		= {10.1145/3613372.3613380},
  abstract	= {Organizations have adopted Continuous Software Engineering
		  (CSE) practices aiming at making software development
		  faster, iterative, integrated, continuous, and aligned with
		  the business. In this context, they often use different
		  applications (e.g., project management tools, source
		  repositories, and quality assessment tools) that store
		  valuable data to support daily activities and
		  decision-making. However, data items often remain spread in
		  different applications that adopt different data and
		  behavioral models, posing a barrier to integrated data
		  usage. As a consequence, data-driven software development
		  is uncommon, missing valuable opportunities for product and
		  process improvement. In this paper, we explore an ontology
		  network addressing CSE aspects to develop a data
		  integration solution in which networked ontologies are the
		  basis to build reusable and autonomous software components
		  that work together in a system federation to provide
		  meaningful integrated data. We achieve a comprehensive and
		  flexible solution that can be used as a whole or partially,
		  by extracting only the components related to the subdomains
		  of interest.},
  booktitle	= {Proceedings of the XXXVII Brazilian Symposium on Software
		  Engineering},
  pages		= {31–36},
  numpages	= {6},
  keywords	= {Continuous Software Engineering, Data Integration,
		  Ontology},
  location	= {Campo Grande, Brazil},
  series	= {SBES '23}
}

@InBook{	  10.1145/3191315.3191321,
  author	= {De Cat, Broes and Bogaerts, Bart and Bruynooghe, Maurice
		  and Janssens, Gerda and Denecker, Marc},
  title		= {Predicate logic as a modeling language: the IDP system},
  year		= {2018},
  isbn		= {9781970001990},
  publisher	= {Association for Computing Machinery and Morgan \&amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3191315.3191321},
  booktitle	= {Declarative Logic Programming: Theory, Systems, and
		  Applications},
  pages		= {279–323},
  numpages	= {45}
}

@InProceedings{	  10.1145/3699682.3728348,
  author	= {Sampaio de Alencar, Rafaella and Demirtas, Mehmet Arif and
		  Saha, Adittya Soukarjya and Shi, Yang and Brusilovsky,
		  Peter},
  title		= {Integrating Expert Knowledge With Automated Knowledge
		  Component Extraction for Student Modeling},
  year		= {2025},
  isbn		= {9798400713132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3699682.3728348},
  doi		= {10.1145/3699682.3728348},
  abstract	= {Knowledge tracing is a method to model students’
		  knowledge and enable personalized education in many STEM
		  disciplines such as mathematics and physics, but has so far
		  still been a challenging task in computing disciplines. One
		  key obstacle to successful knowledge tracing in computing
		  education lies in the accurate extraction of knowledge
		  components (KCs), since multiple intertwined KCs are
		  practiced at the same time for programming problems. In
		  this paper, we address the limitations of current methods
		  and explore a hybrid approach for KC extraction, which
		  combines automated code parsing with an expert-built
		  ontology. We use an introductory (CS1) Java benchmark
		  dataset to compare its KC extraction performance with the
		  traditional extraction methods using a state-of-the-art
		  evaluation approach based on learning curves. Our
		  preliminary results show considerable improvement over
		  traditional methods of student modeling. The results
		  indicate the opportunity to improve automated KC extraction
		  in CS education by incorporating expert knowledge into the
		  process.},
  booktitle	= {Proceedings of the 33rd ACM Conference on User Modeling,
		  Adaptation and Personalization},
  pages		= {307–312},
  numpages	= {6},
  keywords	= {knowledge components, computing education, student
		  modeling, intelligent tutoring systems, learning curves},
  location	= { },
  series	= {UMAP '25}
}

@InProceedings{	  10.1145/3688671.3688735,
  author	= {Dimitropoulos, Konstantinos and Hatzilygeroudis, Ioannis},
  title		= {An Ontology-Knowledge Graph Based Context Representation
		  Scheme for Robotic Problems},
  year		= {2024},
  isbn		= {9798400709821},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3688671.3688735},
  doi		= {10.1145/3688671.3688735},
  abstract	= {Context representation is a crucial part of a variety of
		  robotic and other applications. Context refers to the
		  environment, the (robotic or other) tasks as well as
		  human-machine/environment interactions. One of the most
		  utilized methods for representing context knowledge is
		  ontologies. An ontology offers among others a highly
		  descriptive structured representation and capabilities for
		  inconsistencies checking and querying. Another well-known
		  method is knowledge graphs, which offers a relation-based
		  visualizable structure and querying capability, which are
		  more convenient than those in ontologies. However,
		  knowledge graphs cannot check inconsistencies. Therefore,
		  we present an approach for context representation
		  development, where an ontology is first created, checked
		  and evaluated, and afterwards is converted into a knowledge
		  graph. This approach assures design and implementation of
		  more convenient and consistent context representations. The
		  approach is applied to the creation of a robotics related
		  ontology, where Prot\'{e}g\'{e} is used for ontology
		  creation and logical consistency checking (via HermiT
		  reasoner), OOPS! is used for ontology evaluation, and Neo4j
		  is used for converting the ontology to a knowledge graph.
		  Example queries in both representations show the
		  preferability to knowledge graph, while an example shows
		  the inability of knowledge graph to trace inconsistencies.},
  booktitle	= {Proceedings of the 13th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {39},
  numpages	= {7},
  keywords	= {knowledge graphs, knowledge representation, ontologies,
		  robot context representation},
  location	= { },
  series	= {SETN '24}
}

@InProceedings{	  10.1145/3404663.3406876,
  author	= {Rencis, Edgars},
  title		= {Knowledge Extraction from Healthcare Data Using
		  User-Adaptable Keywords-Based Query Language},
  year		= {2020},
  isbn		= {9781450377652},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404663.3406876},
  doi		= {10.1145/3404663.3406876},
  abstract	= {Nowadays, the volume of the information gathered by any
		  organization increases more and more rapidly. It is
		  essential to be able to use this information efficiently
		  for it to benefit the operation of the organization. There
		  is no point of gathering the information if it is not
		  converted into knowledge. The knowledge extraction process
		  becomes the backbone of any successful organization.
		  Moreover, the extraction of the knowledge must be quick and
		  efficient, so that the newly-obtained knowledge can be put
		  in use at once. The problem addressed in this paper is how
		  to allow the domain expert to extract the knowledge from
		  their information systems themselves without involving the
		  third party in the form of an IT specialist. This goal is
		  of utmost importance for the domain experts, e.g. hospital
		  managers and physicians, because they need to make
		  decisions based on the available knowledge and to do it
		  rapidly and efficiently. We propose a system in this paper
		  that allows formulating queries in the natural language and
		  that also adapts to the specifics of the user. Our
		  experiments show that such kind of querying could provide
		  an improvement in the decision-making process of healthcare
		  professionals.},
  booktitle	= {Proceedings of the 2020 the 4th International Conference
		  on Information System and Data Mining},
  pages		= {128–131},
  numpages	= {4},
  keywords	= {Natural language processing, hospital management,
		  keywords-containing text, knowledge extraction, query
		  language, query translation},
  location	= {Hawaii, HI, USA},
  series	= {ICISDM '20}
}

@Article{	  10.1145/3439735,
  author	= {Hiebel, Gerald and Asp\"{o}ck, Edeltraud and Kopetzky,
		  Karin},
  title		= {Ontological Modeling for Excavation Documentation and
		  Virtual Reconstruction of an Ancient Egyptian Site},
  year		= {2021},
  issue_date	= {July 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3439735},
  doi		= {10.1145/3439735},
  abstract	= {In this article we introduce our semantic modeling
		  approach for data from over 50 years of excavations at Tell
		  el-Daba in Egypt. The CIDOC CRM with some of its extensions
		  is used as an ontological framework to provide the
		  semantics for creating a knowledge graph containing
		  material remains, excavated areas, and documentation
		  resources. An objective of the project A Puzzle in 4D is to
		  digitize the documentation and create metadata for analog
		  and digital resources in order to provide the data to the
		  research community and facilitate future work for this
		  important archaeological site. Using an example of 3D
		  reconstruction of a tomb, we show how the knowledge graph
		  linked to digital resources can be exploited for a specific
		  task to encounter available information that is essential
		  for a virtual reconstruction. Moreover, we show an approach
		  of modeling to represent the interpretations supporting
		  reconstructions as well as relate them to the sources used,
		  thus providing transparency for the model and provenance
		  data. Modeling for excavation documentation as well as
		  virtual reconstruction has been tailored to the large
		  amount of data processed from the project. The goal is to
		  propose a semantic modeling feasible even on a large scale
		  while still preserving the basic underlying ontological
		  structures.},
  journal	= {J. Comput. Cult. Herit.},
  month		= jul,
  articleno	= {32},
  numpages	= {14},
  keywords	= {ARIADNE, CIDOC CRM, Cultural heritage, archaeology,
		  interpretation, virtual reconstruction}
}

@Article{	  10.1145/3719291,
  author	= {Diamantini, Claudia and Khan, Tarique and Potena, Domenico
		  and Storti, Emanuele},
  title		= {Semantic Models of Performance Indicators: A Systematic
		  Survey},
  year		= {2025},
  issue_date	= {August 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {8},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3719291},
  doi		= {10.1145/3719291},
  abstract	= {Performance indicators and metrics are essential
		  management tools. They provide synthetic objective measures
		  to monitor the progress of a process, set objectives, and
		  assess deviations, enabling effective decision-making. They
		  can also be used for communication purposes, facilitating
		  the sharing of objectives and results or improving the
		  awareness of certain phenomena, thus motivating more
		  responsible and sustainable behaviors. Given their
		  strategic role, it is of paramount importance, as well as
		  challenging, to guarantee that the intended meaning of an
		  indicator is fully shared among stakeholders and that its
		  implementation is aligned with the definition provided by
		  decision makers, as this is a precondition for data quality
		  and trustworthiness of the information system. Formal
		  models, such as ontologies, have been long investigated in
		  the literature to address the issues. This article proposes
		  a comprehensive survey on semantic approaches aimed to
		  specify conceptual definitions of indicators and metrics,
		  illustrating also the advantages of these formal approaches
		  in relevant use cases and application domains.},
  journal	= {ACM Comput. Surv.},
  month		= mar,
  articleno	= {202},
  numpages	= {37},
  keywords	= {KPI, performance indicators, indicators, semantic models,
		  challenges}
}

@InProceedings{	  10.1145/3397056.3397083,
  author	= {Liu, Zhengjun and Sun, Zhi and Chen, Jianfeng and Zhou,
		  Yujin and Yang, Tao and Yang, Hui and Liu, Jie},
  title		= {STIX-based Network Security Knowledge Graph Ontology
		  Modeling Method},
  year		= {2020},
  isbn		= {9781450377416},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397056.3397083},
  doi		= {10.1145/3397056.3397083},
  abstract	= {Network security incidents are complex and unstructured,
		  making them difficult to understand and share. In this
		  paper, we analyzes the commonality between structured
		  threat information representation (STIX) and network
		  security domain knowledge, and proposes a knowledge graph
		  ontology modeling method of network security based on STIX.
		  With the architecture knowledge of STIX, this method
		  generates an ontology schema of network security knowledge
		  graph, through classifying the concepts in the field of
		  network security, describing the attributes of concepts and
		  combing the relationships between concepts. The ontology
		  schema has small redundancy and strong structural
		  hierarchy, and can clearly display the structure of the
		  attack activity and the mutual relationship. Therefore, it
		  can help decision makers to understand security incidents
		  more deeply, and help them make reasonable decisions and
		  share cyber threat intelligence.},
  booktitle	= {Proceedings of the 2020 3rd International Conference on
		  Geoinformatics and Data Analysis},
  pages		= {152–157},
  numpages	= {6},
  keywords	= {Knowledge Graph, Network Security, Ontology, STIX},
  location	= {Marseille, France},
  series	= {ICGDA '20}
}

@InProceedings{	  10.1145/3350546.3352558,
  author	= {Noura, Mahda and Gaedke, Martin},
  title		= {WoTDL: Web of Things Description Language for Automatic
		  Composition},
  year		= {2019},
  isbn		= {9781450369343},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3350546.3352558},
  doi		= {10.1145/3350546.3352558},
  abstract	= {Enabling end users to take a proactive role in the
		  development of Web of Things (WoT) applications that
		  achieves their goals is a challenge for End User
		  Development (EUD) in the context of WoT. This can be
		  achieved through Artificial Intelligence (AI) planning
		  algorithms if the relevant WoT concepts and relationships
		  are described in an interoperable way. Although similar,
		  existing service description languages like WSDL or
		  ontologies like OWL-S are not sufficient to represent all
		  required characteristics of concrete WoT planning
		  scenarios. To address these limitations, in this paper we
		  present the Web of Things Description Language (WoTDL)
		  ontology which is an extension of existing WoT models to
		  describe the key concepts of AI planning for automatic WoT
		  composition. To demonstrate the feasibility of our
		  approach, we follow the best practices recommended by the
		  semantic web community and describe the physical devices of
		  our smart home testbed in an AI planning scenario using
		  WoTDL.},
  booktitle	= {IEEE/WIC/ACM International Conference on Web
		  Intelligence},
  pages		= {413–417},
  numpages	= {5},
  keywords	= {AI Planning, Automatic Composition, Internet of Things,
		  Ontology, Semantic Web, Web of Things},
  location	= {Thessaloniki, Greece},
  series	= {WI '19}
}

@InProceedings{	  10.1145/3696410.3714573,
  author	= {Tsai, Elisa and Mangaokar, Neal and Zheng, Boyuan and
		  Zheng, Haizhong and Prakash, Atul},
  title		= {Harmful Terms and Where to Find Them: Measuring and
		  Modeling Unfavorable Financial Terms and Conditions in
		  Shopping Websites at Scale},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696410.3714573},
  doi		= {10.1145/3696410.3714573},
  abstract	= {Terms and conditions for online shopping websites often
		  contain terms that can have significant financial
		  consequences for customers. Despite their impact, there is
		  currently no comprehensive understanding of the types and
		  potential risks associated with unfavorable financial
		  terms. Furthermore, there are no publicly available
		  detection systems or datasets to systematically identify or
		  mitigate these terms. In this paper, we take the first
		  steps toward solving this problem with three key
		  contributions.First, we introduce TermMiner, an automated
		  data collection and topic modeling pipeline to understand
		  the landscape of unfavorable financial terms. Second, we
		  create ShopTC-100K, a dataset of terms and conditions from
		  shopping websites in the Tranco top 100K list, comprising
		  1.8 million terms from 8,251 websites. Consequently, we
		  develop a taxonomy of 22 types from 4 categories of
		  unfavorable financial terms---spanning purchase,
		  post-purchase, account termination, and legal aspects.
		  Third, we build TermLens, an automated detector that uses
		  Large Language Models (LLMs) to identify unfavorable
		  financial terms.Fine-tuned on an annotated dataset,
		  TermLens achieves an F1 score of 94.6\% and a false
		  positive rate of 2.3\% using GPT-4o. When applied to
		  shopping websites from the Tranco top 100K, we find that
		  42.06\% of these sites contain at least one unfavorable
		  financial term, with such terms being more prevalent on
		  less popular websites. Case studies further highlight the
		  financial risks and customer dissatisfaction associated
		  with unfavorable financial terms, as well as the
		  limitations of existing ecosystem defenses.},
  booktitle	= {Proceedings of the ACM on Web Conference 2025},
  pages		= {990–1003},
  numpages	= {14},
  keywords	= {consumer protection, deceptive content, terms and
		  conditions dataset, topic modeling, unfavorable financial
		  terms},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3631700.3665234,
  author	= {Carta, Salvatore and Giuliani, Alessandro and Manca, Marco
		  Manolo and Piano, Leonardo and Tiddia, Sandro Gabriele},
  title		= {Towards Zero-shot Knowledge Graph building: Automated
		  Schema Inference},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3665234},
  doi		= {10.1145/3631700.3665234},
  abstract	= {In the current Digital Transformation scenario, Knowledge
		  Graphs are essential for comprehending, representing, and
		  exploiting complex information in a structured form. The
		  main paradigm in automatically generating proper Knowledge
		  Graphs relies on predefined schemas or ontologies. Such
		  schemas are typically manually constructed, requiring an
		  intensive human effort, and are often sensitive to
		  information loss due to negligence, incomplete analysis, or
		  human subjectivity or inclination. Limiting human bias and
		  the resulting information loss in creating proper Knowledge
		  Graphs is paramount, particularly for user modeling in
		  various sectors, such as education or healthcare. To this
		  end, we propose a novel approach to automatically
		  generating a proper entity schema. The devised methodology
		  combines the language understanding capabilities of LLM
		  with classical machine learning methods such as clustering
		  to properly build an entity schema from a set of documents.
		  This solution eliminates the need for human intervention
		  and fosters a more efficient and comprehensive knowledge
		  representation. The assessment of our proposal concerns
		  adopting a state-of-the-art entity extraction model
		  (UniNER) to estimate the relevance of the extracted
		  entities based on the generated schema. Results confirm the
		  potential of our approach, as we observed a negligible
		  difference between the topic similarity score obtained with
		  the ground truth and with the automatically generated
		  schema (less than 1\% on average on three different
		  datasets). Such an outcome confirms that the proposed
		  approach may be valuable in automatically creating an
		  entity schema from a set of documents.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {467–473},
  numpages	= {7},
  keywords	= {Large Language Models, Named Entity Recognition, Ontology
		  Learning},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@InProceedings{	  10.1145/3652620.3688567,
  author	= {Cederbladh, Johan and Zimmermann, Thomas C.},
  title		= {How does one Model Appropriately in Systems Engineering?
		  An Initial Conceptual Model Framing Model Appropriateness},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688567},
  doi		= {10.1145/3652620.3688567},
  abstract	= {Appropriateness of models through modelling languages,
		  tools, and methods is at the core of systems modelling. A
		  concrete example is that useful abstraction depends on the
		  contextual case of modelling purpose and need. Naturally,
		  this results in different modelling formalisms and
		  languages being appropriate at different stages of a common
		  development process and for different stakeholders. In
		  Model-Based Systems Engineering (MBSE) many stakeholders
		  are involved in procedural modelling activities.
		  Consequently, there is a need to identify appropriate
		  modelling approaches at each modelling activity and
		  development stage. Current MBSE adoption and application is
		  still in early stages, and consequently lacking in overall
		  modelling contextualisation. In this paper we discuss what
		  facilitates an appropriate systems engineering model and
		  how practitioners can reason about models in industrial
		  contexts by providing an initial conceptual model of how
		  model artefacts support governing systems engineering
		  concerns.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {930–934},
  numpages	= {5},
  keywords	= {appropriate, model-based, systems engineering,
		  decision-making},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3654522.3654607,
  author	= {Nguyen, Dien Thanh and Do, Nhon Van and Tran, Tung Hoang},
  title		= {Ontology-Based Solution for Designing knowledge Retrieval
		  Systems in the Field of Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400716713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3654522.3654607},
  doi		= {10.1145/3654522.3654607},
  abstract	= {Currently, using domain knowledge and semantics in the
		  field of artificial intelligence to conduct designing
		  knowledge retrieval system has attracted great attention
		  from researchers in many different communities. In this
		  paper, we have improved the previous CK_ONTO model to
		  improve the semantic representation technique of documents
		  more effectively. This improved model is used for designing
		  the knowledge querying system based on the ontology-base in
		  education. Its foundation includes a model of concept
		  relations between concepts and rules of the knowledge
		  domain. Besides, we have proposed techniques and algorithms
		  on New_Onto that are presented to solve the problems in
		  this paper. Experimental results show that the knowledge
		  query system works more effectively than the previous
		  knowledge query system and is suitable for students who
		  want to search for artificial intelligence documents for
		  their learning and studying and is emerging for use in the
		  real-world.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Intelligent Information Technology},
  pages		= {558–563},
  numpages	= {6},
  keywords	= {Additional Key Words and Phrases: Ontology-base,
		  artificial intelligence, document representation, graph
		  matching, semantic search},
  location	= {Ho Chi Minh City, Vietnam},
  series	= {ICIIT '24}
}

@InProceedings{	  10.1145/3587259.3627574,
  author	= {Ram\'{o}n-Ferrer, Virginia and Badenes-Olmedo, Carlos and
		  Corcho, Oscar},
  title		= {Automatic Topic Label Generation using Conversational
		  Models},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627574},
  doi		= {10.1145/3587259.3627574},
  abstract	= {In probabilistic topic models, a topic is characterised by
		  a set of words, with a probability associated to each of
		  them. Even though it is not necessary to understand the
		  meaning of topics to perform common downstream tasks where
		  topic models are used, such as topic inference or document
		  similarity, there have been attempts to uncover the
		  semantics of topics by providing labels to them, consisting
		  in a couple of concepts. In this paper we propose a
		  methodology, Conversational Probabilistic Topic Labelling
		  (CPTL), to study whether conversational models can be used
		  to generate labels that describe probabilistic topics given
		  their most representative keywords. We evaluate and compare
		  the performance of a selection of conversational models for
		  the topic label generation task with the performance of a
		  task-specific language model trained to generate topic
		  labels.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {17–24},
  numpages	= {8},
  keywords	= {conversational model, probabilistic topic labelling, topic
		  label, topic label generation},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3551902.3551983,
  author	= {Waseeb, Shakirullah and Vrani\'{c}, Valentino},
  title		= {Toward Organizational Pattern Ontology},
  year		= {2023},
  isbn		= {9781450395946},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3551902.3551983},
  doi		= {10.1145/3551902.3551983},
  abstract	= {Organizational patterns of agile software development are
		  proven practices for dealing organizational principles.
		  Finding and selecting the right pattern is difficult. One
		  way to select a pattern is to follow the sequence and
		  compositions given in a pattern language. However, in
		  general, pattern languages do not always reveal every
		  reliable connection. Patterns are described in informal and
		  unstructured text, making it difficult to understand their
		  connections. This work attempts to provide a conceptual
		  ontology model for organizational patterns – describing
		  the collection of concepts (terms) and their relations.
		  This contribution can be an attempt to express, expose, and
		  share semantic knowledge between patterns using ontology.
		  The resulting ontology can be used on top of the
		  organizational patterns repositories to help retrieve
		  patterns based on their logical connections.},
  booktitle	= {Proceedings of the 27th European Conference on Pattern
		  Languages of Programs},
  articleno	= {20},
  numpages	= {8},
  keywords	= {knowledge base, ontology, organizational patterns,
		  semantics},
  location	= {Irsee, Germany},
  series	= {EuroPLop '22}
}

@InProceedings{	  10.1145/3664476.3670916,
  author	= {Jorquera Valero, Jos\'{e} Mar\'{\i}a and L\'{o}pez
		  Mart\'{\i}nez, Antonio and S\'{a}nchez S\'{a}nchez, Pedro
		  Miguel and Navarro Mart\'{\i}nez, Daniel and Varas
		  L\'{o}pez, Rodrigo and Rojo Lacal, Javier Ignacio and
		  L\'{o}pez Vivar, Antonio and Sotelo Monge, Marco Antonio
		  and Gil P\'{e}rez, Manuel and Mart\'{\i}nez P\'{e}rez, Gregorio},
  title		= {Unlocking the Potential of Knowledge Graphs: A Cyber
		  Defense Ontology for a Knowledge Representation and
		  Reasoning System},
  year		= {2024},
  isbn		= {9798400717185},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664476.3670916},
  doi		= {10.1145/3664476.3670916},
  abstract	= {In today’s dynamic and complex warfare landscape,
		  characterized by the convergence of traditional and
		  emerging threats, the significance of cybersecurity in
		  shaping modern conflicts cannot be overstated. Such trend
		  presents a challenging paradigm shift in how military
		  organizations approach mosaic warfare in the digital age
		  since new attack vectors and targets appear in their
		  landscapes. In this vein, it is pivotal for military teams
		  to have a clear and concise roadmap for cybersecurity
		  incidents linked to potential mosaic warfare. This
		  manuscript introduces a novel approach to bolstering mosaic
		  warfare strategies by integrating an advanced Knowledge
		  Representation and Reasoning system and a tailored
		  ontology. Motivated by the critical role of cybersecurity
		  in contemporary warfare, the proposed system aims to
		  enhance situational awareness, decision-making
		  capabilities, and operational effectiveness in the face of
		  evolving cyber threats. In this sense, this manuscript
		  entails a new ontology that not only covers the
		  cybersecurity realm but also introduces key concepts
		  related to strategic and operational military levels at the
		  same time. The ad-hoc ontology is also compared against
		  other well-known ones, such as MITRE, NATO, or UCO
		  approaches and manifests a significant performance by
		  employing standardized quality metrics for ontologies.
		  Lastly, a realistic mosaic warfare scenario is
		  contextualized to demonstrate the deployment of the
		  proposed system and how it can properly represent all
		  information gathered from heterogeneous data sources.},
  booktitle	= {Proceedings of the 19th International Conference on
		  Availability, Reliability and Security},
  articleno	= {73},
  numpages	= {9},
  keywords	= {Cyber defense, Knowledge graph, Knowledge representation,
		  Mosaic warfare, Ontology, Reasoning},
  location	= {Vienna, Austria},
  series	= {ARES '24}
}

@Article{	  10.1145/3626254,
  author	= {Bikakis, Antonis and Ferrario, Roberta and Jean,
		  St\'{e}phane and Markhoff, B\'{e}atrice and Mosca,
		  Alessandro and Asmundo, Marianna Nicolosi},
  title		= {Editorial: Special Issue on Semantic Web and Ontology
		  Design for Cultural Heritage},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3626254},
  doi		= {10.1145/3626254},
  journal	= {J. Comput. Cult. Herit.},
  month		= nov,
  articleno	= {54},
  numpages	= {5},
  keywords	= {Ontologies, Knowledge Graphs, Cultural Heritage, Digitial
		  Humanities}
}

@Proceedings{	  10.1145/3708319,
  title		= {UMAP Adjunct '25: Adjunct Proceedings of the 33rd ACM
		  Conference on User Modeling, Adaptation and
		  Personalization},
  year		= {2025},
  isbn		= {9798400713996},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3609484,
  author	= {Wei, Tong and Chen, Yuqi},
  title		= {A Ding Ontology of Chinese Bronze},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3609484},
  doi		= {10.1145/3609484},
  abstract	= {Ding is a significant type of Chinese bronze that holds
		  key cultural value. Traditional humanists have primarily
		  focused on dating and classifying Ding. However, in the
		  context of Digital Humanities, the research perspective of
		  humanities scholars is gradually shifting towards
		  data-driven research, with linked data emerging as a
		  popular topic. A well-defined and standard ontology
		  representing the complete domain knowledge is essential for
		  linked Ding data. Unfortunately, most existing ontology
		  cannot represent fine-grained knowledge of Ding or is too
		  restrictive to represent partial knowledge of bronze Ding.
		  In this context, we propose a fine-grained Ding ontology to
		  represent the bronze Ding knowledge. In this paper, we
		  present in detail the Ding ontology of Chinese bronze
		  during the Shang and Zhou dynasties (from 1600 BC to 256
		  BC). We provide a detailed exposition of the Ding ontology
		  and evaluate its effectiveness using OOPS!, OntoMetrics,
		  and by answering competency questions in SPARQL. The
		  building methodology of Ding ontology follows the ISO
		  principles (ISO 1087 and ISO 704). The objective of this
		  paper is to develop an open ontology of Ding during the
		  Shang and Zhou dynasties, which can serve as a valuable
		  resource for bilingual terminology dictionaries. The Ding
		  ontology was published at .},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {69},
  numpages	= {12},
  keywords	= {Cultural heritage, Ontology, Terminology, Digital
		  humanities, Semantic Web}
}

@Article{	  10.1145/3650041,
  author	= {Sekuli\'{c}, Ivan and Alinannejadi, Mohammad and Crestani,
		  Fabio},
  title		= {Analysing Utterances in LLM-Based User Simulation for
		  Conversational Search},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {3},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3650041},
  doi		= {10.1145/3650041},
  abstract	= {Clarifying underlying user information needs by asking
		  clarifying questions is an important feature of modern
		  conversational search systems. However, evaluation of such
		  systems through answering prompted clarifying questions
		  requires significant human effort, which can be
		  time-consuming and expensive. In our recent work, we
		  proposed an approach to tackle these issues with a user
		  simulator, USi. Given a description of an information need,
		  USi is capable of automatically answering clarifying
		  questions about the topic throughout the search session.
		  However, while the answers generated by USi are both in
		  line with the underlying information need and in natural
		  language, a deeper understanding of such utterances is
		  lacking. Thus, in this work, we explore utterance
		  formulation of large language model (LLM)–based user
		  simulators. To this end, we first analyze the differences
		  between USi, based on GPT-2, and the next generation of
		  generative LLMs, such as GPT-3. Then, to gain a deeper
		  understanding of LLM-based utterance generation, we compare
		  the generated answers to the recently proposed set of
		  patterns of human-based query reformulations. Finally, we
		  discuss potential applications as well as limitations of
		  LLM-based user simulators and outline promising directions
		  for future work on the topic.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= may,
  articleno	= {62},
  numpages	= {22},
  keywords	= {User simulation, conversational search, mixed-initiative}
}

@InProceedings{	  10.1145/3534678.3539258,
  author	= {Liu, Fenglin and Yang, Bang and You, Chenyu and Wu, Xian
		  and Ge, Shen and Woicik, Adelaide and Wang, Sheng},
  title		= {Graph-in-Graph Network for Automatic Gene Ontology
		  Description Generation},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539258},
  doi		= {10.1145/3534678.3539258},
  abstract	= {Gene Ontology (GO) is the primary gene function knowledge
		  base that enables computational tasks in biomedicine. The
		  basic element of GO is a term, which includes a set of
		  genes with the same function. Existing research efforts of
		  GO mainly focus on predicting gene term associations. Other
		  tasks, such as generating descriptions of new terms, are
		  rarely pursued. In this paper, we propose a novel task: GO
		  term description generation. This task aims to
		  automatically generate a sentence that describes the
		  function of a GO term belonging to one of the three
		  categories, i.e., molecular function, biological process,
		  and cellular component. To address this task, we propose a
		  Graph-in-Graph network that can efficiently leverage the
		  structural information of GO. The proposed network
		  introduces a two-layer graph: the first layer is a graph of
		  GO terms where each node is also a graph (gene graph). Such
		  a Graph-in-Graph network can derive the biological
		  functions of GO terms and generate proper descriptions. To
		  validate the effectiveness of the proposed network, we
		  build three large-scale benchmark datasets. By
		  incorporating the proposed Graph-in-Graph network, the
		  performances of seven different sequence-to-sequence models
		  can be substantially boosted across all evaluation metrics,
		  with up to 34.7\%, 14.5\%, and 39.1\% relative improvements
		  in BLEU, ROUGE-L, and METEOR, respectively.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1060–1068},
  numpages	= {9},
  keywords	= {bioinformatics, gene ontology, graph representations,
		  natural language generation, sequence-to-sequence
		  learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3316615.3316688,
  author	= {Marurngsith, Worawan and Weawsawangwong, Pakorn},
  title		= {Applying Formal Logic Validation to Enhance Natural
		  Language Understanding},
  year		= {2019},
  isbn		= {9781450365734},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3316615.3316688},
  doi		= {10.1145/3316615.3316688},
  abstract	= {Inconsistencies and ambiguities of annotation can cause
		  vagueness in the results obtained by natural language
		  understanding (NLU). The quality of the type systems used
		  for annotation affects the quality of annotation. To
		  achieve highly accepted sets of annotated documents, the
		  Fleiss' kappa score has been widely used to observe the
		  level of agreement from annotated results, submitted by
		  different human annotators. The challenge is that the kappa
		  score cannot be used to validate the type systems nor to
		  identify any incorrect annotations. Thus, we proposed an
		  application of formal logic for validating type systems and
		  annotations against expert rules. Experiments have been
		  done by using four different type systems and annotation
		  sets created by an expert and three novices. Our proposed
		  formal logic model was used to validate the novice type
		  systems and annotations against the expert rules. The
		  results show that the technique could help identifying
		  inconsistencies between expert and novice annotations, by
		  using a model checker. The number of detected
		  inconsistencies impacts the level of achieved F1 score.
		  Thus, the proposed formal logic technique could be used to
		  guide novice annotators to develop accepted type systems.
		  This will help to enhance the performance of the generated
		  machine learning models used by the NLU.},
  booktitle	= {Proceedings of the 2019 8th International Conference on
		  Software and Computer Applications},
  pages		= {380–384},
  numpages	= {5},
  keywords	= {IBM Watson, Inconsistency detection, Natural language
		  understanding, Validation},
  location	= {Penang, Malaysia},
  series	= {ICSCA '19}
}

@InProceedings{	  10.1145/3424616.3424715,
  author	= {Brutzman, Don and Floty\'{n}ski, Jakub},
  title		= {X3D Ontology for Querying 3D Models on the Semantic Web},
  year		= {2020},
  isbn		= {9781450381697},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3424616.3424715},
  doi		= {10.1145/3424616.3424715},
  abstract	= {The Semantic Web offers significant capabilities that
		  transform the current Web into a global knowledge base
		  including various cross-linked multimedia content with
		  formal descriptions of its semantics understandable to
		  humans and processable by computers. Content on the
		  Semantic Web can be subject to reasoning and queries with
		  standardized languages, methods and tools, which opens new
		  opportunities for collaborative creation, use and
		  exploration of web repositories. However, these
		  opportunities have not been exploited so far by the
		  available 3D formats and modeling tools, which limits the
		  possibilities of search and reuse of 3D content as part of
		  the Semantic Web. This work contributes a semantic
		  development pipeline of the X3D Ontology, with
		  corresponding conversion of X3D models into triple forms
		  suitable for formal query. The ontology design reflects
		  experience accompanying the development of the Extensible
		  3D (X3D) Graphics International Standard, in particular,
		  the X3D Unified Object Model (X3DUOM). This approach
		  combines semantic and syntactic elements of X3D models and
		  metadata to support integration with the Semantic Web. The
		  pipeline enables automatic generation of the X3D Ontology,
		  thereby providing an up-to-date 3D representation with
		  semantics during X3D specification development. By
		  extending commonplace model conversions from other formats
		  to X3D, the ontology presents the potential to enable
		  integration of most forms of 3D content with the Semantic
		  Web.},
  booktitle	= {Proceedings of the 25th International Conference on 3D Web
		  Technology},
  articleno	= {14},
  numpages	= {6},
  keywords	= {Semantic 3D, Semantic Web, Web3D, X3D Ontology, X3DUOM},
  location	= {Virtual Event, Republic of Korea},
  series	= {Web3D '20}
}

@InProceedings{	  10.5555/3492252.3492254,
  author	= {Garc\'{e}s, Lina and Sena, Bruno and Nakagawa, Elisa Y.},
  title		= {Towards an architectural patterns language for
		  systems-of-systems},
  year		= {2021},
  publisher	= {The Hillside Group},
  address	= {USA},
  abstract	= {Systems-of-Systems (SoS) architectures are inherently
		  dynamic; hence, they must support continuous modification
		  in the behaviour and configuration of these systems at
		  runtime as a result of changes in the environment, new SoS
		  missions, and failures or unavailability of constituents.
		  Modifications should occur without affecting the integrity
		  of constituents, the accomplishment of SoS missions,
		  neither their reliability, security, safety, nor other
		  quality attributes. Architecting SoS requires then
		  important investments in human, time, and economic
		  resources, bringing big challenges. Architectural patterns
		  have been widely used to improve software architecture
		  quality, decreasing costs of design and promoting reuse of
		  good practices and well-known solutions. Nowadays, a great
		  amount of architectural patterns is available; most of them
		  are domain-independent, which harness their selection in
		  specific software projects. The main goal of this paper is
		  to contribute to reduce the work of architects during the
		  choice of better architectural patterns for their SoS. For
		  this, we present a set of patterns that are commonly used
		  to construct such architectures. Additionally, benefits of
		  adopting these patterns are described. To demonstrate their
		  feasibility, we present HSH-SoS, a pattern-base
		  architecture for SoS that presents how different patterns
		  can interact to create a solution for SoS. As results, the
		  architectural patterns reported in this work are feasible
		  candidates to compose a language for recurrent problems in
		  SoS architectures. However, formalization of such language
		  is an open issue that we intend to address in future
		  works.},
  booktitle	= {Proceedings of the 26th Conference on Pattern Languages of
		  Programs},
  articleno	= {1},
  numpages	= {24},
  keywords	= {architectural pattern, architectural patterns, patterns
		  language, software architecture, systems-of-systems},
  location	= {Urbana, Illinois},
  series	= {PLoP '19}
}

@Article{	  10.1145/3673226,
  author	= {Uhrmacher, Adelinde M and Frazier, Peter and H\"{a}hnle,
		  Reiner and Kl\"{u}gl, Franziska and Lorig, Fabian and
		  Lud\"{a}scher, Bertram and Nenzi, Laura and Ruiz-Martin,
		  Cristina and Rumpe, Bernhard and Szabo, Claudia and Wainer,
		  Gabriel and Wilsdorf, Pia},
  title		= {Context, Composition, Automation, and Communication: The
		  C2AC Roadmap for Modeling and Simulation},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {34},
  number	= {4},
  issn		= {1049-3301},
  url		= {https://doi.org/10.1145/3673226},
  doi		= {10.1145/3673226},
  abstract	= {Simulation has become, in many application areas, a sine
		  qua non. Most recently, COVID-19 has underlined the
		  importance of simulation studies and limitations in current
		  practices and methods. We identify four goals of
		  methodological work for addressing these limitations. The
		  first is to provide better support for capturing,
		  representing, and evaluating the context of simulation
		  studies, including research questions, assumptions,
		  requirements, and activities contributing to a simulation
		  study. In addition, the composition of simulation models
		  and other simulation studies’ products must be supported
		  beyond syntactical coherence, including aspects of
		  semantics and purpose, enabling their effective reuse. A
		  higher degree of automating simulation studies will
		  contribute to more systematic, standardized simulation
		  studies and their efficiency. Finally, it is essential to
		  invest increased effort into effectively communicating
		  results and the processes involved in simulation studies to
		  enable their use in research and decision making. These
		  goals are not pursued independently of each other, but they
		  will benefit from and sometimes even rely on advances in
		  other sub-fields. In this article, we explore the basis and
		  interdependencies evident in current research and practice
		  and delineate future research directions based on these
		  considerations.},
  journal	= {ACM Trans. Model. Comput. Simul.},
  month		= aug,
  articleno	= {23},
  numpages	= {51},
  keywords	= {Modeling, simulation, state of the art, open challenges,
		  reuse, composition, communication, reproducibility,
		  automation, intelligent modeling and simulation lifecycle}
}

@InProceedings{	  10.1145/3325917.3325948,
  author	= {Rencis, Edgars},
  title		= {Natural Language-Based Knowledge Extraction in Healthcare
		  Domain},
  year		= {2019},
  isbn		= {9781450366359},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3325917.3325948},
  doi		= {10.1145/3325917.3325948},
  abstract	= {There is a growing amount of data in the databases of
		  hospitals. These data could be exploited to alleviate the
		  decision-making process of hospital managers, physicians
		  and researchers. However, these types of end-users often
		  lack the expertise necessary for extracting those data from
		  the database. Several approaches exist in the field of how
		  to allow non-programmers writing queries in a convenient
		  manner, but none of them has yet reached fully satisfactory
		  results. This paper sketches a solution to this problem by
		  introducing means for writing queries in a
		  keywords-containing natural language thus alleviating the
		  query writing process for the end-user. Introducing this
		  approach in the knowledge management system of the
		  organization would greatly benefit the domain experts by
		  allowing them to carry out the decision-making process in a
		  more rapid and less erroneous manner.},
  booktitle	= {Proceedings of the 2019 3rd International Conference on
		  Information System and Data Mining},
  pages		= {138–142},
  numpages	= {5},
  keywords	= {Natural language processing, hospital management,
		  keywords-containing text, knowledge extraction, query
		  language, query translation},
  location	= {Houston, TX, USA},
  series	= {ICISDM '19}
}

@InProceedings{	  10.1145/3638380.3638430,
  author	= {Barcham, Manuhuia},
  title		= {Decolonizing Computing and the Quest for Ontological
		  Justice - Putting Fourth-Wave HCI/IxD Into Practice},
  year		= {2024},
  isbn		= {9798400717079},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638380.3638430},
  doi		= {10.1145/3638380.3638430},
  abstract	= {This paper takes as its starting point the emergent
		  literature on issues of justice and decolonization in the
		  HCI/IxD field and their focus on the ways in which current
		  computing systems unwittingly (but perhaps knowingly)
		  perpetuate existing forms of systemic violence by ignoring
		  oppressive histories and sustained negative impacts against
		  certain groups of people. Linking this to the ontological
		  turn underway in a range of disciplines the paper then
		  looks at how these ideas open up the possibility for the
		  achievement of ontological justice for groups marginalized
		  by the colonial nature of computing. The paper then
		  explores these ideas through a discussion of the
		  experiences of New Zealand Mundefinedori hap\={u} (clans)
		  building out computing infrastructures as part of their
		  resurgence as groups. The paper ends by discussing the ways
		  in which a distinction between upstream and downstream
		  design can provide greater purchase of how we might be able
		  to bring out the shifts that are required to achieve a
		  space of ontological justice through a shift into
		  Fourth-Wave HCI/IxD.},
  booktitle	= {Proceedings of the 35th Australian Computer-Human
		  Interaction Conference},
  pages		= {486–492},
  numpages	= {7},
  keywords	= {Decolonization, Fourth-Wave HCI, Indigenous, Ontological
		  Justice},
  location	= {Wellington, New Zealand},
  series	= {OzCHI '23}
}

@InProceedings{	  10.1145/3486713.3486730,
  author	= {Koutsomitropoulos, Dimitrios},
  title		= {Validating Ontology-based Annotations of Biomedical
		  Resources using Zero-shot Learning},
  year		= {2021},
  isbn		= {9781450385107},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486713.3486730},
  doi		= {10.1145/3486713.3486730},
  abstract	= {Authoritative thesauri in the form of web ontologies offer
		  a sound representation of domain knowledge and can act as a
		  reference point for automated semantic tagging. On the
		  other hand, current language models achieve to capture
		  contextualized semantics of text corpora and can be
		  leveraged towards this goal. We present an approach for
		  injecting subject annotations using query term expansion
		  against such ontologies in the biomedical domain. For the
		  user to have an indication of the usefulness of these
		  suggestions we further propose an online method for
		  validating the quality of annotations using NLI models such
		  as BART and XLM-R. To circumvent training barriers posed by
		  very large label sets and scarcity of data we rely on
		  zero-shot classification and show that semantic matching
		  can contribute above-average thematic annotations. Also, a
		  web-based validation service can be attractive for human
		  curators vs. the overhead of pretraining large,
		  domain-tailored classification models.},
  booktitle	= {The 12th International Conference on Computational
		  Systems-Biology and Bioinformatics},
  pages		= {37–43},
  numpages	= {7},
  keywords	= {MeSH, Thesaurus, biomedical indexing, classification,
		  language models, machine learning, semantic matching},
  location	= {Virtual (GMT+7 Bangkok Time), Thailand},
  series	= {CSBio2021}
}

@Article{	  10.1145/3597304,
  author	= {Lambrix, Patrick},
  title		= {Completing and Debugging Ontologies: State-of-the-art and
		  Challenges in Repairing Ontologies},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {4},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3597304},
  doi		= {10.1145/3597304},
  abstract	= {As semantically enabled applications require high-quality
		  ontologies, developing and maintaining ontologies that are
		  as correct and complete as possible is an important
		  although difficult task in ontology engineering. A key task
		  is ontology debugging and completion. In general, there are
		  two steps: detecting defects and repairing defects. In this
		  article, we discuss the state-of-the-art regarding the
		  repairing step. We do this by formalizing the repairing
		  step as an abductive reasoning problem and situating the
		  state-of-the-art with respect to this framework. We show
		  that there are still many open research problems and show
		  opportunities for further work and advancing the field.},
  journal	= {J. Data and Information Quality},
  month		= nov,
  articleno	= {41},
  numpages	= {38},
  keywords	= {Ontology engineering, ontology debugging, ontology
		  completion, ontology alignment}
}

@InProceedings{	  10.1145/3410352.3410789,
  author	= {Aitim, A. K. and Satybaldiyeva, R. Zh. and Wojcik, W.},
  title		= {The construction of the Kazakh language thesauri in
		  automatic word processing system},
  year		= {2020},
  isbn		= {9781450377362},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3410352.3410789},
  doi		= {10.1145/3410352.3410789},
  abstract	= {In the paper presents an overview of existing electronic
		  Kazakh-language thesauri and their automatic methods of
		  construction and application. The author analyzed the main
		  characteristics of open access thesauri for scientific
		  research, evaluated the dynamics of their development and
		  effectiveness in solving problems of natural language
		  processing. Statistical and linguistic methods of thesaurus
		  construction were studied, which allow to automate the
		  development and reduce the labor costs of expert linguists.
		  It is considered algorithms for selecting key terms from
		  texts and semantic thesaurus links of all types, as well as
		  the quality of application of the resulting thesauri. For
		  illustrate the features of various methods of building
		  thesaurus links, a combined method was developed that
		  generates a specialized thesaurus completely automatically
		  based on the corpus of domain texts and several existing
		  linguistic resources.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Engineering \&amp; MIS 2020},
  articleno	= {53},
  numpages	= {4},
  keywords	= {Kazakh language, Thesaurus, agglutinative languages,
		  morphological thesauri, semantics},
  location	= {Almaty, Kazakhstan},
  series	= {ICEMIS'20}
}

@InProceedings{	  10.1145/3424953.3426540,
  author	= {Faria, Carolinne Roque e and de Barbosa, Cinthyan Renata
		  Sachs C.},
  title		= {System for identifying pests and diseases in soybean crop
		  through natural language},
  year		= {2020},
  isbn		= {9781450381727},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3424953.3426540},
  doi		= {10.1145/3424953.3426540},
  abstract	= {The presence of technologies in the agronomic field has
		  the purpose of proposing the best solutions to the
		  challenges found in agriculture, especially to the problems
		  that affect cultivars. One of the obstacles found is to
		  apply the use of your own language in applications that
		  interact with the user in Brazilian Agribusiness, which
		  would bring gains in time, money and accuracy of the
		  analyzes to be performed. This paper proposes the use of
		  Natural Language Processing techniques for the development
		  of an effective system to assist in the identification of
		  pests and diseases in the soybean crop, stored in a
		  database repository to facilitate access to information and
		  diagnosis of the professional.},
  booktitle	= {Proceedings of the 19th Brazilian Symposium on Human
		  Factors in Computing Systems},
  articleno	= {58},
  numpages	= {6},
  keywords	= {Agriculture 4.0, human-computer interaction, natural
		  language processing},
  location	= {Diamantina, Brazil},
  series	= {IHC '20}
}

@InProceedings{	  10.1145/3316615.3316683,
  author	= {Yanuarifiani, Amarilis Putri and Chua, Fang-Fang and Chan,
		  Gaik-Yee},
  title		= {Automating Business Process Model Generation from
		  Ontology-based Requirements},
  year		= {2019},
  isbn		= {9781450365734},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3316615.3316683},
  doi		= {10.1145/3316615.3316683},
  abstract	= {Requirements elicitation process faces major challenges
		  about how stakeholders can easily verify requirements.
		  Requirements document allows developers to visualize
		  requirements using modeling language to ensure stakeholders
		  have the same perspective as them. It is also effective to
		  give presentations to stakeholders about how business
		  processes will be carried out after the requirements are
		  implemented. Issues are raised in building requirements
		  modeling as business users generally do not have enough
		  knowledge to build requirements models in specific
		  notations. Transforming requirements (natural language)
		  into semi-formal notation (BPMN) manually lead to
		  inconsistency of elements structure. The need to
		  automatically generate requirements model become crucial
		  because it will be the basis for the programming process.
		  Existing studies are mostly concerned on auto-completion of
		  modeling language using domain ontology as basic knowledge,
		  and let the stakeholders building initial requirements
		  model with limited knowledge. The idea of this paper is to
		  propose a methodology for building business process model
		  in semi-formal language (BPMN) to represent future business
		  processes using ontology approach. This research continues
		  from previous study which transform requirements list into
		  requirements ontology to formalize the elements such as
		  problem, actor and process. By using requirements ontology
		  as input, rule-based mapping method is proposed to map
		  ontology instances to BPMN elements.},
  booktitle	= {Proceedings of the 2019 8th International Conference on
		  Software and Computer Applications},
  pages		= {205–209},
  numpages	= {5},
  keywords	= {Semi-formal modeling, auto-generate BPMN, ontology-based
		  requirements},
  location	= {Penang, Malaysia},
  series	= {ICSCA '19}
}

@Article{	  10.1145/3567594,
  author	= {Mahlaza, Zola and Keet, C. Maria},
  title		= {Surface Realization Architecture for Low-resourced African
		  Languages},
  year		= {2023},
  issue_date	= {March 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3567594},
  doi		= {10.1145/3567594},
  abstract	= {There has been growing interest in building surface
		  realization systems to support the automatic generation of
		  text in African languages. Such tools focus on converting
		  abstract representations of meaning to a text. Since
		  African languages are low-resourced, economical use of
		  resources and general maintainability are key
		  considerations. However, there is no existing surface
		  realizer architecture that possesses most of the
		  maintainability characteristics (e.g., modularity,
		  reusability, and analyzability) that will lead to
		  maintainable software that can be used for the languages.
		  Moreover, there is no consensus surface realization
		  architecture created for other languages that can be
		  adapted for the languages in question. In this work, we
		  solve this by creating a novel surface realizer
		  architecture suitable for low-resourced African languages
		  that abides by the features of maintainable software. Its
		  design comes after a granular analysis, classification, and
		  comparison of the architectures used by 77 existing NLG
		  systems. We compare our architecture to existing
		  architectures and show that it supports the most features
		  of a maintainable software product.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= mar,
  articleno	= {84},
  numpages	= {26},
  keywords	= {Natural language generation, software architecture,
		  low-resourced languages, surface realisation}
}

@InProceedings{	  10.1145/3614321.3614367,
  author	= {Silva-Aguilar, Jairo H. and Torres T., Rommel and Estevez,
		  Elsa},
  title		= {Design of an ontology to represent the elaboration of the
		  annual operational plan in the area of public sector
		  planning},
  year		= {2023},
  isbn		= {9798400707421},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3614321.3614367},
  doi		= {10.1145/3614321.3614367},
  abstract	= {The Semantic Web helps to represent knowledge in such a
		  way that it can be easily processed by machines. Ontologies
		  are a tool the Semantic Web uses to define concepts,
		  establish their hierarchy, outline attributes, and
		  determine relationships, which capture knowledge within a
		  specific domain. In the public sector, institutions
		  generate a substantial volume of information. To foster
		  transparency and openness, such information is published on
		  the web. However, in many cases, it is necessary to
		  incorporate semantic content that helps to organize the
		  information so that it can be processed automatically, by
		  government authorities and interested users. One of the
		  fundamental activities in public institutions is planning,
		  which includes a set of processes that contribute to the
		  achievement of previously defined objectives. In this
		  article, we design an ontology whose domain is the
		  elaboration of the Annual Operational Plan in Ecuador. An
		  operational plan is a process that belongs to the Planning
		  area of the Decentralized Autonomous Governments of
		  Ecuador. We propose a set of activities and their
		  respective tasks for developing the ontology based on the
		  Methontology methodology. After its development, we
		  formalized the proposed ontology using Prot\'{e}g\'{e}. The
		  main theoretical contribution of this paper is the
		  definition of an ontology in a specific government area. In
		  addition, from a practical perspective, we developed a tool
		  that facilitates the analysis and processing of data for
		  budget planning in Ecuador.},
  booktitle	= {Proceedings of the 16th International Conference on Theory
		  and Practice of Electronic Governance},
  pages		= {340–346},
  numpages	= {7},
  keywords	= {Government budget, Ontology, Open Government, Planning,
		  Prot\'{e}g\'{e}, Semantic Web},
  location	= {Belo Horizonte, Brazil},
  series	= {ICEGOV '23}
}

@Article{	  10.1145/3626307.3626308,
  author	= {Baader, Franz},
  title		= {Relating Optimal Repairs in Ontology Engineering with
		  Contraction Operations in Belief Change},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {3},
  issn		= {1559-6915},
  url		= {https://doi.org/10.1145/3626307.3626308},
  doi		= {10.1145/3626307.3626308},
  abstract	= {The question of how a given knowledge base can be modified
		  such that certain unwanted consequences are removed has
		  been investigated in the area of ontology engineering under
		  the name of repair and in the area of belief change under
		  the name of contraction. Whereas in the former area the
		  emphasis was more on designing and implementing concrete
		  repair algorithms, the latter area concentrated on
		  characterizing classes of contraction operations by certain
		  postulates they satisfy. In the classical setting, repairs
		  and contractions are subsets of the knowledge base that no
		  longer have the unwanted consequence. This makes these
		  approaches syntax-dependent and may result in removal of
		  more consequences than necessary. To alleviate this
		  problem, gentle repairs and pseudo-constractions have been
		  introduced in the respective research areas, and their
		  connections have been investigated in recent work. Optimal
		  repairs preserve a maximal amount of consequences, but they
		  may not always exist. We show that, if they exist, then
		  they can be obtained by certain pseudo-contraction
		  operations, and thus they comply with the postulates that
		  these operations satisfy. Conversely, under certain
		  conditions, pseudo-contractions are guaranteed to produce
		  optimal repairs. Recently, contraction operations have also
		  been defined for concepts rather than for whole knowledge
		  bases. We show that there is again a close connection
		  between such operations and optimal repairs of a restricted
		  form of knowledge bases.},
  journal	= {SIGAPP Appl. Comput. Rev.},
  month		= sep,
  pages		= {5–18},
  numpages	= {14},
  keywords	= {belief change, description logic, ontology repair}
}

@InProceedings{	  10.1145/3340555.3356098,
  author	= {Zhu, Su and Zhao, Zijian and Zhao, Tiejun and Zong,
		  Chengqing and Yu, Kai},
  title		= {CATSLU: The 1st Chinese Audio-Textual Spoken Language
		  Understanding Challenge},
  year		= {2019},
  isbn		= {9781450368605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340555.3356098},
  doi		= {10.1145/3340555.3356098},
  abstract	= {Spoken language understanding (SLU) is a key component of
		  conversational dialogue systems, which converts user
		  utterances into semantic representations. The previous
		  works almost focus on parsing semantic from textual inputs
		  (top hypothesis of speech recognition and even manual
		  transcripts) while losing information hidden in the audio.
		  We herein describe the 1st Chinese Audio-Textual Spoken
		  Language Understanding Challenge (CATSLU) which introduces
		  a new dataset with audio-textual information, multiple
		  domains and domain knowledge. We introduce two scenarios of
		  audio-textual SLU in which participants are encouraged to
		  utilize data of other domains or not. In this paper, we
		  will describe the challenge and results.},
  booktitle	= {2019 International Conference on Multimodal Interaction},
  pages		= {521–525},
  numpages	= {5},
  keywords	= {datasets, spoken language understanding},
  location	= {Suzhou, China},
  series	= {ICMI '19}
}

@InProceedings{	  10.1145/3614321.3614327,
  author	= {Avgerinos Loutsaris, Michalis and Alexopoulos, Charalampos
		  and Maratsi, Maria Ioanna and Charalabidis, Yannis},
  title		= {Semantic Interoperability for Legal Information: Mapping
		  the European Legislation Identifier (ELI) and Akoma Ntoso
		  (AKN) Ontologies},
  year		= {2023},
  isbn		= {9798400707421},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3614321.3614327},
  doi		= {10.1145/3614321.3614327},
  abstract	= {The legislative landscape, characterized by overwhelming
		  amounts of legal data which, on many occasions, is only
		  accessible by legal experts, the fragmented nature of
		  information and the ever-increasing number of disparate
		  systems, have given more impetus to the interoperability
		  realm of legal data. The semantic interoperability of
		  Linked Open Legal Data (LOLD) requires rich and
		  well-defined metadata, as well as the establishment of
		  standards, in order to be able to connect and link these
		  scattered legal data resources. This is usually achieved
		  through the transformation of legal information into
		  structured format and through the utilization of legal
		  ontologies whose main purpose is to connect the legal basis
		  of two or more countries by allowing for reusability and
		  the ability to adequately represent legal information,
		  which is understood and retrieved across borders. Within
		  the context of this study, the European Legislation
		  Identifier (ELI) and Akoma Ntoso (AKN) ontologies are
		  mapped in order to make legal data compatible and reusable
		  in as many contexts as possible and to support the Linked
		  Open Legal Data (LOLD) concept. The mapping of these two
		  widely used legal ontologies was evaluated by domain
		  experts and strongly validated by tools. The usefulness of
		  the produced mapping is proven through its real-life
		  context application, although one thing to consider
		  regarding possible future perspectives, could be the
		  inclusion and mapping of more legal ontologies to expand
		  the application domain and improve the semantic
		  interoperability of legal information. These mappings could
		  be either achieved using similar methodological approaches
		  or applications of automated and AI-based ontology mapping
		  techniques.},
  booktitle	= {Proceedings of the 16th International Conference on Theory
		  and Practice of Electronic Governance},
  pages		= {41–53},
  numpages	= {13},
  keywords	= {OGD, Open Government Data, legal data, legal information,
		  linked open legal data (LOLD), ontology mapping, semantic
		  interoperability},
  location	= {Belo Horizonte, Brazil},
  series	= {ICEGOV '23}
}

@InProceedings{	  10.1145/3342428.3342662,
  author	= {Afacan, Yasemin and Surer, Elif},
  title		= {Modeling a User-Oriented Ontology on Accessible Homes for
		  Supporting Activities of Daily Living (ADL) in Healthy
		  Aging},
  year		= {2019},
  isbn		= {9781450362610},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3342428.3342662},
  doi		= {10.1145/3342428.3342662},
  abstract	= {Inaccessibility of the buildings is the most common
		  obstacle which presents barriers for older adults with
		  different motor abilities. An inclusive design process,
		  where elderly and designers work together, is required to
		  overcome this obstacle. To do so, this study proposes a
		  user-oriented model (i) to define a knowledge presentation
		  for designers; (ii) to assist them during the development
		  of accessible homes and (iii) to accommodate exemplary home
		  attributes for activities of daily living (ADL). The
		  ontology for this model was first constructed by collecting
		  user information through LEGO® Serious Play® on the four
		  subdomains of motor abilities: (1) strength; (2) balance;
		  (3) locomotion; and (4) endurance. The findings of this
		  study are significant for future aging studies and mobile
		  computing researches in terms of indicating that diverse
		  motor ability difficulties are associated with different
		  requirements of accessibility attributes, and structured
		  knowledge is required to diagrammatize their association
		  with ADL.},
  booktitle	= {Proceedings of the 5th EAI International Conference on
		  Smart Objects and Technologies for Social Good},
  pages		= {67–71},
  numpages	= {5},
  keywords	= {Accessible Home, Activities of Daily Living (ADL),
		  Assisted Living, Ontology},
  location	= {Valencia, Spain},
  series	= {GoodTechs '19}
}

@InProceedings{	  10.1145/3698322.3698326,
  author	= {Almeida, Francisca and Pinho, Daniel and Aguiar, Ademar},
  title		= {Validating Pattern Languages: A systematic literature
		  review},
  year		= {2024},
  isbn		= {9798400716836},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698322.3698326},
  doi		= {10.1145/3698322.3698326},
  abstract	= {The concept of patterns and pattern languages, although
		  very common in software nowadays, was first approached by
		  Christopher Alexander, in the area of architecture, in the
		  book A pattern language: towns, buildings, construction.
		  However, it was only in 1980 that the term was adapted for
		  software development, gaining its popularity in 1994.
		  Despite the fact that the concept of patterns has been used
		  in the area of software development for more than 40 years,
		  there is still no consensus on the best method to validate
		  patterns and patterns languages, and the existing methods
		  are scattered in several different papers and research
		  across the scientific community. As such, in this paper, we
		  conduct a systematic literature review about the existing
		  methods in the scientific community to validate patterns
		  and pattern languages.},
  booktitle	= {Proceedings of the 29th European Conference on Pattern
		  Languages of Programs, People, and Practices},
  articleno	= {34},
  numpages	= {8},
  keywords	= {Patterns, pattern languages, pattern validation, software
		  development},
  location	= { },
  series	= {EuroPLoP '24}
}

@InBook{	  10.1145/3477322.3477328,
  author	= {Pieraccini, Roberto},
  title		= {Natural Language Understanding in Socially Interactive
		  Agents},
  year		= {2021},
  isbn		= {9781450387200},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  url		= {https://doi.org/10.1145/3477322.3477328},
  booktitle	= {The Handbook on Socially Interactive Agents: 20 Years of
		  Research on Embodied Conversational Agents, Intelligent
		  Virtual Agents, and Social Robotics Volume 1: Methods,
		  Behavior, Cognition},
  pages		= {147–172},
  numpages	= {26}
}

@InProceedings{	  10.1145/3550356.3561553,
  author	= {T\"{o}pel, Daniel and Kaczmarek-He\ss{}, Monika},
  title		= {Towards flexible creation of multi-level models: bottom-up
		  change support in the modeling and programming environment
		  XModeler},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561553},
  doi		= {10.1145/3550356.3561553},
  abstract	= {A process of a multi-level model creation follows
		  typically the top-down approach, i.e., it requires first
		  defining concepts and relations on the highest
		  classification levels, which only then can be used to
		  create concepts on the lower ones. Empirical insights into
		  the process of multi-level model creation suggest however,
		  that this strategy may be counter-intuitive and
		  challenging, especially for non-experts. This paper
		  addresses this problem by focusing on the idea of flexible
		  multi-level model creation, understood as an intertwined
		  application of top-down and bottom-up strategies. As a
		  first step towards realizing this vision for multi-level
		  models in general, and those created with the XModeler and
		  Flexible Meta-Modeling and Execution Language (FMMLx) in
		  particular, in this paper, we select a set of relevant
		  multi-level refactoring patterns, adapt them to our
		  approach, and implement them in the supporting tool. We
		  illustrate the flexible creation process using an exemplary
		  scenario.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {404–413},
  numpages	= {10},
  keywords	= {XModeler, bottom-up modeling, flexible meta-modeling and
		  execution language FMMLx, flexible modeling process,
		  multi-level modeling, multi-level refactoring patterns},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3292500.3330710,
  author	= {Weng, Wei-Hung and Chung, Yu-An and Szolovits, Peter},
  title		= {Unsupervised Clinical Language Translation},
  year		= {2019},
  isbn		= {9781450362016},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3292500.3330710},
  doi		= {10.1145/3292500.3330710},
  abstract	= {As patients' access to their doctors' clinical notes
		  becomes common, translating professional, clinical jargon
		  to layperson-understandable language is essential to
		  improve patient-clinician communication. Such translation
		  yields better clinical outcomes by enhancing patients'
		  understanding of their own health conditions, and thus
		  improving patients' involvement in their own care. Existing
		  research has used dictionary-based word replacement or
		  definition insertion to approach the need. However, these
		  methods are limited by expert curation, which is hard to
		  scale and has trouble generalizing to unseen datasets that
		  do not share an overlapping vocabulary. In contrast, we
		  approach the clinical word and sentence translation problem
		  in a completely unsupervised manner. We show that a
		  framework using representation learning, bilingual
		  dictionary induction and statistical machine translation
		  yields the best precision at 10 of 0.827 on
		  professional-to-consumer word translation, and mean opinion
		  scores of 4.10 and 4.28 out of 5 for clinical correctness
		  and layperson readability, respectively, on sentence
		  translation. Our fully-unsupervised strategy overcomes the
		  curation problem, and the clinically meaningful evaluation
		  reduces biases from inappropriate evaluators, which are
		  critical in clinical machine learning.},
  booktitle	= {Proceedings of the 25th ACM SIGKDD International
		  Conference on Knowledge Discovery \&amp; Data Mining},
  pages		= {3121–3131},
  numpages	= {11},
  keywords	= {consumer health, machine translation, representation
		  learning, unsupervised learning},
  location	= {Anchorage, AK, USA},
  series	= {KDD '19}
}

@Article{	  10.1145/3686981,
  author	= {Zhu, Mengxiao and Wang, Xin and Wang, Xiantao and Chen,
		  Zihang and Huang, Wei},
  title		= {Application of Prompt Learning Models in Identifying the
		  Collaborative Problem Solving Skills in an Online Task},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW2},
  url		= {https://doi.org/10.1145/3686981},
  doi		= {10.1145/3686981},
  abstract	= {Collaborative problem solving (CPS) competence is
		  considered one of the essential 21st-century skills. To
		  facilitate the assessment and learning of CPS competence,
		  researchers have proposed a series of frameworks to
		  conceptualize CPS and explored ways to make sense of the
		  complex processes involved in collaborative problem
		  solving. However, encoding explicit behaviors into
		  subskills within the frameworks of CPS skills is still a
		  challenging task. Traditional studies have relied on manual
		  coding to decipher behavioral data for CPS, but such coding
		  methods can be very time-consuming and cannot support
		  real-time analyses. Scholars have begun to explore
		  approaches for constructing automatic coding models.
		  Nevertheless, the existing models built using machine
		  learning or deep learning techniques depend on a large
		  amount of training data and have relatively low accuracy.
		  To address these problems, this paper proposes a
		  prompt-based learning pre-trained model. The model can
		  achieve high performance even with limited training data.
		  In this study, three experiments were conducted, and the
		  results showed that our model not only produced the highest
		  accuracy, macro F1 score, and kappa values on large
		  training sets, but also performed the best on small
		  training sets of the CPS behavioral data. The application
		  of the proposed prompt-based learning pre-trained model
		  contributes to the CPS skills coding task and can also be
		  used for other CSCW coding tasks to replace manual
		  coding.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= nov,
  articleno	= {442},
  numpages	= {23},
  keywords	= {automatic coding, collaborative problem solving, natural
		  language processing, prompt-based learning}
}

@InProceedings{	  10.1145/3487664.3487789,
  author	= {Mohamed, Aya and Auer, Dagmar and Hofer, Daniel and
		  K\"{u}ng, Josef},
  title		= {Extended XACML Language and Architecture for Access
		  Control in Graph-structured Data},
  year		= {2022},
  isbn		= {9781450395564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487664.3487789},
  doi		= {10.1145/3487664.3487789},
  abstract	= {The rapidly increasing use of graph databases for a wide
		  variety of applications demands flexible authorization and
		  fine-grained access control at the level of attributes
		  associated with the basic entities (i.e., accessing
		  subject, requested resource, performed action, and
		  environmental conditions) but also the vertices and edges
		  along a particular access path. We present a solution for
		  authorization policy specification and enforcement in a
		  graph database to apply fine-grained path-specific
		  constraints on graph-structured data. Therefore, we extend
		  the well-established declarative policy definition language
		  eXtensible Access Control Markup Language (XACML) and its
		  architecture to describe path patterns and enforce the
		  policies using the standard functional components of XACML.
		  Our approach, XACML for Graph-structured data (XACML4G),
		  defines an extended XACML grammar for the authorization
		  policy and access request. To enforce XACML4G policies, we
		  relied on the extensibility points of the XACML
		  architecture and added proprietary extensions. We show the
		  significance of our approach by means of a demonstration
		  prototype in the university domain. Finally, we provide an
		  initial evaluation of the expressiveness and performance of
		  XACML4G with regard to XACML.},
  booktitle	= {The 23rd International Conference on Information
		  Integration and Web Intelligence},
  pages		= {367–374},
  numpages	= {8},
  keywords	= {Access Control, Authorization Policy, Graph Database,
		  Graph-structured Data, XACML, XACML4G},
  location	= {Linz, Austria},
  series	= {iiWAS2021}
}

@InProceedings{	  10.1145/3357419.3357442,
  author	= {Tapia-Leon, Mariela and Aveiga, Carlos and Chicaiza,
		  Janneth and Su\'{a}rez-Figueroa, Mari Carmen},
  title		= {Ontological Model for the Semantic Description of
		  Syllabuses},
  year		= {2019},
  isbn		= {9781450371889},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357419.3357442},
  doi		= {10.1145/3357419.3357442},
  abstract	= {The syllabus is a relevant document to organize how the
		  teaching-learning process will be carried out during an
		  academic course in Higher Education Institutions (HEI).
		  Usually, this document is written in a human-readable
		  format that do not enable automatic processing through
		  intelligent services to support teaching and learning.
		  Therefore, we created OntoSyllabus ontology for the
		  representation of syllabuses applying the NeOn methodology.
		  The semantic model of a syllabus will allow the
		  comprehension for both: machines and humans, and it will
		  facilitate the interchange of data between different
		  services and applications. The ontology was created based
		  on the results of our three previous studies, which helped
		  us to determinate the terms and relations in the syllabus
		  ontology. The documentation and the computable model are
		  available on the Internet for their reuse.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Information Communication and Management},
  pages		= {175–180},
  numpages	= {6},
  keywords	= {Higher Education Institution, NeOn Methodology, Ontology,
		  Semantic Web, Syllabus},
  location	= {Prague, Czech Republic},
  series	= {ICICM '19}
}

@InProceedings{	  10.5555/3237383.3237828,
  author	= {Pomarlan, Mihai and Bateman, John},
  title		= {Robot Program Construction via Grounded Natural Language
		  Semantics \&amp; Simulation},
  year		= {2018},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Robots acting in semi-structured, human environments need
		  to understand the effects of their actions and the
		  instructions given by a human user. Simulation has been
		  considered a promising reasoning technique to help tackle
		  both problems. In this paper, we present a system that
		  constructs an executable robot program from a linguistic
		  semantic specification produced by parsing a natural
		  language sentence; in effect, our system grounds the
		  semantic specification into the produced robot plan. The
		  plan can then be run in a simulated environment, which
		  allows one to infer more about the plan than was present in
		  the initial semantic specification. Our system allows
		  modeling how actions can be modified by subclauses, which
		  we showcase by a transport action. Simulation runs allow
		  discovery of better parameters, either locally for a
		  subtask or such that the entire task is better performed;
		  simulation reveals these parameterizations may differ.},
  booktitle	= {Proceedings of the 17th International Conference on
		  Autonomous Agents and MultiAgent Systems},
  pages		= {857–864},
  numpages	= {8},
  keywords	= {cognitive robotics, human-robot interaction, language
		  grounding, robotic agent languages},
  location	= {Stockholm, Sweden},
  series	= {AAMAS '18}
}

@Article{	  10.5555/3722479.3722530,
  author	= {Kollapally, Navya Martin and Geller, James and Morreale,
		  Patricia and Kwak, Daehan},
  title		= {An Ontology for Social Determinants of Education (SDoEd)
		  Based on Human-AI Collaborative Approach},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Consortium for Computing Sciences in Colleges},
  address	= {Evansville, IN, USA},
  volume	= {40},
  number	= {3},
  issn		= {1937-4771},
  abstract	= {The use of computational ontologies is well-established in
		  the field of Medical Informatics. The topic of Social
		  Determinants of Health (SDoH) has also received extensive
		  attention. Work at the intersection of ontologies and SDoH
		  has been published. However, a standardized framework for
		  Social Determinants of Education (SDoEd) is lacking. In
		  this paper, we are closing the gap by introducing an SDoEd
		  ontology for creating a precise conceptualization of the
		  interplay between life circumstances of students and their
		  possible educational achievements. The ontology was
		  developed utilizing suggestions from ChatGPT-3.5-010422 and
		  validated using peer-reviewed research articles. The first
		  version of developed ontology was evaluated by human
		  experts in the field of education and validated using
		  standard ontology evaluation software. This version of the
		  SDoEd ontology contains 231 domain concepts, 10 object
		  properties, and 24 data properties.},
  journal	= {J. Comput. Sci. Coll.},
  month		= oct,
  pages		= {191–203},
  numpages	= {13}
}

@Article{	  10.1145/3375547,
  author	= {Abulaish, Muhammad and Kamal, Ashraf and Zaki, Mohammed
		  J.},
  title		= {A Survey of Figurative Language and Its Computational
		  Detection in Online Social Networks},
  year		= {2020},
  issue_date	= {February 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {1},
  issn		= {1559-1131},
  url		= {https://doi.org/10.1145/3375547},
  doi		= {10.1145/3375547},
  abstract	= {The frequent usage of figurative language on online social
		  networks, especially on Twitter, has the potential to
		  mislead traditional sentiment analysis and recommender
		  systems. Due to the extensive use of slangs, bashes,
		  flames, and non-literal texts, tweets are a great source of
		  figurative language, such as sarcasm, irony, metaphor,
		  simile, hyperbole, humor, and satire. Starting with a brief
		  introduction of figurative language and its various
		  categories, this article presents an in-depth survey of the
		  state-of-the-art techniques for computational detection of
		  seven different figurative language categories, mainly on
		  Twitter. For each figurative language category, we present
		  details about the characterizing features, datasets, and
		  state-of-the-art computational detection approaches.
		  Finally, we discuss open challenges and future directions
		  of research for each figurative language category.},
  journal	= {ACM Trans. Web},
  month		= feb,
  articleno	= {3},
  numpages	= {52},
  keywords	= {Social network analysis, figurative language, humor
		  recognition, hyperbole detection, irony detection, metaphor
		  detection, sarcasm detection, satire detection, simile
		  detection}
}

@InProceedings{	  10.5555/3373669.3373682,
  author	= {Knote, Robin and S\"{o}llner, Matthias and Leimeister, Jan
		  Marco},
  title		= {Towards a pattern language for smart personal assistants},
  year		= {2020},
  publisher	= {The Hillside Group},
  address	= {USA},
  abstract	= {Supporting users in their daily activities, thus, making
		  their lives more comfortable, has long been a goal for
		  consumer-oriented systems development. With the rise of
		  smart personal assistants (SPAs), however, we have reached
		  a new milestone along the path towards this goal. These
		  systems assist their owners by providing personalized and
		  context-dependent information and service. Today's
		  implementations reach from conversational agents, such as
		  Siri, Cortana or Google Assistant, over chatbots, which are
		  primarily text-based, to cognitive assistants, which assist
		  according to a user's current cognitive or emotional state.
		  However, although both research and practice proceed with
		  full pace, recurring design elements of SPAs have not yet
		  been investigated. We hence propose a pattern language for
		  smart personal assistants to guide further empirical and
		  design efforts. Therefore, we review existing information
		  systems, computer science and human-computer interaction
		  literature to find recurring design characteristics among
		  115 different assistants. The resulting pattern language
		  contains 22 patterns that specify the interaction behavior
		  and the intelligence of smart personal assistants.},
  booktitle	= {Proceedings of the 25th Conference on Pattern Languages of
		  Programs},
  articleno	= {14},
  numpages	= {16},
  keywords	= {pattern language, smart personal assistants},
  location	= {Portland, Oregon},
  series	= {PLoP '18}
}

@InProceedings{	  10.1145/3626772.3657815,
  author	= {Joko, Hideaki and Chatterjee, Shubham and Ramsay, Andrew
		  and de Vries, Arjen P. and Dalton, Jeff and Hasibi,
		  Faegheh},
  title		= {Doing Personal LAPS: LLM-Augmented Dialogue Construction
		  for Personalized Multi-Session Conversational Search},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657815},
  doi		= {10.1145/3626772.3657815},
  abstract	= {The future of conversational agents will provide users
		  with personalized information responses. However, a
		  significant challenge in developing models is the lack of
		  large-scale dialogue datasets that span multiple sessions
		  and reflect real-world user preferences. Previous
		  approaches rely on experts in a wizard-of-oz setup that is
		  difficult to scale, particularly for personalized tasks.
		  Our method, LAPS, addresses this by using large language
		  models (LLMs) to guide a single human worker in generating
		  personalized dialogues. This method has proven to speed up
		  the creation process and improve quality. LAPS can collect
		  large-scale, human-written, multi-session, and multi-domain
		  conversations, including extracting user preferences. When
		  compared to existing datasets, LAPS-produced conversations
		  are as natural and diverse as expert-created ones, which
		  stays in contrast with fully synthetic methods. The
		  collected dataset is suited to train preference extraction
		  and personalized response generation. Our results show that
		  responses generated explicitly using extracted preferences
		  better match user's actual preferences, highlighting the
		  value of using extracted preferences over simple dialogue
		  history. Overall, LAPS introduces a new method to leverage
		  LLMs to create realistic personalized conversational data
		  more efficiently and effectively than previous methods.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {796–806},
  numpages	= {11},
  keywords	= {conversational search, dialogue collection,
		  personalization},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Article{	  10.1145/3605910,
  author	= {Koch, In\^{e}s and Teixeira Lopes, Carla and Ribeiro,
		  Cristina},
  title		= {Moving from ISAD(G) to a CIDOC CRM-based Linked Data Model
		  in the Portuguese Archives},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3605910},
  doi		= {10.1145/3605910},
  abstract	= {Archives are facing numerous challenges. On the one hand,
		  archival assets are evolving to encompass digitized
		  documents and increasing quantities of born-digital
		  information in diverse formats. On the other hand, the
		  audience is changing along with how it wishes to access
		  archival material. Moreover, the interoperability
		  requirements of cultural heritage repositories are growing.
		  In this context, the Portuguese Archives started an
		  ambitious program aiming to evolve its data model, migrate
		  existing records, and build a new archival management
		  system appropriate to both archival tasks and public
		  access. The overall goal is to have a fine-grained and
		  flexible description, more machine-actionable than the
		  current one. This work describes ArchOnto, a linked open
		  data model for archives, and rules for its automatic
		  population from existing records. ArchOnto adopts a
		  semantic web approach and encompasses the CIDOC Conceptual
		  Reference Model and additional ontologies, envisioning
		  interoperability with datasets curated by multiple
		  communities of practice. Existing ISAD(G)-conforming
		  descriptions are being migrated to the new model using the
		  direct mappings provided here. We used a sample of 25
		  records associated with different description levels to
		  validate the completeness and conformity of ArchOnto to
		  existing data. This work is in progress and is original in
		  several respects: (1) it is one of the first approaches to
		  use CIDOC CRM in the context of archives, identifying
		  problems and questions that emerged during the process and
		  pinpointing possible solutions; (2) it addresses the
		  balance in the model between the migration of existing
		  records and the construction of new ones by archive
		  professionals; and (3) it adopts an open world view on
		  linking archival data to global information sources.},
  journal	= {J. Comput. Cult. Herit.},
  month		= nov,
  articleno	= {71},
  numpages	= {21},
  keywords	= {Cultural heritage, archives, archival description, linked
		  open data, semantic web, data migration}
}

@InProceedings{	  10.1145/3670013.3670022,
  author	= {Sitthisak, Onjira and Pradubsuwun, Denduang},
  title		= {Analysis of Generated Assessment Items from the COMBA
		  Competency Model},
  year		= {2024},
  isbn		= {9798400717062},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670013.3670022},
  doi		= {10.1145/3670013.3670022},
  abstract	= {This paper proposes an item analysis of generated
		  assessment items from the COMBA competency model. Two
		  metrics are used including difficulty and discrimination.
		  We generate assessment items by COMBA for evaluating the
		  learner's competence in the Python programming class and
		  use them to examine the cognitive level of learners. The
		  item analysis is applied to check conformance between the
		  capability level of the generated assessment items and the
		  cognitive level of learners. Experimenting with the
		  generated assessment items, the capability level affects
		  the difficulty and discrimination of their items.},
  booktitle	= {Proceedings of the 2024 15th International Conference on
		  E-Education, E-Business, E-Management and E-Learning},
  pages		= {117–122},
  numpages	= {6},
  keywords	= {Adaptive Assessment, Competency Model, Item Analysis,
		  Ontology},
  location	= {Fukuoka-shi, Japan},
  series	= {IC4E '24}
}

@InProceedings{	  10.1145/3343485.3343500,
  author	= {Zhomartkyzy, Gulnaz and Kumargazhanova, Saule and Popova,
		  Galina and Suleimenova, Laura},
  title		= {Development of University Scientific Knowledge Ontological
		  Model},
  year		= {2019},
  isbn		= {9781450371681},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3343485.3343500},
  doi		= {10.1145/3343485.3343500},
  abstract	= {An ontology is a link between objects of knowledge and a
		  connecting bridge between various steps of Knowledge
		  Processes. Ontology development is an important aspect of
		  knowledge management solution support (KM-solutions). In
		  this paper, we consider a university scientific knowledge
		  ontological model which is one of the knowledge management
		  systems tools. The main functions of the university
		  scientific knowledge ontology are given. The main classes,
		  properties and relations of ontology for maintaining the
		  knowledge base of educational resources are described.},
  booktitle	= {Proceedings of the 2019 2nd International Conference on
		  Mathematics and Statistics},
  pages		= {40–45},
  numpages	= {6},
  keywords	= {Electronic scientific resources, Knowledge base, Knowledge
		  management, Ontological engineering, Scientific knowledge
		  ontology},
  location	= {Prague, Czech Republic},
  series	= {ICoMS '19}
}

@InProceedings{	  10.1145/3737609.3747119,
  author	= {Hughes, Margaret and DeSota, Elianna and Victor, Matthew
		  and Lynn, Stuart and Stormonth-Darling, John M, John and
		  Barry, Liz},
  title		= {Towards Interoperability: Pursuing an ontology for data
		  exchange between deliberative democratic platforms},
  year		= {2025},
  isbn		= {9798400719684},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3737609.3747119},
  doi		= {10.1145/3737609.3747119},
  abstract	= {In response to the fragmented state of civic engagement
		  tools and the urgent challenges facing democratic systems,
		  this paper introduces a shared, contributor-driven ontology
		  to connect diverse civic tech platforms, emerging from the
		  work of the Interoperable Deliberative Tool cohort at
		  Metagov. By integrating platforms like Voice to Vision,
		  Assemblis, and Decidim, we enable the flow of deliberative
		  data across contexts, supporting more cohesive
		  decision-making. This approach helps bridge gaps between
		  input, analysis, and action, enhancing democratic
		  resilience in crisis moments. Through our work, we
		  demonstrate how interoperability can strengthen civic
		  engagement and provide a foundation for more responsive,
		  collaborative governance.},
  booktitle	= {Adjunct Proceedings of the Sixth Decennial Aarhus
		  Conference: Computing X Crisis},
  articleno	= {19},
  numpages	= {5},
  keywords	= {Governance, Interoperability, Deliberative Democracy, Data
		  Commons, Civic Technology, Digital Civics},
  location	= { },
  series	= {AAR Adjunct '25}
}

@InProceedings{	  10.1145/3550356.3561577,
  author	= {Delabeye, Romain and Penas, Olivia and Plateaux,
		  R\'{e}gis},
  title		= {Scalable ontology-based V&amp;V process for heterogeneous
		  systems and applications},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561577},
  doi		= {10.1145/3550356.3561577},
  abstract	= {This work focuses on ongoing research within the EU-funded
		  EnerMan project aiming at improving the energy efficiency
		  of manufacturing systems. Industrial use cases are
		  generally too constrained to easily proceed to the
		  verification and validation (V&amp;V) of the scientific
		  approaches tackling their challenges. In this context, we
		  propose an ontology-based framework with a methodology
		  assessing the scalability of heterogeneous systems,
		  environments, and missions in a V&amp;V context. Indeed,
		  projecting these industrial and laboratory applications
		  onto a meaningful ontology allows them to be flattened out
		  to the same scale from a semantic point of view. Reasoning
		  is used to evaluate the extent to which a given scientific
		  approach can be verified on a laboratory use case different
		  from the industrial scenario on which it has to be
		  validated. The framework has been implemented using
		  Prot\'{e}g\'{e} and Owlready2, and applied to a scientific
		  approach focused on a blind source separation technique
		  used to identify system operating modes in a black box
		  manner, tested on a coffee machine and two industrial case
		  studies (a vehicle testbed's heating ventilation and air
		  conditioning system, and a chocolate production line).},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {341–350},
  numpages	= {10},
  keywords	= {heterogeneous systems, ontology, requirements analysis,
		  scalability, verification \&amp; validation},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3638837.3638882,
  author	= {Yu, Rongdong and Zhong, Yaoyi and Xu, Yunliang and Wen,
		  Jie and Pan, Quanhong and Sha, Wanli and Wang, Zhan},
  title		= {Leveraging Graph Databases for Automated OPC UA
		  Information Model Construction},
  year		= {2024},
  isbn		= {9798400709265},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638837.3638882},
  doi		= {10.1145/3638837.3638882},
  abstract	= {To address the OPC UA transformation requirements of
		  conventional factory data collection systems, we propose an
		  innovative method for the automated construction of OPC UA
		  information models utilizing graph databases. This method
		  not only streamlines the process but also minimizes the
		  need for extensive manual labor and specialized expertise.
		  Drawing from the resemblance between OPC UA information
		  models and OWL ontologies, we have devised mapping rules
		  that facilitate the translation of knowledge graph data
		  into OPC UA format. Leveraging the existing Neo4j database
		  within the factory as the primary information source,
		  we’ve developed an OWL ontology construction module for
		  exporting ontology files. In parallel, an information model
		  construction module has been designed to convert the
		  ontology file into an OPC UA XML file, complete with the
		  OPC UA information model. This XML file adheres to the
		  official OPC UA specification, ensured through the use of
		  the UA-ModelCompiler tool. To validate the effectiveness
		  and viability of this construction method, we conducted
		  functional tests and evaluations using a publicly available
		  database. The results of these tests confirm the
		  feasibility of our approach, marking a significant
		  advancement in OPC UA information model construction for
		  factory data systems.},
  booktitle	= {Proceedings of the 2023 12th International Conference on
		  Networks, Communication and Computing},
  pages		= {294–299},
  numpages	= {6},
  keywords	= {Neo4j, OPC UA information model, OWL ontology, graph
		  database},
  location	= {Osaka, Japan},
  series	= {ICNCC '23}
}

@InProceedings{	  10.5555/3466184.3466386,
  author	= {Ramzy, Nour and Martens, Christian James and Singh, Shreya
		  and Ponsignon, Thomas and Ehm, Hans},
  title		= {First steps towards bridging simulation and ontology to
		  ease the model creation on the example of semiconductor
		  industry},
  year		= {2021},
  isbn		= {9781728194998},
  publisher	= {IEEE Press},
  abstract	= {With diverse product mixes in fabs, high demand
		  volatility, and numerous manufacturing steps spread across
		  different facilities, it is impossible to analyze the
		  combined impacts of multiple operations in semiconductor
		  supply chains without a modeling tool like simulation. This
		  paper explains how ontologies can be used to develop and
		  deploy simulation applications, with interoperability and
		  knowledge sharing at the semantic level. This paper
		  proposes a concept to automatically build simulations using
		  ontologies and its preliminary results. The proposed
		  approach seeks to save time and effort expended in
		  recreating the information for different use cases that
		  already exists elsewhere. The use case provides first
		  indications that with an enhancement of a so-called Digital
		  Reference with Semantic Web Technologies, modeling and
		  simulation of semiconductor supply chains will not only
		  become much faster but also require less modeling efforts
		  because of the reusability property.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1789–1800},
  numpages	= {12},
  location	= {Orlando, Florida},
  series	= {WSC '20}
}

@InProceedings{	  10.1145/3543873.3587613,
  author	= {Li, Huanyu and Abd Nikooie Pour, Mina and Li, Ying and
		  Lindecrantz, Mikael and Blomqvist, Eva and Lambrix,
		  Patrick},
  title		= {A Survey of General Ontologies for the Cross-Industry
		  Domain of Circular Economy},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587613},
  doi		= {10.1145/3543873.3587613},
  abstract	= {Circular Economy has the goal to reduce value loss and
		  avoid waste by extending the life span of materials and
		  products, including circulating materials or product parts
		  before they become waste. Circular economy models (e.g.,
		  circular value networks) are typically complex and
		  networked, involving different cross-industry domains. In
		  the context of a circular value network, multiple actors,
		  such as suppliers, manufacturers, recyclers, and product
		  end-users, may be involved. In addition, there may be
		  various flows of resources, energy, information and value
		  throughout the network. This means that we face the
		  challenge that the data and information from cross-industry
		  domains in a circular economy model are not built on common
		  ground, and as a result are difficult to understand and use
		  for both humans and machines. Using ontologies to represent
		  domain knowledge can enable actors and stakeholders from
		  different industries in the circular economy to communicate
		  using a common language. The knowledge domains involved
		  include circular economy, sustainability, materials,
		  products, manufacturing, and logistics. The objective of
		  this paper is to investigate the landscape of current
		  ontologies for these domains. This will enable us to in the
		  future explore what existing knowledge can be adapted or
		  used to develop ontologies for circular value networks.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {731–741},
  numpages	= {11},
  keywords	= {Circular Economy, Cross-Industry Domain, Ontology,
		  Standard},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3360901.3364433,
  author	= {Lieber, Sven and De Meester, Ben and Dimou, Anastasia and
		  Verborgh, Ruben},
  title		= {MontoloStats - Ontology Modeling Statistics},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364433},
  doi		= {10.1145/3360901.3364433},
  abstract	= {Within ontology engineering concepts are modeled as
		  classes and relationships, and restrictions as axioms.
		  Reusing ontologies requires assessing if existing
		  ontologies are suited for an application scenario.
		  Different scenarios not only influence concept modeling,
		  but also the use of different restriction types, such as
		  subclass relationships or disjointness between concepts.
		  However, metadata about the use of such restriction types
		  is currently unavailable, preventing accurate assessments
		  for reuse. We created the RDF Data Cube-based dataset
		  MontoloStats, which contains restriction use statistics for
		  660 LOV and 565 BioPortal ontologies. We analyze the
		  dataset and discuss the findings and their implications for
		  ontology reuse. The MontoloStats dataset reveals that 94\%
		  of LOV and 95\% of BioPortal ontologies use RDFS-based
		  restriction types, 49\% of LOV and 52\% of BioPortal
		  ontologies use at least one OWL-based restriction type, and
		  different literal value-related restriction types are not
		  or barely used. Our dataset provides modeling insights,
		  beneficial for ontology reuse to discover and compare reuse
		  candidates, but can also be the basis of new research that
		  investigates novel ontology engineering methodologies with
		  respect to restrictions definition.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {69–76},
  numpages	= {8},
  keywords	= {axioms, ontology engineering, rdf, restrictions,
		  statistics},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.1145/3652620.3688209,
  author	= {Hinkel, Georg},
  title		= {Modeling a Warehouse system using refinements and
		  decomposition: A contribution to the MULTI Warehouse
		  challenge},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688209},
  doi		= {10.1145/3652620.3688209},
  abstract	= {Traditional two-level modeling has limitations when it
		  comes to represent domain-specific, non-transitive
		  instantiation relationships. Despite many approaches tackle
		  this problem, there is no consensus on how to models these
		  cases best. To have a common benchmark to discuss the
		  different approaches, the MULTI workshop has set the
		  warehouse model challenge. In this paper, we present and
		  discuss a solution of this challenge applying deep modeling
		  through refinements and structural decomposition - a
		  technology that supports deep modeling with just few
		  extensions to EMOF [12].},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {770–779},
  numpages	= {10},
  keywords	= {deep modeling, NMF, refinements and decomposition},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3652620.3688340,
  author	= {Fiyouzisabah, Zahra and Galasso, Jessie and Fokaefs,
		  Marios and Famelis, Michalis},
  title		= {Towards Rapid Design of Compartmental Models},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688340},
  doi		= {10.1145/3652620.3688340},
  abstract	= {In times of crisis, epidemiologists can come under great
		  pressure to model rapidly evolving diseases and to produce
		  analyses about the effects of potential public health
		  interventions. Taking previously developed, tested, and
		  validated model components as the base on which to
		  prototype new infectious disease models can save precious
		  time and effort. However, there is currently no systematic
		  process for quickly navigating a corpus of existing
		  epidemiological models or identifying and reusing their
		  most useful components. In this paper, we propose a vision
		  to accelerate the creation of prototype compartmental
		  models for infectious diseases. We outline a semi-automated
		  process that epidemiologists can use to create prototypes
		  that have been partially completed with reused fragments
		  from existing models. Epidemiologists can thus focus on
		  modelling the novel aspects of an ongoing public health
		  crisis, as opposed to aspects of it that are already more
		  or less well understood in previous work. Our approach
		  comprises five steps in total, including identifying useful
		  components in a corpus of infectious disease models,
		  generating potential candidate prototypes, and organizing
		  them in a formal data structure that allows navigation and
		  exploration by the modellers. We outline 13 challenges
		  ahead and discuss potential solutions based on formal
		  modelling techniques.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {1041–1045},
  numpages	= {5},
  keywords	= {domain specific modelling, reuse, evolution, prototyping,
		  formal concept analysis, scientific computing,
		  compartmental models},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3570991.3571058,
  author	= {Gupta, Akshay and Kumar, Suresh and Kumar P, Sreenivasa},
  title		= {Solving age-word problems using domain ontology and BERT},
  year		= {2023},
  isbn		= {9781450397971},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570991.3571058},
  doi		= {10.1145/3570991.3571058},
  abstract	= {An age word problem (ageWP) typically involves sentences
		  that express relationships between the age of the agents
		  and asks for the age of one of them. Automatically solving
		  ageWPs is a challenging task as we need to tackle temporal
		  relationships between the agent’s ages, frame and solve
		  the equations for the required unknowns. To the best of our
		  knowledge, there exists only one ageWP dataset consisting
		  of just 124 examples. The dataset is too small to employ a
		  learning-based solver, mainly consisting of ageWPs with
		  simple temporal relationships. To address this issue, in
		  our earlier work, we designed a description-logic based
		  ontology (ageWP-ont) for the domain of age word problems
		  and utilized it to automatically generate a large number of
		  ageWPs. Sentences in these ageWPs relate the ages of agents
		  in a temporally complex manner. In this paper, we focus on
		  solving these problems. We analyzed an existing
		  learning-based solver of algebraic word-problems that uses
		  a traditional machine learning approach and found that the
		  solver can be adapted to our domain. But we found that this
		  approach does not seem to perform well, perhaps due to the
		  complex nature of the ageWPs. As we have the ontology of
		  the domain on hand, we propose a new approach of utilizing
		  it in the deep-learning based NLU component of the
		  solution. We annotate parts of the ageWP sentences with
		  class-names from ageWP-ont and train a BERT-based language
		  model (LM) that learns to predict the instances for these
		  classes in the given sentences. An RDF graph is populated
		  with these values and serves as a concrete problem-specific
		  instance of the ontology. The dataset for training the LM
		  is automatically generated with the help of ageWP-ont.
		  Finally, for the actual solving of a given ageWP, we make
		  use of its RDF graph and employ Semantic Web Rule Language
		  (SWRL) rules. We implemented the proposed system and
		  achieved 68.8\% accuracy. The work demonstrates that
		  combining deep learning with ontologies can give impressive
		  results.},
  booktitle	= {Proceedings of the 6th Joint International Conference on
		  Data Science \&amp; Management of Data (10th ACM IKDD CODS
		  and 28th COMAD)},
  pages		= {95–103},
  numpages	= {9},
  keywords	= {Age-word problem solver, BERT, OWL-DL ontology, SWRL},
  location	= {Mumbai, India},
  series	= {CODS-COMAD '23}
}

@Article{	  10.1145/3745021,
  author	= {Xu, Heng-Da and Mao, Xian-Ling and Sun, Fanshu and Che,
		  Tian-Yi and Xu, Chun and Huang, Heyan},
  title		= {AgentTOD: A Task-Oriented Dialogue Agent with a Flexible
		  and Adaptive API Calling Paradigm},
  year		= {2025},
  issue_date	= {September 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {5},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3745021},
  doi		= {10.1145/3745021},
  abstract	= {Task-oriented dialogue (TOD) systems play a vital role in
		  numerous assistance and service scenarios, significantly
		  improving people’s daily lives. Conventionally, a TOD
		  system adheres to a fixed paradigm, where it must first
		  extract user goals and query external databases before it
		  can generate the final response. However, this fixed
		  extract-and-query paradigm is not always optimal for all
		  dialogue turns, which is redundant for the simple turns
		  that do not need external information, and is inadequate
		  for the complex turns that need to interact with the
		  external world multiple times. To address the limitations,
		  in this article, we propose AgentTOD, a novel TOD framework
		  that uses a large language model (LLM) as the intelligent
		  agent to achieve a flexible dialogue paradigm. AgentTOD
		  deprecates the traditional modular architecture (including
		  dialogue state tracking and dialogue policy) by utilizing
		  an LLM as the controller brain to determine when and how to
		  call the provided APIs to obtain external information. It
		  can choose to call APIs any number of times with various
		  parameters until it’s enough to reply to the user.
		  Besides, to train AgentTOD, we construct a large and
		  comprehensive TOD dataset, called TrajsTOD (Trajectories of
		  TODs), which consists of 66k+ user-agent dialogue
		  trajectories converted from eight popular TOD datasets
		  covering 60 domains. TrajsTOD is constructed with minimal
		  dialogue annotations where only the API calling logs are
		  needed and can empower AgentTOD with the general ability to
		  call APIs and generate responses according to the task
		  definition. Extensive experimental results on the
		  MultiWOZ-series and SGD datasets demonstrate AgentTOD has
		  superior performance on TODs as well as a superior
		  adaptability to new task scenarios.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= aug,
  articleno	= {136},
  numpages	= {32},
  keywords	= {Task-Oriented Dialogue, Large Language Models, Intelligent
		  Agents}
}

@InProceedings{	  10.1145/3313831.3376315,
  author	= {Strengers, Yolande and Qu, Lizhen and Xu, Qiongkai and
		  Knibbe, Jarrod},
  title		= {Adhering, Steering, and Queering: Treatment of Gender in
		  Natural Language Generation},
  year		= {2020},
  isbn		= {9781450367080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3313831.3376315},
  doi		= {10.1145/3313831.3376315},
  abstract	= {Natural Language Generation (NLG) supports the creation of
		  personalized, contextualized, and targeted content.
		  However, the algorithms underpinning NLG have come under
		  scrutiny for reinforcing gender, racial, and other
		  problematic biases. Recent research in NLG seeks to remove
		  these biases through principles of fairness and privacy.
		  Drawing on gender and queer theories from sociology and
		  Science and Technology studies, we consider how NLG can
		  contribute towards the advancement of gender equity in
		  society. We propose a conceptual framework and technical
		  parameters for aligning NLG with feminist HCI qualities. We
		  present three approaches: (1) adhering to current
		  approaches of removing sensitive gender attributes, (2)
		  steering gender differences away from the norm, and (3)
		  queering gender by troubling stereotypes. We discuss the
		  advantages and limitations of these approaches across three
		  hypothetical scenarios; newspaper headlines, job
		  advertisements, and chatbots. We conclude by discussing
		  considerations for implementing this framework and related
		  ethical and equity agendas.},
  booktitle	= {Proceedings of the 2020 CHI Conference on Human Factors in
		  Computing Systems},
  pages		= {1–14},
  numpages	= {14},
  keywords	= {feminist hci, natural language generation},
  location	= {Honolulu, HI, USA},
  series	= {CHI '20}
}

@InProceedings{	  10.1145/3711896.3736926,
  author	= {Wu, Xuan and Zhao, Yizheng},
  title		= {A Neuro-Symbolic Approach to Symbol Grounding for
		  ALC-Ontologies},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3736926},
  doi		= {10.1145/3711896.3736926},
  abstract	= {Neuro-symbolic computing aims to integrate neural learning
		  with symbolic reasoning to address the fundamental
		  challenge of symbol grounding. While neural networks excel
		  at pattern recognition, they struggle to maintain logical
		  consistency. Conversely, symbolic systems provide formal
		  reasoning capabilities but lack mechanisms for handling
		  perceptual uncertainty. This paper introduces EmALC , a
		  novel neuro-symbolic framework that bridges neural
		  perception with symbolic logic through differentiable fuzzy
		  semantics. Our approach addresses a key limitation of
		  existing methods: while previous neuro-symbolic approaches
		  like Logic Tensor Networks employ first-order fuzzy logic,
		  where key reasoning problems are undecidable, EmALC ensures
		  decidable reasoning by leveraging a fuzzy variant of ALC --
		  a decidable fragment of first-order logic. Unlike previous
		  approaches that often compromise logical soundness for
		  learning capability, EmALC maintains provable semantic
		  consistency through a hierarchical loss function while
		  mitigating reasoning shortcuts via rule-based revision
		  strategies. Experimental evaluation demonstrates EmALC's
		  effectiveness: on ontology revision tasks, it achieves
		  100\% success rate in correcting masked groundings while
		  preserving semantic integrity; on semantic image
		  interpretation tasks, it improves object classification
		  F1-scores by up to 5.56\% through ontology-guided knowledge
		  revision.},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {3240–3249},
  numpages	= {10},
  keywords	= {description logic, differentiable fuzzy semantics,
		  neuro-symbolic computing, symbol grounding},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@InProceedings{	  10.1145/3633637.3633645,
  author	= {Wang, Changlong and Gan, Tingting and Li, Xingyu and
		  Zhang, Linghan and Wang, Xijie},
  title		= {An Ontology-enhanced Knowledge Graph Embedding Method},
  year		= {2024},
  isbn		= {9798400707988},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3633637.3633645},
  doi		= {10.1145/3633637.3633645},
  abstract	= {Knowledge graph embedding maps entities and relationships
		  of the graph into low dimensional dense vectors to expresse
		  the semantic information, meantime provides effective
		  support for downstream tasks such as link prediction.
		  However, the existing knowledge graph embedding methods
		  mainly focus on the explicit structured information in the
		  graph and rarely use the entailed rich ontological
		  knowledge. Therefore in the paper, a method for injecting
		  ontology information into the embedding model is proposed,
		  ontology information including class hierarchy information
		  and relationship attribute constraints,especially symmetry
		  attributes are considerd. By taking ontology information as
		  extra constraints, the loss function is further refined.the
		  generation of training samples is optimized and the number
		  of false negative samples is limited. Experiments on the
		  two datasets of DBpedia15K and NELL show that the embedding
		  model can be further optimized by injecting ontology
		  information. Specially, the hit rate of triple prediction
		  is 63.70\% for the no-type, and for type-triples the MR and
		  the H@10 are 13.51 and 96.59\% respectively. The proposed
		  model has better performance than the basic model, which
		  further confirms the effectiveness of the prior knowledge
		  of ontology in knowledge graph embedding learning.},
  booktitle	= {Proceedings of the 2023 12th International Conference on
		  Computing and Pattern Recognition},
  pages		= {51–57},
  numpages	= {7},
  keywords	= {Knowledge embedding, Knowledge graph, Ontology
		  information, Reasoning, Symmetry},
  location	= {Qingdao, China},
  series	= {ICCPR '23}
}

@Article{	  10.14778/3401960.3401970,
  author	= {Kim, Hyeonji and So, Byeong-Hoon and Han, Wook-Shin and
		  Lee, Hongrae},
  title		= {Natural language to SQL: where are we today?},
  year		= {2020},
  issue_date	= {June 2020},
  publisher	= {VLDB Endowment},
  volume	= {13},
  number	= {10},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3401960.3401970},
  doi		= {10.14778/3401960.3401970},
  abstract	= {Translating natural language to SQL (NL2SQL) has received
		  extensive attention lately, especially with the recent
		  success of deep learning technologies. However, despite the
		  large number of studies, we do not have a thorough
		  understanding of how good existing techniques really are
		  and how much is applicable to real-world situations. A key
		  difficulty is that different studies are based on different
		  datasets, which often have their own limitations and
		  assumptions that are implicitly hidden in the context or
		  datasets. Moreover, a couple of evaluation metrics are
		  commonly employed but they are rather simplistic and do not
		  properly depict the accuracy of results, as will be shown
		  in our experiments. To provide a holistic view of NL2SQL
		  technologies and access current advancements, we perform
		  extensive experiments under our unified framework using
		  eleven of recent techniques over 10+ benchmarks including a
		  new benchmark (WTQ) and TPC-H. We provide a comprehensive
		  survey of recent NL2SQL methods, introducing a taxonomy of
		  them. We reveal major assumptions of the methods and
		  classify translation errors through extensive experiments.
		  We also provide a practical tool for validation by using
		  existing, mature database technologies such as query
		  rewrite and database testing. We then suggest future
		  research directions so that the translation can be used in
		  practice.},
  journal	= {Proc. VLDB Endow.},
  month		= jun,
  pages		= {1737–1750},
  numpages	= {14}
}

@InProceedings{	  10.1145/3555776.3577862,
  author	= {Rollo, Federica and Po, Laura and Castellucci,
		  Alessandro},
  title		= {CEM: an Ontology for Crime Events in Newspaper Articles},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3577862},
  doi		= {10.1145/3555776.3577862},
  abstract	= {The adoption of semantic technologies for the
		  representation of crime events can help law enforcement
		  agencies (LEAs) in crime prevention and investigation.
		  Moreover, online newspapers and social networks are
		  valuable sources for crime intelligence gathering. In this
		  paper, we propose a new lightweight ontology to model crime
		  events as they are usually described in online news
		  articles. The Crime Event Model (CEM) can integrate
		  specific data about crimes, i.e., where and when they
		  occurred, who is involved (author, victim, and other
		  subjects involved), which is the reason for the occurrence,
		  and details about the source of information (e.g., the news
		  article). Extracting structured data from multiple online
		  sources and interconnecting them in a Knowledge Graph using
		  CEM allow events relationships extraction, patterns and
		  trends identification, and event recommendation.The CEM
		  ontology is available at https://w3id.org/CEMontology.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1762–1765},
  numpages	= {4},
  keywords	= {newspaper, crime analysis, lightweight ontology},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@InProceedings{	  10.1145/3627915.3627925,
  author	= {Zhou, Zhenhuan and Chen, Yingyu and Wu, Rujing and Tao,
		  Jing},
  title		= {Ontology-based Case Representation of Mechatronic Product
		  Eco-design and Development of a Cloud-based Case Library},
  year		= {2023},
  isbn		= {9798400700590},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627915.3627925},
  doi		= {10.1145/3627915.3627925},
  abstract	= {Product life cycle eco-design is challenging,
		  knowledge-intensive and information dependent. This study
		  aims to develop a case library system for eco-design
		  knowledge management and to facilitate eco-design practice
		  of mechatronic product (i.e. manufacturing equipment,
		  construction machinery, vehicles). An ontology-based
		  representation of mechatronic product life cycle eco-design
		  is proposed, which enables the structuring and
		  standardization of related data and the storage of such
		  data in a computer-processable manner. Descriptive features
		  of eco-design case are defined in accordance to the
		  ontology model, based on which the similarity between
		  eco-design case and case queries is calculated using the
		  nearest neighbour method and case retrieval is realized
		  based on the calculated similarity. With the proposed
		  ontology model, a cloud-based eco-design case library is
		  then developed, which provide the benefits of easy
		  deployment and maintenance, and better accessibility.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Computer Science and Application Engineering},
  articleno	= {49},
  numpages	= {7},
  keywords	= {Cloud-based, Life Cycle Eco-Design, Ontology, Web
		  Service},
  location	= {Virtual Event, China},
  series	= {CSAE '23}
}

@InProceedings{	  10.1145/3411764.3445645,
  author	= {Wang, Qiaosi and Saha, Koustuv and Gregori, Eric and
		  Joyner, David and Goel, Ashok},
  title		= {Towards Mutual Theory of Mind in Human-AI Interaction: How
		  Language Reflects What Students Perceive About a Virtual
		  Teaching Assistant},
  year		= {2021},
  isbn		= {9781450380966},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411764.3445645},
  doi		= {10.1145/3411764.3445645},
  abstract	= {Building conversational agents that can conduct natural
		  and prolonged conversations has been a major technical and
		  design challenge, especially for community-facing
		  conversational agents. We posit Mutual Theory of Mind as a
		  theoretical framework to design for natural long-term
		  human-AI interactions. From this perspective, we explore a
		  community’s perception of a question-answering
		  conversational agent through self-reported surveys and
		  computational linguistic approach in the context of online
		  education. We first examine long-term temporal changes in
		  students’ perception of Jill Watson (JW), a virtual
		  teaching assistant deployed in an online class discussion
		  forum. We then explore the feasibility of inferring
		  students’ perceptions of JW through linguistic features
		  extracted from student-JW dialogues. We find that
		  students’ perception of JW’s anthropomorphism and
		  intelligence changed significantly over time. Regression
		  analyses reveal that linguistic verbosity, readability,
		  sentiment, diversity, and adaptability reflect student
		  perception of JW. We discuss implications for building
		  adaptive community-facing conversational agents as
		  long-term companions and designing towards Mutual Theory of
		  Mind in human-AI interaction.},
  booktitle	= {Proceedings of the 2021 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {384},
  numpages	= {14},
  keywords	= {theory of mind, online education, online community,
		  language analysis, human-AI interaction, conversational
		  agent},
  location	= {Yokohama, Japan},
  series	= {CHI '21}
}

@InProceedings{	  10.1145/3615366.3625069,
  author	= {Oliveira, Luiza Bartels and Araujo, Marco Antonio and
		  Dantas, Mario Antonio},
  title		= {A case study on the development of an ontology for
		  maintenance services of heavy machinery electronic
		  components},
  year		= {2023},
  isbn		= {9798400708442},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3615366.3625069},
  doi		= {10.1145/3615366.3625069},
  abstract	= {Inadequate data organization within a company can result
		  in decreased efficiency, increased costs, and longer
		  delivery times. In the context of an electronic maintenance
		  laboratory servicing a mining company internally, the lack
		  of data organization hampers the conversion of information
		  into actionable knowledge, affecting delivery efficiency.
		  This study aims to address these issues by transforming
		  existing data into structured knowledge using the
		  Methontology and OntoForInfoScience methodologies. The
		  ontology model was validated by both experts and a software
		  plug-in and seeks to offers a systematic representation of
		  the domain, facilitating data understanding and
		  utilization, ultimately leading to cost and time savings in
		  deliveries and ongoing process improvements.},
  booktitle	= {Proceedings of the 12th Latin-American Symposium on
		  Dependable and Secure Computing},
  pages		= {188–191},
  numpages	= {4},
  keywords	= {semantic data, ontology, mining, maintenance service.,
		  maintenance data, Knowledge acquisition},
  location	= {La Paz, Bolivia},
  series	= {LADC '23}
}

@InProceedings{	  10.1145/3330431.3330449,
  author	= {Zulfiya, Kaderkeyeva and Gulmira, Bekmanova and Altynbek,
		  Sharipbay},
  title		= {Ontological model for student's knowledge assessment},
  year		= {2019},
  isbn		= {9781450372121},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3330431.3330449},
  doi		= {10.1145/3330431.3330449},
  abstract	= {This article considers the ontological model for knowledge
		  representation on the example of the discipline "Database
		  Theory".Depending on the chosen intelligent system
		  development environment, knowledge must be represented by
		  certain data structures. In this paper, the ontological
		  model is chosen as a model for knowledge representation.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Engineering and MIS},
  articleno	= {18},
  numpages	= {5},
  keywords	= {sets, ontology, logic, knowledge models, knowledge base,
		  knowledge, artificial intelligence},
  location	= {Astana, Kazakhstan},
  series	= {ICEMIS '19}
}

@InProceedings{	  10.1145/3371158.3371198,
  author	= {Joshi, Salil Rajeev and Venkatesh, Bharath and Thomas,
		  Dawn and Jiao, Yue and Roy, Shourya},
  title		= {A Natural Language and Interactive End-to-End Querying and
		  Reporting System},
  year		= {2020},
  isbn		= {9781450377386},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3371158.3371198},
  doi		= {10.1145/3371158.3371198},
  abstract	= {Natural language query understanding for unstructured
		  textual sources has seen significant progress over the last
		  couple of decades. For structured data, while the ecosystem
		  has evolved with regard to data storage and retrieval
		  mechanisms, the query language has remained predominantly
		  SQL (or SQL-like). Towards making the latter more natural
		  there has been recent research emphasis on Natural Language
		  Interface to DataBases (NLIDB) systems. Piggybacking on the
		  rise of 'deep learning' systems, the state-of-the-art NLIDB
		  solutions over large parallel and standard benchmarks (viz,
		  WikiSQL and Spider) primarily rely on attention based
		  sequence-to-sequence models.Building industry grade NLIDB
		  solutions for making big data ecosystem accessible by truly
		  natural and unstructured querying mechanism presents
		  several challenges. These include lack of availability of
		  parallel corpora, diversity in underlying data schema, wide
		  variability in the nature of queries to context and dialog
		  management in interactive systems. In this paper, we
		  present an end-to-end system Query Enterprise Data (QED)
		  towards making enterprise descriptive analytics and
		  reporting easier and natural. We elaborate in detail how we
		  addressed the challenges mentioned above and novel features
		  such as handling incomplete queries in incremental fashion
		  as well as highlight the role of an assistive user
		  interface that provides a better user experience. Finally,
		  we conclude the paper with observations and lessons learnt
		  from the experience of transferring and deploying a
		  research solution to industry grade practical deployment.},
  booktitle	= {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
  pages		= {261–267},
  numpages	= {7},
  keywords	= {Semantic Parsing, SQL, Natural Language Understanding,
		  NLIDB, NL2SQL, Information Retrieval},
  location	= {Hyderabad, India},
  series	= {CoDS COMAD 2020}
}

@InProceedings{	  10.1145/3357236.3395489,
  author	= {Papenmeier, Andrea and Sliwa, Alfred and Kern, Dagmar and
		  Hienert, Daniel and Aker, Ahmet and Fuhr, Norbert},
  title		= { 'A Modern Up-To-Date Laptop' - Vagueness in Natural
		  Language Queries for Product Search},
  year		= {2020},
  isbn		= {9781450369749},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357236.3395489},
  doi		= {10.1145/3357236.3395489},
  abstract	= {With the rise of voice assistants and an increase in
		  mobile search usage, natural language has become an
		  important query language. So far, most of the current
		  systems are not able to process these queries because of
		  the vagueness and ambiguity in natural language. Users have
		  adapted their query formulation to what they think the
		  search engine is capable of, which adds to their cognitive
		  burden. With our research, we contribute to the design of
		  interactive search systems by investigating the genuine
		  information need in a product search scenario. In a
		  crowd-sourcing experiment, we collected 132 information
		  needs in natural language. We examine the vagueness of the
		  formulations and their match to retailer-generated content
		  and user-generated product reviews. Our findings reveal
		  high variance on the level of vagueness and the potential
		  of user reviews as a source for supporting users with
		  rather vague search intents.},
  booktitle	= {Proceedings of the 2020 ACM Designing Interactive Systems
		  Conference},
  pages		= {2077–2089},
  numpages	= {13},
  keywords	= {vagueness, query formulation, natural language,
		  information retrieval, information need},
  location	= {Eindhoven, Netherlands},
  series	= {DIS '20}
}

@Article{	  10.1145/3604561,
  author	= {Hu, Hengyi and Kerschberg, Larry},
  title		= {Improving Causal Bayesian Networks Using Expertise in
		  Authoritative Medical Ontologies},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {4},
  url		= {https://doi.org/10.1145/3604561},
  doi		= {10.1145/3604561},
  abstract	= {Discovering causal relationships among symptoms is a
		  topical issue in the analysis of observational patient
		  datasets. A Causal Bayesian Network (CBN) is a popular
		  analytical framework for causal inference. While there are
		  many methods and algorithms capable of learning a Bayesian
		  network, they are reliant on the complexity and
		  thoroughness of the algorithm and do not consider prior
		  expertise from authoritative sources. This article proposes
		  a novel method of extracting prior causal knowledge
		  contained in Authoritative Medical Ontologies (AMOs) and
		  using this prior knowledge to orient arcs in a CBN learned
		  from observational patient data. Since AMOs are robust
		  biomedical ontologies containing the collective knowledge
		  of the experts who created them, utilizing the ordering
		  information contained within them produces improved CBNs
		  that provide additional insight into the disease domain.To
		  demonstrate our method, we obtained prior causal ordering
		  information among symptoms from three AMOs: (1) the Medical
		  Dictionary for Regulatory Activities Terminology (MedDRA),
		  (2) the International Classification of Diseases Version 10
		  Clinical Modification (ICD-10-CM), and (3) Systematized
		  Nomenclature of Medicine Clinical Terms (SNOMED CT). The
		  prior ontological knowledge from these three AMOs is then
		  used to orient arcs in a series of CBNs learned from the
		  National Institutes of Mental Health study on Sequenced
		  Treatment Alternatives to Relieve Depression (STAR*D)
		  patient dataset using the Max-Min Hill-Climbing (MMHC)
		  algorithm. Six distinct CBNs are generated using MMHC: an
		  unmodified baseline model using only the algorithm, three
		  CBNs oriented with ordered-variable pairs from MedDRA,
		  ICD-10-CM, and SNOMED CT, and two more with ordered pairs
		  from a combination of these AMOs. The resulting CBNs
		  modified using ordered-variable pairs significantly change
		  the structure of the network. The agreement between the
		  Modified networks and the Baseline ranges from 50\% to
		  90\%. A modified network using ordering information from
		  all ontologies obtained an agreement of 50\% (10 out of 20
		  arcs exist in both the Baseline and Modified models) while
		  maintaining comparable predictive accuracy. This indicates
		  that the Modified CBN reflects the causal claims in the
		  AMOs and agrees with both the AMOs and the observational
		  STAR*D dataset. Furthermore, the Modified models discovered
		  new potentially causal relationships among symptoms in the
		  model, while eliminating weaker edges in a qualitative
		  analysis of the significance of these relationships in
		  existing epidemiological research.},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= oct,
  articleno	= {20},
  numpages	= {32},
  keywords	= {ontology evolution, ontology, healthcare information
		  technology, healthcare data, causality, causal networks,
		  causal inference, Bayesian networks, data management, data
		  mining, Patient data}
}

@InProceedings{	  10.1145/3666015.3666016,
  author	= {Boudjemila, Chahrazed and Dagnat, Fabien and
		  Mart\'{\i}nez, Salvador},
  title		= {Maintaining Security Consistency During System Development
		  with Security-Oriented Model Federation},
  year		= {2024},
  isbn		= {9798400709913},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3666015.3666016},
  doi		= {10.1145/3666015.3666016},
  abstract	= {Multi-modeling is an approach within the MDE realm that
		  promotes the development of complex systems by decomposing
		  them in sets of heterogeneous models. These models are
		  defined using different modeling languages and constructed
		  using diverse tools. They represent different but often
		  interdependent views. However, the models of a system are
		  far from being static. They change to accommodate new
		  requirements, functionality improvements, bug fixes, and
		  other evolution events. These changes represent a challenge
		  w.r.t. consistency. This is especially true in
		  security-critical scenarios. Indeed, security information
		  is often integrated within the systems models so that
		  security requirements are met following what is called
		  "security-by-design". In such scenarios, the security
		  concern of the systems models must remain consistent across
		  changes so that security properties continue to hold. In
		  order to tackle this problem, we propose a methodology to
		  enhance the (multi)model-based design phase of a system
		  development process. It comprises the creation of a
		  security federation in which security dependencies between
		  the different models are reified and equipped with security
		  rules expressing security consistency requirements. Then,
		  whenever a model is changed, the security rules are
		  evaluated to monitor the consistency of security across the
		  system models. We evaluate the capabilities of this
		  methodology by a prototype implementation and its
		  application to different use cases.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Software and Systems Processes},
  pages		= {66–76},
  numpages	= {11},
  keywords	= {Model-driven engineering, model evolution., model
		  federation, security by design},
  location	= {M\, Germany},
  series	= {ICSSP '24}
}

@InProceedings{	  10.1145/3366030.3366070,
  author	= {Shaaban, Abdelkader Magdy and Schmittner, Christoph and
		  Gruber, Thomas and Mohamed, A. Baith and Quirchmayr, Gerald
		  and Schikuta, Erich},
  title		= {Ontology-Based Model for Automotive Security Verification
		  and Validation},
  year		= {2020},
  isbn		= {9781450371797},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366030.3366070},
  doi		= {10.1145/3366030.3366070},
  abstract	= {Modern automobiles are considered semi-autonomous vehicles
		  regarding new adaptive technologies. New cars consist of a
		  vast number of electronic units for managing and
		  controlling the functional safety in a vehicle. In the
		  vehicular industry, safety and security are considered two
		  sides for the same coin. Therefore, improving functional
		  safety in the vehicular industry is essential to protect
		  the vehicle from different attack scenarios. This work
		  introduces an ontology-based model for security
		  verification and validation in the vehicular domain. The
		  model performs a series of logical quires and inference
		  rules to ensure that the security requirements are
		  fulfilled. It endeavors to enhance the current security
		  state of a vehicle by selecting additional security
		  requirements that can handle existence security weaknesses
		  and meet the actual security goal.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {73–82},
  numpages	= {10},
  keywords	= {Verification and Validation, Threats, Security
		  Requirements, Protection Profile, Ontology, Automotive},
  location	= {Munich, Germany},
  series	= {iiWAS2019}
}

@InProceedings{	  10.1145/3194104.3194105,
  author	= {Weigelt, Sebastian and Hey, Tobias and Landh\"{a}u\ss{}er,
		  Mathias},
  title		= {Integrating a dialog component into a framework for spoken
		  language understanding},
  year		= {2018},
  isbn		= {9781450357234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3194104.3194105},
  doi		= {10.1145/3194104.3194105},
  abstract	= {Spoken language interfaces are the latest trend in human
		  computer interaction. Users enjoy the newly found freedom
		  but developers face an unfamiliar and daunting task.
		  Creating reactive spoken language interfaces requires
		  skills in natural language processing. We show how a
		  developer can integrate a dialog component in a natural
		  language processing system by means of software engineering
		  methods. Our research project PARSE that aims at
		  naturalistic end-user programming in spoken natural
		  language serves as an example. We integrate a dialog
		  component with PARSE without affecting its other
		  components: We modularize the dialog management and
		  introduce dialog acts that bundle a trigger for the dialog
		  and the reaction of the system. We implemented three dialog
		  acts to address the following issues: speech recognition
		  uncertainties, coreference ambiguities, and incomplete
		  conditionals.We conducted a user study with ten subjects to
		  evaluate our approach. The dialog component achieved
		  resolution rates from 23\% to 50\% (depending on the dialog
		  act) and introduces a negligible number of errors. We
		  expect the overall performance to increase even further
		  with the implementation of additional dialog acts.},
  booktitle	= {Proceedings of the 6th International Workshop on Realizing
		  Artificial Intelligence Synergies in Software Engineering},
  pages		= {1–7},
  numpages	= {7},
  keywords	= {programming in natural language, naturalistic programming,
		  natural language processing for software engineering,
		  knowledge-based software engineering, human-computer
		  interaction, enduser programming, dialog systems, dialog
		  integration},
  location	= {Gothenburg, Sweden},
  series	= {RAISE '18}
}

@InProceedings{	  10.1145/3706598.3713789,
  author	= {Velloso, Eduardo and Hornb\ae{}k, Kasper},
  title		= {Theorising in HCI using Causal Models},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713789},
  doi		= {10.1145/3706598.3713789},
  abstract	= {Although the literature on Human-Computer Interaction
		  (HCI) catalogues many theories, it offers surprisingly few
		  tools for theorising. This paper critiques dominant
		  approaches to engaging with theory and proposes a working
		  model for theorising in HCI. We then present graphical
		  causal modelling as an effective theorising tool. This
		  includes a step-by-step guide to building causal models and
		  examples of their use in different stages of the research
		  process. We explain how causal models help develop
		  method-agnostic representations of research problems using
		  directed acyclic graphs, identify potential confounders,
		  and construct alternative interpretations of data. Finally,
		  we discuss their limitations and challenges for adoption by
		  the HCI community.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {484},
  numpages	= {17},
  keywords	= {Causal modelling, HCI theory, directed acyclic graphs},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3314221.3314594,
  author	= {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad
		  and Socher, Richard and Lam, Monica S.},
  title		= {Genie: a generator of natural language semantic parsers
		  for virtual assistant commands},
  year		= {2019},
  isbn		= {9781450367127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3314221.3314594},
  doi		= {10.1145/3314221.3314594},
  abstract	= {To understand diverse natural language commands, virtual
		  assistants today are trained with numerous labor-intensive,
		  manually annotated sentences. This paper presents a
		  methodology and the Genie toolkit that can handle new
		  compound commands with significantly less manual effort. We
		  advocate formalizing the capability of virtual assistants
		  with a Virtual Assistant Programming Language (VAPL) and
		  using a neural semantic parser to translate natural
		  language into VAPL code. Genie needs only a small realistic
		  set of input sentences for validating the neural model.
		  Developers write templates to synthesize data; Genie uses
		  crowdsourced paraphrases and data augmentation, along with
		  the synthesized data, to train a semantic parser. We also
		  propose design principles that make VAPL languages amenable
		  to natural language translation. We apply these principles
		  to revise ThingTalk, the language used by the Almond
		  virtual assistant. We use Genie to build the first semantic
		  parser that can support compound virtual assistants
		  commands with unquoted free-form parameters. Genie achieves
		  a 62\% accuracy on realistic user inputs. We demonstrate
		  Genie’s generality by showing a 19\% and 31\% improvement
		  over the previous state of the art on a music skill,
		  aggregate functions, and access control.},
  booktitle	= {Proceedings of the 40th ACM SIGPLAN Conference on
		  Programming Language Design and Implementation},
  pages		= {394–410},
  numpages	= {17},
  keywords	= {virtual assistants, training data generation, semantic
		  parsing, data engineering, data augmentation},
  location	= {Phoenix, AZ, USA},
  series	= {PLDI 2019}
}

@InProceedings{	  10.1145/3472301.3484327,
  author	= {Castro, Murillo V. H. B. and Barcellos, Monalessa P. and
		  de A. Falbo, Ricardo and Costa, Simone D.},
  title		= {Using Ontologies to aid Knowledge Sharing in HCI Design},
  year		= {2021},
  isbn		= {9781450386173},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3472301.3484327},
  doi		= {10.1145/3472301.3484327},
  abstract	= {Developing interactive systems is a challenging task that
		  involves concerns related to the human-computer interaction
		  (HCI), such as usability and user experience. Therefore,
		  HCI design is a core issue when developing such systems. It
		  often involves people with different backgrounds (e.g.,
		  Arts, Software Engineering, Design), which makes knowledge
		  transfer a challenging issue. Ontologies have been
		  acknowledged as a successful approach to represent domain
		  knowledge and support knowledge-based solutions. Hence, in
		  this work, we propose to explore ontologies to represent
		  structured knowledge and improve knowledge sharing in HCI
		  design. We briefly present the Human-Computer Interaction
		  Design Ontology (HCIDO), a reference ontology that
		  addresses HCI design aspects that connect HCI and Software
		  Engineering concerns. By making knowledge related to the
		  HCI design domain explicit and structured, HCIDO has helped
		  us to develop KTID, a tool that aims to support capturing
		  and sharing useful knowledge to aid in HCI design.
		  Preliminary results indicate that the tool may be
		  particularly useful for novice HCI designers.},
  booktitle	= {Proceedings of the XX Brazilian Symposium on Human Factors
		  in Computing Systems},
  articleno	= {50},
  numpages	= {7},
  keywords	= {Ontology, Knowledge, HCI Design},
  location	= {Virtual Event, Brazil},
  series	= {IHC '21}
}

@Article{	  10.1145/3729530,
  author	= {Pradeep, Anagha and Mamidi, Radhika},
  title		= {Sandar\'{s}ana: A Survey on Sanskrit Computational
		  Linguistics and Digital Infrastructure for Sanskrit},
  year		= {2025},
  issue_date	= {October 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {10},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3729530},
  doi		= {10.1145/3729530},
  abstract	= {Computational Linguistics is an interdisciplinary field of
		  computer science and linguistics that focuses on designing
		  computational models and algorithms for processing,
		  analyzing, and generating human language. Over recent
		  years, this field has made substantial progress. While its
		  primary emphasis tends to center around widely spoken
		  languages, there is equal importance in investigating
		  languages that are not commonly spoken but have contributed
		  immensely to the literature, culture, and philosophy of the
		  society. Thus, this survey article comprehensively delves
		  into the exploration of computational tasks undertaken for
		  Sanskrit, an ancient language of the Indian sub-continent
		  steeped in a wealth of literary heritage. The purpose of
		  this study is to provide an overview of the progress made
		  thus far in the computational analysis of Sanskrit, while
		  also reviewing the current digital infrastructure that
		  supports these efforts. Additionally, our study also
		  identifies potential avenues for future research, serving
		  as a reference for anyone interested in advancing their
		  exploration in this field.},
  journal	= {ACM Comput. Surv.},
  month		= may,
  articleno	= {254},
  numpages	= {38},
  keywords	= {Sanskrit computational linguistics,
		  Pundefinedundefinedini,
		  Aundefinedundefinedundefineddhyundefinedyundefined}
}

@InProceedings{	  10.1145/3652620.3687791,
  author	= {Shaked, Avi and Messe, Nan and Melham, Tom},
  title		= {Modelling Tool Extension for Vulnerability Management},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687791},
  doi		= {10.1145/3652620.3687791},
  abstract	= {Managing vulnerabilities with respect to the design of
		  systems is essential to securing systems and establishing
		  their trustworthiness. Until now, there has been no
		  modelling tool to support vulnerability management within
		  the context of system design. We present a new, open-source
		  extension of a systems security design and assessment tool.
		  First and foremost, this extension integrates a pertinent
		  vulnerability management domain ontology into the tool's
		  underlying metamodel. Based on the extended metamodel, the
		  enriched tool supports importing information from
		  vulnerability-related knowledge bases as well as capturing
		  new vulnerability information and security rules. This
		  information can then be used in an integrative and scalable
		  form to analyse and reason about the security of systems
		  designs. The extended tool now includes an automated
		  reasoning mechanism for establishing the vulnerability
		  posture of systems designs.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {56–60},
  numpages	= {5},
  keywords	= {model driven engineering, threat modelling, vulnerability
		  management, security by design},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3330431.3330438,
  author	= {Suleimenova, Laura and Zhomartkyzy, Gulnaz and
		  Kumargazhanova, Saule},
  title		= {Integration data models based on ontology},
  year		= {2019},
  isbn		= {9781450372121},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3330431.3330438},
  doi		= {10.1145/3330431.3330438},
  abstract	= {Evaluation of the performance of university staff can be
		  carried out with different goals, consistent with the
		  strategic goals of the institution. To provide an
		  appropriate data set, an appropriate structure must be
		  provided that ensures that neither incomplete nor reliable
		  data will be used. To provide such a structure with the
		  best possible features data from the various available
		  sources should be integrated. The article provides an
		  overview of the existing integration problems and
		  approaches to solving this problem.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Engineering and MIS},
  articleno	= {7},
  numpages	= {5},
  keywords	= {ontology, ontological model, monitoring, information
		  system, data integration},
  location	= {Astana, Kazakhstan},
  series	= {ICEMIS '19}
}

@InProceedings{	  10.1145/3704268.3748680,
  author	= {Barron, Ryan C. and Eren, Maksim E. and Stanev, Valentin
		  and Matuszek, Cynthia and Alexandrov, Boian S.},
  title		= {Topic Modeling and Link-Prediction for Material Property
		  Discovery},
  year		= {2025},
  isbn		= {9798400713514},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704268.3748680},
  doi		= {10.1145/3704268.3748680},
  abstract	= {Link prediction is a key network analysis technique that
		  infers missing or future relations between nodes in a
		  graph, based on observed patterns of connectivity.
		  Scientific literature networks and knowledge graphs are
		  typically large, sparse, and noisy, and often contain
		  missing links, potential but unobserved connections,
		  between concepts, entities, or methods. Here, we present an
		  AI-driven hierarchical link prediction framework that
		  integrates matrix factorization to infer hidden
		  associations and steer discovery in complex material
		  domains. Our method combines Hierarchical Nonnegative
		  Matrix Factorization (HNMFk), Boolean matrix factorization
		  (BNMFk) with automatic model selection. These discrete
		  factors are then fused with Logistic matrix factorization
		  (LMF), we use to construct a three-level topic tree from a
		  46,862-document corpus focused on 73 transition-metal
		  dichalcogenides (TMDs). This class of materials has been
		  studied in a variety of physics fields and has a multitude
		  of current and potential applications.An ensemble BNMFk +
		  LMF approach fuses discrete interpretability with
		  probabilistic scoring. The resulting HNMFk clusters map
		  each material onto coherent research themes, such as
		  superconductivity, energy storage, and tribology, and
		  highlight missing or weakly connected links between topics
		  and materials, suggesting novel hypotheses for
		  cross-disciplinary exploration. We validate our method by
		  removing publications about superconductivity in well-known
		  superconductors, and demonstrate that the model correctly
		  predicts their association with the superconducting TMD
		  clusters. This highlights the ability of the method to find
		  hidden connections in a graph of material to latent topic
		  associations built from scientific literature. This is
		  especially useful when examining a diverse corpus of
		  scientific documents covering the same class of phenomena
		  or materials but originating from distinct communities and
		  perspectives. The inferred links generating new hypotheses,
		  produced by our method, are exposed through an interactive
		  Streamlit dashboard, designed for scientific discovery.},
  booktitle	= {Proceedings of the 2025 ACM Symposium on Document
		  Engineering},
  articleno	= {10},
  numpages	= {4},
  keywords	= {LMF, Link Prediction, Matrix completion, NMF, NMFk},
  location	= {Nottingham, United Kingdom},
  series	= {DocEng '25}
}

@InProceedings{	  10.1145/3468791.3469119,
  author	= {Sima, Ana Claudia and Mendes de Farias, Tarcisio and
		  Anisimova, Maria and Dessimoz, Christophe and
		  Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger,
		  Kurt},
  title		= {Bio-SODA: Enabling Natural Language Question Answering
		  over Knowledge Graphs without Training Data},
  year		= {2021},
  isbn		= {9781450384131},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3468791.3469119},
  doi		= {10.1145/3468791.3469119},
  abstract	= {The problem of natural language processing over structured
		  data has become a growing research field, both within the
		  relational database and the Semantic Web community, with
		  significant efforts involved in question answering over
		  knowledge graphs (KGQA). However, many of these approaches
		  are either specifically targeted at open-domain question
		  answering using DBpedia, or require large training datasets
		  to translate a natural language question to SPARQL in order
		  to query the knowledge graph. Hence, these approaches often
		  cannot be applied directly to complex scientific datasets
		  where no prior training data is available. In this paper,
		  we focus on the challenges of natural language processing
		  over knowledge graphs of scientific datasets. In
		  particular, we introduce Bio-SODA, a natural language
		  processing engine that does not require training data in
		  the form of question-answer pairs for generating SPARQL
		  queries. Bio-SODA uses a generic graph-based approach for
		  translating user questions to a ranked list of SPARQL
		  candidate queries. Furthermore, Bio-SODA uses a novel
		  ranking algorithm that includes node centrality as a
		  measure of relevance for selecting the best SPARQL
		  candidate query. Our experiments with real-world datasets
		  across several scientific domains, including the official
		  bioinformatics Question Answering over Linked Data (QALD)
		  challenge, as well as the CORDIS dataset of European
		  projects, show that Bio-SODA outperforms publicly available
		  KGQA systems by an F1-score of least 20\% and by an even
		  higher factor on more complex bioinformatics datasets.},
  booktitle	= {Proceedings of the 33rd International Conference on
		  Scientific and Statistical Database Management},
  pages		= {61–72},
  numpages	= {12},
  keywords	= {Ranking, Question Answering, Knowledge Graphs},
  location	= {Tampa, FL, USA},
  series	= {SSDBM '21}
}

@Article{	  10.1145/3747585,
  author	= {Rost, Mattias},
  title		= {Reclaiming the Computer through LLM-Mediated Computing},
  year		= {2025},
  issue_date	= {September - October 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {32},
  number	= {5},
  issn		= {1072-5520},
  url		= {https://doi.org/10.1145/3747585},
  doi		= {10.1145/3747585},
  journal	= {Interactions},
  month		= aug,
  pages		= {26–31},
  numpages	= {6}
}

@InProceedings{	  10.1145/3485447.3511921,
  author	= {Ye, Hongbin and Zhang, Ningyu and Deng, Shumin and Chen,
		  Xiang and Chen, Hui and Xiong, Feiyu and Chen, Xi and Chen,
		  Huajun},
  title		= {Ontology-enhanced Prompt-tuning for Few-shot Learning},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511921},
  doi		= {10.1145/3485447.3511921},
  abstract	= {Few-shot Learning (FSL) is aimed to make predictions based
		  on a limited number of samples. Structured data such as
		  knowledge graphs and ontology libraries has been leveraged
		  to benefit the few-shot setting in various tasks. However,
		  the priors adopted by the existing methods suffer from
		  challenging knowledge missing, knowledge noise, and
		  knowledge heterogeneity, which hinder the performance for
		  few-shot learning. In this study, we explore knowledge
		  injection for FSL with pre-trained language models and
		  propose ontology-enhanced prompt-tuning (OntoPrompt).
		  Specifically, we develop the ontology transformation based
		  on the external knowledge graph to address the knowledge
		  missing issue, which fulfills and converts structure
		  knowledge to text. We further introduce span-sensitive
		  knowledge injection via a visible matrix to select
		  informative knowledge to handle the knowledge noise issue.
		  To bridge the gap between knowledge and text, we propose a
		  collective training algorithm to optimize representations
		  jointly. We evaluate our proposed OntoPrompt in three
		  tasks, including relation extraction, event extraction, and
		  knowledge graph completion, with eight datasets.
		  Experimental results demonstrate that our approach can
		  obtain better few-shot performance than baselines.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {778–787},
  numpages	= {10},
  keywords	= {Event Extraction, Few-shot Learning, Knowledge Graph
		  Completion, Ontology, Prompt-tuning, Relation Extraction},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Article{	  10.1145/3631976,
  author	= {Cederbladh, Johan and Cicchetti, Antonio and Suryadevara,
		  Jagadish},
  title		= {Early Validation and Verification of System Behaviour in
		  Model-based Systems Engineering: A Systematic Literature
		  Review},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {3},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3631976},
  doi		= {10.1145/3631976},
  abstract	= {In the Systems Engineering (SE) domain there has been a
		  paradigm shift from document-based to model-based system
		  development artefacts; in fact, new methodologies are
		  emerging to meet the increasing complexity of current
		  systems and the corresponding growing need of digital
		  workflows. In this regard, Model-Based Systems Engineering
		  (MBSE) is considered as a key enabler by many central
		  players of the SE community. MBSE has reached an adequate
		  level of maturity, and there exist documented success
		  stories in its adoption in industry. In particular, one
		  significant benefit of utilising MBSE when compared to the
		  traditional manual and document-centric workflows is that
		  models are available from early phases of systems
		  development; these enable a multitude of analyses prior any
		  implementation effort together with other relevant
		  capabilities, like the automation of development tasks.
		  Nonetheless, it is noticeable there is a lack of a common
		  understanding for how formal analyses for the verification
		  and validation (V&amp;V) of systems behaviour, specifically
		  in the early phases of development, could be placed in an
		  MBSE setting.In this article, we report on the planning,
		  execution, and results of a systematic literature review
		  regarding the early V&amp;V of systems behaviour in the
		  context of model-based systems engineering. The review aims
		  to provide a structured representation of the state of the
		  art with respect to motivations, proposed solutions, and
		  limitations. From an initial set of potentially relevant
		  701 peer-reviewed publications we selected 149 primary
		  studies, which we analysed according to a rigorous data
		  extraction, analysis, and synthesis process. Based on our
		  results, early V&amp;V has usually the goal of checking the
		  quality of a system design to avoid discovering flaws when
		  parts are being concretely realised; SysML is a de facto
		  standard for describing the system under study, while the
		  solutions for the analyses tend to be varied; also V&amp;V
		  analyses tend to target varied properties with a slight
		  predominance of functional concerns, and following the
		  variation mentioned so far the proposed solutions are
		  largely context specific; the proposed approaches are
		  usually presented without explicit limitations, while when
		  limitations are discussed, readiness of the solutions,
		  handling of analyses simplifications/assumptions, and
		  languages/tools integration are among the most frequently
		  mentioned issues.Based on the survey results and the
		  standard SE practices, we discuss how the current
		  state-of-the-art MBSE supports early V&amp;V of systems
		  behaviour with a special focus on industrial adoption and
		  identify relevant challenges to be researched further.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= mar,
  articleno	= {81},
  numpages	= {67},
  keywords	= {MBSE, validation, verification, system behaviour,
		  systematic literature review}
}

@InProceedings{	  10.1145/3708319.3733654,
  author	= {Ferilli, Stefano},
  title		= {Knowledge Graph-based User Models and Personalized Access
		  for Cultural Heritage},
  year		= {2025},
  isbn		= {9798400713996},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708319.3733654},
  doi		= {10.1145/3708319.3733654},
  abstract	= {Cultural Heritage is opening up from the professional
		  community to a wider public, generating an increasing
		  demand for culture and an associated economic turnaround.
		  This step requires to differentiate the behavior of
		  Cultural Heritage systems, dealing with a wide variety of
		  backgrounds, expectations, contexts, aims, educational and
		  cultural level, preferences and interests. Computer Science
		  and Artificial Intelligence can play a key role in this
		  landscape, fine-tuning the fruition of cultural items to
		  every kind of stakeholder and even to single users. In this
		  paper we present an approach to personalization of Cultural
		  Heritage fruition based on Knowledge Graphs. An approach to
		  describe user models, and to use them for extracting
		  personalized information, is proposed, and a platform that
		  embeds this approach is described.},
  booktitle	= {Adjunct Proceedings of the 33rd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {437–441},
  numpages	= {5},
  keywords	= {User Models, Personalized Information Access, Knowledge
		  Graphs, Cultural Heritage},
  location	= { },
  series	= {UMAP Adjunct '25}
}

@InProceedings{	  10.1145/3357223.3362701,
  author	= {Dogga, Pradeep and Narasimhan, Karthik and Sivaraman,
		  Anirudh and Netravali, Ravi},
  title		= {A System-Wide Debugging Assistant Powered by Natural
		  Language Processing},
  year		= {2019},
  isbn		= {9781450369732},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357223.3362701},
  doi		= {10.1145/3357223.3362701},
  abstract	= {Despite advances in debugging tools, systems debugging
		  today remains largely manual. A developer typically follows
		  an iterative and time-consuming process to move from a
		  reported bug to a bug fix. This is because developers are
		  still responsible for making sense of system-wide
		  semantics, bridging together outputs and features from
		  existing debugging tools, and extracting information from
		  many diverse data sources (e.g., bug reports, source code,
		  comments, documentation, and execution traces). We believe
		  that the latest statistical natural language processing
		  (NLP) techniques can help automatically analyze these data
		  sources and significantly improve the systems debugging
		  experience. We present early results to highlight the
		  promise of NLP-powered debugging, and discuss systems and
		  learning challenges that must be overcome to realize this
		  vision.},
  booktitle	= {Proceedings of the ACM Symposium on Cloud Computing},
  pages		= {171–177},
  numpages	= {7},
  keywords	= {systems debugging, natural language processing},
  location	= {Santa Cruz, CA, USA},
  series	= {SoCC '19}
}

@InProceedings{	  10.1145/3715336.3735832,
  author	= {Calderwood, Alex and Chung, John Joon Young and Sun,
		  Yuqian and Roemmele, Melissa and Kreminski, Max},
  title		= {Phraselette: A Poet’s Procedural Palette},
  year		= {2025},
  isbn		= {9798400714856},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715336.3735832},
  doi		= {10.1145/3715336.3735832},
  abstract	= {According to the recently introduced theory of artistic
		  support tools, creativity support tools exert normative
		  influences over artistic production, instantiating a
		  normative ground that shapes both the process and product
		  of artistic expression. We argue that the normative ground
		  of most existing automated writing tools is misaligned with
		  writerly values and identify a potential alternative
		  frame—material writing support—for experimental poetry
		  tools that flexibly support the finding, processing,
		  transforming, and shaping of text(s). Based on this frame,
		  we introduce Phraselette, an artistic material writing
		  support interface that helps experimental poets search for
		  words and phrases. To provide material writing support,
		  Phraselette is designed to counter the dominant mode of
		  automated writing tools, while offering language model
		  affordances in line with writerly values. We further report
		  on an extended expert evaluation involving 10 published
		  poets that indicates support for both our framing of
		  material writing support and for Phraselette itself.},
  booktitle	= {Proceedings of the 2025 ACM Designing Interactive Systems
		  Conference},
  pages		= {2701–2717},
  numpages	= {17},
  keywords	= {Creative Writing, Language Models, Poetry, Artistic
		  Support Tool, CST, Material, Search},
  location	= { },
  series	= {DIS '25}
}

@InProceedings{	  10.1145/3332165.3347899,
  author	= {Li, Toby Jia-Jun and Radensky, Marissa and Jia, Justin and
		  Singarajah, Kirielle and Mitchell, Tom M. and Myers, Brad
		  A.},
  title		= {PUMICE: A Multi-Modal Agent that Learns Concepts and
		  Conditionals from Natural Language and Demonstrations},
  year		= {2019},
  isbn		= {9781450368162},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3332165.3347899},
  doi		= {10.1145/3332165.3347899},
  abstract	= {Natural language programming is a promising approach to
		  enable end users to instruct new tasks for intelligent
		  agents. However, our formative study found that end users
		  would often use unclear, ambiguous or vague concepts when
		  naturally instructing tasks in natural language, especially
		  when specifying conditionals. Existing systems have limited
		  support for letting the user teach agents new concepts or
		  explaining unclear concepts. In this paper, we describe a
		  new multi-modal domain-independent approach that combines
		  natural language programming and
		  programming-by-demonstration to allow users to first
		  naturally describe tasks and associated conditions at a
		  high level, and then collaborate with the agent to
		  recursively resolve any ambiguities or vagueness through
		  conversations and demonstrations. Users can also define new
		  procedures and concepts by demonstrating and referring to
		  contents within GUIs of existing mobile apps. We
		  demonstrate this approach in PUMICE, an end-user
		  programmable agent that implements this approach. A lab
		  study with 10 users showed its usability.},
  booktitle	= {Proceedings of the 32nd Annual ACM Symposium on User
		  Interface Software and Technology},
  pages		= {577–589},
  numpages	= {13},
  keywords	= {programming by demonstration, natural language
		  programming, multi-modal interaction, end user
		  development},
  location	= {New Orleans, LA, USA},
  series	= {UIST '19}
}

@InProceedings{	  10.1145/3360901.3364448,
  author	= {Szekely, Pedro and Garijo, Daniel and Bhatia, Divij and
		  Wu, Jiasheng and Yao, Yixiang and Pujara, Jay},
  title		= {T2WML: Table To Wikidata Mapping Language},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364448},
  doi		= {10.1145/3360901.3364448},
  abstract	= {The web contains millions of useful spreadsheets and CSV
		  files, but these files are difficult to use in applications
		  because they use a wide variety of data layouts and
		  terminology. We present Table To Wikidata Mapping Language
		  (T2WML), a language that makes it easy to map and link
		  arbitrary spreadsheets and CSV files to the Wikidata data
		  model. The output of T2WML consists of Wikidata statements
		  that can be loaded in the public Wikidata knowledge base or
		  in a Wikidata clone repository, creating an augmented
		  Wikidata knowledge graph that application developers can
		  query using SPARQL.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {267–270},
  numpages	= {4},
  keywords	= {wikidata, rdf, knowledge graphs, entity linking},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.1145/3430984.3431002,
  author	= {Sen, Jaydeep and Saha, Diptikalyan and Mittal, Ashish and
		  Sankaranarayanan, Karthik},
  title		= {Optimizing Interpretation Generation in Natural Language
		  Query Answering for Real Time End Users},
  year		= {2021},
  isbn		= {9781450388177},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3430984.3431002},
  doi		= {10.1145/3430984.3431002},
  abstract	= {Natural Language Querying over Database is gaining
		  popularity across different use cases. Most common of them
		  is to democratize the process of data analysis and querying
		  of backend data to naive end users especially business
		  users, obviating the need of knowing back end query
		  language. Natural Language Query answering systems have
		  thus seen widespread usage in industry too where business
		  users want to search their own data to make business
		  decisions. However, a common challenge faced by any natural
		  language query answering system is generation of precise
		  interpretations. The research community although tries to
		  handle the problem via asking clarification questions back
		  to the user, in industry setup this remains an ineffective
		  solution due to various practical usage limitations. For
		  example, it is not fair to assume any end user will be
		  aware of the correct option to answer these clarification
		  questions. Moreover, involving clarification questions and
		  user feedbacks makes the system unusable by one shot API
		  calls, which is the most intuitive usage among common use
		  cases in industry like automated report generation. In this
		  paper, we investigate practical ways to address the problem
		  of precise interpretation generation. We propose novel
		  algorithms to make use of existing technologies like
		  Functional Partitioning of Ontology and Lazy Inclusion to
		  solve this problem. We take our previous state-of-the-art
		  paper ATHENA and further extend it to include our proposed
		  methods. We test with 3 benchmark ontologies to empirically
		  demonstrate the huge improvement over state-of-the-art
		  results by factors of at least 400\% in number of
		  interpretation generation and also in the computation
		  time.},
  booktitle	= {Proceedings of the 3rd ACM India Joint International
		  Conference on Data Science \&amp; Management of Data (8th
		  ACM IKDD CODS \&amp; 26th COMAD)},
  pages		= {341–349},
  numpages	= {9},
  location	= {Bangalore, India},
  series	= {CODS-COMAD '21}
}

@InProceedings{	  10.1145/3550356.3561548,
  author	= {Medinacelli, Luis Palacios and Noyrit, Florian and
		  Mraidha, Chokri},
  title		= {Augmenting model-based systems engineering with
		  knowledge},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561548},
  doi		= {10.1145/3550356.3561548},
  abstract	= {This article presents a general approach for the
		  integration of Knowledge Bases into Model-Based Systems
		  Engineering tools. In existing tools, domain-specific
		  modeling languages are well supported. However when it
		  comes to enforcing design constraints, existing approaches
		  are verbose, it is difficult to be complete and consistent,
		  and the reuse of knowledge is only possible in a limited
		  way (mainly through model libraries). Furthermore, current
		  tools usually lack or have limited capability to detect
		  semantic errors, ability to evaluate the models with
		  respect to formal expert knowledge, and the ability to
		  understand what is being designed. Our work addresses these
		  limitations through the semantic annotation of UML models
		  in Papyrus (an MBSE Tool), to attach domain-specific
		  semantics to the models. This integration enables not only
		  reasoning capabilities over the annotated models, but the
		  models can be shared with semantic-compatible tools and
		  stakeholders. Moreover, the models can reuse and integrate
		  knowledge generated outside the tooling environment. The
		  approach's feasibility is demonstrated through an
		  implementation that defines a technology stack, with
		  emphasis on the mapping of UML elements and its
		  counterparts in the ontology. We address the coherence and
		  preservation of the semantics throughout the transformation
		  process, which enable the formalization of constraints
		  coming from the UML's system design. Finally, we illustrate
		  the reasoning capabilities by evaluating expert knowledge
		  via SPARQL queries and SWRL rules.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {351–358},
  numpages	= {8},
  keywords	= {semantic interoperability, ontology, model-driven
		  engineering, knowledge based engineering, UML, Papyrus},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3679318.3685501,
  author	= {Krapp, Eva and Neuhaus, Robin and Hassenzahl, Marc and
		  Laschke, Matthias},
  title		= {In a Quasi-Social Relationship With ChatGPT. An
		  Autoethnography on Engaging With Prompt-Engineered LLM
		  Personas},
  year		= {2024},
  isbn		= {9798400709661},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3679318.3685501},
  doi		= {10.1145/3679318.3685501},
  abstract	= {As conversational AI like ChatGPT becomes more
		  sophisticated, understanding emerging quasi-social
		  relationships with it is crucial. Through analytical
		  autoethnography, we explore the nuances of these
		  relationships with two autobiographically designed ChatGPT
		  personas to augment the social needs of the first author:
		  the Endless Enthusiast (always responding positively and
		  encouragingly) and the Socratic Tutor (asking questions to
		  stimulate critical thinking). After six weeks of
		  interaction, we find that for a successful relationship,
		  the non-human counterpart must be authentic about its
		  machine nature and limitations. Using deception to appear
		  more human-like makes the relationship fail. We thus
		  suggest designing machine relationships as complementary to
		  human-human relationships. For authentic interactions,
		  humans should be in control, with the machine authentically
		  assuming a role that "naturally" fits machines. Here, the
		  unique qualities of machines in social interactions offer
		  promising starting points for designing such roles.},
  booktitle	= {Proceedings of the 13th Nordic Conference on
		  Human-Computer Interaction},
  articleno	= {79},
  numpages	= {16},
  keywords	= {autobiographical research through design, autoethnography,
		  human-AI interaction},
  location	= {Uppsala, Sweden},
  series	= {NordiCHI '24}
}

@InBook{	  10.5555/3712729.3712962,
  author	= {Blas, Mar\'{\i}a Julia and Gonnet, Silvio},
  title		= {A Modeling Framework for Complex Systems},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {This paper presents a modeling framework for defining
		  abstractions of real-world complex systems promoting the
		  development of discrete-event simulation models based on
		  DEVS. An ontology, a metamodel, and a reasoner are combined
		  in one single structure to allow an upgrade of an
		  abstraction model to an implementation model. Our
		  motivation is to reduce the effort related to the modeling
		  part when specifying DEVS models for complex systems
		  described from an abstraction of reality built over a
		  research question. Applications include an easier
		  introduction to M&amp;S for students of any scientific
		  field that can define an abstraction model with an easier
		  introduction to DEVS models (from formalization to
		  implementation).},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2809–2820},
  numpages	= {12}
}

@InProceedings{	  10.1145/3674213.3674222,
  author	= {Mori, Kosuke and Kondo, Takao and Teraoka, Fumio},
  title		= {Detecting Inconsistency between Network Design and Current
		  State Based on Network Ontology Bonsai},
  year		= {2024},
  isbn		= {9798400709852},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3674213.3674222},
  doi		= {10.1145/3674213.3674222},
  abstract	= {Generally, a network administrator designs, constructs,
		  and operates an enterprise network. Since inconsistency
		  between the network design understood by the administrator
		  and the actual network configuration might arise due to
		  mistakes or errors, a method for automatically detecting
		  such inconsistency is needed. The following four techniques
		  are necessary for this purpose. (i) A machine-readable
		  notation to represent the network configuration. (ii) A
		  tool to write down the network design using the
		  machine-readable notation. (iii) A tool to automatically
		  detect the current network configuration and write it down
		  in the machine-readable notation. (iv) A tool to compare
		  the two outputs generated in (ii) and (iii). This paper
		  employs the network ontology called Bonsai for (i). Bonsai
		  can represent not only physical configurations but also
		  virtualization technologies such as VLAN and overlay. This
		  paper proposes three tools, nc-design, nc-detect, and
		  nc-diff for (ii)-(iv), and confirms that they work as
		  expected in the test network. This paper also measures
		  their fundamental performance.},
  booktitle	= {Proceedings of the Asian Internet Engineering Conference
		  2024},
  pages		= {76–84},
  numpages	= {9},
  keywords	= {network configuration detection, network management,
		  network ontology},
  location	= {Sydney, NSW, Australia},
  series	= {AINTEC '24}
}

@Article{	  10.1145/3708492,
  author	= {Wang, Di and Kogan, Marina},
  title		= {Resonance+: Operationalizing Protective Action Decision
		  Model for Finding Information Useful for Public Information
		  Officers},
  year		= {2025},
  issue_date	= {December 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {3–4},
  url		= {https://doi.org/10.1145/3708492},
  doi		= {10.1145/3708492},
  abstract	= {Microblogging platforms have been increasingly used by the
		  public in crisis situations, enabling more participatory
		  crisis communication between the official response channels
		  and the affected community. However, the sheer volume of
		  crisis-related messages on social media can make it
		  challenging for officials to find pertinent information and
		  understand the public’s perception of evolving risks. To
		  address this issue, crisis informatics researchers have
		  proposed a variety of technological solutions, but there
		  has been limited examination of the cognitive and
		  perceptual processes and subsequent responses of the
		  affected population. Yet, this information is critical for
		  the crisis response officials to gauge public’s
		  understanding of the event, their perception of
		  event-related risk, and perception of incident response and
		  recovery efforts, in turn enabling the officials to craft
		  crisis communication messaging more effectively. Taking
		  cues from the Protective Action Decision Model, we
		  conceptualize a metric (resonance+) that prioritizes the
		  cognitive and perceptual processes of the affected
		  population, quantifying shifts in collective attention and
		  information exposure for each tweet. Based on resonance+,
		  we develop a principled, scalable pipeline that recommends
		  content relating to people’s cognitive and perceptual
		  processes. Our results suggest that resonance+ is
		  generalizable across different types of natural hazards. We
		  have also demonstrated its applicability for near-real-time
		  scenarios. According to the feedback from the target users,
		  the local public information officers in emergency
		  management, the messages recommended by our pipeline are
		  useful in their tasks of understanding public perception
		  and finding hopeful narratives, potentially leading to more
		  effective crisis communications.},
  journal	= {Trans. Soc. Comput.},
  month		= feb,
  articleno	= {6},
  numpages	= {37},
  keywords	= {Crisis informatics, social media data, crisis
		  communication, word embedding, protective action decision
		  model}
}

@InProceedings{	  10.1145/3600100.3623729,
  author	= {Hwang, Min Young and Akinci, Burcu and Berges, Mario},
  title		= {FSBrick: An information model for representing
		  fault-symptom relationships in HVAC systems},
  year		= {2023},
  isbn		= {9798400702303},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600100.3623729},
  doi		= {10.1145/3600100.3623729},
  abstract	= {Current fault diagnosis (FD) methods for Heating,
		  Ventilation, and Air Conditioning (HVAC) systems do not
		  accommodate for system reconfigurations throughout the
		  systems’ operational lifetime. However, system
		  reconfiguration can change the causal relationship between
		  faults and symptoms, which leads to a drop in FD accuracy.
		  In this paper, we present Fault-Symptom Brick (FSBrick), an
		  extension to the Brick metadata schema intended to
		  represent information necessary to propagate system
		  configuration changes onto FD algorithms, and ultimately
		  revise FSRs explicitly or implicitly. We motivate the need
		  to represent FSRs by illustrating their changes when the
		  system reconfigures. Then, we survey existing efforts to
		  represent FSRs within the HVAC sector and adjacent fields,
		  and choose to extend Brick. We introduce the FSBrick
		  architecture and discuss which extensions are added to
		  represent FSRs. To evaluate the coverage of FSBrick, we
		  implement FSBrick on (i) the motivational case study
		  scenario, (ii) Building Automation Systems’
		  representation of FSRs from 3 HVACs, and (iii) FSRs from 7
		  FD method papers, and find that FSBrick can represent
		  88.2\% of faults, 87.7\% of fault severities, and 72.5\% of
		  symptoms. The analyses show that both Brick and FSBrick
		  should be expanded further to cover HVAC component and
		  property information, and mathematical and logical
		  statements used to formulate FSRs in real life and
		  literature. As there is currently no generic and extensible
		  information model to represent FSRs in commercial
		  buildings, FSBrick paves the way to future extensions that
		  would aid the automated revision of FD algorithms upon
		  system reconfiguration.},
  booktitle	= {Proceedings of the 10th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {69–78},
  numpages	= {10},
  keywords	= {causal relationship, fault diagnosis, fault-symptom
		  relationships, ontology, semantic information modeling},
  location	= {Istanbul, Turkey},
  series	= {BuildSys '23}
}

@Article{	  10.1145/3589230,
  author	= {Trentin, Mia and Felicetti, Achille},
  title		= {Between Written and Visual Communication: CIDOC CRM
		  Ontology for Medieval and Early Modern European Graffiti},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3589230},
  doi		= {10.1145/3589230},
  abstract	= {The development of graffiti studies during the last couple
		  of decades highlighted the relevance and potential of
		  graffiti as a complementary source for understanding
		  different aspects of past societies. Moreover, the
		  availability of digital documentation techniques crucially
		  increased data production, showing the widespread presence
		  of graffiti in Medieval and Early Modern contexts across
		  Europe.However, the approach to historical graffiti has not
		  been yet structured. Guidelines, specific analytical tools,
		  and descriptors are still missing due to various reasons.
		  First, graffiti are a multiform and multimodal graphic
		  expression, so texts, signs, and images must be considered
		  together despite their different communicative nature.
		  Secondly, due to their variety in forms and contents,
		  graffiti have been studied from many perspectives (e.g.,
		  epigraphy, palaeography, history, art history, maritime
		  studies), following the specific interests of each scholar.
		  Consequently, the numerous and extensive contributions
		  concerning graffiti highlight the lack of shared standards
		  and approaches, hindering data analysis and
		  interoperability. The panorama emerging is fragmentary and
		  unstructured.This article thus aims to offer a first step
		  toward the development of a specific methodology for the
		  analysis and study of Medieval and Early Modern European
		  graffiti. Precisely, a specific ontology adopting CIDOC CRM
		  for Medieval and Early Modern graffiti will be presented,
		  as developed in a preliminary form within the DIGIGRAF
		  project1 with the support of the ARIADNEplus2 network.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {55},
  numpages	= {18},
  keywords	= {CIDOC CRM, epigraphy, Medieval Graffiti}
}

@Article{	  10.1145/3656468,
  author	= {Gray, Colin M. and Chivukula, Shruthi Sai and Johns, Janna
		  and Will, Matthew and Obi, Ike and Li, Ziqing},
  title		= {Languaging Ethics in Technology Practice},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {2},
  url		= {https://doi.org/10.1145/3656468},
  doi		= {10.1145/3656468},
  abstract	= {Ethics as embodied by technology practitioners resists
		  simple definition—particularly as it relates to the
		  interplay of identity, organizational, and professional
		  complexity. In this article, we use the linguistic notion
		  of languaging as an analytic lens to describe how
		  technology and design practitioners negotiate their
		  conception of ethics as they reflect upon their everyday
		  work. We engaged 12 practitioners in individual co-creation
		  workshops, encouraging them to reflect on their ethical
		  role in their everyday work through a series of generative
		  and evaluative activities. We analyzed these data to
		  identify how each practitioner reasoned about ethics
		  through language and artifacts, finding that practitioners
		  used a range of rhetorical tropes to describe their ethical
		  commitments and beliefs in ways that were complex and
		  sometimes contradictory. Across three cases, we describe
		  how ethics was negotiated through language across three key
		  zones of ecological emergence: the practitioners’
		  “core” beliefs about ethics, internal and external
		  ecological elements that shaped or mediated these core
		  beliefs, and the ultimate boundaries they reported refusing
		  to cross. Building on these findings, we describe how the
		  languaging of ethics reveals opportunities to
		  definitionally and practically engage with ethics in
		  technology ethics research, practice, and education.},
  journal	= {ACM J. Responsib. Comput.},
  month		= jun,
  articleno	= {15},
  numpages	= {15},
  keywords	= {Ethics, technology practice, languaging, rhetoric,
		  co-creation}
}

@InProceedings{	  10.1145/3698300.3698319,
  author	= {Zhou, Bo and Chen, Ji and Wu, Shuo},
  title		= {A Knowledge Graph Construction System Based on Digital
		  Elevation Model},
  year		= {2024},
  isbn		= {9798400717512},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698300.3698319},
  doi		= {10.1145/3698300.3698319},
  abstract	= {In recent years, Integration of knowledge graph technology
		  into various domains has become a prominent trend,
		  fostering the emergence of domain-specific knowledge graph
		  tailored to particular fields. Digital Elevation Model
		  (DEM), as fundamental data for terrain analysis, has
		  widespread applications in surveying, hydrology,
		  geomorphology, and Geographic Information Systems (GIS).
		  However, research on constructing knowledge graph based on
		  DEM remains relatively limited. This study presents a
		  system for constructing knowledge graph based on DEM. In
		  this system, valley and ridge lines are delineated and
		  organized into slope units with distinct boundaries and
		  semantic clarity, refined from DEM. These slope units serve
		  as entities, and their spatial relationships form the
		  connections within the knowledge graph. This system
		  features a user-friendly interface, enabling users to
		  intuitively view terrain data and the knowledge graph.
		  Additionally, it supports natural language Question and
		  Answer (Q&amp;A) functionality, allowing users to query
		  terrain information, explore relationships between slope
		  units, and calculate the shortest paths. By providing new
		  perspectives and tools, this system advances the research
		  and application of terrain data. Continuous improvement and
		  expansion are expected to enhance terrain analysis
		  technology and create innovative opportunities in related
		  disciplines.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Big Data Technologies},
  pages		= {45–50},
  numpages	= {6},
  keywords	= {Interactive Q&amp;A, Slope units, ridge lines, valley
		  lines},
  location	= { },
  series	= {ICBDT '24}
}

@Article{	  10.1109/taslp.2020.2980152,
  author	= {Zhu, Su and Zhao, Zijian and Ma, Rao and Yu, Kai},
  title		= {Prior Knowledge Driven Label Embedding for Slot Filling in
		  Natural Language Understanding},
  year		= {2020},
  issue_date	= {2020},
  publisher	= {IEEE Press},
  volume	= {28},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2020.2980152},
  doi		= {10.1109/TASLP.2020.2980152},
  abstract	= {Traditional slot filling in natural language understanding
		  (NLU) predicts a one-hot vector for each word. This form of
		  label representation lacks semantic correlation modeling,
		  which leads to severe data sparsity problem, especially
		  when adapting an NLU model to a new domain. To address this
		  issue, a novel label embedding based slot filling framework
		  is proposed in this article. Here, distributed label
		  embedding is constructed for each slot using prior
		  knowledge. Three encoding methods are investigated to
		  incorporate different kinds of prior knowledge about slots:
		  atomic concepts, slot descriptions, and slot exemplars. The
		  proposed label embeddings tend to share text patterns and
		  reuses data with different slot labels. This makes it
		  useful for adaptive NLU with limited data. Also, since
		  label embedding is independent of NLU model, it is
		  compatible with almost all deep learning based slot filling
		  models. The proposed approaches are evaluated on three
		  datasets. Experiments on single domain and domain
		  adaptation tasks show that label embedding achieves
		  significant performance improvement over traditional
		  one-hot label representation as well as advanced zero-shot
		  approaches.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= may,
  pages		= {1440–1451},
  numpages	= {12}
}

@Proceedings{	  10.1145/3698322,
  title		= {EuroPLoP '24: Proceedings of the 29th European Conference
		  on Pattern Languages of Programs, People, and Practices},
  year		= {2024},
  isbn		= {9798400716836},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3337722.3341860,
  author	= {Mobramaein, Afshin and Whitehead, Jim},
  title		= {A methodology for designing natural language interfaces
		  for procedural content generation},
  year		= {2019},
  isbn		= {9781450372176},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3337722.3341860},
  doi		= {10.1145/3337722.3341860},
  abstract	= {Procedural Content Generation (PCG) uses algorithmic
		  techniques to create a wide variety of content for games.
		  These generators often have a large number of parameters,
		  making it difficult for non-technical designers to explore
		  the design space of generated artifacts. Natural language
		  interfaces for generators can map natural language keywords
		  to parameter space changes spanning multiple simultaneous
		  parameters and afford use of expressive language. This way,
		  designers can navigate to interesting points in the design
		  space of a generator by describing desired properties of
		  the artifact using a series of natural language
		  descriptors. We present a design methodology that designers
		  can use to develop natural language interfaces for
		  procedural content generation systems. This design
		  methodology begins by defining a design vocabulary that can
		  describe the output of a generator, mapping the vocabulary
		  to a series of parameters, and translating natural language
		  queries to movements in the generator's design space. We
		  further address issues around designer intent
		  understanding, design space exploration and workflows using
		  natural language interfaces in PCG. An example and
		  implementation of our methodology is provided demonstrating
		  its application to existing plug-ins for content creation
		  in the Unity3D engine},
  booktitle	= {Proceedings of the 14th International Conference on the
		  Foundations of Digital Games},
  articleno	= {102},
  numpages	= {9},
  keywords	= {procedural content generation, natural language
		  interfaces, design methodology},
  location	= {San Luis Obispo, California, USA},
  series	= {FDG '19}
}

@InProceedings{	  10.1145/3236024.3275427,
  author	= {Hosseini, Mitra Bokaei},
  title		= {Semantic inference from natural language privacy policies
		  and Android code},
  year		= {2018},
  isbn		= {9781450355735},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3236024.3275427},
  doi		= {10.1145/3236024.3275427},
  abstract	= {Mobile apps collect dierent categories of personal
		  information to provide users with various services.
		  Companies use privacy policies containing critical
		  requirements to inform users about their data practices.
		  With the growing access to personal information and the
		  scale of mobile app deployment, traceability of links
		  between privacy policy requirements and app code is
		  increasingly important. Automated traceability can be
		  achieved using natural language processing and code
		  analysis techniques. However, such techniques must address
		  two main challenges: ambiguity in privacy policy
		  terminology and unbounded information types provided by
		  users through input elds in GUI. In this work, we propose
		  approaches to interpret abstract terms in privacy policies,
		  identify information types in Android layout code, and
		  create a mapping between them using natural language
		  processing techniques.},
  booktitle	= {Proceedings of the 2018 26th ACM Joint Meeting on European
		  Software Engineering Conference and Symposium on the
		  Foundations of Software Engineering},
  pages		= {940–943},
  numpages	= {4},
  keywords	= {natural Language Processing, Traceability, Requirements
		  Engineering, Privacy},
  location	= {Lake Buena Vista, FL, USA},
  series	= {ESEC/FSE 2018}
}

@InProceedings{	  10.1145/3706598.3714210,
  author	= {Xie, Jingyi and Yu, Rui and Zhang, He and Billah, Syed
		  Masum and Lee, Sooyeon and Carroll, John M.},
  title		= {Beyond Visual Perception: Insights from Smartphone
		  Interaction of Visually Impaired Users with Large
		  Multimodal Models},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3714210},
  doi		= {10.1145/3706598.3714210},
  abstract	= {Large multimodal models (LMMs) have enabled new AI-powered
		  applications that help people with visual impairments (PVI)
		  receive natural language descriptions of their surroundings
		  through audible text. We investigated how this emerging
		  paradigm of visual assistance transforms how PVI perform
		  and manage their daily tasks. Moving beyond usability
		  assessments, we examined both the capabilities and
		  limitations of LMM-based tools in personal and social
		  contexts, while exploring design implications for their
		  future development. Through interviews with 14 visually
		  impaired users of Be My AI (an LMM-based application) and
		  analysis of its image descriptions from both study
		  participants and social media platforms, we identified two
		  key limitations. First, these systems’ context awareness
		  suffers from hallucinations and misinterpretations of
		  social contexts, styles, and human identities. Second,
		  their intent-oriented capabilities often fail to grasp and
		  act on users’ intentions. Based on these findings, we
		  propose design strategies for improving both human-AI and
		  AI-AI interactions, contributing to the development of more
		  effective, interactive, and personalized assistive
		  technologies.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {62},
  numpages	= {17},
  keywords	= {People with visual impairments (PVI); large multimodal
		  models (LMMs), Human-AI interaction, visual question
		  answering (VQA); remote sighted assistance (RSA), Be My
		  Eyes, Be My AI.},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3191697.3214331,
  author	= {Basman, Antranig},
  title		= {Critique of ‘Semprola: a semiotic programming
		  language’},
  year		= {2018},
  isbn		= {9781450355131},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3191697.3214331},
  doi		= {10.1145/3191697.3214331},
  abstract	= {We supply a critique of the paper Semprola: A Semiotic
		  Programming Language, suggesting directions in which its
		  work of bringing semiotics to programming can be refined,
		  and supplying opinions on areas where it may be
		  refounded.},
  booktitle	= {Companion Proceedings of the 2nd International Conference
		  on the Art, Science, and Engineering of Programming},
  pages		= {214–217},
  numpages	= {4},
  keywords	= {Subjective Programming, Semiotics, Ontologies, Information
		  Architecture, Cognitive Ergonomics},
  location	= {Nice, France},
  series	= {Programming '18}
}

@InProceedings{	  10.1145/3652620.3688557,
  author	= {Manellanga, Rajitha and David, Istvan},
  title		= {Participatory and Collaborative Modeling of Sustainable
		  Systems: A Systematic Review},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688557},
  doi		= {10.1145/3652620.3688557},
  abstract	= {Sustainability has become a key characteristic of modern
		  systems. Unfortunately, the convoluted nature of
		  sustainability limits its understanding and hinders the
		  design of sustainable systems. Thus, cooperation among a
		  diverse set of stakeholders is paramount to sound
		  sustainability-related decisions. Collaborative modeling
		  has demonstrated benefits in facilitating cooperation
		  between technical experts in engineering problems; but
		  fails to include non-technical stakeholders in the modeling
		  endeavor. In contrast, participatory modeling excels in
		  facilitating high-level modeling among a diverse set of
		  stakeholders, often of non-technical profiles; but fails to
		  generate actionable engineering models. To instigate a
		  convergence between the two disciplines, we systematically
		  survey the field of collaborative and participatory
		  modeling for sustainable systems. By analyzing 24 primary
		  studies (published until June 2024), we identify common
		  challenges, cooperation models, modeling formalisms and
		  tools; and recommend future avenues of research.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {645–654},
  numpages	= {10},
  keywords	= {collaboration, MDE, model-driven, model-based,
		  participatory modeling, survey, sustainability, systematic
		  literture review},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3600100.3623720,
  author	= {He, Fang and Wang, Dan and Sun, Yaojie},
  title		= {Ontology Integration for Building Systems and Energy
		  Storage Systems},
  year		= {2023},
  isbn		= {9798400702303},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600100.3623720},
  doi		= {10.1145/3600100.3623720},
  abstract	= {A building ontology defines the concepts and organization
		  of building data. Such knowledge can be assistance with
		  automatic data access and support data-driven applications
		  in buildings. With technological advances in batteries and
		  energy storage, an increasing number of data-driven
		  building applications now involve both building systems and
		  energy storage systems (ESS), e.g., peak load shaving
		  (PLS). However, existing building ontologies, e.g., Brick,
		  are not designed to include concepts from ESS systems.
		  Given the emergence of building-ESS applications, it has
		  become important to develop ontologies that can cover
		  knowledge about both building and ESS systems. Building
		  systems and ESS systems fall under different industry
		  sectors and there are building ontologies and ESS
		  ontologies that have been developed independently. To
		  maximally reuse existing knowledge, we leverage ontology
		  integration technologies. We present a building-energy
		  storage ontology integration (BESOI) system that can extend
		  a building ontology with appropriate ESS ontologies. Our
		  system handles ambiguity, incoherence, and redundancy
		  problems in ontology integration. We evaluate BESOI on four
		  building-ESS applications by extending Brick, a notable
		  building ontology, with different ESS ontologies. The
		  results show that BESOI can extend the coverage of Brick
		  from 68.09\% to 95.74\% on the concepts of applications.},
  booktitle	= {Proceedings of the 10th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {212–215},
  numpages	= {4},
  keywords	= {Building Application, Energy Storage System, Metadata,
		  Ontology Integration},
  location	= {Istanbul, Turkey},
  series	= {BuildSys '23}
}

@InProceedings{	  10.1145/3266237.3266270,
  author	= {Peixoto, Mariana Maia and Silva, Carla},
  title		= {Specifying privacy requirements with goal-oriented
		  modeling languages},
  year		= {2018},
  isbn		= {9781450365031},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3266237.3266270},
  doi		= {10.1145/3266237.3266270},
  abstract	= {Context: Privacy of personal data is a growing concern
		  regarding users of software systems. In this sense, the
		  literature reports that in order to avoid privacy breaches,
		  there must be systematic approaches to specify privacy
		  requirements from the early activities of software
		  development. Objective: Motivated by this situation, this
		  paper presents a framework of privacy modeling capabilities
		  that must be addressed by requirements modeling languages
		  to better support privacy specification. The capabilities
		  will be used to compare three goal-oriented modeling
		  languages (i*, NFR-Framework and Secure-Tropos). Method:
		  The framework was created with basis on a conceptual
		  foundation and a conceptual model of privacy built from an
		  analysis of a standard, a regulation, guidelines and other
		  bibliographical sources related to privacy. A health care
		  example is used to illustrate how the framework can be used
		  to compare the chosen modeling languages. Results: Fourteen
		  privacy modeling capabilities were defined in the framework
		  and it was observed that the analyzed modeling languages do
		  not fully support them. Conclusions: The proposed framework
		  contributes towards the consolidation of a privacy
		  conceptual foundation that can be used to evaluate modeling
		  languages for privacy in Requirements Engineering. The
		  comparison performed by using this framework indicates
		  Secure-Tropos as the most complete language to model
		  privacy among the analyzed goal-oriented modeling
		  languages.},
  booktitle	= {Proceedings of the XXXII Brazilian Symposium on Software
		  Engineering},
  pages		= {112–121},
  numpages	= {10},
  keywords	= {requirements modeling, requirements engineering, privacy,
		  goal-oriented languages},
  location	= {Sao Carlos, Brazil},
  series	= {SBES '18}
}

@InProceedings{	  10.1145/3583780.3614771,
  author	= {Zhao, Yizheng},
  title		= {Highly-Optimized Forgetting for Creating Signature-Based
		  Views of Ontologies},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614771},
  doi		= {10.1145/3583780.3614771},
  abstract	= {Uniform interpolation (UI) is a non-standard reasoning
		  service that seeks to project an ontology down to its
		  sub-signature --- given an ontology taking a certain
		  signature, and a subset Σ of "relevant names'' of that
		  signature, compute a new ontology, called a uniform
		  interpolant, that uses only the relevant names while
		  preserving the semantics of the relevant names in the
		  uniform interpolant. UI is of great potential importance
		  since it may be used in a variety of applications where
		  suitable views of ontologies need to be computed. However,
		  this potential can only be fully realized if a highly
		  optimized method for computing such views exists. Previous
		  research has shown that computing uniform interpolants of
		  ELH-ontologies is a computationally extremely hard problem
		  --- a finite uniform interpolant does not always exist for
		  ELH, and if it exists, then there exists one of at most
		  triple exponential size in terms of the original ontology,
		  and that, in the worst case, no shorter interpolant exists.
		  Despite the inherent difficulty of the problem, in this
		  paper, we present a highly optimized forgetting method for
		  computing uniform interpolants of ELH-ontologies, and show
		  however that, with good reduction and inference strategies,
		  such uniform interpolants can be efficiently computed. The
		  method is an improvement of the one presented in our
		  previous work. What sets it apart is its flexibility to
		  treat concept names of different types differently,
		  effectively cutting down on the inferences involved. This
		  treatment is primarily driven by the polarities of the
		  concept names within an ontology. A comprehensive
		  evaluation with a prototypical implementation of the method
		  shows &gt;95\% average success rates over two popular
		  benchmark datasets and demonstrates a clear computational
		  advantage over state-of-the-art systems.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3444–3452},
  numpages	= {9},
  keywords	= {uniform interpolation, ontologies, forgetting, description
		  logics},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3405962.3405977,
  author	= {G\'{o}mez-Suta, Manuela and Echeverry-Correa, Juli\'{a}n
		  D. and Soto-Mej\'{\i}a, Jos\'{e} A.},
  title		= {Semi-automatic extraction and validation of concepts in
		  ontology learning from texts in Spanish},
  year		= {2020},
  isbn		= {9781450375429},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3405962.3405977},
  doi		= {10.1145/3405962.3405977},
  abstract	= {The construction of ontologies from texts in Spanish is a
		  challenge since this language lacks conceptual databases to
		  validate abstract ontology structures as concepts and
		  relations between them. The preceding generates the
		  necessity of using manual evaluation by human experts;
		  carrying high expenses that limit the calibration of
		  algorithm parameters and large-scale evaluations. This
		  document presents a proposal to evaluate abstract ontology
		  structures through the task of semantic clustering of
		  documents, without the expensive necessity of using manual
		  evaluation or conceptual databases. The proposal is not
		  only affordable but also applicable to model data and
		  domains that lack structured knowledge resources. The
		  experiments lead to the extraction and validation of the
		  ontology structures from texts in Spanish regarding the
		  domain of the Colombian armed conflict.},
  booktitle	= {Proceedings of the 10th International Conference on Web
		  Intelligence, Mining and Semantics},
  pages		= {7–16},
  numpages	= {10},
  keywords	= {evaluation, concepts, Spanish, Ontology learning},
  location	= {Biarritz, France},
  series	= {WIMS 2020}
}

@Article{	  10.1145/3759454,
  author	= {Berardinelli, Luca and Muttillo, Vittoriano and Eramo,
		  Romina and Bruneliere, Hugo and Rahimi, Abbas and
		  Cicchetti, Antonio and Giner-Miguelez, Joan and G\'{o}mez,
		  Abel and Potena, Pasqualina and Saadatmand, Mehrdad},
  title		= {Model Driven Engineering, Artificial Intelligence, and
		  DevOps for Software and Systems Engineering: A Systematic
		  Mapping Study of Synergies and Challenges},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3759454},
  doi		= {10.1145/3759454},
  abstract	= {This paper presents a systematic mapping study classifying
		  existing scientific contributions on synergies of Model
		  Driven Engineering (MDE), Artificial Intelligence/Machine
		  Learning (AI/ML), and DevOps, with the overall objective of
		  supporting the continuous development of Cyber-Physical
		  Systems (CPSs). We collected papers from bibliographic
		  sources and selected primary studies to analyse. Then, we
		  characterised and classified the current state of the art,
		  focusing on 1) main aspects already tackled at the
		  intersection of at least two of the three studied areas,
		  and 2) findings emerging from the analysis as a framework
		  for potential future research, notably regarding the
		  integration of the three studied areas. The results reveal
		  that few approaches combine MDE, AI/ML, and DevOps for
		  software and systems engineering. In contrast, several
		  approaches have combined two of them, specifically MDE and
		  DevOps. Approaches combining AI/ML with MDE or DevOps are
		  also becoming more frequent and will most likely continue
		  to progress in the future. These synergies cover a range of
		  engineering activities, from requirements and design to
		  monitoring, maintenance, and evolution. Open research
		  challenges include advancing AI/ML, MDE, and DevOps
		  integration, supporting scalable, data-oriented solutions,
		  proposing new continuous engineering methods, and adapting
		  DevOps practices to diverse systems.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= aug,
  keywords	= {Model-Driven Engineering, DevOps, Continuous Integration,
		  Artificial Intelligence, Machine Learning, Cyber-Physical
		  Systems, Internet of Things, Cloud Computing}
}

@InProceedings{	  10.1145/3701716.3715478,
  author	= {Pernisch, Romana and Dobriy, Daniil and Polleres, Axel},
  title		= {The Massive Problem of Remote Changes in Ontology Reuse},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715478},
  doi		= {10.1145/3701716.3715478},
  abstract	= {Reusing existing datasets is a common practice in the
		  Semantic Web, and it is also highly encouraged. Previous
		  work on linking datasets has introduced and analysed
		  different ways of linking but has failed to discuss the
		  meaning and intentions behind the reuse of entities. This
		  problem is aggravated by the fact Knowledge Graphs (KGs)
		  and ontologies change over time. Currently, we lack an
		  analysis of what impact the asymmetric evolution of the
		  reused KGs has. Therefore, in this short paper, we evaluate
		  how severe the problem of impacting remote changes is in
		  practice by analysing the evolution of real-world
		  ontologies. To this end, we collect a large corpus of open
		  biomedical ontologies (759 ontologies) and provide
		  statistics on their evolution, reuse (46.65\%) and
		  impacting changes (33.38\%). We find that these KGs
		  experience enormous amounts of impacting term reuse
		  (7.59\%), and the extent of the problem has been overlooked
		  on a massive scale.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {1254–1258},
  numpages	= {5},
  keywords	= {kg evolution, kg evolution impact, kg reuse},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@Article{	  10.1613/jair.1.12789,
  author	= {Caro-Mart\'{\i}nez, Marta and Jim\'{e}nez-D\'{\i}az,
		  Guillermo and Recio-Garc\'{\i}a, Juan A.},
  title		= {Conceptual Modeling of Explainable Recommender Systems: An
		  Ontological Formalization to Guide Their Design and
		  Development},
  year		= {2021},
  issue_date	= {Sep 2021},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {71},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.12789},
  doi		= {10.1613/jair.1.12789},
  abstract	= {With the increasing importance of e-commerce and the
		  immense variety of products, users need help to decide
		  which ones are the most interesting to them. This is one of
		  the main goals of recommender systems. However, users’
		  trust may be compromised if they do not understand how or
		  why the recommendation was achieved. Here, explanations are
		  essential to improve user confidence in recommender systems
		  and to make the recommendation useful. Providing
		  explanation capabilities into recommender systems is not an
		  easy task as their success depends on several aspects such
		  as the explanation’s goal, the user’s expectation, the
		  knowledge available, or the presentation method. Therefore,
		  this work proposes a conceptual model to alleviate this
		  problem by defining the requirements of explanations for
		  recommender systems. Our goal is to provide a model that
		  guides the development of effective explanations for
		  recommender systems as they are correctly designed and
		  suited to the user’s needs. Although earlier explanation
		  taxonomies sustain this work, our model includes new
		  concepts not considered in previous works. Moreover, we
		  make a novel contribution regarding the formalization of
		  this model as an ontology that can be integrated into the
		  development of proper explanations for recommender
		  systems.},
  journal	= {J. Artif. Int. Res.},
  month		= sep,
  pages		= {557–589},
  numpages	= {33},
  keywords	= {ontologies, knowledge representation}
}

@InProceedings{	  10.1145/3360901.3364444,
  author	= {Badenes-Olmedo, Carlos and Redondo-Garc\'{\i}a, Jos\'{e}
		  Luis and Corcho, Oscar},
  title		= {Scalable Cross-lingual Document Similarity through
		  Language-specific Concept Hierarchies},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364444},
  doi		= {10.1145/3360901.3364444},
  abstract	= {With the ongoing growth in number of digital articles in a
		  wider set of languages and the expanding use of different
		  languages, we need annotation methods that enable browsing
		  multi-lingual corpora. Multilingual probabilistic topic
		  models have recently emerged as a group of semi-supervised
		  machine learning models that can be used to perform
		  thematic explorations on collections of texts in multiple
		  languages. However, these approaches require theme-aligned
		  training data to create a language-independent space. This
		  constraint limits the amount of scenarios that this
		  technique can offer solutions to train and makes it
		  difficult to scale up to situations where a huge collection
		  of multi-lingual documents are required during the training
		  phase. This paper presents an unsupervised document
		  similarity algorithm that does not require parallel or
		  comparable corpora, or any other type of translation
		  resource. The algorithm annotates topics automatically
		  created from documents in a single language with
		  cross-lingual labels and describes documents by hierarchies
		  of multi-lingual concepts from independently-trained
		  models. Experiments performed on the English, Spanish and
		  French editions of JCR-Acquis corpora reveal promising
		  results on classifying and sorting documents by similar
		  content.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {147–153},
  numpages	= {7},
  keywords	= {topic models, large-scale text analysis, cross-lingual
		  semantic similarity},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@Article{	  10.1145/3639409,
  author	= {Geeganage, Dakshi Kapugama and Xu, Yue and Li, Yuefeng},
  title		= {A Semantics-enhanced Topic Modelling Technique:
		  Semantic-LDA},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {4},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3639409},
  doi		= {10.1145/3639409},
  abstract	= {Topic modelling is a beneficial technique used to discover
		  latent topics in text collections. But to correctly
		  understand the text content and generate a meaningful topic
		  list, semantics are important. By ignoring semantics, that
		  is, not attempting to grasp the meaning of the words, most
		  of the existing topic modelling approaches can generate
		  some meaningless topic words. Even existing semantic-based
		  approaches usually interpret the meanings of words without
		  considering the context and related words. In this article,
		  we introduce a semantic-based topic model called
		  semantic-LDA that captures the semantics of words in a text
		  collection using concepts from an external ontology. A new
		  method is introduced to identify and quantify the
		  concept–word relationships based on matching words from
		  the input text collection with concepts from an ontology
		  without using pre-calculated values from the ontology that
		  quantify the relationships between the words and concepts.
		  These pre-calculated values may not reflect the actual
		  relationships between words and concepts for the input
		  collection, because they are derived from datasets used to
		  build the ontology rather than from the input collection
		  itself. Instead, quantifying the relationship based on the
		  word distribution in the input collection is more realistic
		  and beneficial in the semantic capture process.
		  Furthermore, an ambiguity handling mechanism is introduced
		  to interpret the unmatched words, that is, words for which
		  there are no matching concepts in the ontology. Thus, this
		  article makes a significant contribution by introducing a
		  semantic-based topic model that calculates the
		  word–concept relationships directly from the input text
		  collection. The proposed semantic-based topic model and an
		  enhanced version with the disambiguation mechanism were
		  evaluated against a set of state-of-the-art systems, and
		  our approaches outperformed the baseline systems in both
		  topic quality and information filtering evaluations.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= feb,
  articleno	= {93},
  numpages	= {27},
  keywords	= {Topic modelling, semantics, concepts, disambiguation}
}

@InProceedings{	  10.5555/3712729.3712749,
  author	= {Tian, David and Squires, Hazel Y. and Buckley, Charlotte
		  and Gillespie, Duncan and Tattan-Birch, Harry and Shahab,
		  Lion and West, Robert and Brennan, Alan and Brown, Jamie
		  and Purshouse, Robin C.},
  title		= {Incorporating the Com-B Model for Behavior Change into an
		  Agent-Based Model of Smoking Behaviors: An Object-Oriented
		  Design},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {Modeling trajectories in cigarette smoking prevalence,
		  initiation and quitting for populations and subgroups of
		  populations is important for policy planning and
		  evaluation. This paper proposes an agent-based model (ABM)
		  design for simulating the smoking behaviors of a population
		  using the Capability, Opportunity, Motivation - Behavior
		  (COM-B) model. Capability, Opportunity and Motivation are
		  modeled as latent composite attributes which are composed
		  of observable factors associated with smoking behaviors.
		  Three forms of the COM-B model are proposed to explain the
		  transitions between smoking behaviors: initiating regular
		  smoking uptake, making a quit attempt and quitting
		  successfully. The ABM design follows object-oriented
		  principles and extends an existing generic software
		  architecture for mechanism-based modeling. The potential of
		  the model to assess the impact of smoking policies is
		  illustrated and discussed.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {252–263},
  numpages	= {12},
  location	= {Orlando, Florida, USA},
  series	= {WSC '24}
}

@InProceedings{	  10.1145/3448748.3448796,
  author	= {Liang, Changwei and Kong, Jiangping and Wu, Xiyu},
  title		= {A Speech-Driven 3-D Tongue Model with Realistic Movement
		  in Mandarin Chinese},
  year		= {2021},
  isbn		= {9781450390002},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3448748.3448796},
  doi		= {10.1145/3448748.3448796},
  abstract	= {In this paper, a new speech driven 3-D geometric tongue
		  model is constructed. The constructed 3-D tongue shape is
		  controlled with control points on 2-D midsagittal tongue
		  curve, and speech-driven inverse estimation based on the
		  constructed model is evaluated by empirical data. X-Ray 2-D
		  vocal tract motion videos are tagged for the midsagittal
		  tongue motion, and static 3-D vocal tracts of 20 phonemes
		  are collected with MRI for the realistic 3-D tongue shape.
		  MFCC are calculated from the videos as acoustic features,
		  and are then used in a LSTM-RNN to predict the control
		  points movement of the tongue shape. Three geometrically
		  intuitive control points are selected to represent and
		  calculate the midsagittal line of the tongue through linear
		  regression. Cross-sections on the central lines of the
		  tongues, whose height, width and angle are then predicted
		  from the midsagittal line, are reconstructed with geometric
		  curves, and the shape of each cross-section are then placed
		  on the midsagittal line to get the overall predicted moving
		  grid of the 3-D tongue. In this 3-D tongue model, acoustic
		  features and realistic tongue motion are mapped directly to
		  preserve more realistic articulatory details, and the
		  control points are intuitive for non-experts to control the
		  model, and the geometric tongue shapes predicted are
		  comparable with realistic tongue dynamics. Based on the
		  proposed method, the speech-driven prediction is evaluated
		  with the realistic data, which proved this proposed method
		  feasible.},
  booktitle	= {Proceedings of the 2021 International Conference on
		  Bioinformatics and Intelligent Computing},
  pages		= {297–302},
  numpages	= {6},
  keywords	= {3-D tongue model, Mandarin Chinese, realistic dynamics,
		  speech-driven},
  location	= {Harbin, China},
  series	= {BIC '21}
}

@Article{	  10.1145/3554733,
  author	= {Sharma, Neha and Soni, Mukesh and Kumar, Sumit and Kumar,
		  Rajeev and Deb, Nabamita and Shrivastava, Anurag},
  title		= {Supervised Machine Learning Method for Ontology-based
		  Financial Decisions in the Stock Market},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3554733},
  doi		= {10.1145/3554733},
  abstract	= {For changing semantics, ontological and information
		  presentation, as well as computational linguistics for
		  Asian social networks, are one of the most essential
		  platforms for offering enhanced and real-time data mapping,
		  as well as huge data access across diverse big data sources
		  on the web architecture, information extraction mining,
		  statistical modeling and data modeling, database control,
		  and so on. The concept of opinion or sentiment analysis is
		  often used to predict or classify the textual data,
		  sentiment, affect, subjectivity, and other emotional states
		  in online text. Recognizing the message's positive and
		  negative thoughts or opinions by examining the author's
		  goals will aid in a better understanding of the text's
		  content in terms of the stock market. An intelligent
		  ontology and knowledge Asian social network solution can
		  improve the effectiveness of a company's decision making
		  support procedures by deriving important information about
		  users from a wide variety of web sources. However, ontology
		  is concerned primarily with problem-solving knowledge
		  discovery. The utilization of Internet-based modernizations
		  welcomed a significant effect on the Indian stock exchange.
		  News related to the stock market in the most recent decade
		  plays a vital role for the brokers or users. This article
		  focuses on predicting stock market news sentiments based on
		  their polarity and textual information using the concept of
		  ontological knowledge-based Convolution Neural Network
		  (CNN) as a machine learning approach. Optimal features are
		  essential for the sentiment classification model to predict
		  the stock's textual reviews' exact sentiment. Therefore,
		  the swarm-based Artificial Bee Colony (ABC) algorithm is
		  utilized with the Lexicon feature extraction approach using
		  a novel fitness function. The main motivation for combining
		  ABC and CNN is to accelerate model training, which is why
		  the suggested approach is effective in predicting emotions
		  from stock news.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {139},
  numpages	= {24},
  keywords	= {Convolution Neural Networks (CNN), Artificial Bee Colony
		  Algorithm (ABC), Lexicon feature extraction, sentiment
		  analysis, opinion mining, Stock market}
}

@InProceedings{	  10.1145/3600100.3623744,
  author	= {Mavrokapnidis, Dimitris and Fierro, Gabe and Husmann,
		  Maria and Korolija, Ivan and Rovas, Dimitrios},
  title		= {SeeQ: A Programming Model for Portable Data-Driven
		  Building Applications},
  year		= {2023},
  isbn		= {9798400702303},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600100.3623744},
  doi		= {10.1145/3600100.3623744},
  abstract	= {This paper introduces SeeQ, a programming model and an
		  abstraction framework that facilitates the development of
		  portable data-driven building applications. Data-driven
		  approaches can provide insights into building operations
		  and guide decision-making to achieve operational
		  objectives. Yet the configuration of such applications per
		  building requires extensive effort and tacit knowledge. In
		  SeeQ, we propose a portable programming model and build a
		  software system that enables self-configuration and
		  execution across diverse buildings. The configuration of
		  each building is captured in a unified data model — in
		  this paper, we work with the Brick ontology without loss of
		  generality. SeeQ focuses on the distinction between the
		  application logic and the configuration of an application
		  against building-specific data inputs and systems. We test
		  the proposed approach by configuring and deploying a
		  diverse range of applications across five heterogeneous
		  real-world buildings. The analysis shows the potential of
		  SeeQ to significantly reduce the efforts associated with
		  the delivery of building analytics.},
  booktitle	= {Proceedings of the 10th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {159–168},
  numpages	= {10},
  keywords	= {Analytics, Brick, Metadata, Ontologies, Portability,
		  Programming, RDF, SHACL, Scalability, Semantic Web},
  location	= {Istanbul, Turkey},
  series	= {BuildSys '23}
}

@InProceedings{	  10.1145/3638067.3638088,
  author	= {Cardoso, Daiane de Ascen\c{c}\~{a}o and Henriques, Felipe
		  da Rocha and Belloze, Kele Teixeira},
  title		= {Ontology Visualization in BioPortal: Methodological
		  Triangulation for Analyzing Accessibility, Communicability,
		  and Usability},
  year		= {2024},
  isbn		= {9798400717154},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638067.3638088},
  doi		= {10.1145/3638067.3638088},
  abstract	= {Visualizing biomedical ontologies is an ongoing and highly
		  relevant challenge in dealing with the vast volume of daily
		  data and the miscellaneous use cases of ontologies.
		  BioPortal is the most well-known tool for visualizing
		  biomedical ontologies, bringing together hundreds of
		  ontologies of varying sizes, depths, and complexities. This
		  article aims to establish a methodological triangulation
		  using SIM-SR based on Semiotic Engineering, the WCAG
		  guidelines from the W3C consortium, and ergonomic
		  principles to evaluate the experience of users who utilize
		  screen readers and users who do not use this feature
		  without conducting user experiments. The objective is to
		  investigate and intersect the usability, communicability,
		  and accessibility of BioPortal. Additionally, it seeks to
		  understand how ontologies can be analyzed by as many people
		  as possible, ensuring they are comprehensible and enhancing
		  users’ cognitive understanding of the explored domain.},
  booktitle	= {Proceedings of the XXII Brazilian Symposium on Human
		  Factors in Computing Systems},
  articleno	= {60},
  numpages	= {11},
  keywords	= {Communicability, Digital Acessibility, Evaluation Methods,
		  Ontology, Semiotic Engineer, Usability},
  location	= {Macei\'{o}, Brazil},
  series	= {IHC '23}
}

@InProceedings{	  10.1145/3368640.3368641,
  author	= {Danenas, Paulius and Skersys, Tomas and Butleris,
		  Rimantas},
  title		= {Enhancing the extraction of SBVR business vocabularies and
		  business rules from UML use case diagrams with natural
		  language processing},
  year		= {2019},
  isbn		= {9781450372923},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368640.3368641},
  doi		= {10.1145/3368640.3368641},
  abstract	= {Being among the best-selling and most advanced features of
		  model-driven development, model-to-model transformation
		  could help improving one of the most time- and
		  resource-consuming efforts in the process of model-driven
		  information systems engineering, namely, discovery and
		  specification of business vocabularies and business rules
		  within the problem domain. Nonetheless, despite the
		  relatively high levels of automation throughout the whole
		  systems' model-driven development process, business
		  modeling stage remains among the most under re-searched
		  areas throughout the whole process. In this paper, we
		  introduce a novel natural language processing (NLP)
		  technique to one of our latest developments for the
		  automatic extraction of SBVR business vocabularies and
		  business rules from UML use case diagrams. This development
		  remains arguably the most comprehensive development of this
		  kind currently available in public. The experiment provided
		  proof that the developed NLP enhancement delivered even
		  better extraction results compared to the already
		  satisfactory performance of the previous development. This
		  work contributes to the research in the areas of model
		  transformations and NLP within the model-driven development
		  of information systems, and beyond.},
  booktitle	= {Proceedings of the 23rd Pan-Hellenic Conference on
		  Informatics},
  pages		= {1–8},
  numpages	= {8},
  keywords	= {use case diagram, natural language processing, model
		  transformation, business vocabulary, business rules, UML,
		  SBVR},
  location	= {Nicosia, Cyprus},
  series	= {PCI '19}
}

@InProceedings{	  10.1145/3325730.3325757,
  author	= {Memon, Kamran Ali and Xiaoling, Xia},
  title		= {Deciphering and Analyzing Software Requirements employing
		  the techniques of Natural Language Processing},
  year		= {2019},
  isbn		= {9781450362580},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3325730.3325757},
  doi		= {10.1145/3325730.3325757},
  abstract	= {Deciphering human language by Requirement Analysts is the
		  key issue in Software Development. Clients communicate
		  their software requirements in raw form. In this paper, we
		  are presenting certain techniques of Natural Language
		  processing which work out greatly to extract information
		  properly and minimizing the bugs that may generate in later
		  parts of Software Development. In today's era, the latest
		  technological development in Artificial Intelligence has
		  enabled machines to process the text to a certain level.
		  Natural Language understanding is so far the most critical
		  problem; the Software community is facing today in
		  requirements gathering. In this study, using the techniques
		  of Natural Language Interpretation, Testers and Software
		  Developers can chalk out the more exact requirements from
		  customers which can improve the Quality of Software to a
		  certain level.},
  booktitle	= {Proceedings of the 2019 4th International Conference on
		  Mathematics and Artificial Intelligence},
  pages		= {153–156},
  numpages	= {4},
  keywords	= {Software Requirements, Natural Language Understanding,
		  Natural Language Processing, NLP Techniques, Linguistics},
  location	= {Chegndu, China},
  series	= {ICMAI '19}
}

@InProceedings{	  10.1145/3706599.3719925,
  author	= {Binksmith, Adam and Mansoor, Hamid and Nacenta, Miguel A
		  and Toniolo, Alice},
  title		= {Designing Progressive Model Elicitation Tools to Support
		  Complex Cognitive Activities},
  year		= {2025},
  isbn		= {9798400713958},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706599.3719925},
  doi		= {10.1145/3706599.3719925},
  abstract	= {Externalizations such as sketches and diagrams are
		  effective in helping individuals plan for complex projects
		  and designs. Offloading complex information on a different
		  medium (e.g. paper) frees up cognitive resources for
		  sophisticated thinking. However, there are significant
		  challenges in effectively using externalizations and
		  notations (formalized externalizations). It is difficult to
		  envision formal representations of complex ideas at the
		  beginning and it is also hard to keep track of evolutions
		  within notations. In addition, existing notations may be
		  insufficient or inflexible for our specific purposes. We
		  call instances of such thinking in which humans have to
		  work through complex information and action space as
		  instances of "Progressive Model Elicitation" (PME). We
		  explore ways to support PME processes by presenting a set
		  of design principles and Schematica, a prototype that
		  implements those design principles to support PME. We
		  present illustrative examples of Schematica use to support
		  the process of PME.},
  booktitle	= {Proceedings of the Extended Abstracts of the CHI
		  Conference on Human Factors in Computing Systems},
  articleno	= {179},
  numpages	= {8},
  keywords	= {Externalization, Notation, Diagramming, Mental Model},
  location	= { },
  series	= {CHI EA '25}
}

@InProceedings{	  10.1145/3711542.3711571,
  author	= {undefinedlgen, Bahar and Hattab, Georges},
  title		= {A Lexical Simplification Framework for Turkish},
  year		= {2025},
  isbn		= {9798400717383},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711542.3711571},
  doi		= {10.1145/3711542.3711571},
  abstract	= {Lexical simplification is a fundamental step towards
		  improving the accessibility, comprehension, and readability
		  of texts, particularly in languages with limited linguistic
		  resources. In this study, we adopt a state-of-the-art
		  lexical simplification approach and propose a lexical
		  simplification framework tailored to Turkish. Our framework
		  leverages a combination of complex word identification
		  tasks and substitution generation through pre-trained
		  language models to identify complex lexical units using
		  selective substitution ranking approaches and algorithms
		  and replace them with simpler alternatives, thereby
		  improving text readability. This work makes three key
		  contributions: (i) a comprehensive study of lexical
		  simplification for Turkish, including the complex word
		  identification subtask; (ii) a rigorous comparison of
		  various language models for candidate generation using the
		  masked language modeling objective; and (iii) an in-depth
		  exploration of the impact of different complexity
		  thresholds and additional parameters on overall
		  performance. Our framework demonstrates a strong capability
		  to balance simplification and contextual preservation,
		  offering an effective solution to lexical simplification in
		  Turkish.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {218–224},
  numpages	= {7},
  keywords	= {BERT, Text simplification, artificial intelligence for
		  social good, complex word identification, lexical
		  simplification, low-resource languages, pre-trained
		  language model},
  location	= { },
  series	= {NLPIR '24}
}

@InProceedings{	  10.1145/3624486.3624489,
  author	= {Bednar, Peter and Sarnovsky, Martin and Vanko, Jakub
		  Ivan},
  title		= {Cognitive Architecture for Process industries},
  year		= {2023},
  isbn		= {9798400708350},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3624486.3624489},
  doi		= {10.1145/3624486.3624489},
  abstract	= {This paper introduces a Cross-Sectorial Big Data
		  Processing platform which provides tools for the semantic
		  modelling of the data analytical processes and for the
		  automatic generation of data analysis scripts for solving
		  the described problems. The main contribution of this paper
		  is the cognitive component for the automatic extraction of
		  the task definition from the narrative description of the
		  problem based on the Large Language Models (LLMs). We have
		  evaluated the proposed method on five problems from the
		  different domains and found that the automatic extraction
		  of the task definition can have promising results that can
		  be applied to full-automatic data analytics.},
  booktitle	= {Proceedings of the 3rd Eclipse Security, AI, Architecture
		  and Modelling Conference on Cloud to Edge Continuum},
  pages		= {15–20},
  numpages	= {6},
  keywords	= {Data analytics, Large language models, Ontologies},
  location	= {Ludwigsburg, Germany},
  series	= {eSAAM '23}
}

@InProceedings{	  10.1145/3613904.3642887,
  author	= {Chen, Liuqing and Jiang, Zhaojun and Xia, Duowei and Cai,
		  Zebin and Sun, Lingyun and Childs, Peter and Zuo, Haoyu},
  title		= {BIDTrainer: An LLMs-driven Education Tool for Enhancing
		  the Understanding and Reasoning in Bio-inspired Design},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642887},
  doi		= {10.1145/3613904.3642887},
  abstract	= {Bio-inspired design (BID) fosters innovations in
		  engineering. Learning BID is crucial for developing
		  multidisciplinary innovation skills of designers and
		  engineers. Current BID education aims to enhance
		  learners’ understanding and analogical reasoning skills.
		  However, it often heavily relies on the teachers’
		  expertise. When learners pursue independent learning using
		  some educational tools, they face challenges in
		  understanding and reasoning practice within this
		  multidisciplinary field. Additionally, evaluating their
		  learning outcomes comprehensively becomes problematic.
		  Addressing these challenges, we introduce a LLMs-driven BID
		  education method based on a structured ontology and three
		  strategies: enhancing understanding through LLMs-enpowered
		  "learning by asking", assisting reasoning by providing
		  hints and feedback, and assessing learning outcomes through
		  benchmarking against existing BID cases. Implementing the
		  method, we developed BIDTrainer, a BID education tool. User
		  studies indicate that learners using BIDTrainer understood
		  BID knowledge better, reason faster with higher
		  interactivity than the baseline, and BIDTrainer assessed
		  the learning outcomes consistent with experts.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {676},
  numpages	= {20},
  keywords	= {Analogy training, Bio-inspired design, Design education,
		  Design evaluation},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3579987.3586572,
  author	= {Gupta, Tushar and Sural, Shamik},
  title		= {Ontology-based Evaluation of ABAC Policies for
		  Inter-Organizational Resource Sharing},
  year		= {2023},
  isbn		= {9798400700996},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579987.3586572},
  doi		= {10.1145/3579987.3586572},
  abstract	= {Attribute-based Access Control (ABAC), as the name
		  suggests, determines whether an access request be granted
		  based on the attributes or characteristics of the
		  requesting user, those of the requested resource, and the
		  environmental condition in which the request is generated.
		  An important advantage of such an identity-agnostic model
		  is that access control can be imposed even on users from
		  other organizations if they are able to prove their
		  attributes to the reference monitor of the organization
		  whose resources are being accessed. It would, however,
		  require a mechanism for mapping the attributes and their
		  values among these organizations. We propose an ontology
		  based method for addressing this requirement. Besides
		  meeting the needs of collaborative accesses, we show how
		  such an approach can be made to naturally support
		  hierarchical ABAC policies as well as controlled relaxation
		  during policy enforcement.},
  booktitle	= {Proceedings of the 9th ACM International Workshop on
		  Security and Privacy Analytics},
  pages		= {85–94},
  numpages	= {10},
  keywords	= {resource sharing, policy relaxation, ontology, digital
		  signature, attribute-based access control, at- tribute
		  hierarchy},
  location	= {Charlotte, NC, USA},
  series	= {IWSPA '23}
}

@InProceedings{	  10.1145/3745238.3745485,
  author	= {Ji, Tingting and Xu, Jingyun and Chen, Weisong and Lin,
		  Shengkai and Wu, Yican and Li, Wei},
  title		= {Multivariate data extraction method of enterprise digital
		  internal audit based on knowledge map and LDA model},
  year		= {2025},
  isbn		= {9798400712791},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3745238.3745485},
  doi		= {10.1145/3745238.3745485},
  abstract	= {The explosive growth of enterprise data, with diverse data
		  types and complex structure, provides abundant information
		  resources for enterprise's operational decision-making,
		  risk management and compliance review. However, how to
		  extract valuable information from these massive data
		  efficiently and accurately has become a key problem to be
		  solved urgently in the digital internal audit of
		  enterprises. Therefore, this paper proposes a multivariate
		  data extraction method for enterprise digital internal
		  audit based on knowledge map and LDA model. Define ontology
		  and entity, construct multivariate data security knowledge
		  map of enterprise digital internal audit, obtain the
		  thematic correlation between documents and words, and
		  construct LDA model; The features are aggregated to get the
		  entity data vector, and the multivariate data extraction of
		  enterprise digital internal audit is realized. The
		  experimental results show that the information coverage of
		  this method is high, and it shows significant advantages in
		  the cost of multivariate data extraction in enterprise
		  digital internal audit.},
  booktitle	= {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater
		  Bay Area International Conference on Digital Economy and
		  Artificial Intelligence},
  pages		= {1577–1581},
  numpages	= {5},
  keywords	= {Enterprise digitalization, Internal audit multivariate
		  data, Data extraction, Knowledge map, LDA model},
  location	= { },
  series	= {DEAI '25}
}

@InProceedings{	  10.1145/3629527.3652897,
  author	= {Suman, Shekhar and Chu, Xiaoyu and Niewenhuis, Dante and
		  Talluri, Sacheendra and De Matteis, Tiziano and Iosup,
		  Alexandru},
  title		= {Enabling Operational Data Analytics for Datacenters
		  through Ontologies, Monitoring, and Simulation-based
		  Prediction},
  year		= {2024},
  isbn		= {9798400704451},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3629527.3652897},
  doi		= {10.1145/3629527.3652897},
  abstract	= {Datacenters are key components in the ICT infrastructure
		  supporting our digital society. Datacenter operations are
		  hampered by operational complexity and dynamics, risking to
		  reduce or even offset the performance, energy efficiency,
		  and other datacenter benefits. A promising emerging
		  technology, Operational Data Analytics~(ODA), promises to
		  collect and use monitoring data to improve datacenter
		  operations. However, it is challenging to organize, share,
		  and leverage the massive and heterogeneous data resulting
		  from monitoring datacenters. Addressing this combined
		  challenge, starting from the idea that graphs could provide
		  a good abstraction, in this work we present our early work
		  on designing and implementing a graph-based approach for
		  datacenter ODA. We focus on two main components of
		  datacenter ODA. First, we design, implement, and validate
		  agraph-based ontology for datacenters that captures both
		  high-level meta-data information and low-level metrics of
		  operational data collected from real-world datacenters, and
		  maps them to a graph structure for better organization and
		  further use. Second, we design and implementODAbler, a
		  software framework for datacenter ODA, which combines ODA
		  data with an online simulator to make predictions about
		  current operational decisions and other what-if scenarios.
		  We take the first steps to illustrate the practical use of
		  ODAbler, and explore its potential to support datacenter
		  ODA through graph-based analysis. Our work helps construct
		  the case that graph-based ontologies have great value for
		  datacenter ODA and, further, to improving datacenter
		  operations.},
  booktitle	= {Companion of the 15th ACM/SPEC International Conference on
		  Performance Engineering},
  pages		= {120–126},
  numpages	= {7},
  keywords	= {\%oda monitoring, analysis, datacenter, graph-based
		  ontology, mapping, odabler, opendc, operational data
		  analytics, simulation},
  location	= {London, United Kingdom},
  series	= {ICPE '24 Companion}
}

@InProceedings{	  10.1145/3366424.3382699,
  author	= {Wang, Chunpei and Zhang, Xiaowang},
  title		= {Q-BERT: A BERT-based Framework for Computing SPARQL
		  Similarity in Natural Language},
  year		= {2020},
  isbn		= {9781450370240},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366424.3382699},
  doi		= {10.1145/3366424.3382699},
  abstract	= {In this paper, we present a pre-trained transformer
		  network Q-BERT, in which siamese network architecture is
		  employed to produce semantically meaningful embeddings of
		  SPARQL queries to be compared via cosine-similarity. A core
		  idea of Q-BERT is to put SPARQL query into the category of
		  natural language to ensure that each entity mention or
		  relation phrase in different knowledge bases has the same
		  vector representation. Moreover, based on Q-BERT, we
		  present a practical approach for the SPARQL
		  query-empty-answer problem by accessing the RDF repository.
		  The experiments on real datasets show the effectiveness and
		  efficiency of Q-BERT.},
  booktitle	= {Companion Proceedings of the Web Conference 2020},
  pages		= {65–66},
  numpages	= {2},
  keywords	= {Siamese, Semantic Similarity, SPARQL, BERT},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@InProceedings{	  10.1145/3652620.3688558,
  author	= {Marchezan, Luciano and Homolka, Marcel and Blokhin, Andrei
		  and Assun\c{c}\~{a}o, Wesley K. G. and Herac, Edvin and
		  Egyed, Alexander},
  title		= {A Tool for Collaborative Consistency Checking During
		  Modeling},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688558},
  doi		= {10.1145/3652620.3688558},
  abstract	= {Consistency checking is widely used to detect
		  inconsistencies in engineering artifacts. In collaborative
		  modeling environments, however, maintaining model
		  consistency is challenging due to the frequent changes
		  introduced by multiple engineers, leading to possibly
		  numerous inconsistencies. These inconsistencies need to be
		  identified and shared among engineers to collaboratively
		  resolve them and prevent cascading problems. Despite
		  extensive research in consistency checking, there remains a
		  lack of tools that support collaborative consistency
		  checking (C3). C3 is defined as the process of checking and
		  fixing inconsistencies during collaborative modeling,
		  whether engineers work synchronously or asynchronously. To
		  address this gap, we introduce a prototype tool integrated
		  into the DesignSpace environment which allows collaborative
		  work during modeling and consistency checking. Our tool is
		  implemented to support a streamlined version of UML (sUML)
		  with various diagrams such as class, use case, sequence,
		  and state-machine. We showcase the tool's features with an
		  example of a robotic arm, highlighting its effectiveness in
		  collaborative consistency checking. We also compare our
		  tool with related work, showing that collaborative features
		  are currently lacking in the proposed tools. A video
		  showcasing the tool is available at
		  https://www.youtube.com/watch?v=9kweKeBHx5Y},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {655–659},
  numpages	= {5},
  keywords	= {collaborative modeling, consistency checking, tool demo},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3459637.3482307,
  author	= {Liu, Zhao and Lu, Chang and Alghamdi, Ghadah and Schmidt,
		  Renate A. and Zhao, Yizheng},
  title		= {Tracking Semantic Evolutionary Changes in Large-Scale
		  Ontological Knowledge Bases},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482307},
  doi		= {10.1145/3459637.3482307},
  abstract	= {This paper is concerned with the problem of computing the
		  semantic difference between different versions of
		  large-scale ontological knowledge bases using a uniform
		  interpolation (UI) approach. The semantic difference
		  between two versions of an ontology are the axioms entailed
		  by one version but not the other version, reflecting the
		  evolutionary changes of the content of the ontology. In
		  general, computing such axioms is not computationally
		  feasible, since there are infinitely many of them. UI is an
		  advanced reasoning technique that seeks to create
		  restricted views of ontologies; it provides an effective
		  means for computing a finite representation of the
		  difference between two ontologies. While existing UI
		  methods are designed for languages that are either more
		  expressive or less expressive than the description logic
		  ELH, the underlying language of typical large-scale
		  ontologies, in this paper, we introduce a practical UI
		  method tailored for the task of computing the semantic
		  difference in large-scale ELH-ontologies. The method is
		  terminating, sound, and can always compute UI results
		  possibly including fresh definer symbols. Two case studies
		  on different versions of the SNOMED CT terminology show
		  that the method has overcome major limitations of existing
		  UI methods and can be used to reveal modeling changes that
		  have occurred over successive releases of SNOMED CT.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {1130–1139},
  numpages	= {10},
  keywords	= {uniform interpolation, semantic difference, ontologies,
		  knowledge representation and reasoning, forgetting,
		  description logics},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3640310.3674084,
  author	= {Bach, Jean-Christophe and Beugnard, Antoine and Champeau,
		  Jo\"{e}l and Dagnat, Fabien and Gu\'{e}rin, Sylvain and
		  Mart\'{\i}nez, Salvador},
  title		= {10 years of Model Federation with Openflexo: Challenges
		  and Lessons Learned},
  year		= {2024},
  isbn		= {9798400705045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640310.3674084},
  doi		= {10.1145/3640310.3674084},
  abstract	= {In the context of complex system development,
		  heterogeneous modeling responds to the need to integrate
		  several domains. This need requires the use of the most
		  appropriate formalism and tooling for each domain to be
		  efficient. Model federation promotes the semantic
		  interoperability of heterogeneous models by providing the
		  means to reify correspondences between different model
		  elements, add custom behaviors and bridge the gap between
		  technological spaces. As such, it can be used as an
		  infrastructure to address many different system engineering
		  problems. This is what we have been doing for over a
		  decade, as part of a close collaboration between a small
		  software engineering startup and academia. This paper
		  reports on this experience.Concretely, we discuss the
		  context, ambitions, and challenges that led to the
		  inception of our practice of model federation, and we
		  present five use cases experiences, stemming from real
		  industrial and academic needs, and elaborate on lessons
		  learned. In addition, we also report on challenges and
		  lessons learned regarding the development and maintenance
		  of a model-driven model federation tool, the Openflexo
		  framework. Finally, we set up a road map for the future of
		  model federation and Openflexo.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {25–36},
  numpages	= {12},
  keywords	= {Experience report, Model federation, Model management},
  location	= {Linz, Austria},
  series	= {MODELS '24}
}

@InProceedings{	  10.1145/3555776.3577616,
  author	= {Bozzato, Loris and Serafini, Luciano},
  title		= {Ontology-Mediated Data Migration: Deriving Migration Rules
		  by Reasoning on Schema Descriptions},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3577616},
  doi		= {10.1145/3555776.3577616},
  abstract	= {Migration of data across information systems is a
		  knowledge intensive task: the definition of mappings
		  between systems requires knowledge of the source and target
		  (relational) schemas and their interpretation of the shared
		  domain. Moreover, direct schema mappings need often to be
		  re-defined for each new migration instance, in order to
		  accommodate the variations caused by the change of systems
		  and representation conventions. A possible solution to such
		  problems is the use of an intermediate ontological model,
		  that can be used as a lingua franca for the description of
		  schemas, by defining mappings from and to the ontology.
		  While this helps in making explicit the semantics of the
		  schemas, the problem remains on how to extract a direct
		  mapping from source to target schema from this intermediate
		  representation.In this paper, we present our ongoing work
		  in building an ontology-based migration system in the
		  scenario of banking information systems. In the
		  architecture of the system, an ontology defines an
		  intermediate semantic description for the source and target
		  schemas. We introduce a reasoning method for the automatic
		  extraction of migration rules starting from the semantic
		  descriptions of the schemas. The procedure for computation
		  of migration rules is then implemented via reasoning over
		  an Answer Set Programming encoding.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1724–1731},
  numpages	= {8},
  keywords	= {ontology mediated migration, relational schema mapping,
		  data migration},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@InProceedings{	  10.1145/3465481.3470024,
  author	= {Merah, Yazid and Kenaza, Tayeb},
  title		= {Ontology-based Cyber Risk Monitoring Using Cyber Threat
		  Intelligence},
  year		= {2021},
  isbn		= {9781450390514},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3465481.3470024},
  doi		= {10.1145/3465481.3470024},
  abstract	= {Efficient cyber risk assessment needs to consider all
		  security alerts provided by cybersecurity solutions
		  deployed in a network. To build a reliable overview of
		  cyber risk, there is a need to adopt continuous monitoring
		  of emerged cyber threats related to that risk. Indeed, the
		  integration of Cyber Threat Intelligence (CTI) into
		  cybersecurity solutions provides valuable information about
		  threats, targets, and potential vulnerabilities. Structured
		  Threat Information eXpression (STIX), as a language for
		  expressing information about cyber threats in a structured
		  and unambiguous manner, is becoming a de facto standard for
		  sharing information about cyber threats. In addition,
		  ontology-based semantic knowledge modeling has become a
		  promising solution that provides a machine-readable
		  language for downstream work in cybersecurity
		  problem-solving. In this paper, we propose an ontology
		  using CTI for risk monitoring. This latter improves an
		  existing ontology, originally proposed to be used within a
		  SIEM (Security Information Event Management), by extending
		  it and aligning it with the STIX concepts.},
  booktitle	= {Proceedings of the 16th International Conference on
		  Availability, Reliability and Security},
  articleno	= {88},
  numpages	= {8},
  keywords	= {Cyber Threat Intelligence, OWL, Ontology, Risk
		  Assessment},
  location	= {Vienna, Austria},
  series	= {ARES '21}
}

@InProceedings{	  10.1145/3652620.3688223,
  author	= {Hallak, Yara and Blouin, Dominique and Pautet, Laurent and
		  Saab, Layale and Laborie, Baptiste and Mittal, Rakshit},
  title		= {Model Management at Renault Virtual Simulation Team: State
		  of Practice, Challenges and Research Directions},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688223},
  doi		= {10.1145/3652620.3688223},
  abstract	= {In the automotive industry, new systems are being
		  developed to enhance vehicle safety and driver convenience.
		  These systems are increasingly complex to build and
		  maintain. To develop these systems Renault makes intensive
		  use of simulation and must deal with thousands of models.
		  This huge number of models must be well managed. To manage
		  these models, Renault has developed the SysML-based Model
		  Identity Card (MIC), used with a Model-Based Simulation
		  (MBSi) approach. However, despite this first solution,
		  managing simulation models remains a difficult task.In this
		  paper, we describe the current simulation model management
		  approaches used at Renault, and their shortcomings and
		  challenges in the modelling and simulation of complex
		  automotive systems. We use Advanced Driver Assistance
		  System (ADAS) and its Automatic Emergency Breaking (AEB)
		  sub-system as examples to illustrate the utilization of the
		  MIC and demonstrate current practices. From these examples,
		  we derive main challenges faced by the virtual simulation
		  team and propose research directions to solve them, based
		  on state of the art methodologies for simulation models'
		  validation and verification management.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {1005–1014},
  numpages	= {10},
  keywords	= {model management, model-based systems engineering, model
		  identity card, model-based simulation, advanced driver
		  assistance system},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3652620.3688256,
  author	= {Abbasi, Faima and Brimont, Pierre and Pruski, Cedric and
		  Sottet, Jean-S\'{e}bastien},
  title		= {Understanding Semantic Drift in Model Driven Digital
		  Twins},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688256},
  doi		= {10.1145/3652620.3688256},
  abstract	= {Digital twins have revolutionized the industry in recent
		  years by providing virtual representations of physical
		  assets, systems, or processes, and relying on real-time
		  data for effective functioning. These twins enable
		  real-time monitoring, analysis, and simulation of
		  real-world entities through the extensive use of various
		  digital models, including design models, scientific models,
		  and data models that capture the status and behaviour of
		  the corresponding physical entities. However, as the real
		  world evolves, these models must adapt to maintain
		  consistency with their physical counterparts. This
		  adaptation process can lead to semantic drift, a
		  misalignment between the digital representation and the
		  physical reality over time. In this paper, we propose a
		  classification and formalization of different types of
		  semantic drift and review how this concept is understood in
		  the literature on model-driven digital twins. We further
		  illustrate the scenarios associated with each type of
		  semantic drift using an urban mobility use case, explicitly
		  highlighting the practical implications.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {419–430},
  numpages	= {12},
  keywords	= {digital twins, semantic drift, scientific models, data
		  models, design models},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3360901.3364439,
  author	= {Jain, Prateek and Verma, Kunal and Gaikwad, Aniket and
		  Gadde, Pramod},
  title		= {Understanding Financial Transaction Documents using
		  Natural Language Processing},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364439},
  doi		= {10.1145/3360901.3364439},
  abstract	= {In this paper, we share our experiences creating NLP based
		  AI platform for finance - Appzen (http://www.appzen.com).
		  AppZen's auditing technology is being utilized by over 500
		  enterprise customers including multiple Fortune 500
		  companies for auditing employee expenses. AppZen's
		  technology can process, analyze and identify relationships
		  between various kinds of transaction documents such as -
		  receipts, invoices, contracts and purchase orders. Each
		  type of transaction document requires custom processing and
		  analysis due to the diversity in language and structure of
		  the document. Contracts typically require deep
		  understanding of the content such as identifying sentence
		  structures, identifying entities and relationships between
		  them compared to receipts and invoices, which are somewhat
		  semi-structured and require a different kind of processing.
		  We elaborate on the challenges we have experienced and use
		  of NLP in conjunction with a lightweight semantic layer to
		  alleviate these challenges.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {255–258},
  numpages	= {4},
  keywords	= {nlp, information extraction, financial auditing, feature
		  engineering},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.1145/3706598.3713940,
  author	= {Zhu, Jichen and Sanches, Pedro and Tsaknaki, Vasiliki and
		  van der Maden, Willem and Kaklopoulou, Irene},
  title		= {The Centers and Margins of Modeling Humans in Well-being
		  Technologies},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713940},
  doi		= {10.1145/3706598.3713940},
  abstract	= {This paper critically examines the machine learning (ML)
		  modeling of humans in three case studies of well-being
		  technologies. Through a critical technical approach, it
		  examines how these apps were experienced in daily life
		  (technology in use) to surface breakdowns and to identify
		  the assumptions about the “human” body entrenched in
		  the ML models (technology design). To address these issues,
		  this paper applies agential realism to decenter
		  foundational assumptions, such as body regularity and
		  health/illness binaries, and speculates more inclusive
		  design and ML modeling paths that acknowledge irregularity,
		  human-system entanglements, and uncertain transitions. This
		  work is among the first to explore the implications of
		  decentering theories in computational modeling of human
		  bodies and well-being, offering insights for more inclusive
		  technologies and speculations toward posthuman-centered ML
		  modeling.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {518},
  numpages	= {16},
  keywords	= {Decentering, Machine Learning Modeling, Well-being,
		  Diffraction, Agential Realism},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3706598.3713985,
  author	= {Paudel, Shreyasha and Loos, Sabine and Soden, Robert},
  title		= {Hype versus Historical Continuity: Situating the Rise of
		  AI in Climate and Disaster Risk Modeling},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713985},
  doi		= {10.1145/3706598.3713985},
  abstract	= {As governments increasingly adopt Artificial Intelligence
		  (AI) across different application sectors, advocates argue
		  that it will create new disruptions by democratizing
		  access, improving accuracy, and lowering costs. In
		  practice, uncritical adoption of AI tools has been shown to
		  cause significant harms. Our study uses a historical lens
		  to examine the uptake of AI in climate risk management
		  through a study of climate and disaster risk modeling.
		  These techniques originated in the insurance industry, but
		  are now incorporated into many climate and disaster
		  governance processes. Using the concept of ‘insurance
		  logics’, we demonstrate that many of the original aspects
		  of disaster risk modeling remain despite the transfer of
		  risk assessment tools from the insurance industry to the
		  public sector and new techniques made possible by AI. This
		  highlights technological continuity, rather than
		  disruption, as a key driver of contemporary risk modeling
		  practice. Doing so helps to unsettle problematic, though
		  challenging to identify, aspects of supposedly disruptive
		  technologies and create possibilities for alternatives.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {258},
  numpages	= {17},
  keywords	= {History, Technological Evolution, Climate Risk, Disaster
		  Risk Models, AI Hype, Responsible AI},
  location	= { },
  series	= {CHI '25}
}

@Article{	  10.1145/3626196,
  author	= {kumari, Rani and Sah, Dinesh Kumar and Cengiz, Korhan and
		  Ivkovi\'{c}, Nikola and Balaji, Prasanalakshmi},
  title		= {Automatic graph construction and Exploring different types
		  of LSTMs for Asian Hindi languages for Medical review
		  Sentiment Analysis},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3626196},
  doi		= {10.1145/3626196},
  abstract	= {Sentiment Analysis (SA) of medical reviews is crucial for
		  improving healthcare outcomes. However, analyzing sentiment
		  in low-resource languages such as Asian Hindi presents
		  significant challenges. In this study, we propose an
		  automatic graph construction approach to extract relevant
		  features from medical reviews in Asian Hindi languages. We
		  explore different types of Long Short-Term Memory (LSTMs),
		  including traditional LSTMs, bidirectional LSTMs, and
		  attention-based LSTMs, to classify the sentiment of medical
		  reviews. Our proposed approach uses attention-based LSTM
		  architecture and pre-trained Word2Vec embeddings to achieve
		  high accuracy. We compare the proposed approach with
		  existing models using various evaluation metrics, including
		  accuracy, precision, recall, and F1-score. The results
		  demonstrate that our proposed approach outperforms all
		  existing models in terms of accuracy, achieving an accuracy
		  score of 81\%. These findings could have implications for
		  improving healthcare outcomes by enabling better monitoring
		  of patient feedback and identifying areas for improvement
		  in medical services.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  keywords	= {Graph construction, Long short-term memory, Deep learning,
		  Hindi language}
}

@InProceedings{	  10.1145/3652620.3688215,
  author	= {Fu, Yuhong and Selway, Matt and Grossmann, Georg and Kaur,
		  Karamjit and Stumptner, Markus},
  title		= {Modelling a Warehouse with SLICER: A Contribution to the
		  MULTI Warehouse Challenge},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688215},
  doi		= {10.1145/3652620.3688215},
  abstract	= {In this paper we apply the SLICER multi-level modelling
		  framework on the MULTI 2024 Warehouse Challenge. SLICER was
		  first introduced at the ER 2015 conference[17] and formally
		  specified in [18]. It has been implemented in an
		  object-oriented knowledge representation and reasoning
		  system and the framework identifies additional semantic
		  subcategories for the traditional object-oriented
		  relationship types and uses these distinctions for a new
		  style of formal characterisation of level relationships.
		  The framework is executable via its implementation in a
		  dynamic metamodeling and object logic environment. We first
		  introduce SLICER, then describe its application on the
		  Warehouse Challenge requirements and then discuss the
		  advantages of the SLICER representation.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {828–837},
  numpages	= {10},
  keywords	= {multi-level modelling, warehouse challenge, SLICER},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3635059.3635087,
  author	= {Gasidou, Anastasia and Kotsifakos, Dimitrios and
		  Douligeris, Christos},
  title		= {Specific modeling issues for designing the transformation
		  of a smart city},
  year		= {2024},
  isbn		= {9798400716263},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3635059.3635087},
  doi		= {10.1145/3635059.3635087},
  abstract	= {In this short paper, we present methods and solutions for
		  specific issues of concern to engineers and application
		  technologists related to modeling a smart city's
		  transformation. The main body of developing "structural
		  design modeling" for a smart city utilizes the Unified
		  Modeling Language™ (UML®) conceptual approach. Our
		  approach provides an overview of the specific domains
		  (points - nodes) related to the topic. Our discussion does
		  not delve into significant issues. It does not solidify the
		  proposed approaches but as a research proposal, it remains
		  at a level that could be described as a presentation or an
		  argumentative position. Our short paper heavily relies on
		  diagrams of models. This short paper focuses on specific
		  design issues related to modeling mappings specifically for
		  the design and modeling processes for a smart city.},
  booktitle	= {Proceedings of the 27th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {181–184},
  numpages	= {4},
  keywords	= {Engineering, Informatics, Modeling Patterns, Project,
		  Science, Technology},
  location	= {Lamia, Greece},
  series	= {PCI '23}
}

@InProceedings{	  10.1145/3264746.3264786,
  author	= {Kim, Gihoon and Choi, Chang and Choi, Junho},
  title		= {Ontology modeling for APT attack detection in an IoT-based
		  power system},
  year		= {2018},
  isbn		= {9781450358859},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3264746.3264786},
  doi		= {10.1145/3264746.3264786},
  abstract	= {Smart grid technology is the core technology for the
		  next-generation power grid system with enhanced energy
		  efficiency through decision-making communication between
		  suppliers and consumers enabled by integrating the IoT into
		  the existing grid. This open architecture allowing
		  bilateral information exchange makes it vulnerable to
		  various types of cyberattack. APT attacks, one of the most
		  common cyberattacks, are highly tricky and sophisticated
		  attacks that can circumvent the existing detection
		  technology and attack the targeted system after a certain
		  latent period after intrusion. This paper proposes an
		  ontology-based attack detection system capable of early
		  detection of and response to APT attacks by analyzing their
		  attacking patterns.},
  booktitle	= {Proceedings of the 2018 Conference on Research in Adaptive
		  and Convergent Systems},
  pages		= {160–164},
  numpages	= {5},
  keywords	= {smart grid, ontology, IoT, APT attack},
  location	= {Honolulu, Hawaii},
  series	= {RACS '18}
}

@InProceedings{	  10.1145/3209219.3213598,
  author	= {Moon, DeKita G.},
  title		= {Modeling Learners' Interest with a Domain-Independent
		  Ontology-Based Framework},
  year		= {2018},
  isbn		= {9781450355896},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209219.3213598},
  doi		= {10.1145/3209219.3213598},
  abstract	= {Ontologies are recognized as a promising approach to
		  support the reusability and interoperability of learners'
		  preferences; which is useful for the optimization and
		  flexibility of data and resources. However, little to no
		  research on adaptive learning systems or semantic
		  technologies explore personalized experiences based on the
		  various out-of-school experiences and activities of the
		  users. This research investigates the design, development,
		  and evaluation of an ontology-based framework for students'
		  interests in a math word problem generator that may be
		  applied to various other learning systems and possibly
		  other domains. The cohesiveness of the problems in addition
		  to the usability, usefulness, and the short-term
		  effectiveness of the derived technology will be
		  investigated by comparing the generated questions to
		  numerical and traditional Algebra I problems. We aim to
		  better understand students' interests to identify the role
		  that their interests can play in semantic technologies,
		  further supporting the recent advances in ontology-based
		  educational technologies and personalized math word problem
		  generators.},
  booktitle	= {Proceedings of the 26th Conference on User Modeling,
		  Adaptation and Personalization},
  pages		= {345–348},
  numpages	= {4},
  keywords	= {semantic technologies, knowledge graphs, human-centered
		  computing, educational technologies, domain ontologies},
  location	= {Singapore, Singapore},
  series	= {UMAP '18}
}

@InBook{	  10.1145/3715668.3735608,
  author	= {Calderwood, Alex and Kim, Taewook and Sun, Yuqian and
		  Roemmele, Melissa and Chung, John Joon Young and Kreminski,
		  Max},
  title		= {Supporting Material Writing Practice with Phraselette, a
		  Palette of Phrases},
  year		= {2025},
  isbn		= {9798400714863},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715668.3735608},
  abstract	= {We present a demonstration of Phraselette, an artistic
		  support tool designed for compatibility with the writerly
		  values of experimental poets. Following a theory of
		  “material writing support”, we introduce affordances
		  for selecting short spans of text (on the order of a few
		  words) to vary; constraining text generation procedures
		  (some based on language models) to adhere to poetic intent;
		  and searching through large spaces of potential variations
		  for phrases that satisfy users’ constraints in unexpected
		  but evocative ways. Phraselette has been validated through
		  an extended expert evaluation with 10 published poets; we
		  found that, in contrast to the dominant prompting-based
		  approach to interacting with language models as writing
		  support tools, Phraselette is better aligned with
		  experimental poetry practice, providing deeper support for
		  navigating spaces of potential interpretations of poetic
		  text.},
  booktitle	= {Companion Publication of the 2025 ACM Designing
		  Interactive Systems Conference},
  pages		= {236–241},
  numpages	= {6}
}

@InProceedings{	  10.1145/3589335.3651256,
  author	= {Chaturvedi, Rochana},
  title		= {Temporal Knowledge Graph Extraction and Modeling across
		  Multiple Documents for Health Risk Prediction},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651256},
  doi		= {10.1145/3589335.3651256},
  abstract	= {Clinical text in electronic health records (EHR) holds
		  vital cues into a patient's journey, often absent in
		  structured EHR data. Evidence-based healthcare decisions
		  demand accurate extraction and modeling of these cues. The
		  goal of our study is to predict Type-II Diabetes by
		  utilizing concept-based models of visit sequences from
		  longitudinal EHR data. We undertake the challenging task of
		  fine-grained temporal information extraction from clinical
		  text using a recent span-based approach with pre-trained
		  transformers. We achieve a new state-of-the-art in
		  end-to-end relation extraction from 2012 clinical temporal
		  relations corpus. We propose to apply our model to a new
		  dataset and extract patient-centric temporal knowledge
		  graphs from their visits-fusing temporal orderings within
		  documents and across visits. Beyond the current focus of
		  our work on Type-II Diabetes risk prediction from EHR, our
		  versatile framework can be extended to other domains
		  including web-based healthcare systems for personalized
		  medicine. It can not only model health outcomes having long
		  progression timelines but also various socio-economic
		  outcomes such as conflict, natural disasters, and financial
		  markets by leveraging news, reports, and social-media text
		  for extracting and modeling irregular time-series and help
		  inform a variety of web-based applications and policies.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1182–1185},
  numpages	= {4},
  keywords	= {dynamic graphs, multidocument classification, natural
		  language processing, patient-centric, temporal information
		  extraction, temporal knowledge graphs, time series
		  classification},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3330431.3330453,
  author	= {Niyazova, R. and Aktayeva, Al. and Davletkireeva, L.},
  title		= {An ontology based model for user profile building using
		  social network},
  year		= {2019},
  isbn		= {9781450372121},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3330431.3330453},
  doi		= {10.1145/3330431.3330453},
  abstract	= {The structure and basic principles of technology for
		  increasing the probability of identifying subjects of
		  information processes of open Internet resources based on
		  ontology methods are considered. Based on this ontology the
		  knowledge base intended for creation of the program systems
		  supporting ensuring information security has been realized.
		  The developed ontological knowledge base has been used when
		  developing the software complex intended for identification
		  of the user of social networks when ensuring information
		  security, monitoring and preventing threats.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Engineering and MIS},
  articleno	= {21},
  numpages	= {4},
  keywords	= {social network, ontology, knowledge base, identification,
		  cybersecurity, SPARQL},
  location	= {Astana, Kazakhstan},
  series	= {ICEMIS '19}
}

@InProceedings{	  10.1145/3550355.3552449,
  author	= {Parvizimosaed, Alireza and Roveri, Marco and Rasti, Aidin
		  and Amyot, Daniel and Logrippo, Luigi and Mylopoulos,
		  John},
  title		= {Model-checking legal contracts with SymboleoPC},
  year		= {2022},
  isbn		= {9781450394666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550355.3552449},
  doi		= {10.1145/3550355.3552449},
  abstract	= {Legal contracts specify requirements for business
		  transactions. As any other requirements specification,
		  contracts may contain errors and violate properties
		  expected by contracting parties. Symboleo was recently
		  proposed as a formal specification language for legal
		  contracts. This paper presents SymboleoPC, a tool for
		  analyzing Symboleo contracts using model checking. It
		  highlights the architecture, implementation and testing of
		  the tool, as well as a scalability evaluation with respect
		  to the size of contracts and properties to be checked
		  through a series of experiments. The results suggest that
		  SymboleoPC can be usefully applied to the analysis of
		  formal specifications of contracts with real-life sizes and
		  structures.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems},
  pages		= {278–288},
  numpages	= {11},
  keywords	= {software requirements specifications, smart contracts,
		  performance analysis, nuXmv, model checking, legal
		  contracts, formal specification languages},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3701551.3703477,
  author	= {Roy, Soumyadeep and Sundaram, Sowmya S. and Wolff, Dominik
		  and Ganguly, Niloy},
  title		= {Building Trustworthy AI Models for Medicine: From Theory
		  to Applications},
  year		= {2025},
  isbn		= {9798400713293},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701551.3703477},
  doi		= {10.1145/3701551.3703477},
  abstract	= {AI is emerging as an efficient companion in medicine.
		  While AI holds promise for reducing the cognitive load of
		  researchers and practitioners, its adoption is often
		  hindered by a lack of trust in new AI advancements. We
		  present sophisticated techniques for developing trustworthy
		  artificial intelligence (AI) models in medicine, bridging
		  breakthroughs in AI research with practical healthcare
		  applications. We will discuss in-depth the four stages
		  (Design, Development, Implementation, and Evaluation)
		  involved in the process of building trustworthy AI models
		  customized for the medical domain. We present various
		  techniques for incorporating important Trustworthy AI
		  principles like data privacy, robustness, explainability,
		  interpretability, medical experts-in-the-loop, and risk
		  assessment while developing AI models for medicine. In
		  contrast to prior tutorials, we make the following two key
		  contributions: (i) While explaining the 'Implementation'
		  stage, we cover various real-world healthcare applications
		  developed as part of research projects in academia in
		  collaboration with medical schools in India and Germany.
		  (ii) By including a health informatics professional as one
		  of the tutorial organizers, we provide a fresh and
		  much-needed perspective on the research challenges and
		  mitigation strategies in building AI models for medicine.},
  booktitle	= {Proceedings of the Eighteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1012–1015},
  numpages	= {4},
  keywords	= {knowledge integration in healthcare, medical NLP,
		  trustworthy AI},
  location	= {Hannover, Germany},
  series	= {WSDM '25}
}

@InProceedings{	  10.1145/3701716.3715286,
  author	= {Kousa, Ilona M.},
  title		= {Data-Driven Democracy? Using Social Media and AI for
		  Knowledge Co-production in Energy Transition Research},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715286},
  doi		= {10.1145/3701716.3715286},
  abstract	= {As global challenges grow increasingly difficult to
		  address, the need for relevant knowledge in policymaking
		  has become more critical than ever. In fields such as
		  sustainability research, discussions about the
		  co-production of knowledge emphasise the involvement of
		  diverse stakeholders in generating relevant information for
		  democratic decision-making processes. Particularly in
		  technology-intensive areas like energy, the voices of
		  experts have traditionally dominated, often marginalising
		  the perspectives of citizens affected by energy policies
		  and leading to epistemic injustices. Social media serves as
		  a central forum for political activity and civic
		  engagement, making it an important research environment for
		  understanding the complexities of knowledge production and
		  sharing. In my dissertation, I explore the opportunities
		  and challenges of integrating citizen voices into the
		  co-production of knowledge through social media using
		  artificial intelligence (AI) based methods. My approach
		  includes analysing stakeholder perspectives across social
		  media, policymakers' speeches, and traditional media, using
		  the discourse on energy justice in the context of
		  sustainable energy transition as empirical material.
		  Additionally, I conduct a comparative analysis of a
		  rule-based ontological classification tool and a large
		  language model (LLM) chatbot as qualitative content
		  analysis tools. I demonstrate that social media data can
		  reveal critical perspectives and marginalised discourses
		  that might otherwise be overlooked. However, social media
		  platforms are also used to spread disinformation and incite
		  polarisation, which undermines their reliability as
		  accurate reflections of prevailing attitudes and opinions
		  in society. In addition, although AI methods can make
		  information processing more effective and support the use
		  of large datasets, it is important to consider their
		  inherent limitations. I address the biases and ethical
		  concerns associated with social media data and AI-based
		  analysis tools and discuss potential solutions to mitigate
		  these challenges.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {701–704},
  numpages	= {4},
  keywords	= {computational social sciences, large language models,
		  natural language processing, social media},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3732775.3733586,
  author	= {Gokdemir, Ozan and Siebenschuh, Carlo and Brace, Alexander
		  and Wells, Azton and Hsu, Brian and Hippe, Kyle and Setty,
		  Priyanka and Ajith, Aswathy and Pauloski, J. Gregory and
		  Sastry, Varuni and Foreman, Sam and Zheng, Huihuo and Ma,
		  Heng and Kale, Bharat and Chia, Nicholas and Gibbs, Thomas
		  and Papka, Michael and Brettin, Thomas and Alexander,
		  Francis and Anandkumar, Anima and Foster, Ian and Stevens,
		  Rick and Vishwanath, Venkatram and Ramanathan, Arvind},
  title		= {HiPerRAG: High-Performance Retrieval Augmented Generation
		  for Scientific Insights},
  year		= {2025},
  isbn		= {9798400718861},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3732775.3733586},
  doi		= {10.1145/3732775.3733586},
  abstract	= {The volume of scientific literature is growing
		  exponentially, leading to underutilized discoveries,
		  duplicated efforts, and limited cross-disciplinary
		  collaboration. Retrieval-Augmented Generation (RAG) offers
		  a way to assist scientists by improving the factuality of
		  Large Language Models (LLMs) in processing this influx of
		  information. However, scaling RAG to handle millions of
		  articles introduces significant challenges, including the
		  high computational costs associated with parsing documents
		  and embedding scientific knowledge, as well as the
		  algorithmic complexity of aligning these representations
		  with the nuanced semantics of scientific content. To
		  address these issues, we introduce HiPerRAG, a RAG workflow
		  powered by high performance computing (HPC) to index and
		  retrieve knowledge from more than 3.6 million scientific
		  articles. At its core are Oreo, a high-throughput model for
		  multimodal document parsing, and ColTrast, a query-aware
		  encoder fine-tuning algorithm that enhances retrieval
		  accuracy by using contrastive learning and late-interaction
		  techniques. HiPerRAG delivers robust performance on
		  existing scientific question answering (Q/A) benchmarks and
		  two new benchmarks introduced in this work, achieving 90\%
		  accuracy on SciQ and 76\% on PubMedQA—outperforming both
		  domain-specific models like PubMedGPT and commercial LLMs
		  such as GPT-4. Scaling to thousands of GPUs on the Polaris,
		  Sunspot, and Frontier supercomputers, HiPerRAG delivers
		  million document-scale RAG workflows for unifying
		  scientific knowledge and fostering interdisciplinary
		  innovation.},
  booktitle	= {Proceedings of the Platform for Advanced Scientific
		  Computing Conference},
  pages		= {1–13},
  numpages	= {13},
  keywords	= {HPC, AI, large language models, retrieval-augmented
		  generation, metric learning, neural information retrieval},
  location	= {FHNW University of Applied Sciences and Arts Northwestern
		  Switzerland, Brugg-Windisch, Switzerland},
  series	= {PASC '25}
}

@Article{	  10.5555/3241691.3241693,
  author	= {Gatt, Albert and Krahmer, Emiel},
  title		= {Survey of the state of the art in natural language
		  generation: core tasks, applications and evaluation},
  year		= {2018},
  issue_date	= {January 2018},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {61},
  number	= {1},
  issn		= {1076-9757},
  abstract	= {This paper surveys the current state of the art in Natural
		  Language Generation (NLG), defined as the task of
		  generating text or speech from non-linguistic input. A
		  survey of NLG is timely in view of the changes that the
		  field has undergone over the past two decades, especially
		  in relation to new (usually data-driven) methods, as well
		  as new applications of NLG technology. This survey
		  therefore aims to (a) give an up-to-date synthesis of
		  research on the core tasks in NLG and the architectures
		  adopted in which such tasks are organised; (b) highlight a
		  number of recent research topics that have arisen partly as
		  a result of growing synergies between NLG and other areas
		  of artifical intelligence; (c) draw attention to the
		  challenges in NLG evaluation, relating them to similar
		  challenges faced in other areas of nlp, with an emphasis on
		  different evaluation methods and the relationships between
		  them.},
  journal	= {J. Artif. Int. Res.},
  month		= jan,
  pages		= {65–170},
  numpages	= {106}
}

@InProceedings{	  10.1145/3555776.3577719,
  author	= {Baader, Franz},
  title		= {Optimal Repairs in Ontology Engineering as
		  Pseudo-Contractions in Belief Change},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3577719},
  doi		= {10.1145/3555776.3577719},
  abstract	= {The question of how a given knowledge base can be modified
		  such that certain unwanted consequences are removed has
		  been investigated in the area of knowledge engineering
		  under the name of repair and in the area of belief change
		  under the name of contraction. Whereas in the former area
		  the emphasis was more on designing and implementing
		  concrete repair algorithms, the latter area concentrated on
		  characterizing classes of contraction operations by certain
		  postulates they satisfy. In the classical setting, repairs
		  and contractions are subsets of the knowledge base that no
		  longer have the unwanted consequence. This makes these
		  approaches syntax-dependent and may result in removal of
		  more consequences than necessary. To alleviate this
		  problem, gentle repairs and pseudo-constractions have been
		  introduced in the respective research areas, and their
		  connections have been investigated in recent work. Optimal
		  repairs preserve a maximal amount of consequences, but they
		  may not always exist. We show that, if they exist, then
		  they can be obtained by certain pseudo-contraction
		  operations, and thus they comply with the postulates that
		  these operations satisfy. Conversely, under certain
		  conditions, pseudo-contractions are guaranteed to produce
		  optimal repairs.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {983–990},
  numpages	= {8},
  keywords	= {description logic, ontology repair, belief change},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@InProceedings{	  10.1145/3550355.3552421,
  author	= {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L. C. and
		  Kienzle, J\"{o}rg},
  title		= {Machine learning-based incremental learning in interactive
		  domain modelling},
  year		= {2022},
  isbn		= {9781450394666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550355.3552421},
  doi		= {10.1145/3550355.3552421},
  abstract	= {In domain modelling, practitioners manually transform
		  informal requirements written in natural language (problem
		  descriptions) to more concise and analyzable domain models
		  expressed with class diagrams. With automated domain
		  modelling support using existing approaches, manual
		  modifications may still be required in extracted domain
		  models and problem descriptions to make them more accurate
		  and concise. For example, educators teaching software
		  engineering courses at universities usually use an
		  incremental approach to build modelling exercises to
		  restrict students in using intended modelling patterns.
		  These modifications result in the evolution of domain
		  modelling exercises over time. To assist practitioners in
		  this evolution, a synergy between interactive support and
		  automated domain modelling is required. In this paper, we
		  propose a bot-assisted approach to allow practitioners
		  perform domain modelling quickly and interactively.
		  Furthermore, we provide an incremental learning strategy
		  empowered by machine learning to improve the accuracy of
		  the bot's suggestions and extracted domain models by
		  analyzing practitioners' decisions over time. We evaluate
		  the performance of our bot using test problem descriptions
		  which shows that practitioners can expect to get useful
		  support from the bot when applied to exercises of similar
		  size and complexity, with precision, recall, and F2 scores
		  over 85\%. Finally, we evaluate our incremental learning
		  strategy where we observe a reduction in the required
		  manual modifications by 70\% and an improvement of F2
		  scores of extracted domain models by 4.2\% when using our
		  proposed approach and learning strategy together.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems},
  pages		= {176–186},
  numpages	= {11},
  keywords	= {natural language processing (NLP), machine learning (ML),
		  incremental learning, evolution, domain models, decisions,
		  bot},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3571788.3571797,
  author	= {Georges, Thomas and Rice, Liam and Huchard, Marianne and
		  K\"{o}nig, M\'{e}lanie and Nebut, Cl\'{e}mentine and
		  Tibermacine, Chouki},
  title		= {Guiding Feature Models Synthesis from User-Stories: An
		  Exploratory Approach},
  year		= {2023},
  isbn		= {9798400700019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571788.3571797},
  doi		= {10.1145/3571788.3571797},
  abstract	= {User-stories are commonly used to define requirements in
		  agile project management. In Software Product Lines (SPL),
		  a user-story corresponds to a feature description (or part
		  of it), that can be shared by several products. In
		  practice, large SPL include a huge number of user-stories,
		  making variability hard to grasp and handle. In this paper
		  we present an exploratory approach that aims to guide the
		  synthesis of Feature Models that capture and structure the
		  commonalities and the variability expressed in these
		  user-stories. The built Feature Models aim to help the
		  project understanding, maintenance and evolution. Our
		  approach first decomposes the user-stories to extract the
		  roles and the features, using natural language processing
		  techniques. In a second step, we group user-stories having
		  the same topics thanks to a clustering method. This
		  contributes to extract more general features. In a third
		  step, we leverage the use of Formal Concept Analysis to
		  extract logical constraints between the features that guide
		  Feature Model synthesis. We illustrate our approach using a
		  dataset from our industrial partner.},
  booktitle	= {Proceedings of the 17th International Working Conference
		  on Variability Modelling of Software-Intensive Systems},
  pages		= {65–70},
  numpages	= {6},
  keywords	= {User Story, Software Product Line, SPL domain engineering,
		  Reengineering, Natural Language Processing, Formal Concept
		  Analysis, Feature Model, Agile Process},
  location	= {Odense, Denmark},
  series	= {VaMoS '23}
}

@InProceedings{	  10.1145/3366424.3384363,
  author	= {Degbelo, Auriol and Sherpa, Ang},
  title		= {Open Geodata Reuse: Towards Natural Language Interfaces to
		  Web APIs},
  year		= {2020},
  isbn		= {9781450370240},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366424.3384363},
  doi		= {10.1145/3366424.3384363},
  abstract	= {Open Government datasets have been flooding the Web
		  recently, and Application Programming Interfaces (APIs) are
		  key to the development of services on top of these
		  datasets. An issue of current APIs worldwide is that their
		  learnability is limited. This work has explored the
		  potential of querying APIs using natural language terms to
		  mitigate that issue. A user study with 20 participants has
		  demonstrated that a natural-language-based, along with an
		  order-agnostic approach to API design can produce easily
		  learnable APIs for both novice and experienced API users.
		  These insights can pave the way for a paradigm change on
		  Web-API design for open geodata retrieval and beyond.},
  booktitle	= {Companion Proceedings of the Web Conference 2020},
  pages		= {703–710},
  numpages	= {8},
  keywords	= {Smart Cities, Open Government Data, Geographic Information
		  Retrieval, API Usability, API Learnability, API Design
		  Conventions},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@Article{	  10.1145/3677614,
  author	= {Razin, Yosef S. and Feigh, Karen M.},
  title		= {Converging Measures and an Emergent Model: A Meta-Analysis
		  of Human-Machine Trust Questionnaires},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {4},
  url		= {https://doi.org/10.1145/3677614},
  doi		= {10.1145/3677614},
  abstract	= {Trust is crucial for technological acceptance, continued
		  usage, and teamwork. However, human-robot trust, and
		  human-machine trust more generally, suffer from
		  terminological disagreement and construct proliferation. By
		  comparing, mapping, and analyzing well-constructed trust
		  survey instruments, this work uncovers a consensus
		  structure of trust in human–machine interaction. To do
		  so, we identify the most frequently cited and
		  best-validated human-machine and human-robot trust
		  questionnaires as well as the best-established factors that
		  form the dimensions and antecedents of such trust. To
		  reduce both confusion and construct proliferation, we
		  provide a detailed mapping of terminology between
		  questionnaires. Furthermore, we perform a meta-analysis of
		  the regression models which emerged from the experiments
		  that employed multi-factorial survey instruments. Based on
		  this meta-analysis, we provide the most complete,
		  experimentally validated model of human-machine and
		  human-robot trust to date. This convergent model
		  establishes an integrated framework for future research. It
		  determines the current boundaries of trust measurement and
		  where further investigation and validation are necessary.
		  We close by discussing how to choose an appropriate trust
		  survey instrument and how to design for trust. By
		  identifying the internal workings of trust, a more complete
		  basis for measuring trust is developed that is widely
		  applicable.},
  journal	= {J. Hum.-Robot Interact.},
  month		= nov,
  articleno	= {58},
  numpages	= {41},
  keywords	= {Human-Robot Trust, Shared Mental Models}
}

@InProceedings{	  10.1145/3550356.3559576,
  author	= {B\'{u}r, M\'{a}rton and Stirewalt, Kurt},
  title		= {ORM ontologies with executable derivation rules to support
		  semantic search in large-scale data applications},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3559576},
  doi		= {10.1145/3550356.3559576},
  abstract	= {A semantic layer maps complex enterprise data into an
		  ontology with abstract business concepts that are
		  well-known to business users. Chief data officers invest
		  significant effort to create and update these ontologies,
		  while data scientists do feature engineering by combining
		  already existing concepts and features of the domain.
		  However, it is a significant challenge to catalogue and
		  maintain the numerous features pertaining to an ontology,
		  which leads to duplicated features and unnecessary
		  complexity. In this work, we propose to combine ontologies
		  captured using the Object-Role Modeling notation with
		  derivation rules defined in a datalog-like language called
		  Rel, which allows the creation of a semantic layer with
		  feature search capability. Our prototype framework uses the
		  RAI Knowledge Graph Management System, which provides
		  automated and incremental derivation rule evaluation.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {81–82},
  numpages	= {2},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.5555/3213214.3213223,
  author	= {Amissah, Matthew and Toba, Ange-Lionel and Handley, Holly
		  A. H. and Seck, Mamadou},
  title		= {Towards a framework for executable systems modeling: an
		  executable systems modeling language (ESysML)},
  year		= {2018},
  isbn		= {9781510860186},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {The Systems Modeling Language (SysML), which is the
		  de-facto modeling standard in the systems engineering
		  community, consists of a number of independently derived
		  methodologies (i.e. state charts, activity diagrams etc.)
		  which have been co-opted into a single modeling framework.
		  This and the lack of an overarching meta-model that
		  specifies relationships and rules governing the various
		  language constructs precludes their uniform application
		  across diagram types. This has resulted in a large unwieldy
		  and at best semi-formal language specification, with
		  adverse implications for interoperability of modeling tools
		  and model execution. This paper presents an executable
		  language that re-factors the SysML language schema and
		  offers an equivalent textual syntax for model specification
		  in tandem with the existing graphical syntax. This is aimed
		  at supporting the development of time based simulation
		  models useful for decision support and architecture
		  verification and validation in systems engineering.},
  booktitle	= {Proceedings of the Model-Driven Approaches for Simulation
		  Engineering Symposium},
  articleno	= {9},
  numpages	= {12},
  keywords	= {model driven engineering, model based systems engineering,
		  SysML},
  location	= {Baltimore, Maryland},
  series	= {Mod4Sim '18}
}

@InProceedings{	  10.1145/3589334.3645567,
  author	= {El Husseini, Wafaa and El Vaigh, Cheikh Brahim and
		  Goasdou\'{e}, Fran\c{c}ois and Jaudoin, H\'{e}l\`{e}ne},
  title		= {Query Optimization for Ontology-Mediated Query Answering},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645567},
  doi		= {10.1145/3589334.3645567},
  abstract	= {Ontology-mediated query answering (OMQA) consists in
		  asking database queries on knowledge bases (KBs); a KB is a
		  set of facts called the KB's database, which is described
		  by domain knowledge called the KB's ontology. A
		  widely-investigated OMQA technique is FO-rewriting: every
		  query asked on a KB is reformulated w.r.t. the KB's
		  ontology, so that its answers are computed by the
		  relational evaluation of the query reformulation on the
		  KB's database. Crucially, because FO-rewriting compiles the
		  domain knowledge relevant to queries into their
		  reformulations, query reformulations may be complex and
		  their optimization is the crux of efficiency. We devise a
		  novel optimization framework for a large set of OMQA
		  settings that enjoy FO-rewriting: conjunctive queries,
		  i.e., the core select-project-join queries, asked on KBs
		  expressed using datalog+/-, description logics, existential
		  rules, OWL, or RDFS. We optimize the query reformulations
		  produced by state-of-the-art FO-rewriting algorithms by
		  computing rapidly, with the help of a KB's database
		  summary, simpler (contained) queries with the same answers
		  that can be evaluated faster by RDBMSs. We show on a
		  well-established OMQA benchmark that time performance is
		  significantly improved by our optimization framework in
		  general, up to three orders of magnitude.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2138–2148},
  numpages	= {11},
  keywords	= {data summarization, existential rules, query
		  optimization},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3620666,
  title		= {ASPLOS '24: Proceedings of the 29th ACM International
		  Conference on Architectural Support for Programming
		  Languages and Operating Systems, Volume 3},
  year		= {2024},
  isbn		= {9798400703867},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  abstract	= {Welcome to the third volume of ASPLOS'24: the 29th ACM
		  International Conference on Architectural Support for
		  Programming Languages and Operating Systems. This document
		  is mostly dedicated to the 2024 fall cycle but also
		  provides some statistics summarizing all three cycles.We
		  introduced several notable changes to ASPLOS this year,
		  most of which were discussed in our previous messages from
		  program chairs in Volume 1 and 2, including: (1)
		  significantly increasing the program committee size to over
		  220 members (more than twice the size of last year); (2)
		  foregoing synchronous program committee (PC) meetings and
		  instead making all decisions online; (3) overhauling the
		  review assignment process; (4) developing an automated
		  submission format violation identifier script that
		  uncovers, e.g., disallowed vertical space manipulations
		  that "squeeze" space; (5) introducing the new ASPLOS role
		  of Program Vice Chairs to cope with the increased number of
		  submissions and the added load caused by foregoing
		  synchronous program committee; and (6) characterizing a
		  systematic problem that ASPLOS is facing in reviewing
		  quantum computing submissions, describing how we addressed
		  it and highlighting how we believe that it should be
		  handled in the future.Assuming readers have read our
		  previous messages, here, we will only describe differences
		  between the current cycle and the previous ones. These
		  include: (1) Finally unifying submission and acceptance
		  paper formatting instructions (forgoing the `jpaper' class)
		  to rid authors of accepted papers from the need to
		  reformat; (2) Describing the methodology we employed to
		  select best papers, which we believe ensures quality and
		  hope will persist; and (3) Reporting the ethical incidents
		  we encountered and how we handled them. In the final,
		  fourth volume, when the outcome of the ASPLOS'24 fall major
		  revisions will become known, we plan to conduct a broader
		  analysis of all the data we have gathered throughout the
		  year.Following are some key statistics of the fall cycle:
		  340 submissions were finalized (43\% more than last year's
		  fall count and 17\% less than our summer cycle) of which
		  111 are related to accelerators/FPGAs/GPUs, 105 to machine
		  learning, 54 to security, 50 to datacenter/cloud and 50 to
		  storage/memory; 183 (54\%) submissions were promoted to the
		  second review round; 39 (11.5\%) papers were accepted (of
		  which 19 were awarded artifact evaluation badges); 33
		  (9.7\%) submissions were allowed to submit major revisions
		  and are currently under review (these will be addressed in
		  the fourth volume of ASPLOS'24 and will be presented in
		  ASPLOS'25 if accepted); 1,368 reviews were uploaded; and
		  4,949 comments were generated during online discussions, of
		  which 4,070 were dedicated to the submissions that made it
		  to the second review round.This year, in the submission
		  form, we asked authors to specify which of the three ASPLOS
		  research areas are related to their submitted work.
		  Analyzing this data revealed that 80\%, 39\%, and 29\% of
		  the submissions are categorized by their authors as related
		  to architecture, operating systems, and programming
		  languages, respectively, generating the highest difference
		  we have observed across the cycles between architecture and
		  the other two. About 46\% of the fall submissions are
		  "interdisciplinary," namely, were associated with two or
		  more of the three areas.Overall, throughout all the
		  ASPLOS'24 cycles, we received 922 submissions, constituting
		  a 1.54x increase compared to last year. Our reviewers
		  submitted a total of 3,634 reviews containing more than 2.6
		  million words, and we also generated 12,655 online comments
		  consisting of nearly 1.2 million words. As planned, PC
		  members submitted an average of 15.7 reviews and a median
		  of 15, and external review committee (ERC) members
		  submitted an average of 4.7 and a median of 5.We accepted
		  170 papers thus far, written by 1100 authors, leading to an
		  18.4\% acceptance rate, with the aforementioned 33 major
		  revisions still under review. Assuming that the revision
		  acceptance rate will be similar to that of previous cycles,
		  we estimate that ASPLOS'24 will accept nearly 200 (!)
		  papers, namely, 21\%–22\% of the submissions.The
		  ASPLOS'24 program consists of 193 papers: the 170 papers we
		  accepted thus far and, in addition, 23 major revisions from
		  the fall cycle of ASPLOS'23, which were re-reviewed and
		  accepted. The full details are available in the PDF of the
		  front matter.},
  location	= {La Jolla, CA, USA}
}

@Article{	  10.1145/3579839,
  author	= {Zaji, Amirhossein and Liu, Zheng and Bando, Takashi and
		  Zhao, Lihua},
  title		= {Ontology-Based Driving Simulation for Traffic Lights
		  Optimization},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {3},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3579839},
  doi		= {10.1145/3579839},
  abstract	= {Traffic lights optimization is one of the principal
		  components to lessen the traffic flow and travel time in an
		  urban area. The present article seeks to introduce a novel
		  procedure to design the traffic lights in a city using
		  evolutionary-based optimization algorithms in combination
		  with an ontology-based driving behavior simulation
		  framework. Accordingly, an ontology-based knowledge base is
		  introduced to provide a machine-understandable knowledge of
		  roads and intersections, traffic rules, and driving
		  behaviors. Then, a simulation environment is developed to
		  inspect car behavior in real time. To optimize the traffic
		  lights, a sine-based equation was defined for each traffic
		  light, and the total travel time of the vehicles was
		  considered as the cost function in the optimization
		  algorithm. The optimization was performed with 5, 10, 15,
		  20, 25, and 30 vehicles in the urban areas. Based on the
		  results, in contrast to uncontrolled intersections without
		  traffic lights, optimized traffic lights can significantly
		  contribute to total travel time-saving. To conclude, due to
		  an escalation in the number of vehicles, the significance
		  of optimized traffic lights has encountered an increase,
		  and unoptimized traffic lights could increase total travel
		  time even more than a city deprived of any traffic light.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= mar,
  articleno	= {39},
  numpages	= {26},
  keywords	= {traffic light, autonomous car, driving behavior, knowledge
		  representation, ontology, Evolutionary optimization}
}

@Article{	  10.14778/3611540.3611634,
  author	= {Halevy, Alon and Choi, Yejin and Floratou, Avrilia and
		  Franklin, Michael J. and Noy, Natasha and Wang, Haixun},
  title		= {Will LLMs Reshape, Supercharge, or Kill Data Science?
		  (VLDB 2023 Panel)},
  year		= {2023},
  issue_date	= {August 2023},
  publisher	= {VLDB Endowment},
  volume	= {16},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3611540.3611634},
  doi		= {10.14778/3611540.3611634},
  abstract	= {Large language models (LLMs) have recently taken the world
		  by storm, promising potentially game changing opportunities
		  in multiple fields. Naturally, there is significant promise
		  in applying LLMs to the management of structured data, or
		  more generally, to the processes involved in data science.
		  At the very least, LLMs have the potential to provide
		  substantial advancements in long-standing challenges that
		  our community has been tackling for decades. On the other
		  hand, they may introduce completely new capabilities that
		  we have only dreamed of thus far. This panel will bring
		  together a few leading experts who have been thinking about
		  these opportunities from various perspectives and fielding
		  them in research prototypes and even in commercial
		  applications.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {4114–4115},
  numpages	= {2}
}

@InProceedings{	  10.1145/3640794.3665537,
  author	= {Seaborn, Katie and Gessinger, Iona and Yoshida, Suzuka and
		  Cowan, Benjamin R. and Doyle, Philip R.},
  title		= {Cross-Cultural Validation of Partner Models for Voice User
		  Interfaces},
  year		= {2024},
  isbn		= {9798400705113},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640794.3665537},
  doi		= {10.1145/3640794.3665537},
  abstract	= {Recent research has begun to assess people’s perceptions
		  of voice user interfaces (VUIs) as dialogue partners,
		  termed partner models. Current self-report measures are
		  only available in English, limiting research to
		  English-speaking users. To improve the diversity of user
		  samples and contexts that inform partner modelling
		  research, we translated, localized, and evaluated the
		  Partner Modelling Questionnaire (PMQ) for non-English
		  speaking Western (German, n=185) and East Asian (Japanese,
		  n=198) cohorts where VUI use is popular. Through
		  confirmatory factor analysis (CFA), we find that the scale
		  produces equivalent levels of “goodness-to-fit” for
		  both our German and Japanese translations, confirming its
		  cross-cultural validity. Still, the structure of the
		  communicative flexibility factor did not replicate directly
		  across Western and East Asian cohorts. We discuss how our
		  translations can open up critical research on cultural
		  similarities and differences in partner model use and
		  design, whilst highlighting the challenges for ensuring
		  accurate translation across cultural contexts.},
  booktitle	= {Proceedings of the 6th ACM Conference on Conversational
		  User Interfaces},
  articleno	= {19},
  numpages	= {10},
  keywords	= {conversational user interfaces, cross-cultural research,
		  human-computer interaction, human-machine dialogue, mental
		  models, partner models, speech interfaces, voice user
		  interfaces},
  location	= {Luxembourg, Luxembourg},
  series	= {CUI '24}
}

@InProceedings{	  10.1145/3711542.3711580,
  author	= {Jiang, Xiaorui and Khan, Kulsoom and Vasantha, Sumithra
		  Thinakara and Haider, Sajjad},
  title		= {Evidence Extraction for Automated Medical Coding:
		  Preliminary Evaluation},
  year		= {2025},
  isbn		= {9798400717383},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711542.3711580},
  doi		= {10.1145/3711542.3711580},
  abstract	= {Coding clinical texts in standard language such as ICD is
		  an important but tedious and error-prone process. Automated
		  medical coding algorithms suffer problems due to the
		  combined the challenge of handling the significant length
		  of clinical text, the complexity of the huge code hierarchy
		  and the lack of interpretability to ensure user trust.
		  Large language models (LLM) have also been proven
		  struggling with this task in recent studies. Recent efforts
		  have been made to annotate an evidence-supported medical
		  coding dataset. The current study makes the first empirical
		  investigation into how well (small) fine-tuned pretrained
		  language models (PLM) and LLMs could identify the sentences
		  containing medical evidence supporting the assigned codes.
		  Hierarchical sequential sentence classification and GPT-3.5
		  in the zero-shot setting were tested for evidence sentence
		  extraction. Extra evaluation was performed to investigate
		  how evidence extraction impacts clinical coding and what
		  implications it has towards the future generation
		  algorithms for automated medical coding.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {18–23},
  numpages	= {6},
  keywords	= {ICD coding, code evidence, sequential sentence
		  classification, large language model},
  location	= { },
  series	= {NLPIR '24}
}

@InProceedings{	  10.1145/3706890.3706997,
  author	= {Nan, Beier and Gu, Jinguang and Qiu, Chen and Wu,
		  Jingyun},
  title		= {Construction and application of medical history knowledge
		  graph based on UIE model fine-tuning},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706997},
  doi		= {10.1145/3706890.3706997},
  abstract	= {Objective: To achieve the automated extraction of complex
		  medical history knowledge from Chinese electronic medical
		  records, a fine-tuned UIE extraction model is utilized to
		  automatically obtain medical history knowledge and
		  construct a knowledge graph (KG) of medical histories.
		  Method: Taking the current medical history as an example, a
		  fundamental knowledge base of Chinese medical history was
		  first built. Then, based on this knowledge base, a training
		  set was annotated, and the UIE model was fine-tuned using
		  this training set. The fine-tuned UIE model was then used
		  to extract medical history knowledge, which was processed
		  and stored to generate a KG of medical histories. Results:
		  The fine-tuned UIE model achieved entity, relationship, and
		  event extraction tasks. Subsequently, the extracted medical
		  history information was processed and stored, successfully
		  constructing a KG of the current medical history.
		  Conclusion: This method realizes the completion of entity,
		  relation, and event extraction tasks by training only one
		  model, efficiently achieving automatic extraction of
		  specified medical history knowledge from Chinese electronic
		  medical records and constructing a KG of medical histories.
		  It helps organize and analyze complex medical history
		  knowledge for clinical use, offering practical value.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {621–626},
  numpages	= {6},
  keywords	= {Constructing KG, Information extraction, Medical history
		  knowledge, Model fine-tuning},
  location	= { },
  series	= {ISAIMS '24}
}

@Article{	  10.1613/jair.1.13939,
  author	= {\"{O}zcep, \"{O}zg\"{u}r L\"{u}tf\"{u} and Leemhuis, Mena
		  and Wolter, Diedrich},
  title		= {Embedding Ontologies in the Description Logic ALC by
		  Axis-Aligned Cones},
  year		= {2024},
  issue_date	= {Jan 2024},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {78},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.13939},
  doi		= {10.1613/jair.1.13939},
  abstract	= {This paper is concerned with knowledge graph embedding
		  with background knowledge, taking the formal perspective of
		  logics. In knowledge graph embedding, knowledge—
		  expressed as a set of triples of the form (a R b) (“a is
		  R-related to b”)—is embedded into a real-valued vector
		  space. The embedding helps exploiting geometrical
		  regularities of the space in order to tackle typical
		  inductive tasks of machine learning such as link
		  prediction. Recent embedding approaches also consider
		  incorporating background knowledge, in which the intended
		  meanings of the symbols a, R, b are further constrained via
		  axioms of a theory. Of particular interest are theories
		  expressed in a formal language with a neat semantics and a
		  good balance between expressivity and feasibility. In that
		  case, the knowledge graph together with the background can
		  be considered to be an ontology. This paper develops a
		  cone-based theory for embedding in order to advance the
		  expressivity of the ontology: it works (at least) with
		  ontologies expressed in the description logic ALC, which
		  comprises restricted existential and universal quantifiers,
		  as well as concept negation and concept disjunction. In
		  order to align the classical Tarskian Style semantics for
		  ALC with the sub-symbolic representation of triples, we use
		  the notion of a geometric model of an ALC ontology and
		  show, as one of our main results, that an ALC ontology is
		  satisfiable in the classical sense iff it is satisfiable by
		  a geometric model based on cones. The geometric model, if
		  treated as a partial model, can even be chosen to be
		  faithful, i.e., to reflect all and only the knowledge
		  captured by the ontology. We introduce the class of
		  axis-aligned cones and show that modulo simple geometric
		  operations any distributive logic (such as ALC) interpreted
		  over cones employs this class of cones. Cones are also
		  attractive from a machine learning perspective on knowledge
		  graph embeddings since they give rise to applying conic
		  optimization techniques.},
  journal	= {J. Artif. Int. Res.},
  month		= jan,
  numpages	= {51}
}

@InProceedings{	  10.1145/3511808.3557184,
  author	= {Paulus, Alexander and Burgdorf, Andreas and Langer,
		  Tristan and Pomp, Andr\'{e} and Meisen, Tobias and Pol,
		  Sebastian},
  title		= {PLASMA: A Semantic Modeling Tool for Domain Experts},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557184},
  doi		= {10.1145/3511808.3557184},
  abstract	= {In recent years, Knowledge Graphs and Ontology-based Data
		  Management have proven to be particularly effective in the
		  efficient management and consolidation of heterogeneous
		  data sources. In this context, semantic modeling has proven
		  to be a useful approach for creating semantic data
		  annotations. However, automatically generated semantic
		  models usually need to be revised by a domain expert, who
		  is often not familiar with semantic technologies. For
		  addressing this issue, we propose the PLASMA semantic
		  modeling tool, which aims at enabling domain experts to
		  build semantic models from scratch or refine models created
		  by automatic algorithms. We demonstrate the use of the tool
		  and its user interface in two different semantic data
		  management use cases for integrating smart city data in a
		  public funded project, called City Dataspace, and for
		  creating semantic models in an industrial use case at
		  Siemens AG.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {4946–4950},
  numpages	= {5},
  keywords	= {semantic refinement, semantic modeling, resource
		  description framework, graphical user interface},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@Article{	  10.1145/3447735,
  author	= {Darwish, Kareem and Habash, Nizar and Abbas, Mourad and
		  Al-Khalifa, Hend and Al-Natsheh, Huseein T. and Bouamor,
		  Houda and Bouzoubaa, Karim and Cavalli-Sforza, Violetta and
		  El-Beltagy, Samhaa R. and El-Hajj, Wassim and Jarrar,
		  Mustafa and Mubarak, Hamdy},
  title		= {A panoramic survey of natural language processing in the
		  Arab world},
  year		= {2021},
  issue_date	= {April 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {64},
  number	= {4},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3447735},
  doi		= {10.1145/3447735},
  journal	= {Commun. ACM},
  month		= mar,
  pages		= {72–81},
  numpages	= {10}
}

@InProceedings{	  10.1145/3724154.3724349,
  author	= {Gao, Juan and Guo, Xiangyun and Xiang, Tianqi},
  title		= {Research on the Evolutionary Path of Public Opinion in
		  Networked Emergencies Based on Large Models——Taking the
		  Gansu earthquake as an example},
  year		= {2025},
  isbn		= {9798400711862},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3724154.3724349},
  doi		= {10.1145/3724154.3724349},
  abstract	= {Construct a rational map of online public opinion on
		  emergencies, reveals the evolutionary paths and
		  developmental contexts of public opinion incidents,
		  providing decision support for government departments in
		  managing and guiding public sentiments. Select Sina Weibo
		  as the data source, use octopus collector to collect Gansu
		  earthquake Weibo data, and then perform data preprocessing.
		  The causal relationships of the events were determined
		  based on a rule-based template matching method, and a
		  multi-round dialogue mechanism based on large language
		  models was used to automatically extract causal event
		  pairs. The K-means++ clustering algorithm was employed to
		  generalize the events and construct a network public
		  opinion event logic graph, analyzing the evolutionary paths
		  of online public sentiments. The research findings indicate
		  that public sentiment regarding the Gansu earthquake is
		  characterized by multiple levels and complex causal
		  relationships. The involvement of multiple causal chains
		  contributes to a high degree of complexity in the evolution
		  of public sentiment. By constructing an event logic graph,
		  the development process of the Gansu earthquake public
		  opinion event has been demonstrated, which can provide
		  technical support for real-time monitoring, in-depth
		  analysis, and precise response of public opinion.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Big Data Economy and Information Management},
  pages		= {1206–1211},
  numpages	= {6},
  keywords	= {Multiple rounds of dialogue, emergencies, evolutionary
		  pathways, large model, online public opinion},
  location	= { },
  series	= {BDEIM '24}
}

@Article{	  10.1145/3640314,
  author	= {Zahid, Maryam and Bucaioni, Alessio and Flammini,
		  Francesco},
  title		= {Model-based Trustworthiness Evaluation of Autonomous
		  Cyber-Physical Production Systems: A Systematic Mapping
		  Study},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3640314},
  doi		= {10.1145/3640314},
  abstract	= {The fourth industrial revolution, i.e., Industry 4.0, is
		  associated with Cyber-Physical Systems (CPS), which are
		  entities integrating hardware (e.g., smart sensors and
		  actuators connected through the Industrial Internet of
		  Things) together with control and analytics software used
		  to drive and support decisions at several levels. The
		  latest developments in Artificial Intelligence (AI) and
		  Machine Learning (ML) have enabled increased autonomy and
		  closer human-robot cooperation in the production and
		  manufacturing industry, thus leading to Autonomous
		  Cyber-Physical Production Systems (ACPPS) and paving the
		  way to the fifth industrial revolution (i.e., Industry
		  5.0). ACPPS are increasingly critical due to the possible
		  consequences of their malfunctions on human co-workers, and
		  therefore, evaluating their trustworthiness is essential.
		  This article reviews research trends, relevant attributes,
		  modeling languages, and tools related to the model-based
		  trustworthiness evaluation of ACPPS. As in many other
		  engineering disciplines and domains, model-based
		  approaches, including stochastic and formal analysis tools,
		  are essential to master the increasing complexity and
		  criticality of ACPPS and to prove relevant attributes such
		  as system safety in the presence of intelligent behaviors
		  and uncertainties.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {157},
  numpages	= {28},
  keywords	= {Autonomous cyber-physical production systems,
		  cyber-physical manufacturing systems, industry 4.0,
		  industry 5.0, automation, trustworthiness, models, mapping
		  study}
}

@Proceedings{	  10.1145/3727582,
  title		= {PROMISE '25: Proceedings of the 21st International
		  Conference on Predictive Models and Data Analytics in
		  Software Engineering},
  year		= {2025},
  isbn		= {9798400715945},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Trondheim, Norway}
}

@Article{	  10.1109/taslp.2024.3497586,
  author	= {Ma, Hao and Peng, Zhiyuan and Li, Xu and Shao, Mingjie and
		  Wu, Xixin and Liu, Ju},
  title		= {CLAPSep: Leveraging Contrastive Pre-Trained Model for
		  Multi-Modal Query-Conditioned Target Sound Extraction},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3497586},
  doi		= {10.1109/TASLP.2024.3497586},
  abstract	= {Universal sound separation (USS) aims to extract arbitrary
		  types of sounds from real-world recordings. This can be
		  achieved by language-queried target sound extraction (TSE),
		  which typically consists of two components: a query network
		  that converts user queries into conditional embeddings, and
		  a separation network that extracts the target sound
		  accordingly. Existing methods commonly train models from
		  scratch. As a consequence, substantial data and
		  computational resources are required to make the randomly
		  initialized model comprehend sound events and perform
		  separation accordingly. In this paper, we propose to
		  integrate pre-trained models into TSE models to address the
		  above issue. To be specific, we tailor and adapt the
		  powerful contrastive language-audio pre-trained model
		  (CLAP) for USS, denoted as CLAPSep. CLAPSep also accepts
		  flexible user inputs, taking both positive and negative
		  user prompts of uni- and/or multi-modalities for target
		  sound extraction. These key features of CLAPSep can not
		  only enhance the extraction performance but also improve
		  the versatility of its application. We provide extensive
		  experiments on 5 diverse datasets to demonstrate the
		  superior performance and zero- and few-shot
		  generalizability of our proposed CLAPSep with fast training
		  convergence, surpassing previous methods by a significant
		  margin. Full codes and some audio examples are released for
		  reproduction and evaluation.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {4945–4960},
  numpages	= {16}
}

@InProceedings{	  10.1145/3701716.3717819,
  author	= {Jia, Runsong and Zhang, Bowen and M\'{e}ndez, Sergio
		  Jos\'{e} Rodr\'{\i}guez and Omran, Pouya G.},
  title		= {StructRAG: Structure-Aware RAG Framework with Scholarly
		  Knowledge Graph for Diverse Question Answering},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3717819},
  doi		= {10.1145/3701716.3717819},
  abstract	= {Recent advances in Large Language Models (LLMs) and
		  Retrieval-Augmented Generation (RAG) have shown promise in
		  academic question answering. However, existing approaches
		  often fail to fully utilize document structural information
		  and lack diversity in retrieved contexts. This paper
		  presents StructRAG, a structure-aware RAG framework that
		  leverages scholarly knowledge graphs for enhanced question
		  answering. Our framework features three key innovations:
		  (1) an automated knowledge graph construction pipeline
		  based on Deep Document Model (DDM) that preserves document
		  hierarchical structure, (2) a structure-aware retrieval
		  mechanism that combines semantic relevance with source
		  diversity, and (3) a context-enhanced generation approach
		  that integrates structural metadata for improved answer
		  synthesis. Experimental results on 329 computer science
		  papers demonstrate that StructRAG significantly outperforms
		  vanilla RAG baseline. While maintaining comparable semantic
		  accuracy (91\% vs 90\%), our approach achieves
		  substantially higher diversity in generated answers
		  (Distinct-1: 62\% vs 52\%, Distinct-2: 89\% vs 78\%) and
		  better answer quality across all metrics, with notable
		  improvements in relevance (29\%) and readability (36.5\%).
		  These results demonstrate that StructRAG effectively
		  enhances both the diversity and quality of academic
		  question answering.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2567–2573},
  numpages	= {7},
  keywords	= {deep document model, knowledge graph, knowledge graph
		  construction, large language models, retrieval-augmented
		  generation},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3405962.3405983,
  author	= {Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis
		  and Symeonidis, Moysis and Pallis, George and Dikaiakos,
		  Marios and Pouis, Loukas and Orphanou, Kalia and
		  Lampathaki, Fenareti and Alexandrou, Dimitrios},
  title		= {The ICARUS Ontology: A general aviation ontology developed
		  using a multi-layer approach},
  year		= {2020},
  isbn		= {9781450375429},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3405962.3405983},
  doi		= {10.1145/3405962.3405983},
  abstract	= {The management of aviation data is a great challenge in
		  the aviation industry, as they are complex and can be
		  derived from heterogeneous data sources. To handle this
		  challenge, ontologies can be applied to facilitate the
		  modelling of the data across multiple data sources. This
		  paper presents an aviation domain ontology, the ICARUS
		  ontology, which aims at facilitating the semantic
		  description and integration of information resources that
		  represent the various assets of the ICARUS platform and
		  their use. To present the functionality and usability of
		  the proposed ontology, we present the results of querying
		  the ontology using SPARQL queries through three use case
		  scenarios. As shown from the evaluation, the ICARUS
		  ontology enables the integration and reasoning over
		  multiple sources of heterogeneous aviation-related data,
		  the semantic description of metadata produced by ICARUS,
		  and their storage in a knowledge-base which is dynamically
		  updated and provides access to its contents via SPARQL
		  queries.},
  booktitle	= {Proceedings of the 10th International Conference on Web
		  Intelligence, Mining and Semantics},
  pages		= {21–32},
  numpages	= {12},
  keywords	= {services, queries, ontology, datasets, aviation},
  location	= {Biarritz, France},
  series	= {WIMS 2020}
}

@InProceedings{	  10.1145/3360901.3364449,
  author	= {Vu, Binh and Pujara, Jay and Knoblock, Craig A.},
  title		= {D-REPR: A Language for Describing and Mapping
		  Diversely-Structured Data Sources to RDF},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364449},
  doi		= {10.1145/3360901.3364449},
  abstract	= {Publishing data sources to knowledge graphs is a
		  complicated and laborious process as data sources are often
		  heterogeneous, hierarchical and interlinked. As an example,
		  food price datasets may contain product prices of various
		  units at different markets and times, and different
		  providers can have many choices of formats such as CSV,
		  JSON or spreadsheet. Beyond data formats, these datasets
		  may have differing layout, where one dataset may be
		  organized as a row-based table or relational table (prices
		  are in one column), while another may use a matrix table
		  (prices are in one matrix). To address these problems, we
		  present a novel data description language for mapping
		  datasets to RDF. In particular, our language supports
		  specifying the locations of source attributes in the
		  sources, mapping of the attributes to ontologies, and
		  simple rules to join the data of these attributes to output
		  final RDF triples. Unlike existing approaches, our language
		  is not restricted to specific data layouts such as the
		  Nested Relational Model, or to specific data formats, such
		  as spreadsheet. Our broad data description language
		  presents a format-independent solution, allowing
		  interlinking among multiple heterogeneous sources and
		  representing many diverse data structures that existing
		  tools are unable to handle.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {189–196},
  numpages	= {8},
  keywords	= {rdf mapping, linked data, knowledge graph},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.1145/3622896.3622922,
  author	= {Li, Min and Xu, Jianliang and Wu, Xiaoquan},
  title		= {Construction of Traditional Culture Ontology Based on
		  Representation and Role},
  year		= {2023},
  isbn		= {9798400708190},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3622896.3622922},
  doi		= {10.1145/3622896.3622922},
  abstract	= {Traditional culture refers to a culture that has evolved
		  from civilization and can reflect the characteristics and
		  spirit of a nation. However, at present, the traditional
		  cultural ontology only focuses on modeling and data
		  organization of a certain type of traditional culture,
		  which is a certain distance from the massive cultural
		  information and urgent cultural needs. This article starts
		  from the perspective of protecting and inheriting
		  traditional culture, and based on representation,
		  incorporates role theory to construct a cultural ontology
		  centered on poetry, calligraphy, and painting. The
		  traditional cultural ontology constructed in this article
		  can further contribute to the field of artificial
		  intelligence.},
  booktitle	= {Proceedings of the 2023 4th International Conference on
		  Control, Robotics and Intelligent System},
  pages		= {156–159},
  numpages	= {4},
  keywords	= {traditional culture, role theory, representation
		  ontology},
  location	= {Guangzhou, China},
  series	= {CCRIS '23}
}

@InProceedings{	  10.1145/3706890.3706971,
  author	= {Man, Jianping and Hu, Zhensheng and Liu, Hongze and Yang,
		  Rui and Liu, Jingjing and Chen, Ziyi and Zhou, Yi},
  title		= {A Model for Epilepsy Named Entity Recognition Based on
		  Chinese EEG Reports},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706971},
  doi		= {10.1145/3706890.3706971},
  abstract	= {It is critical for clinical diagnosis and research to
		  extract and utilize the medical information from
		  electroencephalogram (EEG) reports of epilepsy. With the
		  widespread application of deep learning techniques in
		  health care, particularly in natural language processing
		  tasks, named entity recognition (NER) has emerged as an
		  essential tool for information extraction from medical
		  text. This study proposed an epilepsy NER model for EEG
		  reports to improve the efficiency of epilepsy text
		  processing. 17,606 paragraphs from real Chinese epilepsy
		  EEG reports as data samples, were meticulously annotated
		  with 17 entity types by clinical experts. The model used
		  the BERT and the Global Pointer (GP) algorithm to identify
		  nested and non-nested entities, achieved outstanding
		  performance in the Precision, Recall and F1 score. The
		  experimental results demonstrate that our method
		  significantly enhances the effectiveness of epilepsy entity
		  recognition, providing robust support for the automated
		  extraction and analysis of medical information.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {467–472},
  numpages	= {6},
  keywords	= {BERT model, EEG reports, Epilepsy, Global Pointer
		  algorithm, Named entity recognition},
  location	= { },
  series	= {ISAIMS '24}
}

@InProceedings{	  10.1145/3299819.3299830,
  author	= {Delaney, Steven and Chan, Christopher Chun Ki and Smith,
		  Doug},
  title		= {Natural Language Processing for Productivity Metrics for
		  Software Development Profiling in Enterprise Applications},
  year		= {2018},
  isbn		= {9781450366236},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3299819.3299830},
  doi		= {10.1145/3299819.3299830},
  abstract	= {In this paper, we utilize ontology-based information
		  extraction for semantic analysis and terminology linking
		  from a corpus of software requirement specification
		  documents from 400 enterprise-level software development
		  projects. The purpose for this ontology is to perform
		  semi-supervised learning on enterprise-level specification
		  documents towards an automated method of defining
		  productivity metrics for software development profiling.
		  Profiling an enterprise-level software development project
		  in the context of productivity is necessary in order to
		  objectively measure productivity of a software development
		  project and to identify areas of improvement in software
		  development when compared to similar software development
		  profiles or benchmark of these profiles. We developed a
		  semi-novel methodology of applying NLP OBIE techniques
		  towards determining software development productivity
		  metrics, and evaluated this methodology on multiple
		  practical enterprise-level software projects.},
  booktitle	= {Proceedings of the 2018 Artificial Intelligence and Cloud
		  Computing Conference},
  pages		= {83–87},
  numpages	= {5},
  keywords	= {Software Development Productivity, Software Development,
		  Natural Language Processing},
  location	= {Tokyo, Japan},
  series	= {AICCC '18}
}

@InProceedings{	  10.1145/3556223.3556262,
  author	= {Tramontana, Emiliano and Verga, Gabriella},
  title		= {Keeping Researchers Updated by Automatically Enriching an
		  Ontology in the Medical Field},
  year		= {2022},
  isbn		= {9781450396349},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3556223.3556262},
  doi		= {10.1145/3556223.3556262},
  abstract	= {Ontologies provide a common and standard dictionary of
		  terms in some domain for researchers to easily exchange
		  data. Forming an ontology requires several years of work
		  performed by human experts, and an ontology for a given
		  domain is thought to be stable for many years.
		  Nevertheless, as scientific articles are continuously
		  published to gather knowledge on recent findings, existing
		  ontologies risk becoming stale, or require further human
		  effort. Moreover, searching new articles without referring
		  to an ontology can be very time consuming and confusing,
		  especially for novice researchers. We propose an approach
		  for automatically relating newly available published
		  articles to existing ontologies. By automatically selecting
		  relevant scientific articles and making them appear besides
		  other data in an ontology, we aim at supporting experienced
		  and novice researchers. Therefore, as knowledge grows and
		  articles are available, the ontology used by researchers
		  will also be automatically connected, allowing them to
		  readily discover new findings. To validate the
		  effectiveness of the proposed approach, we have enriched
		  OBIB, an ontology for biobanking, with a selection of
		  articles extracted from PubMed. The approach is general
		  enough and can be applied to other ontologies or
		  publishers.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Computer and Communications Management},
  pages		= {257–262},
  numpages	= {6},
  keywords	= {e-learning, PubMed, Ontology enrichment, Ontologies,
		  Knowledge management},
  location	= {Okayama, Japan},
  series	= {ICCCM '22}
}

@Article{	  10.1145/3594723,
  author	= {Koho, Mikko and Coladangelo, L. P. and Ransom, Lynn and
		  Emery, Doug},
  title		= {Wikibase Model for Premodern Manuscript Metadata
		  Harmonization, Linked Data Integration, and Discovery},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3594723},
  doi		= {10.1145/3594723},
  abstract	= {To facilitate discovery of premodern manuscripts in U.S.
		  memory institutions, Digital Scriptorium, a growing
		  consortium of over 35 institutional members representing
		  American libraries, museums, and other cultural heritage
		  institutions, has developed a digital platform for an
		  online national union catalog. The platform will allow
		  low-barrier and efficient collection, aggregation, and
		  enrichment of member metadata and sustainably publish it as
		  Linked Open Data. This article describes the methods and
		  principles behind the data model development and the
		  decision to use Wikibase. The results of the prototype
		  implementation and testing phase demonstrate the
		  practicality and sustainability of Digital Scriptorium’s
		  approach to building an online national union catalog based
		  on Linked Open Data technologies and practices.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {56},
  numpages	= {25},
  keywords	= {semantic web, cultural heritage, digital humanities,
		  premodern manuscripts, data interoperability, Wikibase,
		  data modeling, Linked Open Data}
}

@Proceedings{	  10.1145/3699682,
  title		= {UMAP '25: Proceedings of the 33rd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  year		= {2025},
  isbn		= {9798400713132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InBook{	  10.1145/3191315.3191325,
  author	= {Christiansen, Henning and Dahl, Ver\'{o}nica},
  title		= {Natural language processing with (tabled and constraint)
		  logic programming},
  year		= {2018},
  isbn		= {9781970001990},
  publisher	= {Association for Computing Machinery and Morgan \&amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3191315.3191325},
  abstract	= {We survey the evolution of natural language processing as
		  it relates to Logic Programming, with particular focus on
		  David Scott Warren's crucial contributions such as tabling,
		  and the relationship with hypothetical reasoning and
		  constraint based programming. These topics lead naturally
		  to a view of parsing as constraint solving, which extends
		  to grammar inference. Our exposition of the subject is
		  intuitive and example-driven, with references to more
		  formal presentations when needed.},
  booktitle	= {Declarative Logic Programming: Theory, Systems, and
		  Applications},
  pages		= {477–511},
  numpages	= {35}
}

@InProceedings{	  10.1145/3297280.3297661,
  author	= {Ojino, Ronald},
  title		= {User's profile ontology-based semantic model for
		  personalized hotel room recommendation in the web of
		  things: student research abstract},
  year		= {2019},
  isbn		= {9781450359337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297280.3297661},
  doi		= {10.1145/3297280.3297661},
  abstract	= {The hotel industry is not exempt from technology
		  disruption and will only keep its competitiveness through
		  strategy [3]including adjusting to the new customers
		  demands, reacting to competitors' innovations and taking
		  advantage of the new technological developments.
		  Personalization can enable an organization to customize its
		  offerings at the individual level. This paper seeks to
		  design and evaluate a novel Web of Things personalization
		  model that effectively utilizes information about a
		  customer's preferences to create a personalized service
		  space.},
  booktitle	= {Proceedings of the 34th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {2314–2316},
  numpages	= {3},
  keywords	= {machine learning, ontology, personalization, semantics},
  location	= {Limassol, Cyprus},
  series	= {SAC '19}
}

@InProceedings{	  10.1145/3427423.3427431,
  author	= {Togatorop, Parmonangan R. and Siagian, Rosa and
		  Nainggolan, Yolanda and Simanungkalit, Kaleb},
  title		= {Implementation of ontology-based on Word2Vec and DBSCAN
		  for part-of-speech},
  year		= {2021},
  isbn		= {9781450376051},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3427423.3427431},
  doi		= {10.1145/3427423.3427431},
  abstract	= {POS tagging is a process of marking text into an
		  appropriate word-class based on word definitions and word
		  relationships. In general, several POS tagging approaches
		  have been applied in Bahasa Indonesia namely rule-based,
		  stochastic, and neural. Besides, there is another approach
		  to POS tagging which has been applied to English, namely
		  the approach using ontology. This approach has not yet been
		  applied to Bahasa Indonesia so we will implement an
		  ontology to conduct POS tagging in Bahasa Indonesia. In
		  this study, the ontology was constructed using the Word2Vec
		  and the DBSCAN clustering method. The Word2Vec model is
		  implemented to extract each word in vector form based on
		  its context and the DBSCAN clustering method is implemented
		  for the classification process of word classes based on
		  word vectors modeled by Word2Vec. The process of POS
		  tagging with ontology is carried out in several stages,
		  namely: data collection using web scraping techniques from
		  Kompas.com and Detik.com online news articles, text
		  preprocessing, Word2Vec feature building, clustering with
		  DBSCAN, ontology construction and evaluation. The
		  experiments carried out in this study were to choose the
		  optimal parameter values from DBSCAN in forming word
		  clusters for ontology construction. Overall, the
		  implementation of ontology with Word2Vec and DBSCAN can do
		  POS tagging with the highest accuracy value of 0.62, the
		  highest precision value of 0.79, the highest recall value
		  of 0.62, and the highest f1-score of 0.67.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Sustainable Information Engineering and Technology},
  pages		= {51–56},
  numpages	= {6},
  keywords	= {ontology, bahasa indonesia, Word2Vec, POS tagging,
		  DBSCAN},
  location	= {Malang, Indonesia},
  series	= {SIET '20}
}

@Article{	  10.1145/3737880,
  author	= {Subagdja, Budhitama and Shanthoshigaa, D. and Tan,
		  Ah-Hwee},
  title		= {DisambiguART: A Neural-based Inference Model for Knowledge
		  Graph Disambiguation},
  year		= {2025},
  issue_date	= {July 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {6},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3737880},
  doi		= {10.1145/3737880},
  abstract	= {One main challenge in constructing a Knowledge Graph (KG)
		  is to deal with ambiguity. Specifically, an entity in the
		  graph can be assigned with multiple meanings while two or
		  more entities considered to have different meanings may
		  actually be the same. Assigning an entity with the correct
		  meaning may involve re-evaluation of its relevant contexts.
		  This costly operation typically involves searching for
		  other similar entities within the KG such that the context
		  can be determined. In this article, a new model called
		  DisambiguART is proposed leveraging multi-channel matching
		  and inference in a self-organizing neural network for sense
		  disambiguation in KGs. Unlike other disambiguation methods
		  that rely on representation learning to identify the
		  relevant contexts whereby similarities among entities are
		  learned, DisambiguART extends the working principle of
		  multi-channel Adaptive Resonance Theory (ART) to conduct
		  inferences directly over the graph representation through
		  bi-directional interactions of bottom-up activations and
		  top-down matching to find similar entities and select the
		  correct meaning according to the right context. The
		  proposed method is evaluated on the tasks of entity sense
		  disambiguation in three domain KGs (jet engine, biomedical,
		  and kinship) and author name disambiguation in
		  bibliographic KGs, demonstrating the effectiveness and
		  efficiency of DisambiguART against the state-of-the-art
		  methods.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= jul,
  articleno	= {121},
  numpages	= {29},
  keywords	= {Knowledge Graphs, Graph Embeddings, Entity Disambiguation,
		  Adaptive Resonance Theory}
}

@InProceedings{	  10.1145/3338188.3338215,
  author	= {Yingbo, Dong and Xia, Hou},
  title		= {Business Modeling and Reasoning Based on Process
		  Ontology},
  year		= {2019},
  isbn		= {9781450362931},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3338188.3338215},
  doi		= {10.1145/3338188.3338215},
  abstract	= {In this paper, a method of business modeling and reasoning
		  based on process ontology is proposed to solve the problem
		  that existing in the banking business field such as large
		  amount of data and the complexity to analysis the
		  relationship between data. We use process ontology
		  technology to describe each basic element in the business
		  process, and on this basis, design constraints and
		  inference rules, and carry out corresponding ontology
		  knowledge reasoning by use the inference rules. Practice
		  results show that the establishment of process ontology
		  model can effectively represent the knowledge involved in
		  the business process and discover the hidden information
		  contained in the knowledge.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Frontiers of Educational Technologies},
  pages		= {143–147},
  numpages	= {5},
  keywords	= {process ontology, knowledge reasoning, inference rules,
		  banking business field},
  location	= {Beijing, China},
  series	= {ICFET '19}
}

@InProceedings{	  10.1145/3459930.3469562,
  author	= {Lyons, Robert and Low, Geoffrey Ross and Congdon, Clare
		  Bates and Ceruolo, Melissa and Ballesteros, Marissa and
		  Cambria, Steven and DePetrillo, Paolo},
  title		= {Towards an extensible ontology for streaming sensor data
		  for clinical trials},
  year		= {2021},
  isbn		= {9781450384506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459930.3469562},
  doi		= {10.1145/3459930.3469562},
  abstract	= {The use of wearable sensors for clinical trials can lead
		  to better data collection and a better patient experience
		  during trials, and can further allow more patients to
		  participate in trials by allowing more remote monitoring
		  and fewer site visits. However, extracting maximum value
		  from the data collected via streaming sensors presents some
		  specific technical challenges, including processing the
		  data in real time, and storing the sensor data in a
		  representation that facilitates the use of biomarker
		  algorithms that can be used and reused with different
		  similar sensors, at different scales, and across different
		  clinical trials. Here we present our initial work on
		  SORBET, a Sensor Ontology for Reusable Biometric
		  Expressions and Transformations. Our design strategy is
		  presented, along with the initial design and examples.
		  While this ontology has been created for the Medidata
		  Sensor Cloud product, it is our hope that others working in
		  this space will join us in extending and hardening this
		  ontology, as we expand it to incorporate more sensors and
		  more needs for clinical trials research.},
  booktitle	= {Proceedings of the 12th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {48},
  numpages	= {6},
  keywords	= {health informatics, ontology engineering, semantic
		  networks},
  location	= {Gainesville, Florida},
  series	= {BCB '21}
}

@InProceedings{	  10.1145/3654522.3654561,
  author	= {Nguyen, Dien Thanh and Do, Nhon Van and Tran, Tung Hoang},
  title		= {A Model of Topic for Document Retrieval Systems in the
		  Field of Artificial Intelligence for Information Technology
		  students},
  year		= {2024},
  isbn		= {9798400716713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3654522.3654561},
  doi		= {10.1145/3654522.3654561},
  abstract	= {This article proposes a topic definition, topic modeling
		  and document search techniques related to topics in the
		  field of artificial intelligence, thereby building a
		  document search application based on topic. Document
		  warehouse includes ebooks and papers. The application will
		  serve the need to search for documents by topic for
		  information technology students during the learning and
		  research process, to solve that problem. Realizing that
		  Ontology and document search techniques are a suitable and
		  powerful approach for organizing the knowledge base as well
		  as solving the problem of retrieving documents by topic
		  with high efficiency. From there, the following databases
		  are built: the Ebook and Paper database is stored in a
		  structured form, the topic database is to store a set of
		  topics in the field of artificial intelligence, Ontology
		  database for ebooks and papers, keyphrase graph database to
		  store topic and document representations and semantic
		  similarity matching techniques between documents and
		  topics.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Intelligent Information Technology},
  pages		= {376–385},
  numpages	= {10},
  keywords	= {Additional Key Words and Phrases: Topic definition, domain
		  ontology, graph matching, topic modeling, topic
		  representation},
  location	= {Ho Chi Minh City, Vietnam},
  series	= {ICIIT '24}
}

@InProceedings{	  10.1145/3643655.3643876,
  author	= {Rossi, Maria Teresa and Tundo, Alessandro and Mariani,
		  Leonardo},
  title		= {Towards Model-Driven Dashboard Generation for
		  Systems-of-Systems},
  year		= {2024},
  isbn		= {9798400705571},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643655.3643876},
  doi		= {10.1145/3643655.3643876},
  abstract	= {Configuring and evolving dashboards in complex and
		  large-scale Systems-of-Systems (SoS) can be an expensive
		  and cumbersome task due to the many Key Performance
		  Indicators (KPIs) that are usually collected and have to be
		  arranged in a number of visualizations. Unfortunately,
		  setting up dashboards is still a largely manual and
		  error-prone task requiring extensive human
		  intervention.This short paper describes emerging results
		  about the definition of a model-driven technology-agnostic
		  approach that can automatically transform a simple list of
		  KPIs into a dashboard model, and then translate the model
		  into an actual dashboard for a target dashboard technology.
		  Dashboard customization can be efficiently obtained by
		  solely modifying the abstract model representation, freeing
		  operators from expensive interactions with actual
		  dashboards.},
  booktitle	= {Proceedings of the 12th ACM/IEEE International Workshop on
		  Software Engineering for Systems-of-Systems and Software
		  Ecosystems},
  pages		= {9–12},
  numpages	= {4},
  keywords	= {automatic dashboard generation, model-driven engineering,
		  model-based dashboard, systems of systems, monitoring
		  dashboard},
  location	= {Lisbon, Portugal},
  series	= {SESoS '24}
}

@InProceedings{	  10.1145/3550356.3556502,
  author	= {Boubekeur, Younes and Singh, Prabhsimran and Mussbacher,
		  Gunter},
  title		= {A DSL and model transformations to specify learning
		  corpora for modeling assistants},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3556502},
  doi		= {10.1145/3550356.3556502},
  abstract	= {Software engineering undergraduate students spend a
		  significant time learning various topics related to
		  software design, including notably model-driven engineering
		  (MDE), where different types of structural and behavioral
		  models are used to design, implement, and validate an
		  application. MDE instructors spend a lot of time covering
		  modeling concepts, which is more difficult with
		  ever-increasing class sizes. Online resources, such as
		  learning corpora for domain modeling, can aid in this
		  learning process by serving as a more dynamic textbook
		  alternative or as part of a larger interactive application
		  with domain modeling exercises and tutorials. A Learning
		  Corpus (LC) is an extensible list of entries representing
		  possible mistakes that could occur when defining a model,
		  e.g., Missing Abstraction-Occurrence pattern in the case of
		  a domain model. Each LC entry includes progressive levels
		  of feedback, including written responses, quizzes, and
		  references to external resources. To make it easy for
		  instructors to customize the entries as well as add their
		  own, we propose a novel, simple, and intuitive approach
		  based on an internal domain-specific language that supports
		  features such as context-specific information and concise
		  arbitrary metamodel navigation with shorthands.
		  Transformations to source code as well as Markdown and
		  LATEX enable use of the LC entries in different contexts.
		  These transformations as well as the integration of the
		  generated code in a sample Modeling Assistant application
		  verify and validate the LC metamodel and specification.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {95–102},
  numpages	= {8},
  keywords	= {model-driven engineering (MDE), model transformation,
		  learning corpus, feedback, domain modeling},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3587259.3627554,
  author	= {Iglesias-Molina, Ana and Toledo, Jhon and Corcho, Oscar
		  and Chaves-Fraga, David},
  title		= {Re-Construction Impact on Metadata Representation Models},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627554},
  doi		= {10.1145/3587259.3627554},
  abstract	= {Reification in knowledge graphs has been present since the
		  inception of RDF to allow capturing additional information
		  in triples, usually metadata. The need of adopting or
		  changing a metadata representation in a pre-existing graph
		  to enhance the knowledge capture and access can lead to
		  inducing complex structural changes in the graph, according
		  the target representation’s schema. In these situations,
		  it is necessary to decide whether to construct the
		  knowledge graph again from its original sources, or to
		  re-construct it using the current version of the graph. In
		  this paper we conduct an empirical study to analyze which
		  re-construction approach is more suitable for switching the
		  representation approach from the created graph ensuring
		  that the additional represented knowledge is preserved. We
		  study four well-known metadata representations, using
		  mapping languages to construct the graph, and SPARQL
		  CONSTRUCT queries to re-construct it. With this work we aim
		  to provide insights about the impact of re-construction on
		  metadata representations interoperability and the
		  implications of different approaches.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {197–205},
  numpages	= {9},
  keywords	= {Declarative Mappings., Knowledge Graphs, Metadata,
		  SPARQL},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3631700.3665182,
  author	= {Jeromela, Jovan and Conlan, Owen},
  title		= {Devising Scrutable User Models for Time Management
		  Assistants},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3665182},
  doi		= {10.1145/3631700.3665182},
  abstract	= {Intelligent Personal Assistants (IPAs) have become
		  ubiquitous through integration into smartphones, smart
		  speakers, and standalone devices. However, prior studies
		  raised noteworthy usability concerns and determined that
		  IPAs remain primarily used for simple tasks. Such findings
		  contrast with the reported user aspirations for more
		  proactive and truly personalised IPAs. By focusing on the
		  use case of time management, in this paper, we contemplate
		  how scrutability – i.e. the ability of the user to study
		  their assistant and its underlying user model – fits
		  within the vision of more complex IPAs. Furthermore, we
		  describe our ongoing project investigating user interest in
		  and expectations of the scrutability of a proactive
		  calendaring assistant. Lastly, by deliberating on the
		  challenges and benefits of making IPAs scrutable, this
		  paper outlines potential avenues for further research.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {250–255},
  numpages	= {6},
  keywords	= {Explainability, Intelligent personal assistants, Proactive
		  dialogue systems, Time management},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@InProceedings{	  10.1145/3640310.3674096,
  author	= {Rajbhoj, Asha and Pathan, Ajim and Sant, Tanay and
		  Kulkarni, Vinay and Nistala, Padmalata and Pandey, Rajesh
		  and Narasimhan, Sabarinathan and Thiagarajan, Geetha},
  title		= {AutoMW: Model-based Automated Medical Writing},
  year		= {2024},
  isbn		= {9798400705045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640310.3674096},
  doi		= {10.1145/3640310.3674096},
  abstract	= {Medical Writing is an art of writing scientific documents
		  which includes regulatory and research-related content. To
		  obtain approval for marketing new medicines, pharmaceutical
		  companies are obligated to provide drug authorities with a
		  huge volume of documents related to clinical trials.
		  Creating these clinical trial documents is a time, effort,
		  and skill-intensive process as the required information
		  exists in fragmented form distributed across various
		  information sources. To overcome these challenges in
		  medical writing, we propose Automated Medical Writing tool
		  (AutoMW). AutoMW enables the digitalization of information
		  from different sources of information using a
		  meta-model-based approach and leverages these models for
		  the automated generation of clinical trial documents as per
		  the regulatory authority document templates. This paper
		  describes the approach and illustrates its utility and
		  efficacy in real-world clinical trial application of two
		  use cases - breast cancer, and diabetes.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {257–267},
  numpages	= {11},
  keywords	= {Automated Content Generation, Clinical Trial
		  Documentation, MDE, Medical Writing, NLP},
  location	= {Linz, Austria},
  series	= {MODELS '24}
}

@InProceedings{	  10.1145/3696952.3696987,
  author	= {Hu, Yan and Wang, Hongli},
  title		= {A Review of Mining User Needs Based on Text Sentiment
		  Analysis Technology and KANO Model},
  year		= {2024},
  isbn		= {9798400718076},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696952.3696987},
  doi		= {10.1145/3696952.3696987},
  abstract	= {This article reviews the research on user demand mining
		  based on online reviews. Through text sentiment analysis
		  techniques, such as sentiment lexicons, rules, and machine
		  learning methods, user needs are explored. The article
		  emphasizes the value of sentiment analysis of online
		  reviews in mining user needs and explores its relationship
		  with product sales and the application of the KANO model in
		  demand classification. Finally, it summarizes the
		  challenges of current research, future trends, and the
		  importance of combining sentiment analysis with user demand
		  mining.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Intelligent Information Processing},
  pages		= {257–264},
  numpages	= {8},
  keywords	= {Deep learning, Machine learning, Online reviews, Text
		  analysis},
  location	= { },
  series	= {ICIIP '24}
}

@InProceedings{	  10.1145/3178372.3179515,
  author	= {Ginsbach, Philip and Crawford, Lewis and O'Boyle, Michael
		  F. P.},
  title		= {CAnDL: a domain specific language for compiler analysis},
  year		= {2018},
  isbn		= {9781450356442},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3178372.3179515},
  doi		= {10.1145/3178372.3179515},
  abstract	= {Optimizing compilers require sophisticated program
		  analysis and transformations to exploit modern hardware.
		  Implementing the appropriate analysis for a compiler
		  optimization is a time consuming activity. For example, in
		  LLVM, tens of thousands of lines of code are required to
		  detect appropriate places to apply peephole optimizations.
		  It is a barrier to the rapid prototyping and evaluation of
		  new optimizations. In this paper we present the Compiler
		  Analysis Description Language (CAnDL), a domain specific
		  language for compiler analysis. CAnDL is a constraint based
		  language that operates over LLVM's intermediate
		  representation. The compiler developer writes a CAnDL
		  program, which is then compiled by the CAnDL compiler into
		  a C++ LLVM pass. It provides a uniform manner in which to
		  describe compiler analysis and can be applied to a range of
		  compiler analysis problems, reducing code length and
		  complexity. We implemented and evaluated CAnDL on a number
		  of real world use cases: eliminating redundant operations;
		  graphics code optimization; identifying static control flow
		  regions. In all cases were we able to express the analysis
		  more briefly than competing approaches.},
  booktitle	= {Proceedings of the 27th International Conference on
		  Compiler Construction},
  pages		= {151–162},
  numpages	= {12},
  keywords	= {LLVM, constraint programming, optimization},
  location	= {Vienna, Austria},
  series	= {CC '18}
}

@InProceedings{	  10.1145/3579170.3579265,
  author	= {Ollier, Guillaume and Arnez, Fabio and Adedjouma, Morayo
		  and Lallement, Rapha\"{e}l and Gerasimou, Simos and
		  Mraidha, Chokri},
  title		= {Towards an Ontological Methodology for Dynamic
		  Dependability Management of Unmanned Aerial Vehicles},
  year		= {2023},
  isbn		= {9798400700453},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579170.3579265},
  doi		= {10.1145/3579170.3579265},
  abstract	= {Dynamic Dependability Management (DDM) is a promising
		  approach to guarantee and monitor the ability of
		  safety-critical Automated Systems (ASs) to deliver the
		  intended service with an acceptable risk level. However,
		  the non-interpretability and lack of specifications of the
		  Learning-Enabled Component (LEC) used in ASs make this
		  mission particularly challenging. Some existing DDM
		  techniques overcome these limitations by using
		  probabilistic environmental perception knowledge associated
		  with predicting behavior changes for the agents in the
		  environment. Ontology-based methods allow using a formal
		  and traceable representation of AS usage scenarios to
		  support the design process of the DDM component of such
		  ASs. This paper presents a methodology to perform this
		  design process, starting from the AS specification stage
		  and including threat analysis and requirements
		  identification. The present paper focuses on the
		  formalization of an ontology modeling language allowing the
		  interpretation of logical usage scenarios, i.e., a formal
		  description of the scenario represented by state variables.
		  The proposed supervisory system also considers the
		  uncertainty estimation and interaction between AS
		  components through the whole perception-planning-control
		  pipeline. This methodology is illustrated in this paper on
		  a use case involving Unmanned Aerial Vehicles (UAVs).},
  booktitle	= {Proceedings of the DroneSE and RAPIDO: System Engineering
		  for Constrained Embedded Systems},
  pages		= {12–19},
  numpages	= {8},
  keywords	= {Autonomous Systems, Cyber-Physical Systems, Dynamic Risk
		  Management, Real-time Monitoring, Safety-critical Systems},
  location	= {Toulouse, France},
  series	= {RAPIDO '23}
}

@Article{	  10.1145/3675781,
  author	= {Pham, Quoc-Hung and Le, Huu-Loi and Dang Nhat, Minh and
		  Tran T., Khang and Tran-Tien, Manh and Dang, Viet-Hung and
		  Vu, Huy-The and Nguyen, Minh-Tien and Phan, Xuan-Hieu},
  title		= {Towards Vietnamese Question and Answer Generation: An
		  Empirical Study},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {9},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3675781},
  doi		= {10.1145/3675781},
  abstract	= {Question-answer generation (QAG) is a challenging task
		  that generates both questions and answers from a given
		  input paragraph context. The QAG task has recently achieved
		  promising results thanks to the appearance of large
		  pre-trained language models, yet, QAG models are mainly
		  implemented in common languages, e.g., English. There still
		  remains a gap in domain and language adaptation of these
		  QAG models to low-resource languages such as Vietnamese. To
		  address the gap, this article presents a large-scale and
		  systematic study of QAG in Vietnamese. To do that, we first
		  implement several QAG models by using the common
		  fine-tuning techniques based on powerful pre-trained
		  language models. We next introduce a set of instructions
		  designed for the QAG task. These instructions are used to
		  fine-tuned the pre-trained language and large language
		  models. Extensive experimental results of both automatic
		  and human evaluation on five benchmark machine reading
		  comprehension datasets show two important points. First,
		  the instruction-tuning method has the potential to enhance
		  the performance of QAG models. Second, large language
		  models trained in English need more data for fine-tuning to
		  work well on the downstream QAG tasks of low-resource
		  languages. We also provide a prototype system to
		  demonstrate how our QAG models actually work. The code for
		  fine-tuning QAG models and instructions are also made
		  available.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  articleno	= {132},
  numpages	= {28},
  keywords	= {Natural language processing, question and answer
		  generation, large pre-trained language models, instruction
		  fine-tuning, BARTPho, ViT5, LlaMa2}
}

@InProceedings{	  10.1145/3703790.3703803,
  author	= {Papadakis, Nikolaos and Bouloukakis, Georgios and
		  Magoutis, Kostas},
  title		= {Enabling IoT-enhanced Data Models for Context-aware
		  Hydropower Plants},
  year		= {2025},
  isbn		= {9798400712852},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703790.3703803},
  doi		= {10.1145/3703790.3703803},
  abstract	= {Hydroelectric power, or hydropower, harnesses the
		  potential energy of water descending from higher to lower
		  elevations to generate electricity. As a well-established
		  and cost-effective renewable energy technology, it not only
		  produces power but also supports significant water
		  management services. The integration of Internet of Things
		  (IoT) technologies in hydropower plants has shown
		  significant potential in enhancing monitoring, efficiency,
		  and control capabilities. However, current implementations
		  often lack a holistic and standardized approach to
		  contextual modeling. To address this gap, this paper
		  presents a comprehensive approach to modeling the
		  structural and operational components of hydropower plants
		  (HPPs) using NGSI-LD data models. We propose detailed
		  NGSI-LD data models that incorporate both static properties
		  (e.g., location, structural attributes), relationships
		  (e.g., component interactions, hierarchical dependencies)
		  and dynamic properties (e.g., real-time sensor data,
		  operational status). These models are designed to
		  facilitate efficient data integration, support
		  decision-making processes, and enable the development of
		  interoperable and replicable IoT applications for smart
		  hydropower plants. We validate our approach through
		  deployment and testing on a federated context broker
		  architecture using real-world data from HPPs.},
  booktitle	= {Proceedings of the 14th International Conference on the
		  Internet of Things},
  pages		= {108–116},
  numpages	= {9},
  keywords	= {Renewable Energy, Data Models, Internet of Things (IoT),
		  NGSI-LD, Smart Energy, Interoperability, Context-Aware
		  Systems},
  location	= { },
  series	= {IoT '24}
}

@InProceedings{	  10.1145/3579051.3579074,
  author	= {Zhou, Baifan and Tan, Zhipeng and Zheng, Zhuoxun and Zhou,
		  Dongzhuoran and Savkovic, Ognjen and Kharlamov, Evgeny},
  title		= {Towards A Visualisation Ontology for Reusable Visual
		  Analytics},
  year		= {2023},
  isbn		= {9781450399876},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579051.3579074},
  doi		= {10.1145/3579051.3579074},
  abstract	= {Data analytics including machine learning analytics is
		  essential to extract insights from production data in
		  modern industries. Visual analytics is essential for data
		  analytics for e.g., presenting the data to provide an
		  instinctive perception in exploratory data analysis,
		  facilitating the presentation of data analysis results and
		  the subsequent discussion on that. Visual analytics should
		  allow a transparent common ground for discussion between
		  experts in data analysis projects, given the
		  multidisciplinary background of these experts. However, a
		  standarised and formalised way of describing the knowledge
		  and practice of visualisation is still lacking in the
		  industry, which hamstrings the transparency and reusability
		  of visual analytics. A visualisation ontology which models
		  the nature and procedure of visualisation is well-suited to
		  provide such standardisation. Currently a few studies
		  discuss partially the modelling of visualisation, but
		  insufficiently study the procedure of visualisation tasks,
		  which is important for transparency and reusability
		  especially in an industrial scenario. To this end, we
		  present our ongoing work of development of the
		  visualisation ontology in industrial scenarios at Bosch. We
		  also demonstrate its benefits with case studies and
		  knowledge graph based on our ontology.},
  booktitle	= {Proceedings of the 11th International Joint Conference on
		  Knowledge Graphs},
  pages		= {99–103},
  numpages	= {5},
  keywords	= {knowledge graph, ontology engineering, visual analytics},
  location	= {Hangzhou, China},
  series	= {IJCKG '22}
}

@InProceedings{	  10.5555/3522802.3522881,
  author	= {Saleh, Nurul and Bell, David and Sulaiman, Zuharabih},
  title		= {Hybrid conceptual modeling for simulation: an ontology
		  approach during covid-19},
  year		= {2022},
  publisher	= {IEEE Press},
  abstract	= {The recent outbreak of Covid-19 caused by SARS-CoV-2
		  infection that started in Wuhan, China, has quickly spread
		  worldwide. Due to the aggressive number of cases, the
		  entire healthcare system has to respond and make decisions
		  promptly to ensure it does not fail. Researchers have
		  investigated the integration between ontology, algorithms
		  and process modeling to facilitate simulation modeling in
		  emergency departments and have produced a Minimal-Viable
		  Simulation Ontology (MVSimO). However, the "minimalism" of
		  the ontology has yet to be explored to cover pandemic
		  settings. Responding to this, modelers must redesign
		  services that are Covid-19 safe and better reflect changing
		  realities. This study proposes a novel method that
		  conceptualizes processes within the domain from a
		  Discrete-Event Simulation (DES) perspective and utilizes
		  prediction data from an Agent-Based Simulation (ABS) model
		  to improve the accuracy of existing models. This hybrid
		  approach can be helpful to support local decision making
		  around resources allocation.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  articleno	= {108},
  numpages	= {11},
  location	= {Phoenix, Arizona},
  series	= {WSC '21}
}

@Article{	  10.1145/3295738,
  author	= {Bj\o{}rner, Dines},
  title		= {Domain Analysis and Description Principles, Techniques,
		  and Modelling Languages},
  year		= {2019},
  issue_date	= {April 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {28},
  number	= {2},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3295738},
  doi		= {10.1145/3295738},
  abstract	= {We present a method for analysing and describing
		  domains.By a domain we shall understand a rationally
		  describable segment of a human assisted reality, i.e., of
		  the world, its physical parts: natural [“God-given”]
		  and artifactual [“human-made”], and living species:
		  plants and animals including, notably, humans. These are
		  endurants (“still”), as well as perdurants
		  (“alive”). Emphasis is placed on
		  “human-assistedness,” that is, that there is at least
		  one (human-made) artifact and, therefore, that humans are a
		  primary cause for change of endurant states as well as
		  perdurant behaviours.By a method we shall mean a set of
		  principles of analysis and for selecting and applying a
		  number of techniques and tools in the construction of some
		  artifact, say a domain description. We shall present a
		  method for constructing domain descriptions. Among the
		  tools we shall only be concerned with are the analysis and
		  synthesis languages.Domain science and engineering marks a
		  new area of computing science. Just as we are formalising
		  the syntax and semantics of programming languages, so we
		  are formalising the syntax and semantics of human-assisted
		  domains. Just as physicists are studying the natural
		  physical world, endowing it with mathematical models, so
		  we, computing scientists, are studying these domains,
		  endowing them with mathematical models, A difference
		  between the endeavours of physicists and ours lies in the
		  tools: The physics models are based on classical
		  mathematics, differential equations and integrals, and so
		  on; our models are based on mathematical logic, set theory,
		  and algebra [1].Where physicists thus classically use a
		  variety of differential and integral calculi to model the
		  physical world, we shall be using the analysis and
		  description calculi presented in this article to model
		  primarily artifactual domains.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= mar,
  articleno	= {8},
  numpages	= {67},
  keywords	= {Domain engineering, domain analysis and description
		  calculi, transcendental deduction}
}

@InProceedings{	  10.1145/3750022.3750459,
  author	= {Zhai, Rundi and Liu, Jianmin and Miao, Yukai and Chen, Li
		  and Li, Dan and Cui, Baojiang and Zhang, Peng and Zhai,
		  Ennan and Ding, Zishuo},
  title		= {ConfSum: Towards Automatic Summarization of Network-scale
		  Operational Intents from Device Configurations},
  year		= {2025},
  isbn		= {9798400721038},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3750022.3750459},
  doi		= {10.1145/3750022.3750459},
  abstract	= {When network operators need to understand the high-level
		  intent behind a network's existing device configurations,
		  they must engage in a tedious and error-prone process of
		  manually reverse-engineering the low-level commands. We
		  propose Configuration Intent Summarization (CIS), a new
		  task that aims to automate this process by generating
		  human-readable summaries of the intents embedded across a
		  network's configurations. CIS is challenging due to the
		  diversity of intents, the semantic gap between
		  device-specific configurations and network-wide intents,
		  and the need to reason about interactions between multiple
		  devices' configurations. We present ConfSum, a system that
		  addresses these challenges by leveraging the unique ability
		  of large language models (LLMs) to parse semi-structured
		  configuration files and summarize them in natural language.
		  However, the full CIS task requires reasoning about device
		  interactions and other complexities that are beyond the
		  capabilities of LLMs alone. To enhance the LLM's robustness
		  to these challenges, ConfSum introduces novel techniques
		  for retrieving relevant examples to augment LLM prompts,
		  decomposing the generation process to handle multi-device
		  intents, and integrating with formal validation tools. Our
		  experiments demonstrate that Conf-Sum achieves high intent
		  coverage while generating summaries that match the quality
		  of human experts.},
  booktitle	= {Proceedings of the 2nd Workshop on Formal Methods Aided
		  Network Operation},
  pages		= {19–24},
  numpages	= {6},
  keywords	= {Formal Methods, Large Language Models, Network
		  management},
  location	= {Coimbra, Portugal},
  series	= {FMANO '25}
}

@InProceedings{	  10.1145/3308560.3316602,
  author	= {Stein, Daniel and Shterionov, Dimitar and Way, Andy},
  title		= {Towards language-agnostic alignment of product titles and
		  descriptions: a neural approach},
  year		= {2019},
  isbn		= {9781450366755},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308560.3316602},
  doi		= {10.1145/3308560.3316602},
  abstract	= {The quality of e-Commerce services largely depends on the
		  accessibility of product content as well as its
		  completeness and correctness. Nowadays, many sellers target
		  cross-country and cross-lingual markets via active or
		  passive cross-border trade, fostering the desire for
		  seamless user experiences. While machine translation (MT)
		  is very helpful for crossing language barriers,
		  automatically matching existing items for sale (e.g. the
		  smartphone in front of me) to the same product (all
		  smartphones of the same brand/type/colour/condition) can be
		  challenging, especially because the seller’s description
		  can often be erroneous or incomplete. This task we refer to
		  as item alignment in multilingual e-commerce catalogues. To
		  facilitate this task, we develop a pipeline of tools for
		  item classification based on cross-lingual text similarity,
		  exploiting recurrent neural networks (RNNs) with and
		  without pre-trained word-embeddings. Furthermore, we
		  combine our language agnostic RNN classifiers with an
		  in-domain MT system to further reduce the linguistic and
		  stylistic differences between the investigated data, aiming
		  to boost our performance. The quality of the methods as
		  well as their training speed is compared on an in-domain
		  data set for English–German products.},
  booktitle	= {Companion Proceedings of The 2019 World Wide Web
		  Conference},
  pages		= {387–392},
  numpages	= {6},
  location	= {San Francisco, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3459637.3482466,
  author	= {Cima, Gianluca and Croce, Federico and Lenzerini,
		  Maurizio},
  title		= {Query Definability and Its Approximations in
		  Ontology-based Data Management},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482466},
  doi		= {10.1145/3459637.3482466},
  abstract	= {Given an input dataset (i.e., a set of tuples), query
		  definability in Ontology-based Data Management (OBDM)
		  amounts to finding a query over the ontology whose certain
		  answers coincide with the tuples in the given dataset. We
		  refer to such a query as a characterization of the dataset
		  with respect to the OBDM system. Our first contribution is
		  to propose approximations of perfect characterizations in
		  terms of recall (complete characterizations) and precision
		  (sound characterizations). A second contribution is to
		  present a thorough complexity analysis of three
		  computational problems, namely verification (check whether
		  a given query is a perfect, or an approximated
		  characterization of a given dataset), existence (check
		  whether a perfect, or a best approximated characterization
		  of a given dataset exists), and computation (compute a
		  perfect, or best approximated characterization of a given
		  dataset).},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {271–280},
  numpages	= {10},
  keywords	= {ontology based data management, semantic technologies},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3555776.3577795,
  author	= {Engelberg, Gal and Fumagalli, Mattia and Kuboszek, Adrian
		  and Klein, Dan and Soffer, Pnina and Guizzardi, Giancarlo},
  title		= {An Ontology-Driven Approach for Process-Aware Risk
		  Propagation},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3577795},
  doi		= {10.1145/3555776.3577795},
  abstract	= {Risk Propagation (RP) is a central technique that allows
		  the calculation of the cascading effect of risk within a
		  system. At the current state, there is a lack of risk
		  propagation solutions that can be used to assess the impact
		  of risk at different levels of abstraction, accounting for
		  actors, processes, physical-digital objects, and their
		  relations. To fill this gap, in this paper, we propose a
		  process-aware risk propagation approach that builds on two
		  main components: i. an ontology, which supports
		  functionalities typical of Semantic Web technologies (SWT),
		  and ii. an ad hoc method to calculate the propagation of
		  risk within the given system. We implemented our approach
		  in a proof-of-concept tool, which was validated in the
		  cybersecurity domain.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1742–1745},
  numpages	= {4},
  keywords	= {risk propagation, risk analytics, ontology-driven risk
		  propagation},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@Article{	  10.1145/3746626.3746627,
  author	= {Zampierin, Luca and Frasincar, Flavius},
  title		= {An Unsupervised Approach Based on Attentional Neural
		  Models for Aspect-Based Sentiment Classification},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {25},
  number	= {2},
  issn		= {1559-6915},
  url		= {https://doi.org/10.1145/3746626.3746627},
  doi		= {10.1145/3746626.3746627},
  abstract	= {Due to the vast amount of reviews available on the Web, in
		  the past decades, a growing share of work has focused on
		  sentiment analysis. Aspect-based sentiment classification
		  is the subtask that seeks to detect the sentiment expressed
		  by the content creators towards a defined target within a
		  sentence. This paper introduces three novel unsupervised
		  attentional neural network models for aspect-based
		  sentiment classification, and tests them on English
		  restaurant reviews. The first model employs an
		  autoencoder-like structure to learn a sentiment embedding
		  matrix where each row of the matrix represents the
		  embedding for one sentiment. To improve the model, a
		  target-based attention mechanism is included that
		  de-emphasizes irrelevant words. Last, a redundancy and a
		  seed regularization term constrain the sentiment embedding
		  matrix. The second model extends the first by including a
		  Bi-LSTM layer in the attention mechanism to exploit
		  contextual information. The third model further adapts a
		  Left-Center-Right separated neural network with Rotatory
		  attention structure from the supervised realm to an
		  unsupervised setting. Although all three models construct
		  meaningful sentiment embeddings, experimental results
		  indicate that the inclusion of the Bi-LSTM in the attention
		  mechanism leads to a more precise attention mechanism and,
		  thus, better predictions. The best model, i.e., the second,
		  outperforms all investigated unsupervised and weakly
		  supervised algorithms for aspect-based sentiment
		  classification from the literature.},
  journal	= {SIGAPP Appl. Comput. Rev.},
  month		= jul,
  pages		= {5–17},
  numpages	= {13},
  keywords	= {attentional neural model, unsupervised learning,
		  aspect-based sentiment classification}
}

@InProceedings{	  10.5555/3716662.3716801,
  author	= {Wolfe, Robert and Mitra, Tanushree},
  title		= {The Implications of Open Generative Models in
		  Human-Centered Data Science Work: A Case Study with
		  Fact-Checking Organizations},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {Calls to use open generative language models in academic
		  research have highlighted the need for reproducibility and
		  transparency in scientific research. However, the impact of
		  generative AI extends well beyond academia, as corporations
		  and public interest organizations have begun integrating
		  these models into their data science pipelines. We expand
		  this lens to include the impact of open models on
		  organizations, focusing specifically on fact-checking
		  organizations, which use AI to observe and analyze large
		  volumes of circulating misinformation, yet must also ensure
		  the reproducibility and impartiality of their work. We
		  wanted to understand where fact-checking organizations use
		  open models in their data science pipelines; what motivates
		  their use of open models or proprietary models; and how
		  their use of open or proprietary models can inform research
		  on the societal impact of generative AI. To answer these
		  questions, we conducted an interview study with N=24
		  professionals at 20 fact-checking organizations on six
		  continents. Based on these interviews, we offer a
		  five-component conceptual model of where fact-checking
		  organizations employ generative AI to support or automate
		  parts of their data science pipeline, including Data
		  Ingestion, Data Analysis, Data Retrieval, Data Delivery,
		  and Data Sharing. We then provide taxonomies of
		  fact-checking organizations' motivations for using open
		  models and the limitations that prevent them for further
		  adopting open models, finding that they prefer open models
		  for Organizational Autonomy, Data Privacy and Ownership,
		  Application Specificity, and Capability Transparency.
		  However, they nonetheless use proprietary models due to
		  perceived advantages in Performance, Usability, and Safety,
		  as well as Opportunity Costs related to participation in
		  emerging generative AI ecosystems. Finally, we propose a
		  research agenda to address limitations of both open and
		  proprietary models. Our research provides novel perspective
		  on open models in data-driven organizations.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {1595–1607},
  numpages	= {13},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@InProceedings{	  10.1145/3736229.3736262,
  author	= {Roper, Bernard and Packer, Heather},
  title		= {PROVPub: A Publication Model to Support Research
		  Evaluation and Acknowledgement},
  year		= {2025},
  isbn		= {9798400719417},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3736229.3736262},
  doi		= {10.1145/3736229.3736262},
  abstract	= {The Research Excellence Framework (REF) is vital for
		  assessing the quality and impact of UK academic research,
		  requiring a transparent provenance trail for research
		  outputs. A major challenge is linking publications to their
		  underlying research activities. This paper introduces
		  PROVPub, a provenance model that automates and documents
		  the publication lifecycle, improving traceability and
		  compliance with REF requirements. PROVPub extends existing
		  frameworks like Git2PROV&nbsp;[5] by integrating metadata
		  from institutional repositories, publisher databases, and
		  Crossref. It captures key stages—submission, peer review,
		  acceptance, and publication—to create a structured,
		  verifiable record. This enhances efficiency, reduces
		  administrative burdens, and strengthens research impact
		  assessment. Through case studies, we demonstrate
		  PROVPub’s role in REF compliance and automated research
		  tracking. Our findings indicate that PROVPub enhances
		  transparency and scalability, benefiting institutions
		  seeking to streamline REF submissions and improve research
		  evaluation.},
  booktitle	= {Proceedings of the ProvenanceWeek 2025},
  pages		= {11–17},
  numpages	= {7},
  keywords	= {Provenance, Research Evaluation, Impact Assessment, REF,
		  PROV},
  location	= { },
  series	= {PW' 25}
}

@InProceedings{	  10.1145/3318464.3386145,
  author	= {Liu, Bang and Guo, Weidong and Niu, Di and Luo, Jinwen and
		  Wang, Chaoyue and Wen, Zhen and Xu, Yu},
  title		= {GIANT: Scalable Creation of a Web-scale Ontology},
  year		= {2020},
  isbn		= {9781450367356},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318464.3386145},
  doi		= {10.1145/3318464.3386145},
  abstract	= {Understanding what online users may pay attention to on
		  the web is key to content recommendation and search
		  services. These services will benefit from a highly
		  structured and web-scale ontology of entities, concepts,
		  events, topics and categories. While existing knowledge
		  bases and taxonomies embody a large volume of entities and
		  categories, we argue that they fail to discover properly
		  grained concepts, events and topics in the language style
		  of online users. Neither is a logically structured ontology
		  maintained among these notions. In this paper, we present
		  GIANT, a mechanism to construct a user-centered, web-scale,
		  structured ontology, containing a large number of natural
		  language phrases conforming to user attentions at various
		  granularities, mined from the vast volume of web documents
		  and search click logs. Various types of edges are also
		  constructed to maintain a hierarchy in the ontology. We
		  present our detailed techniques used in GIANT, and evaluate
		  the proposed models and methods as compared to a variety of
		  baselines, as well as deploy the resulted Attention
		  Ontology in real-world applications, involving over a
		  billion users, to observe its effect on content
		  recommendation. The online performance of the ontology
		  built by GIANT proves that it can significantly improve the
		  click-through rate in news feeds recommendation.},
  booktitle	= {Proceedings of the 2020 ACM SIGMOD International
		  Conference on Management of Data},
  pages		= {393–409},
  numpages	= {17},
  keywords	= {concept mining, document understanding, event mining,
		  ontology creation, user interest modeling},
  location	= {Portland, OR, USA},
  series	= {SIGMOD '20}
}

@Article{	  10.1109/taslp.2024.3485547,
  author	= {Ma, Jun-Yu and Gu, Jia-Chen and Ling, Zhen-Hua and Liu,
		  Quan and Liu, Cong and Hu, Guoping},
  title		= {Syntax-Augmented Hierarchical Interactive Encoder for
		  Zero-Shot Cross-Lingual Information Extraction},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3485547},
  doi		= {10.1109/TASLP.2024.3485547},
  abstract	= {Zero-shot cross-lingual information extraction (IE) aims
		  at constructing an IE model for some low-resource target
		  languages, given annotations exclusively in some
		  rich-resource languages. Recent studies have shown
		  language-universal features can bridge the gap between
		  languages. However, prior work has neither explored the
		  potential of establishing interactions between
		  language-universal features and contextual representations
		  nor incorporated features that can effectively model
		  constituent span attributes and relationships between
		  multiple spans. In this study, a
		  &lt;bold&gt;s&lt;/bold&gt;yntax-augmented
		  &lt;bold&gt;h&lt;/bold&gt;ierarchical
		  &lt;bold&gt;in&lt;/bold&gt;teractive
		  &lt;bold&gt;e&lt;/bold&gt;ncoder (SHINE) is proposed to
		  transfer cross-lingual IE knowledge. The proposed encoder
		  is capable of interactively capturing complementary
		  information between features and contextual information, to
		  derive language-agnostic representations for various
		  cross-lingual IE tasks. Concretely, a multi-level
		  interaction network is designed to hierarchically interact
		  the complementary information to strengthen domain
		  adaptability. Besides, in addition to the well-studied
		  word-level syntax features of part-of-speech and dependency
		  relation, a new span-level syntax feature of constituency
		  structure is introduced to model the constituent span
		  information which is crucial for IE. Experiments across
		  seven languages on three IE tasks and four benchmarks
		  verify the effectiveness and generalization ability of the
		  proposed method.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= oct,
  pages		= {4795–4809},
  numpages	= {15}
}

@InProceedings{	  10.1145/3616131.3616143,
  author	= {Zhu, Li and Qi, Xiangtao},
  title		= {Research on The Construction of Ontology-based Music Works
		  Knowledge Base},
  year		= {2023},
  isbn		= {9798400707339},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616131.3616143},
  doi		= {10.1145/3616131.3616143},
  abstract	= {The article formally explores the construction and
		  reasoning of music domain ontology based on ontological
		  methodology, proposes an improved ontology construction
		  method, defines an ontology knowledge representation model,
		  builds up an ontology of music works and its inference
		  rules, achieves ontology-driven knowledge representation,
		  storage, query and reasoning of music works, and makes some
		  quest research for the construction and application of
		  domain ontology.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Cloud and Big Data Computing},
  pages		= {81–88},
  numpages	= {8},
  location	= {Manchester, United Kingdom},
  series	= {ICCBDC '23}
}

@InProceedings{	  10.1145/3535511.3535555,
  author	= {Monteiro, Lucas Henrique de Assis and Rodrigues, Cleyton
		  M\'{a}rio de Oliveira and Sousa, A\^{e}da Monalliza Cunha
		  de},
  title		= {An Information System for Law Integrating Ontological
		  Bases with a Legal Reasoner Chatbot},
  year		= {2022},
  isbn		= {9781450396981},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3535511.3535555},
  doi		= {10.1145/3535511.3535555},
  abstract	= {Context: The Semantic Web aims to assign meanings to
		  resources available on the internet so that humans and
		  computers can understand them. It can be used in the most
		  diverse contexts, facilitating the development of systems
		  where expert knowledge is formalized through
		  logical-mathematical resources, mitigating potential
		  inconsistencies, and promoting more human-friendly
		  interaction services. Problem: The existence of semantic
		  anomalies (use of rhetorical language, polysemy and
		  inaccuracies) in the Brazilian Legal Domain enables the use
		  of Semantic Web standards and technologies to mitigate
		  these problems. Solution: This work deals with the
		  development of an Information System that uses resources
		  from the Semantic Web for the formal representation and the
		  realization of legal inferences about Crimes Against
		  Property. SI Theory: The Behavioral Decision Theory was
		  approached, mainly in the incorporation of real patterns of
		  decision making. Method: Bibliographic and documentary
		  research methods were used to list the main concepts
		  related to the Criminal Types investigated. The research is
		  prescriptive and has a quali-quantitative approach. Summary
		  of Results: A prototype system is presented, integrating
		  ontologies of Brazilian Law with a chatbot that enables
		  interaction with users in natural language, as well as
		  performing reasoning tasks based on the knowledge
		  formalized in these ontologies. Contributions and Impact in
		  the IS area: The research will contribute to the automation
		  of decision-making processes involving crimes against
		  property, serving as an aid for professionals or law
		  students and for legal simulations by ordinary people.
		  Furthermore, it will serve as a reference for the
		  development of other information systems with similar
		  objectives in other contexts.},
  booktitle	= {Proceedings of the XVIII Brazilian Symposium on
		  Information Systems},
  articleno	= {44},
  numpages	= {8},
  keywords	= {Chatbot, Law, Ontology},
  location	= {Curitiba, Brazil},
  series	= {SBSI '22}
}

@InProceedings{	  10.1145/3543507.3583533,
  author	= {Song, Miao-Hui and Zhang, Lan and Yuan, Mu and Li, Zichong
		  and Song, Qi and Liu, Yijun and Zheng, Guidong},
  title		= {CoTel: Ontology-Neural Co-Enhanced Text Labeling},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583533},
  doi		= {10.1145/3543507.3583533},
  abstract	= {The success of many web services relies on the large-scale
		  domain-specific high-quality labeled dataset. Insufficient
		  public datasets motivate us to reduce the cost of data
		  labeling while maintaining high accuracy in support of
		  intelligent web applications. The rule-based method and the
		  learning-based method are common techniques for labeling.
		  In this work, we study how to utilize the rule-based and
		  learning-based methods for resource-effective text
		  labeling. We propose CoTel, the first ontology-neural
		  co-enhanced framework for text labeling. We propose
		  critical ontology extraction in the rule-based module and
		  ontology-enhanced loss prediction in the learning-based
		  module. CoTel can integrate explicit labeling rules and
		  implicit labeling models and make them help each other to
		  improve resource efficiency in text labeling tasks. We
		  evaluate CoTel on both public datasets and real
		  applications with three different tasks. Compared with the
		  baseline, CoTel can reduce the time cost by 64.75\% (a
		  2.84\texttimes{} speedup) and the number of labeling by
		  62.07\%.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1897–1906},
  numpages	= {10},
  keywords	= {active learning, knowledge enhancement, pseudo labeling,
		  text labeling},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@Article{	  10.1145/3427201,
  author	= {Swalens, Janwillem and Koster, Joeri De and Meuter,
		  Wolfgang De},
  title		= {Chocola: Composable Concurrency Language},
  year		= {2021},
  issue_date	= {December 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {4},
  issn		= {0164-0925},
  url		= {https://doi.org/10.1145/3427201},
  doi		= {10.1145/3427201},
  abstract	= {Programmers often combine different concurrency models in
		  a single program, in each part of the program using the
		  model that fits best. Many programming languages, such as
		  Clojure, Scala, and Java, cater to this need by supporting
		  different concurrency models. However, existing programming
		  languages often combine concurrency models in an ad hoc
		  way, and the semantics of the combinations are not always
		  well defined.This article studies the combination of three
		  concurrency models: futures, transactions, and actors. We
		  show that a naive combination of these models invalidates
		  the guarantees they normally provide, thereby breaking the
		  assumptions of programmers. Hence, we present Chocola: a
		  unified language of futures, transactions, and actors that
		  maintains the guarantees of all three models wherever
		  possible, even when they are combined.We describe and
		  formalize the semantics of this language and prove the
		  guarantees it provides. We also provide an implementation
		  as an extension of Clojure and demonstrated that it can
		  improve the performance of three benchmark applications for
		  relatively little effort from the developer.},
  journal	= {ACM Trans. Program. Lang. Syst.},
  month		= jan,
  articleno	= {17},
  numpages	= {56},
  keywords	= {Futures, actor model, software transactional memory}
}

@InProceedings{	  10.1145/3444757.3485111,
  author	= {Drissi, Amani and Khemiri, Ahmed and Sassi, Salma and
		  Chbeir, Richard},
  title		= {A New Automatic Ontology Construction Method Based on
		  Machine Learning Techniques: Application on financial
		  corpus},
  year		= {2021},
  isbn		= {9781450383141},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3444757.3485111},
  doi		= {10.1145/3444757.3485111},
  abstract	= {Ontology Learning is a process of (semi)automatically
		  creating, maintaining, and transferring various forms of
		  information into an ontology with minimum human
		  intervention to guarantee a better knowledge representation
		  and sharing. In recent years, the research on automating
		  financial data modeling has become a hot topic among
		  researchers because of the exponential increase of the
		  number of financial documents and the heterogeneous of
		  financial data [19, 20]. So, we highlight the emergence of
		  new computational tools and methods to deal with the
		  automatic modeling and exploration of large financial
		  corpus. That's why, we propose here a solution named
		  Norms2Onto which is a semi-automatic ontology construction
		  method based on machine learning algorithms to facilitate
		  the reading and ease updating of financial data and to
		  guarantee their understanding. Experiments have been
		  conducted to measure the effectiveness of our solution
		  compared to a manual classification made by an domain
		  expert. The results show the superiority of our approach.},
  booktitle	= {Proceedings of the 13th International Conference on
		  Management of Digital EcoSystems},
  pages		= {57–61},
  numpages	= {5},
  keywords	= {IFRS Accounting Standards, Machine Learning, Ontology
		  Construction, Ontology Learning},
  location	= {Virtual Event, Tunisia},
  series	= {MEDES '21}
}

@InProceedings{	  10.1145/3172871.3172873,
  author	= {Prasad, Gollapudi VRJ Sai and Chimalakonda, Sridhar and
		  Choppella, Venkatesh},
  title		= {Towards a Domain-Specific Language for the Renarration of
		  Web Pages},
  year		= {2018},
  isbn		= {9781450363983},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3172871.3172873},
  doi		= {10.1145/3172871.3172873},
  abstract	= {We are interested in the problem of enabling
		  transformation of existing, already published web pages. We
		  call this Renarration of web content. In our earlier work,
		  we had already established the role and importance of
		  renarration for improving Web Accessibility. There are
		  nearly a billion websites on the web, making transformation
		  of pages a domain on its own. In this paper, we present the
		  development of a Domain-Specific Language (DSL) for the
		  purpose of web page transformation. We show how the design
		  and implementation of our DSL is driven by our problem
		  domain, its terminology and its unique requirements. We
		  take up an existing online video-course delivery system,
		  which has accessibility challenges, as a specific case to
		  demonstrate our DSL. We end with insights and reflections
		  for future work in both DSL and web page transformations.},
  booktitle	= {Proceedings of the 11th Innovations in Software
		  Engineering Conference},
  articleno	= {3},
  numpages	= {10},
  keywords	= {Domain Specific Language (DSL), Renarration, Web Page
		  Transformation},
  location	= {Hyderabad, India},
  series	= {ISEC '18}
}

@InProceedings{	  10.1145/3543712.3543739,
  author	= {Ouchaou, Linda and Nacer, Hassina and Charoy, Francois and
		  Youcef, Samir},
  title		= {Ontology-Based Cognitive Service Discovery \&amp;
		  Composition},
  year		= {2022},
  isbn		= {9781450396226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543712.3543739},
  doi		= {10.1145/3543712.3543739},
  abstract	= {Cloud cognitive computing has received a lot of attention
		  lately especially for tackling real-world problems such as
		  vision, natural language processing, fraud detection,
		  sentiment analysis and speech recognition. This paradigm is
		  based on cloud serverless computing and it provides machine
		  learning based functions to end users. Part of the appeal
		  in adopting this paradigm is its simplicity and the future
		  promises a fast-growing serverless-native ecosystem in
		  which service discovery and composition methods must be
		  provided. However, serverless platforms still lack
		  automated searching methods and the research community’s
		  attention regarding this issue has been limited. In this
		  paper, we propose an ontology-based approach for
		  discovering and composing cognitive functions in serverless
		  platforms in order to automate the searching process and
		  semantically answer users’ requirements. We also carried
		  a set of experiments to verify the correctness and the
		  feasibility of our approach and discuss the influence of
		  cognitive services nature on the outcomes of the discovery
		  and composition approach.},
  booktitle	= {Proceedings of the 2022 8th International Conference on
		  Computer Technology Applications},
  pages		= {154–162},
  numpages	= {9},
  keywords	= {Cognitive services, Composition, Discovery, FaaS,
		  Ontology., Serverless computing},
  location	= {Vienna, Austria},
  series	= {ICCTA '22}
}

@Article{	  10.1145/3594727,
  author	= {Candela, Gustavo and Pereda, Javier and S\'{a}ez, Dolores
		  and Escobar, Pilar and S\'{a}nchez, Alexander and Torres,
		  Andr\'{e}s Villa and Palacios, Albert A. and McDonough,
		  Kelly and Murrieta-Flores, Patricia},
  title		= {An Ontological Approach for Unlocking the Colonial
		  Archive},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3594727},
  doi		= {10.1145/3594727},
  abstract	= {Cultural Heritage institutions have been exploring new
		  ways of making available their catalogues in digital
		  format. Recently, new approaches have emerged as methods to
		  reuse and make available the contents for computational
		  purposes. This work introduces a methodology to transform
		  digital collections into Linked Open Data following best
		  practices. The framework has been applied to Indigenous and
		  Spanish colonial archives based on the collection
		  Relaciones Geogr\'{a}ficas of Mexico and Guatemala provided
		  by the LLILAS Benson Latin American Studies and
		  Collections. The results of this work are publicly
		  available. This work aims at encouraging Cultural Heritage
		  institutions to publish and reuse their digital collections
		  using advanced methods and techniques.},
  journal	= {J. Comput. Cult. Herit.},
  month		= nov,
  articleno	= {74},
  numpages	= {18},
  keywords	= {Linked Open Data, metadata, collections as data, knowledge
		  graph}
}

@Article{	  10.1145/3653977,
  author	= {Moraitou, Efthymia and Christodoulou, Yannis and Kotis,
		  Konstantinos and Caridakis, George},
  title		= {An Ontology-Based Framework for Supporting Decision-Making
		  in Conservation and Restoration Interventions for Cultural
		  Heritage},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3653977},
  doi		= {10.1145/3653977},
  abstract	= {Decision-making (DM) is the backbone of the Conservation
		  and Restoration (CnR) of Cultural Heritage (CH). The
		  demands of the DM process for information organization and
		  management have raised issues that the CnR community
		  attempts to solve by creating DM-support tools and systems,
		  which, among others, exploit Semantic Web (SW)
		  technologies. Regarding the tools and systems that focus on
		  the DM process of selecting an intervention option
		  (CnR-DM-I), they present benefits, as well as limitations,
		  regarding the (1) completeness of representation of the
		  relevant knowledge in a unified manner, (2) facilitation of
		  recording the CnR-DM-I process per se, in terms of the
		  problem at hand as well as the intervention parameters,
		  requirements, and criteria, and (3) recommendation and
		  further exploration of CnR intervention options in a
		  systematic manner. This work proposes an ontology-based
		  framework as a means to overcome those limitations. The
		  proposed framework (DS-CnRI) sets at its core a formal
		  ontology which provides the necessary entities to represent
		  expert knowledge related to CnR-DM-I. The ontology also
		  includes rules which provide useful inferences to assist
		  the CnR-DM-I process. The proposed framework has been
		  deployed and evaluated in collaboration with conservators.
		  Initial evaluation results show that the framework assists
		  conservators in CnR-DM-I to detect and select the most
		  suitable intervention options, to better understand the
		  limitations of different options, and to document the
		  process of reaching their decision.},
  journal	= {J. Comput. Cult. Herit.},
  month		= may,
  articleno	= {41},
  numpages	= {24},
  keywords	= {Decision-support services, semantic Web technologies,
		  knowledge representation, conservation, restoration,
		  cultural heritage}
}

@InProceedings{	  10.1145/3627043.3659564,
  author	= {Kitto, Kirsty},
  title		= {Will a Skills Passport ever get me through the lifelong
		  learning border?: Two critical challenges facing
		  personalised user models for lifelong learning},
  year		= {2024},
  isbn		= {9798400704338},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627043.3659564},
  doi		= {10.1145/3627043.3659564},
  abstract	= {Lifelong personalised learning is often described as the
		  holy grail of the educational data sciences, but work on
		  the topic is sporadic and we are yet to achieve this goal
		  in a meaningful form. In the wake of the skills shortages
		  arising from national responses to COVID-19 this problem
		  has again become a topic of interest. A number of proposals
		  have emerged that some sort of a skills passport would help
		  individuals, educational institutions, and employers to
		  identify training and recruitment needs according to
		  identified skills gaps. And yet, we are a long way from
		  achieving a skills passport that could support lifelong
		  learning despite more than 25 years of work on the topic.
		  This paper draws attention to two of the critical
		  socio-technical challenges facing skills passports, and
		  lifelong learner models in general. This leads to a
		  proposal for how we might move towards a useful skills
		  passport that can cross the “skills sector border”.},
  booktitle	= {Proceedings of the 32nd ACM Conference on User Modeling,
		  Adaptation and Personalization},
  pages		= {132–142},
  numpages	= {11},
  keywords	= {contextualisation, data portability, lifelong learning,
		  personal user model, skills},
  location	= {Cagliari, Italy},
  series	= {UMAP '24}
}

@Proceedings{	  10.1145/3663338,
  title		= {ApPLIED'24: Proceedings of the 2024 Workshop on Advanced
		  Tools, Programming Languages, and PLatforms for
		  Implementing and Evaluating algorithms for Distributed
		  systems},
  year		= {2024},
  isbn		= {9798400706707},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {ApPLIED aims to bring together distributed system
		  designers and practitioners from academia and industry to
		  share their experiences and perspectives in designing and
		  building distributed systems.},
  location	= {Nantes, France}
}

@Article{	  10.1145/3691642,
  author	= {Argotti, Yann and Kenfaoui, Yasmine and Baron, Claude and
		  Abran, Alain and Esteban, Philippe},
  title		= {An Operational Quality Model of Embedded Software Aligned
		  with ISO 25000},
  year		= {2024},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {1},
  issn		= {1539-9087},
  url		= {https://doi.org/10.1145/3691642},
  doi		= {10.1145/3691642},
  abstract	= {Embedded systems omnipresent in everyday life and industry
		  are mainly composed of hardware and software that must
		  comply with a number of standards and regulations. However,
		  there is no consensus on the quality characteristics and
		  subcharacteristics of embedded software. This article
		  presents the steps for modeling an operational quality
		  model for embedded software aligned with the ISO 25000
		  series of quality models for traditional computer systems.
		  From a literature review composed of 40 studies on quality
		  modeling for embedded systems and software, 85 of the most
		  frequent quality characteristics and subcharacteristics
		  were first identified, including a subset of 16 referenced
		  or cited in at least 25\% of the literature. Next, the
		  design of a quality model for embedded software aligned
		  with the ISO 25000 series was proposed with 13
		  characteristics and 27 subcharacteristics. The operational
		  aspect of this quality model for embedded software is
		  addressed next through a set of measures and measurement
		  functions from ISO 25000 to aggregate the results of the
		  quantification of the characteristics and
		  subcharacteristics. A survey involving 25 embedded software
		  specialists is presented next to gauge, using Fleiss's
		  Kappa criteria, their agreement with the proposed quality
		  model. Furthermore, the computed importance weights derived
		  from the survey participants’ individual opinions were
		  compared with those derived from an analysis of 40 embedded
		  software studies, bolstering the credibility of the model.
		  The results of this study suggest that the proposed quality
		  model can serve as a framework for evaluating and
		  understanding the quality characteristics across diverse
		  expertise levels. Furthermore, the convergence between the
		  survey and the literature strengthens the model's
		  credibility by anchoring it in both established literature
		  and practitioners’ agreements.},
  journal	= {ACM Trans. Embed. Comput. Syst.},
  month		= nov,
  articleno	= {9},
  numpages	= {41},
  keywords	= {Quality model, quality characteristics, measure, ISO,
		  qualimetry}
}

@Article{	  10.1109/tcbb.2018.2849968,
  author	= {Zeng, Zexian and Deng, Yu and Li, Xiaoyu and Naumann,
		  Tristan and Luo, Yuan},
  title		= {Natural Language Processing for EHR-Based Computational
		  Phenotyping},
  year		= {2019},
  issue_date	= {January 2019},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {16},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2018.2849968},
  doi		= {10.1109/TCBB.2018.2849968},
  abstract	= {This article reviews recent advances in applying natural
		  language processing NLP to Electronic Health Records EHRs
		  for computational phenotyping. NLP-based computational
		  phenotyping has numerous applications including diagnosis
		  categorization, novel phenotype discovery, clinical trial
		  screening, pharmacogenomics, drug-drug interaction DDI, and
		  adverse drug event ADE detection, as well as genome-wide
		  and phenome-wide association studies. Significant progress
		  has been made in algorithm development and resource
		  construction for computational phenotyping. Among the
		  surveyed methods, well-designed keyword search and
		  rule-based systems often achieve good performance. However,
		  the construction of keyword and rule lists requires
		  significant manual effort, which is difficult to scale.
		  Supervised machine learning models have been favored
		  because they are capable of acquiring both classification
		  patterns and structures from data. Recently, deep learning
		  and unsupervised learning have received growing attention,
		  with the former favored for its performance and the latter
		  for its ability to find novel phenotypes. Integrating
		  heterogeneous data sources have become increasingly
		  important and have shown promise in improving model
		  performance. Often, better performance is achieved by
		  combining multiple modalities of information. Despite these
		  many advances, challenges and opportunities remain for
		  NLP-based computational phenotyping, including better model
		  interpretability and generalizability, and proper
		  characterization of feature relations in clinical
		  narratives.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jan,
  pages		= {139–153},
  numpages	= {15}
}

@InProceedings{	  10.1145/3639592.3639610,
  author	= {Chawuthai, Rathachai and Kertkeidkachorn, Natthawut and
		  Racharak, Teeradaj},
  title		= {Modelling an RDF Knowledge Graph with Transitivity and
		  Symmetry for Bus Route Path Finding},
  year		= {2024},
  isbn		= {9798400716225},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639592.3639610},
  doi		= {10.1145/3639592.3639610},
  abstract	= {A key property of Linked Data is the representation and
		  publication of data as an inter-connected labelled graph
		  where different resources linked to each other form a
		  network of meaningful information. A problem of path
		  finding can be seen as searching important relationships
		  between resources, such as, looking for chains of
		  intermediate nodes. In this paper, we tackle this problem
		  in the context of public transport navigation system, where
		  we aim to find candidates of bus route path given two bus
		  stations. We model a novel lightweight bus network as
		  Resource Description Framework (RDF) triples of directed
		  bus lines and walking paths between connected stations.
		  Indeed, we demonstrate that lightweight bus network can be
		  achieved by exploiting the sub-property of RDF Schema
		  (RDFS) and the transitivity and symmetry provided by Web
		  Ontology Language (OWL). We also perform a scalability test
		  of our approach using a real-world bus network in Bangkok,
		  Thailand. Various patterns of SPARQL Protocol and RDF Query
		  Language (SPARQL) query statements are validated, showing
		  the usefulness of the RDF model. The further step of this
		  paper is to work with bus schedules and travel time
		  analysis in order to select some proper candidates for
		  users through an application.},
  booktitle	= {Proceedings of the 2023 6th Artificial Intelligence and
		  Cloud Computing Conference},
  pages		= {126–134},
  numpages	= {9},
  keywords	= {Graph Traversal, Knowledge Graph, Path Finding, Semantic
		  Web, Transport Navigation System},
  location	= {Kyoto, Japan},
  series	= {AICCC '23}
}

@InProceedings{	  10.1145/3669754.3669784,
  author	= {Jayawardena, Lasal and Yapa, Prasan},
  title		= {Improving Quality and Domain-Relevancy of Paraphrase
		  Generation with Graph-Based Retrieval Augmented
		  Generation},
  year		= {2024},
  isbn		= {9798400717055},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3669754.3669784},
  doi		= {10.1145/3669754.3669784},
  abstract	= {Paraphrase generation is a fundamental area of research in
		  Natural Language Processing (NLP) and Natural Language
		  Generation (NLG), due to its sequence-to-sequence (Seq2Seq)
		  nature. Paraphrasing, spanning across various domains,
		  poses challenges for simpler model architectures due to the
		  extensive knowledge required to generate paraphrases. The
		  added constraint of generating diverse paraphrases further
		  complicates the task for models trained on existing
		  datasets. We present a methodology that leverages
		  Graph-Based Retrieval Augmented Generation (G-RAG), capable
		  of utilizing both entity and phrasal knowledge to address
		  this issue. We demonstrate through experiments that this
		  approach enables both complex models like Large Language
		  models (LLMs) and smaller Seq2Seq models to generate more
		  diverse paraphrases without compromising semantic
		  similarity. Furthermore, this approach’s capacity to
		  integrate domain-specific knowledge makes it particularly
		  effective across different domains, enhancing its
		  applicability in varied contexts. The results are further
		  corroborated by human evaluation and extensive quantitative
		  analysis focusing on semantic similarity, lexical
		  diversity, syntactic diversity, and grammatical correctness
		  to gauge high-quality paraphrases.},
  booktitle	= {Proceedings of the 2024 10th International Conference on
		  Computing and Artificial Intelligence},
  pages		= {196–208},
  numpages	= {13},
  keywords	= {Graph-based Knowledge, Large Language Models, Natural
		  Language Processing, Paraphrase Generation,
		  Sequence-to-Sequence Models},
  location	= {Bali Island, Indonesia},
  series	= {ICCAI '24}
}

@InProceedings{	  10.1145/3631700.3653062,
  author	= {Purificato, Erasmo and Boratto, Ludovico and De Luca,
		  Ernesto William},
  title		= {Paradigm Shifts in User Modeling: A Journey from
		  Historical Foundations to Emerging Trends},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3653062},
  doi		= {10.1145/3631700.3653062},
  abstract	= {The presented tutorial aims to serve as a comprehensive
		  roadmap for the UMAP community into the current user
		  modeling research, focusing on the paradigm shifts that
		  have transformed the research landscape in recent times. We
		  will provide a complete overview of the large,
		  long-standing, and ever-growing research fields of user
		  modeling and user profiling, both from a historical and a
		  technical point of view. We will then examine the
		  definitions associated with each key term in this research
		  domain, aiming to eliminate ambiguity and confusion in
		  their usage. As the core of our tutorial, we present
		  in-depth the paradigm shifts that have occurred in recent
		  years, especially due to technological evolution, as well
		  as the current research directions and novel trends in the
		  field. In particular, we illustrate and discuss the
		  advances in the following topics: implicit and explicit
		  user profiling, user behavior modeling, user
		  representation, and beyond-accuracy perspectives. The
		  audience will be engaged in discussions during the whole
		  presentation to foster the development of an interactive
		  event. Detailed information and resources about the
		  tutorial are available on the website:
		  https://link.erasmopurif.com/tutorial-umap24.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {13–16},
  numpages	= {4},
  keywords	= {Paradigm Shifts, User Modeling, User Profiling},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@Article{	  10.14778/3229863.3236248,
  author	= {Jammi, Manasa and Sen, Jaydeep and Mittal, Ashish and
		  Verma, Sagar and Pahuja, Vardaan and Ananthanarayanan, Rema
		  and Lohia, Pranay and Karanam, Hima and Saha, Diptikalyan
		  and Sankaranarayanan, Karthik},
  title		= {Tooling framework for instantiating natural language
		  querying system},
  year		= {2018},
  issue_date	= {August 2018},
  publisher	= {VLDB Endowment},
  volume	= {11},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3229863.3236248},
  doi		= {10.14778/3229863.3236248},
  abstract	= {Recent times have seen a growing demand for natural
		  language querying (NLQ) interfaces to retrieve information
		  from the structured data sources such as knowledge bases.
		  Using this interface, business users can directly interact
		  with a database without the knowledge of the query language
		  or the data schema. Our earlier work describes a natural
		  language query engine called ATHENA which has several
		  shortcoming around ease of use and compatibility with data
		  stores, formats and flows. In this demonstration paper, we
		  present a tooling framework to address these challenges so
		  that one can instantiate an NLQ system with utmost ease.
		  Our framework makes it easy and practically applicable to
		  all NLIDB scenarios involving different sources of
		  structured data, file formats, and ontologies to enable
		  natural language querying on top of them with minimal human
		  configuration. We present the tool design and the solution
		  to the challenges towards building such a system and
		  demonstrate its applicability in the medical domain.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {2014–2017},
  numpages	= {4}
}

@InProceedings{	  10.1145/3733723.3742469,
  author	= {Cuzzocrea, Alfredo},
  title		= {Vector Databases for Modelling, Managing and Querying Big
		  Scientific Data: Models, Issues, Paradigms},
  year		= {2025},
  isbn		= {9798400714627},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3733723.3742469},
  doi		= {10.1145/3733723.3742469},
  abstract	= {Inspired by the emergence of scientific data in fields
		  like as astronomy, climate research, and genomics, which
		  presents significant challenges for conventional database
		  systems. This vision paper investigates the adaptation of
		  vector databases in order to describe, handle, and query
		  large-scale scientific data. Moreover, we propose a
		  road-map for embedding-centric data infrastructures, along
		  with an analysis of current advancements and identification
		  of important future research directions, including
		  explainability and scalability. This research provides a
		  foundational basis for next-generation interdisciplinary
		  scientific discovery.},
  booktitle	= {Proceedings of the 37th International Conference on
		  Scalable Scientific Data Management},
  articleno	= {24},
  numpages	= {5},
  keywords	= {Vector Databases, Advanced Scientific Data Representation,
		  Advanced Scientific Data Management, Big Scientific Data},
  location	= { },
  series	= {SSDBM '25}
}

@InProceedings{	  10.1145/3644523.3644610,
  author	= {Si, Tiange},
  title		= {A Hybrid Data-Knowledge-Driven Domain Ontology
		  Construction Method for China's Financial Domain},
  year		= {2024},
  isbn		= {9798400709517},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644523.3644610},
  doi		= {10.1145/3644523.3644610},
  abstract	= {The rapid increase of data in China's financial domain has
		  brought difficulties in data organization and management,
		  and the traditional flat organization of financial big data
		  ignores the rich knowledge association in the data, and the
		  study of knowledge association in the financial domain has
		  received more and more attention from academia. Financial
		  ontology can meet the demand of the financial industry for
		  data quality and program rigor, and reflect the structure
		  of the financial capital market and the associations
		  between different entities. This study investigates and
		  compares various ontology construction methods at home and
		  abroad, and proposes a data-knowledge hybrid-driven
		  approach to construct domain ontologies, which realizes the
		  collaboration among people, knowledge and data with the
		  core of "knowledge generation-data expansion-quality
		  control". Under the guidance of this method, this study
		  invited a research group of experts in the financial field
		  to successfully construct the first financial ontology
		  applicable to China's financial system by using financial
		  data provided by several financial and economic information
		  providers, such as Wind, CSMAR, CNRDS, etc., which provides
		  exploration and guidance for the development of
		  informatization in China's financial field, and on the
		  basis of which a more comprehensive knowledge graph can be
		  constructed in the future to further promote the
		  construction of information resources in China's financial
		  domain.},
  booktitle	= {Proceedings of the 2023 4th International Conference on
		  Computer Science and Management Technology},
  pages		= {480–486},
  numpages	= {7},
  location	= {Xi'an, China},
  series	= {ICCSMT '23}
}

@InProceedings{	  10.1145/3534678.3539453,
  author	= {Geng, Yuxia and Chen, Jiaoyan and Zhang, Wen and Xu,
		  Yajing and Chen, Zhuo and Z. Pan, Jeff and Huang, Yufeng
		  and Xiong, Feiyu and Chen, Huajun},
  title		= {Disentangled Ontology Embedding for Zero-shot Learning},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539453},
  doi		= {10.1145/3534678.3539453},
  abstract	= {Knowledge Graph (KG) and its variant of ontology have been
		  widely used for knowledge representation, and have shown to
		  be quite effective in augmenting Zero-shot Learning (ZSL).
		  However, existing ZSL methods that utilize KGs all neglect
		  the intrinsic complexity of inter-class relationships
		  represented in KGs. One typical feature is that a class is
		  often related to other classes in different semantic
		  aspects. In this paper, we focus on ontologies for
		  augmenting ZSL, and propose to learn disentangled ontology
		  embeddings guided by ontology properties to capture and
		  utilize more fine-grained class relationships in different
		  aspects. We also contribute a new ZSL framework named
		  DOZSL, which contains two new ZSL solutions based on
		  generative models and graph propagation models,
		  respectively, for effectively utilizing the disentangled
		  ontology embeddings. Extensive evaluations have been
		  conducted on five benchmarks across zero-shot image
		  classification (ZS-IMGC) and zero-shot KG completion
		  (ZS-KGC). DOZSL often achieves better performance than the
		  state-of-the-art, and its components have been verified by
		  ablation studies and case studies. Our codes and datasets
		  are available at https://github.com/zjukg/DOZSL.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {443–453},
  numpages	= {11},
  keywords	= {disentangled representation learning, knowledge graph,
		  ontology, zero-shot learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3428757.3429091,
  author	= {Cardinale, Yudith and Cornejo-Lupa, Mar\'{\i}a A. and
		  Ticona-Herrera, Regina and Barrios-Aranibar, Dennis},
  title		= {A Methodological Approach to Compare Ontologies: Proposal
		  and Application for SLAM Ontologies},
  year		= {2021},
  isbn		= {9781450389228},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3428757.3429091},
  doi		= {10.1145/3428757.3429091},
  abstract	= {Representation of the knowledge related to any domain with
		  flexible and well-defined models, such as ontologies,
		  provides the base to develop efficient and interoperable
		  solutions. Hence, a proliferation of ontologies in many
		  domains is unleashed. It is necessary to define how to
		  compare such ontologies to decide which one is the most
		  suitable for specific needs of users/developers. Since the
		  emerging developing of ontologies, several studies have
		  proposed criteria to evaluate them. Nevertheless, there is
		  still a lack of practical and reproducible guidelines to
		  drive a comparative evaluation of ontologies as a
		  systematic process. In this paper, we propose a
		  methodological process to qualitatively and quantitatively
		  compare ontologies at Lexical, Structural, and Domain
		  Knowledge levels, considering Correctness and Quality
		  perspectives. Since the evaluation methods of our proposal
		  are based in a golden-standard, it can be customized to
		  compare ontologies in any domain. To show the suitability
		  of our proposal, we apply our methodological approach to
		  conduct a comparative study of ontologies in the robotic
		  domain, in particularly for the Simultaneous Localization
		  and Mapping (SLAM) problem. With this study case, we
		  demonstrate that with this methodological comparative
		  process, we are able to identify the strengths and
		  weaknesses of ontologies, as well as the gaps still needed
		  to fill in the target domain (SLAM for our study case).},
  booktitle	= {Proceedings of the 22nd International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {223–233},
  numpages	= {11},
  keywords	= {Autonomous and Mobile Robots, Ontologies Evaluation,
		  Ontology, SLAM},
  location	= {Chiang Mai, Thailand},
  series	= {iiWAS '20}
}

@Article{	  10.1145/3277006.3277017,
  author	= {Deutch, Daniel and Frost, Nave and Gilad, Amir},
  title		= {Natural Language Explanations for Query Results},
  year		= {2018},
  issue_date	= {March 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {47},
  number	= {1},
  issn		= {0163-5808},
  url		= {https://doi.org/10.1145/3277006.3277017},
  doi		= {10.1145/3277006.3277017},
  abstract	= {Multiple lines of research have developed Natural Language
		  (NL) interfaces for formulating database queries. We build
		  upon this work, but focus on presenting a highly detailed
		  form of the answers in NL. The answers that we present are
		  importantly based on the provenance of tuples in the query
		  result, detailing not only the results but also their
		  explanations. We develop a novel method for transforming
		  provenance information to NL, by leveraging the original NL
		  query structure. Furthermore, since provenance information
		  is typically large and complex, we present two solutions
		  for its effective presentation as NL text: one that is
		  based on provenance factorization, with novel desiderata
		  relevant to the NL case, and one that is based on
		  summarization.},
  journal	= {SIGMOD Rec.},
  month		= sep,
  pages		= {42–49},
  numpages	= {8}
}

@InProceedings{	  10.1145/3695080.3695151,
  author	= {Jiang, Xinran and Zhang, Wenxue},
  title		= {"Construction of Conceptual Layer of Knowledge Graph of
		  State-target Differentiation \&amp; Treatment based on
		  Ontology and Prot\'{e}g\'{e}"},
  year		= {2024},
  isbn		= {9798400710223},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3695080.3695151},
  doi		= {10.1145/3695080.3695151},
  abstract	= {As an innovative strategy, "State-target Differentiation
		  \&amp; Treatment" (STDT) combines traditional Chinese
		  medicine and Western medicine, making it a prominent
		  development direction in the current Traditional Chinese
		  Medicine (TCM) diagnosis and treatment system. With the
		  deepening of research in this field, a significant number
		  of research achievements was coming out. However, massive
		  scattered data lack of integration and structuring.
		  Therefore, domain knowledge storage and relational
		  reasoning become pressing issues in the field of STDT. This
		  paper obtains raw data from literature retrieval database
		  and domain monograph of STDT. By analyzing the existing
		  semantic framework of TCM and the clinical characteristics
		  of STDT, we summarize the knowledge structure of STDT
		  theory in three aspects "class, data properties, and object
		  properties " in the form of table. Finally, the ontology of
		  STDT knowledge graph is constructed with Protege, a
		  commonly used ontology development tool. From the
		  therapeutic process of macro-control of physical state to
		  the knowledge of the micro-pharmacological level of TCM
		  targets, the visualized display of the STDT knowledge graph
		  realizes the combination of "state" and "target". The
		  utilization of ontology technology enables the blending,
		  recombination, and reasoning of the domain knowledge of
		  STDT, laying the foundation and offering reference for the
		  subsequent construction of the domain knowledge base.},
  booktitle	= {Proceedings of the 2024 International Conference on Cloud
		  Computing and Big Data},
  pages		= {414–422},
  numpages	= {9},
  location	= {Dali, China},
  series	= {ICCBD '24}
}

@InProceedings{	  10.1145/3672608.3707826,
  author	= {Jungmann, Michelle and Lazarova-Molnar, Sanja},
  title		= {Fusing Expert Knowledge and Internet of Things Data for
		  Digital Twin Models: Addressing Uncertainty in Expert
		  Statements},
  year		= {2025},
  isbn		= {9798400706295},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672608.3707826},
  doi		= {10.1145/3672608.3707826},
  abstract	= {Extracting Digital Twin models by fusing expert knowledge
		  with Internet of Things data remains a challenging and open
		  research area. Existing literature offers very limited
		  approaches for seamless and systematic extraction of
		  Digital Twin models from these combined sources. In this
		  paper, we address the research gap by proposing a novel
		  approach that considers and integrates the uncertainty
		  inherent in human expert knowledge into the extraction
		  processes of Digital Twin models. Given that experts
		  possess unique experiences, contextual understandings and
		  judgements, their knowledge can be highly divergent,
		  complex, ambiguous, and even incorrect or incomplete.
		  Consequently, not all expert knowledge statements should be
		  equally weighted in the resulting simulation models. Our
		  contributions include a comprehensive literature review on
		  the uncertainty in expert knowledge and the proposal of an
		  approach to integrate this uncertainty in the extraction of
		  Digital Twin models from fused expert knowledge and IoT
		  data. We demonstrate our approach through a case study in
		  reliability assessment.1},
  booktitle	= {Proceedings of the 40th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {874–881},
  numpages	= {8},
  keywords	= {ACM proceedings, digital twins, fusion of data and expert
		  knowledge, uncertainty in expert knowledge, industry 4.0},
  location	= {Catania International Airport, Catania, Italy},
  series	= {SAC '25}
}

@InProceedings{	  10.1145/3477314.3507301,
  author	= {Martin, Tomas and Fuentes, Victor and Valtchev, Petko and
		  Diallo, Abdoulaye Banir\'{e} and Lacroix, Ren\'{e}},
  title		= {Generalized graph pattern discovery in linked data with
		  data properties and a domain ontology},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3507301},
  doi		= {10.1145/3477314.3507301},
  abstract	= {Nowadays, in many practical situations, analytical tasks
		  need to be performed on complex heterogeneous data, often
		  described by a domain ontology (DO). Such cases abound in
		  life science fields such as agro-informatics, where
		  observations and measures on animals/plants are logged for
		  subsequent mining. The data is naturally structured as
		  graph(s), unlabelled and missing some values, hence it fits
		  well pattern mining. In our own precision farming project
		  aimed at decision support for dairy cow management, we mine
		  for knowledge in milk production data. In one task, we aim
		  at contrast patterns explaining the relative impact of
		  independent production factors. To that end,
		  ontologically-generalized graph patterns (OGPs), a variety
		  of generalized graph patterns, where vertices and edges are
		  labelled by DO classes and properties, respectively, were
		  defined. A mining methodology was also designed that
		  reconciles OWL DOs, abstraction from RDF graphs and
		  literals in data. To address the well-known cost-related
		  limitations of graph mining -exacerbated here by
		  class/property specializations and data properties- we
		  split the mining task into (1) mining of generic object
		  property topology patterns and (2) label refinement. Those
		  focus on two sorts of OGPs, called topologies and class
		  stars, respectively, which, after being mined separately,
		  get (3) assembled into fully-fledged OGPs.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1890–1899},
  numpages	= {10},
  keywords	= {generalized patterns, graph data, ontologies, pattern
		  mining},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@Article{	  10.1145/3148239,
  author	= {Bertossi, Leopoldo and Milani, Mostafa},
  title		= {Ontological Multidimensional Data Models and Contextual
		  Data Quality},
  year		= {2018},
  issue_date	= {September 2017},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {3},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3148239},
  doi		= {10.1145/3148239},
  abstract	= {Data quality assessment and data cleaning are
		  context-dependent activities. Motivated by this
		  observation, we propose the Ontological Multidimensional
		  Data Model (OMD model), which can be used to model and
		  represent contexts as logic-based ontologies. The data
		  under assessment are mapped into the context for additional
		  analysis, processing, and quality data extraction. The
		  resulting contexts allow for the representation of
		  dimensions, and multidimensional data quality assessment
		  becomes possible. At the core of a multidimensional
		  context, we include a generalized multidimensional data
		  model and a Datalog± ontology with provably good
		  properties in terms of query answering. These main
		  components are used to represent dimension hierarchies,
		  dimensional constraints, and dimensional rules and define
		  predicates for quality data specification. Query answering
		  relies on and triggers navigation through dimension
		  hierarchies and becomes the basic tool for the extraction
		  of quality data. The OMD model is interesting per se beyond
		  applications to data quality. It allows for a logic-based
		  and computationally tractable representation of
		  multidimensional data, extending previous multidimensional
		  data models with additional expressive power and
		  functionalities.},
  journal	= {J. Data and Information Quality},
  month		= jan,
  articleno	= {14},
  numpages	= {36},
  keywords	= {Datalog±, Ontology-based data access, query answering,
		  weakly-sticky programs}
}

@Article{	  10.1145/3674501,
  author	= {Zhao, Xiaoyan and Deng, Yang and Yang, Min and Wang,
		  Lingzhi and Zhang, Rui and Cheng, Hong and Lam, Wai and
		  Shen, Ying and Xu, Ruifeng},
  title		= {A Comprehensive Survey on Relation Extraction: Recent
		  Advances and New Frontiers},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {11},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3674501},
  doi		= {10.1145/3674501},
  abstract	= {Relation extraction (RE) involves identifying the
		  relations between entities from underlying content. RE
		  serves as the foundation for many natural language
		  processing (NLP) and information retrieval applications,
		  such as knowledge graph completion and question answering.
		  In recent years, deep neural networks have dominated the
		  field of RE and made noticeable progress. Subsequently, the
		  large pre-trained language models (PLMs) have taken the
		  state-of-the-art RE to a new level. This survey provides a
		  comprehensive review of existing deep learning techniques
		  for RE. First, we introduce RE resources, including
		  datasets and evaluation metrics. Second, we propose a new
		  taxonomy to categorize existing works from three
		  perspectives, i.e., text representation, context encoding,
		  and triplet prediction. Third, we discuss several important
		  challenges faced by RE and summarize potential techniques
		  to tackle these challenges. Finally, we outline some
		  promising future directions and prospects in this field.
		  This survey is expected to facilitate researchers’
		  collaborative efforts to address the challenges of
		  real-world RE systems.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {293},
  numpages	= {39},
  keywords	= {Relation extraction, deep learning, pre-trained language
		  models, low-resource relation extraction}
}

@InProceedings{	  10.1145/3701716.3715487,
  author	= {Mitra, Aniket and Venugopal, Vinu E.},
  title		= {Stat-n-Ball: Enhancing Probabilistic Knowledge Graph
		  Embeddings with Geometric and Confidence-Aware Models},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715487},
  doi		= {10.1145/3701716.3715487},
  abstract	= {Region-based Knowledge Graph Embedding (R-KGE) models,
		  which represent entities as convex shapes (e.g., balls) and
		  relations as geometric transformations in vector space,
		  offer a promising approach for explainable and accurate
		  reasoning over ontologies. However, existing R-KGE models
		  assume perfect reliability of Knowledge Graphs (KGs), which
		  is often unrealistic as real-world KGs are noisy and
		  incomplete. To address this, Probabilistic Knowledge Graphs
		  (P-KGs) associate axioms with confidence scores, capturing
		  the uncertainty of their truthfulness. We propose
		  Stat-n-Ball, a novel R-KGE framework that incorporates
		  confidence scores by representing axioms' certainty as
		  overlapping volumes between entities in vector space. Our
		  approach enhances the geometric representation of KGs,
		  enabling accurate link prediction and confidence estimation
		  in probabilistic settings. Experimental evaluations on
		  standard P-KG datasets demonstrate that Stat-n-Ball
		  achieves atleast a 2\texttimes{} improvement in entity
		  association detection and a minimum 5\% reduction in
		  confidence prediction error compared to state-of-the-art
		  models. These results underscore its effectiveness in
		  handling noisy and uncertain KGs while preserving logical
		  and semantic integrity.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {1194–1198},
  numpages	= {5},
  keywords	= {n-ball embedding, noisy knowledge graphs, probabilistic
		  knowledge graph embedding},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3236405.3236427,
  author	= {Li, Yang},
  title		= {Feature and variability extraction from natural language
		  software requirements specifications},
  year		= {2018},
  isbn		= {9781450359450},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3236405.3236427},
  doi		= {10.1145/3236405.3236427},
  abstract	= {Extracting feature and variability from requirement
		  specifications is an indispensable activity to support
		  systematic integration related single software systems into
		  Software Product Line (SPL). Performing variability
		  extraction is time-consuming and inefficient, since massive
		  textual requirements need to be analyzed and classified.
		  Despite the improvement of automatically features and
		  relationships extraction techniques, existing approaches
		  are not able to provide high accuracy and applicability in
		  real-world scenarios. The aim of my doctoral research is to
		  develop an automated technique for extracting features and
		  variability which provides reliable solutions to simplify
		  the work of domain analysis. I carefully analyzed the state
		  of the art and identified main limitations so far: accuracy
		  and automation. Based on these insights, I am developing a
		  methodology to address this challenges by making use of
		  advanced Natural Language Processing (NLP) and machine
		  learning techniques. In addition, I plan to design
		  reasonable case study to evaluate the proposed approaches
		  and empirical study to investigate usability in practice.},
  booktitle	= {Proceedings of the 22nd International Systems and Software
		  Product Line Conference - Volume 2},
  pages		= {72–78},
  numpages	= {7},
  keywords	= {feature identification, requirement documents, reverse
		  engineering, software product lines, variability
		  extraction},
  location	= {Gothenburg, Sweden},
  series	= {SPLC '18}
}

@InBook{	  10.5555/3712729.3712835,
  author	= {Tolk, Andreas},
  title		= {Hybrid Modeling Integrating Artificial Intelligence and
		  Modeling \&amp; Simulation Paradigms},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {This paper discusses the complementary relationship
		  between Modeling and Simulation (M&amp;S) and Artificial
		  Intelligence (AI) methods like machine learning. While
		  M&amp;S uses algorithms to model system behavior from input
		  parameters, AI learns patterns from correlation in data.
		  The paper argues that hybrid models combining M&amp;S and
		  AI can be more powerful than either alone. It provides a
		  conceptual framework showing how M&amp;S and AI can be
		  integrated in sequential, parallel, complementary or
		  competitive configurations. Several example applications
		  are given where AI enhances M&amp;S and vice versa, such as
		  using AI to optimize simulation parameters, generate
		  synthetic training data for AI from simulations, interpret
		  AI model behavior through simulation, and automate aspects
		  of simulation development with AI assistance. The potential
		  benefits of hybrid AI/M&amp;S modeling span improved
		  accuracy, efficiency, trustworthiness and
		  cross-disciplinary collaboration. The paper calls for
		  further research developing a solid theoretical foundation
		  for merging these complementary paradigms.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1271–1280},
  numpages	= {10}
}

@InProceedings{	  10.1145/3624007.3624060,
  author	= {Correa Restrepo, Camilo and Robin, Jacques and Mazo,
		  Raul},
  title		= {Generating Constraint Programs for Variability Model
		  Reasoning: A DSL and Solver-Agnostic Approach},
  year		= {2023},
  isbn		= {9798400704062},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3624007.3624060},
  doi		= {10.1145/3624007.3624060},
  abstract	= {Verifying and configuring large Software Product Lines
		  (SPL) requires automation tools. Current state-of-the-art
		  approaches involve translating variability models into a
		  formalism accepted as input by a constraint solver. There
		  are currently no standards for variability modeling
		  languages (VML). There is also a variety of constraint
		  solver input languages. This has resulted in a
		  multiplication of ad-hoc architectures and tools
		  specialized for a single pair of VML and solver,
		  fragmenting the SPL community. To overcome this limitation,
		  we propose a novel architecture based on model-driven code
		  generation, where the syntax and semantics of VMLs can be
		  declaratively specified as data, and a standard,
		  human-readable, formal pivot language is used between the
		  VML and the solver input language. This architecture is the
		  first to be fully generic by being agnostic to both VML and
		  the solver paradigm. To validate the genericity of the
		  approach, we have implemented a prototype tool together
		  with declarative specifications for the syntax and
		  semantics of two different VMLs and two different solver
		  families. One VML is for classic, static SPL, and the other
		  for run-time reconfigurable dynamic SPL with soft
		  constraints to be optimized during configuration. The two
		  solver families are Constraint Satisfaction Programs (CSP)
		  and Constraint Logic Programs (CLP).},
  booktitle	= {Proceedings of the 22nd ACM SIGPLAN International
		  Conference on Generative Programming: Concepts and
		  Experiences},
  pages		= {138–152},
  numpages	= {15},
  keywords	= {Automated Reasoning, Configuration Automation, Generic
		  Architecture, Software Product Lines},
  location	= {Cascais, Portugal},
  series	= {GPCE 2023}
}

@InBook{	  10.1145/3672608.3707884,
  author	= {Zampierin, Luca and Frasincar, Flavius},
  title		= {An Unsupervised Approach for Aspect-Based Sentiment
		  Classification Using Attentional Neural Models},
  year		= {2025},
  isbn		= {9798400706295},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672608.3707884},
  abstract	= {With the vast amount of reviews available on the Web, in
		  the past decades, a growing share of literature has focused
		  on sentiment analysis. Aspect-based sentiment
		  classification is the subtask that seeks to detect the
		  sentiment expressed by the content creators towards a
		  defined target typically within a sentence. This paper
		  introduces two novel unsupervised attentional neural
		  network models for aspect-based sentiment classification,
		  and tests them on English restaurant reviews. The first
		  model employs an autoencoder-like structure to learn a
		  sentiment embedding matrix where each row of the matrix
		  represents the embedding for one sentiment. To improve the
		  model, a target-based attention mechanism is included that
		  de-emphasizes irrelevant words. Last, a redundancy and a
		  seed regularization term constrain the sentiment embedding
		  matrix. The second model extends the first by including a
		  Bi-LSTM layer in the attention mechanism to exploit
		  contextual information. Although both models construct
		  meaningful sentiment embeddings, experimental results
		  indicate that the inclusion of the Bi-LSTM in the attention
		  mechanism leads to a more precise attention mechanisms and,
		  thus, better predictions. The best model, i.e., the second,
		  outperforms all investigated unsupervised and weakly
		  supervised algorithms for aspect-based sentiment
		  classification from the literature.},
  booktitle	= {Proceedings of the 40th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {2069–2077},
  numpages	= {9}
}

@Article{	  10.1145/3623379,
  author	= {Saidi, Rakia and Jarray, Fethi},
  title		= {Stacking of BERT and CNN Models for Arabic Word Sense
		  Disambiguation},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {11},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3623379},
  doi		= {10.1145/3623379},
  abstract	= {We propose a new approach for Arabic Word Sense
		  Disambiguation (AWSD) by hybridization of single-layer
		  Convolutional Neural Network (CNN) with contextual
		  representation (BERT). WSD is the task of automatically
		  detecting the correct meaning of a word used in a given
		  context. WSD can be performed as a classification task, and
		  the context is generally a short sentence. Kim [26] proved
		  that combining a CNN with an RNN (recurrent neural network)
		  provides a good result for text classification. Here, we
		  use a concatenation of BERT models as a word embedding to
		  get simultaneously the target and context representation.
		  Our approach improves the performance of WSD in Arabic
		  languages. The experimental results show that our model
		  outperforms the state-of-the-art approaches and improves
		  the accuracy of 96.42\% on the Arabic WordNet dataset.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {247},
  numpages	= {14},
  keywords	= {Word sense disambiguation, Arabic text, supervised
		  approach, transformer, BERT, convolutional neural network}
}

@Proceedings{	  10.1145/3689217,
  title		= {LAMPS '24: Proceedings of the 1st ACM Workshop on Large AI
		  Systems and Models with Privacy and Safety Analysis},
  year		= {2024},
  isbn		= {9798400712098},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {This volume contains papers presented at the 1st ACM
		  Workshop on Large AI Systems and Models with Privacy and
		  Safety Analysis (LAMPS), which was held on October 14,
		  2024, in Salt Lake City, USA.This year we received 18 paper
		  submissions from Australia, China, Italy, Luxembourg,
		  Singapore, South Korea, USA, and Vietnam, and 11
		  high-quality papers were accepted. Each contributed paper
		  was rigorously peer-reviewed by reviewers who were drawn
		  from a pool of expert technical committee members in
		  machine learning security and privacy. Each paper received
		  two detailed review comments and one meta review comments
		  that summarise the weaknesses/issues to be addressed in the
		  camera-ready revision or future work.},
  location	= {Salt Lake City, UT, USA}
}

@Article{	  10.1145/3150227,
  author	= {Bergmayr, Alexander and Breitenb\"{u}cher, Uwe and Ferry,
		  Nicolas and Rossini, Alessandro and Solberg, Arnor and
		  Wimmer, Manuel and Kappel, Gerti and Leymann, Frank},
  title		= {A Systematic Review of Cloud Modeling Languages},
  year		= {2018},
  issue_date	= {January 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {51},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3150227},
  doi		= {10.1145/3150227},
  abstract	= {Modern cloud computing environments support a relatively
		  high degree of automation in service provisioning, which
		  allows cloud service customers (CSCs) to dynamically
		  acquire services required for deploying cloud applications.
		  Cloud modeling languages (CMLs) have been proposed to
		  address the diversity of features provided by cloud
		  computing environments and support different application
		  scenarios, such as migrating existing applications to the
		  cloud, developing new cloud applications, or optimizing
		  them. There is, however, still much debate in the research
		  community on what a CML is, and what aspects of a cloud
		  application and its target cloud computing environment
		  should be modeled by a CML. Furthermore, the distinction
		  between CMLs on a fine-grain level exposing their modeling
		  concepts is rarely made. In this article, we investigate
		  the diverse features currently provided by existing CMLs.
		  We classify and compare them according to a common
		  framework with the goal to support CSCs in selecting the
		  CML that fits the needs of their application scenario and
		  setting. As a result, not only features of existing CMLs
		  are pointed out for which extensive support is already
		  provided but also in which existing CMLs are deficient,
		  thereby suggesting a research agenda.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {22},
  numpages	= {38},
  keywords	= {Cloud computing, domain-specific languages, modeling}
}

@InProceedings{	  10.1145/3447548.3467138,
  author	= {Hao, Junheng and Lei, Chuan and Efthymiou, Vasilis and
		  Quamar, Abdul and \"{O}zcan, Fatma and Sun, Yizhou and
		  Wang, Wei},
  title		= {MEDTO: Medical Data to Ontology Matching Using Hybrid
		  Graph Neural Networks},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467138},
  doi		= {10.1145/3447548.3467138},
  abstract	= {Medical ontologies are widely used to describe and
		  organize medical terminologies and to support many critical
		  applications on healthcare databases. These ontologies are
		  often manually curated (e.g., UMLS, SNOMED CT, and MeSH) by
		  medical experts. Medical databases, on the other hand, are
		  often created by database administrators, using different
		  terminology and structures. The discrepancies between
		  medical ontologies and databases compromise
		  interoperability between them. Data to ontology matching is
		  the process of finding semantic correspondences between
		  tables in databases to standard ontologies. Existing
		  solutions such as ontology matching have mostly focused on
		  engineering features from terminological, structural, and
		  semantic model information extracted from the ontologies.
		  However, this is often labor intensive and the accuracy
		  varies greatly across different ontologies. Worse yet, the
		  ontology capturing a medical database is often not given in
		  practice. In this paper, we propose MEDTO, a novel
		  end-to-end framework that consists of three innovative
		  techniques: (1) a lightweight yet effective method that
		  bootstrap a semantically rich ontology from a given medical
		  database, (2) a hyperbolic graph convolution layer that
		  encodes hierarchical concepts in the hyperbolic space, and
		  (3) a heterogeneous graph layer that encodes both local and
		  global context information of a concept. Experiments on two
		  real-world medical datasets matching against SNOMED CT show
		  significant improvements compared to the state-of-the-art
		  methods. MEDTO also consistently achieves competitive
		  results on a benchmark from the Ontology Alignment
		  Evaluation Initiative.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery \&amp; Data Mining},
  pages		= {2946–2954},
  numpages	= {9},
  keywords	= {graph neural network, medical data, ontology matching},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@Proceedings{	  10.1145/3722565,
  title		= {FMSys: Proceedings of the 2nd International Workshop on
		  Foundation Models for Cyber-Physical Systems \&amp;
		  Internet of Things},
  year		= {2025},
  isbn		= {9798400716089},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Irvine, CA, USA}
}

@InProceedings{	  10.1145/3561877.3561896,
  author	= {Wang, Xiaoning and Zhang, Yang},
  title		= {Research on CGAN-BERT and RGAN-BERT Models for Short Text
		  Classification based on Semi-Supervised Model},
  year		= {2022},
  isbn		= {9781450396837},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3561877.3561896},
  doi		= {10.1145/3561877.3561896},
  abstract	= {With the rapid development of artificial intelligence, a
		  large number of short texts cause a certain degree of
		  information redundancy. Text classification technology can
		  help people classify and process information, and has
		  important applications in the fields of recommendation
		  system, public opinion monitoring and information
		  retrieval. However, short text information in different
		  fields has the characteristics of industry professionalism
		  and fast updating of language style. The resulting problems
		  such as poor applicability of the model and bottlenecks in
		  annotation make the effect of traditional classification
		  methods in short text analysis limited. Therefore, we
		  propose semi supervised text classification models
		  CGAN-BERT and RGAN-BERT based on convolutional neural
		  network and cyclic neural network respectively. The newly
		  designed generator and discriminator are more conducive to
		  the game process. We evaluate our model and other classical
		  models on several public data sets. The experimental
		  results show that our proposed model is better than other
		  models.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Information Science and Systems},
  pages		= {118–124},
  numpages	= {7},
  keywords	= {Recurrent Neural Network, convolutional neural networks,
		  gan, natural language processing, text classification},
  location	= {Beijing, China},
  series	= {ICISS '22}
}

@InProceedings{	  10.1145/3568562.3568602,
  author	= {Agafonov, Anton and Ponomarev, Andrew},
  title		= {An Experiment on Localization of Ontology Concepts in Deep
		  Convolutional Neural Networks},
  year		= {2022},
  isbn		= {9781450397254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3568562.3568602},
  doi		= {10.1145/3568562.3568602},
  abstract	= {Deep neural networks have recently evolved into a powerful
		  AI tool, reaching near-human performance level in many
		  tasks, and in some tasks even surpassing it. However, a
		  significant drawback of neural networks is the lack of
		  explainability and interpretability — it is hard to say
		  why a neural network arrived to a certain conclusion. This
		  significantly limits application of neural networks in
		  critical tasks and undermines trust in human-AI
		  collaboration. It has been recently shown that internal
		  representations constructed by a neural network can often
		  be aligned with a domain ontology. This opens a promising
		  way to provide explanations of a neural network in human
		  terms. In this paper, we discuss the results of the
		  experiment aimed at understanding what layers of a neural
		  network are the most perspective for the alignment with
		  given ontology concept. To do so, we build concept
		  localization maps for XTRAINS — a synthetic dataset
		  consisting of images and their ontological annotations. The
		  importance of such maps is that they can be used for the
		  development of efficient concept alignment heuristics. The
		  experiment mostly supports the intuition that high-level
		  concepts are localized mostly in the activations of last
		  layers of a neural network (near its head), while
		  lower-level concepts might be better extracted from middle
		  layers.},
  booktitle	= {Proceedings of the 11th International Symposium on
		  Information and Communication Technology},
  pages		= {82–87},
  numpages	= {6},
  keywords	= {XAI, explainable AI, neural networks, neuro-symbolic
		  intelligence, ontologies},
  location	= {Hanoi, Vietnam},
  series	= {SoICT '22}
}

@InProceedings{	  10.1145/3701716.3715172,
  author	= {Dew, Rebecca and Li, Mingzhao and Liu, Weidong and Baratha
		  Raj, Sandya},
  title		= {Developing a Comprehensive Task Framework for Effective
		  Workforce Analysis},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715172},
  doi		= {10.1145/3701716.3715172},
  abstract	= {Effective workforce analysis, planning, and management
		  require a deep understanding of the tasks and skills
		  associated with different roles. This paper introduces a
		  novel methodology for developing a comprehensive task
		  framework that leverages Large Language Models (LLMs) and
		  large-scale job ads data. We first propose an innovative
		  approach to task taxonomy design, which involves the
		  decomposition and reconstruction of tasks into a
		  hierarchical structure based on action-object pairings,
		  systematically refined using LLMs. The methodology extends
		  to integrating the taxonomy with occupation and skill
		  linkages derived from job ads, ensuring alignment with
		  real-world workforce dynamics. Finally, we demonstrate the
		  practical value of this framework through a visual
		  analytics system that enables interactive exploration and
		  analysis of tasks, occupations, and associated skills,
		  highlighting its potential to transform workforce analysis.
		  Demo video: https://bit.ly/41txBZK},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2819–2822},
  numpages	= {4},
  keywords	= {gpt, large language model, visual analytics, workforce
		  analysis},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3716368.3735295,
  author	= {Gautam, Ashish and Patton, Robert and Potok, Thomas and
		  Kannan, Ramakrishnan and Aimone, James and Severa,
		  William},
  title		= {AI-Powered Knowledge Graphs for Neuromorphic and
		  Energy-Efficient Computing},
  year		= {2025},
  isbn		= {9798400714962},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3716368.3735295},
  doi		= {10.1145/3716368.3735295},
  abstract	= {The surge in scientific literature obscures breakthroughs
		  and hinders the discovery of new research paths. We propose
		  an artificial intelligence (AI) powered framework using
		  large language models (LLMs) and knowledge graphs (KGs) to
		  automate parts of scientific discovery, focusing on
		  energy-efficient AI circuits. Our hybrid approach combines
		  LLMs, structured data, and ontology-based reasoning to
		  construct a comprehensive knowledge graph that integrates
		  insights across computational neuroscience, spiking neuron
		  models, learning rules, architectural motifs, and
		  neuromorphic device technologies. This multi-domain
		  representation enables the generation of hypotheses that
		  connect biological function with implementable,
		  energy-efficient hardware architectures. Using KG
		  embeddings and graph neural networks, the framework
		  generates hypotheses for novel circuits, validates them
		  through optimization on exascale HPC systems, and with
		  tools like SuperNeuro and Fugu, the most promising designs
		  will be prototyped in hardware. This open-source system
		  aims to accelerate discoveries and bridging neuroscience
		  with hardware innovation, drive collaboration, and unlock
		  new opportunities in low-power AI computing.},
  booktitle	= {Proceedings of the Great Lakes Symposium on VLSI 2025},
  pages		= {996–1001},
  numpages	= {6},
  keywords	= {Knowledge graphs, neuromorphic computing, energy-efficient
		  circuits, large language models, hypothesis generation,
		  spiking neural networks, architectural synthesis,
		  scientific discovery automation, STDP, cortical
		  microcircuits.},
  location	= { },
  series	= {GLSVLSI '25}
}

@InProceedings{	  10.1145/3652620.3688206,
  author	= {Rabbi, Fazle},
  title		= {A Model-Based Framework for Exploring Conflict Dynamics},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688206},
  doi		= {10.1145/3652620.3688206},
  abstract	= {This paper introduces a novel framework for conflict
		  analysis that leverages advanced visual modeling
		  techniques. By employing comparative analysis, key
		  variables influencing armed conflicts are identified and
		  analyzed. The framework includes a meta-model representing
		  domain concepts such as the goals and strategies of
		  conflicting parties, escalating stages, and impacts of
		  conflicts.Conflict escalation is a complex process
		  characterized by interactions between opposing parties.
		  This paper presents a structured model that outlines how
		  conflicts evolve and intensify over time. We adapt a
		  meta-modeling framework called the Diagram Predicate
		  Framework (DPF) to represent conflict-related concepts and
		  extend it to support abstract view generation. This
		  framework facilitates the analysis of conflict trends and
		  the study of dynamics across various levels of
		  abstraction.A computational model based on category theory
		  is proposed for trend analysis, enabling the extraction of
		  patterns of conflict evolution and the comparison of
		  strategies and goals at different escalation stages.
		  Categorical operations such as pullback and limit
		  construction are employed to compute conflict evolution and
		  identify common structures among conflict instances,
		  providing insights into conflict dynamics across diverse
		  zones.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {745–754},
  numpages	= {10},
  keywords	= {conflict analysis, computational journalism, category
		  theory, metamodeling},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.1145/3527838,
  author	= {Akram, Muhammad Waseem and Salman, Muhammad and Bashir,
		  Muhammad Farrukh and Salman, Syed Muhammad Saad and
		  Gadekallu, Thippa Reddy and Javed, Abdul Rehman},
  title		= {A Novel Deep Auto-Encoder Based Linguistics Clustering
		  Model for Social Text},
  year		= {2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3527838},
  doi		= {10.1145/3527838},
  abstract	= {The wide adoption of media and social media has increased
		  the amount of digital content to an enormous level. Natural
		  language processing (NLP) techniques provide an opportunity
		  to extract and explore meaningful information from a large
		  amount of text. Among natural languages, Urdu is one of the
		  widely used languages worldwide for spoken and written
		  communications. Due to its wide adopt-ability, digital
		  content in the Urdu language is increasing briskly,
		  especially with social media and online NEWS feeds.
		  Government agencies and advertisers must filter and
		  understand the content to analyze the trends and cohorts in
		  their interest and national prerogative. Clustering is
		  considered a baseline and one of the first steps in natural
		  language understanding. There are many state-of-the-art
		  clustering techniques specifically for English, French, and
		  Arabic, but no significant research has been conducted in
		  Urdu language processing. Doing it for short text segments
		  is challenging because of limited features and the absence
		  of meaningful language discourse and nuance. Many
		  rule-based NLP techniques are adopted to overcome these
		  issues, relying on human-designed features and rules.
		  Therefore, these methods do not promise remarkable results.
		  Alongside NLP, deep learning techniques are pretty
		  efficient in capturing contextual information with minimal
		  noise compared to other traditional methods. By taking on
		  this challenging job, we develop a deep learning-based
		  technique for Urdu short text clustering for the very first
		  time without a human-designed feature. In this paper, we
		  propose a method of short text clustering using a deep
		  neural network that automatically learns feature
		  representations and clustering assignments simultaneously.
		  This method learns clustering objectives by converting the
		  high dimensional feature space to a low dimensional feature
		  space. Our experiments on the Urdu NEWS headlines dataset
		  show remarkable results compared to state-of-the-art
		  methods.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  keywords	= {Clustering, Urdu, Social media, Text, Low resource
		  language}
}

@InProceedings{	  10.1145/3606305.3606321,
  author	= {Ivanova, Tatyana Ivanova},
  title		= {Semi-automatic ontology development for supporting
		  personalized tutoring},
  year		= {2023},
  isbn		= {9798400700477},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3606305.3606321},
  doi		= {10.1145/3606305.3606321},
  abstract	= {Many researches have working the last two decades on
		  ontology learning, using NLP, data mining, and machine
		  learning, but experiments show, that every ontology
		  learning method have his precision and recall less than 1,
		  and automatically developed ontologies need from human
		  evaluation and modification. So, participation of experts
		  is a must in ontology development to guarantee high quality
		  and the semi-automatic ontology development is the only
		  approach having potential to ensure cheaper development of
		  high-quality ontologies. In this research we will discuss
		  semi-automatic support of ontology engineering process,
		  based on automated knowledge extraction from text,
		  semi-structured or structured sources. Most of automated
		  ontology development methods and algorithms are domain –
		  dependent. We analyze specifics of semi-automatic ontology
		  development in the e-learning domain and propose software
		  architecture of semi-automatic ontology development
		  framework for educational ontologies.},
  booktitle	= {Proceedings of the 24th International Conference on
		  Computer Systems and Technologies},
  pages		= {180–185},
  numpages	= {6},
  location	= {Ruse, Bulgaria},
  series	= {CompSysTech '23}
}

@InProceedings{	  10.1145/3383583.3398529,
  author	= {Scharpf, Philipp and Schubotz, Moritz and Youssef, Abdou
		  and Hamborg, Felix and Meuschke, Norman and Gipp, Bela},
  title		= {Classification and Clustering of arXiv Documents,
		  Sections, and Abstracts, Comparing Encodings of Natural and
		  Mathematical Language},
  year		= {2020},
  isbn		= {9781450375856},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383583.3398529},
  doi		= {10.1145/3383583.3398529},
  abstract	= {In this paper, we show how selecting and combining
		  encodings of natural and mathematical language affect
		  classification and clustering of documents with
		  mathematical content. We demonstrate this by using sets of
		  documents, sections, and abstracts from the arXiv preprint
		  server that are labeled by their subject class
		  (mathematics, computer science, physics, etc.) to compare
		  different encodings of text and formulae and evaluate the
		  performance and runtimes of selected classification and
		  clustering algorithms. Our encodings achieve classification
		  accuracies up to 82.8\% and cluster purities up to 69.4\%
		  (number of clusters equals number of classes), and 99.9\%
		  (unspecified number of clusters) respectively. We observe a
		  relatively low correlation between text and math
		  similarity, which indicates the independence of text and
		  formulae and motivates treating them as separate features
		  of a document. The classification and clustering can be
		  employed, e.g., for document search and recommendation.
		  Furthermore, we show that the computer outperforms a human
		  expert when classifying documents. Finally, we evaluate and
		  discuss multi-label classification and formula
		  semantification.},
  booktitle	= {Proceedings of the ACM/IEEE Joint Conference on Digital
		  Libraries in 2020},
  pages		= {137–146},
  numpages	= {10},
  keywords	= {document classification, document clustering, information
		  retrieval, machine learning, mathematical information
		  retrieval},
  location	= {Virtual Event, China},
  series	= {JCDL '20}
}

@InProceedings{	  10.1145/3571473.3571502,
  author	= {Castro, Murillo and Barcellos, Monalessa},
  title		= {An Ontology to support Knowledge Management Solutions for
		  Human-Computer Interaction Design},
  year		= {2023},
  isbn		= {9781450399999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571473.3571502},
  doi		= {10.1145/3571473.3571502},
  abstract	= {Developing interactive systems is a challenging task that
		  involves concerns related to the human-computer interaction
		  (HCI), such as usability and user experience. Therefore,
		  HCI design is a core issue to the quality of such systems.
		  HCI design often involves people with different backgrounds
		  (e.g., Arts, Software Engineering, Design). This makes
		  knowledge transfer a challenging issue due to the lack of a
		  common conceptualization about HCI design, leading to
		  semantic interoperability problems, such as ambiguity and
		  imprecision when interpreting shared information.
		  Ontologies have been acknowledged as a successful approach
		  to represent domain knowledge and support knowledge-based
		  solutions. Hence, in this work, we propose to explore the
		  use of ontologies to represent structured knowledge of HCI
		  design and improve knowledge sharing in this context. We
		  developed the Human-Computer Interaction Design Ontology
		  (HCIDO), which is part of the Human-Computer Interaction
		  Ontology Network (HCI-ON) and is connected to the Software
		  Engineering Ontology Network (SEON). By making knowledge
		  related to the HCI design domain explicit and structured,
		  HCIDO helped us to develop KTID, a tool that aims to
		  support capturing and sharing knowledge to aid in HCI
		  design by allowing HCI designers to annotate information
		  about design choices in design artifacts shared with HCI
		  design stakeholders. Preliminary results indicate that the
		  tool can be particularly useful for novice HCI designers.},
  booktitle	= {Proceedings of the XXI Brazilian Symposium on Software
		  Quality},
  articleno	= {33},
  numpages	= {10},
  keywords	= {HCI Design, Knowledge Management, Ontology, User
		  Interface},
  location	= {Curitiba, Brazil},
  series	= {SBQS '22}
}

@InProceedings{	  10.1145/3644713.3644729,
  author	= {Meftah, Mohammed Charaf Eddine and Kazar, Okba},
  title		= {A new ontological approach for multilingual scientific
		  research},
  year		= {2024},
  isbn		= {9798400709036},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644713.3644729},
  doi		= {10.1145/3644713.3644729},
  abstract	= {Abstract— Scientific research is all the actions
		  undertaken to produce and develop scientific knowledge. The
		  representation of this knowledge can take various forms: it
		  can be publications, reports, patents, etc. This knowledge
		  can be incorporated into scientific social networks and
		  applications. The scientific social networks in their
		  current form depend on the title, the keywords, and the
		  ontology to compare and link the relationship between
		  different scientific research; there may be different
		  scientific research with the same keywords; the same
		  scientific research may have different titles. In addition,
		  scientific research may be written in different languages,
		  but current scientific social networks do not take into
		  account the multiple languages of researchers and research.
		  they cannot link the relationship and compare scientific
		  research written in different languages. To solve these
		  problems, this paper proposes the use of a multi-lingual
		  ontology to determine and describe scientific research.
		  This work uses ontology in the context of a conceptual
		  indexation, by separating the concept from the term, thus,
		  the result of this work according to the proposed approach
		  will make it possible to express the concept (knowledge
		  extra-linguistic), in different languages, this will allow
		  comparing and linking the relationship between scientific
		  research written in different languages and measuring the
		  percentage of similarity and difference between them.
		  Dealing with multilingualism in scientific research is a
		  very important contribution to this field.},
  booktitle	= {Proceedings of the 7th International Conference on Future
		  Networks and Distributed Systems},
  pages		= {114–124},
  numpages	= {11},
  location	= {Dubai, United Arab Emirates},
  series	= {ICFNDS '23}
}

@Article{	  10.14778/3659437.3659461,
  author	= {Kayali, Moe and Lykov, Anton and Fountalis, Ilias and
		  Vasiloglou, Nikolaos and Olteanu, Dan and Suciu, Dan},
  title		= {Chorus: Foundation Models for Unified Data Discovery and
		  Exploration},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {8},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3659437.3659461},
  doi		= {10.14778/3659437.3659461},
  abstract	= {We apply foundation models to data discovery and
		  exploration tasks. Foundation models are large language
		  models (LLMS) that show promising performance on a range of
		  diverse tasks unrelated to their training. We show that
		  these models are highly applicable to the data discovery
		  and data exploration domain. When carefully used, they have
		  superior capability on three representative tasks:
		  table-class detection, column-type annotation and
		  join-column prediction. On all three tasks, we show that a
		  foundation-model-based approach outperforms the
		  task-specific models and so the state of the art. Further,
		  our approach often surpasses human-expert task performance.
		  We investigate the fundamental characteristics of this
		  approach including generalizability to several foundation
		  models and the impact of non-determinism on the outputs.
		  All in all, this suggests a future direction in which
		  disparate data management tasks can be unified under
		  foundation models.},
  journal	= {Proc. VLDB Endow.},
  month		= apr,
  pages		= {2104–2114},
  numpages	= {11}
}

@Article{	  10.1145/3604550,
  author	= {Liu, Yaochen and Li, Qiuchi and Wang, Benyou and Zhang,
		  Yazhou and Song, Dawei},
  title		= {A Survey of Quantum-cognitively Inspired Sentiment
		  Analysis Models},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3604550},
  doi		= {10.1145/3604550},
  abstract	= {Quantum theory, originally proposed as a physical theory
		  to describe the motions of microscopic particles, has been
		  applied to various non-physics domains involving human
		  cognition and decision-making that are inherently uncertain
		  and exhibit certain non-classical, quantum-like
		  characteristics. Sentiment analysis is a typical example of
		  such domains. In the last few years, by leveraging the
		  modeling power of quantum probability (a non-classical
		  probability stemming from quantum mechanics methodology)
		  and deep neural networks, a range of novel
		  quantum-cognitively inspired models for sentiment analysis
		  have emerged and performed well. This survey presents a
		  timely overview of the latest developments in this
		  fascinating cross-disciplinary area. We first provide a
		  background of quantum probability and quantum cognition at
		  a theoretical level, analyzing their advantages over
		  classical theories in modeling the cognitive aspects of
		  sentiment analysis. Then, recent quantum-cognitively
		  inspired models are introduced and discussed in detail,
		  focusing on how they approach the key challenges of the
		  sentiment analysis task. Finally, we discuss the
		  limitations of the current research and highlight future
		  research directions.},
  journal	= {ACM Comput. Surv.},
  month		= aug,
  articleno	= {15},
  numpages	= {37},
  keywords	= {Quantum-cognitively inspired models, non-classical
		  probability from quantum mechanics methodology, sentiment
		  analysis, sarcasm detection, emotion recognition}
}

@InProceedings{	  10.1145/3554364.3559139,
  author	= {de Freitas, Alexandre A. C. and Scalser, Murilo B. and
		  Costa, Simone D. and Barcellos, Monalessa P.},
  title		= {Towards an ontology-based approach to develop software
		  systems with adaptive user interface},
  year		= {2022},
  isbn		= {9781450395069},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3554364.3559139},
  doi		= {10.1145/3554364.3559139},
  abstract	= {The new ways of manipulating computers, smartphones and
		  other devices have brought challenges such as the need to
		  ensure a good usability when different user types use the
		  same system. Adaptive user interface (AUI) systems are a
		  possible solution. They change the user interface to better
		  meet the needs of different users. However, developing such
		  systems is not trivial. It is necessary to capture the
		  users' characteristics and preferences and constantly adapt
		  the system accordingly. In this paper, we discuss the use
		  of ontologies to support the development of AUI systems. We
		  argue that by providing structured knowledge about such
		  systems, ontologies help understand how they work and offer
		  a basis to structure them, identify the necessary
		  adaptations and implement mechanisms to make them happen in
		  run-time. We have explored the use of ontologies from an
		  ontology network to develop a social network about academic
		  subjects that automatically adapts its interface according
		  to the low vision and colorblind user's needs and usage
		  characteristics. The first version of an ontology-based
		  process to guide the development of AUI systems raised from
		  this experience.},
  booktitle	= {Proceedings of the 21st Brazilian Symposium on Human
		  Factors in Computing Systems},
  articleno	= {43},
  numpages	= {7},
  keywords	= {adaptive user interface, ontology, ontology network},
  location	= {Diamantina, Brazil},
  series	= {IHC '22}
}

@InProceedings{	  10.1145/3459637.3482387,
  author	= {Wisniewski, Dawid and Potoniec, Jedrzej and Lawrynowicz,
		  Agnieszka},
  title		= {SeeQuery: An Automatic Method for Recommending
		  Translations of Ontology Competency Questions into
		  SPARQL-OWL},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482387},
  doi		= {10.1145/3459637.3482387},
  abstract	= {Ontology authoring is a complicated and error-prone
		  process since the knowledge being modeled is expressed
		  using logic-based formalisms, in which logical consequences
		  of the knowledge have to be foreseen. To make that process
		  easier, competency questions (CQs), being questions
		  expressed in natural language are often stated to trace
		  both the correctness and completeness of the ontology at a
		  given time. However, CQs have to be translated into a
		  formal language, like ontology query language (SPARQL-OWL),
		  to query the ontology. Since the translation step is
		  time-consuming and requires familiarity with the query
		  language used, in this paper, we propose an automatic
		  method named SeeQuery, which recommends SPARQL-OWL queries
		  being translations of CQs stated against a given ontology.
		  It consists of a pipeline of transformations based on
		  template matching and filling, being motivated by the
		  biggest to date publicly available CQ to SPARQL-OWL
		  datasets. We provide a detailed description of SeeQuery and
		  evaluate the method on a separate set of 2 ontologies with
		  their CQs. It is, to date, the only automatic method
		  available for recommending SPARQL-OWL queries out of CQs.
		  The source code of SeeQuery is available at:
		  https://github.com/dwisniewski/SeeQuery.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {2119–2128},
  numpages	= {10},
  keywords	= {automatic translation, competency questions, ontology
		  authoring, semantic similarity, sparql-owl},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3460210.3493568,
  author	= {Alharbi, Reham and Tamma, Valentina and Grasso, Floriana},
  title		= {Characterising the Gap Between Theory and Practice of
		  Ontology Reuse},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493568},
  doi		= {10.1145/3460210.3493568},
  abstract	= {Ontology reuse is a complex process that requires the
		  support of methodologies and tools to minimise errors and
		  to keep the ontologies consistent and coherent. Although
		  the vast majority of ontology engineering methodologies
		  include a reuse phase, and reuse has been investigated for
		  different tasks and purposes (e.g.ontology integration),
		  this body of work does not seem to translate into practice,
		  neither in the form of strict criteria for reuse, nor as a
		  set of community proposed guidelines. In this paper, we
		  report the salient results from a study aimed at ontology
		  developers and practitioners, whose objective is to gain an
		  insight into the gap between the theory and the practice of
		  ontology reuse. Thefocus of our study is to gain
		  practitioners' views on i) their preferred reuse
		  approaches; ii) the types of ontologies they tend to reuse
		  (e.g. specific domain ontologies or upper-level
		  ontologies)iii) what reporting information they deem useful
		  when deciding which ontology to reuse; iv) what are the
		  main reasons deterring them from reusing an ontology. Our
		  findings confirm and extend established results from the
		  literature, but in addition, the study provides a fresh
		  view on the practice of reuse with an explicit focus on
		  highly experienced developers and moderately experienced
		  ones. The study corroborates the need for a comprehensive
		  set of recommendations, that are widely accepted by the
		  community, and are possibly implemented in development
		  tools.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {217–224},
  numpages	= {8},
  keywords	= {challenges to ontology reuse, ontology development
		  methodologies, ontology engineering, ontology reuse},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@Article{	  10.1145/3657299,
  author	= {Xia, Bolun (Namir) and Rawte, Vipula and Gupta, Aparna and
		  Zaki, Mohammed},
  title		= {FETILDA: Evaluation Framework for Effective
		  Representations of Long Financial Documents},
  year		= {2024},
  issue_date	= {August 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {7},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3657299},
  doi		= {10.1145/3657299},
  abstract	= {In the financial sphere, there is a wealth of accumulated
		  unstructured financial data, such as the textual disclosure
		  documents that companies submit on a regular basis to
		  regulatory agencies, such as the Securities and Exchange
		  Commission. These documents are typically very long and
		  tend to contain valuable soft information about a
		  company’s performance that is not present in quantitative
		  predictors. It is therefore of great interest to learn
		  predictive models from these long textual documents,
		  especially for forecasting numerical key performance
		  indicators. In recent years, there has been great progress
		  in natural language processing via pre-trained language
		  models (LMs) learned from large corpora of textual data.
		  This prompts the important question of whether they can be
		  used effectively to produce representations for long
		  documents, as well as how we can evaluate the effectiveness
		  of representations produced by various LMs. Our work
		  focuses on answering this critical question, namely, the
		  evaluation of the efficacy of various LMs in extracting
		  useful soft information from long textual documents for
		  prediction tasks. In this article, we propose and implement
		  a deep learning evaluation framework that utilizes a
		  sequential chunking approach combined with an attention
		  mechanism. We perform an extensive set of experiments on a
		  collection of 10-K reports submitted annually by U.S.
		  banks, and another dataset of reports submitted by U.S.
		  companies, to investigate thoroughly the performance of
		  different types of language models. Overall, our framework
		  using LMs outperforms strong baseline methods for textual
		  modeling as well as for numerical regression. Our work
		  provides better insights into how utilizing pre-trained
		  domain-specific and fine-tuned long-input LMs for
		  representing long documents can improve the quality of
		  representation of textual data and, therefore, help in
		  improving predictive analyses.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= jun,
  articleno	= {182},
  numpages	= {27},
  keywords	= {Text regression, language models, long text documents,
		  financial documents, 10-K reports}
}

@InProceedings{	  10.1145/3432291.3432306,
  author	= {Contreras, Jennifer O. and Ballera, Melvin A. and Festijo,
		  Enrique D.},
  title		= {Ontology Learning using Hybrid Machine Learning Algorithms
		  for Disaster Risk Management},
  year		= {2020},
  isbn		= {9781450375733},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3432291.3432306},
  doi		= {10.1145/3432291.3432306},
  abstract	= {Disaster is inevitable but manageable thru careful
		  planning, preparation and immediate response strategies.
		  During typhoons, earthquakes and other calamities,
		  agreement about language is vital to understand each other
		  well to avoid high number of deaths, delay in access to
		  basic needs and slow response time. However, some of the
		  people involved in this domain find it hard to coordinate
		  and respond to different emergency situations due to lack
		  of familiarization and knowledge about the different terms
		  or concepts. In disaster risk management, the consistency
		  and reusability of the sharing of information is important
		  to avoid possible risks. Due to this reason, an ontology is
		  incorporated to aid in the disaster management process. The
		  use of ontology enables quick retrieving and incorporating
		  "consistent data" and information related to disaster
		  management which plays an important for making decisions
		  efficiently. This paper aims to implement and evaluate the
		  accuracy of Support Vector Machine (SVM) and Neural Network
		  (NN) learning-based ontology for disaster risk management
		  to enhance the classification of concepts (keywords)
		  generated for the domain ontology. The experiment shows
		  that the hybrid SVM and NN machine learning algorithm
		  outperformed the accuracy of SVM and NN based on the
		  precision, recall and F-Measure criterion.},
  booktitle	= {Proceedings of the 2020 3rd International Conference on
		  Signal Processing and Machine Learning},
  pages		= {13–20},
  numpages	= {8},
  keywords	= {Disaster Risk Management, Hybrid Algorithm, Machine
		  Learning, Natural Language Processing, Neural Network,
		  Ontology Learning, Support Vector Machine},
  location	= {Beijing, China},
  series	= {SPML '20}
}

@InProceedings{	  10.1145/3707292.3707356,
  author	= {Zhao, Pei and Zhang, Longxing and Zhao, Jiawen},
  title		= {Complete the exploration of low-resource knowledge graph
		  completion based on large model technology},
  year		= {2025},
  isbn		= {9798400707308},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3707292.3707356},
  doi		= {10.1145/3707292.3707356},
  abstract	= {In the construction and application of knowledge graph, it
		  is a realistic research problem to complete the knowledge
		  in the low resource field. Traditional methods that rely on
		  manual annotation and rules are not only costly, but also
		  have limitations in coverage and scalability. To solve this
		  problem, this paper proposes a large model technique,
		  combining fine-tuning and knowledge transfer strategies.
		  Firstly, to improve the ability of fine-tuning of the large
		  model, the rich knowledge learned by the large model in the
		  high resource field to assist the completion of the low
		  resource knowledge graph through knowledge transfer
		  technology to make up for the shortage of direct
		  extraction. The experimental results show that this method
		  can effectively improve the completion rate and accuracy of
		  the knowledge graph, especially in the completion of entity
		  relations and attribute filling. Furthermore, we explore
		  the impact of different fine-tuning strategies and
		  knowledge transfer methods on the completion effect,
		  providing experimental empirical and theoretical support
		  for future studies on similar issues.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Artificial Intelligence and Intelligent Information
		  Processing},
  pages		= {140–145},
  numpages	= {6},
  keywords	= {Fine-tuning, Knowledge graph, Large model technology, Low
		  resource, Transfer learning},
  location	= { },
  series	= {AIIIP '24}
}

@Article{	  10.1613/jair.1.13511,
  author	= {Artale, Alessandro and Kontchakov, Roman and Kovtunova,
		  Alisa and Ryzhikov, Vladislav and Wolter, Frank and
		  Zakharyaschev, Michael},
  title		= {First-Order Rewritability and Complexity of
		  Two-Dimensional Temporal Ontology-Mediated Queries},
  year		= {2022},
  issue_date	= {Dec 2022},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {75},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.13511},
  doi		= {10.1613/jair.1.13511},
  abstract	= {Aiming at ontology-based data access to temporal data, we
		  design two-dimensional temporal ontology and query
		  languages by combining logics from the (extended) DL-Lite
		  family with linear temporal logic LTL over discrete time
		  (Z,&lt;). Our main concern is first-order rewritability of
		  ontology-mediated queries (OMQs) that consist of a 2D
		  ontology and a positive temporal instance query. Our target
		  languages for FO-rewritings are two-sorted FO(&lt;) --
		  first-order logic with sorts for time instants ordered by
		  the built-in precedence relation &lt; and for the domain of
		  individuals---its extension FO(&lt;, ≡) with the standard
		  congruence predicates t ≡ 0 (mod n), for any fixed n &gt;
		  1, and FO(RPR) that admits relational primitive recursion.
		  In terms of circuit complexity, FO(&lt;, ≡)- and
		  FO(RPR)-rewritability guarantee answering OMQs in uniform
		  AC0 and NC1, respectively. We proceed in three steps.
		  First, we define a hierarchy of 2D DL-Lite/LTL ontology
		  languages and investigate the FO-rewritability of OMQs with
		  atomic queries by constructing projections onto 1D LTL OMQs
		  and employing recent results on the FO-rewritability of
		  propositional LTL OMQs. As the projections involve deciding
		  consistency of ontologies and data, we also consider the
		  consistency problem for our languages. While the
		  undecidability of consistency for 2D ontology languages
		  with expressive Boolean role inclusions might be expected,
		  we also show that, rather surprisingly, the restriction to
		  Krom and Horn role inclusions leads to decidability (and
		  ExpSpace-completeness), even if one admits full Booleans on
		  concepts. As a final step, we lift some of the
		  rewritability results for atomic OMQs to OMQs with
		  expressive positive temporal instance queries. The lifting
		  results are based on an in-depth study of the canonical
		  models and only concern Horn ontologies.},
  journal	= {J. Artif. Int. Res.},
  month		= dec,
  numpages	= {69}
}

@InProceedings{	  10.1145/3652620.3686251,
  author	= {Zavada, \'{A}rmin and Marussy, Krist\'{o}f and Moln\'{a}r,
		  Vince},
  title		= {From Transpilers to Semantic Libraries: Formal
		  Verification With Pluggable Semantics},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3686251},
  doi		= {10.1145/3652620.3686251},
  abstract	= {In the field of model-based systems engineering, there is
		  an increasing demand for the application of formal methods.
		  However, this requires expertise in formal methods, which
		  cannot be expected from systems engineers. While several
		  attempts have been made to bridge this gap, there are still
		  open questions. (1) With the trend shifting towards
		  ontological languages, systems are modeled as classes of 4D
		  occurrences, rather than a 3D system evolving with time,
		  which hinders the application of state-of-the-art model
		  checking algorithms. (2) Ontological reasoning cannot
		  handle the state space explosion problem, and can even make
		  it harder for verifiers to operate efficiently. (3) When
		  operationalizing ontological languages, we need to validate
		  the conformance of the two semantics, even in the presence
		  of optimizations. (4) On top of all, these challenges must
		  be solved for every new engineering language, version, or
		  variant. In this paper, we propose a new approach to
		  address the aforementioned challenges. To validate its
		  feasibility, we present a prototype tool and evaluate it on
		  a SysML model.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {311–317},
  numpages	= {7},
  keywords	= {model-based systems engineering, kernel modeling language,
		  formal verification, declarative interpretation,
		  metaprogramming, semantic libraries, operational
		  libraries},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3708036.3708130,
  author	= {Cao, Wanghua and Xia, Huan and Xie, Rongdong and Hu,
		  Jiangyu},
  title		= {Research on the Construction of a Knowledge Graph for Miao
		  Medicine Based on the BERT-BiLSTM-CRF Model},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708130},
  doi		= {10.1145/3708036.3708130},
  abstract	= {This study focuses on constructing a knowledge graph for
		  Miao medicine to promote the understanding of traditional
		  Miao cultural practices, enhance public health
		  capabilities, and preserve this intangible cultural
		  heritage. The BERT-BiLSTM-CRF model, combined with the
		  manual review, was utilized for entity recognition, while
		  SBERT technology was used for entity alignment. As a
		  result, a comprehensive knowledge graph of Miao medicine
		  was constructed, including 11,111 nodes and 143,751
		  relationships. This knowledge graph systematically
		  organizes the complex knowledge system of Miao medicine,
		  including diseases, symptoms, drugs, and their
		  interrelations. It supports efficient knowledge querying,
		  reasoning, and discovery, contributing valuable insights
		  into applying knowledge graph technologies in traditional
		  medicine. Moreover, this research offers a framework that
		  can be adapted for constructing knowledge graphs in other
		  conventional medical systems.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {555–561},
  numpages	= {7},
  keywords	= {BERT-BiLSTM-CRF model, Miao medicine, intangible cultural
		  heritage, knowledge graph},
  location	= { },
  series	= {ICCSMT '24}
}

@InProceedings{	  10.5555/3291291.3291311,
  author	= {Boyer, John M.},
  title		= {Natural language question answering in the financial
		  domain},
  year		= {2018},
  publisher	= {IBM Corp.},
  address	= {USA},
  abstract	= {This paper describes a natural language question answering
		  system focused on answering financial domain questions
		  using a daily updated corpus of financial reports.
		  Financial entity types of interest included company stocks,
		  country bonds, currencies, industries, commodities, and
		  diversified assets. Financial questions of interest
		  included explanatory and factual questions about entities
		  as well as financial outlook for entities.An important
		  architectural divergence emerged between the approach
		  required for answering financial outlook questions versus
		  the approach for answering other financial information
		  questions. The financial domain focus also introduced
		  additional challenges to open domain natural language
		  processing that were addressed in the areas of document
		  ingestion, question classification accuracy, question
		  analysis techniques, speed of machine learning, answer
		  ranking by linguistic confidence versus temporality, and
		  system accuracy assessment.},
  booktitle	= {Proceedings of the 28th Annual International Conference on
		  Computer Science and Software Engineering},
  pages		= {189–200},
  numpages	= {12},
  keywords	= {financial domain, financial information retrieval,
		  financial sentiment analysis, question analysis, question
		  answering systems, question classification, text
		  analytics},
  location	= {Markham, Ontario, Canada},
  series	= {CASCON '18}
}

@InProceedings{	  10.1145/3531073.3531081,
  author	= {Fu, Bo and Steichen, Ben},
  title		= {Impending Success or Failure? An Investigation of
		  Gaze-Based User Predictions During Interaction with
		  Ontology Visualizations},
  year		= {2022},
  isbn		= {9781450397193},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3531073.3531081},
  doi		= {10.1145/3531073.3531081},
  abstract	= {Designing and developing innovative visualizations to
		  assist humans in the process of generating and
		  understanding complex semantic data has become an important
		  element in supporting effective human-ontology interaction,
		  as visual cues are likely to provide clarity, promote
		  insight, and amplify cognition. While recent research has
		  indicated potential benefits of applying novel adaptive
		  technologies, typical ontology visualization techniques
		  have traditionally followed a one-size-fits-all approach
		  that often ignores an individual user's preferences,
		  abilities, and visual needs. In an effort to realize
		  adaptive ontology visualization, this paper presents a
		  potential solution to predict a user's likely success and
		  failure in real time, and prior to task completion, by
		  applying established machine learning models on eye gaze
		  generated during an interactive session. These predictions
		  are envisioned to inform future adaptive ontology
		  visualizations that could potentially adjust its visual
		  cues or recommend alternative visualizations in real time
		  to improve individual user success. This paper presents
		  findings from a series of experiments to demonstrate the
		  feasibility of gaze-based success and failure predictions
		  in real time that can be achieved with a number of
		  off-the-shelf classifiers without the need of expert
		  configurations in the presence of mixed user backgrounds
		  and task domains across two commonly used fundamental
		  ontology visualization techniques.},
  booktitle	= {Proceedings of the 2022 International Conference on
		  Advanced Visual Interfaces},
  articleno	= {7},
  numpages	= {9},
  keywords	= {Eye Tracking, Ontology Visualization, Predictive
		  Analytics},
  location	= {Frascati, Rome, Italy},
  series	= {AVI '22}
}

@InProceedings{	  10.1145/3464385.3464744,
  author	= {Calvanese, Diego and Ding, Linfang and Mosca, Alessandro
		  and Xiao, Guohui},
  title		= {Realizing Ontology-based Reusable Interfaces for Data
		  Access via Virtual Knowledge Graphs},
  year		= {2021},
  isbn		= {9781450389778},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3464385.3464744},
  doi		= {10.1145/3464385.3464744},
  abstract	= {In this paper, we present a comprehensive framework, which
		  we call VKG-UI, for realizing ontology-based reusable user
		  interfaces (UIs) for data access via virtual knowledge
		  graphs (VKGs). The VKG approach uses an ontology to model
		  the domain of interest and to hide the heterogeneity of the
		  underlying data sources. Reusable UIs can be built by
		  relying on queries that are issued to the VKG system and
		  that use the high level vocabulary from the ontology layer.
		  This use of VKGs allows for decoupling the data from the
		  UIs, and brings great reusability in designing the latter.
		  To illustrate our approach, we introduce significant use
		  cases with various types of UIs, including programming,
		  graphic, natural language, and voice interfaces.},
  booktitle	= {Proceedings of the 14th Biannual Conference of the Italian
		  SIGCHI Chapter},
  articleno	= {35},
  numpages	= {5},
  keywords	= {data access, ontology, user interface, virtual knowledge
		  graph},
  location	= {Bolzano, Italy},
  series	= {CHItaly '21}
}

@InProceedings{	  10.1145/3487553.3524723,
  author	= {Yang, Sean T. and Howe, Bill},
  title		= {Surj: Ontological Learning for Fast, Accurate, and Robust
		  Hierarchical Multi-label Classification},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524723},
  doi		= {10.1145/3487553.3524723},
  abstract	= {We consider multi-label classification in the context of
		  complex hierarchical relationships organized into an
		  ontology. These situations are ubiquitous in learning
		  problems on the web and in science, where rich domain
		  models are developed but labeled data is rare. Most
		  existing solutions model the problem as a sequence of
		  simpler problems: one classifier for each level in the
		  hierarchy, or one classifier for each label. These
		  approaches require more training data, which is often
		  unavailable in practice: as the ontology grows in size and
		  complexity, it becomes unlikely to find training examples
		  for all expected combinations. In this paper, we learn
		  offline representations of the ontology using a graph
		  autoencoder and separately learn to classify input records,
		  reducing dependence on training data: Since the
		  relationships between labels are encoded independently of
		  training data, the model can make predictions even for
		  underrepresented labels, naturally generalize to
		  DAG-structured ontologies, remain robust to low-data
		  regimes, and, with minor offline retraining, tolerate
		  evolving ontologies. We show empirically that our label
		  predictions respect the hierarchy (predicting a descendant
		  implies predicting its ancestors) and propose a method of
		  evaluating hierarchy violations that properly ignores
		  irrelevant violations. Our main result is that our model
		  outperforms all state-of-the-art models on 17 of 20
		  datasets across multiple domains by a significant margin,
		  even with limited training data.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {1106–1114},
  numpages	= {9},
  keywords	= {Graph Learning, Hierarchical Multi-label classification,
		  Ontology Learning},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3659211.3659257,
  author	= {Tang, Hailin and Feng, Jun and Zhou, Siyuan},
  title		= {Generic Ontologies for Digital Watersheds},
  year		= {2024},
  isbn		= {9798400716669},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3659211.3659257},
  doi		= {10.1145/3659211.3659257},
  abstract	= {Digital Watershed represents an effective strategy for
		  addressing floods and mitigating their associated risks.
		  However, a notable challenge lies in the absence of a
		  universally applicable modeling approach for constructing
		  digital watersheds. The wealth of data and knowledge about
		  watersheds is currently managed in a fragmented manner,
		  impeding a comprehensive and cohesive understanding of the
		  subject. This paper addresses the fragmented control
		  landscape in watershed management by introducing generic
		  ontologies, including water conservancy object ontology,
		  model ontology, rainfall and runoff scene-mode ontology,
		  and event ontology. These ontologies standardize the
		  representation of water conservancy objects, hydrological
		  models, and expert knowledge while also defining structured
		  representations for physical events, scheduling rules, and
		  business processes, which contribute to breaking the
		  paradigm of "one watershed, one system" and facilitate
		  integrated flood prediction and scheduling.},
  booktitle	= {Proceedings of the 2023 4th International Conference on
		  Big Data Economy and Information Management},
  pages		= {260–266},
  numpages	= {7},
  location	= {Zhengzhou, China},
  series	= {BDEIM '23}
}

@Proceedings{	  10.1145/3627043,
  title		= {UMAP '24: Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  year		= {2024},
  isbn		= {9798400704338},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Cagliari, Italy}
}

@InProceedings{	  10.1145/3627673.3679662,
  author	= {Balsebre, Pasquale and Huang, Weiming and Cong, Gao and
		  Li, Yi},
  title		= {City Foundation Models for Learning General Purpose
		  Representations from OpenStreetMap},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679662},
  doi		= {10.1145/3627673.3679662},
  abstract	= {Pre-trained Foundation Models (PFMs) have ushered in a
		  paradigm-shift in AI, due to their ability to learn
		  general-purpose representations that can be readily
		  employed in downstream tasks. While PFMs have been
		  successfully adopted in various fields such as NLP and
		  Computer Vision, their capacity in handling geospatial data
		  remains limited. This can be attributed to the intrinsic
		  heterogeneity of such data, which encompasses different
		  types, including points, segments and regions, as well as
		  multiple information modalities. The proliferation of
		  Volunteered Geographic Information initiatives, like
		  OpenStreetMap, unveils a promising opportunity to bridge
		  this gap. In this paper, we present CityFM, a
		  self-supervised framework to train a foundation model
		  within a selected geographical area. CityFM relies solely
		  on open data from OSM, and produces multimodal
		  representations, incorporating spatial, visual, and textual
		  information. We analyse the entity representations
		  generated by our foundation models from a qualitative
		  perspective, and conduct experiments on road, building, and
		  region-level downstream tasks. In all the experiments,
		  CityFM achieves performance superior to, or on par with,
		  application-specific algorithms.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {87–97},
  numpages	= {11},
  keywords	= {contrastive learning, foundation models, geospatial data},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3460120.3485353,
  author	= {Christian, Ryan and Dutta, Sharmishtha and Park, Youngja
		  and Rastogi, Nidhi},
  title		= {An Ontology-driven Knowledge Graph for Android Malware},
  year		= {2021},
  isbn		= {9781450384544},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460120.3485353},
  doi		= {10.1145/3460120.3485353},
  abstract	= {We present MalONT2.0 -- an ontology for malware threat
		  intelligence [4]. New classes (attack patterns,
		  infrastructural resources to enable attacks, malware
		  analysis to incorporate static analysis, and dynamic
		  analysis of binaries) and relations have been added
		  following a broadened scope of core competency questions.
		  MalONT2.0 allows researchers to extensively capture all
		  requisite classes and relations that gather semantic and
		  syntactic characteristics of an android malware attack.
		  This ontology forms the basis for the malware threat
		  intelligence knowledge graph, MalKG, which we exemplify
		  using three different, non-overlapping demonstrations.
		  Malware features have been extracted from openCTI reports
		  on android threat intelligence shared on the Internet and
		  written in the form of unstructured text. Some of these
		  sources are blogs, threat intelligence reports, tweets, and
		  news articles. The smallest unit of information that
		  captures malware features is written as triples comprising
		  head and tail entities, each connected with a relation. In
		  the poster and demonstration, we discuss MalONT2.0 and
		  MalKG.},
  booktitle	= {Proceedings of the 2021 ACM SIGSAC Conference on Computer
		  and Communications Security},
  pages		= {2435–2437},
  numpages	= {3},
  keywords	= {inference, knowledge graphs, malware, ontology},
  location	= {Virtual Event, Republic of Korea},
  series	= {CCS '21}
}

@InProceedings{	  10.1145/3724154.3724251,
  author	= {Yu, Jie and Yao, Minghui},
  title		= {Multidimensional model for stock prediction using
		  investors’ commentary},
  year		= {2025},
  isbn		= {9798400711862},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3724154.3724251},
  doi		= {10.1145/3724154.3724251},
  abstract	= {Forecasting share prices has always been a major concern
		  in the financial sector, as several factors and investor's
		  comments influence share price movements have a tangible
		  impact on the stock market. The research is the
		  quantification of investor comments as indicators of
		  emotional tendencies and constructs multidimensional
		  feature sets to predict stock prices based on stock
		  fundamentals, technical aspects, and information aspects.
		  To enhance the correlation between the emotional tendencies
		  of stock investors and other characteristics, meanwhile, to
		  extract implicit information from these characteristics, we
		  have created a sliding window structure with Pearson
		  correlation, and channel convolution attention
		  modules(CCAM). This paper develops a Multidimensional
		  Fusion Model (MFM) consisting of four components: the
		  text-only model, the one-dimensional price model I, the
		  multidimensional price model II, and the data fusion
		  module. Multi-source heterogeneous data is used to
		  construct the dataset, which contains numerical data on
		  stock fundamentals and technical aspects, and text data on
		  information aspects, which collects investor comments
		  during a moving window period. Expert knowledge is used to
		  mark the emotional polarity of the text in the batch. The
		  constructed text-only model aims to reveal the implicit
		  relationship between the comment texts. To quantify the
		  emotional tendency of massive stock investors on a given
		  day, the Bert/ Enhanced Representation through Knowledge
		  Integration (ERNIE) of word embedding method and the
		  Convolutional Neural Networks (CNN)/ Deep Pyramid-CNN
		  (DPCNN)/ Region-CNN (RCNN) classifier are used models.
		  Model I (CEEMDAN-WOA-BiLSTM) analyses unidimensional stock
		  closing prices and interprets stock price fluctuations as
		  digital signals, whereas Model II (CCAM-Attention-BiLSTM
		  based on Pearson correlation data integration) utilizes
		  multidimensional data encompassing emotional tendencies. On
		  this basis, a constrained Data Fusion Module (DFM) is
		  proposed to process the prediction results of Model I and
		  Model II. For the Stock Index dataset 000881.SZ, covering
		  the period 2017-2023. The BERT-DPCNN-MFM model is one of
		  the most efficient MFMs produced, it obtained 0.0947,
		  0.0699, and 0.0090 results in RMSE, MAE, and MSE,
		  respectively. Simulation results verify the effectiveness
		  of the model, while proving that the model can reflect the
		  short-term fluctuation of stock prices.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Big Data Economy and Information Management},
  pages		= {582–593},
  numpages	= {12},
  keywords	= {CCAM-Attention-BiLSTM, Complete Ensemble Empirical Mode
		  Decomposition with Adaptive Noise (CEEMDAN), Pearson
		  correlation, Stock predictors, emotional tendencies},
  location	= { },
  series	= {BDEIM '24}
}

@InProceedings{	  10.1145/3587259.3627559,
  author	= {Martorana, Margherita and Kuhn, Tobias and Siebes, Ronald
		  and Van Ossenbruggen, Jacco},
  title		= {Advancing data sharing and reusability for restricted
		  access data on the Web: introducing the DataSet-Variable
		  Ontology},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627559},
  doi		= {10.1145/3587259.3627559},
  abstract	= {In response to the increasing volume of research data
		  being generated, more and more data portals have been
		  designed to facilitate data findability and accessibility.
		  However, a significant portion of this data remains
		  confidential or restricted due to its sensitive nature,
		  such as patient data or census microdata. While maintaining
		  confidentiality prohibits its public release, the emergence
		  of portals supporting rich metadata can help enable
		  researchers to at least discover the existence of
		  restricted access data, empowering them to assess the
		  suitability of the data before requesting access. Existing
		  standards, such as CSV on the Web and RDF Data Cube, have
		  been adopted to facilitate data management, integration,
		  and re-use of data on the Web. However, the current
		  landscape still lacks adequate standards not only to
		  effectively describe restricted access data while
		  preserving confidentiality but also to facilitate its
		  discovery. In this work, we investigate the relationship
		  between the structural, statistical, and semantic elements
		  of restricted access tabular data, and we explore how such
		  relationship can be formally modeled in a way that is
		  Findable, Accessible, Interoperable, and Reusable. We
		  introduce the DataSet-Variable Ontology (DSV), that by
		  combining CSV on the Web and RDF Data Cube standards,
		  leveraging semantic technologies and Linked Data
		  principles, and introducing variable-level metadata, aims
		  to capture high-quality metadata to support the management
		  and re-use of restricted access data on the Web. As
		  evaluation, we conducted a case study where we applied DSV
		  to four different datasets from different statistical
		  governmental agencies. We employed a set of competency
		  questions to assess the ontology’s ability to support
		  knowledge discovery and data exploration. By describing
		  high-quality metadata, both at the dataset- and variable
		  levels, while maintaining data privacy, this novel ontology
		  facilitates data interoperability, discovery, and re-use
		  and it empowers researchers to manage, integrate, and
		  analyze complex restricted access data sources.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {83–91},
  numpages	= {9},
  keywords	= {FAIR principles, Privacy-preserving web data, Restricted
		  access data, Semantic Web, Variable-level metadata},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@Article{	  10.1145/3625299,
  author	= {Hou, Wenjun and Bai, Bing and Cai, Chenyang},
  title		= {CR-TransR: A Knowledge Graph Embedding Model for Cultural
		  Domain},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {1},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3625299},
  doi		= {10.1145/3625299},
  abstract	= {As a combination of information computing technology and
		  the cultural field, cultural computing is gaining more
		  attention. The knowledge graph is also gradually applied as
		  a particular data structure in the cultural area. Based on
		  the domain knowledge graph data of the Beijing Municipal
		  Social Science Project “Mining and Utilization of
		  Cultural Resources in the Ancient Capital of Beijing,”
		  this article proposes a graph representation learning model
		  CR-TransR that integrates cultural attributes. Through the
		  analysis of the data in the cultural field of the ancient
		  capital of Beijing, a cultural feature dictionary is
		  constructed, and a domain-specific feature matrix is
		  constructed in the form of word vector splicing. The
		  feature matrix is used to constrain the embedding graph
		  model TransR, and then the feature matrix and the TransR
		  model are jointly trained to complete the embedded
		  expression of the knowledge graph. Finally, a comparative
		  experiment is carried out on the Beijing ancient capital
		  cultural knowledge graph dataset and the effects of the
		  classic graph embedding algorithms TransE, TransH, and
		  TransR. At the same time, we try to reproduce the embedding
		  method with the core idea of neighbor node information
		  aggregation as the core idea, and CRTransR are compared.
		  The experimental tasks include link prediction and triplet
		  classification, and the experimental results show that the
		  CRTransR model performs better.},
  journal	= {J. Comput. Cult. Herit.},
  month		= feb,
  articleno	= {12},
  numpages	= {11},
  keywords	= {Beijing ancient capital culture, knowledge graph, cultural
		  calculation, graph embedding}
}

@Proceedings{	  10.1145/3745533,
  title		= {CAMMIC '25: Proceedings of the 2025 5th International
		  Conference on Applied Mathematics, Modelling and
		  Intelligent Computing},
  year		= {2025},
  isbn		= {9798400713873},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3423334.3431452,
  author	= {Laddada, Wissame and Duchateau, Fabien and Favetta, Franck
		  and Moncla, Ludovic},
  title		= {Ontology-Based Approach for Neighborhood and Real Estate
		  Recommendations},
  year		= {2020},
  isbn		= {9781450381604},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3423334.3431452},
  doi		= {10.1145/3423334.3431452},
  abstract	= {Suggesting services or products to people is a task that
		  should be handled by recommendation systems due to the
		  important increase of information and the multitude of user
		  criteria. In fact, when expressing wishes for a product, a
		  user is influenced by his/her tastes or priorities. These
		  influential characteristics tend to be challenging
		  regarding their integration into recommendation systems,
		  because interaction between the products/services and the
		  user has to be captured through its preferences.
		  Recommendation systems for neighborhood and real estate
		  search are no exception, and to achieve reliable
		  recommendation, we developed an ontology NAREO
		  (Neighborhood And Real Estate Ontology) where environment
		  characteristics related to user preferences are modeled
		  with other geo-semantic descriptions. This ontology can be
		  enriched by SWRL (Semantic Web Rule Language) rules that
		  enhance the semantics of our knowledge base and allow
		  reasoning process through built-ins. To illustrate a use
		  case, we provide a basic set of predefined rules for the
		  recommendation context. User preferences are managed
		  through SPARQL queries taking into account the result of
		  inferences.},
  booktitle	= {Proceedings of the 4th ACM SIGSPATIAL Workshop on
		  Location-Based Recommendations, Geosocial Networks, and
		  Geoadvertising},
  articleno	= {4},
  numpages	= {10},
  keywords	= {Ontology, Recommendation Systems, SWRL reasoning, Spatial
		  modeling},
  location	= {Seattle, WA, USA},
  series	= {LocalRec'20}
}

@InProceedings{	  10.1145/3538969.3544453,
  author	= {Charalambous, Markos and Farao, Aristeidis and
		  Kalantzantonakis, George and Kanakakis, Panagiotis and
		  Salamanos, Nikos and Kotsifakos, Evangelos and Froudakis,
		  Evangellos},
  title		= {Analyzing Coverages of Cyber Insurance Policies Using
		  Ontology},
  year		= {2022},
  isbn		= {9781450396707},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3538969.3544453},
  doi		= {10.1145/3538969.3544453},
  abstract	= {In an era where all the transactions, businesses and
		  services are becoming digital and online, the data assets
		  and the services protection are of utmost importance.
		  Cyber-insurance companies are offering a wide range of
		  coverages, but they also have exclusions. Customers of
		  these companies need to be able to understand the terms and
		  conditions of the related contracts and furthermore they
		  need to be able to compare various offerings in order to
		  determine the most appropriate solutions for their needs.
		  The research in the area is very limited while at the same
		  time the related market is growing, giving every potential
		  solution a high value. In this paper, we propose a
		  methodology and a prototype system that will help customers
		  to compare contracts based on a pre-defined ontology that
		  is describing cyber-insurance terms. After a first
		  preliminary analysis and validation, our approach accuracy
		  is averaging at almost 50\%, giving a promising initial
		  evaluation. Fine tuning, larger data set assessment and
		  ontology refinement will be our next steps to improve the
		  accuracy of our tool. Real user evaluation will follow, in
		  order to evaluate the tool in real world cases.},
  booktitle	= {Proceedings of the 17th International Conference on
		  Availability, Reliability and Security},
  articleno	= {108},
  numpages	= {7},
  keywords	= {Coverages, Cyber-insurance, Exclusions, Ontology, Premium,
		  Weakest link},
  location	= {Vienna, Austria},
  series	= {ARES '22}
}

@Proceedings{	  10.1145/3628034,
  title		= {EuroPLoP '23: Proceedings of the 28th European Conference
		  on Pattern Languages of Programs},
  year		= {2023},
  isbn		= {9798400700408},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Irsee, Germany}
}

@InProceedings{	  10.1145/3656766.3656929,
  author	= {Xu, Lianzheng and Fu, Deqian and Qiu, Jianlong},
  title		= {Trusted Non-intrusive Data Exchange based on Ontology in
		  Logistics Industry},
  year		= {2024},
  isbn		= {9798400716478},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3656766.3656929},
  doi		= {10.1145/3656766.3656929},
  abstract	= {The logistics industry is becoming increasingly important
		  in our daily lives, leading to a growing demand for
		  digitalization within the sector. However, due to concerns
		  about data privacy, logistics entities have formed natural
		  information silos, which have made logistics data difficult
		  to exchange and share. To address this issue, this paper
		  proposes an ontology-based logistics data exchange model
		  that utilizes blockchain technology to establish user trust
		  among information silos. The model in this paper fuses
		  non-intrusive data access and Ontology-Based Data Access
		  (OBDA) to achieve both precise control over the data access
		  process and interconnection among heterogeneous systems.
		  The proposed model creates a new trustworthy and
		  controllable solution for the circulation of logistics data
		  within the logistics industry, without the need for the
		  secondary development of logistics information systems.},
  booktitle	= {Proceedings of the 2023 3rd International Conference on
		  Big Data, Artificial Intelligence and Risk Management},
  pages		= {986–991},
  numpages	= {6},
  location	= {Chengdu, China},
  series	= {ICBAR '23}
}

@Article{	  10.1145/3626961,
  author	= {Roy, Chiradeep and Nourani, Mahsan and Arya, Shivvrat and
		  Shanbhag, Mahesh and Rahman, Tahrima and Ragan, Eric D. and
		  Ruozzi, Nicholas and Gogate, Vibhav},
  title		= {Explainable Activity Recognition in Videos using Deep
		  Learning and Tractable Probabilistic Models},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {4},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3626961},
  doi		= {10.1145/3626961},
  abstract	= {We consider the following video activity recognition (VAR)
		  task: given a video, infer the set of activities being
		  performed in the video and assign each frame to an
		  activity. Although VAR can be solved accurately using
		  existing deep learning techniques, deep networks are
		  neither interpretable nor explainable and as a result their
		  use is problematic in high stakes decision-making
		  applications (in healthcare, experimental Biology,
		  aviation, law, etc.). In such applications, failure may
		  lead to disastrous consequences and therefore it is
		  necessary that the user is able to either understand the
		  inner workings of the model or probe it to understand its
		  reasoning patterns for a given decision. We address these
		  limitations of deep networks by proposing a new approach
		  that feeds the output of a deep model into a tractable,
		  interpretable probabilistic model called a dynamic
		  conditional cutset network that is defined over the
		  explanatory and output variables and then performing joint
		  inference over the combined model. The two key benefits of
		  using cutset networks are: (a) they explicitly model the
		  relationship between the output and explanatory variables
		  and as a result, the combined model is likely to be more
		  accurate than the vanilla deep model and (b) they can
		  answer reasoning queries in polynomial time and as a
		  result, they can derive meaningful explanations by
		  efficiently answering explanation queries. We demonstrate
		  the efficacy of our approach on two datasets, Textually
		  Annotated Cooking Scenes (TACoS), and wet lab, using
		  conventional evaluation measures such as the Jaccard Index
		  and Hamming Loss, as well as a human-subjects study.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= dec,
  articleno	= {29},
  numpages	= {32},
  keywords	= {Temporal models, dynamic Bayesian networks, cutset
		  networks, tractable probabilistic models}
}

@Article{	  10.1109/tcbb.2024.3377928,
  author	= {Palacio, Ana Le\'{o}n and S., Alberto Garc\'{\i}a and
		  Rom\'{a}n, Jos\'{e} Fabi\'{a}n Reyes and Costa, Mireia and
		  Pastor, Oscar},
  title		= {The Delfos Platform: A Conceptual Model-Based Solution for
		  the Enhancement of Precision Medicine},
  year		= {2024},
  issue_date	= {Sept.-Oct. 2024},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {21},
  number	= {5},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2024.3377928},
  doi		= {10.1109/TCBB.2024.3377928},
  abstract	= {The use in the clinical practice of the vast amount of
		  genomic data generated by current sequencing technologies
		  constitutes a bottleneck for the progress of Precision
		  Medicine (PM). Various problems inherent to the genomics
		  domain (i.e., dispersion, heterogeneity, discrepancies,
		  lack of standardization, and data quality issues) remain
		  unsolved. In this paper, we present the Delfos platform, a
		  conceptual model-based solution developed following a
		  rigorous methodological and ontological background, whose
		  main aim is to minimize the impact of these problems when
		  transferring the research results to clinical practice.
		  This paper presents the SILE method that provides
		  methodological support for the Delfos platform, the
		  Conceptual Schema of the Genome that provides a shared
		  understanding of the domain, and the technological
		  architecture behind the implementation of the platform.
		  This paper also exemplifies the use of the Delfos platform
		  through two use cases that involve the study of the DNA
		  variants associated with the risk of developing Dilated
		  Cardiomyopathies and Neuroblastoma.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= mar,
  pages		= {1242–1253},
  numpages	= {12}
}

@InProceedings{	  10.5555/3539845.3539968,
  author	= {Herrmann, Martin and Witt, Christian and Lake, Laureen and
		  Guneshka, Stefani and Heinzemann, Christian and Bonarens,
		  Frank and Feifel, Patrick and Funke, Simon},
  title		= {Using ontologies for dataset engineering in automotive AI
		  applications},
  year		= {2022},
  isbn		= {9783981926361},
  publisher	= {European Design and Automation Association},
  address	= {Leuven, BEL},
  abstract	= {Basis of a robust safety strategy for an automated driving
		  function based on neural networks is a detailed description
		  of its input domain, i.e. a description of the environment,
		  in which the function is used. This is required to describe
		  its functional system boundaries and to perform a
		  comprehensive safety analysis. Moreover, it allows to
		  tailor datasets specifically designed for safety related
		  validation tests. Ontologies fulfill the task to gather
		  expert knowledge and model information to enable computer
		  aided processing, while using a notion understandable for
		  humans. In this contribution, we propose a methodology for
		  domain analysis to build up an ontology for perception of
		  autonomous vehicles including characteristic features that
		  become important when dealing with neural networks.
		  Additionally, the method is demonstrated by the creation of
		  a synthetic test dataset for an Euro NCAP-like use case.},
  booktitle	= {Proceedings of the 2022 Conference \&amp; Exhibition on
		  Design, Automation \&amp; Test in Europe},
  pages		= {526–531},
  numpages	= {6},
  keywords	= {artificial intelligence, autonomous driving, dataset
		  engineering, neural network, ontology},
  location	= {Antwerp, Belgium},
  series	= {DATE '22}
}

@InProceedings{	  10.1145/3318464.3386139,
  author	= {Quamar, Abdul and Lei, Chuan and Miller, Dorian and Ozcan,
		  Fatma and Kreulen, Jeffrey and Moore, Robert J. and
		  Efthymiou, Vasilis},
  title		= {An Ontology-Based Conversation System for Knowledge
		  Bases},
  year		= {2020},
  isbn		= {9781450367356},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318464.3386139},
  doi		= {10.1145/3318464.3386139},
  abstract	= {Domain-specific knowledge bases (KB), carefully curated
		  from various data sources, provide an invaluable reference
		  for professionals. Conversation systems make these KBs
		  easily accessible to professionals and are gaining
		  popularity due to recent advances in natural language
		  understanding and AI. Despite the increasing use of various
		  conversation systems in open-domain applications, the
		  requirements of a domain-specific conversation system are
		  quite different and challenging. In this paper, we propose
		  an ontology-based conversation system for domain-specific
		  KBs. In particular, we exploit the domain knowledge
		  inherent in the domain ontology to identify user intents,
		  and the corresponding entities to bootstrap the
		  conversation space. We incorporate the feedback from domain
		  experts to further refine these patterns, and use them to
		  generate training samples for the conversation model,
		  lifting the heavy burden from the conversation designers.
		  We have incorporated our innovations into a conversation
		  agent focused on healthcare as a feature of the IBM
		  Micromedex product.},
  booktitle	= {Proceedings of the 2020 ACM SIGMOD International
		  Conference on Management of Data},
  pages		= {361–376},
  numpages	= {16},
  keywords	= {conversation systems, knowledge bases, ontology-driven},
  location	= {Portland, OR, USA},
  series	= {SIGMOD '20}
}

@InProceedings{	  10.1145/3570748.3570760,
  author	= {Kuchii, Kanta and Kondo, Takao and Teraoka, Fumio},
  title		= {KANVAS: A Network Information Sharing Framework Based on
		  Network Ontology Bonsai},
  year		= {2022},
  isbn		= {9781450399814},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570748.3570760},
  doi		= {10.1145/3570748.3570760},
  abstract	= {Demands for acquiring Internet behavior are increasing for
		  Internet-scale network understanding such as inter-AS path
		  management and traffic engineering. Although there are
		  several efforts to make Internet behavior public, most of
		  the public information is not structured and it is hard for
		  applications to use such information. This paper proposes a
		  network information sharing framework called KANVAS. It
		  defines a network ontology called Bonsai which models
		  network structure from viewpoints of physical, logical,
		  service, and operation network structures. Bonsai can
		  express network virtualization technologies such as link
		  aggregation (LAG), VLAN, L2 over L3 tunneling, and virtual
		  routing and forwarding (VRF). Applications can access
		  network information via useful API. As a first step of
		  development of KANVAS and Bonsai, this paper describes
		  network information sharing within a single domain focusing
		  on failure localization and throughput monitoring as
		  examples. Evaluation results on a PoC system show that the
		  time for failure localization is short enough and a
		  throughput monitoring tool can choose appropriate
		  monitoring points.},
  booktitle	= {Proceedings of the 17th Asian Internet Engineering
		  Conference},
  pages		= {79–87},
  numpages	= {9},
  keywords	= {fault localization, network management, network ontology,
		  traffic monitoring},
  location	= {Hiroshima, Japan},
  series	= {AINTEC '22}
}

@Proceedings{	  10.1145/3649158,
  title		= {SACMAT 2024: Proceedings of the 29th ACM Symposium on
		  Access Control Models and Technologies},
  year		= {2024},
  isbn		= {9798400704918},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the 29th ACM
		  Symposium on Access Control Models and Technologies (SACMAT
		  2024). This year's symposium continues its tradition of
		  being the premier venue for presenting research results and
		  experience reports on cutting edge advances on access
		  control, including models, systems, applications, and
		  theory, while also embracing an expanded focus on the
		  general area of computer and information security and
		  privacy. The overarching goal of the symposium is to share
		  novel access control and computer security solutions that
		  fulfill the needs of emerging applications and
		  environments, and also to identify new directions for
		  future research and development. ACM SACMAT provides
		  researchers and also practitioners with a unique
		  opportunity to share their perspectives with others
		  interested in the various aspects of access control and
		  computer security.},
  location	= {San Antonio, TX, USA}
}

@InProceedings{	  10.1145/3377170.3377279,
  author	= {Wilson, R. S. I. and Ginige, Athula and Goonetillake, J.
		  S. and Indika, W. A.},
  title		= {User Needs-driven Enrichment of Ontology: A case study in
		  Sri Lankan Agriculture},
  year		= {2020},
  isbn		= {9781450376631},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3377170.3377279},
  doi		= {10.1145/3377170.3377279},
  abstract	= {This study describes the mobile-based user needs-driven
		  knowledge management system that supports the decision
		  making process by considering user needs and preferences.
		  Agriculture is one of the domains, in which, users seek
		  specific information and knowledge relevant to their needs
		  rather than searching and accessing general information
		  from the Web, books, magazines or other information
		  sources. Thus, the conceptualized solution was created by
		  applying participatory sensing, natural language processing
		  and ontology theories and techniques in a novel way in
		  order to satisfy the user needs. The user-centered
		  agriculture ontology that has been developed in our
		  previous work is extended to make an up-to-date knowledge
		  base by capturing user needs and preferences through
		  participatory sensing. The methods of ontology evolution
		  from unstructured data were analyzed to build a technique
		  to enrich the user-centered ontology. The Modified Delphi
		  method is used for verifying the correctness and relevancy
		  of the ontology and the application-based evaluation is
		  applied for checking the functional correctness of the
		  system.},
  booktitle	= {Proceedings of the 2019 7th International Conference on
		  Information Technology: IoT and Smart City},
  pages		= {581–586},
  numpages	= {6},
  keywords	= {Agriculture, Knowledge base, Ontology enrichment,
		  Participatory sensing, Question Answering, User needs in
		  context},
  location	= {Shanghai, China},
  series	= {ICIT '19}
}

@InProceedings{	  10.1145/3724979.3725047,
  author	= {Zhang, Ying and Liu, Yuan and Pan, Xiaoyong and Shen,
		  Hongbin},
  title		= {Ocean archaea PPI prediction with pretraining models},
  year		= {2025},
  isbn		= {9798400712203},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3724979.3725047},
  doi		= {10.1145/3724979.3725047},
  abstract	= {Protein-Protein Interaction (PPI) provides important
		  insights into the metabolic mechanisms of different
		  biological processes. Although PPIs in some organisms have
		  been investigated systematically, PPIs in the ocean archaea
		  remain largely unexplored. But such species have special
		  investigation value since their adaptation to extreme
		  living conditions may generate unique PPIs. In this paper,
		  we aim to characterize and predict PPIs in ocean archaea to
		  advance understanding of their metabolic networks. First,
		  we collect all ocean archaea PPIs with high confidence from
		  STRING database and analyze the PPI network features,
		  including centrality and enrichment analysis. The
		  functional enrichment results of the largest connecting
		  subgraph in the PPI network show most PPIs in our
		  constructed dataset is related to the translation and
		  transcription processes. Then, we generate an equal number
		  of negative PPI pairs, whose members have either different
		  subcellular locations or GO terms. We also use the
		  generated dataset to test the performance of three
		  pretraining methods and their ensemble methods in the
		  binary PPI prediction task. Our results suggest the
		  ensemble methods could be applied to further improve
		  models’ performance. Fine-tuned models trained on the
		  ocean archaea dataset are expected to predict the other
		  ocean archaea PPIs that are not included in the STRING
		  database and get more understanding about the ocean archaea
		  PPI universe.},
  booktitle	= {Proceedings of the 2025 5th International Conference on
		  Bioinformatics and Intelligent Computing},
  pages		= {440–445},
  numpages	= {6},
  keywords	= {Binary PPI prediction, Ocean archaea, PPI network,
		  Pretraining model},
  location	= { },
  series	= {BIC '25}
}

@InProceedings{	  10.1145/3589334.3645584,
  author	= {Shi, Jingchuan and Dong, Hang and Chen, Jiaoyan and Wu,
		  Zhe and Horrocks, Ian},
  title		= {Taxonomy Completion via Implicit Concept Insertion},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645584},
  doi		= {10.1145/3589334.3645584},
  abstract	= {beginabstract High quality taxonomies play a critical role
		  in various domains such as e-commerce, web search and
		  ontology engineering. While there has been extensive work
		  on expanding taxonomies from externally mined data, there
		  has been less attention paid to enriching taxonomies by
		  exploiting existing concepts and structure within the
		  taxonomy. In this work, we show the usefulness of this kind
		  of enrichment, and explore its viability with a new
		  taxonomy completion system ICON (I mplicit CON cept
		  Insertion). ICON generates new concepts by identifying
		  implicit concepts based on the existing concept structure,
		  generating names for such concepts and inserting them in
		  appropriate positions within the taxonomy. ICON integrates
		  techniques from entity retrieval, text summary, and
		  subsumption prediction; this modular architecture offers
		  high flexibility while achieving state-of-the-art
		  performance. We have evaluated ICON on two e-commerce
		  taxonomies, and the results show that it offers significant
		  advantages over strong baselines including recent taxonomy
		  completion models and the large language model, ChatGPT.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2159–2169},
  numpages	= {11},
  keywords	= {ontology engineering, pre-trained language model, taxonomy
		  completion, taxonomy enrichment, text summarisation},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3278293.3278302,
  author	= {Zhao, Grace and Zhang, Xiaowen},
  title		= {Domain-Specific Ontology Concept Extraction and Hierarchy
		  Extension},
  year		= {2018},
  isbn		= {9781450365512},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3278293.3278302},
  doi		= {10.1145/3278293.3278302},
  abstract	= {The domain-specific vernaculars and notations have been a
		  hurdle to automatic ontology building and augmentation,
		  since most of the ontology learning methods are essentially
		  based on the natural language studies and lexicosyntactic
		  pattern explorations. This paper proposes two robust
		  approaches to ontology hierarchical enhancement, in
		  particular, adding new terms to the ontology graph. We
		  designed our learning models from a computational vantage
		  point, examining the inter-relationship between documents,
		  ontology dictionary terms, and the graph structure of the
		  seed ontology. We then take advantage of late studies of
		  neural networks and machine learning to perform
		  classification over the inter-related data, and insert the
		  new term at the most desirable nodal place on the domain
		  ontology graph.},
  booktitle	= {Proceedings of the 2nd International Conference on Natural
		  Language Processing and Information Retrieval},
  pages		= {60–64},
  numpages	= {5},
  keywords	= {Domain Knowledge Learning, Ontology Engineering, Ontology
		  Hierarchy Extension, Supervised Machine Learning},
  location	= {Bangkok, Thailand},
  series	= {NLPIR '18}
}

@InProceedings{	  10.1145/3685651.3686700,
  author	= {Haverinen, Henry and Janhunen, Tomi and
		  P\"{a}iv\"{a}rinta, Tero and Lempinen, Sami and Kaartinen,
		  Suvi and Meril\"{a}, Sami},
  title		= {Automating Cybersecurity Compliance in DevSecOps with Open
		  Information Model for Security as Code},
  year		= {2024},
  isbn		= {9798400709845},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3685651.3686700},
  doi		= {10.1145/3685651.3686700},
  abstract	= {Software development teams meet increasing requirements to
		  implement cybersecurity management in compliance with
		  standards and regulations. However, adopting a compliant
		  cybersecurity management system and DevSecOps practices as
		  part of a software development process has turned out to be
		  tedious and expensive in practice. Open-source communities
		  and open ecosystems, which lack tools and realistic
		  practices for compliant cybersecurity management, face
		  these difficulties as well. This paper suggests a set of
		  requirements and a solution that are based on long-term
		  experience in adopting standard compliant DevSecOps
		  processes in industry. The proposed solution, called
		  Cyberismo, facilitates the adoption of compliance and
		  cybersecurity management, improves collaboration on
		  cybersecurity in company internal projects, cross-company
		  projects, and open-source projects, and automates the
		  compliance and cybersecurity management in software
		  development by way of an open information model
		  representation format, and an open-source tool to manage
		  the information model. As the information model uses a
		  simple plain text format that can be managed by automated
		  DevSecOps tool chains, it can be understood as an instance
		  of the Everything as Code and Security as Code paradigms.
		  The proposed solution is designed as modular, tailorable to
		  the organisation and its existing tools, and flexible
		  enough to model both process- and technology-related
		  information. It automates both the validation of how
		  compliance requirements have been met and the gathering and
		  archiving of evidence of compliance. The information model
		  is mapped to a logic program conforming to the Answer Set
		  Programming (ASP) paradigm for knowledge representation.
		  The mapping enables flexible query evaluation and
		  reasoning, including the calculation of performance
		  measures and automated policy checks. However, developers,
		  product owners and other end-users of the solution do not
		  necessarily need to know how to write logic programs, as
		  logic programs can be encapsulated in content modules made
		  available for the users. By putting the ease of adoption of
		  compliant DevSecOps processes by the practitioners in the
		  spotlight, this paper concludes that it is both necessary
		  and possible to meet all the proposed requirements.},
  booktitle	= {Proceedings of the 4th Eclipse Security, AI, Architecture
		  and Modelling Conference on Data Space},
  pages		= {93–102},
  numpages	= {10},
  keywords	= {Cybersecurity management, DevOps, DevSecOps, Everything as
		  Code, Security as Code, answer set programming, compliance,
		  knowledge representation and reasoning, open source
		  security, process adoption},
  location	= {Mainz, Germany},
  series	= {eSAAM '24}
}

@InProceedings{	  10.1145/3564719.3568689,
  author	= {Butting, Arvid and Kirchhof, J\"{o}rg Christian and
		  Kleiss, Anno and Michael, Judith and Orlov, Radoslav and
		  Rumpe, Bernhard},
  title		= {Model-Driven IoT App Stores: Deploying Customizable
		  Software Products to Heterogeneous Devices},
  year		= {2022},
  isbn		= {9781450399203},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3564719.3568689},
  doi		= {10.1145/3564719.3568689},
  abstract	= {Internet of Things (IoT) devices and the software they
		  execute are often strongly coupled with vendors
		  preinstalling their software at the factory. Future IoT
		  applications are expected to be distributed via app stores.
		  A strong coupling between hard- and software hinders the
		  rise of such app stores. Existing model-driven approaches
		  for developing IoT applications focus largely on the
		  behavior specification and message exchange but generate
		  code that targets a specific set of devices. By raising the
		  level of abstraction, models can be utilized to decouple
		  hard- and software and adapt to various infrastructures. We
		  present a concept for a model-driven app store that
		  decouples hardware and software development of product
		  lines of IoT applications.},
  booktitle	= {Proceedings of the 21st ACM SIGPLAN International
		  Conference on Generative Programming: Concepts and
		  Experiences},
  pages		= {108–121},
  numpages	= {14},
  keywords	= {App Store, Architecture Description Language, Internet of
		  Things, Low-Code, Model-Driven Engineering},
  location	= {Auckland, New Zealand},
  series	= {GPCE 2022}
}

@Article{	  10.1109/tcbb.2024.3429234,
  author	= {Mundotiya, Rajesh Kumar and Priya, Juhi and Kuwarbi, Divya
		  and Singh, Teekam},
  title		= {Enhancing Generalizability in Biomedical Entity
		  Recognition: Self-Attention PCA-CLS Model},
  year		= {2024},
  issue_date	= {Nov.-Dec. 2024},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {21},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2024.3429234},
  doi		= {10.1109/TCBB.2024.3429234},
  abstract	= {One of the primary tasks in the early stages of data
		  mining involves the identification of entities from
		  biomedical corpora. Traditional approaches relying on
		  robust feature engineering face challenges when learning
		  from available (un-)annotated data using data-driven models
		  like deep learning-based architectures. Despite leveraging
		  large corpora and advanced deep learning models, domain
		  generalization remains an issue. Attention mechanisms are
		  effective in capturing longer sentence dependencies and
		  extracting semantic and syntactic information from limited
		  annotated datasets. To address out-of-vocabulary challenges
		  in biomedical text, the PCA-CLS (Position and Contextual
		  Attention with CNN-LSTM-Softmax) model combines global
		  self-attention and character-level convolutional neural
		  network techniques. The model's performance is evaluated on
		  eight distinct biomedical domain datasets encompassing
		  entities such as genes, drugs, diseases, and species. The
		  PCA-CLS model outperforms several state-of-the-art models,
		  achieving notable F&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$_{1}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic
		  xlink:href="mundotiya-ieq1-3429234.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-scores,
		  including 88.19% on BC2GM, 85.44% on JNLPBA, 90.80% on
		  BC5CDR-chemical, 87.07% on BC5CDR-disease, 89.18% on
		  BC4CHEMD, 88.81% on NCBI, and 91.59% on the s800 dataset.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jul,
  pages		= {1934–1941},
  numpages	= {8}
}

@InProceedings{	  10.1109/ase56229.2023.00019,
  author	= {Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi,
		  Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and
		  Kaulgud, Vikrant},
  title		= {Smart Prompt Advisor: Multi-Objective Prompt Framework for
		  Consistency and Best Practices},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00019},
  doi		= {10.1109/ASE56229.2023.00019},
  abstract	= {Recent breakthroughs in Large Language Models (LLM),
		  comprised of billions of parameters, have achieved the
		  ability to unveil exceptional insight into a wide range of
		  Natural Language Processing (NLP) tasks. The onus of the
		  performance of these models lies in the sophistication and
		  completeness of the input prompt. Minimizing the
		  enhancement cycles of prompt with improvised keywords
		  becomes critically important as it directly affects the
		  time to market and cost of the developing solution.
		  However, this process inevitably has a trade-off between
		  the learning curve/proficiency of the user and completeness
		  of the prompt, as generating such a solutions is an
		  incremental process. In this paper, we have designed a
		  novel solution and implemented it in the form of a plugin
		  for Visual Studio Code IDE, which can optimize this
		  trade-off, by learning the underlying prompt intent to
		  enhance with keywords. This will tend to align with
		  developers' collection of semantics while developing a
		  secure code, ensuring parameter and local variable names,
		  return expressions, simple pre and post-conditions, and
		  basic control and data flow are met.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {1846–1848},
  numpages	= {3},
  keywords	= {prompt engineering, artificial intelligence, deep
		  learning, LLM, ontology},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@InProceedings{	  10.1145/3575813.3595190,
  author	= {Mavrokapnidis, Dimitris and Fierro, Gabe and Korolija,
		  Ivan and Rovas, Dimitrios},
  title		= {A Programming Model for Portable Fault Detection and
		  Diagnosis},
  year		= {2023},
  isbn		= {9798400700323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3575813.3595190},
  doi		= {10.1145/3575813.3595190},
  abstract	= {Portable applications support the write once, deploy
		  everywhere paradigm. This paradigm is particularly
		  attractive in building applications, where current practice
		  involves the manual deployment and configuration of such
		  applications, requiring significant engineering effort and
		  concomitant costs. This is a tedious and error-prone
		  process which does not scale well. Notwithstanding recent
		  advances in semantic data modelling that allow a unified
		  representation of buildings, we still miss a paradigm for
		  deploying portable building applications at scale. This
		  paper introduces a portable programming model for such
		  applications, which we examine in the context of
		  Fault-Detection and Diagnosis (FDD). In particular, we look
		  at the separation of the FDD logic and the configuration
		  with specific data inputs. We architect a software system
		  that enables their self-configuration and execution across
		  various building configurations, expressed in terms of
		  Brick metadata models. Our initial results from authoring
		  and executing APAR (AHU Performance Assessment Rules) on
		  multiple AHUs of two museums demonstrate the potential of
		  our model to reduce repetitive tasks and deployment costs
		  of FDD applications.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Future Energy Systems},
  pages		= {127–131},
  numpages	= {5},
  keywords	= {Brick, FDD, Metadata, Ontologies, Portability,
		  Programming, RDF, SHACL, Scalability, Semantic Web},
  location	= {Orlando, FL, USA},
  series	= {e-Energy '23}
}

@InProceedings{	  10.1145/3587716.3587723,
  author	= {Huang, Xinyi and Cheng, Lianglun and Deng, Jianfeng and
		  Wang, Tao},
  title		= {Binocular attention-based stacked BiLSTM NER model for
		  Supply chain management event knowledge graph
		  construction},
  year		= {2023},
  isbn		= {9781450398411},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587716.3587723},
  doi		= {10.1145/3587716.3587723},
  abstract	= {Extracting fine-grained event ontology knowledge based on
		  supply chain management (SCM) related corpus and
		  constructing knowledge graph (KG) has important guiding
		  significance and knowledge support for the efficient
		  implementation and development of SCM in manufacturing
		  enterprises. Recently, research on the KG of SCM has not
		  gained sufficient attention. This paper aims to propose an
		  event logical KG construction approach for SCM.
		  Specifically, a stacked BiLSTM entity recognition model
		  based on the binocular attention mechanism is proposed,
		  called the SBBAN model. Firstly, the character feature
		  attention mechanism is used to infer the key information
		  that contributes greatly to entity recognition in the text
		  sequence. Character weighted features and character
		  features splicing are used as new character input features.
		  Then the deep semantic abstract features of text sequence
		  are obtained by stacked BiLSTM. In addition, a
		  self-attention mechanism is added to obtain the deep
		  context relevant features. Experimental results show that
		  the model shows better performance in in comparison with
		  the state-of-the-art algorithms to complete the matching of
		  event argument entities and offer knowledge support for
		  SCM.},
  booktitle	= {Proceedings of the 2023 15th International Conference on
		  Machine Learning and Computing},
  pages		= {40–46},
  numpages	= {7},
  keywords	= {event logic knowledge graph, named entity recognition,
		  stacked BiLSTM, supply chain management ontology},
  location	= {Zhuhai, China},
  series	= {ICMLC '23}
}

@Article{	  10.1145/3329124,
  author	= {McDaniel, Melinda and Storey, Veda C.},
  title		= {Evaluating Domain Ontologies: Clarification,
		  Classification, and Challenges},
  year		= {2019},
  issue_date	= {July 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3329124},
  doi		= {10.1145/3329124},
  abstract	= {The number of applications being developed that require
		  access to knowledge about the real world has increased
		  rapidly over the past two decades. Domain ontologies, which
		  formalize the terms being used in a discipline, have become
		  essential for research in areas such as Machine Learning,
		  the Internet of Things, Robotics, and Natural Language
		  Processing, because they enable separate systems to
		  exchange information. The quality of these domain
		  ontologies, however, must be ensured for meaningful
		  communication. Assessing the quality of domain ontologies
		  for their suitability to potential applications remains
		  difficult, even though a variety of frameworks and metrics
		  have been developed for doing so. This article reviews
		  domain ontology assessment efforts to highlight the work
		  that has been carried out and to clarify the important
		  issues that remain. These assessment efforts are classified
		  into five distinct evaluation approaches and the state of
		  the art of each described. Challenges associated with
		  domain ontology assessment are outlined and recommendations
		  are made for future research and applications.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {70},
  numpages	= {44},
  keywords	= {Ontology, applied ontology, assessment, domain ontology,
		  evaluation, metrics, ontology application, ontology
		  development, task-ontology fit}
}

@InProceedings{	  10.1145/3543434.3543590,
  author	= {Ortiz-Rodriguez, Fernando and Tiwari, Sanju and Panchal,
		  Ronak and Medina-Quintero, Jose Melchor and Barrera,
		  Ruben},
  title		= {MEXIN: Multidialectal Ontology supporting NLP approach to
		  improve government electronic communication with the
		  Mexican Ethnic Groups},
  year		= {2022},
  isbn		= {9781450397490},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543434.3543590},
  doi		= {10.1145/3543434.3543590},
  abstract	= {The government services usually target all citizens, but
		  sometimes physical services nor technology-based services
		  do not cover all people. This research aims to tackle
		  services given to underrepresented citizens in Mexico
		  (Indigenous people) and apply NLP techniques supported by
		  ontologies to achieve accurate translation to most dialects
		  spoken in Mexico. The scope of this paper only tests with
		  Mayan dialect spoken primarily in the Mexican peninsula.},
  booktitle	= {Proceedings of the 23rd Annual International Conference on
		  Digital Government Research},
  pages		= {461–463},
  numpages	= {3},
  keywords	= {NLP, Ontologies, Semantic Web, eGovernment},
  location	= {Virtual Event, Republic of Korea},
  series	= {dg.o '22}
}

@InProceedings{	  10.5555/3712729.3712933,
  author	= {Jungmann, Michelle and Lazarova-Molnar, Sanja},
  title		= {Fusing Expert Knowledge and Data for Simulation Model
		  Discovery in Digital Twins: A Case Study from Reliability
		  Modeling},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {Integrating expert knowledge in data-driven Digital Twins
		  can lead to better-informed underlying models. Achieving
		  systematic integration, however, remains a complex
		  challenge. In this study, we propose an initial approach
		  for hybrid model extraction by systematically fusing expert
		  knowledge statements with Internet of Things data from
		  manufacturing systems, such as event and state logs. We
		  outline two main strategies to facilitate the fusion of
		  data and expert knowledge in a systematic way. We,
		  furthermore, present a case study in reliability assessment
		  of manufacturing systems showcasing our methodology within
		  this specific domain. Using our four fusion algorithms, we
		  automatically extract reliability models from both data and
		  expert knowledge statements. Finally, we conduct a
		  comprehensive analysis of the results and draw conclusions
		  regarding the efficacy of the fusion algorithms for Digital
		  Twin model extractions.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2463–2474},
  numpages	= {12},
  location	= {Orlando, Florida, USA},
  series	= {WSC '24}
}

@InProceedings{	  10.1145/3590777.3590783,
  author	= {Putrevu, Venkata Sai Charan and Chunduri, Hrushikesh and
		  Putrevu, Mohan Anand and Shukla, Sandeep},
  title		= {A Framework for Advanced Persistent Threat Attribution
		  using Zachman Ontology},
  year		= {2023},
  isbn		= {9781450398299},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3590777.3590783},
  doi		= {10.1145/3590777.3590783},
  abstract	= {Advanced Persistent Threat (APT) is a type of cyber attack
		  that infiltrates a targeted organization and exfiltrates
		  sensitive data over an extended period of time or to cause
		  sabotage. Recently, there has been a trend of nation states
		  backing APT groups in order to further their political and
		  financial interests, making the APT attribution process
		  increasingly important. The APT attribution process
		  involves identifying the actors behind an attack and their
		  motivations, using a method of logical inference called
		  abductive reasoning to determine the most likely
		  explanation for a set of observations. While various
		  attribution methods and frameworks have been proposed by
		  the security community, many of them lack granularity and
		  are dependent on the skills of practitioners rather than a
		  standardized process. This can hinder both the
		  understandability and reproducibility of attribution
		  efforts as this process is practiced but not engineered. To
		  address these issues, we propose a new framework for the
		  APT attribution process based on the Zachman ontology,
		  which offers greater granularity by posing specific
		  primitive questions at various levels of the attribution
		  process. This allows for more accurate conclusions about
		  the attackers and their motivations, helping organizations
		  to better protect themselves against future attacks.},
  booktitle	= {Proceedings of the 2023 European Interdisciplinary
		  Cybersecurity Conference},
  pages		= {34–41},
  numpages	= {8},
  keywords	= {APT, Attribution Framework, Cyber Criminology, Cyber
		  Investigation., Zachman Ontology},
  location	= {Stavanger, Norway},
  series	= {EICC '23}
}

@InProceedings{	  10.1145/3718491.3718494,
  author	= {Tang, Yanfei and Wang, Hui and Luo, Huilan and Li, Qin},
  title		= {A fuzzy expert system based recommendation model for
		  nuclear wastewater research collaborators},
  year		= {2025},
  isbn		= {9798400710865},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3718491.3718494},
  doi		= {10.1145/3718491.3718494},
  abstract	= {In order to help scholars in the field of nuclear
		  wastewater explore potential research collaborators more
		  efficiently, this paper proposes a fuzzy expert
		  system-based research collaborator recommendation model
		  (FCR-NWR) to address the problems of existing collaboration
		  recommendation models in terms of textual representation,
		  incomplete extraction of scholars' features, and the lack
		  of diversified information and reasoning decision-making
		  methods. The model fully obtains text features through
		  multilevel text analysis, combines with new methods to
		  calculate scholars' comprehensive academic ability and
		  scientific research cooperation intensity, and finally
		  utilizes the expert system to simulate the knowledge and
		  decision-making process of human experts for collaborator
		  recommendation. Through this model, scholars in the field
		  of nuclear wastewater can integrate into the academic
		  community faster, find potential collaborators, and promote
		  academic co-development. The experimental results show that
		  FCR-NWR outperforms traditional methods in terms of
		  precision, recall and F1 value.},
  booktitle	= {Proceedings of the 4th Asia-Pacific Artificial
		  Intelligence and Big Data Forum},
  pages		= {13–16},
  numpages	= {4},
  keywords	= {collaboration recommendation, decision simulation, expert
		  system, fuzzy set, hybrid modeling},
  location	= { },
  series	= {AIBDF '24}
}

@InProceedings{	  10.5555/3712729.3712889,
  author	= {Lei\ss{}au, Madlene and Laroque, Christoph},
  title		= {Breaking Barriers in Semiconductor Simulations: An
		  Automated Low-Code Framework for Model-Structure
		  Synchronisation and Large-Scale Simulation Studies},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {The paradigm shift towards Industry 4.0 and the emerging
		  trends of Industry 5.0 present ongoing challenges in
		  production planning and control. In response to these
		  dynamics, discrete event-driven simulation methods are
		  gaining prominence as an operational decision-support-tool,
		  particularly in the semiconductor industry. This paper
		  introduces an automated low-code framework designed to
		  synchronize model structures across simulation tool
		  boundaries for extensive simulation studies, using the
		  Semiconductor Manufacturing Testbed 2020 as a test
		  reference, and aims to serve as a helpful tool for
		  simulation experiments. Key aspects include model structure
		  synchronization, Design of Experiments, and the distributed
		  execution of large-scale simulation studies.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1919–1930},
  numpages	= {12},
  location	= {Orlando, Florida, USA},
  series	= {WSC '24}
}

@Article{	  10.1145/3345517,
  author	= {Abdulhameed, Tiba Zaki and Zitouni, Imed and Abdel-Qader,
		  Ikhlas},
  title		= {Wasf-Vec: Topology-based Word Embedding for Modern
		  Standard Arabic and Iraqi Dialect Ontology},
  year		= {2019},
  issue_date	= {March 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3345517},
  doi		= {10.1145/3345517},
  abstract	= {Word clustering is a serious challenge in low-resource
		  languages. Since words that share semantics are expected to
		  be clustered together, it is common to use a feature vector
		  representation generated from a distributional theory-based
		  word embedding method. The goal of this work is to utilize
		  Modern Standard Arabic (MSA) for better clustering
		  performance of the low-resource Iraqi vocabulary. We began
		  with a new Dialect Fast Stemming Algorithm (DFSA) that
		  utilizes the MSA data. The proposed algorithm achieved 0.85
		  accuracy measured by the F1 score. Then, the distributional
		  theory-based word embedding method and a new simple, yet
		  effective, feature vector named Wasf-Vec word embedding are
		  tested. Wasf-Vec word representation utilizes a word’s
		  topology features. The difference between Wasf-Vec and
		  distributional theory-based word embedding is that Wasf-Vec
		  captures relations that are not contextually based. The
		  embedding is followed by an analysis of how the dialect
		  words are clustered within other MSA words. The analysis is
		  based on the word semantic relations that are well
		  supported by solid linguistic theories to shed light on the
		  strong and weak word relation representations identified by
		  each embedding method. The analysis is handled by
		  visualizing the feature vector in two-dimensional (2D)
		  space. The feature vectors of the distributional
		  theory-based word embedding method are plotted in 2D space
		  using the t-sne algorithm, while the Wasf-Vec feature
		  vectors are plotted directly in 2D space. A word’s
		  nearest neighbors and the distance-histograms of the
		  plotted words are examined. For validation purpose of the
		  word classification used in this article, the produced
		  classes are employed in Class-based Language Modeling
		  (CBLM). Wasf-Vec CBLM achieved a 7\% lower perplexity (pp)
		  than the distributional theory-based word embedding method
		  CBLM. This result is significant when working with
		  low-resource languages.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= dec,
  articleno	= {22},
  numpages	= {27},
  keywords	= {2D visualizing, Arabic language, Topology, Word embedding,
		  class-based language modeling, dialect, morphology,
		  orthographic, phonology, words classification, words
		  features, words ontology}
}

@Proceedings{	  10.1145/3563359,
  title		= {UMAP '23 Adjunct: Adjunct Proceedings of the 31st ACM
		  Conference on User Modeling, Adaptation and
		  Personalization},
  year		= {2023},
  isbn		= {9781450398916},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Limassol, Cyprus}
}

@InProceedings{	  10.1145/3722237.3722341,
  author	= {Zhang, Baoliang and Xu, Yinghui and Li, Chengyuan and
		  Jiang, Yuan and Cui, Wenchao and Yue, Hongxu and Hui, Chen
		  and Hu, Ziwei},
  title		= {Research on Intelligent Management and Assisted Decision
		  Making Technology of Electric Power Research Enterprise
		  Based on Knowledge Engineering},
  year		= {2025},
  isbn		= {9798400712692},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3722237.3722341},
  doi		= {10.1145/3722237.3722341},
  abstract	= {Currently, the internal management knowledge of electric
		  power research enterprises is difficult to integrate and
		  structured organization, resulting in greatly reduced
		  decision-making efficiency. Knowledge Graph(KG), as an
		  efficient knowledge organization, helps to connect the
		  complex knowledge network. However, for a Q&amp;A platform
		  based solely on knowledge graphs, it can be challenging to
		  interpret and analyze natural language input, especially
		  when considering historical Q&amp;A data. In this paper, we
		  combine the KG with a Large Language Model(LLM), choose the
		  ERNIE model fine-tuned by the text of electric power domain
		  knowledge, and use the TransE method to integrate the KG
		  into the model's structure to build a knowledge-based
		  Q&amp;A system that assists researchers in decision-making.
		  The experimental results show that the ERNIE model combined
		  with KG outperforms the comparison model in terms of
		  precision, recall, and F1 value in entity and relation
		  classification tasks, and the performance is further
		  improved by increasing the number of attention heads.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Artificial Intelligence and Education},
  pages		= {590–600},
  numpages	= {11},
  keywords	= {Knowledge Graph embedding, Knowledge Quizzing, Large
		  Language Model},
  location	= { },
  series	= {ICAIE '24}
}

@InProceedings{	  10.1145/3701716.3715167,
  author	= {Wang, Ludi and Song, Dongze and Cui, Qiang and Chen,
		  Xueqing and Zhou, Yuanchun and Cui, Wenjuan and Du, Yi},
  title		= {AutoDive+: An Adaptive Model Enhanced Multimodal Online
		  Annotation Tool},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715167},
  doi		= {10.1145/3701716.3715167},
  abstract	= {The demand for data by machine learning models, especially
		  pre-trained large-scale models, has surged dramatically in
		  recent times, resulting in an urgent need for efficient
		  data annotation. Despite the prevalence of annotation
		  tools, they grapple with challenges such as high data
		  conversion costs, limited scalability, and inefficiencies
		  in annotating multimodal data. To tackle these challenges,
		  this paper introduces AutoDive+, an advanced data
		  annotation tool empowered by active learning mechanisms and
		  integrated automatic extraction models. Expanding upon the
		  capabilities of its predecessor, AutoDive, which excels in
		  direct text annotation within PDF documents, AutoDive+
		  undergoes extensive architectural enhancements. It not only
		  exploits high-value resource ranking modules to boost
		  annotation efficiency across entity and content annotation
		  but also boasts scalable interfaces for seamless
		  integration of new annotation modes and modalities.
		  Furthermore, our novel model community concept facilitates
		  adaptable integration of appropriate automatic extraction
		  and annotation models. We substantiate the effectiveness of
		  our proposed modules through user studies spanning diverse
		  domains. Our vision entails the establishment of an
		  annotation task-driven open community, bolstered by the
		  support of AutoDive+. A live demo of Autodive+ is available
		  at http://autodive.sciwiki.cn/, and a video demo
		  http://autodive.sciwiki.cn/introVideo/introduce-v2.0.mp4.
		  The source code is available at
		  https://github.com/Autodive.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2919–2922},
  numpages	= {4},
  keywords	= {human in the loop, intelligent extraction, multimodal
		  annotation tool},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3517804.3524166,
  author	= {Lutz, Carsten and Przybylko, Marcin},
  title		= {Efficiently Enumerating Answers to Ontology-Mediated
		  Queries},
  year		= {2022},
  isbn		= {9781450392600},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3517804.3524166},
  doi		= {10.1145/3517804.3524166},
  abstract	= {We study the enumeration of answers to ontology-mediated
		  queries (OMQs) where the ontology is a set of guarded TGDs
		  or formulated in the description logic ELI and the query is
		  a conjunctive query (CQ). In addition to the traditional
		  notion of an answer, we propose and study two novel notions
		  of partial answers that can take into account nulls
		  generated by existential quantifiers in the ontology. Our
		  main result is that enumeration of the traditional complete
		  answers and of both kinds of partial answers is possible
		  with linear-time preprocessing and constant delay for OMQs
		  that are both acyclic and free-connex acyclic. We also
		  provide partially matching lower bounds. Similar results
		  are obtained for the related problems of testing a single
		  answer in linear time and of testing multiple answers in
		  constant time after linear time preprocessing. In both
		  cases, the border between tractability and intractability
		  is characterized by similar, but slightly different
		  acyclicity properties.},
  booktitle	= {Proceedings of the 41st ACM SIGMOD-SIGACT-SIGAI Symposium
		  on Principles of Database Systems},
  pages		= {277–289},
  numpages	= {13},
  keywords	= {constant delay, description logic, enumeration,
		  ontology-mediated queries, partial answers, tuple
		  generating dependencies},
  location	= {Philadelphia, PA, USA},
  series	= {PODS '22}
}

@InProceedings{	  10.5555/3213214.3213217,
  author	= {Deatcu, Christina and Folkerts, Hendrik and Pawletta,
		  Thorsten and Durak, Umut},
  title		= {Design patterns for variability modeling using SES
		  ontology},
  year		= {2018},
  isbn		= {9781510860186},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {The System Entity Structure (SES) is a high level approach
		  for variability modeling, particularly in simulation
		  engineering, which is under continuous development. In this
		  context, an enhanced framework is introduced that supports
		  dynamic variability evolution using the SES approach.
		  However, the main focus is to start a discussion about a
		  set of design patterns, which were developed to analyze the
		  tree design and computing aspects of System Entity
		  Structures. As development of our MATLAB-based SES toolbox
		  for construction and pruning of SES trees proceeded, the
		  necessity to have some generalized examples for testing and
		  verification came more and more into awareness. We propose
		  a set of design patterns that, if completely representable
		  and computable by a certain tool, support all aspects of
		  SES theory. In addition, the patterns give users
		  substantial support for developing SES models for other
		  applications.},
  booktitle	= {Proceedings of the Model-Driven Approaches for Simulation
		  Engineering Symposium},
  articleno	= {3},
  numpages	= {12},
  keywords	= {MATLAB/simulink, SES, model generation, simulation
		  engineering, system entity structure, variability modeling,
		  versatile systems},
  location	= {Baltimore, Maryland},
  series	= {Mod4Sim '18}
}

@Article{	  10.1145/3408316,
  author	= {Cornejo-Lupa, Mar\'{\i}a A. and Ticona-Herrera, Regina P.
		  and Cardinale, Yudith and Barrios-Aranibar, Dennis},
  title		= {A Survey of Ontologies for Simultaneous Localization and
		  Mapping in Mobile Robots},
  year		= {2020},
  issue_date	= {September 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {5},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3408316},
  doi		= {10.1145/3408316},
  abstract	= {Autonomous robots are playing important roles in academic,
		  technological, and scientific activities. Thus, their
		  behavior is getting more complex, particularly, in tasks
		  related to mapping an environment and localizing
		  themselves. These tasks comprise the Simultaneous
		  Localization and Mapping (SLAM) problem. Representation of
		  knowledge related to the SLAM problem with a standard,
		  flexible, and well-defined model, provides the base to
		  develop efficient and interoperable solutions. As many
		  existing works demonstrate, Semantic Web seems to be a
		  clear approach, since they have formulated ontologies, as
		  the base data model to represent such knowledge. In this
		  article, we survey the most popular and recent SLAM
		  ontologies with our aim being threefold: (i) propose a
		  classification of SLAM ontologies according to the main
		  knowledge needed to model the SLAM problem; (ii) identify
		  existing ontologies for classifying, comparing, and
		  contrasting them, in order to conceptualize SLAM domain for
		  mobile robots; and (iii) pin-down lessons to learn from
		  existing solutions in order to design better solutions and
		  identify new research directions and further improvements.
		  We compare the identified SLAM ontologies according to the
		  proposed classification and, finally, we explore new data
		  fields to enrich existing ontologies and highlight new
		  possibilities in terms of performance and efficiency for
		  SLAM solutions.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {103},
  numpages	= {26},
  keywords	= {SLAM, Web ontologies, robots, semantic web}
}

@InProceedings{	  10.5555/3712729.3712866,
  author	= {Winkels, Jan and \"{O}zkul, Felix and Sutherland, Robin
		  and L\"{o}hn, Jannik and Wenzel, Sigrid and Rehof, Jakob},
  title		= {Component-Based Synthesis of Structural Variants of
		  Simulation Models for Changeable Material Flow Systems},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {Despite relevant research endeavors, modeling efforts
		  related to the building of discrete-event simulation models
		  for planning changeable material flow systems still limit
		  their practical application. This is because simulation
		  experts have to model many possible structural variants and
		  compare them based on key performance indicators such as
		  throughput, workload or investment costs, while also
		  ensuring sufficient system changeability. This article
		  presents a methodology for reducing efforts for structural
		  variation during the experimental phase of a simulation
		  study. Starting from a valid initial simulation model,
		  structural variants of this simulation model are
		  automatically generated by applying component-based
		  software synthesis which uses combinatorial logic; thereby,
		  a range of simulation models is provided for the user. This
		  paper presents the outlined methodology using a case study
		  and places it in the research context of reducing efforts
		  associated with the design and execution of simulation
		  experiments.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1657–1668},
  numpages	= {12},
  location	= {Orlando, Florida, USA},
  series	= {WSC '24}
}

@Proceedings{	  10.1145/3715340,
  title		= {VaMoS '25: Proceedings of the 19th International Working
		  Conference on Variability Modelling of Software-Intensive
		  Systems},
  year		= {2025},
  isbn		= {9798400714412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3685651,
  title		= {eSAAM '24: Proceedings of the 4th Eclipse Security, AI,
		  Architecture and Modelling Conference on Data Space},
  year		= {2024},
  isbn		= {9798400709845},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Mainz, Germany}
}

@Proceedings{	  10.1145/3650105,
  title		= {FORGE '24: Proceedings of the 2024 IEEE/ACM First
		  International Conference on AI Foundation Models and
		  Software Engineering},
  year		= {2024},
  isbn		= {9798400706097},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {FORGE aims to bring researchers, practitioners, and
		  educators from the AI and Software Engineering community to
		  solve the new challenges we meet in the era of foundation
		  models.},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3412841.3442142,
  author	= {Gala, Alexander Pinto-De la and Cardinale, Yudith and
		  Dongo, Irvin and Ticona-Herrera, Regina},
  title		= {Towards an ontology for urban tourism},
  year		= {2021},
  isbn		= {9781450381048},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3412841.3442142},
  doi		= {10.1145/3412841.3442142},
  abstract	= {Nowadays, diffusion and preservation of cultural heritage
		  are being supported by technology on the Web. Thus, the
		  online availability of urban tourism information, as part
		  of cultural heritage, has been of enormous relevance to
		  activate the tourism in many countries. The necessity of a
		  well-defined and standard model for representing this
		  knowledge is being managed by semantic web technologies,
		  such as ontologies. However, current proposals represent
		  partial knowledge of cultural heritage. In this context,
		  this work proposes an ontology for indoor and outdoor
		  environments of a city to represent the cultural heritage
		  knowledge based on the UNESCO definitions. This ontology
		  has a three-level architecture (Upper, Middle, and Lower
		  ontologies) in accordance with a purpose of modularity and
		  levels of specificity. To demonstrate the utility and
		  suitability of our proposal, we have developed a parser to
		  map and convert a museum repository (in CSV format) to RDF
		  triples. With this case of study, we demonstrated that, by
		  using our ontology, it is possible to represent the
		  knowledge of urban tourism domains of a city.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on Applied
		  Computing},
  pages		= {1887–1890},
  numpages	= {4},
  keywords	= {automatic population, ontology, urban tourism},
  location	= {Virtual Event, Republic of Korea},
  series	= {SAC '21}
}

@InProceedings{	  10.1145/3543507.3584185,
  author	= {Gautam, Nikita and Shumway, David and Kowalcyk, Megan and
		  Khanal, Sarthak and Caragea, Doina and Caragea, Cornelia
		  and Mcginty, Hande and Dorevitch, Samuel},
  title		= {Leveraging Existing Literature on the Web and Deep Neural
		  Models to Build a Knowledge Graph Focused on Water Quality
		  and Health Risks},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3584185},
  doi		= {10.1145/3543507.3584185},
  abstract	= {A knowledge graph focusing on water quality in relation to
		  health risks posed by water activities (such as diving or
		  swimming) is not currently available. To address this
		  limitation, we first use existing resources to construct a
		  knowledge graph relevant to water quality and health risks
		  using KNowledge Acquisition and Representation Methodology
		  (KNARM). Subsequently, we explore knowledge graph
		  completion approaches for maintaining and updating the
		  graph. Specifically, we manually identify a set of
		  domain-specific UMLS concepts and use them to extract a
		  graph of approximately 75,000 semantic triples from the
		  Semantic MEDLINE database (which contains
		  head-relation-tail triples extracted from PubMed). Using
		  the resulting knowledge graph, we experiment with the
		  KG-BERT approach for graph completion by employing
		  pre-trained BERT/RoBERTa models and also models fine-tuned
		  on a collection of water quality and health risks abstracts
		  retrieved from the Web of Science. Experimental results
		  show that KG-BERT with BERT/RoBERTa models fine-tuned on a
		  domain-specific corpus improves the performance of KG-BERT
		  with pre-trained models. Furthermore, KG-BERT gives better
		  results than several translational distance or semantic
		  matching baseline models.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {4161–4171},
  numpages	= {11},
  keywords	= {BERT, Knowledge graph, diving, health risks, knowledge
		  graph completion, water quality, water recreation},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3550356.3556509,
  author	= {Tong, Li and Yiting, Wang and Congkai, Geng},
  title		= {Detection of anomalous modeling behavior: a goal-driven
		  data mining approach},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3556509},
  doi		= {10.1145/3550356.3556509},
  abstract	= {Precise and timely detection of anomalous modeling
		  behaviors is critical for both teaching and application of
		  modeling methods. Existing methods usually focus on
		  evaluating the modeling results rather than mining the
		  knowledge hidden in the modeling process. In this paper, we
		  propose to monitor and analyze the modeling process in
		  order to timely detect anomalous modeling behaviors,
		  potentially contributing to a comprehensive assessment of
		  the modeling practice. Specifically, we propose to
		  systematically build a goal model for characterizing the
		  normal modeling behaviors, which establishes the
		  connections between modelers' high-level modeling behaviors
		  and low-level modeling operations. On top of such a goal
		  model, we propose a data mining-based approach to
		  semi-automatically validate the design of the goal model
		  and explore other normal modeling behaviors. Then, we
		  propose to automatically detect anomalous modeling
		  behaviors by capturing normal modeling behaviors obtained
		  from the goal model and actual modeling sequences. We have
		  developed and deployed a data-flow diagram modeling
		  platform, which implemented our proposed approach. We have
		  conducted an experiment with 57 participants, the
		  preliminary results of which show that our approach can
		  effectively detect modelers' anomalous behaviors. The
		  experiment results are beneficial for not only assessing
		  the modelers' performance but also identifying the
		  usability issues of the modeling tool.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {142–145},
  numpages	= {4},
  keywords	= {anomalous modeling behavior detection, data mining, goal
		  modeling},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@Article{	  10.1145/3616111,
  author	= {Mehmood, Faiza and Shahzadi, Rehab and Ghafoor, Hina and
		  Asim, Muhammad Nabeel and Ghani, Muhammad Usman and
		  Mahmood, Waqar and Dengel, Andreas},
  title		= {EnML: Multi-label Ensemble Learning for Urdu Text
		  Classification},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {9},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3616111},
  doi		= {10.1145/3616111},
  abstract	= {Exponential growth of electronic data requires advanced
		  multi-label classification approaches for the development
		  of natural language processing (NLP) applications such as
		  recommendation systems, drug reaction detection, hate
		  speech detection, and opinion recognition/mining. To date,
		  several machine and deep learning–based multi-label
		  classification methodologies have been proposed for
		  English, French, German, Chinese, Arabic, and other
		  developed languages. Urdu is the 11th largest language in
		  the world and has no computer-aided multi-label textual
		  news classification approach. Unlike other languages, Urdu
		  is lacking multi-label text classification datasets that
		  can be used to benchmark the performance of existing
		  machine and deep learning methodologies. With an aim to
		  accelerate and expedite research for the development of
		  Urdu multi-label text classification–based applications,
		  this article provides multiple contributions as follows:
		  First, it provides a manually annotated multi-label textual
		  news classification dataset for the Urdu language. Second,
		  it benchmarks the performance of traditional machine
		  learning approaches particularly by adapting three data
		  transformation approaches along with three top-performing
		  machine learning classifiers and four algorithm
		  adaptation-based approaches. Third, it benchmarks
		  performance of 16 existing deep learning approaches and the
		  four most widely used language models. Finally, it provides
		  an ensemble approach that reaps the benefits of three
		  different deep learning architectures to precisely predict
		  different classes associated with a particular Urdu textual
		  document. Experimental results reveal that proposed
		  ensemble approach performance values (87\% accuracy, 92\%
		  F1-score, and 8\% hamming loss) are significantly higher
		  than adapted machine and deep learning–based approaches.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= sep,
  articleno	= {227},
  numpages	= {31},
  keywords	= {Multi-label Urdu text classification, multi-label Urdu
		  news dataset, traditional machine learning, deep learning,
		  language models, multi-label ensemble learning, data
		  transformation methods}
}

@InProceedings{	  10.5555/3492252.3492274,
  author	= {Fassbinder, Marcelo and Fassbinder, Aracele and
		  Fioravanti, Maria Lydia and Barbosa, Ellen Francine},
  title		= {Towards an educational design pattern language to support
		  the development of open educational resources in videos for
		  the MOOC context},
  year		= {2021},
  publisher	= {The Hillside Group},
  address	= {USA},
  abstract	= {The creation and adoption of Massive Open Online Courses
		  (MOOCs) can bring many benefits and impact on education,
		  such as put forward diversity in education; enhance
		  student's learning by encouraging and engaging them for
		  lifelong learning; connect with more individuals in
		  informal contexts creating opportunities to transition to
		  formal higher education or lifelong learning activities;
		  force a re-conceptualization of higher education through
		  the use of online study; enhance teachers' skills from
		  developing Open Educational Resources (OERs) and adopting
		  learner-centered pedagogical approaches and active learning
		  strategies; among others. In such perspective, several
		  studies have investigated the potential benefits about the
		  use of videos as a support for the process of teaching in
		  virtual learning environments and, particularly, in the
		  context of MOOCs. However, video production for MOOCs still
		  presents several challenges that need to be better
		  investigated. It is because, in general, educators and MOOC
		  teams (e.g. educators, learning designers, and educational
		  technologists) are still using ad hoc decision-making
		  procedures based on empirical knowledge obtained from their
		  experiences with face-to-face courses or even traditional
		  virtual courses. There are also gaps about what adaptations
		  need to be performed to video formats and what are the
		  attributes and steps needed to support the production and
		  validation of videos for the MOOC context. In addition,
		  since MOOCs are part of the Open Educational Movement, as
		  well as the OERs, it is also important to reflect on the
		  construction of OERs in the form of videos for the MOOC
		  context. There is, therefore, a need for research that
		  investigates the current theoretical panorama involving
		  video construction for MOOCs, in order to propose
		  strategies empirically validated and useful to support and
		  guide MOOC teams during the development of videos more
		  theoretically informed. Considering such context, the main
		  objective of this paper is to present a set of patterns and
		  move towards the development of an educational design
		  pattern language able to support MOOC teams in the
		  construction of OERs in the form of videos. The paper
		  presents a life cycle for OERs production in the form of
		  videos. From such cycle activities, we extracted nine
		  patterns that are presented in the form of patlets
		  (problem-solution pair). The patterns were also collected
		  and refined from a literature review on guidelines about
		  the production of MOOCs, OERs, and educational videos. The
		  patterns are divided into five categories related to the
		  life cycle: analysis, planning development, evaluation, and
		  distribution. These patterns are expected to offer a
		  compelling alternative to guide MOOC teams in designing
		  OERs in the form of videos to enhance learning experiences,
		  increase student engagement in the course, and emphasize
		  self-directed learning, which are requirements for quality
		  in MOOCs.},
  booktitle	= {Proceedings of the 26th Conference on Pattern Languages of
		  Programs},
  articleno	= {18},
  numpages	= {10},
  keywords	= {MOOCs, educational design patterns, educational videos,
		  learning design, patterns},
  location	= {Urbana, Illinois},
  series	= {PLoP '19}
}

@InProceedings{	  10.1145/3726302.3729988,
  author	= {Li, Wanli and Qian, Tieyun and Song, Yi and Zhang, Zeyu
		  and Li, Jiawei and Chen, Zhuang and Zou, Lixin},
  title		= {Generative Meta-Learning for Zero-Shot Relation Triplet
		  Extraction},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3729988},
  doi		= {10.1145/3726302.3729988},
  abstract	= {Zero-shot Relation Triplet Extraction (ZeroRTE) aims to
		  extract relation triplets from texts containing unseen
		  relation types. This capability benefits various downstream
		  information retrieval (IR) tasks. The primary challenge
		  lies in enabling models to generalize effectively to unseen
		  relation categories. Existing approaches typically leverage
		  the knowledge embedded in pre-trained language models to
		  accomplish the generalization process. However, these
		  methods focus solely on fitting the training data during
		  training, without specifically improving the model's
		  generalization performance, resulting in limited
		  generalization capability. For this reason, we explore the
		  integration of bi-level optimization (BLO) with pre-trained
		  language models for learning generalized knowledge directly
		  from the training data, and propose a generative
		  meta-learning framework which exploits the
		  'learning-to-learn' ability of meta-learning to boost the
		  generalization capability of generative
		  models.Specifically, we introduce a BLO approach that
		  simultaneously addresses data fitting and generalization.
		  This is achieved by constructing an upper-level loss to
		  focus on generalization and a lower-level loss to ensure
		  accurate data fitting. Building on this, we subsequently
		  develop three generative meta-learning methods, each
		  tailored to a distinct category of meta-learning. Extensive
		  experimental results demonstrate that our framework
		  performs well on the ZeroRTE task. Our code is available at
		  https://github.com/leeworry/TGM-MetaLearning.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1371–1381},
  numpages	= {11},
  keywords	= {meta-learning, pre-trained language model, relation
		  triplet extraction, zero-shot learning},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@InProceedings{	  10.1145/3707292.3707392,
  author	= {Duan, Jinjian and Wei, Deming and Wang, Meiqing and Chen,
		  Gaohui and Zhang, Yang and Gao, Yanhua},
  title		= {Knowledge-based Retrieval Methods for Enhancing Aerospace
		  Model Software Documentation},
  year		= {2025},
  isbn		= {9798400707308},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3707292.3707392},
  doi		= {10.1145/3707292.3707392},
  abstract	= {Addressing the critical challenges of poor structure, weak
		  relevance, and limited reusability in aerospace model
		  software development documents, we propose a
		  knowledge-based retrieval method. This method constructs a
		  knowledge base by analyzing key document features,
		  including storage methods, file formats, content structure,
		  inter-document associations, and naming conventions. The
		  system implements structured document management and
		  associates documents based on software configuration
		  information. By partitioning the retrieval domain,
		  constructing semantic vectors, and applying
		  multi-dimensional weight configurations, the method enables
		  efficient retrieval in domains characterized by dense
		  information and frequent term repetition. An aerospace
		  model software document retrieval system has been developed
		  and preliminarily implemented in an aerospace enterprise,
		  demonstrating its effectiveness in improving retrieval
		  efficiency and enhancing the reuse of development
		  knowledge.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Artificial Intelligence and Intelligent Information
		  Processing},
  pages		= {372–378},
  numpages	= {7},
  keywords	= {Intelligent Retrieval 2, Knowledge Base 4, Knowledge Reuse
		  3, Model Software Document 1},
  location	= { },
  series	= {AIIIP '24}
}

@InProceedings{	  10.1145/3545947.3576365,
  author	= {Katyshev, Alexander and Anikin, Anton and Sychev, Oleg},
  title		= {Using Transformer Models for Knowledge Graph Construction
		  in Computer Science Education},
  year		= {2023},
  isbn		= {9781450394338},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3545947.3576365},
  doi		= {10.1145/3545947.3576365},
  abstract	= {The volume of information that can be used in the
		  development of knowledge bases that can be used in
		  education is constantly increasing. Also, this amount of
		  data is very difficult to process and store. When designing
		  a knowledge base to optimize the educational process, it is
		  important to use ontologies. At the moment, the creation of
		  an ontological knowledge model is the most promising option
		  for storing and processing information. The article
		  describes effective approaches for generating an
		  ontological model using machine learning models based on
		  the Transformer model.},
  booktitle	= {Proceedings of the 54th ACM Technical Symposium on
		  Computer Science Education V. 2},
  pages		= {1421},
  numpages	= {1},
  keywords	= {concepts, machine learning, neural networks, ontological
		  graph, ontologies, relations between concepts, semantics,
		  transformers},
  location	= {Toronto ON, Canada},
  series	= {SIGCSE 2023}
}

@Article{	  10.1145/3479012,
  author	= {Troncoso, Alvaro R. Ortiz},
  title		= {Ontology-Based Approach to Creating Semantic Wikis},
  year		= {2022},
  issue_date	= {June 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3479012},
  doi		= {10.1145/3479012},
  abstract	= {Maintaining a semantic wiki is challenging. Coping with
		  increasingly complex wikis led to the development of a
		  methodical approach for simplifying the creation and
		  maintenance of semantic wikis. The methodical approach used
		  involves modeling the semantic relationships underlying the
		  information in the wiki as an ontology, and then
		  programmatically creating the wiki from the ontology
		  constructs. A methodical approach for creating and
		  maintaining wikis greatly simplifies the creation and
		  maintenance of semantic wikis. Furthermore, reusing
		  vocabularies and taxonomies throughout several projects
		  becomes manageable. The approach was implemented as open
		  source software that maps ontology constructs to the wiki
		  artifacts, and practical applications are discussed.},
  journal	= {J. Comput. Cult. Herit.},
  month		= apr,
  articleno	= {28},
  numpages	= {7},
  keywords	= {Semantic wikis, collaborative research environment,
		  domain-driven software development}
}

@Article{	  10.1145/3589338,
  author	= {Storey, Veda C. and Lukyanenko, Roman and Castellanos,
		  Arturo},
  title		= {Conceptual Modeling: Topics, Themes, and Technology
		  Trends},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {14s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3589338},
  doi		= {10.1145/3589338},
  abstract	= {Conceptual modeling is an important part of information
		  systems development and use that involves identifying and
		  representing relevant aspects of reality. Although the past
		  decades have experienced continuous digitalization of
		  services and products that impact business and society,
		  conceptual modeling efforts are still required to support
		  new technologies as they emerge. This paper surveys
		  research on conceptual modeling over the past five decades
		  and shows how its topics and trends continue to evolve to
		  accommodate emerging technologies, while remaining grounded
		  in basic constructs. We survey over 5,300 papers that
		  address conceptual modeling topics from the 1970s to the
		  present, which are collected from 35 multidisciplinary
		  journals and conferences, and use them as the basis from
		  which to analyze the progression of conceptual modeling.
		  The important role that conceptual modeling should play in
		  our evolving digital world is discussed, and future
		  research directions proposed.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {317},
  numpages	= {38},
  keywords	= {Conceptual modeling, digital world, database, information
		  systems, information technology, structured literature
		  review, clustering analysis}
}

@Article{	  10.1145/3487066,
  author	= {Zorrilla, Asier L\'{o}pez and Torres, M. In\'{e}s},
  title		= {A Multilingual Neural Coaching Model with Enhanced
		  Long-term Dialogue Structure},
  year		= {2022},
  issue_date	= {June 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {12},
  number	= {2},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3487066},
  doi		= {10.1145/3487066},
  abstract	= {In this work we develop a fully data driven conversational
		  agent capable of carrying out motivational coaching
		  sessions in Spanish, French, Norwegian, and English. Unlike
		  the majority of coaching, and in general well-being related
		  conversational agents that can be found in the literature,
		  ours is not designed by hand-crafted rules. Instead, we
		  directly model the coaching strategy of professionals with
		  end users. To this end, we gather a set of virtual coaching
		  sessions through a Wizard of Oz platform, and apply state
		  of the art Natural Language Processing techniques. We
		  employ a transfer learning approach, pretraining GPT2
		  neural language models and fine-tuning them on our corpus.
		  However, since these only take as input a local dialogue
		  history, a simple fine-tuning procedure is not capable of
		  modeling the long-term dialogue strategies that appear in
		  coaching sessions. To alleviate this issue, we first
		  propose to learn dialogue phase and scenario embeddings in
		  the fine-tuning stage. These indicate to the model at which
		  part of the dialogue it is and which kind of coaching
		  session it is carrying out. Second, we develop a global
		  deep learning system which controls the long-term structure
		  of the dialogue. We also show that this global module can
		  be used to visualize and interpret the decisions taken by
		  the the conversational agent, and that the learnt
		  representations are comparable to dialogue acts. Automatic
		  and human evaluation show that our proposals serve to
		  improve the baseline models. Finally, interaction
		  experiments with coaching experts indicate that the system
		  is usable and gives rise to positive emotions in Spanish,
		  French and English, while the results in Norwegian point
		  out that there is still work to be done in fully data
		  driven approaches with very low resource languages.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= jul,
  articleno	= {16},
  numpages	= {47},
  keywords	= {Dialogue system, coaching, multilingual, transfer
		  learning, explainable artificial intelligence}
}

@InProceedings{	  10.1145/3625007.3627305,
  author	= {Mu, Wenchuan and Lim, Kwan Hui},
  title		= {Modelling Text Similarity: A Survey},
  year		= {2024},
  isbn		= {9798400704093},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625007.3627305},
  doi		= {10.1145/3625007.3627305},
  abstract	= {Online social networking services such as Twitter and
		  Instagram have become pervasive platforms for engaging in
		  discussions on a wide array of topics. These platforms
		  cater to both mainstream subjects, like music and movies,
		  as well as more specialized areas, such as politics. With
		  the growing volume of textual data generated on these
		  platforms, the ability to define and identify similar texts
		  becomes crucial for effective investigation and clustering.
		  In this paper, we explore the challenges and significance
		  of text similarity regression models in the context of
		  online social networking services. We delve into the
		  methods and techniques employed to define and find
		  similarities among texts, enabling the extraction of
		  meaningful patterns and insights. Specifically, we
		  categorize text similarity regression models into four
		  distinct types: set-theoretic, sequence-theoretic,
		  real-vector, and end-to-end methods. This categorization is
		  based on the mathematical formalisation of similarity used
		  by each model. Ultimately, our survey aims to provide a
		  comprehensive overview of the interlinkages between
		  independently proposed methods for text similarity. By
		  understanding the strengths and weaknesses of these
		  methods, researchers can make informed decisions when
		  designing novel approaches and algorithms. We hope this
		  survey serves as a valuable resource for advancing the
		  state-of-the-art in addressing the complex problem of text
		  similarity.},
  booktitle	= {Proceedings of the 2023 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {698–705},
  numpages	= {8},
  keywords	= {modelling and simulation, deep learning and embeddings,
		  algorithms and techniques},
  location	= {Kusadasi, Turkiye},
  series	= {ASONAM '23}
}

@InProceedings{	  10.1145/3183440.3195087,
  author	= {Atchison, Abigail and Anderson, Haley and Berardi,
		  Christina and Best, Natalie and Firmani, Cristiano and
		  German, Rene and Linstead, Erik},
  title		= {A topic analysis of the R programming language},
  year		= {2018},
  isbn		= {9781450356633},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3183440.3195087},
  doi		= {10.1145/3183440.3195087},
  abstract	= {We leverage Latent Dirichlet Allocation to analyze R
		  source code from 10,051 R packages in order to better
		  understand the topic space of scientific computing. Our
		  method is able to identify several generic programming
		  concepts and, more importantly, identify concepts that are
		  highly specific to scientific and high performance
		  computing applications.},
  booktitle	= {Proceedings of the 40th International Conference on
		  Software Engineering: Companion Proceeedings},
  pages		= {183–184},
  numpages	= {2},
  keywords	= {R, machine learning, topic modeling},
  location	= {Gothenburg, Sweden},
  series	= {ICSE '18}
}

@InProceedings{	  10.1145/3466933.3466946,
  author	= {Helfer, Gilson Augusto and Costa, Adilson Ben da and
		  Bavaresco, Rodrigo Simon and Barbosa, Jorge Luis
		  Vict\'{o}ria},
  title		= {Tellus-Onto: uma ontologia para classifica\c{c}\~{a}o e
		  infer\^{e}ncia de solos na agricultura de precis\~{a}o:
		  Tellus-Onto: an ontology for soil classification and
		  inference in precision agriculture},
  year		= {2021},
  isbn		= {9781450384919},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3466933.3466946},
  doi		= {10.1145/3466933.3466946},
  abstract	= {Soil analysis laboratories demand large volumes of data
		  used in precision agriculture. Among them, parameters that
		  represent soil fertility such as texture and organic matter
		  guide the fertilization process. However, this process can
		  take time, thus limiting its usefulness. Therefore, this
		  article proposes an ontology called Tellus-Onto that
		  extends the state of the art in the classification of
		  Brazilian soils according to the organic and textural
		  composition. A series of axioms and semantic rules provided
		  consultations and inferences about their instantiated
		  basis. In order to test the ontology, we added 98 soil
		  sample results and their classifications were inferred
		  precisely and automatically. Laborat\'{o}rios de
		  an\'{a}lises de solos demandam volumes grandes de dados
		  empregados na agricultura de precis\~{a}o. Dentre eles,
		  par\^{a}metros que representam fertilidade de solos como
		  textura e mat\'{e}ria org\^{a}nica orientam o processo de
		  aduba\c{c}\~{a}o. No entanto, este processo pode se tornar
		  demorado, limitando assim sua utilidade. Sendo assim, este
		  artigo prop\~{o}e uma ontologia denominada Tellus-Onto que
		  estende o estado da arte na classifica\c{c}\~{a}o de solos
		  brasileiros de acordo com a composi\c{c}\~{a}o org\^{a}nica
		  e textural. Uma s\'{e}rie de axiomas e regras
		  sem\^{a}nticas foram empregadas para proporcionar a
		  realiza\c{c}\~{a}o de consultas e infer\^{e}ncias sobre sua
		  base instanciada. Para testar a ontologia foram
		  instanciados 98 resultados de amostras de solos e inferidos
		  suas classifica\c{c}\~{o}es de modo preciso e autom\'{a}tico.},
  booktitle	= {Proceedings of the XVII Brazilian Symposium on Information
		  Systems},
  articleno	= {13},
  numpages	= {7},
  keywords	= {agricultura de precis\~{a}o, classifica\c{c}\~{a}o de
		  solos, ontology, precision agriculture ontologia, soil
		  classification},
  location	= {Uberl\^{a}ndia, Brazil},
  series	= {SBSI '21}
}

@InProceedings{	  10.1145/3428757.3429143,
  author	= {Rocha, Rodrigo and Bion, Danillo and Azevedo, Ryan and
		  Gomes, Arthur and Cordeiro, Diogo and Leandro, Renan and
		  Silva, Israel and Freitas, Fred},
  title		= {A Syntactic and Semantic Assessment of a Global Software
		  Engineering Domain Ontology},
  year		= {2021},
  isbn		= {9781450389228},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3428757.3429143},
  doi		= {10.1145/3428757.3429143},
  abstract	= {Globalization has allowed organizations to intensify the
		  search for solutions that minimize challenges, reduce costs
		  and optimize processes. In this way, global software
		  development has emerged as an attempt to use the best
		  resources for its limitations.In distributed environments,
		  the use of Ontologies brings some benefits such as a
		  uniform understanding of information among teams and ease
		  of communication, as well as making for the lack of a
		  reference model that can be applied in a distributed
		  context.This work aims to propose a viable form of
		  validation for DKDonto a domain ontology developed for
		  Global Software Engineering. The validation allowed a
		  broader and more targeted assessment, different from its
		  original validation, which was carried out in a controlled
		  environment, limited to answering questions already known
		  by the knowledge base itself.The main result of this work
		  is a satisfactory evaluation of the ontology, enabling it
		  to be used and shared by companies or institutions, as well
		  as the presentation of a set of methods and ways to
		  evaluate and verify domain ontologies to be used in
		  different domains.},
  booktitle	= {Proceedings of the 22nd International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {253–262},
  numpages	= {10},
  keywords	= {Evaluation, Global Software Development, Ontology},
  location	= {Chiang Mai, Thailand},
  series	= {iiWAS '20}
}

@InProceedings{	  10.1145/3586183.3606715,
  author	= {Zaidi, Ali and Turbeville, Kelsey and Ivan\v{c}i\'{c},
		  Kristijan and Moss, Jason and Gutierrez Villalobos, Jenny
		  and Sagar, Aravind and Li, Huiying and Mehra, Charu and Li,
		  Sixuan and Hutchins, Scott and Kumar, Ranjitha},
  title		= {Learning Custom Experience Ontologies via Embedding-based
		  Feedback Loops},
  year		= {2023},
  isbn		= {9798400701320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3586183.3606715},
  doi		= {10.1145/3586183.3606715},
  abstract	= {Organizations increasingly rely on behavioral analytics
		  tools like Google Analytics to monitor their digital
		  experiences. Making sense of the data these tools capture,
		  however, requires manual event tagging and filtering —
		  often a tedious process. Prior approaches have trained
		  machine learning models to automatically tag interaction
		  data, but draw from fixed digital experience vocabularies
		  which cannot be easily augmented or customized. This paper
		  introduces a novel machine learning interaction pattern
		  that generates customized tag predictions for
		  organizations. The approach employs a general user
		  experience word embedding to bootstrap an initial set of
		  predictions, which can then be refined and customized by
		  users to adapt the underlying vector space, iteratively
		  improving the quality of future predictions. The paper
		  presents a needfinding study that grounds the design
		  choices of the system, and describes a real-world
		  deployment as part of UserTesting.com that demonstrates the
		  efficacy of the approach.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {111},
  numpages	= {13},
  keywords	= {Sankey diagrams, UX research, clickstream analytics,
		  sequence alignment, usability testing},
  location	= {San Francisco, CA, USA},
  series	= {UIST '23}
}

@InProceedings{	  10.1145/3589132.3625618,
  author	= {Wang, Renzhong and Najafabadi, Maryam and Zhang, Chiqun
		  and Chen, Long-Qi and Olenina, Tanya and Yankov, Dragomir},
  title		= {GPT Applications in Relevance Model Training in Map
		  Search},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589132.3625618},
  doi		= {10.1145/3589132.3625618},
  abstract	= {Understanding map queries and retrieving correct entity
		  results are the two main relevance tasks in Map search.
		  They are usually performed by a set of task specific
		  machine learning models. Collecting large amount of high
		  quality labelled data for training such models is a
		  time-consuming and labor-intensive process. Although
		  various methods have been studied for producing pseudo data
		  labels, they are limited in their effectiveness when
		  applied across different languages or tasks. The recently
		  released Large Language models (LLMs), including ChatGPT
		  and GPT-4 (GPT for short), have demonstrated
		  state-of-the-art performance in text understanding by using
		  simple prompt instructions with only a handful of examples
		  for in-context learning. In this paper, we explore GPT as a
		  cost-effective alternative for both data labeling and
		  synthetic data generation, where we subsequently use data
		  obtained from this approach to train various task specific
		  models such as maps intent detection, address detection,
		  address parsing, geo-entity ranking, and rank scores
		  calibration. GPT demonstrates strong potential in
		  generating otherwise hard-to-synthesize data. We observe
		  significant accuracy and relevance improvement across all
		  task specific models when trained or fine-tuned on data
		  generated by GPT. Lastly, we propose a general framework
		  combining labeled data from GPT with other sources and a
		  prompt fine-tune structure to guide GPT model in completing
		  a given task.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {68},
  numpages	= {4},
  keywords	= {GPT, query processing, maps service, information
		  retrieval},
  location	= {Hamburg, Germany},
  series	= {SIGSPATIAL '23}
}

@InProceedings{	  10.1145/3368691.3368745,
  author	= {Sharipbay, Altynbek and Razakhova, Bibigul and Mukanova,
		  Assel and Yergesh, Banu and Yelibayeva, Gaziza},
  title		= {Syntax parsing model of Kazakh simple sentences},
  year		= {2019},
  isbn		= {9781450372848},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368691.3368745},
  doi		= {10.1145/3368691.3368745},
  abstract	= {This paper proposes a syntactic analysis of Kazakh simple
		  sentences taking into account their semantics. To do this,
		  first, the syntactic rules of sentences are described using
		  formal grammar, then parsing trees and ontological models
		  are built to determine the semantics of their components
		  and the relationships between them. As a formal grammar
		  used Chomsky's context-free grammar, and ontological models
		  were built in the environment of Protege.},
  booktitle	= {Proceedings of the Second International Conference on Data
		  Science, E-Learning and Information Systems},
  articleno	= {54},
  numpages	= {5},
  keywords	= {formal grammar, ontological model, parsing of Kazakh
		  language, parsing tree, simple sentences, syntactic rules
		  of sentences},
  location	= {Dubai, United Arab Emirates},
  series	= {DATA '19}
}

@InProceedings{	  10.1145/3563357.3564083,
  author	= {Fierro, Gabe and Saha, Avijit and Shapinsky, Tobias and
		  Steen, Matthew and Eslinger, Hannah},
  title		= {Application-driven creation of building metadata models
		  with semantic sufficiency},
  year		= {2022},
  isbn		= {9781450398909},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3563357.3564083},
  doi		= {10.1145/3563357.3564083},
  abstract	= {Semantic metadata models such as Brick, RealEstateCore,
		  Project Haystack, and BOT promise to simplify and lower the
		  cost of developing software for smart buildings, enabling
		  the widespread deployment of energy efficiency
		  applications. However, creating these models remains a
		  challenge. Despite recent advances in creating models from
		  existing digital representations like point labels and
		  architectural models, there is still no feedback mechanism
		  to ensure that the human input to these methods results in
		  a model that can actually support the desired software.In
		  this paper, we introduce the notion of semantic
		  sufficiency, a practical principle for semantic metadata
		  model creation that asserts that a model is "finished" when
		  it contains the metadata necessary to support a given set
		  of applications. To support semantic sufficiency, we design
		  a standard representation for capturing application
		  metadata requirements and a templating system for
		  generating common metadata model components with limited
		  user input. We then construct an iterative model creation
		  workflow that integrates metadata requirements to direct
		  the model creation effort, and present several novel
		  optimizations that increase the model utility while
		  minimizing the effort by a human operator. These new
		  abstractions for model creation and validation lower model
		  development costs and ensure the utility of the resulting
		  model, thus facilitating the adoption of intelligent
		  building applications.},
  booktitle	= {Proceedings of the 9th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {228–237},
  numpages	= {10},
  keywords	= {applications, brick, metadata, ontology, semantics},
  location	= {Boston, Massachusetts},
  series	= {BuildSys '22}
}

@InProceedings{	  10.1145/3492547.3492680,
  author	= {Bekmanova, Gulmira and Nazyrova, Aizhan and Sharipbay,
		  Altynbek and Suvorovsky, Oleg and Somzhurek, Baubek},
  title		= {Two approaches of improving e-learning models qualities},
  year		= {2021},
  isbn		= {9781450390446},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3492547.3492680},
  doi		= {10.1145/3492547.3492680},
  abstract	= {ABSTRACTThe proposed two approaches of improving
		  e-learning models qualities let us redesign existing models
		  and e-learning systems.Model for organizing blended and
		  distance learning involves the creation of an individual
		  learning path, which makes it flexible. Ontological model
		  used like a tool for optimization and redesign existing
		  system modules.},
  booktitle	= {The 7th International Conference on Engineering \&amp; MIS
		  2021},
  articleno	= {83},
  numpages	= {3},
  keywords	= {Blended learning, artificial intelligence, distance
		  learning, e-learning, e-learning system optimization,
		  ontological model, personalized training},
  location	= {Almaty, Kazakhstan},
  series	= {ICEMIS'21}
}

@InProceedings{	  10.1145/3485447.3511924,
  author	= {Xiang, Yue and Wu, Xuan and Lu, Chang and Zhao, Yizheng},
  title		= {Creating Signature-Based Views for Description Logic
		  Ontologies with Transitivity and Qualified Number
		  Restrictions},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511924},
  doi		= {10.1145/3485447.3511924},
  abstract	= {Developing ontologies for the Semantic Web is a
		  time-consuming and error-prone task that typically requires
		  the investment of considerable manpower and resources, as
		  well as collaborative efforts. A potentially better idea is
		  to reuse the “off-the-shelf” ontologies, whenever
		  possible, somehow as per certain demands and requirements.
		  A promising way to achieve ontology reuse is through
		  creating views of ontologies, analogous to creating views
		  of databases, with the resulting views focusing on specific
		  topics and content of the original ontologies. This paper
		  explores the problem of creating views of ontologies using
		  a uniform interpolation approach. In particular, we develop
		  a novel and practical uniform interpolation method for
		  creating signature-based views for ontologies specified in
		  the description logic , a very expressive description logic
		  for which uniform interpolation has not been fully
		  addressed. The method is terminating and sound, and
		  computes uniform interpolants of -ontologies by eliminating
		  from the input ontologies the names not used in the view
		  using a forgetting procedure. This makes it the first and
		  so far the only approach to eliminate both concept and
		  (non-transitive) role names from -ontologies. Despite the
		  inherent difficulty of uniform interpolation for this level
		  of expressivity, an empirical evaluation with a
		  prototypical implementation show very good success rates on
		  a corpus of real-world ontologies, and demonstrates clear
		  algorithmic advantage over the state-of-the-art system
		  LETHE. This is extremely useful from the semantic web
		  perspective, as it provides knowledge engineers with a
		  powerful tool to create views of ontologies for ontology
		  reuse.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {808–817},
  numpages	= {10},
  keywords	= {Description Logics, Forgetting, Knowledge Representation
		  and Reasoning, Ontologies, The Semantic Web, Uniform
		  Interpolation},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3736733.3736737,
  author	= {Fletcher, George and Nahurna, Olha and Prytula, Matvii and
		  Stoyanovich, Julia},
  title		= {CREDAL: Close Reading of Data Models},
  year		= {2025},
  isbn		= {9798400719592},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3736733.3736737},
  doi		= {10.1145/3736733.3736737},
  abstract	= {Data models are foundational to the creation of data and
		  any data-driven system. Every algorithm, ML model,
		  statistical model, and database depends on a data model to
		  function. As such, data models are rich sites for examining
		  the material, social, and political conditions shaping
		  technical systems. Inspired by literary criticism, we
		  propose close readings of data models—treating them as
		  artifacts to be analyzed like texts. This practice
		  highlights the materiality, genealogy, techne, closure, and
		  design of data systems.While literary theory teaches that
		  no single reading is "correct," systematic guidance is
		  vital—especially for those in computing and data science,
		  where sociopolitical dimensions are often overlooked. To
		  address this gap, we introduce the CREDAL methodology for
		  close readings of data models. We describe its iterative
		  development and share results from a qualitative
		  evaluation, demonstrating its usability and value for
		  critical data studies.},
  booktitle	= {Proceedings of the Workshop on Human-In-the-Loop Data
		  Analytics},
  articleno	= {4},
  numpages	= {7},
  location	= {Intercontinental Berlin, Berlin, Germany},
  series	= {HILDA '25}
}

@Article{	  10.1145/3565364,
  author	= {Aameri, Bahar and Gr\"{u}ninger, Michael},
  title		= {Reducible Theories and Amalgamations of Models},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {1},
  issn		= {1529-3785},
  url		= {https://doi.org/10.1145/3565364},
  doi		= {10.1145/3565364},
  abstract	= {Within knowledge representation in artificial
		  intelligence, a first-order ontology is a theory in
		  first-order logic that axiomatizes the concepts in some
		  domain. Ontology verification is concerned with the
		  relationship between the intended models of an ontology and
		  the models of the axiomatization of the ontology. In
		  particular, we want to characterize the models of an
		  ontology up to isomorphism and determine whether or not
		  these models are equivalent to the intended models of the
		  ontology. Unfortunately, it can be quite difficult to
		  characterize the models of an ontology up to isomorphism.
		  In the first half of this article, we review the different
		  metalogical relationships between first-order theories and
		  identify which relationship is needed for ontology
		  verification. In particular, we will demonstrate that the
		  notion of logical synonymy is needed to specify a
		  representation theorem for the class of models of one
		  first-order ontology with respect to another. In the second
		  half of the article, we discuss the notion of reducible
		  theories and show we can specify representation theorems by
		  which models are constructed by amalgamating models of the
		  constituent ontologies.},
  journal	= {ACM Trans. Comput. Logic},
  month		= jan,
  articleno	= {9},
  numpages	= {24},
  keywords	= {Amalgamations of models, reducible theories, synonymous
		  theories, relative interpretation, model theory,
		  first-order logic}
}

@InProceedings{	  10.1145/3417990.3421406,
  author	= {Chammard, Thomas Boyer and Regalia, Blake and Karban,
		  Robert and Gomes, Ivan},
  title		= {Assisted authoring of model-based systems engineering
		  documents},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3421406},
  doi		= {10.1145/3417990.3421406},
  abstract	= {In systems engineering practices, system design and
		  analysis have historically been performed using a
		  document-centric approach where stakeholders produce a
		  number of documents that represent their views on a system
		  under development. Given the ad-hoc, disparate, and
		  informal nature of natural language documents, these views
		  become quickly inconsistent. Rigor in engineering work is
		  also lost in the transition from model-based engineering
		  design and analysis to engineering documents. Once the
		  documents are delivered, the engineering portion of the
		  work is disconnected. In the Open Model Based Engineering
		  Environment (OpenMBEE), Cross-References (aka
		  transclusions) synthesize relevant engineering information
		  where model elements are not simply hyperlinked, but
		  de-referenced in place in a document, upgrading a
		  document-based process with model-based engineering
		  technology. Those Cross-References are nowadays partially
		  created manually, putting a burden on the engineer who is
		  authoring the document. This paper presents an approach
		  which can assist the engineer by providing
		  machine-generated suggestions for Cross-References using
		  language processing, graph analysis, and clustering
		  technologies on model data managed by the OpenMBEE
		  infrastructure.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {36},
  numpages	= {7},
  keywords	= {MBSE, OpenMBEE, SysML, clustering, entity linking, graph
		  analysis, natural language processing},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@InProceedings{	  10.1145/3443467.3443723,
  author	= {Yuan, Ming and Yang, Shulin and Gu, Mengdie and Gu,
		  Huijie},
  title		= {Analysis and Research on Book Recommendation Model Based
		  on Big Data},
  year		= {2021},
  isbn		= {9781450387811},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3443467.3443723},
  doi		= {10.1145/3443467.3443723},
  abstract	= {In the context of big data, how to define user behavior
		  models and provide personalized reading services for
		  readers by mining large amounts of user data is a problem
		  that the current reading platform needs to optimize
		  urgently. First, we need to analyze the user behavior model
		  to construct the research status and existing problems, in
		  order to provide large data personalized services, targeted
		  user behavior model based on reading platform construction
		  strategies and construction methods, and designs a user
		  logging library is utilized to extract the user interest in
		  dominant and recessive demand ontology of personalized
		  service plan. The user behavior model based on ontology can
		  be technically seamlessly connected with the big data
		  analysis platform, so as to provide real-time and accurate
		  services, which can effectively deal with the challenges of
		  "knowledge trek", "information overload" and "emotional
		  loss" faced by the personalized service of the reading
		  platform in the current big data environment.},
  booktitle	= {Proceedings of the 2020 4th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {21–25},
  numpages	= {5},
  keywords	= {Big Data, Linked Data, Ontology, Personalized Service,
		  User Behavior Model},
  location	= {Xiamen, China},
  series	= {EITCE '20}
}

@InProceedings{	  10.1145/3711896.3737444,
  author	= {Difallah, Djellel},
  title		= {WikiRAG: Revisiting Wikidata KGC Datasets with Community
		  Updates and Retrieval-Augmented Generation},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3737444},
  doi		= {10.1145/3711896.3737444},
  abstract	= {Link prediction is an important task for knowledge graph
		  completion and curation, and it has received significant
		  attention from the research community. However, researchers
		  often train and evaluate new models on small or outdated
		  datasets that do not reflect the current state of
		  knowledge, thereby disregarding new information and the
		  rich textual content often linked to knowledge graphs. As a
		  result, many opportunities to leverage these dimensions are
		  missed. We introduce WikiRAG, a framework for knowledge
		  completion and evaluation derived from Wikidata and
		  Wikipedia, which enables research integrating retrieval
		  techniques and large language models. Our framework
		  combines the following contributions: (i) We revisit the
		  Wikidata5M dataset by updating it to reflect the current
		  state of Wikidata and providing automated tools for its
		  periodic maintenance. (ii) We enrich the dataset with
		  long-form textual content sourced from Wikipedia, enabling
		  research that goes beyond traditional graph structures and
		  shallow text methods toward dense retrieval techniques.
		  (iii) We propose a simple yet effective baseline that
		  leverages retrieval-augmented generation, demonstrating the
		  utility of the dataset and integrating language model
		  capabilities for link prediction. The revised dataset,
		  coined Wikidata5M-RE, shows that the original graph grew by
		  roughly 50\% in the number of edges, while 10\% of the
		  edges have been removed. A comparative analysis of classic
		  methods demonstrates that these changes can impact
		  downstream task evaluation. Finally, our evaluation of
		  WikiRAG's KGC method shows an improvement of up to 9\% in
		  link prediction accuracy over state-of-the-art baselines,
		  setting the stage for a new avenue in knowledge completion
		  that uses deep information extraction. The source code,
		  data, and other artifacts have been made available on the
		  project website: https://github.com/colab-nyuad/WikiRAG},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {5391–5401},
  numpages	= {11},
  keywords	= {benchmarking, knowledge graph completion, large language
		  models, retrieval augmented generation, wikidata},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@InProceedings{	  10.1145/3487664.3487781,
  author	= {Martinez-Gil, Jorge and Yin, Shaoyi and K\"{u}ng, Josef
		  and Morvan, Franck},
  title		= {Matching Large Biomedical Ontologies Using Symbolic
		  Regression},
  year		= {2022},
  isbn		= {9781450395564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487664.3487781},
  doi		= {10.1145/3487664.3487781},
  abstract	= {The problem of ontology matching consists of finding the
		  semantic correspondences between two ontologies that,
		  although belonging to the same domain, have been developed
		  separately. Matching methods are of great importance since
		  they allow us to find the pivot points from which an
		  automatic data integration process can be established.
		  Unlike the most recent developments based on deep learning,
		  this study presents our research on the development of new
		  methods for ontology matching that are accurate and
		  interpretable at the same time. For this purpose, we rely
		  on a symbolic regression model specifically trained to find
		  the mathematical expression that can solve the ground truth
		  accurately, with the possibility of being understood by a
		  human operator and forcing the processor to consume as
		  little energy as possible. The experimental evaluation
		  results show that our approach seems to be promising.},
  booktitle	= {The 23rd International Conference on Information
		  Integration and Web Intelligence},
  pages		= {162–167},
  numpages	= {6},
  keywords	= {Information Integration, Ontology Matching, Similarity
		  Measures},
  location	= {Linz, Austria},
  series	= {iiWAS2021}
}

@InProceedings{	  10.1145/3674658.3674660,
  author	= {Kohli, Nikita and Tomal, Jabed and Lin, Wenjun and Yan,
		  Yan},
  title		= {PentaPen: Combining Penalized Models to Identify Important
		  SNPs on Whole-genome Arabidopsis thaliana Data},
  year		= {2024},
  isbn		= {9798400717666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3674658.3674660},
  doi		= {10.1145/3674658.3674660},
  abstract	= {In the rapidly advancing field of genomics, the
		  identification of Single Nucleotide Polymorphisms (SNPs)
		  plays a crucial role in understanding complex phenotypic
		  traits. This study introduces “PentaPen”, an innovative
		  computational workflow which combines the strengths of five
		  penalized models to achieve improved accuracy in SNP
		  detection. We compare the performance of PentaPen with
		  existing models, highlighting its advantages in solving
		  problems arising from when the number of predictors exceeds
		  the number of samples. Beyond model comparison, we provide
		  insights into PentaPen’s effectiveness in utilizing all
		  SNPs as input, streamlines data pre-processing, and
		  leverages parallel computation, enabling the workflow a
		  considerable stride in SNP detection. Furthermore, a
		  thorough evaluation and comparison of computational
		  complexities signifies competitive edge of the workflow
		  over individual penalized models. As future research
		  directions, we propose applications of PentaPen to
		  plant-specific characteristics and suggest further
		  explorations to assess the robustness of its findings. In
		  summary, this manuscript presents the genomics community
		  with a tool that combines computational efficiency with
		  high-precision SNP detection, making a strong contribution
		  to the field of genomic research.},
  booktitle	= {Proceedings of the 2024 16th International Conference on
		  Bioinformatics and Biomedical Technology},
  pages		= {9–16},
  numpages	= {8},
  keywords	= {Genome-Wide Association Study, Single Nucleotide
		  Polymorphism, SNP Identification, Machine Learning, High
		  Dimensional Data, Regression, Classification},
  location	= { },
  series	= {ICBBT '24}
}

@InProceedings{	  10.1145/3640310.3674085,
  author	= {Ben Chaaben, Meriem and Ben Sghaier, Oussama and Dhaouadi,
		  Mouna and Elrasheed, Nafisa and Darif, Ikram and Jaoua,
		  Imen and Oakes, Bentley and Syriani, Eugene and Hamdaqa,
		  Mohammad},
  title		= {Toward Intelligent Generation of Tailored Graphical
		  Concrete Syntax},
  year		= {2024},
  isbn		= {9798400705045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640310.3674085},
  doi		= {10.1145/3640310.3674085},
  abstract	= {In model-driven engineering, the concrete syntax of a
		  domain-specific modeling language (DSML) is fundamental as
		  it constitutes the primary point of interaction between the
		  user and the DSML. Nevertheless, the conventional
		  one-size-fits-all approach to concrete syntax often
		  undermines the effectiveness of DSMLs, as it fails to
		  accommodate the diverse constraints and specific
		  requirements inherent to diverse users and usage contexts.
		  Such shortcomings can lead to a significant decline in the
		  performance, usability, and efficiency of DSMLs. This
		  vision paper proposes a conceptual framework to generate
		  concrete syntax intelligently. Our framework considers
		  multiple concerns of users and aims to align the concrete
		  syntax with the context of the DSML usage. Additionally, we
		  detail a baseline process to employ our framework in
		  practice, leveraging large language models to expedite the
		  generation of tailored concrete syntax. We illustrate the
		  potential of our vision with two concrete examples and
		  discuss the shortcomings and research challenges of current
		  intelligent generation techniques.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {160–171},
  numpages	= {12},
  keywords	= {Artificial Intelligence, Concrete Syntax, Domain-specific
		  Modeling Languages, Large Language Models},
  location	= {Linz, Austria},
  series	= {MODELS '24}
}

@InProceedings{	  10.1145/3640457.3687114,
  author	= {Anelli, Vito Walter and Ferrara, Antonio and Musto,
		  Cataldo and Narducci, Fedelucio and Ragone, Azzurra and
		  Zanker, Markus},
  title		= {Sixth Knowledge-aware and Conversational Recommender
		  Systems Workshop (KaRS)},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3687114},
  doi		= {10.1145/3640457.3687114},
  abstract	= {Recommender systems, though widely used, often struggle to
		  engage users effectively. While deep learning methods have
		  enhanced connections between users and items, they often
		  neglect the user’s perspective. Knowledge-based
		  approaches, utilizing knowledge graphs, offer semantic
		  insights and address issues like knowledge graph
		  embeddings, hybrid recommendation, and interpretable
		  recommendation. More recently, neural-symbolic systems,
		  combining data-driven and symbolic techniques, show promise
		  in recommendation systems, especially when used with
		  knowledge graphs. Moreover, content features become vital
		  in conversational recommender systems, which demand
		  multi-turn dialogues. Recent literature highlights
		  increasing interest in this area, particularly with the
		  emergence of Large Language Models (LLMs), which excel in
		  understanding user queries and generating recommendations
		  in natural language. Sixth Knowledge-aware and
		  Conversational Recommender Systems (KaRS) Workshop aims to
		  disseminate advancements and discuss about challenges and
		  opportunities.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {1245–1249},
  numpages	= {5},
  keywords	= {conversational agents, knowledge graphs, large language
		  models, natural language processing, neuro-symbolic,
		  recommender systems},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3438872.3439091,
  author	= {Pei, Pei and Xuejing, Ding and Deqing, Zhang},
  title		= {Construction of Curriculum Knowledge Map based on
		  Ontology},
  year		= {2020},
  isbn		= {9781450388306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3438872.3439091},
  doi		= {10.1145/3438872.3439091},
  abstract	= {With the rapid development of science and technology,
		  knowledge update is accelerating, which puts forward higher
		  requirements for teachers and students in Colleges and
		  universities, and needs to grasp the latest development
		  trends of curriculum knowledge related fields faster, more
		  comprehensive and more accurate. Many schools have
		  digitized their educational resources. However, the
		  traditional sharing of educational resources lacks a
		  unified knowledge representation structure, which makes the
		  sharing and reuse of learning resources unsatisfactory.
		  Curriculum is the core of school knowledge teaching. It
		  evaluates the curriculum system and discipline system in
		  order to achieve certain teaching objectives. Curriculum
		  knowledge includes explicit knowledge and tacit
		  knowledge.Knowledge map is a kind of model which can
		  describe knowledge in semantic and knowledge level. Its
		  purpose is to acquire, organize and present knowledge in a
		  general and intuitive way, to search and match knowledge
		  quickly, so as to improve the utilization of knowledge by
		  learners and knowledge workers. Curriculum knowledge has
		  obvious ontology characteristics, and there are many
		  inconveniences and shortcomings in the presentation of
		  traditional knowledge map. Using ontology technology to
		  construct curriculum knowledge map can not only reflect the
		  relationship between knowledge modules, but also realize
		  the mining and representation of more knowledge relations
		  and types such as tacit knowledge to a certain extent.},
  booktitle	= {Proceedings of the 2020 2nd International Conference on
		  Robotics, Intelligent Control and Artificial Intelligence},
  pages		= {259–265},
  numpages	= {7},
  keywords	= {curriculum knowledge, knowledge map, ontology, ontology
		  construction},
  location	= {Shanghai, China},
  series	= {RICAI '20}
}

@InProceedings{	  10.1145/3514094.3534137,
  author	= {Franklin, Jade S. and Bhanot, Karan and Ghalwash, Mohamed
		  and Bennett, Kristin P. and McCusker, Jamie and McGuinness,
		  Deborah L.},
  title		= {An Ontology for Fairness Metrics},
  year		= {2022},
  isbn		= {9781450392471},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3514094.3534137},
  doi		= {10.1145/3514094.3534137},
  abstract	= {Recent research has revealed that many machine-learning
		  models and the datasets they are trained on suffer from
		  various forms of bias, and a large number of different
		  fairness metrics have been created to measure this bias.
		  However, determining which metrics to use, as well as
		  interpreting their results, is difficult for a non-expert
		  due to a lack of clear guidance and issues of ambiguity or
		  alternate naming schemes between different research papers.
		  To address this knowledge gap, we present the Fairness
		  Metrics Ontology (FMO), a comprehensive and extensible
		  knowledge resource that defines each fairness metric,
		  describes their use cases, and details the relationships
		  between them. We include additional concepts related to
		  fairness and machine learning models, enabling the
		  representation of specific fairness information within a
		  resource description framework (RDF) knowledge graph. We
		  evaluate the ontology by examining the process of how
		  reasoning-based queries to the ontology were used to guide
		  the fairness metric-based evaluation of a synthetic data
		  model.},
  booktitle	= {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {265–275},
  numpages	= {11},
  keywords	= {bias, fairness metric, machine learning evaluation, rdf
		  knowledge graph},
  location	= {Oxford, United Kingdom},
  series	= {AIES '22}
}

@InBook{	  10.1145/3677389.3702536,
  author	= {Zielinski, Andrea and Hirzel, Simon and Arnold-Keifer,
		  Sonja},
  title		= {Enhancing Digital Libraries with Automated Definition
		  Generation},
  year		= {2025},
  isbn		= {9798400710933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677389.3702536},
  abstract	= {Scientific domains encompass many concepts that require a
		  concise term definition to enable a common understanding
		  among researchers, in particular for interdisciplinary
		  fields. In digital libraries, information access and
		  sharing is often facilitated by terminology databases.
		  However, building up such resources is expensive to produce
		  manually and requires expert knowledge. Automatically
		  generating definitions for scientific terms has become a
		  hot research topic recently that can reduce the manual
		  burden. However, current methods heavily rely on large
		  language models (LLMs) that store factual knowledge in
		  their parameters, so that knowledge cannot be easily
		  updated for emerging scientific terms. Furthermore, a major
		  shortcoming of these models is that they are prone to
		  hallucination and their output is difficult to control. To
		  bridge these gaps, we propose to address the task of
		  definition generation through guided abstractive
		  summarization, incorporating key information from external
		  resources. At test time, we augment the model with
		  retrieved abstracts from Scopus and use automatically
		  extracted topics and keywords as guidance, both essential
		  for definition generation. To this aim, our approach takes
		  into account two relevant sub-tasks in the process, a)
		  predicting the topic class and b) generating hypernym
		  candidates for the term. Our proposed pipelined approach
		  for automatic guided definition generation achieves
		  significant performance improvement over the standard
		  baselines as well as relevant prior works on this problem.
		  We use BLEU, ROUGE and BERTScore to automatically evaluate
		  the quality of the systems on our benchmark and carry out a
		  human evaluation to assess fluency, relevancy, coherence
		  and factuality of the output. Our experiments show that
		  LLMs can provide fluent and coherent definitions, and are
		  often on par with human created definitions. Yet, there is
		  still room for improvement on identifying relevant content
		  and improving factual correctness.},
  booktitle	= {Proceedings of the 24th ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {57},
  numpages	= {11}
}

@InProceedings{	  10.1145/3319499.3328238,
  author	= {Ousmer, Mehdi and Vanderdonckt, Jean and Buraga, Sabin},
  title		= {An ontology for reasoning on body-based gestures},
  year		= {2019},
  isbn		= {9781450367455},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3319499.3328238},
  doi		= {10.1145/3319499.3328238},
  abstract	= {Body-based gestures, such as acquired by Kinect sensor,
		  today benefit from efficient tools for their recognition
		  and development, but less for automated reasoning. To
		  facilitate this activity, an ontology for structuring
		  body-based gestures, based on user, body and body parts,
		  gestures, and environment, is designed and encoded in
		  Ontology Web Language according to modelling triples
		  (subject, predicate, object). As a proof-of-concept and to
		  feed this ontology, a gesture elicitation study collected
		  24 participants X 19 referents for IoT tasks = 456 elicited
		  body-based gestures, which were classified and expressed
		  according to the ontology.},
  booktitle	= {Proceedings of the ACM SIGCHI Symposium on Engineering
		  Interactive Computing Systems},
  articleno	= {17},
  numpages	= {6},
  keywords	= {Microsoft Kinect, gesture interaction, natural gestures,
		  ontology web language, resource description file},
  location	= {Valencia, Spain},
  series	= {EICS '19}
}

@InProceedings{	  10.1145/3555776.3577696,
  author	= {Heng, Samedi and Tsilionis, Konstantinos and Wautelet,
		  Yves},
  title		= {Building User Stories and Behavior Driven Development
		  Scenarios with a Strict Set of Concepts: Ontology, Benefits
		  and Primary Validation},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3577696},
  doi		= {10.1145/3555776.3577696},
  abstract	= {Behavior Driven Development (BDD) offers a way to express
		  scenarios, written in structured natural language, on how
		  the system should act to fulfill a requirement. Such a test
		  scenario is written together with the requirement; this way
		  these two can be conceived in unison, nested into each
		  other. Lots of templates have been written to construct BDD
		  scenarios and various practices were born out of usage. We
		  fail to find documentation on templates. A strict set of
		  templates with a clear definition of the used keywords
		  aligned with the intends of BDD has been proposed recently
		  in the form of an ontology. The present paper (i) evaluates
		  the ontology on existing BDD scenarios found in the GitHub
		  repository (exogenous validation) and (ii) merges an
		  ontology for user story elements' representation with one
		  for expressing BDD scenarios to evaluate its ability to
		  guide the writing of BDD scenarios (endogenous validation).
		  By linking both through strictly-identified concepts, we
		  (i) provide guidance to the practitioner in the agile
		  requirements engineering phase and (ii) with the adequate
		  tagging of elements during the requirements engineering
		  process we get meta-data allowing to suggest treatments to
		  the BDD scenario (e.g. test automation or forward
		  engineering into a software architecture or even code).},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1422–1429},
  numpages	= {8},
  keywords	= {behavior driven development, user stories, test scenarios,
		  agile development, scrum},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@InProceedings{	  10.1145/3445969.3450429,
  author	= {Chukkapalli, Sai Sree Laya and Aziz, Shaik Barakhat and
		  Alotaibi, Nouran and Mittal, Sudip and Gupta, Maanak and
		  Abdelsalam, Mahmoud},
  title		= {Ontology driven AI and Access Control Systems for Smart
		  Fisheries},
  year		= {2021},
  isbn		= {9781450383196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3445969.3450429},
  doi		= {10.1145/3445969.3450429},
  abstract	= {Increasing number of internet connected devices has paved
		  a path for smarter ecosystems in various sectors such as
		  agriculture, aquaculture, manufacturing, healthcare, etc.
		  Especially, integrating technologies like big data,
		  artificial intelligence (AI), blockchain, etc. with
		  internet connected devices has increased efficiency and
		  productivity. Therefore, fishery farmers have started
		  adopting smart fisheries technologies to better manage
		  their fish farms. Despite their technological advancements
		  smart fisheries are exposed and vulnerable to cyber-attacks
		  that would cause a negative impact on the ecosystem both
		  physically and economically.Therefore in this paper, we
		  present a smart fisheries ecosystem where the architecture
		  describes various interactions that happen between internet
		  connected devices. We develop a smart fisheries ontology
		  based on the architecture and implement Attribute Based
		  Access Control System (ABAC) where access to resources of
		  smart fisheries is granted by evaluating the requests. We
		  also discuss how access control decisions are made in
		  multiple use case scenarios of a smart fisheries ecosystem.
		  Furthermore, we elaborate on some AI applications that
		  would enhance the smart fisheries ecosystem.},
  booktitle	= {Proceedings of the 2021 ACM Workshop on Secure and
		  Trustworthy Cyber-Physical Systems},
  pages		= {59–68},
  numpages	= {10},
  keywords	= {access control, artificial intelligence, cybersecurity,
		  ontology, smart fisheries},
  location	= {Virtual Event, USA},
  series	= {SAT-CPS '21}
}

@InProceedings{	  10.1145/3733102.3733152,
  author	= {Vielhauer, Claus and Loewe, Fabian and Pilgermann,
		  Michael},
  title		= {Towards Modeling Hidden \&amp; Steganographic Malware
		  Communication based on Images},
  year		= {2025},
  isbn		= {9798400718878},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3733102.3733152},
  doi		= {10.1145/3733102.3733152},
  abstract	= {Recently, an increasing number of IT security incidents
		  involving malware, which makes use of hidden and
		  steganographic channels for malicious communication (a.k.a.
		  as "stegomalware"), can be observed in the wild. Especially
		  the use of images to hide malicious code is rising. In
		  consideration of this shift, a new model is proposed in
		  this paper, which aims to help security professionals to
		  identify and analyze incidents revolving around
		  steganographic malware in the future. The model focuses on
		  practical aspects of steganalysis of communication data to
		  elaborate linking properties to previous code analysis
		  knowledge. The model features two distinct roles that
		  interact with a knowledge base which stores malware
		  features and helps building a context for the incident. For
		  evaluation, two image steganography malware types are
		  chosen from popular databases (malpedia and MITRE
		  ATT&amp;CK®), which are analyzed in multiple steps
		  including steganalysis and code analysis. It is
		  conceptually shown, how the extracted features can be
		  stored in a knowledge base for later use to identify
		  stegomalware from communication data without the need of a
		  thorough code analysis. This allows to uncover previously
		  hidden meta-information about the examined malicious
		  programs, enrich the incident’s forensic context traces
		  and thus allows for thorough forensic insights, including
		  attribution and improved preventive security measures in
		  the future.},
  booktitle	= {Proceedings of the 2025 ACM Workshop on Information Hiding
		  and Multimedia Security},
  pages		= {52–63},
  numpages	= {12},
  keywords	= {Media forensics, stegomalware, image steganography,
		  attribution},
  location	= { },
  series	= {IH&amp;MMSEC '25}
}

@Article{	  10.1109/tcbb.2022.3170301,
  author	= {Feng, Yuhao and Qi, Lei and Tian, Weidong},
  title		= {PhenoBERT: A Combined Deep Learning Method for Automated
		  Recognition of Human Phenotype Ontology},
  year		= {2022},
  issue_date	= {March-April 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {2},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3170301},
  doi		= {10.1109/TCBB.2022.3170301},
  abstract	= {Automated recognition of Human Phenotype Ontology (HPO)
		  terms from clinical texts is of significant interest to the
		  field of clinical data mining. In this study, we develop a
		  combined deep learning method named PhenoBERT for this
		  purpose. PhenoBERT uses BERT, currently the
		  state-of-the-art NLP model, as its core model for
		  evaluating whether a clinically relevant text segment (CTS)
		  could be represented by an HPO term. However, to avoid
		  unnecessary comparison of a CTS with each of ∼14,000 HPO
		  terms using BERT, we introduce a two-levels CNN module
		  consisting of a series of CNN models organized at two
		  levels in PhenoBERT. For a given CTS, the CNN module
		  produces only a short list of candidate HPO terms for BERT
		  to evaluate, significantly improving the computational
		  efficiency. In addition, BERT is able to assign an ancestor
		  HPO term to a CTS when recognition of the direct HPO term
		  is not successful, mimicking the process of HPO term
		  assignment by human. In two benchmarks, PhenoBERT
		  outperforms four traditional dictionary-based methods and
		  two recently developed deep learning-based methods in two
		  benchmark tests, and its advantage is more obvious when the
		  recognition task is more challenging. As such, PhenoBERT is
		  of great use for assisting in the mining of clinical text
		  data.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= apr,
  pages		= {1269–1277},
  numpages	= {9}
}

@InProceedings{	  10.1145/3644116.3644311,
  author	= {Zhang, Xiyan and Hu, Chenping and Zhang, Lei and Huang,
		  Zhisheng and Qin, Hongyun},
  title		= {Analysis of the mechanism of antipsychotics induced
		  abnormal ECG using Medical Ontologies and Medical Knowledge
		  Graphs},
  year		= {2024},
  isbn		= {9798400708138},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644116.3644311},
  doi		= {10.1145/3644116.3644311},
  abstract	= {Objective: Our aim is to understand the underlying
		  mechanisms of antipsychotics-induced ECG abnormalities by
		  using artificial intelligence (AI) method and information
		  processing technique. Methods: This study employed the
		  SNOMED CT (Systematized Nomenclature of Medicine - Clinical
		  Terms) and the knowledge graph of brain science, to find
		  relevant concepts associated with ECG abnormalities and
		  antipsychotics usage. Results: Approximately 30 relevant
		  publications were found, only 10 articles were included in
		  this comprehensive analysis study. Our findings suggest
		  that the underlying mechanisms of antipsychotic-induced ECG
		  abnormalities include immediate, medium and lasting effects
		  on the predisposing level, the autonomic nervous system,
		  cardiomyocytes, and the blood vessel levels. Conclusions:
		  The potential mechanisms underpinning
		  antipsychotics-induced ECG abnormalities include
		  predisposing, the autonomic nervous system, cardiomyocytes,
		  and the blood vessel levels by use of the NOMED CT and the
		  knowledge graph of brain science.},
  booktitle	= {Proceedings of the 2023 4th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {1146–1151},
  numpages	= {6},
  location	= {Chengdu, China},
  series	= {ISAIMS '23}
}

@Article{	  10.1145/3733234,
  author	= {Zhao, Chuang and Tang, Hui and Zhao, Hongke and Li,
		  Xiaomeng},
  title		= {Beyond Sequential Patterns: Rethinking Healthcare
		  Predictions with Contextual Insights},
  year		= {2025},
  issue_date	= {July 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3733234},
  doi		= {10.1145/3733234},
  abstract	= {Healthcare predictions, such as readmission prediction,
		  stand as a cornerstone of societal well-being, exerting a
		  profound influence on individual health outcomes and
		  communal vitality. Existing research primarily employs
		  advanced graph neural networks and sequential algorithms
		  for patient modeling, with a focus on discerning the
		  connections and sequential patterns inherent in Electronic
		  Health Records (EHRs). However, the heterogeneity of entity
		  interactions, the locality of EHR data, and the oversight
		  of target relevance hinder further improvements. To address
		  these limitations, we introduce a novel framework Beyond
		  Sequential Patterns (BSP), which facilitates precise
		  healthcare predictions by incorporating tri-contextual
		  information. Specifically, we establish a symptom-driven
		  hypergraph network with four semantic hyperedges tailored
		  to the intricacies of the healthcare scenario, such as
		  ontology. This serves as a global context, tracking the
		  heterogeneous entity collaboration within and across
		  patients. Moreover, we construct an extensive knowledge
		  graph leveraging existing medical databases and large
		  language models. By sampling and refining knowledge
		  subgraphs as local context, we bolster the semantic
		  associations of medical entities from closed-set EHR data
		  to the open world. Finally, we introduce the candidate
		  context, an explicit entity-relation loss. It enforces the
		  neighbor consistency between the target and the
		  representation during optimization, thus accounting for
		  correlations among targets. Extensive experiments and
		  rigorous robustness analysis on five tasks derived from
		  four large medical datasets underscore the BSP’s
		  superiority over the leading baselines, with improvements
		  of 11\%, 3\%, 11\%, 3.5\%, and 2\% across five tasks,
		  demonstrating the efficacy of incorporating diverse contexts.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jul,
  articleno	= {107},
  numpages	= {32},
  keywords	= {Healthcare prediction, Hypergraph learning, Large language
		  model}
}

@InProceedings{	  10.1145/3590777.3590785,
  author	= {Ukegbu, Chibuzo and Neupane, Ramesh and Mehrpouyan, Hoda},
  title		= {Ontology-based Framework for Boundary Verification of
		  Safety and Security Properties in Industrial Control
		  Systems},
  year		= {2023},
  isbn		= {9781450398299},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3590777.3590785},
  doi		= {10.1145/3590777.3590785},
  abstract	= {As part of Industrial Control Systems (ICS), the control
		  logic controls the physical processes of critical
		  infrastructures such as power plants and water and gas
		  distribution. The Programmable Logic Controller (PLC)
		  commonly manages these processes through actuators based on
		  information received from sensor readings. Therefore,
		  boundary checking is essential in ICS because sensor
		  readings and actuator values must be within the safe range
		  to ensure safe and secure ICS operation. In this paper, we
		  propose an ontology-based approach to provide the knowledge
		  required to verify the boundaries of ICS components with
		  respect to their safety and security specifications. For
		  the proof of concept, the formal model of the Programmable
		  Logic Controller (PLC) is created in UPPAAL and validated
		  in UPPAAL-API. Then, the proposed boundary verification
		  algorithm is used to import the required information from
		  the safety/security ontology},
  booktitle	= {Proceedings of the 2023 European Interdisciplinary
		  Cybersecurity Conference},
  pages		= {47–52},
  numpages	= {6},
  keywords	= {Control Systems, Formal Verification, Security
		  Properties},
  location	= {Stavanger, Norway},
  series	= {EICC '23}
}

@InProceedings{	  10.1145/3429523.3429534,
  author	= {Wu, Jifang and Lv, Jianghua and Guo, Haoming and Ma,
		  Shilong},
  title		= {Ontology Matching by Jointly Encoding Terminological
		  Description and Network Structure},
  year		= {2020},
  isbn		= {9781450375276},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3429523.3429534},
  doi		= {10.1145/3429523.3429534},
  abstract	= {Ontology matching is usually performed to find semantic
		  correspondences between the entity elements of different
		  ontologies to enable interoperability. Current research on
		  ontology matching has largely focused on representation
		  learning. However, there still exist two limitations.
		  Firstly, they are only used in the element level matching
		  phase, ignoring relations of the entity. Secondly, the
		  final alignment threshold is usually determined manually
		  within these methods. It is difficult for an expert to
		  adjust the threshold value and even more for non-expert
		  user. To address these issues, we propose an alternative
		  ontology matching framework, which models the matching
		  process by embedding techniques with jointly encoding
		  ontology terminological description and network structure.
		  We further improve our iterative final alignment method by
		  introducing an automatic adjustment of threshold method.
		  Finally, we perform an experimental evaluation and compare
		  it with state-of-the-art ontology matching systems on four
		  Ontology Alignment Evaluation Initiative (OAEI) datasets.
		  Our approach performs better than most of the systems and
		  achieves a competitive performance. Moreover, we obtained
		  F-measure values of 93.8\% and 90.8\% on the OAEI Large
		  Biomedical Ontologies FMA-NCI and FMA-SNOMED subtasks.},
  booktitle	= {Proceedings of the 2020 5th International Conference on
		  Cloud Computing and Internet of Things},
  pages		= {77–85},
  numpages	= {9},
  keywords	= {Ontology matching, final alignment, graph attention-based
		  autoencoder, network embedding, semantic similarity},
  location	= {Okinawa, Japan},
  series	= {CCIOT '20}
}

@InProceedings{	  10.1145/3651671.3651779,
  author	= {Liu, Lijuan and Shi, Li},
  title		= {An Intelligent Risk Mining Method by Whole
		  Process-Oriented Risk Analysis Model},
  year		= {2024},
  isbn		= {9798400709234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3651671.3651779},
  doi		= {10.1145/3651671.3651779},
  abstract	= {This paper proposes a risk intelligent analysis method
		  oriented to the whole process of events and applications,
		  it is used to solve the problem of difficult and incomplete
		  identification of risks, to dig out the risks behind it,
		  and to provide help to ensure the safety of the public.
		  Specifically, through step-by-step mining of incidents and
		  application usage risks, combined with the event
		  evolutionary graph, the model is run to calculate the
		  similarity of text risks for comprehensive analysis, the
		  innovative point is to propose a risk weighting model for
		  the subject and object entities. Experiment shows that the
		  method can reflect the complete risk of events and
		  applications, and has improved recall and precision. The
		  advantage of this method is the integration of multiple
		  perspectives and disciplines, which has guiding
		  significance in practice.},
  booktitle	= {Proceedings of the 2024 16th International Conference on
		  Machine Learning and Computing},
  pages		= {616–620},
  numpages	= {5},
  keywords	= {Whole Process-Oriented analysis, artificial intelligence,
		  event evolutionary graph, risk mining},
  location	= {Shenzhen, China},
  series	= {ICMLC '24}
}

@InProceedings{	  10.1145/3366424.3383755,
  author	= {Geng, Qian and Deng, Siyu and Jia, Danping and Jin, Jian},
  title		= {Cross-domain Ontology Construction and Alignment from
		  Online Product Reviews},
  year		= {2020},
  isbn		= {9781450370240},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366424.3383755},
  doi		= {10.1145/3366424.3383755},
  abstract	= {Online customer product reviews often contain detailed
		  sentiment attitudes towards different aspects of products
		  and these online opinions help potential consumers to be
		  familiar with products. The introduction of
		  well-constructed domain ontology from online product
		  reviews helps potential consumers to obtain relevant
		  information about products quickly. Nonetheless, they may
		  compare products in multiple domains for purchase
		  decisions. On this basis, the comparison of products in
		  different domains induces that ontology alignment becomes a
		  fundamental task to form a cross-domain ontology. In this
		  paper, a series of natural language processing approaches
		  are applied to construct two domain ontologies from online
		  product reviews automatically. Next, a new ontology
		  alignment method is proposed for consumers to make purchase
		  decisions regarding cross-domain product comparisons, in
		  which a semantic-based algorithm and a structure-based
		  algorithm are integrated to form a cross-domain ontology.
		  Categories of experiments were conducted on reviews of
		  smartphone and digital camera. Compared with benchmarked
		  alignment tools, the proposed method performed 5\% better
		  in terms of F1 on ontology alignment. Finally, a case study
		  with a customer friendly website is illustrated to present
		  how the alignment of cross-domain ontology help consumers
		  on purchase decision support.},
  booktitle	= {Companion Proceedings of the Web Conference 2020},
  pages		= {401–408},
  numpages	= {8},
  keywords	= {ontology alignment, ontology construction, product
		  comparison, product review, purchase decision making.},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@Article{	  10.1145/3641859,
  author	= {Che, Shangkun and Liu, Hongyan and Liu, Shen},
  title		= {Tagging Items with Emerging Tags: A Neural Topic Model
		  Based Few-Shot Learning Approach},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3641859},
  doi		= {10.1145/3641859},
  abstract	= {The tagging system has become a primary tool to organize
		  information resources on the Internet, which benefits both
		  users and the platforms. To build a successful tagging
		  system, automatic tagging methods are desired. With the
		  development of society, new tags keep emerging. The problem
		  of tagging items with emerging tags is an open challenge
		  for an automatic tagging system, and it has not been well
		  studied in the literature. We define this problem as a
		  tag-centered cold-start problem in this study and propose a
		  novel neural topic model based few-shot learning method
		  named NTFSL to solve the problem. In our proposed method,
		  we innovatively fuse the topic modeling task with the
		  few-shot learning task, endowing the model with the
		  capability to infer effective topics to solve the
		  tag-centered cold-start problem with the property of
		  interpretability. Meanwhile, we propose a novel neural
		  topic model for the topic modeling task to improve the
		  quality of inferred topics, which helps enhance the tagging
		  performance. Furthermore, we develop a novel inference
		  method based on the variational auto-encoding framework for
		  model inference. We conducted extensive experiments on two
		  real-world datasets, and the results demonstrate the
		  superior performance of our proposed model compared with
		  state-of-the-art machine learning methods. Case studies
		  also show the interpretability of the model.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= mar,
  articleno	= {102},
  numpages	= {37},
  keywords	= {Few-shot learning, neural topic model, automatic tagging,
		  generative probabilistic model, classification, deep
		  learning}
}

@Article{	  10.1145/3660521,
  author	= {Zeng, Kaisheng and Jin, Hailong and Lv, Xin and Zhu,
		  Fangwei and Hou, Lei and Zhang, Yi and Pang, Fan and Qi, Yu
		  and Liu, Dingxiao and Li, Juanzi and Feng, Ling},
  title		= {XLORE 3: A Large-Scale Multilingual Knowledge Graph from
		  Heterogeneous Wiki Knowledge Resources},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3660521},
  doi		= {10.1145/3660521},
  abstract	= {In recent years, knowledge graph (KG) has attracted
		  significant attention from academia and industry, resulting
		  in the development of numerous technologies for KG
		  construction, completion, and application. XLORE is one of
		  the largest multilingual KGs built from Baidu Baike and
		  Wikipedia via a series of knowledge modeling and
		  acquisition methods. In this article, we utilize systematic
		  methods to improve XLORE's data quality and present its
		  latest version, XLORE 3, which enables the effective
		  integration and management of heterogeneous knowledge from
		  diverse resources. Compared with previous versions, XLORE 3
		  has three major advantages: (1) We design a comprehensive
		  and reasonable schema, namely XLORE ontology, which can
		  effectively organize and manage entities from various
		  resources. (2) We merge equivalent entities in different
		  languages to facilitate knowledge sharing. We provide a
		  large-scale entity linking system to establish the
		  associations between unstructured text and structured KG.
		  (3) We design a multi-strategy knowledge completion
		  framework, which leverages pre-trained language models and
		  vast amounts of unstructured text to discover missing and
		  new facts. The resulting KG contains 446 concepts, 2,608
		  properties, 66 million entities, and more than 2 billion
		  facts. It is available and downloadable online at ,
		  providing a valuable resource for researchers and
		  practitioners in various fields.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= aug,
  articleno	= {145},
  numpages	= {47},
  keywords	= {Knowledge graph, knowledge management, knowledge fusion,
		  knowledge completion, schema construction, entity typing,
		  entity alignment, entity linking}
}

@InProceedings{	  10.1145/3589334.3645648,
  author	= {Jackermeier, Mathias and Chen, Jiaoyan and Horrocks, Ian},
  title		= {Dual Box Embeddings for the Description Logic EL++},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645648},
  doi		= {10.1145/3589334.3645648},
  abstract	= {OWL ontologies, whose formal semantics are rooted in
		  Description Logic (DL), have been widely used for knowledge
		  representation. Similar to Knowledge Graphs (KGs),
		  ontologies are often incomplete, and maintaining and
		  constructing them has proved challenging. While classical
		  deductive reasoning algorithms use the precise formal
		  semantics of an ontology to predict missing facts, recent
		  years have witnessed growing interest in inductive
		  reasoning techniques that can derive probable facts from an
		  ontology. Similar to KGs, a promising approach is to learn
		  ontology embeddings in a latent vector space, while
		  additionally ensuring they adhere to the semantics of the
		  underlying DL. While a variety of approaches have been
		  proposed, current ontology embedding methods suffer from
		  several shortcomings, especially that they all fail to
		  faithfully model one-to-many, many-to-one, and many-to-many
		  relations and role inclusion axioms. To address this
		  problem and improve ontology completion performance, we
		  propose a novel ontology embedding method named Box2EL for
		  the DL EL++, which represents both concepts and roles as
		  boxes (i.e., axis-aligned hyperrectangles), and models
		  inter-concept relationships using a bumping mechanism. We
		  theoretically prove the soundness of Box2EL and conduct an
		  extensive experimental evaluation, achieving
		  state-of-the-art results across a variety of datasets on
		  the tasks of subsumption prediction, role assertion
		  prediction, and approximating deductive reasoning.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2250–2258},
  numpages	= {9},
  keywords	= {description logic, link prediction, ontology completion,
		  ontology embedding, web ontology language},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1145/3277591,
  author	= {Bounhas, Ibrahim},
  title		= {On the Usage of a Classical Arabic Corpus as a Language
		  Resource: Related Research and Key Challenges},
  year		= {2019},
  issue_date	= {September 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3277591},
  doi		= {10.1145/3277591},
  abstract	= {This article presents a literature review of
		  computer-science-related research applied on hadith, a kind
		  of Arabic narration which appeared in the 7th century. We
		  study and compare existent works in several fields of
		  Natural Language Processing (NLP), Information Retrieval
		  (IR), and Knowledge Extraction (KE). Thus, we illicit their
		  main drawbacks and identify some perspectives, which may be
		  considered by the research community. We also study the
		  characteristics of these types of documents, by enumerating
		  the advantages/limits of using hadith as a language
		  resource. Moreover, our study shows that previous studies
		  used different collections of hadiths, thus making it hard
		  to compare their results objectively. Besides, many
		  preprocessing steps are recurrent through these
		  applications, thus wasting a lot of time. Consequently, the
		  key issues for building generic language resources from
		  hadiths are discussed, taking into account the relevance of
		  related literature and the wide community of researchers
		  that are interested in these narrations. The ultimate goal
		  is to structure hadith books for multiple usages, thus
		  building common collections which may be exploited in
		  future applications.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  articleno	= {23},
  numpages	= {45},
  keywords	= {Hadith processing, hadith knowledge extraction, hadith
		  mining, hadith retrieval}
}

@InProceedings{	  10.1145/3459930.3469524,
  author	= {Mohan, Sunil and Angell, Rico and Monath, Nicholas and
		  McCallum, Andrew},
  title		= {Low resource recognition and linking of biomedical
		  concepts from a large ontology},
  year		= {2021},
  isbn		= {9781450384506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459930.3469524},
  doi		= {10.1145/3459930.3469524},
  abstract	= {Tools to explore scientific literature are essential for
		  scientists, especially in biomedicine, where about a
		  million new papers are published every year. Many such
		  tools provide users the ability to search for specific
		  entities (e.g. proteins, diseases) by tracking their
		  mentions in papers. PubMed, the most well known database of
		  biomedical papers, relies on human curators to add these
		  annotations. This can take several weeks for new papers,
		  and not all papers get tagged. Machine learning models have
		  been developed to facilitate the semantic indexing of
		  scientific papers. However their performance on the more
		  comprehensive ontologies of biomedical concepts does not
		  reach the levels of typical entity recognition problems
		  studied in NLP. In large part this is due to their low
		  resources, where the ontologies are large, there is a lack
		  of descriptive text defining most entities, and labeled
		  data can only cover a small portion of the ontology. In
		  this paper, we develop a new model that overcomes these
		  challenges by (1) generalizing to entities unseen at
		  training time, and (2) incorporating linking predictions
		  into the mention segmentation decisions. Our approach
		  achieves new state-of-the-art results for the UMLS ontology
		  in both traditional recognition/linking (+8 F1 pts) as well
		  as semantic indexing-based evaluation (+10 F1 pts).},
  booktitle	= {Proceedings of the 12th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {54},
  numpages	= {10},
  keywords	= {UMLS, biomedical concept recognition, deep learning, named
		  entity recognition and linking},
  location	= {Gainesville, Florida},
  series	= {BCB '21}
}

@Article{	  10.1145/3494560,
  author	= {Abulaish, Muhammad and Fazil, Mohd and Zaki, Mohammed J.},
  title		= {Domain-Specific Keyword Extraction Using Joint Modeling of
		  Local and Global Contextual Semantics},
  year		= {2022},
  issue_date	= {August 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3494560},
  doi		= {10.1145/3494560},
  abstract	= {Domain-specific keyword extraction is a vital task in the
		  field of text mining. There are various research tasks,
		  such as spam e-mail classification, abusive language
		  detection, sentiment analysis, and emotion mining, where a
		  set of domain-specific keywords (aka lexicon) is highly
		  effective. Existing works for keyword extraction list all
		  keywords rather than domain-specific keywords from a
		  document corpus. Moreover, most of the existing approaches
		  perform well on formal document corpuses but fail on noisy
		  and informal user-generated content in online social media.
		  In this article, we present a hybrid approach by jointly
		  modeling the local and global contextual semantics of
		  words, utilizing the strength of distributional word
		  representation and contrasting-domain corpus for
		  domain-specific keyword extraction. Starting with a seed
		  set of a few domain-specific keywords, we model the text
		  corpus as a weighted word-graph. In this graph, the initial
		  weight of a node (word) represents its semantic association
		  with the target domain calculated as a linear combination
		  of three semantic association metrics, and the weight of an
		  edge connecting a pair of nodes represents the
		  co-occurrence count of the respective words. Thereafter, a
		  modified PageRank method is applied to the word-graph to
		  identify the most relevant words for expanding the initial
		  set of domain-specific keywords. We evaluate our method
		  over both formal and informal text corpuses (comprising six
		  datasets), and show that it performs significantly better
		  in comparison to state-of-the-art methods. Furthermore, we
		  generalize our approach to handle the language-agnostic
		  case, and show that it outperforms existing
		  language-agnostic approaches.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= jan,
  articleno	= {70},
  numpages	= {30},
  keywords	= {Text mining, information extraction, domain-specific
		  keyword extraction, language-agnostic keyword extraction}
}

@Proceedings{	  10.1145/3631700,
  title		= {UMAP Adjunct '24: Adjunct Proceedings of the 32nd ACM
		  Conference on User Modeling, Adaptation and
		  Personalization},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Cagliari, Italy}
}

@InProceedings{	  10.1145/3664476.3670470,
  author	= {Teixeira De Castro, Hugo and Hussain, Ahmed and Blanc,
		  Gregory and El Hachem, Jamal and Blouin, Dominique and
		  Leneutre, Jean and Papadimitratos, Panos},
  title		= {A Model-based Approach for Assessing the Security of
		  Cyber-Physical Systems},
  year		= {2024},
  isbn		= {9798400717185},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664476.3670470},
  doi		= {10.1145/3664476.3670470},
  abstract	= {Cyber-Physical Systems (CPSs) complexity has been
		  continuously increasing to support new life-impacting
		  applications, such as Internet of Things (IoT) devices or
		  Industrial Control Systems (ICSs). These characteristics
		  introduce new critical security challenges to both
		  industrial practitioners and academics. This work
		  investigates how Model-Based System Engineering (MBSE) and
		  attack graph approaches could be leveraged to model secure
		  Cyber-Physical System solutions and identify high-impact
		  attacks early in the system development life cycle. To
		  achieve this, we propose a new framework that comprises (1)
		  an easily adoptable modeling paradigm for Cyber-Physical
		  System representation, (2) an attack-graph-based solution
		  for Cyber-Physical System automatic quantitative security
		  analysis, based on the MulVAL security tool, (3) a set of
		  Model-To-Text (MTT) transformation rules to bridge the gap
		  between SysML and MulVAL. We illustrated the validity of
		  our proposed framework through an autonomous ventilation
		  system example. A Denial of Service (DoS) attack targeting
		  an industrial communication protocol was identified and
		  displayed as attack graphs. In future work, we intend to
		  connect the approach to dynamic security databases for
		  automatic countermeasure selection.},
  booktitle	= {Proceedings of the 19th International Conference on
		  Availability, Reliability and Security},
  articleno	= {121},
  numpages	= {10},
  keywords	= {Critical Infrastructures, Risk Analysis, Security and
		  Privacy for Cyber-Physical Systems, Security by Design.,
		  Threats and Attack Modelling, Usable Security and Privacy},
  location	= {Vienna, Austria},
  series	= {ARES '24}
}

@InProceedings{	  10.1145/3316615.3316682,
  author	= {Rodzman, Shaiful Bakhtiar bin and Suhaili, Siti Suhaima
		  binti and Ismail, Normaly Kamal and Rahman, Nurazzah Abd
		  and Aljunid, Syed Ahmad and Omar, Aslida binti},
  title		= {Domain Specific Classification of Malay Based Complaints
		  using the Complaint Concept Ontologies},
  year		= {2019},
  isbn		= {9781450365734},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3316615.3316682},
  doi		= {10.1145/3316615.3316682},
  abstract	= {The complaint from users is an effective method to
		  identify the quality of services and facilities provided by
		  an organization. The efficiency to respond to users'
		  complaint also depends on an effective workflow. By having
		  an effective method and workflow, the action taken by the
		  management to improve the quality of services and
		  facilities can be done immediately and effectively. One of
		  the ways is by classifying the complaints that will isolate
		  related complaints. This paper presents the implementation
		  of the classification system that combines the application
		  of Complaint Concept Ontologies in Malay language as
		  classifier rules with the BM25 model of Information
		  Retrieval system. Experiments showed the semantic based
		  elements such as Malay ontology may bring the improvement
		  of the classification of the Malay Complaint. The result
		  yielded showed that the proposed classifier produced better
		  result in four category compared to BM25 original score
		  that only produced better result in one category. OBMCS
		  also outperformed the LDA model in all eight categories on
		  the Recall, Precision and F-measure metrics. The finding
		  proven the proposed system is very useful, especially to
		  the Malay complaint in regards of classification for
		  documents in the domain area.},
  booktitle	= {Proceedings of the 2019 8th International Conference on
		  Software and Computer Applications},
  pages		= {481–486},
  numpages	= {6},
  keywords	= {Malay complaint, bm25 model, ontology based
		  classification, semantic classification},
  location	= {Penang, Malaysia},
  series	= {ICSCA '19}
}

@InProceedings{	  10.1145/3520084.3520102,
  author	= {Tramontana, Emiliano and Verga, Gabriella},
  title		= {Ontology Enrichment with Text Extracted from Wikipedia},
  year		= {2022},
  isbn		= {9781450395519},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3520084.3520102},
  doi		= {10.1145/3520084.3520102},
  abstract	= {As biobanks require storing a large amount of data, the
		  use of ontologies offer an effective solution to properly
		  organise data and for data management. However, the
		  specialised jargon embedded into an ontology, especially in
		  the biomedical field, may constitute a difficulty for the
		  people outside the proper domain. Our solution to this is
		  to enhance ontology usability by automatically enriching an
		  ontology. In this article we illustrate an enrichment
		  process that allows us to expand in a controlled way the
		  terms within an ontology. Our proposed enrichment process
		  automatically adds in the ontological structure information
		  extracted from external sources, in order to offer a more
		  complete and clear knowledge of the domain and let users
		  more easily query the ontology. Our enrichment process
		  carefully selects from the wealth of information available
		  in Wikipedia.},
  booktitle	= {Proceedings of the 2022 5th International Conference on
		  Software Engineering and Information Management},
  pages		= {113–117},
  numpages	= {5},
  keywords	= {Additional Key Words and Phrases: Ontologies, Biobanks,
		  OBIB, Ontology enrichment, Wikipedia},
  location	= {Yokohama, Japan},
  series	= {ICSIM '22}
}

@Article{	  10.1145/3744558,
  author	= {Gang, Liu and Wenli, Yang and Tongli, Wang and Zhihao,
		  He},
  title		= {Corpus Fusion and Text Summarization Extraction for
		  Multi-Feature Enhanced Entity Alignment},
  year		= {2025},
  issue_date	= {September 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {9},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3744558},
  doi		= {10.1145/3744558},
  abstract	= {Cross-lingual entity alignment endeavors to identify
		  semantically similar entities within a knowledge graph,
		  facilitating knowledge complementarity and enriching
		  cross-lingual knowledge. In the context of knowledge-driven
		  tasks such as cross-lingual question answering and
		  knowledge recommendation, cross-lingual entity alignment
		  can effectively enhancing the performance of these
		  applications built upon cross-lingual knowledge graphs.
		  However, the current methodologies exhibit constraints in
		  efficiently extracting and combining features of multiple
		  entities, rendering them unable to fully harness the wealth
		  of extensive information provided by the knowledge graph.
		  To address this challenge, we propose CFSE, a novel
		  multi-feature enhanced fusion model, which includes deep
		  extraction of complex entity relationship, name, and
		  attribute features. Complex entity relationship features
		  are extracted based on corpus fusion and RotatE model.
		  Additionally, an algorithm based on BERT for multilingual
		  text summarization was introduced to extract entity name
		  and attribute features. Through comprehensive entity
		  feature extraction, CFSE not only further improves the
		  alignment accuracy, but also helps to maximize the depth
		  mining of knowledge graph information. The effectiveness of
		  CFSE in cross-lingual entity alignment applications was
		  demonstrated through experimental results on the DBP15K
		  dataset.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= sep,
  articleno	= {89},
  numpages	= {15},
  keywords	= {Cross-lingual entity alignment, rotate, BERT, feature
		  fusion, multilingual text summarization extraction}
}

@InBook{	  10.1145/3718491.3718581,
  author	= {Chen, Hongyv and He, Juan},
  title		= {Exploring the diverse interests of tourism participants:
		  an analysis based on Topic model and BiLSTM-CRF},
  year		= {2025},
  isbn		= {9798400710865},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3718491.3718581},
  abstract	= {The rapid development of social network and new technology
		  has led to great changes in tourism. Natural language
		  processing (NLP) has advantages in obtaining information
		  about the vast amount of content generated by online users
		  about travel services and products. In this paper, Gansu
		  province is taken as a sample to establish a corpus
		  knowledge base combining user generated content (UGC) and
		  literature specialty. And a variety of knowledge
		  aggregation methods, such as unsupervised machine learning
		  and knowledge graph methods, are used to effectively
		  organize the massive data in the corpus. Finally, this
		  paper analyzes destination image, tourist preference and
		  tourist behavior patterns. This method effectively
		  addresses the needs of tourism visitors and organization
		  information retrieval. Make some contributions to
		  destination management and tourist behavior recognition.},
  booktitle	= {Proceedings of the 4th Asia-Pacific Artificial
		  Intelligence and Big Data Forum},
  pages		= {550–555},
  numpages	= {6}
}

@Article{	  10.1145/3677376,
  author	= {Zou, Jie and Sun, Aixin and Long, Cheng and Kanoulas,
		  Evangelos},
  title		= {Knowledge-Enhanced Conversational Recommendation via
		  Transformer-Based Sequential Modeling},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3677376},
  doi		= {10.1145/3677376},
  abstract	= {In conversational recommender systems (CRSs),
		  conversations usually involve a set of items and
		  item-related entities or attributes, e.g., director is a
		  related entity of a movie. These items and item-related
		  entities are often mentioned along the development of a
		  dialog, leading to potential sequential dependencies among
		  them. However, most of existing CRSs neglect these
		  potential sequential dependencies. In this article, we
		  first propose a Transformer-based sequential conversational
		  recommendation method, named TSCR, to model the sequential
		  dependencies in the conversations to improve CRS. In TSCR,
		  we represent conversations by items and the item-related
		  entities, and construct user sequences to discover user
		  preferences by considering both the mentioned items and
		  item-related entities. Based on the constructed sequences,
		  we deploy a Cloze task to predict the recommended items
		  along a sequence. Meanwhile, in certain domains, knowledge
		  graphs formed by the items and their related entities are
		  readily available, which provide various different kinds of
		  associations among them. Given that TSCR does not benefit
		  from such knowledge graphs, we then propose a knowledge
		  graph enhanced version of TSCR, called TSCRKG. In specific,
		  we leverage the knowledge graph to offline initialize our
		  model TSCRKG, and augment the user sequence of
		  conversations (i.e., sequence of the mentioned items and
		  item-related entities in the conversation) with multi-hop
		  paths in the knowledge graph. Experimental results
		  demonstrate that our TSCR model significantly outperforms
		  state-of-the-art baselines, and the enhanced version TSCRKG
		  further improves recommendation performance on top of
		  TSCR.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= oct,
  articleno	= {162},
  numpages	= {27},
  keywords	= {Conversational recommendation, sequential recommendation,
		  recommender system, transformer}
}

@InProceedings{	  10.1145/3508546.3508644,
  author	= {Liu, Lijuan and Guo, Chengyu},
  title		= {Retrieval and Evaluation of Target Component Based on
		  Ontology Knowledge},
  year		= {2022},
  isbn		= {9781450385053},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3508546.3508644},
  doi		= {10.1145/3508546.3508644},
  abstract	= {Software reuse most focuses on component based software
		  development (CBSD). However, it's not so accurate and
		  efficient in the process, to solve this problem, this paper
		  proposes an intelligent knowledge-driven method of target
		  component retrieval and evaluation. This method is based on
		  Ontology component description. With the help of the
		  knowledge graph, a semantic mapping is formed between the
		  component to be queried and the component description
		  library, so the entity component is located. In order to
		  measure the component matching performance, it gives
		  multi-angle evaluation of queried candidate components by
		  an evaluation index system. Based on component query
		  information, it makes the searching target clearer, extends
		  the semantic scope of components to be queried. In order to
		  assembly components, a multi agent system (MAS) is also
		  established. The result shows that this method not only
		  makes the component retrieval process higher recall and
		  precision, but also makes the component retrieval process
		  more intelligent by meeting the assembly requirement.},
  booktitle	= {Proceedings of the 2021 4th International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  articleno	= {98},
  numpages	= {6},
  keywords	= {MAS, Ontology, artificial intelligence, knowledge graph,
		  software reuse},
  location	= {Sanya, China},
  series	= {ACAI '21}
}

@InProceedings{	  10.1145/3605763.3625244,
  author	= {Ellerhold, Christian and Schnagl, Johann and Schreck,
		  Thomas},
  title		= {Enterprise Cyber Threat Modeling and Simulation of Loss
		  Events for Cyber Risk Quantification},
  year		= {2023},
  isbn		= {9798400702594},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605763.3625244},
  doi		= {10.1145/3605763.3625244},
  abstract	= {In today's enterprise landscape, effective risk management
		  has emerged as a vital cornerstone. This importance has
		  escalated significantly due to the widespread transition
		  from traditional on-premise infrastructures to dynamic
		  cloud environments. Many organizations rely on qualitative
		  approaches for internal IT and cyber risk management;
		  however, these approaches have notable drawbacks, such as a
		  lack of accuracy and comparability. In this paper, we
		  propose a novel approach to address these limitations by
		  using the Factor Analysis of Information Risk (FAIR)
		  methodology in conjunction with MITRE ATT&amp;CK to model
		  realistic cyberattacks on organizations and measure
		  quantitative risk. We describe how this approach can be
		  used to create an enterprise cyber threat model, providing
		  a case study for a cloud scenario to demonstrate its usage
		  and to illustrate its potential benefits. Our model has
		  demonstrated its practical applicability in enterprise
		  settings as we thoroughly evaluated its effectiveness
		  within two prominent German companies. This allowed us to
		  gain valuable insight into how our proposed approach can
		  enhance an organization's risk management strategies. Our
		  research demonstrates the value of using a quantitative
		  approach like FAIR over qualitative risk assessment
		  methods. Overall, our approach provides a more
		  comprehensive understanding of the risks organizations are
		  facing and offers guidance on implementing effective risk
		  management strategies. This research can help organizations
		  improve their risk management practices and reduce the
		  potential negative impact of cyberattacks.},
  booktitle	= {Proceedings of the 2023 on Cloud Computing Security
		  Workshop},
  pages		= {17–29},
  numpages	= {13},
  keywords	= {cloud computing, cyber risk quantification, enterprise
		  threat model, factor analysis of information risk (fair),
		  mitre att&amp;ck, quantitative risk assessment, unified
		  kill chain},
  location	= {Copenhagen, Denmark},
  series	= {CCSW '23}
}

@InProceedings{	  10.1145/3493700.3493703,
  author	= {Uceda-Sosa, Rosario and Mihindukulasooriya, Nandana and
		  Kumar, Atul and Bansal, Sahil and Nagar, Seema},
  title		= {Domain specific ontologies from Linked Open Data (LOD)},
  year		= {2022},
  isbn		= {9781450385824},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3493700.3493703},
  doi		= {10.1145/3493700.3493703},
  abstract	= {Logical and probabilistic reasoning tasks that require a
		  deeper knowledge of semantics are increasingly relying on
		  general purpose ontologies such as Wikidata and DBpedia.
		  However, tasks such as entity disambiguation and linking
		  may benefit from domain-specific knowledge graphs, which
		  make it more efficient to consume the knowledge and easier
		  to extend with proprietary content. We discuss our
		  experience bootstrapping one such ontology for IT with a
		  domain-agnostic pipeline, and extending it using
		  domain-specific glossaries.},
  booktitle	= {Proceedings of the 5th Joint International Conference on
		  Data Science \&amp; Management of Data (9th ACM IKDD CODS
		  and 27th COMAD)},
  pages		= {105–109},
  numpages	= {5},
  keywords	= {IT Operations, Knowledge Graphs, Ontologies},
  location	= {Bangalore, India},
  series	= {CODS-COMAD '22}
}

@InProceedings{	  10.1145/3466933.3466951,
  author	= {Aguiar, Camila Zacch\'{e} de and Zanetti, F\'{e}lix and
		  Souza, Vitor E. Silva},
  title		= {Source Code Interoperability based on Ontology},
  year		= {2021},
  isbn		= {9781450384919},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3466933.3466951},
  doi		= {10.1145/3466933.3466951},
  abstract	= {The different ways of representing a source code in
		  different programming languages create a heterogeneous
		  context. In addition, the use of multiple programming
		  languages in a single source code (polyglot programming)
		  brings a wide choice of terms from different languages,
		  libraries and structures. These facts prevent the direct
		  exchange of information between source codes of different
		  programming languages, requiring specialized knowledge of
		  the programming languages involved. In this article, we
		  present an ontology-based method for source code
		  interoperability that provides an alternative to mitigate
		  heterogeneity problems, aiming to semantically represent
		  the source code written in different programming languages
		  and apply it from different perspectives in a unified way.
		  In this sense, the method is applied in a lab experiment
		  with the objective of validating its methodological
		  aspects, instantiating their respective phases in different
		  subdomains (object orientation and object/relational
		  mapping) and programming languages (Java and Python) in the
		  code smells detection perspective. In addition, the code
		  smell detector produced is evaluated with a set of
		  real-world software projects written in Java and Python.},
  booktitle	= {Proceedings of the XVII Brazilian Symposium on Information
		  Systems},
  articleno	= {18},
  numpages	= {8},
  keywords	= {applied ontology, code smell detection, interoperabity,
		  ontology, source code},
  location	= {Uberl\^{a}ndia, Brazil},
  series	= {SBSI '21}
}

@InProceedings{	  10.1145/3460866.3461772,
  author	= {C\'{e}rin, Christophe and Andres, Fr\'{e}d\'{e}ric and
		  Geldwerth-Feniger, Danielle},
  title		= {Towards an emulation tool based on ontologies and data
		  life cycles for studying smart buildings},
  year		= {2021},
  isbn		= {9781450384650},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460866.3461772},
  doi		= {10.1145/3460866.3461772},
  abstract	= {In this paper, we share our vision to study a complex
		  Information Technology (IT) system handling a massive
		  amount of data in the context of 'smart buildings.' One
		  technique for analyzing complex IT systems relies on
		  emulation, where the final software system is fully
		  deployed on real architectures, and is evaluated in
		  considering "small" instances of situations the system is
		  supposed to solve. We propose a software architecture for
		  studying the ecosystem of 'smart buildings'. This software
		  architecture is built: 1) on top of ontologies for the
		  description of smart buildings; 2) on a special tool for
		  mastering the life cycle of data produced by sensors and
		  actuators inside the buildings.We assume that it is equally
		  important to model both the building's components and the
		  flow of data produced inside the building. We use existing
		  software components for both goals and to make real our
		  concerns. According to a translational methodology, we also
		  discuss use cases for illustrating the potential of our
		  approach and the particular challenges associated with
		  making the two main components of our emulation tool
		  inter-operate.Therefore, our main contribution is to
		  propose a comprehensive, ambitious and realistic research
		  plan to guide communities. The paper illustrates how
		  computer scientists and smart buildings domain scientists
		  may communicate to address and solve specific research
		  problems related to Big Data in emergent distributed
		  environments. We are also guessing that experimental
		  results that can demonstrate the practicality of the
		  proposed combination of tools could be devised in the
		  future, based on our broad vision. The paper is, first and
		  foremost, a visionary paper.},
  booktitle	= {Proceedings of the International Workshop on Big Data in
		  Emergent Distributed Environments},
  articleno	= {8},
  numpages	= {15},
  keywords	= {big data tools, data life cycle, emulation principles,
		  ontology, smart buildings, systems and methods},
  location	= {Virtual Event, China},
  series	= {BiDEDE '21}
}

@InProceedings{	  10.1145/3425329.3425377,
  author	= {Fei, Huang and Youling, Chen and Dongsheng, Xu},
  title		= {Formal Description of Manufacturing Process based on
		  Domain Ontology Construction},
  year		= {2020},
  isbn		= {9781450387873},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3425329.3425377},
  doi		= {10.1145/3425329.3425377},
  abstract	= {In order to solve the problem of process knowledge
		  sharing, integration and reuse in the field of machinery
		  manufacturing due to the complexity, dispersion and
		  diversity of process knowledge. Taking into account the
		  advantages of ontology in knowledge representation, this
		  paper proposes an ontology-based knowledge management
		  framework in the production line. On the basis of
		  inductively analyzing the attributes of the mechanical
		  manufacturing process attributes and intra-process and
		  interprocess relationships, an improved conceptual ontology
		  expression model of the 4-tuple process is proposed.},
  booktitle	= {Proceedings of the 2nd World Symposium on Software
		  Engineering},
  pages		= {246–251},
  numpages	= {6},
  keywords	= {Process knowledge, domain ontology, ontology modeling,
		  semantic analysis},
  location	= {Chengdu, China},
  series	= {WSSE '20}
}

@InProceedings{	  10.1145/3652620.3688213,
  author	= {Atkinson, Colin and K\"{u}hne, Thomas and Lange, Arne},
  title		= {Misconceptions about Potency-Based Deep Instantiation},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688213},
  doi		= {10.1145/3652620.3688213},
  abstract	= {Multi-level modeling languages differ in their approaches
		  for controlling the properties of model elements over
		  multiple modeling levels. Over the years the original
		  approach for deeply characterizing model elements, the
		  potency-based deep instantiation mechanism, has received a
		  number of criticisms related to its flexibility, level
		  stability, ontological soundness, type safety, and ability
		  to reduce accidental complexity. However, some of these
		  criticisms are founded on misconceptions and thus cannot
		  usefully inform multi-level modeling language designs. In
		  this paper we identify and clarify these misconceptions in
		  order to help guide future considerations of language
		  feature trade-offs and design choices.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {810–817},
  numpages	= {8},
  keywords	= {multi-level modeling, deep instantiation, ontologies,
		  accidental complexity, type safety, potency},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Proceedings{	  10.1145/3584684,
  title		= {ApPLIED 2023: Proceedings of the 5th workshop on Advanced
		  tools, programming languages, and PLatforms for
		  Implementing and Evaluating algorithms for Distributed
		  systems},
  year		= {2023},
  isbn		= {9798400701283},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Orlando, FL, USA}
}

@Article{	  10.1145/3756009,
  author	= {Cheng, Qing and Zeng, Zefan and Hu, Xingchen and Si,
		  Yuehang and Liu, Zhong},
  title		= {A Survey of Event Causality Identification: Taxonomy,
		  Challenges, Assessment, and Prospects},
  year		= {2025},
  issue_date	= {February 2026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {58},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3756009},
  doi		= {10.1145/3756009},
  abstract	= {Event Causality Identification (ECI) has become an
		  essential task in Natural Language Processing (NLP),
		  focused on automatically detecting causal relationships
		  between events within texts. This comprehensive survey
		  systematically investigates fundamental concepts and
		  models, developing a systematic taxonomy and critically
		  evaluating diverse models. We begin by defining core
		  concepts, formalizing the ECI problem, and outlining
		  standard evaluation protocols. Our classification framework
		  divides ECI models into two primary tasks: Sentence-level
		  Event Causality Identification (SECI) and Document-level
		  Event Causality Identification (DECI). For SECI, we review
		  models employing feature pattern-based matching, machine
		  learning classifiers, deep semantic encoding, prompt-based
		  fine-tuning, and causal knowledge pre-training, alongside
		  data augmentation strategies. For DECI, we focus on
		  approaches utilizing deep semantic encoding, event graph
		  reasoning, and prompt-based fine-tuning. Special attention
		  is given to recent advancements in multi-lingual and
		  cross-lingual ECI, as well as zero-shot ECI leveraging
		  Large Language Models (LLMs). We analyze the strengths,
		  limitations, and unresolved challenges associated with each
		  approach. Extensive quantitative evaluations are conducted
		  on four benchmark datasets to rigorously assess the
		  performance of various ECI models. We conclude by
		  discussing future research directions and highlighting
		  opportunities to advance the field further.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {59},
  numpages	= {37},
  keywords	= {Natural language processing, event, causality, information
		  extraction, representation learning, knowledge reasoning}
}

@InProceedings{	  10.1145/3630138.3630488,
  author	= {Yin, Xuefeng and Luo, Jianfei},
  title		= {The Statistic Study of Thai Middle School Students'
		  Acquisition in Chinese Negative Structures},
  year		= {2024},
  isbn		= {9781450399951},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3630138.3630488},
  doi		= {10.1145/3630138.3630488},
  abstract	= {Among the large amounts of Chinese negatives, bu and mei
		  are the most frequently-used with complicated syntactic
		  function and differences on the aspects of semantics and
		  pragmatics becoming the key and hard one in Chinese
		  teaching. However, in Thailand language, the negative
		  ไม่ is a quite simple syntactic structure
		  representing almost all negative usage. This paper based on
		  the former theoretical study takes two grades 72 students
		  as the object of the study, making relative 20 questions in
		  10 syntactic conditions, conducting the operation of the
		  average on the subjective effects and in 10 syntactic
		  categories. Relative cross-contrast data analysis on the
		  average accuracy between every two types of syntactic
		  conditions included as well. Statistics show that students
		  in Thai mother tongue always make biased errors about bu
		  and mei for lacking enough correspondence in their mother
		  tongue. According to the pairwise comparison inside of the
		  category of simple and complicated syntactic condition, we
		  observe that there contains a difficult level of
		  acquisition about bu and mei from easy to difficult which
		  is fixed collocation &lt; simple syntactic condition &lt;
		  volitive and mental verbs &lt; complicated syntactic
		  condition.},
  booktitle	= {Proceedings of the 2023 International Conference on Power,
		  Communication, Computing and Networking Technologies},
  articleno	= {70},
  numpages	= {8},
  location	= {Wuhan, China},
  series	= {PCCNT '23}
}

@InProceedings{	  10.1145/3436756.3437037,
  author	= {Basse, Adrien and Diatta, Baboucar and Deme, Cherif Bachir
		  and Ndiaye, Ndeye Massata},
  title		= {Ontology-Based Framework For Automatic Generation Of SQL
		  Assessment},
  year		= {2021},
  isbn		= {9781450388276},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3436756.3437037},
  doi		= {10.1145/3436756.3437037},
  abstract	= {Assessment measures learner progresses in acquisition of
		  knowledge and skills. It helps teachers to refine their
		  teaching strategies and better deliver knowledge and
		  skills. Likewise, It makes it possible not only to simulate
		  learners’ thinking but to measure the value or quality of
		  their work. Creating effective assessment of learning is a
		  difficult task for teachers. This article presents an
		  automatic question generating system for individual
		  assessment of learners’ practical knowledge.},
  booktitle	= {Proceedings of the 12th International Conference on
		  Education Technology and Computers},
  pages		= {152–156},
  numpages	= {5},
  keywords	= {SQL language, assessment, ontology, question generation},
  location	= {London, United Kingdom},
  series	= {ICETC '20}
}

@InProceedings{	  10.1145/3659677.3659992,
  author	= {Saada, Hajer and Orizio, Riccardo and Sebastio, Stefano},
  title		= {Modeling and Conducting Security Risk Assessment of Smart
		  Airport Infrastructures with SecRAM},
  year		= {2024},
  isbn		= {9798400709296},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3659677.3659992},
  doi		= {10.1145/3659677.3659992},
  abstract	= {Despite the COVID-19 pandemic caused flights and routes
		  cut back inverting an otherwise astonishing growth trend,
		  air passenger traffic is increasing every year. As a
		  result, airports are going through a continuous digital
		  transformation enhancing their infrastructure to keep up
		  their growth and to offer passengers an improved and
		  seamless experience. On the other hand, a far-reaching
		  digital infrastructure poses new cyber-security challenges.
		  In response to such a novel and connected ecosystem adopted
		  by airports, this paper presents a security risk assessment
		  to identify and mitigate the consequences of attacks
		  threatening the digital passenger process of smart
		  airports. Also, we propose an extension of SecRAM, a risk
		  assessment methodology proposed by the SESAR Joint
		  Undertaking for the Air Traffic Management (ATM), to enable
		  a continuous assessment as the threat landscape evolves.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Networking, Intelligent Systems and Security},
  articleno	= {59},
  numpages	= {7},
  keywords	= {Model-Based System Engineering, SecRAM, Security Risk
		  Assessment, Smart Airports},
  location	= {Meknes, AA, Morocco},
  series	= {NISS '24}
}

@Article{	  10.1145/3487919,
  author	= {Pfannem\"{u}ller, Martin and Breitbach, Martin and
		  Weckesser, Markus and Becker, Christian and Schmerl,
		  Bradley and Sch\"{u}rr, Andy and Krupitzer, Christian},
  title		= {REACT-ION: A Model-based Runtime Environment for
		  Situation-aware Adaptations},
  year		= {2021},
  issue_date	= {December 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {4},
  issn		= {1556-4665},
  url		= {https://doi.org/10.1145/3487919},
  doi		= {10.1145/3487919},
  abstract	= {Trends such as the Internet of Things lead to a growing
		  number of networked devices and to a variety of
		  communication systems. Adding self-adaptive capabilities to
		  these communication systems is one approach to reducing
		  administrative effort and coping with changing execution
		  contexts. Existing frameworks can help reducing development
		  effort but are neither tailored toward the use in
		  communication systems nor easily usable without knowledge
		  in self-adaptive systems development. Accordingly, in
		  previous work, we proposed REACT, a reusable, model-based
		  runtime environment to complement communication systems
		  with adaptive behavior. REACT addresses heterogeneity and
		  distribution aspects of such systems and reduces
		  development effort. In this article, we propose
		  REACT-ION—an extension of REACT for situation awareness.
		  REACT-ION offers a context management module that is able
		  to acquire, store, disseminate, and reason on context data.
		  The context management module is the basis for (i)
		  proactive adaptation with REACT-ION and (ii)
		  self-improvement of the underlying feedback loop. REACT-ION
		  can be used to optimize adaptation decisions at runtime
		  based on the current situation. Therefore, it can cope with
		  uncertainty and situations that were not foreseeable at
		  design time. We show and evaluate in two case studies how
		  REACT-ION’s situation awareness enables proactive
		  adaptation and self-improvement.},
  journal	= {ACM Trans. Auton. Adapt. Syst.},
  month		= dec,
  articleno	= {12},
  numpages	= {29},
  keywords	= {Self-adaptive systems, model-based, runtime environment,
		  framework, situation awareness}
}

@InProceedings{	  10.1145/3463677.3463730,
  author	= {Avgerinos Loutsaris, Michalis and Lachana, Zoi and
		  Alexopoulos, Charalampos and Charalabidis, Yannis},
  title		= {Legal Text Processing: Combing two legal ontological
		  approaches through text mining},
  year		= {2021},
  isbn		= {9781450384926},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3463677.3463730},
  doi		= {10.1145/3463677.3463730},
  abstract	= {The globalization of communication networks and the
		  possibilities offered by the information and communication
		  technologies (ICTs) significantly change the public
		  sector's operation and services. Digital Governance is now
		  integrated into administrations' policies and programs at
		  all levels: local, regional, national, European. At the
		  national level, there is a requirement to provide
		  electronic public services according to citizens' needs
		  while, in the sense of globalization, at the European
		  level, there are many programs (e.g., the Europe 2005 and
		  i2010 program) emphasizing the Digital Governance world (or
		  better Digital Governance community) that indicates rapid
		  changes not only in the sense of the change in the public
		  sector's systems but also in the mentality that the public
		  sector operates. On the other hand, Digital Governance's
		  evolution affects societies intensively, emphasizing the
		  importance of cross-border interaction and information
		  sharing between them. [6]. Concerning the legal informatics
		  domain, this can result in changing governments' operations
		  in many ways [2]. By now, the massive amount of each
		  country's legal information currently remains fragmented
		  across multiple national databases and systems or even
		  better legal databases. Most of these legal databases
		  result from the significant advancements in the “legal
		  informatics” research field that observed since
		  governments have started to promote the development of
		  legal information systems [9]. This research contributes to
		  this purpose by developing an open and automated legal
		  system capable of providing any EU country's legal
		  information based on the existing ontologies.},
  booktitle	= {Proceedings of the 22nd Annual International Conference on
		  Digital Government Research},
  pages		= {522–532},
  numpages	= {11},
  keywords	= {big open legal data, legal information systems, legal
		  ontologies},
  location	= {Omaha, NE, USA},
  series	= {dg.o '21}
}

@InProceedings{	  10.1145/3718751.3718912,
  author	= {Fang, Yuxuan and Hu, Xiaochen and Chen, Shangyin},
  title		= {Research on the influencing factors of creation model of
		  knowledge graphs for design under the perspective of graph
		  workflow},
  year		= {2025},
  isbn		= {9798400709753},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3718751.3718912},
  doi		= {10.1145/3718751.3718912},
  abstract	= {Based on the analysis of SAPAD-AHP model, this study
		  explores the influencing factors of graph structure in the
		  construction of knowledge graph creation platform, and
		  through the construction of the collaborative platform, the
		  study is carried out in teaching and research practice.
		  With the development of the interdisciplinary system and
		  the establishment of many emerging disciplines, design as
		  the intersection of science, engineering and liberal arts
		  combined, its information grooming and knowledge production
		  mode has also undergone an important transformation, and
		  the processing capacity in the face of a variety of
		  information intersection has greatly stimulated the demand
		  for the construction of personal knowledge bases and
		  knowledge graphs. The current solutions relying on tree
		  structure and linear structure have considerable
		  limitations. In this paper, we analyze the mapping of
		  "Behavior-Product-Meaning" of the existing solutions
		  through SAPAD method, and obtain the user's needs through
		  cluster analysis, and filter the multi-layer core meaning
		  clusters by calculating the weight of the general meaning
		  clusters through the AHP method, so as to obtain the user's
		  core needs. Based on the results of the requirements study,
		  the study proposes a design scheme for personal knowledge
		  graph construction based on graph structure, which for the
		  first time connects graph structure and text content in
		  tandem at the interaction aspect, integrates and simplifies
		  the workflow, and examines its information combing and
		  image editing capabilities in teaching and research
		  practice, providing a valuable reference for the design of
		  knowledge graph solutions for the design category.},
  booktitle	= {Proceedings of the 2024 4th International Conference on
		  Big Data, Artificial Intelligence and Risk Management},
  pages		= {971–984},
  numpages	= {14},
  keywords	= {collaborative platform, graph workflow, knowledge graph},
  location	= { },
  series	= {ICBAR '24}
}

@InProceedings{	  10.1145/3711542.3711550,
  author	= {Druselmann, Maria and Harbusch, Karin},
  title		= {A Dataset of Semantically Related Multiword Terms of the
		  Electrical Engineering Domain},
  year		= {2025},
  isbn		= {9798400717383},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711542.3711550},
  doi		= {10.1145/3711542.3711550},
  abstract	= {This paper presents the EEMWT dataset, a collection of
		  1873 triplets of co-hyponymic multiword terms of the
		  electrical engineering domain. Each triplet combines an
		  anchor term with closely and distantly related co-hyponymic
		  terms. The degree of semantic relatedness is determined by
		  the presence or absence of shared domain-specific semantic
		  features. The primary purpose of this dataset is to serve
		  as a tool for evaluating domain-specific language
		  representation models and embedding vector pooling
		  techniques by assessing the semantic relatedness of
		  co-hyponymic multiword terms. The novelty of the dataset
		  lies in the development approach, which replaces
		  intuition-based scaling of relatedness with linguistically
		  justified semantic-feature-based judgment. Furthermore, the
		  traditional method of calculating interrater agreement rate
		  is replaced with a statistical analysis of vector space
		  distance between low and highly related terms. These
		  innovations in dataset construction and evaluation have the
		  potential to significantly reduce the costs associated with
		  expert questionnaires in dataset development.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {258–264},
  numpages	= {7},
  keywords	= {dataset of semantically related terms, domain-specific
		  language representation model, multiword term, semantic
		  relatedness},
  location	= { },
  series	= {NLPIR '24}
}

@InProceedings{	  10.5555/3398761.3398925,
  author	= {van den Berg, Line and Atencia, Manuel and Euzenat,
		  J\`{e}rome},
  title		= {Agent Ontology Alignment Repair through Dynamic Epistemic
		  Logic},
  year		= {2020},
  isbn		= {9781450375184},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Ontology alignments enable agents to communicate while
		  preserving heterogeneity in their information. Alignments
		  may not be provided as input and should be able to evolve
		  when communication fails or when new information
		  contradicting the alignment is acquired. In the Alignment
		  Repair Game (ARG) this evolution is achieved via adaptation
		  operators. ARG was evaluated experimentally and the
		  experiments showed that agents converge towards successful
		  communication and improve their alignments. However,
		  whether the adaptation operators are formally correct,
		  complete or redundant is still an open question. In this
		  paper, we introduce a formal framework based on Dynamic
		  Epistemic Logic that allows us to answer this question.
		  This framework allows us (1) to express the ontologies and
		  alignments used, (2) to model the ARG adaptation operators
		  through announcements and conservative upgrades and (3) to
		  formally establish the correctness, partial redundancy and
		  incompleteness of the adaptation operators in ARG.},
  booktitle	= {Proceedings of the 19th International Conference on
		  Autonomous Agents and MultiAgent Systems},
  pages		= {1422–1430},
  numpages	= {9},
  keywords	= {agent communication, alignment repair, dynamic epistemic
		  logic, ontology alignment},
  location	= {Auckland, New Zealand},
  series	= {AAMAS '20}
}

@Article{	  10.1145/3665252.3665262,
  author	= {Doan, AnHai},
  title		= {Technical Perspective: Unicorn: A Unified Multi-Tasking
		  Matching Model},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {1},
  issn		= {0163-5808},
  url		= {https://doi.org/10.1145/3665252.3665262},
  doi		= {10.1145/3665252.3665262},
  abstract	= {Data integration has been a long-standing challenge for
		  data management. It has recently received significant
		  attention due to at least three main reasons. First, many
		  data science projects require integrating data from
		  disparate sources before analysis can be carried out to
		  extract insights. Second, many organizations want to build
		  knowledge graphs, such as Customer 360s, Product 360s, and
		  Supplier 360s, which capture all available information
		  about the customers, products, and suppliers of an
		  organization. Building such knowledge graphs often requires
		  integrating data from multiple sources. Finally, there is
		  also an increasing need to integrate a massive amount of
		  data to create training data for AI models, such as large
		  language models.},
  journal	= {SIGMOD Rec.},
  month		= may,
  pages		= {43},
  numpages	= {1}
}

@InProceedings{	  10.1145/3184558.3191654,
  author	= {Dimanidis, Anastasios and Chatzidimitriou, Kyriakos C. and
		  Symeonidis, Andreas L.},
  title		= {A Natural Language Driven Approach for Automated Web API
		  Development: Gherkin2OAS},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3191654},
  doi		= {10.1145/3184558.3191654},
  abstract	= {Speeding up the development process of Web Services, while
		  adhering to high quality software standards is a typical
		  requirement in the software industry. This is why industry
		  specialists usually suggest "driven by" development
		  approaches to tackle this problem. In this paper, we
		  propose such a methodology that employs Specification
		  Driven Development and Behavior Driven Development in order
		  to facilitate the phases of Web Service requirements
		  elicitation and specification. Furthermore, we introduce
		  gherkin2OAS, a software tool that aspires to bridge the
		  aforementioned development approaches. Through the
		  suggested methodology and tool, one may design and build
		  RESTful services fast, while ensuring proper
		  functionality.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {1869–1874},
  numpages	= {6},
  keywords	= {behavior driven development, gherkin, open API
		  specification, restful API},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3613372.3613378,
  author	= {Dalcin, Guilherme and Bolzan, Willian and Lazzari, Luan
		  and Farias, Kleinner},
  title		= {Recommendation of UML Model Conflicts: Unveiling the
		  Biometric Lens for Conflict Resolution},
  year		= {2023},
  isbn		= {9798400707872},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613372.3613378},
  doi		= {10.1145/3613372.3613378},
  abstract	= {Model merging assumes a pivotal role in numerous
		  model-centric software development tasks, e.g., evolving
		  UML models to add new features or even reconciling UML
		  models developed collaboratively by distributed development
		  teams. Usually, UML model elements to-be-merged conflict
		  with each other. Unfortunately, resolving conflicts remains
		  a highly cognitive and error-prone task. Today, wearable
		  devices capable of capturing biometric data are a reality.
		  Recent studies indicate that the developer’s cognitive
		  indicators may affect developers while performing
		  development tasks. However, the current literature has
		  neglected the recommendation of conflicts sensitive to the
		  cognitive activities of software developers. This study,
		  therefore, introduces BACR, a biometric-aware approach to
		  recommend UML model conflicts using machine learning. BACR
		  helps UML model merging to push a step forward,
		  recommending model conflicts based on appropriate biometric
		  indicators and using a behavior sequence transformer model.
		  Our approach is based on four scientific institutions. It
		  represents the first effort in supporting the
		  prioritization of cognitively relevant UML model conflicts
		  by developers, mitigating the risk of making incorrect
		  decisions and preventing potential downstream issues.},
  booktitle	= {Proceedings of the XXXVII Brazilian Symposium on Software
		  Engineering},
  pages		= {83–88},
  numpages	= {6},
  keywords	= {Biometrics, Cognitive Load, Model Merging, Software
		  Modeling},
  location	= {Campo Grande, Brazil},
  series	= {SBES '23}
}

@InProceedings{	  10.1145/3442442.3451388,
  author	= {Tian, Ke and Chen, Hua},
  title		= {aiai at the FinSim-2 task: Finance Domain Terms Automatic
		  Classification Via Word Ontology and Embedding},
  year		= {2021},
  isbn		= {9781450383134},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442442.3451388},
  doi		= {10.1145/3442442.3451388},
  abstract	= {This paper describes the method that we submitted to the
		  FinSim-2 task on learning similarities for the financial
		  domain. This task aims to automatically classify the
		  Financial domain terms into the most relevant hypernym (or
		  top-level) concept in an external ontology. This paper
		  shows the result of experiments using the Catboost,
		  Attention-LSTM, BERT, RoBERTa to develop an automatic
		  finance domain classifier via word ontology and embedding.
		  The experiment result demonstrates that each model could be
		  an effective method to tackle the FinSim-2 task,
		  respectively.},
  booktitle	= {Companion Proceedings of the Web Conference 2021},
  pages		= {320–322},
  numpages	= {3},
  keywords	= {Attention, BERT, Catboost, FinSim-2 task, LSTM, Ontology,
		  RoBERTa, Word2vec},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Article{	  10.14778/3583140.3583148,
  author	= {F\"{u}rst, Jonathan and Argerich, Mauricio Fadel and
		  Cheng, Bin},
  title		= {VersaMatch: Ontology Matching with Weak Supervision},
  year		= {2023},
  issue_date	= {February 2023},
  publisher	= {VLDB Endowment},
  volume	= {16},
  number	= {6},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3583140.3583148},
  doi		= {10.14778/3583140.3583148},
  abstract	= {Ontology matching is crucial to data integration for
		  across-silo data sharing and has been mainly addressed with
		  heuristic and machine learning (ML) methods. While
		  heuristic methods are often inflexible and hard to extend
		  to new domains, ML methods rely on substantial and hard to
		  obtain amounts of labeled training data. To overcome these
		  limitations, we propose VersaMatch, a flexible,
		  weakly-supervised ontology matching system. VersaMatch
		  employs various weak supervision sources, such as heuristic
		  rules, pattern matching, and external knowledge bases, to
		  produce labels from a large amount of unlabeled data for
		  training a discriminative ML model. For prediction,
		  VersaMatch develops a novel ensemble model combining the
		  weak supervision sources with the discriminative model to
		  support generalization while retaining a high precision.
		  Our ensemble method boosts end model performance by 4
		  points compared to a traditional weak-supervision baseline.
		  In addition, compared to state-of-the-art ontology
		  matchers, VersaMatch achieves an overall 4-point
		  performance improvement in F1 score across 26 ontology
		  combinations from different domains. For recently released,
		  in-the-wild datasets, VersaMatch beats the next best
		  matchers by 9 points in F1. Furthermore, its core
		  weak-supervision logic can easily be improved by adding
		  more knowledge sources and collecting more unlabeled data
		  for training.},
  journal	= {Proc. VLDB Endow.},
  month		= feb,
  pages		= {1305–1318},
  numpages	= {14}
}

@InProceedings{	  10.1145/3643834.3661498,
  author	= {Sivertsen, Christian and L\o{}vlie, Anders Sundnes},
  title		= {Exploring Aesthetic Qualities of Deep Generative Models
		  through Technological (Art) Mediation},
  year		= {2024},
  isbn		= {9798400705830},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643834.3661498},
  doi		= {10.1145/3643834.3661498},
  abstract	= {Deep Generative Models (DGM) have had a great impact both
		  on visual art and broader visual culture. In this
		  research-through-design project we investigate the use of a
		  DGM for helping museum visitors explore the aesthetics of
		  Edvard Munch’s art. We designed and built an interactive
		  drawing table that allows a user to explore a StyleGAN
		  model trained on sketches by Edvard Munch. The paper makes
		  two novel contributions: 1. It presents a system that
		  allows users to interact with a DGM by drawing on paper
		  (rather than the typical text prompts used by most current
		  systems). 2. We demonstrate how this mode and quality of
		  interaction establish a unique perspective on Munch’s
		  drawings as a practice. Through qualitative evaluation, we
		  discuss how this setup led users towards a specific
		  hermeneutic drawing strategy that enables building
		  competency with the model and by proxy the data it is
		  trained on. We suggest that the resulting interaction may
		  contribute to an "education of attention" helping museum
		  visitors to become attentive to certain visual qualities in
		  Munch’s drawing practice. Finally, we discuss how the
		  concepts of technological mediation and relationality are
		  useful for designing how the output of a DGM is understood
		  by its users.},
  booktitle	= {Proceedings of the 2024 ACM Designing Interactive Systems
		  Conference},
  pages		= {2738–2752},
  numpages	= {15},
  keywords	= {aesthetics, deep generative model, drawing, fine art,
		  interaction design, machine learning, postphenomenology,
		  stylegan},
  location	= {Copenhagen, Denmark},
  series	= {DIS '24}
}

@InProceedings{	  10.1145/3600211.3604702,
  author	= {Rismani, Shalaleh and Moon, AJung},
  title		= {What does it mean to be a responsible AI practitioner: An
		  ontology of roles and skills},
  year		= {2023},
  isbn		= {9798400702310},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600211.3604702},
  doi		= {10.1145/3600211.3604702},
  abstract	= {With the growing need to regulate AI systems across a wide
		  variety of application domains, a new set of occupations
		  has emerged in the industry. The so-called responsible
		  Artificial Intelligence (AI) practitioners or AI ethicists
		  are generally tasked with interpreting and operationalizing
		  best practices for ethical and safe design of AI systems.
		  Due to the nascent nature of these roles, however, it is
		  unclear to future employers and aspiring AI ethicists what
		  specific function these roles serve and what skills are
		  necessary to serve the functions. Without clarity on these,
		  we cannot train future AI ethicists with meaningful
		  learning objectives. In this work, we examine what
		  responsible AI practitioners do in the industry and what
		  skills they employ on the job. We propose an ontology of
		  existing roles alongside skills and competencies that serve
		  each role. We created this ontology by examining the job
		  postings for such roles over a two-year period (2020-2022)
		  and conducting expert interviews with fourteen individuals
		  who currently hold such a role in the industry. Our
		  ontology contributes to business leaders looking to build
		  responsible AI teams and provides educators with a set of
		  competencies that an AI ethics curriculum can prioritize.},
  booktitle	= {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {584–595},
  numpages	= {12},
  keywords	= {Competency Framework, Education, Responsible AI
		  Practitioner},
  location	= {Montr\'{e}al, QC, Canada},
  series	= {AIES '23}
}

@InProceedings{	  10.1145/3494322.3494338,
  author	= {Hviid, Jakob and Johansen, Aslak and Caleb Sangogboye,
		  Fisayo and Kj\ae{}rgaard, Mikkel Baun},
  title		= {OPM: An Ontology-Based Package Manager for Building
		  Operating Systems},
  year		= {2022},
  isbn		= {9781450385664},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3494322.3494338},
  doi		= {10.1145/3494322.3494338},
  abstract	= {The energy sector is experiencing new challenges with the
		  move to green energy. One of these challenges is keeping a
		  stable energy grid when transitioning the production to
		  unpredictable energy generation from green sources. Demand
		  Response (DR) can mitigate some of the lack of
		  predictability by influencing the consumer’s load
		  profile. Unfortunately, the cost of implementing DR, and
		  the required infrastructure, vastly overshadows the
		  benefits for the consumer, thereby negating the incentive
		  to invest. Therefore, reducing the initial cost of
		  investment is a critical factor for the success of DR.
		  Building Operating Systems (BOS) is one possible avenue to
		  achieve DR functionality in buildings. This paper seeks to
		  reduce initial investment costs of BOSes, by introducing an
		  ontology-based package manager (OPM), that dynamically
		  resolves dependencies and installs services. An
		  ontology-based approach to dependency resolution allows for
		  loosely defined dependencies but also takes the context of
		  the service into account, as well as requirements in terms
		  of sensor availability and physical layout of the building.
		  The OPM is evaluated by deploying a BOS and accompanying
		  services for occupancy prediction. By significantly
		  reducing deployment complexity, results show considerable
		  time savings, and thereby cost reductions, on deployment
		  and maintenance activities.},
  booktitle	= {Proceedings of the 11th International Conference on the
		  Internet of Things},
  pages		= {118–125},
  numpages	= {8},
  keywords	= {OWL, Package manager, building operating systems,
		  containerization, dependency resolution, deployment,
		  ontology},
  location	= {St.Gallen, Switzerland},
  series	= {IoT '21}
}

@InProceedings{	  10.1145/3583780.3615051,
  author	= {Wang, Mengying and Guan, Sheng and Ma, Hanchao and Bian,
		  Yiyang and Che, Haolai and Daundkar, Abhishek and
		  Sehirlioglu, Alp and Wu, Yinghui},
  title		= {Selecting Top-k Data Science Models by Example Dataset},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615051},
  doi		= {10.1145/3583780.3615051},
  abstract	= {Data analytical pipelines routinely involve various
		  domain-specific data science models. Such models require
		  expensive manual or training effort and often incur
		  expensive validation costs (e.g., via scientific simulation
		  analysis). Meanwhile, high-value models remain to be
		  ad-hocly created, isolated, and underutilized for a broad
		  community. Searching and accessing proper models for data
		  analysis pipelines is desirable yet challenging for users
		  without domain knowledge. This paper introduces ModsNet, a
		  novel MODel SelectioN framework that only requires an
		  Example daTaset. (1) We investigate the following problem:
		  Given a library of pre-trained models, a limited amount of
		  historical observations of their performance, and an
		  "example" dataset as a query, return k models that are
		  expected to perform the best over the query dataset. (2) We
		  formulate a regression problem and introduce a
		  knowledge-enhanced framework using a model-data interaction
		  graph. Unlike traditional methods, (1) ModsNet uses a
		  dynamic, cost-bounded "probe-and-select" strategy to
		  incrementally identify promising pre-trained models in a
		  strict cold-start scenario (when a new dataset without any
		  interaction with existing models is given). (2) To reduce
		  the learning cost, we develop a clustering-based
		  sparsification strategy to prune unpromising models and
		  their interactions. (3) We showcase of ModsNet built on top
		  of a crowdsourced materials knowledge base platform. Our
		  experiments verified its effectiveness, efficiency, and
		  applications over real-world analytical pipelines.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2686–2695},
  numpages	= {10},
  keywords	= {GNN-based recommendation, knowledge graph, model
		  selection},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3670105.3670139,
  author	= {Lin, Wangqun and Xu, Jing and Tian, Yu and Peng, Baoyun
		  and Li, Yan and Ge, Yawei},
  title		= {Cognitive Intelligence: Driven by Knowledge Graph and Big
		  Model Collaboration},
  year		= {2024},
  isbn		= {9798400716751},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670105.3670139},
  doi		= {10.1145/3670105.3670139},
  abstract	= {Abstract: Cognitive intelligence is primarily
		  characterized by the understanding, reasoning, cognition,
		  and decision-making of complex things. It is a higher-order
		  form of artificial intelligence development. This paper
		  provides an in-depth analysis of two representative
		  technologies, knowledge graph and big model, which promote
		  the development of cognitive intelligence. Firstly, we
		  systematically sorts out the characteristics, advantages,
		  and shortcomings of these two technologies. Secondly, we
		  proposes technical approaches and main methods for the
		  mutual enhancement of knowledge graph and big model.
		  Finally, we provides the main direction for the integrated
		  development of knowledge graph and big model to promote the
		  development of cognitive intelligence. We hope our work can
		  provide reference and inspiration for relevant engineers
		  and technical researchers.CCS Concepts: .Computing
		  methodologies → Artificial intelligence; Knowledge
		  representation and reasoning},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computing, Networks and Internet of Things},
  pages		= {204–209},
  numpages	= {6},
  keywords	= {artificial intelligence, big model, cognitive
		  intelligence, knowledge graph},
  location	= {Tokyo, Japan},
  series	= {CNIOT '24}
}

@InProceedings{	  10.1145/3503161.3548098,
  author	= {Wang, Xiao and Gan, Tian and Wei, Yinwei and Wu, Jianlong
		  and Meng, Dai and Nie, Liqiang},
  title		= {Micro-video Tagging via Jointly Modeling Social Influence
		  and Tag Relation},
  year		= {2022},
  isbn		= {9781450392037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503161.3548098},
  doi		= {10.1145/3503161.3548098},
  abstract	= {The last decade has witnessed the proliferation of
		  micro-videos on various user-generated content platforms.
		  According to our statistics, around 85.7\% of micro-videos
		  lack annotation. In this paper, we focus on annotating
		  micro-videos with tags. Existing methods mostly focus on
		  analyzing video content, neglecting users' social influence
		  and tag relation. Meanwhile, existing tag relation
		  construction methods suffer from either deficient
		  performance or low tag coverage. To jointly model social
		  influence and tag relation, we formulate micro-video
		  tagging as a link prediction problem in a constructed
		  heterogeneous network. Specifically, the tag relation
		  (represented by tag ontology) is constructed in a
		  semi-supervised manner. Then, we combine tag relation,
		  video-tag annotation, and user follow relation to build the
		  network. Afterward, a better video and tag representation
		  are derived through Behavior Spread modeling and visual and
		  linguistic knowledge aggregation. Finally, the semantic
		  similarity between each micro-video and all candidate tags
		  is calculated in this video-tag network. Extensive
		  experiments on industrial datasets of three verticals
		  verify the superiority of our model compared with several
		  state-of-the-art baselines.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Multimedia},
  pages		= {4478–4486},
  numpages	= {9},
  keywords	= {behavior spread, micro-video tagging, ontology
		  construction},
  location	= {Lisboa, Portugal},
  series	= {MM '22}
}

@InProceedings{	  10.1145/3442381.3450042,
  author	= {Geng, Yuxia and Chen, Jiaoyan and Chen, Zhuo and Pan, Jeff
		  Z. and Ye, Zhiquan and Yuan, Zonggang and Jia, Yantao and
		  Chen, Huajun},
  title		= {OntoZSL: Ontology-enhanced Zero-shot Learning},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450042},
  doi		= {10.1145/3442381.3450042},
  abstract	= {Zero-shot Learning (ZSL), which aims to predict for those
		  classes that have never appeared in the training data, has
		  arisen hot research interests. The key of implementing ZSL
		  is to leverage the prior knowledge of classes which builds
		  the semantic relationship between classes and enables the
		  transfer of the learned models (e.g., features) from
		  training classes (i.e., seen classes) to unseen classes.
		  However, the priors adopted by the existing methods are
		  relatively limited with incomplete semantics. In this
		  paper, we explore richer and more competitive prior
		  knowledge to model the inter-class relationship for ZSL via
		  ontology-based knowledge representation and semantic
		  embedding. Meanwhile, to address the data imbalance between
		  seen classes and unseen classes, we developed a generative
		  ZSL framework with Generative Adversarial Networks (GANs).
		  Our main findings include: (i) an ontology-enhanced ZSL
		  framework that can be applied to different domains, such as
		  image classification (IMGC) and knowledge graph completion
		  (KGC); (ii) a comprehensive evaluation with multiple
		  zero-shot datasets from different domains, where our method
		  often achieves better performance than the state-of-the-art
		  models. In particular, on four representative ZSL baselines
		  of IMGC, the ontology-based class semantics outperform the
		  previous priors e.g., the word embeddings of classes by an
		  average of 12.4 accuracy points in the standard ZSL across
		  two example datasets (see Figure&nbsp;4).},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {3325–3336},
  numpages	= {12},
  keywords	= {Generative Adversarial Networks, Image Classification,
		  Knowledge Graph Completion, Ontology, Zero-shot Learning},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InBook{	  10.1145/3581906.3581911,
  author	= {Bilidas, Dimitris},
  title		= {Ontologies and Linked Data},
  year		= {2023},
  isbn		= {9798400707407},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  url		= {https://doi.org/10.1145/3581906.3581911},
  booktitle	= {Geospatial Data Science: A Hands-on Approach for Building
		  Geospatial Applications Using Linked Data Technologies},
  pages		= {53–66},
  numpages	= {14}
}

@Proceedings{	  10.1145/3660853,
  title		= {AICCONF '24: Proceedings of the Cognitive Models and
		  Artificial Intelligence Conference},
  year		= {2024},
  isbn		= {9798400716928},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {undefinedstanbul, Turkiye}
}

@InProceedings{	  10.1145/3404709.3404770,
  author	= {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
  title		= {Scaled Scrum Framework for Cooperative Domain Ontology
		  Evolution},
  year		= {2020},
  isbn		= {9781450375337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404709.3404770},
  doi		= {10.1145/3404709.3404770},
  abstract	= {The field of research in ontology engineering appears to
		  be mature, considering the vast number of contemporary
		  methods and instruments for the formalization and
		  application of knowledge representation models. However,
		  the evolutionary aspects of ontologies are still little
		  understood and supported. This is especially important in
		  distributed and collaborative settings like the Semantic
		  web, where ontologies naturally co-operate with their user
		  communities. Various organizations and teams are building
		  common ground in this context. Ontology is instrumental in
		  this process through the formal description of shared
		  knowledge. Such semanticity constitutes a sound basis for
		  defining, sharing (business) objectives and interests and
		  eventually developing useful collaborative services and
		  systems. In this "complex" and dynamic environment, a
		  collaborative model for process change requires more
		  powerful methodologies for engineering, argumentation and
		  negotiation. Software Engineering provides teamwork, team
		  management, feedback management, versioning, merging, and
		  evolving software artifacts with a wealth of techniques and
		  tools. Many of these techniques can be used again in an
		  ontology engineering environment. This paper examines how
		  this problem can be resolved using Scrum and Nexus
		  frameworks, which are among the most robust models for
		  software development.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Frontiers of Educational Technologies},
  pages		= {135–143},
  numpages	= {9},
  keywords	= {Collaborative Evolution, Inter-organizational Ontology,
		  Nexus, Ontology Evolution, Scrum},
  location	= {Tokyo, Japan},
  series	= {ICFET '20}
}

@Article{	  10.1145/3429251,
  author	= {Joy, Jeevamol and Raj, Nisha S. and V. G., Renumol},
  title		= {Ontology-based E-learning Content Recommender System for
		  Addressing the Pure Cold-start Problem},
  year		= {2021},
  issue_date	= {September 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {3},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3429251},
  doi		= {10.1145/3429251},
  abstract	= {E-learning recommender systems are gaining significance
		  nowadays due to its ability to enhance the learning
		  experience by providing tailor-made services based on
		  learner preferences. A Personalized Learning Environment
		  (PLE) that automatically adapts to learner characteristics
		  such as learning styles and knowledge level can recommend
		  appropriate learning resources that would favor the
		  learning process and improve learning outcomes. The pure
		  cold-start problem is a relevant issue in PLEs, which
		  arises due to the lack of prior information about the new
		  learner in the PLE to create appropriate recommendations.
		  This article introduces a semantic framework based on
		  ontology to address the pure cold-start problem in content
		  recommenders. The ontology encapsulates the domain
		  knowledge about the learners as well as Learning Objects
		  (LOs). The semantic model that we built has been
		  experimented with different combinations of the key learner
		  parameters such as learning style, knowledge level, and
		  background knowledge. The proposed framework utilizes these
		  parameters to build natural learner groups from the learner
		  ontology using SPARQL queries. The ontology holds 480
		  learners’ data, 468 annotated learning objects with 5,600
		  learner ratings. A multivariate k-means clustering
		  algorithm, an unsupervised machine learning technique for
		  grouping similar data, is used to evaluate the learner
		  similarity computation accuracy. The learner satisfaction
		  achieved with the proposed model is measured based on the
		  ratings given by the 40 participants of the experiments.
		  From the evaluation perspective, it is evident that 79\% of
		  the learners are satisfied with the recommendations
		  generated by the proposed model in pure cold-start
		  condition.},
  journal	= {J. Data and Information Quality},
  month		= apr,
  articleno	= {16},
  numpages	= {27},
  keywords	= {Learner profile, learning object, personalized learning
		  environment, pure cold-start problem, content recommenders,
		  ontology, multivariate clustering}
}

@InProceedings{	  10.1145/3701716.3715178,
  author	= {Wu, Junfeng and He, Jing and Liu, Hai and Zheng, Zhaoqi
		  and Cao, Yichen and Chen, Xingguo and Zou, Bingjie and Zou,
		  Ruiping and Zhou, Guohua and Sturgess, David and van
		  Zundert, Andr\'{e}},
  title		= {From Literature to Lab: Hardware-Independent Autonomous
		  Chemical Synthesis with Reinforcement Learning},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715178},
  doi		= {10.1145/3701716.3715178},
  abstract	= {Chemical synthesis, a fundamental process in chemical
		  engineering, traditionally requires extensive manual
		  intervention and expertise, particularly in interpreting
		  scientific literature and translating it into executable
		  workflows. Autonomous systems offer the potential to
		  revolutionize this field by enabling robots to autonomously
		  read scientific literature and form general synthesis
		  workflows. However, a key challenge lies in integrating
		  diverse hardware components, requiring adaptable
		  architectures that can seamlessly operate across various
		  platforms. Furthermore, these systems must be robust,
		  allowing for real-time error correction, and accessible to
		  non-programmers. This paper presents Autonomous Chemical
		  Synthesis System (ACSS), an adaptable architecture for
		  chemical execution systems designed to address these
		  challenges. ACSS enables robots to autonomously read
		  scientific literature and form general synthesis workflows.
		  We achieve hardware independence through
		  protocol-integration reinforcement learning, enabling
		  seamless integration across platforms. Chemical code and
		  hardware descriptions are compiled and converted into
		  robotic instructions for execution. Natural language error
		  correction allows non-programmers to easily adjust the
		  system. Our key contributions are: (1) Literature to Lab:
		  directly extracting synthesis protocols from scientific
		  articles; (2) Hardware Independence: developing an
		  adaptive, reinforcement learning-based model for diverse
		  hardware setups. ACSS has successfully synthesized 12
		  diverse compounds from literature, including lidocaine (a
		  painkiller), Dess-Martin periodinane (an oxidizing agent),
		  and alkyl fluoride (a fluorinating agent). This
		  demonstration highlights the potential of NLP and
		  reinforcement learning for efficient and accessible
		  chemical research and production. Click here to find out
		  more on YouTube.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2923–2926},
  numpages	= {4},
  keywords	= {large language model, natural language processing,
		  reinforcement learning, robot chemist, task dependency
		  graph},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3674558.3674597,
  author	= {Mezhuyev, Vitaliy and Hofmann, Paul},
  title		= {Expert System for Bainite Design: the Approach to Enrich
		  Physical Models with Information Derived from Knowledge
		  Models},
  year		= {2024},
  isbn		= {9798400716386},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3674558.3674597},
  doi		= {10.1145/3674558.3674597},
  abstract	= {The development of a physical model begins with a
		  knowledge model, initially existing as ideas in the mind of
		  a researcher. A transition from knowledge models to strict
		  mathematical formalisms is a challenging process, and may
		  not always be feasible, particularly in the early stages of
		  research. Another problem comes when many experts are
		  participating in the development of new physical knowledge,
		  which may result in inconsistency. To contribute to this
		  domain, the paper presents the development of an expert
		  system (ES), created to capture expert knowledge for the
		  design of a new physical material, namely, the bainite
		  steel. The ES combines physical properties and rules in a
		  unique knowledge model and enriches them by derived from
		  data probabilities. The proposed approach enables users to
		  validate expert knowledge and find contradictions in the
		  logical rules, giving the possibility of mapping them back
		  to physical models.},
  booktitle	= {Proceedings of the 2024 10th International Conference on
		  Computer Technology Applications},
  pages		= {270–275},
  numpages	= {6},
  keywords	= {Bainite steel, Expert system, Material design, Physical
		  modelling, Probabilistic programming},
  location	= {Vienna, Austria},
  series	= {ICCTA '24}
}

@InProceedings{	  10.1145/3652620.3688245,
  author	= {Burattini, Samuele and Zimmermann, Antoine and Picone,
		  Marco and Ricci, Alessandro},
  title		= {Towards Linked Data for Ecosystems of Digital Twins},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688245},
  doi		= {10.1145/3652620.3688245},
  abstract	= {Due to either the inherent complexity of the domain or the
		  evolving nature of systems, we can envision solutions that
		  digitalize assets in a complex domain using an ecosystem of
		  distributed Digital Twins instead of a single monolithic
		  one. To effectively tackle interoperability in such
		  ecosystems, this paper advocates for the introduction of a
		  representation based on Semantic Web technologies enabling
		  the discovery of both Digital Twin structure - i.e. the
		  static information about the asset model and offered
		  services - and state - i.e. the data and metrics collected
		  at runtime - to support the management of ecosystems and
		  the creation of application mashups. A review of the state
		  of the art suggests that currently investigated ways to
		  describe a Digital Twin are not sufficient to achieve this
		  objective. A proposal of key requirements for a Digital
		  Twin representation is outlined leading to the proposal of
		  a core ontology and a Linked Data approach for state
		  management.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {332–337},
  numpages	= {6},
  keywords	= {digital twins, semantic web, interoperability,
		  ontologies},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3322640.3326725,
  author	= {El Ghosh, Mirna and Abdulrab, Habib},
  title		= {The Application of ODCM for Building Well-Founded Legal
		  Domain Ontologies: A Case Study in the Domain of Carriage
		  of Goods by Sea},
  year		= {2019},
  isbn		= {9781450367547},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3322640.3326725},
  doi		= {10.1145/3322640.3326725},
  abstract	= {The ontology engineering community is facing several key
		  challenges about the development of domain ontologies. One
		  major challenge is the building of well-founded domain
		  ontologies. This concept has raised recently and it refers
		  to ontologies that are grounded in validated foundational
		  ontologies. This paper addresses the effective contribution
		  of ontology-driven conceptual modeling process (ODCM) for
		  developing such ontologies in the legal domain. A case
		  study in the domain of carriage of goods by sea is
		  presented.},
  booktitle	= {Proceedings of the Seventeenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {204–208},
  numpages	= {5},
  keywords	= {OntoUML, Ontology-Driven Conceptual Modeling, UFO, legal
		  ontologies, well-founded ontologies},
  location	= {Montreal, QC, Canada},
  series	= {ICAIL '19}
}

@InProceedings{	  10.1145/3705328.3748165,
  author	= {Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni
		  and Marras, Mirko and Medda, Giacomo and Murgia, Giovanni},
  title		= {GreenFoodLens: Sustainability Labels for Food
		  Recommendation},
  year		= {2025},
  isbn		= {9798400713644},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3705328.3748165},
  doi		= {10.1145/3705328.3748165},
  abstract	= {Most food recommender systems aim to boost user engagement
		  by analyzing recipe ingredients and users’ past choices.
		  Even though consumers are paying more attention to
		  sustainability, such as carbon and water footprints, there
		  remains a notable lack of public corpora that combine
		  detailed user–recipe interactions with reliable
		  environmental impact data. This gap makes it hard to build
		  recommendation tools that both match people’s tastes and
		  help reduce ecological damage. To this end, we present
		  GreenFoodLens, a resource that enriches HUMMUS, one of the
		  largest corpora for food recommendation, with environmental
		  impact estimates derived from the hierarchical taxonomy of
		  the SU-EATABLE-LIFE project. We achieved this result
		  through a multi-step process involving human annotations,
		  iterative labeling assessments, knowledge refinement, and
		  constrained generation techniques with large language
		  models. Finally, we evaluate recommendation baselines on
		  HUMMUS augmented with GreenFoodLens labels and find that
		  models are driven by popularity signals, which may
		  exacerbate the environmental impact of users’ recipe
		  choices. These experiments demonstrate the practical
		  benefit of GreenFoodLens for benchmarking and advancing
		  sustainability-aware recommendation research. The resource
		  is available at .},
  booktitle	= {Proceedings of the Nineteenth ACM Conference on
		  Recommender Systems},
  pages		= {764–773},
  numpages	= {10},
  keywords	= {Sustainability, Food Recommendation, Recipe
		  Recommendation, Large Language Model, Constrained
		  Generation, Human Labeling.},
  location	= { },
  series	= {RecSys '25}
}

@InProceedings{	  10.1145/3726122.3726228,
  author	= {Kayumova, Kamola and Akbarova, Shakhnoza and
		  Bobokeldiyeva, Maftuna and Abdukarimova, Gulchehra and
		  Khaydarova, Umida and Xasanova, Zarina},
  title		= {Systematic Mapping of Computational Linguistics in
		  Distributed Knowledge Based Systems and Management},
  year		= {2025},
  isbn		= {9798400711701},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726122.3726228},
  doi		= {10.1145/3726122.3726228},
  abstract	= {Advancements in computational linguistics have enabled the
		  formulation of multiple natural language processing
		  frameworks with considerable semantic accuracy, knowledge
		  representation, and real-time adaptability benefits.
		  Emerging research on distributed knowledge-based systems is
		  challenging traditional conceptions of language processing
		  and data integration, and in the process, opening up
		  windows of opportunity for enhancing the scalability
		  associated with knowledge management in decentralized
		  environments. As little is known about where computational
		  linguistics integration is gaining momentum beyond academic
		  research and software engineering, the purpose of this
		  systematic mapping study is to map in what areas of
		  distributed knowledge management it is perceived to gain
		  traction. Drawing on data from 150 systematically selected
		  research articles and trend analysis in computational
		  linguistics applications, we identify a long tail of
		  application domains and methodological approaches in which
		  a total of 42 unique computational models operate,
		  including techniques such as knowledge graph embeddings,
		  transformer-based architectures, and multimodal language
		  models. Our findings reveal a strong, positive correlation
		  coefficient (r = 0.82) between natural language processing
		  adoption and knowledge retrieval efficiency in distributed
		  systems. However, existing frameworks do not passively
		  comply. Rather, their linguistic adaptability and semantic
		  interpretation mechanisms are integrated into the core
		  functionality of distributed knowledge networks. The study
		  concludes by identifying key research gaps, reflecting on
		  the application of machine learning-enhanced linguistic
		  models in the field of knowledge management, and proposing
		  suggestions for future research directions in distributed
		  data processing. The findings enrich understandings of the
		  workings of computational linguistics methodologies in
		  experiences of real-time knowledge extraction and
		  intelligent information retrieval.},
  booktitle	= {Proceedings of the 8th International Conference on Future
		  Networks \&amp; Distributed Systems},
  pages		= {727–733},
  numpages	= {7},
  location	= { },
  series	= {ICFNDS '24}
}

@InProceedings{	  10.1145/3639233.3639242,
  author	= {Miskell, Cameron and Diaz, Richard and Ganeriwala, Parth
		  and Slhoub, Khaled and Nembhard, Fitzroy},
  title		= {Automated Framework to Extract Software Requirements from
		  Source Code},
  year		= {2024},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639233.3639242},
  doi		= {10.1145/3639233.3639242},
  abstract	= {Software maintenance and innovation are constant
		  challenges across industries, especially as programming
		  languages evolve with technology. Similarly, poor lexicon
		  quality degrades program comprehension, increasing the
		  effort required by developers to improve existing software
		  products. To address these challenges, we propose a novel
		  automated framework that extracts software requirements
		  directly from source code using a baseline AI language
		  model applied to a Java code base. Leveraging natural
		  language processing techniques, the framework validates
		  programs and generates easily readable requirements by
		  analyzing file contents. The framework enhances agility and
		  flexibility by providing comprehensive documentation for
		  existing software systems. It caters to both experienced
		  and less-experienced developers, offering an intuitive
		  graphical user interface and enabling efficient
		  identification and resolution of errors. The resulting
		  output facilitates natural interaction through language
		  processing. By automating the extraction process, the
		  framework allows developers to better understand software
		  systems, make informed decisions, and adapt to evolving
		  needs.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {130–134},
  numpages	= {5},
  keywords	= {AI language model, Extracting functional requirements,
		  Legacy code, Natural Language Processing, Software
		  evolution, Software verification},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '23}
}

@InProceedings{	  10.1145/3617695.3617722,
  author	= {Yang, Yi and Yu, Dekuang and Zhu, Yangyang and Qin, Yuxuan
		  and Wang, Erhao},
  title		= {Design of General Chronic Disease Retrieval Model
		  Framework Based on Chinese Medical Knowledge Graph},
  year		= {2023},
  isbn		= {9798400708015},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3617695.3617722},
  doi		= {10.1145/3617695.3617722},
  abstract	= {This article proposes a method for constructing a chronic
		  disease retrieval model based on the Chinese medical
		  knowledge graph. By combining the Chinese medical knowledge
		  graph with classification retrieval, a chronic disease
		  classification retrieval model based on the medical
		  knowledge graph is constructed, which mainly includes three
		  aspects: constructing medical knowledge graph for
		  retrieval, designing hierarchical classification rules,
		  scheming sorting strategies and display methods. The
		  proposed hierarchical classification retrieval model
		  mechanism and related strategies are conducive to the
		  effective organization of health information, solving the
		  current problems of multi-source heterogeneity and semantic
		  ambiguity, improving the efficiency and quality of user
		  retrieval, has a certain promoting effect on the
		  development of theories and methods related to health
		  information services.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Big Data and Internet of Things},
  pages		= {195–200},
  numpages	= {6},
  keywords	= {Chinese medical knowledge graph, Chronic diseases,
		  construction method, retrieval model framework},
  location	= {Beijing, China},
  series	= {BDIOT '23}
}

@InProceedings{	  10.1145/3404709.3404769,
  author	= {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
  title		= {Blockchain as a Platform for Collaborative Ontology
		  Evolution},
  year		= {2020},
  isbn		= {9781450375337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404709.3404769},
  doi		= {10.1145/3404709.3404769},
  abstract	= {The Semantic Web is an incomplete dream so far, but a
		  revolutionary platform as Blockchain could be the solution
		  to this, not optimal reality. There is still little
		  understanding of, and support for, the evolutionary aspects
		  of ontologies. This is particularly crucial in distributed
		  and collaborative settings such as the Semantic Web, where
		  ontologies naturally co-evolve with their communities of
		  use. In this setting, different organizations and teams
		  collaboratively build a common ground of the domain. In
		  this "complex" and dynamic setting, a collaborative change
		  process model requires more powerful engineering,
		  argumentation and negotiation methodologies. Blockchain
		  offers a robust framework for teams' collaboration and
		  ontology versioning globally between an infinite number of
		  teams. Blockchain is an example of a distributed computing
		  system with high Byzantine fault tolerance. This makes
		  blockchains potentially suitable for the recording of
		  evolution events, ontology records, and other records
		  management activities, such as ontology evolution,
		  transaction processing and ontology documenting provenance.
		  In this paper, after briefly summarizing the significant
		  features of Blockchain, we describe blockchain-empowered
		  solutions for building an evolution model based on
		  blockchain technology and its artifacts.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Frontiers of Educational Technologies},
  pages		= {183–190},
  numpages	= {8},
  keywords	= {Blockchain, Collaborative Evolution, Consensus,
		  Distributed Computing, Inter-organizational Ontology,
		  Ontology Evolution, validation and Evaluation},
  location	= {Tokyo, Japan},
  series	= {ICFET '20}
}

@Article{	  10.1145/3574135,
  author	= {Zhang, Bolin and Tu, Zhiying and Hang, Shaoshi and Chu,
		  Dianhui and Xu, Xiaofei},
  title		= {Conco-ERNIE: Complex User Intent Detect Model for Smart
		  Healthcare Cognitive Bot},
  year		= {2023},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {1},
  issn		= {1533-5399},
  url		= {https://doi.org/10.1145/3574135},
  doi		= {10.1145/3574135},
  abstract	= {The outbreak of Covid-19 has exposed the lack of medical
		  resources, especially the lack of medical personnel. This
		  results in time and space restrictions for medical
		  services, and patients cannot obtain health information all
		  the time and everywhere. Based on the medical knowledge
		  graph, healthcare bots alleviate this burden effectively by
		  providing patients with diagnosis guidance, pre-diagnosis,
		  and post-diagnosis consultation services in the way of
		  human-machine dialogue. However, the medical utterance is
		  more complicated in language structure, and there are
		  complex intention phenomena in semantics. It is a challenge
		  to detect the single intent, multi-intent, and implicit
		  intent of a patient’s utterance. To this end, we create a
		  high-quality annotated Chinese Medical query (utterance)
		  dataset, CMedQ (about 16.8k queries in medical domain which
		  includes single, multiple, and implicit intents). It is
		  hard to detect intent on such a complex dataset through
		  traditional text classification models. Thus, we propose a
		  novel detect model Conco-ERNIE, using concept co-occurrence
		  patterns to enhance the representation of pre-trained model
		  ERNIE. These patterns are mined using Apriori algorithm and
		  will be embedded via Node2Vec. Their features will be
		  aggregated with semantic features into Conco-ERNIE by using
		  an attention module, which can catch user explicit intents
		  and also predict user implicit intents. Experiments on
		  CMedQ demonstrates that Conco-ERNIE achieves outstanding
		  performance over baseline. Based on Conco-ERNIE, we develop
		  an intelligent healthcare bot, MedicalBot. To provide
		  knowledge support for MedicalBot, we also build a Chinese
		  medical graph, CMedKG (about 45k entities and 283k
		  relationships).},
  journal	= {ACM Trans. Internet Technol.},
  month		= feb,
  articleno	= {21},
  numpages	= {24},
  keywords	= {Intent detection, healthcare bot, cognitive service,
		  medical knowledge graph}
}

@InProceedings{	  10.1145/3548608.3559289,
  author	= {Wang, Guodong and Liu, Guohua},
  title		= {A unified modeling method of product demand and
		  manufacturing capability in the textile industry Internet},
  year		= {2022},
  isbn		= {9781450397179},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3548608.3559289},
  doi		= {10.1145/3548608.3559289},
  abstract	= {Through the analysis of multiple cloud manufacturing
		  models, we discussed the definition of the current textile
		  industry Internet manufacturing model, and proposed the
		  concept of the textile industry knowledge graph for the
		  standardized expression and sharing of demand and
		  manufacturing resources under the textile industry Internet
		  manufacturing model; A unified modeling and packaging
		  method for manufacturing requirements and manufacturing
		  capabilities based on knowledge graph technology with
		  standard process flow as the interface is proposed.},
  booktitle	= {Proceedings of the 2022 2nd International Conference on
		  Control and Intelligent Robotics},
  pages		= {690–694},
  numpages	= {5},
  keywords	= {Industrial Internet, cloud manufacturing, knowledge graph,
		  meta-model, ontology},
  location	= {Nanjing, China},
  series	= {ICCIR '22}
}

@InProceedings{	  10.1145/3424978.3425102,
  author	= {Li, Guoxuan},
  title		= {Improving Biomedical Ontology Matching Using
		  Domain-specific Word Embeddings},
  year		= {2020},
  isbn		= {9781450377720},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3424978.3425102},
  doi		= {10.1145/3424978.3425102},
  abstract	= {Biomedical ontology is an effective carrier of biomedical
		  knowledge. In real-world applications, many biomedical
		  ontologies describe knowledge in the same field. In order
		  to make full use of existing knowledge, it is necessary to
		  carry out knowledge fusion to obtain a unified knowledge
		  structure. For this reason, it becomes particularly
		  important to find mapping entities that refer to the same
		  object in different ontologies. At present, a large number
		  of automatic matching systems engineers features and match
		  entities by the name of the entity, the ontology structure
		  and external resources. These methods have achieved
		  encouraging results, but they ignore the semantic
		  information of the entity labels. On the other hand, the
		  representation learning method has already shined in many
		  areas of natural language processing. The word vector
		  obtained by the word embedding method contains semantic
		  information of words. However, a separate representation
		  learning method cannot fully capture the structural
		  information of the ontology. In this paper, we propose a
		  method which combines the representation learning method as
		  a component with traditional feature engineering methods to
		  improve the performance of the matching systems. We tested
		  our method on real-world datasets. Experimental results
		  show that our method can improve the recall and F1-measure
		  of the existing matching systems.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Computer Science and Application Engineering},
  articleno	= {120},
  numpages	= {5},
  keywords	= {Artificial intelligence, Biomedical ontology matching,
		  Feature engineering, Word embedding},
  location	= {Sanya, China},
  series	= {CSAE '20}
}

@InProceedings{	  10.1145/3563657.3596027,
  author	= {Rakib, Mohammad Abu Nasir and Scidmore, Jeremy and
		  Ginsberg, Justin and Torres, Cesar},
  title		= {Thermoplastic Kilnforms: Extending Glass Kilnforming
		  Techniques to Thermoplastic Materials using Ontology-Driven
		  Design},
  year		= {2023},
  isbn		= {9781450398930},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3563657.3596027},
  doi		= {10.1145/3563657.3596027},
  abstract	= {The ecology of thermoplastic materials is rapidly
		  evolving, enabling an exciting landscape of functional,
		  aesthetic, and interactive forms. Despite their utility in
		  fused filament fabrication (FFF), an even larger and
		  untapped design space exists for thermoplastics. In this
		  work, we introduce a design method that leverages
		  similarities with a more mature medium (glass) to guide a
		  material-centered exploration of a new medium
		  (thermoplastics). Through a collaboration between domain
		  experts in thermoplastics and glass, we synthesized an
		  ontology of kilnforming techniques and developed an
		  annotated portfolio of thermoplastic kilnforms that capture
		  generative design directions for altering the
		  phenomenological qualities of plastic, prototyping
		  metamaterials, and composite forms, and engaging with other
		  material practices. We discuss how material parallels can
		  continue to expand the role of thermoplastics as a design
		  material and how ontology-driven design can serve as a
		  means of localizing, questioning, and generating material
		  knowledge.},
  booktitle	= {Proceedings of the 2023 ACM Designing Interactive Systems
		  Conference},
  pages		= {263–281},
  numpages	= {19},
  keywords	= {composites, material exploration, thermoforming},
  location	= {Pittsburgh, PA, USA},
  series	= {DIS '23}
}

@InProceedings{	  10.1145/3571884.3597131,
  author	= {Aicher, Annalena Bea and Kornm\"{u}ller, Daniel and
		  Minker, Wolfgang and Ultes, Stefan},
  title		= {Self-imposed Filter Bubble Model for Argumentative
		  Dialogues},
  year		= {2023},
  isbn		= {9798400700149},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571884.3597131},
  doi		= {10.1145/3571884.3597131},
  abstract	= {During their information seeking people tend to filter out
		  all the parts of the available information that do not fit
		  their existing beliefs or opinions. In this paper we
		  present a model for this “Self-imposed Filter Bubble”
		  (SFB) consisting of four dimensions. Thereby, we aim to 1)
		  estimate the probability of the user being caught in an SFB
		  and consequently, 2) identify suitable clues to reduce this
		  probability in the further course of a dialogue. Using an
		  exemplary implementation in an argumentative dialogue
		  system, we demonstrate the validity and applicability of
		  this model in an online user study with 102 participants.
		  These findings serve as a basis for developing a system
		  strategy to break the user’s SFB and contribute to a
		  sustainable and profound reflection on a topic from all
		  viewpoints.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Conversational User Interfaces},
  articleno	= {23},
  numpages	= {11},
  keywords	= {Computational Argumentation, Confirmation Bias,
		  Cooperative Argumentative Dialogue Systems (ADS), Echo
		  Chambers, User Modeling},
  location	= {Eindhoven, Netherlands},
  series	= {CUI '23}
}

@InProceedings{	  10.1145/3529190.3535693,
  author	= {Adhikari, Ajaya and Wenink, Edwin and van der Waa, Jasper
		  and Bouter, Cornelis and Tolios, Ioannis and Raaijmakers,
		  Stephan},
  title		= {Towards FAIR Explainable AI: a standardized ontology for
		  mapping XAI solutions to use cases, explanations, and AI
		  systems},
  year		= {2022},
  isbn		= {9781450396318},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3529190.3535693},
  doi		= {10.1145/3529190.3535693},
  abstract	= {Several useful taxonomies have been published that survey
		  the eXplainable AI (XAI) research field. However, these
		  taxonomies typically do not show the relation between XAI
		  solutions and several use case aspects, such as the
		  explanation goal or the task context. In order to better
		  connect the field of XAI research with concrete use cases
		  and user needs, we designed the ASCENT (Ai System use Case
		  Explanation oNTology) framework, which is a new ontology
		  and corresponding metadata standard with three
		  complementary modules for different aspects of an XAI
		  solution: one for aspects of AI systems, another for use
		  case aspects, and yet another for explanation properties.
		  The descriptions of XAI solutions in this framework include
		  whether the XAI solution has a positive, negative,
		  inconclusive or unresearched relation with use case
		  elements. Descriptions in ASCENT thus emphasize the (user)
		  evaluation of XAI solutions in order to support finding
		  validated practices for application in industry, as well as
		  being helpful for identifying research gaps. Describing XAI
		  solutions according to the proposed common metadata
		  standard is an important step towards the FAIR (Findable,
		  Accessible, Interoperable, Reusable) usage of XAI
		  solutions.},
  booktitle	= {Proceedings of the 15th International Conference on
		  PErvasive Technologies Related to Assistive Environments},
  pages		= {562–568},
  numpages	= {7},
  keywords	= {ASCENT, FAIR, XAI ontology, user-centered},
  location	= {Corfu, Greece},
  series	= {PETRA '22}
}

@InProceedings{	  10.1109/icse-seip58684.2023.00024,
  author	= {Rajbhoj, Asha and Nistala, Padmalata and Kulkarni, Vinay
		  and Soni, Shivani and Pathan, Ajim},
  title		= {DocToModel: Automated Authoring of Models from Diverse
		  Requirements Specification Documents},
  year		= {2023},
  isbn		= {9798350300376},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ICSE-SEIP58684.2023.00024},
  doi		= {10.1109/ICSE-SEIP58684.2023.00024},
  abstract	= {Early stages of Software Development Life Cycle (SDLC)
		  namely requirement elicitation and requirements analysis
		  have remained document-centric in the industry for
		  market-driven, complex, large-scale business applications
		  and products. The documentation typically runs into
		  hundreds of Natural Language (NL) text documents which
		  requirements engineers need to sift looking for the
		  relevant information and also maintain these documents
		  in-sync over time - a time-consuming and error-prone
		  activity. Much of this difficulty can be overcome if the
		  information is available in a structured form that is
		  amenable to automated processing. Purposive models offer a
		  way out. However, for easy adoption by industry
		  practitioners, these models must be populated from NL text
		  documents in a largely automated manner. This task is
		  characterized by high variability with several documents
		  containing different information conforming to different
		  structures and styles. As a result, purposive information
		  extractors need to be developed for each project/ product.
		  Moreover, being an open-ended space there is no upper bound
		  on the information extractors that need to be developed. To
		  overcome this difficulty, we propose a document structure
		  agnostic and meta-model agnostic tool, DocToModel, for the
		  automated authoring of models from NL text documents. It
		  provides a pattern mapping language to specify a mapping of
		  structured and unstructured document information to
		  meta-model elements, and a pattern interpreter to automate
		  model authoring. The configurable and extensible
		  architecture of DocToModel makes it generic and amenable to
		  easy repurposing for other NL documents. This paper,
		  describes the approach and illustrates its utility and
		  efficacy on multiple real-world case studies.},
  booktitle	= {Proceedings of the 45th International Conference on
		  Software Engineering: Software Engineering in Practice},
  pages		= {199–210},
  numpages	= {12},
  keywords	= {meta-model, automated model authoring, model extraction,
		  document parser, NLP, meta-model pattern, pattern
		  interpreter},
  location	= {Melbourne, Australia},
  series	= {ICSE-SEIP '23}
}

@InProceedings{	  10.1145/3729434.3729456,
  author	= {B\"{o}hm, Karsten},
  title		= {Deep and Surface Representation of Competences in Academic
		  Curricula: A new approach to address human consumers and
		  formal structures using Generative AI},
  year		= {2025},
  isbn		= {9798400712630},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3729434.3729456},
  doi		= {10.1145/3729434.3729456},
  abstract	= {The curricular design of programs in Higher Education is
		  increasingly oriented towards competence-based learning
		  goals in order to provide more specific and explicit
		  qualifications. At the same time the descriptions of
		  competences are mostly based on textual descriptions often
		  in informal style targeted towards different audiences
		  making descriptions vague and harder to compare. Formal
		  models for the specification of competences exist for some
		  time and even semantic models are being developed, e.g.,
		  the European Learning Model. These different approaches
		  lead to a gap between formal and informal competence
		  descriptions that require manual efforts of curriculum
		  designer to maintain. This research borrows the concept of
		  Deep and Surface representation models from the field of
		  linguistics to unify the different approaches.
		  Additionally, it focuses on the functionality of Generative
		  Artificial Intelligence in the form of Large Language
		  Models to close the aforementioned gap. It demonstrates the
		  potential of the technology in both directions and
		  discusses application potential of the concept.},
  booktitle	= {Proceedings of the 6th International Conference on Modern
		  Educational Technology},
  pages		= {78–85},
  numpages	= {8},
  keywords	= {Curricular Design, European Learning Model, Generative
		  Artificial Intelligence, Higher Education, LLM, Semantic
		  Web},
  location	= { },
  series	= {ICMET '24}
}

@InProceedings{	  10.1145/3643666.3648580,
  author	= {Miranda, Darliane and Ara\'{u}jo, Jo\~{a}o and Liebel,
		  Grischa},
  title		= {A Conceptual Model For Web Accessibility Requirements In
		  Agile Development},
  year		= {2024},
  isbn		= {9798400705694},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643666.3648580},
  doi		= {10.1145/3643666.3648580},
  abstract	= {Accessibility is the practice of making content and
		  functionality accessible to all users, regardless of their
		  abilities. Although accessibility is a highly relevant
		  quality attribute, it is often treated as an afterthought
		  in software development, unfortunately excluding people
		  with disabilities from using many web-based systems.
		  Specifically in agile development, sprints focus on new
		  features and quality attributes, such as accessibility, are
		  often not considered sufficiently. In these cases, using
		  conceptual models to understand and analyze requirements
		  that developers have formulated as a set of related user
		  stories is a research opportunity. To increase agile
		  professionals' focus on accessibility, we built a
		  conceptual model for web accessibility, identifying
		  artifacts and concepts used in agile development to specify
		  accessibility. We discuss how this model can be used as a
		  guide to better integrate accessibility considerations into
		  agile software development. Researchers can use the result
		  to define resources that are not currently covered or
		  improve underutilized practices. We plan to use the
		  conceptual model in the next steps to adapt existing agile
		  artifacts and create support tools for web accessibility in
		  agile development.},
  booktitle	= {Proceedings of the 1st IEEE/ACM Workshop on
		  Multi-Disciplinary, Open, and RElevant Requirements
		  Engineering},
  pages		= {15–21},
  numpages	= {7},
  keywords	= {accessibility requirements, agile development, conceptual
		  model, requirements engineering},
  location	= {Lisbon, Portugal},
  series	= {MO2RE 2024}
}

@InProceedings{	  10.1145/3417990.3421414,
  author	= {Partridge, Chris and Mitchell, Andrew and da Silva, Marco
		  and Soto, Oscar Xiberta and West, Matthew and Khan, Mesbah
		  and de Cesare, Sergio},
  title		= {Implicit requirements for ontological multi-level types in
		  the UNICLASS classification},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3421414},
  doi		= {10.1145/3417990.3421414},
  abstract	= {In the multi-level type modeling community, claims that
		  most enterprise application systems use ontologically
		  multi-level types are ubiquitous. To be able to empirically
		  verify this claim one needs to be able to expose the (often
		  underlying) ontological structure and show that it does,
		  indeed, make a commitment to multi-level types. We have not
		  been able to find any published data showing this being
		  done. From a top-level ontology requirements perspective,
		  checking this multi-level type claim is worthwhile. If the
		  datasets for which the top-level ontology is required are
		  ontologically committed to multi-level types, then this is
		  a requirement for the top-level ontology. In this paper, we
		  both present some empirical evidence that this ubiquitous
		  claim is correct as well as describing the process we used
		  to expose the underlying ontological commitments and
		  examine them. We describe how we use the bCLEARer process
		  to analyse the UNICLASS classifications making their
		  implicit ontological commitments explicit. We show how this
		  reveals the requirements for two general ontological
		  commitments; higher-order types and first-class relations.
		  This establishes a requirement for a top-level ontology
		  that includes the UNICLASS classification to be able to
		  accommodate these requirements. From a multi-level type
		  perspective, we have established that the bCLEARer
		  entification process can identify underlying ontological
		  commitments to multi-level type that do not exist in the
		  surface linguistic structure. So, we have a process that we
		  can reuse on other datasets and application systems to help
		  empirically verify the claim that ontological multi-level
		  types are ubiquitous.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {87},
  numpages	= {8},
  keywords	= {UNICLASS, bCLEARer approach, first class relations, higher
		  order types, top-level ontology},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@Proceedings{	  10.1145/3686812,
  title		= {ICCMS '24: Proceedings of the 2024 16th International
		  Conference on Computer Modeling and Simulation},
  year		= {2024},
  isbn		= {9798400717215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dalian, China}
}

@InProceedings{	  10.1145/3524481.3527229,
  author	= {Liu, Yu and Yandrapally, Rahulkrishna and Kalia, Anup K.
		  and Sinha, Saurabh and Tzoref-Brill, Rachel and Mesbah,
		  Ali},
  title		= {CrawLabel: computing natural-language labels for UI test
		  cases},
  year		= {2022},
  isbn		= {9781450392860},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3524481.3527229},
  doi		= {10.1145/3524481.3527229},
  abstract	= {End-to-end test cases that exercise the application under
		  test via its user interface (UI) are known to be hard for
		  developers to read and understand; consequently, diagnosing
		  failures in these tests and maintaining them can be
		  tedious. Techniques for computing natural-language
		  descriptions of test cases can help increase test
		  readability. However, so far, such techniques have been
		  developed for unit test cases; they are not applicable to
		  end-to-end test cases.In this paper, we focus on the
		  problem of computing natural-language labels for the steps
		  of end-to-end UI test cases for web applications. We
		  present two techniques that apply natural-language
		  processing to information available in the browser document
		  object model (DOM). The first technique is an instance of a
		  supervised approach in which labeling-relevant DOM
		  attributes are ranked via manual analysis and fed into
		  label computation. However, supervised approach requires a
		  training dataset. So we propose the second technique, which
		  is unsupervised: it leverages probabilistic context-free
		  grammar learning to compute dominant DOM attributes
		  automatically. We implemented these techniques, along with
		  two simpler baseline techniques, in a tool called CrawLabel
		  (available as a plugin to Crawljax, a state-of-the-art UI
		  test-generation tool for web applications) and evaluated
		  their effectiveness on open-source web applications. Our
		  results indicate that the supervised approach can achieve
		  precision, recall, and Fl-score of 83.38, 60.64, and 66.40,
		  respectively. The unsupervised approach, although less
		  effective, is competitive, achieving scores of 72.37,
		  58.12, and 59.77. We highlight key results and discuss the
		  implications of our findings.},
  booktitle	= {Proceedings of the 3rd ACM/IEEE International Conference
		  on Automation of Software Test},
  pages		= {103–114},
  numpages	= {12},
  location	= {Pittsburgh, Pennsylvania},
  series	= {AST '22}
}

@InProceedings{	  10.1145/3583780.3615036,
  author	= {Dong, Hang and Chen, Jiaoyan and He, Yuan and Liu, Yinan
		  and Horrocks, Ian},
  title		= {Reveal the Unknown: Out-of-Knowledge-Base Mention
		  Discovery with Entity Linking},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615036},
  doi		= {10.1145/3583780.3615036},
  abstract	= {Discovering entity mentions that are out of a Knowledge
		  Base (KB) from texts plays a critical role in KB
		  maintenance, but has not yet been fully explored. The
		  current methods are mostly limited to the simple
		  threshold-based approach and feature-based classification,
		  and the datasets for evaluation are relatively rare. We
		  propose BLINKout, a new BERT-based Entity Linking (EL)
		  method which can identify mentions that do not have
		  corresponding KB entities by matching them to a special NIL
		  entity. To better utilize BERT, we propose new techniques
		  including NIL entity representation and classification,
		  with synonym enhancement. We also apply KB Pruning and
		  Versioning strategies to automatically construct out-of-KB
		  datasets from common in-KB EL datasets. Results on five
		  datasets of clinical notes, biomedical publications, and
		  Wikipedia articles in various domains show the advantages
		  of BLINKout over existing methods to identify out-of-KB
		  mentions for the medical ontologies, UMLS, SNOMED CT, and
		  the general KB, WikiData.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {452–462},
  numpages	= {11},
  keywords	= {WikiData, biomedical ontologies, entity linking, knowledge
		  base enrichment, language models},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3502223.3502235,
  author	= {Zhu, Rui and Shimizu, Cogan and Stephen, Shirly and Zhou,
		  Lu and Cai, Ling and Mai, Gengchen and Janowicz, Krzysztof
		  and Schildhauer, Mark and Hitzler, Pascal},
  title		= {SOSA-SHACL: Shapes Constraint for the Sensor, Observation,
		  Sample, and Actuator Ontology},
  year		= {2022},
  isbn		= {9781450395656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3502223.3502235},
  doi		= {10.1145/3502223.3502235},
  abstract	= {The explosive growth of the Linked Data on the Web has
		  greatly facilitated collecting data from remote sensors,
		  from air quality sensors spread out across a city, to
		  seismograph stations spread across the entire world.
		  Integrating these heterogeneous data can be quite
		  challenging; however one can achieve this through the use
		  of available W3C standards to create a knowledge graph. For
		  this use case, the W3C also provides a standard, the
		  Sensor, Observation, Sample, Actuator (SOSA) Ontology, that
		  allows for the semantic encoding of sensors and their
		  observations. However, even with the guidance of this
		  standard, it may be difficult to produce a correct graph
		  with high fidelity from heterogeneous sources. In this
		  paper we present a set of (data) shape constraints, called
		  SOSA-SHACL, for the SOSA ontology using a data validation
		  language, namely the W3C standard SHACL (Shape Constraint
		  Language). These constraints enable us to evaluate whether
		  the modeled observations in our Knowledge Graph comply with
		  the SOSA recommendations. Furthermore, we show through
		  several case studies how the closed world assumption plays
		  a role in the process of designing such shape constraints,
		  especially as SOSA is based on the open world assumption.},
  booktitle	= {Proceedings of the 10th International Joint Conference on
		  Knowledge Graphs},
  pages		= {99–107},
  numpages	= {9},
  keywords	= {RDF validation, knowledge graph quality assessment and
		  refinement, sensors and observations},
  location	= {Virtual Event, Thailand},
  series	= {IJCKG '21}
}

@InProceedings{	  10.1145/3628034.3628037,
  author	= {Amiri, Amirali and Ntentos, Evangelos and Zdun, Uwe and
		  Geiger, Sebastian},
  title		= {Tool Support for Learning Architectural Guidance Models
		  from Architectural Design Decision Models},
  year		= {2024},
  isbn		= {9798400700408},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3628034.3628037},
  doi		= {10.1145/3628034.3628037},
  abstract	= {This paper presents an approach to architectural knowledge
		  management that does not assume existing architectural
		  design decisions or pattern applications are documented as
		  architectural knowledge, but benefits from more existing
		  data. We drew inspiration from manual qualitative research
		  methods for mining patterns and architectural knowledge and
		  created a guideline model of which the ADD models are
		  instances. We evaluated our approach on 11 cases from the
		  gray literature. We found that it can provide suitable
		  recommendations after modeling only a single case and
		  reaches theoretical saturation and recommendations with low
		  to very low errors after only 6-8 cases. Our approach shows
		  that creating a reusable architectural design space is
		  possible based only on limited case data. Our approach not
		  only provides a novel approach to architectural knowledge
		  management but can also be used as a tool for pattern
		  mining.},
  booktitle	= {Proceedings of the 28th European Conference on Pattern
		  Languages of Programs},
  articleno	= {3},
  numpages	= {14},
  keywords	= {Architectural Design Decisions, Architectural Knowledge
		  Management, Design Patterns},
  location	= {Irsee, Germany},
  series	= {EuroPLoP '23}
}

@InProceedings{	  10.1145/3549037.3561272,
  author	= {Stang, J\o{}rgen and Walther, Dirk and Myrseth, Per},
  title		= {Data quality as a microservice: an ontology and rule based
		  approach for quality assurance of sensor data in
		  manufacturing machines},
  year		= {2022},
  isbn		= {9781450394598},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3549037.3561272},
  doi		= {10.1145/3549037.3561272},
  abstract	= {The manufacturing industry is continuously looking for
		  production improvements resulting in high quality
		  production, reduced waste and competitive advantages. In
		  this article, ontologies, semantic rule logic and
		  microservices have been deployed to suggest a system for
		  quality assurance of manufacturing machine data. The
		  existing upper ontology for manufacturing service
		  description has been used to define both the physical
		  assets as well as the data quality requirements. The system
		  is used to both operationalize data quality monitoring by
		  semantic technology as well as enabling up-front modelling
		  of data quality requirements. The approach is illustrated
		  by a specific speed-feed case for manufacturing machines
		  but could easily be extended to other manufacturing
		  use-cases or even to other industries.},
  booktitle	= {Proceedings of the 2nd International Workshop on Software
		  Engineering and AI for Data Quality in Cyber-Physical
		  Systems/Internet of Things},
  pages		= {3–9},
  numpages	= {7},
  keywords	= {Data Quality, IoT, Manufacturing Machines, Microservices,
		  Ontolologies, Sensor Data},
  location	= {Singapore, Singapore},
  series	= {SEA4DQ 2022}
}

@Article{	  10.1145/3458027,
  author	= {Bromander, Siri and Swimmer, Morton and Muller, Lilly
		  Pijnenburg and J\o{}sang, Audun and Eian, Martin and
		  Skj\o{}tskift, Geir and Borg, Fredrik},
  title		= {Investigating Sharing of Cyber Threat Intelligence and
		  Proposing A New Data Model for Enabling Automation in
		  Knowledge Representation and Exchange},
  year		= {2021},
  issue_date	= {March 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {1},
  url		= {https://doi.org/10.1145/3458027},
  doi		= {10.1145/3458027},
  abstract	= {For a strong, collective defense in the digital domain, we
		  need to produce, consume, analyze, and share cyber threat
		  intelligence. With an increasing amount of available
		  information, we need automation to ensure adequate
		  efficiency. We present the results from a questionnaire
		  investigating the use of standards and standardization and
		  how practitioners share and use cyber threat intelligence
		  (CTI). We propose a strict data model for CTI that enables
		  consumption of all relevant data, data validation, and
		  analysis of consumed content. The main contribution of this
		  article is insight into how CTI is shared and used by
		  practitioners, and the strictness of the data model that
		  enforces input of information and enables automation and
		  deduction of new knowledge.},
  journal	= {Digital Threats},
  month		= oct,
  articleno	= {6},
  numpages	= {22},
  keywords	= {Cyber threat intelligence, security, knowledge graph,
		  ontology}
}

@InProceedings{	  10.1145/3698205.3733934,
  author	= {Chen, Shijun (Cindy)},
  title		= {Conceptualizing Online Feedback Engagement from a
		  Sociomaterial Perspective: An Iceberg Model},
  year		= {2025},
  isbn		= {9798400712913},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698205.3733934},
  doi		= {10.1145/3698205.3733934},
  abstract	= {This study conceptualizes online feedback engagement
		  through a sociomaterial lens, exploring how learners'
		  feedback engagement is dynamically shaped by the
		  entanglement of human and non-human actors in digital
		  environments. While prior research has examined cognitive,
		  behavioral, and affective dimensions of feedback engagement
		  in face-to-face learning environments, few studies have
		  explored how technological affordances and sociocultural
		  values mediate these forms of engagement. Drawing on a
		  sociomaterial perspective, this study proposes a
		  multidimensional framework of feedback engagement
		  comprising cognitive, behavioral, relational, and
		  collaborative dimensions. By synthesizing existing
		  literature and integrating insights from recent empirical
		  studies involving digital feedback tools, the paper
		  highlights how engagement is not solely a learner-driven
		  phenomenon but is co-constructed through sociomaterial
		  arrangements. The framework advances current understandings
		  of online feedback engagement and offers implications for
		  the design of pedagogically sound feedback practices in
		  technology-mediated learning contexts.},
  booktitle	= {Proceedings of the Twelfth ACM Conference on Learning @
		  Scale},
  pages		= {251–255},
  numpages	= {5},
  keywords	= {feedback, feedback engagement, online learning,
		  sociomaterialism},
  location	= {Palermo, Italy},
  series	= {L@S '25}
}

@InProceedings{	  10.1145/3636243.3636256,
  author	= {Doughty, Jacob and Wan, Zipiao and Bompelli, Anishka and
		  Qayum, Jubahed and Wang, Taozhi and Zhang, Juran and Zheng,
		  Yujia and Doyle, Aidan and Sridhar, Pragnya and Agarwal,
		  Arav and Bogart, Christopher and Keylor, Eric and Kultur,
		  Can and Savelka, Jaromir and Sakr, Majd},
  title		= {A Comparative Study of AI-Generated (GPT-4) and
		  Human-crafted MCQs in Programming Education},
  year		= {2024},
  isbn		= {9798400716195},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3636243.3636256},
  doi		= {10.1145/3636243.3636256},
  abstract	= {There is a constant need for educators to develop and
		  maintain effective up-to-date assessments. While there is a
		  growing body of research in computing education on
		  utilizing large language models&nbsp;(LLMs) in generation
		  and engagement with coding exercises, the use of LLMs for
		  generating programming MCQs has not been extensively
		  explored. We analyzed the capability of GPT-4 to produce
		  multiple-choice questions (MCQs) aligned with specific
		  learning objectives (LOs) from Python programming classes
		  in higher education. Specifically, we developed an
		  LLM-powered (GPT-4) system for generation of MCQs from
		  high-level course context and module-level LOs. We
		  evaluated 651 LLM-generated and 449 human-crafted MCQs
		  aligned to 246 LOs from 6 Python courses. We found that
		  GPT-4 was capable of producing MCQs with clear language, a
		  single correct choice, and high-quality distractors. We
		  also observed that the generated MCQs appeared to be
		  well-aligned with the LOs. Our findings can be leveraged by
		  educators wishing to take advantage of the state-of-the-art
		  generative models to support MCQ authoring efforts.},
  booktitle	= {Proceedings of the 26th Australasian Computing Education
		  Conference},
  pages		= {114–123},
  numpages	= {10},
  keywords	= {Assessments, Automated Content Generation, Automatic
		  Generation, GPT-4, LLMs, LOs, Large Language Models,
		  Learning Objectives, MCQs, Multiple-choice Questions},
  location	= {Sydney, NSW, Australia},
  series	= {ACE '24}
}

@InProceedings{	  10.1145/3655532.3655555,
  author	= {Liu, Caihong and Liu, Changhui},
  title		= {Sentiment Analysis of Movie Reviews Based on Sentiment
		  Dictionary and Deep Learning Models},
  year		= {2024},
  isbn		= {9798400708039},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3655532.3655555},
  doi		= {10.1145/3655532.3655555},
  abstract	= {As the Internet era has progressed, platforms such as
		  Douban Movies have spawned a great number of evaluations
		  with personal biases. However, these assessments lack a set
		  length, and the text's expression is varied, not
		  constrained to grammar-related constraints. The expressive
		  style is less formal. As a result, mining and assessing
		  these opinions has substantial economic worth. This
		  experiment utilized a novel sentiment lexicon to adapt
		  informal vocabulary in movie reviews. To improve the
		  accuracy of sentiment analysis in movie reviews, it was
		  integrated with the Albert-BiLSTM-Attention model. The
		  results of six rounds of comparative experiments show that
		  the method suggested in this paper has improved average
		  precision, average recall, and average F1 score in the
		  sentiment classification of this dataset. The suggested
		  model can be used to achieve precise sentiment analysis for
		  film reviews, offering pertinent support and advice for the
		  production team's upcoming films.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Robot Systems and Applications},
  pages		= {144–148},
  numpages	= {5},
  keywords	= {Albert-BiLSTM, Attention mechanisms, deep learning,
		  sentiment lexicon},
  location	= {Wuhan, China},
  series	= {ICRSA '23}
}

@InProceedings{	  10.1145/3511047.3537690,
  author	= {Gena, Cristina and Damiano, Rossana and Mattutino, Claudio
		  and Mazzei, Alessandro and Brighenti, Stefania and
		  Nazzario, Matteo and Meirone, Andrea and Quarato, Camilla
		  and Miraglio, Elisabetta and Ricciardiello, Giulia and
		  Petriglia, Francesco and Liscio, Federica and Piccinni,
		  Giuseppe and Mazzotta, Loredana and Pecone, Cesare and
		  Ricci, Valeria},
  title		= {Ontologies and Open Data for Enriching Personalized Social
		  Moments in Human Robot Interaction},
  year		= {2022},
  isbn		= {9781450392327},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511047.3537690},
  doi		= {10.1145/3511047.3537690},
  abstract	= {This paper describes our proposal for enriching
		  personalized social moments and dialogues between human and
		  robot in the context of the Sugar, Salt \&amp; Pepper
		  laboratory. The lab focused on the use of the Pepper robot
		  in a therapeutic context to promote autonomies and
		  functional acquisitions in highly functioning (Asperger)
		  children with autism. This paper is focused on a post-hoc
		  work aimed at improving the robot's autonomous dialogue
		  strategies. In particular we are integrating the robot's
		  dialogue with a knowledge base to have the robot able to
		  move and reason on an ontology, and thus enriching its
		  dialogue's strategies. For instance, the taxonomic
		  structure of the ontology could allow Pepper to drive the
		  focus of the conversation to related topics or to more
		  general or specific topics, and, in general, it could
		  improve its capability to manage the conversation and
		  disambiguate the input from the user.},
  booktitle	= {Adjunct Proceedings of the 30th ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {151–154},
  numpages	= {4},
  keywords	= {Adaptivity, HRI, Human Behavior Understanding, Social
		  Robots},
  location	= {Barcelona, Spain},
  series	= {UMAP '22 Adjunct}
}

@InProceedings{	  10.1145/3404709.3404771,
  author	= {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
  title		= {Cooperative Domain Ontology Reduction Based on Power
		  Sets},
  year		= {2020},
  isbn		= {9781450375337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404709.3404771},
  doi		= {10.1145/3404709.3404771},
  abstract	= {Ontology is widely used in the areas of knowledge
		  engineering, web-based data mining, and others. The process
		  of developing and evolving inter-organizational domain
		  ontologies is easy to get much redundant information.
		  PowerSets can be used to reduce the attributes of
		  ontologies. In this paper, "Rule Finding Uniqueness," RFU
		  is proposed for learning a set of rules in order to refine
		  an ontology. The algorithm's primary goal is to generate
		  unique rules that not only cover the initial set but also
		  enhance reasoning. The claimed technique compresses
		  Ontologies after it is already built or during the evolving
		  process of the inter-organizational cooperative domain
		  ontology. The proposed method can also be used to
		  strengthen automatic and semi-automatic operations to
		  develop and evolve ontologies. We can consider this
		  approach as a maintenance operation that could be done
		  periodically based on the ontology evolution frequency
		  rate.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Frontiers of Educational Technologies},
  pages		= {196–203},
  numpages	= {8},
  keywords	= {Attributes, Inter-organizational domain ontology, Ontology
		  Reductio, Power Sets},
  location	= {Tokyo, Japan},
  series	= {ICFET '20}
}

@InProceedings{	  10.1145/3428757.3429110,
  author	= {Khiat, Abderrahmane and Elias, Mirette and Foldenauer, Ann
		  Christina and Koehm, Michaela and Blumenstein, Irina and
		  Napolitano, Giulio},
  title		= {Towards an Ontology Representing Characteristics of
		  Inflammatory Bowel Disease},
  year		= {2021},
  isbn		= {9781450389228},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3428757.3429110},
  doi		= {10.1145/3428757.3429110},
  abstract	= {Inflammatory bowel disease (IBD) is a chronic disease
		  characterized by numerous, hard to predict periods of
		  relapse and remission. "Digital twin" approaches,
		  leveraging personalized predictive models, would
		  significantly enhance therapeutic decision-making and
		  cost-effectiveness. However, the associated computational
		  and statistical methods require high quality data from a
		  large population of patients. Such a comprehensive
		  repository is very challenging to build, though, and none
		  is available for IBD. To overcome this, a promising
		  approach is to employ a knowledge graph, which is built
		  from the available data and would help predicting IBD
		  episodes and delivering more relevant personalized therapy
		  at the lowest cost. In this research, we present a
		  knowledge graph developed on the basis of patient records
		  which are collected from one of the largest German
		  gastroentologic outpatient clinic. First, we designed IBD
		  ontology that encompasses the vocabulary, specifications
		  and characteristics associated by physicians with IBD
		  patients, such as disease classification schemas (e.g.,
		  Montreal Classification of IBD), status of the disease
		  activity, and medications. Next, we defined the mappings
		  between ontology entities and database variables.
		  Physicians and project members participating in the
		  Fraunhofer MED2ICIN project, validated the ontology and the
		  knowledge graph. Furthermore, the knowledge graph has been
		  validated against the competency questions compiled by
		  physicians.},
  booktitle	= {Proceedings of the 22nd International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {216–222},
  numpages	= {7},
  keywords	= {IBD, Mappings, Ontology, Ontop, VoCoReg, knowledge graph},
  location	= {Chiang Mai, Thailand},
  series	= {iiWAS '20}
}

@InProceedings{	  10.1145/3466933.3466953,
  author	= {Ribeiro, Elivaldo Lozer Fracalossi and Souza, Marlo and
		  Claro, Daniela Barreiro},
  title		= {MIDAS-OWL: An Ontology for Interoperability between Data
		  and Service Cloud Layers},
  year		= {2021},
  isbn		= {9781450384919},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3466933.3466953},
  doi		= {10.1145/3466933.3466953},
  abstract	= {As different cloud computing services have emerged over
		  the years, the diversity of technologies and the lack of
		  standardization has given rise to an interoperability
		  problem in cloud computing. Cloud computing services
		  include those such as Software as a Service (SaaS),
		  Platform as a Service (PaaS), Infrastructure as a Service
		  (IaaS), and Data as a Service (DaaS). In this context,
		  interoperability enables a service to communicate with
		  another service transparently. Among the solutions proposed
		  in the literature, a middleware can be used to intermediate
		  such communication and to mitigate the lack of
		  interoperability in cloud computing. For instance, the
		  middleware MIDAS (Middleware for DaaS and SaaS) provides
		  transparent interoperability between SaaS and DaaS.
		  Although MIDAS current version promotes syntactic
		  interoperability, semantic interoperability is only
		  superficially addressed. In collaboration with this
		  project, we develop an OWL-based ontology to formally
		  represent the communication between SaaS and DaaS, and
		  discuss its strengths in providing semantic
		  interoperability on MIDAS. We conduct a set of experiments
		  to validate our ontology. We evaluate intrinsic
		  (consistency, correctness, acceptance) and extrinsic
		  (integration between ontology and MIDAS) issues. Results
		  provide evidence that a semantic MIDAS interoperability can
		  be enhanced by our ontology.},
  booktitle	= {Proceedings of the XVII Brazilian Symposium on Information
		  Systems},
  articleno	= {20},
  numpages	= {8},
  keywords	= {cloud computing, cloud services, ontology, semantic
		  interoperability},
  location	= {Uberl\^{a}ndia, Brazil},
  series	= {SBSI '21}
}

@InProceedings{	  10.1145/3665689.3665768,
  author	= {Deng, Qiwen and Han, Yuexia and Sun, Jianfei},
  title		= {A Joint Framework for Predicting Disease-Gene Interactions
		  Based on Pre-trained Models and Graph Attention Networks},
  year		= {2024},
  isbn		= {9798400716645},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665689.3665768},
  doi		= {10.1145/3665689.3665768},
  abstract	= {The study of disease-gene interactions is crucial in
		  biomedical research. Identifying genes associated with
		  diseases can provide critical insights into disease
		  mechanisms, facilitate early diagnosis, and contribute to
		  the development of targeted therapies. In this paper, we
		  propose a novel framework for predicting disease-gene
		  interactions called the PRGAT-DG, which utilizes
		  pre-trained language models and graph attention networks to
		  extract semantic and graph structure features respectively.
		  Moreover, we introduce residual structure to alleviate the
		  problem of excessive smoothing. Experimental results on a
		  dataset released by Stanford University demonstrate the
		  remarkable predictive accuracy of our framework, showcasing
		  its superiority compared to other existing methods. This
		  research holds significant implications for advancing our
		  understanding of disease-gene interaction mechanisms and
		  accelerating the development of relevant therapeutics.},
  booktitle	= {Proceedings of the 2024 4th International Conference on
		  Bioinformatics and Intelligent Computing},
  pages		= {474–478},
  numpages	= {5},
  location	= {Beijing, China},
  series	= {BIC '24}
}

@InProceedings{	  10.1145/3550356.3561604,
  author	= {Biglari, Raheleh and Denil, Joachim},
  title		= {Model validity and tolerance quantification for real-time
		  adaptive approximation},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561604},
  doi		= {10.1145/3550356.3561604},
  abstract	= {Designing a Cyber-physical system (CPS) including modeling
		  the control components and services is a challenging issue.
		  Models and simulations at run-time play a crucial role to
		  implement these control and prediction components.Real-time
		  constraints raise the complexity of designing an efficient
		  CPS system. Having detailed models in making decisions
		  and/or numerous predictions in different contexts is
		  computationally expensive and difficult to schedule on the
		  computational infrastructure.Inspired by substitutability,
		  one strategy for dealing with complex CPS and the
		  contradiction of better real-time performance and reduced
		  cost in CPS is to employ approximated models and switch to
		  the most suited model adaptively at run-time.However, using
		  an approximate model raises the uncertainty on the model's
		  predictions. Nonetheless, the model is appropriate when the
		  uncertainty is within bound. This bound is defined as
		  tolerance which is the permitted amount of uncertainty.In
		  this paper, we propose a method for quantifying the
		  tolerance of cyber-physical systems, where we can switch
		  between the original model and approximated models and how
		  to identify more appropriate models.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {668–676},
  numpages	= {9},
  keywords	= {adaptation, approximation, cyber-physical systems, model
		  validity, real-time systems, tolerance quantification,
		  uncertainty},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3701716.3715532,
  author	= {Mu\~{n}oz, Carlos and Mendoza, Marcelo and Lobel, Hans and
		  Keith, Brian},
  title		= {Imitating Human Reasoning to Extract 5W1H in News},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715532},
  doi		= {10.1145/3701716.3715532},
  abstract	= {Extracting key information from news articles is crucial
		  for advancing search systems. Historically, the 5W1H
		  framework, which organises information based on 'Who',
		  'What', 'When', 'Where', 'Why', and 'How', has been a
		  predominant method in digital journalism empowering search
		  tools. The rise of Large Language Models (LLMs) has sparked
		  new research into their potential for performing such
		  information extraction tasks effectively. Our study
		  examines a novel approach to employing LLMs in the 5W1H
		  extraction process, particularly focusing on their capacity
		  to mimic human reasoning. We introduce two innovative
		  Chain-of-Thought (COT) prompting techniques to extract 5W1H
		  in news: extractive reasoning and question-level reasoning.
		  The former directs the LLM to pinpoint and highlight
		  essential details from texts, while the latter encourages
		  the model to emulate human-like reasoning at the
		  question-response level. Our research methodology includes
		  experiments with leading LLMs using prompting strategies to
		  ascertain the most effective approach. The results indicate
		  that COT prompting significantly outperforms other methods.
		  In addition, we show that the effectiveness of LLMs in such
		  tasks depends greatly on the nature of the questions
		  posed.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {1199–1203},
  numpages	= {5},
  keywords	= {5w1h, imitative reasoning, llm, news},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3550356.3561554,
  author	= {Diaconescu, Ada and Houze, Etienne and Dessalles,
		  Jean-Louis and Vangheluwe, Hans and Franceschini, Romain},
  title		= {Multi-scale model-based explanations for cyber-physical
		  systems: the urban traffic case},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561554},
  doi		= {10.1145/3550356.3561554},
  abstract	= {Automated control in Cyber-Physical Systems (CPS)
		  generates behaviours that may surprise non-expert users.
		  Relevant explanations are required to maintain user trust.
		  Large CPS (e.g., autonomous car networks and smart grids)
		  raise additional scaleability issues for the explanatory
		  processes and complexity issues for generated explanations.
		  We propose a multi-scale system modelling and explanation
		  technique to address these concerns. The idea is to
		  increase the scale, or abstraction level, of the modelled
		  CPS, whenever possible without loss of salient information,
		  so as to produce smaller system representations and hence
		  to reduce the complexity of the explanatory process and of
		  the generated explanations. We illustrate our proposal via
		  an urban traffic case study, modelling traffic at two
		  different scales (i.e., modelling individual cars at a
		  lower-scale; and traffic jams at a higher-scale). We show
		  how a multi-scale explanatory process can use the lower-
		  and higher-scale models to generate either longer (more
		  detailed) explanations, or shorter (more abstract)
		  explanations, respectively. This proof-of-concept
		  illustration offers a basis for further research towards a
		  comprehensive multi-scale explanatory solution for CPS.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {684–691},
  numpages	= {8},
  keywords	= {cyber-physical system, multi-scale model and explanation,
		  traffic simulation},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3655693.3655722,
  author	= {Chetwyn, Robert Andrew and Eian, Martin and J\o{}sang,
		  Audun},
  title		= {Modelling Indicators of Behaviour for Cyber Threat Hunting
		  via Sysmon},
  year		= {2024},
  isbn		= {9798400716515},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3655693.3655722},
  doi		= {10.1145/3655693.3655722},
  abstract	= {Hunting for threats is of capital importance for security
		  teams. Establishing multifaceted contexts around the
		  evolving behaviours of threat actors is paramount for
		  enabling threat hunting teams to tell the malicious from
		  the benign. The MITRE ATT&amp;CK framework is the
		  state-of-art knowledge base for referencing how threat
		  actors conduct their tactics, techniques and procedures.
		  Despite the abstract concepts of techniques being well
		  defined, it is challenging to hunt from an abstract
		  technique concept to security event data. In this work, we
		  develop a data driven knowledge base of threat actor
		  behaviours called Indicators of Behaviour, that use
		  semantic reasoning to infer threat actor behaviours. Unlike
		  generalised techniques in MITRE ATT&amp;CK, these
		  behaviours can be queried from a low level indicator and
		  the behaviour itself. We use MITRE’s Caldera platform to
		  emulate threat actor behaviours and Sysmon for capturing
		  security events and defining the knowledge base’s
		  semantics. By utilising this approach, the semantic
		  reasoner aids threat hunting teams by inferring threat
		  actor behaviour chains from individual interconnected
		  events.},
  booktitle	= {Proceedings of the 2024 European Interdisciplinary
		  Cybersecurity Conference},
  pages		= {95–104},
  numpages	= {10},
  keywords	= {Caldera, MITRE ATT&amp;CK, TTP, Threat Actor Behaviour,
		  Threat Hunting},
  location	= {Xanthi, Greece},
  series	= {EICC '24}
}

@InProceedings{	  10.1145/3411764.3445396,
  author	= {Zhang, Xiaoyu and Chandrasegaran, Senthil and Ma,
		  Kwan-Liu},
  title		= {ConceptScope: Organizing and Visualizing Knowledge in
		  Documents based on Domain Ontology},
  year		= {2021},
  isbn		= {9781450380966},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411764.3445396},
  doi		= {10.1145/3411764.3445396},
  abstract	= {Current text visualization techniques typically provide
		  overviews of document content and structure using intrinsic
		  properties such as term frequencies, co-occurrences, and
		  sentence structures. Such visualizations lack conceptual
		  overviews incorporating domain-relevant knowledge, needed
		  when examining documents such as research articles or
		  technical reports. To address this shortcoming, we present
		  ConceptScope, a technique that utilizes a domain ontology
		  to represent the conceptual relationships in a document in
		  the form of a Bubble Treemap visualization. Multiple
		  coordinated views of document structure and concept
		  hierarchy with text overviews further aid document
		  analysis. ConceptScope facilitates exploration and
		  comparison of single and multiple documents respectively.
		  We demonstrate ConceptScope by visualizing research
		  articles and transcripts of technical presentations in
		  computer science. In a comparative study with DocuBurst, a
		  popular document visualization tool, ConceptScope was found
		  to be more informative in exploring and comparing
		  domain-specific documents, but less so when it came to
		  documents that spanned multiple disciplines.},
  booktitle	= {Proceedings of the 2021 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {19},
  numpages	= {13},
  keywords	= {Knowledge Representation, Ontology, Visualization},
  location	= {Yokohama, Japan},
  series	= {CHI '21}
}

@InProceedings{	  10.1145/3347146.3359069,
  author	= {Li, Jingjing and Wang, Wenlu and Ku, Wei-Shinn and Tian,
		  Yingtao and Wang, Haixun},
  title		= {SpatialNLI: A Spatial Domain Natural Language Interface to
		  Databases Using Spatial Comprehension},
  year		= {2019},
  isbn		= {9781450369091},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3347146.3359069},
  doi		= {10.1145/3347146.3359069},
  abstract	= {A natural language interface (NLI) to databases is an
		  interface that translates a natural language question to a
		  structured query that is executable by database management
		  systems (DBMS). However, an NLI that is trained in the
		  general domain is hard to apply in the spatial domain due
		  to the idiosyncrasy and expressiveness of the spatial
		  questions. Inspired by the machine comprehension model, we
		  propose a spatial comprehension model that is able to
		  recognize the meaning of spatial entities based on the
		  semantics of the context. The spatial semantics learned
		  from the spatial comprehension model is then injected to
		  the natural language question to ease the burden of
		  capturing the spatial-specific semantics. With our spatial
		  comprehension model and information injection, our NLI for
		  the spatial domain, named SpatialNLI, is able to capture
		  the semantic structure of the question and translate it to
		  the corresponding syntax of an executable query accurately.
		  We also experimentally ascertain that SpatialNLI
		  outperforms state-of-the-art methods.},
  booktitle	= {Proceedings of the 27th ACM SIGSPATIAL International
		  Conference on Advances in Geographic Information Systems},
  pages		= {339–348},
  numpages	= {10},
  keywords	= {Natural Language Interface, Spatial Data Science},
  location	= {Chicago, IL, USA},
  series	= {SIGSPATIAL '19}
}

@Article{	  10.1145/3742891,
  author	= {Saleh, Alaa and Morabito, Roberto and Dustdar, Schahram
		  and Tarkoma, Sasu and Pirttikangas, Susanna and Lov\'{e}n,
		  Lauri},
  title		= {Towards Message Brokers for Generative AI: Survey,
		  Challenges, and Opportunities},
  year		= {2025},
  issue_date	= {January 2026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {58},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3742891},
  doi		= {10.1145/3742891},
  abstract	= {In today’s digital world, GenAI is becoming increasingly
		  prevalent by enabling unparalleled content generation
		  capabilities for a wide range of advanced applications.
		  This surge in adoption has sparked a significant increase
		  in demand for data-centric GenAI models spanning the
		  distributed edge-cloud continuum, placing increasing
		  demands on communication infrastructures, highlighting the
		  necessity for robust communication solutions. Central to
		  this need are message brokers, which serve as essential
		  channels for data transfer within various system
		  components. This survey aims at delving into a
		  comprehensive analysis of traditional and modern message
		  brokers based on a variety of criteria, highlighting their
		  critical role in enabling efficient data exchange in
		  distributed AI systems. Furthermore, we explore the
		  intrinsic constraints that the design and operation of each
		  message broker might impose, highlighting their impact on
		  real-world applicability. Finally, this study explores the
		  enhancement of message broker mechanisms tailored to GenAI
		  environments. It considers key factors such as scalability,
		  semantic communication, and distributed inference that can
		  guide future innovations and infrastructure advancements in
		  the realm of GenAI data communication.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {20},
  numpages	= {37},
  keywords	= {Generative AI, message brokers, publish/subscribe
		  paradigm, brokerless, edge computing, large language
		  models}
}

@InProceedings{	  10.1145/3589335.3651500,
  author	= {Guan, Yong and Liu, Dingxiao and Ma, Jinchen and Peng, Hao
		  and Wang, Xiaozhi and Hou, Lei and Li, Ru},
  title		= {Event GDR: Event-Centric Generative Document Retrieval},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651500},
  doi		= {10.1145/3589335.3651500},
  abstract	= {Generative document retrieval, an emerging paradigm in
		  information retrieval, learns to build connections between
		  documents and identifiers within a single model, garnering
		  significant attention. However, there are still two
		  challenges: (1) neglecting inner-content correlation during
		  document representation; (2) lacking explicit semantic
		  structure during identifier construction. Nonetheless,
		  events have enriched relations and well-defined taxonomy,
		  which could facilitate addressing the above two challenges.
		  Inspired by this, we propose Event GDR, an event-centric
		  generative document retrieval model, integrating event
		  knowledge into this task. Specifically, we utilize an
		  exchange-then-reflection method based on multi-agents for
		  event knowledge extraction. For document representation, we
		  employ events and relations to model the document to
		  guarantee the comprehensiveness and inner-content
		  correlation. For identifier construction, we map the events
		  to well-defined event taxonomy to construct the identifiers
		  with explicit semantic structure. Our method achieves
		  significant improvement over the baselines on two datasets,
		  and also hopes to provide insights for future research.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {975–978},
  numpages	= {4},
  keywords	= {event knowledge, generative document retrieval, large
		  language model},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3349341.3349360,
  author	= {Zou, Xiaohui and Zou, Shunpeng and Wang, Xiaoqun},
  title		= {Smart System Studied: New Approaches to Natural Language
		  Understanding},
  year		= {2019},
  isbn		= {9781450371506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3349341.3349360},
  doi		= {10.1145/3349341.3349360},
  abstract	= {This paper aims to focus on the smart system as optimized
		  expert knowledge acquisition system as new approaches to
		  natural language understanding system. This method can
		  finish fine processing for any text segment instantly. The
		  module's precision machining can adopt big production
		  method that combines on the line first, complete coverage
		  and accurate grasp each language point and knowledge point
		  and original point even their respective combination. Its
		  characteristics are teachers and students can use the text
		  analyzed method to do the fine processing of the same
		  knowledge module, and only in Chinese or English, through
		  the selection of keywords and terminology and knowledge
		  modules that can be used as the menu to be selected as the
		  way to achieve knowledge with the system. The result is the
		  learning environment that enables human-computer
		  collaboration system namely smart system to optimize the
		  expert knowledge acquisition and the natural language
		  understanding as a research field that has great
		  significance to human beings. Its significance is that this
		  learning environment software based on the National
		  Excellent Courses by using the language chess with the
		  feature of the introduction on the knowledge big production
		  mode for the textual knowledge module finishing at Peking
		  University.},
  booktitle	= {Proceedings of the 2019 International Conference on
		  Artificial Intelligence and Computer Science},
  pages		= {1–6},
  numpages	= {6},
  keywords	= {Natural Language Understanding, Smart System Studied},
  location	= {Wuhan, Hubei, China},
  series	= {AICS 2019}
}

@InProceedings{	  10.1145/3408308.3427979,
  author	= {He, Fang and Xu, Cheng and Xu, Yanhui and Hong, Dezhi and
		  Wang, Dan},
  title		= {EnergonQL: A Building Independent Acquisitional Query
		  Language for Portable Building Analytics},
  year		= {2020},
  isbn		= {9781450380614},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3408308.3427979},
  doi		= {10.1145/3408308.3427979},
  abstract	= {Emerging building analytics heavily rely on data-driven
		  machine learning algorithms. However, writing these
		  analytics is still challenging: developers not only need to
		  know what data is required but also where this data is in
		  each individual building when writing applications. To
		  bridge this gap between analytics and the actual resources
		  in buildings, we present EnergonQL, a building independent
		  acquisitional data query language that extracts data for
		  building analytics with a declarative query processor.
		  EnergonQL provides logic views of building resources that
		  universally apply to all buildings, thus allowing portable
		  building analytics across buildings. We evaluate EnergonQL
		  with four different building analytics and show that with
		  EnergonQL the line-of-code and development efforts can be
		  effectively reduced.},
  booktitle	= {Proceedings of the 7th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {266–269},
  numpages	= {4},
  keywords	= {Declarative query language, Smart buildings, data
		  analytics},
  location	= {Virtual Event, Japan},
  series	= {BuildSys '20}
}

@InProceedings{	  10.1145/3546157.3546164,
  author	= {Mohseni, Maryam and Maher, Mary Lou},
  title		= {A Framework for Exploring Computational Models of Novelty
		  in Unstructured Text},
  year		= {2022},
  isbn		= {9781450396257},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3546157.3546164},
  doi		= {10.1145/3546157.3546164},
  abstract	= {Novelty modeling in unstructured text data is a research
		  topic within the Natural Language Processing (NLP)
		  Community. Effective novelty models can play a key role in
		  providing relevant and interesting content to the users
		  which is the central goal in many applications including
		  education and recommender systems. This paper presents a
		  framework for comparing different approaches and
		  applications of computational models of novelty in
		  unstructured text data. We focus on computational models
		  that apply methods such as natural language processing and
		  information theory. The framework provides an ontology for
		  computational novelty with respect to the source of text
		  data, methods for representing the data, and models for
		  measuring novelty. We explore the value of the framework by
		  applying it to research on computational novelty in news
		  articles, research publications, books, and recipes. This
		  framework is independent of the type of data in the items
		  and can be used as a tool for researchers to study,
		  compare, and extend existing computational novelty models
		  and applications.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Information System and Data Mining},
  pages		= {36–45},
  numpages	= {10},
  keywords	= {Computational models of novelty, NLP, Recommender systems,
		  Surprise, Unstructured text},
  location	= {Silicon Valley, CA, USA},
  series	= {ICISDM '22}
}

@InProceedings{	  10.5555/3522802.3522965,
  author	= {Wilsdorf, Pia and Fischer, Nadine and Haack, Fiete and
		  Uhrmacher, Adelinde M.},
  title		= {Exploiting provenance and ontologies in supporting best
		  practices for simulation experiments: a case study on
		  sensitivity analysis},
  year		= {2022},
  publisher	= {IEEE Press},
  abstract	= {Simulation studies are intricate processes and user
		  support for conducting more consistent, systematic, and
		  efficient simulation studies is needed. Simulation
		  experiments as one crucial part of a simulation study can
		  benefit from semi-automatic method selection,
		  parameterization, and execution. However, this largely
		  depends on the context in which the experiment is
		  conducted. Context information about a simulation study can
		  be provided in form of provenance that documents which
		  artifacts contributed in developing a simulation model. We
		  present an approach that exploits provenance to support
		  best practices for simulation experiments. The approach
		  relies on 1) explicitly specified provenance information,
		  2) an ontology of methods, 3) best practices rules, and 4)
		  integration with a previously developed experiment
		  generation pipeline. We demonstrate our approach by
		  conducting a sensitivity analysis experiment within a cell
		  biological simulation study.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  articleno	= {194},
  numpages	= {12},
  location	= {Phoenix, Arizona},
  series	= {WSC '21}
}

@InProceedings{	  10.1145/3442381.3449881,
  author	= {Li, Jiaqi and Wu, Xuan and Lu, Chang and Deng, Wenxing and
		  Zhao, Yizheng},
  title		= {Computing Views of OWL Ontologies for the Semantic Web},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3449881},
  doi		= {10.1145/3442381.3449881},
  abstract	= {This paper tackles the problem of computing views of OWL
		  ontologies using a forgetting-based approach. In
		  traditional relational databases, a view is a subset of a
		  database, whereas in ontologies, a view is more than a
		  subset; it contains not only axioms contained in the
		  original ontology, but may also contain newly-derived
		  axioms entailed by the original ontology (implicitly
		  contained in the original ontology). Specifically, given an
		  ontology , the signature of is the set of all the names in
		  , and a view of is a new ontology obtained from using only
		  part of ’s signature, namely the target signature, while
		  preserving all logical entailments up to the target
		  signature. Computing views of OWL ontologies is useful for
		  Semantic Web applications such as ontology-based query
		  answering, in a way that the view can be used as a
		  substitute of the original ontology to answer queries
		  formulated with the target signature, and information
		  hiding, in the sense that it restricts users from viewing
		  certain information of an ontology. Forgetting is a form of
		  non-standard reasoning concerned with eliminating from an
		  ontology a subset of its signature, namely the forgetting
		  signature, in such a way that all logical entailments are
		  preserved up to the target signature. Forgetting can thus
		  be used as a means for computing views of OWL ontologies
		  — the solution of forgetting a set of names from an
		  ontology is the view of for the target signature . In this
		  paper, we present a forgetting-based method for computing
		  views of OWL ontologies specified in the description logic
		  , the basic extended with role hierarchy, nominals and
		  inverse roles. The method is terminating and sound. Despite
		  the method not being complete, an evaluation with a
		  prototype implementation of the method on a corpus of
		  real-world ontologies has shown very good success rates.
		  This is very useful from the perspective of the Semantic
		  Web, as it provides knowledge engineers with a powerful
		  tool for creating views of OWL ontologies.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {2624–2635},
  numpages	= {12},
  keywords	= {Description Logics, Forgetting, Ontology, Semantics Web},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3209914.3209940,
  author	= {Akg\"{u}n, Ayhan and Ayvaz, Serkan},
  title		= {An Approach for Information Discovery Using Ontology In
		  Semantic Web Content},
  year		= {2018},
  isbn		= {9781450364218},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209914.3209940},
  doi		= {10.1145/3209914.3209940},
  abstract	= {Information searching techniques are rapidly developing as
		  the World Wide Web (WWW) evolves. Along with the
		  development of information technologies, the need for
		  acquiring domain knowledge bases, accessing data sources
		  and discovering insights increases. The advancements in
		  knowledge discovery, information management and artificial
		  intelligence require faster data processing, storing more
		  data and developing more intelligent applications. This
		  study provides an information discovery and data
		  integration approach for linked open data in the semantic
		  web. Using semantics embedded in ontologies, data available
		  in knowledge bases can be enhanced to better serve the
		  information needs of users. The entity relationships
		  between resources and resource hierarchies represented as
		  linked open data in semantic web provide semantically rich
		  insights about the data and facilitates knowledge
		  discovery. Graph theory methods can be utilized to enrich
		  the features of data sets in semantic web. In this study,
		  we propose an approach for integrating isolated data
		  sources with semantic web by using ontologies to make them
		  available for information discovery and enhancing the
		  features of semantic data by using graph theory
		  techniques.},
  booktitle	= {Proceedings of the 1st International Conference on
		  Information Science and Systems},
  pages		= {250–255},
  numpages	= {6},
  keywords	= {Graph, information retrieval, ontology, semantic web},
  location	= {Jeju, Republic of Korea},
  series	= {ICISS '18}
}

@Article{	  10.1145/3447523,
  author	= {Maree, Mohammed and Rattrout, Amjad and Altawil, Muhanad
		  and Belkhatir, Mohammed},
  title		= {Multi-modality Search and Recommendation on Palestinian
		  Cultural Heritage Based on the Holy-Land Ontology and
		  Extrinsic Semantic Resources},
  year		= {2021},
  issue_date	= {July 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3447523},
  doi		= {10.1145/3447523},
  abstract	= {The Cultural Heritage (CH) sector and its associated
		  tourism services have been affected notably by the
		  advancement of the Internet as well as the explosive growth
		  of smartphones and other handheld devices. These days,
		  visitors can access reliable CH content using Web and
		  mobile-based interfaces. However, conventional CH systems
		  still lack the ability to provide meaningful semantically
		  overt results that precisely meet user information needs in
		  this domain. In addition, they often ignore the user search
		  context and experience, which hinders their ability to
		  adapt their behavior to the preferences, tasks, interests,
		  and other user functionalities. In this article, we aim to
		  address the issue of designing a precision-oriented
		  multilingual and multi-criteria semantic-based mobile
		  recommender system specifically targeting Palestine's CH, a
		  country with great historical and cultural importance. We
		  aim to better facilitate users’ access to CH content by
		  providing them with multiple search functionalities. In
		  this context, a user can search for relevant information
		  using keywords (a.k.a. tags) or sentence-like queries and
		  the system retrieves all relevant documents based on their
		  semantic similarity. A second option is to search using
		  current location information to retrieve correlated
		  historical places and events. Finally, starting from a
		  picture of interest, a third option makes it possible to
		  extract captions describing its content that can be used to
		  search for additional contextually relevant information.
		  Additionally, the proposed system aims at personalizing
		  users’ experience through progressively delivering output
		  that meets their information needs based on a number of
		  parameters such as users' logging data, interests, previous
		  searches, and location-based information. A prototype of
		  the proposed system has been developed and tested using
		  Android smartphones and a manually constructed ontology
		  enriched with CH links to the Art \&amp; Architecture
		  Thesaurus (AAT) and DBpedia. By comparing our system with
		  similar systems in this domain, findings demonstrate that
		  it provides additional search features and functionalities
		  to users. The proposed Holy-Land ontology is the first of
		  its kind attempting to encode knowledge about Palestine's
		  CH. It plays a crucial role in our proposal, serving as a
		  pivotal entity in the combination of language-based,
		  location-based, and visual-based retrieval strategies.},
  journal	= {J. Comput. Cult. Herit.},
  month		= jul,
  articleno	= {29},
  numpages	= {23},
  keywords	= {Cultural heritage, content-based image retrieval, hybrid
		  recommendation, knowledge-based search, manually
		  constructed ontology, semantic similarity}
}

@InProceedings{	  10.1145/3433174.3433617,
  author	= {Thinyane, Mamello and Christine, Debora},
  title		= {SMART Citizen Cyber Resilience (SC2R) Ontology},
  year		= {2021},
  isbn		= {9781450387514},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3433174.3433617},
  doi		= {10.1145/3433174.3433617},
  abstract	= {Adverse cyber incidents are some of the top risks
		  currently facing the global community. Cybersecurity
		  frameworks and models formulated to mitigate these risks
		  are typically framed from the organizational perspective of
		  governments and the private sector. Further, they are
		  traditionally techno-centric solutions framed from a
		  cybersecurity perspective towards prevention of risks; only
		  recently are they incorporating resilience perspectives
		  towards anticipation of risks and positive adaptation
		  during adverse cyber incidents. This research makes
		  advances on human-centric cyber resilience - from the
		  perspective of citizens and centered on citizens’
		  multi-dimensional experience of adverse cyber incidents. It
		  considers the goal of cyber resilience, from the
		  capabilitarian perspective, as enhancing the cyber
		  capabilities of individuals and achieving positive
		  adaptation in the face of adverse cyber incidents. This
		  paper presents an ontology that is formulated to formalize
		  individuals’ cyber resilience. The paper motivates the
		  need for such an ontology and discusses the constitutive
		  concepts. Finally, it shows how this ontology can support
		  the realization of cyber resilience for individual
		  citizens.},
  booktitle	= {13th International Conference on Security of Information
		  and Networks},
  articleno	= {14},
  numpages	= {8},
  keywords	= {Cyber resilience, Cybersecurity, Human-centric
		  cybersecurity, Ontologies},
  location	= {Merkez, Turkey},
  series	= {SIN 2020}
}

@InProceedings{	  10.5555/3463952.3463986,
  author	= {Bourahla, Yasser and Atencia, Manuel and Euzenat,
		  J\'{e}r\^{o}me},
  title		= {Knowledge Improvement and Diversity under
		  Interaction-Driven Adaptation of Learned Ontologies},
  year		= {2021},
  isbn		= {9781450383073},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {When agents independently learn knowledge, such as
		  ontologies, about their environment, it may be diverse,
		  incorrect or incomplete. This knowledge heterogeneity could
		  lead agents to disagree, thus hindering their cooperation.
		  Existing approaches usually deal with this interaction
		  problem by relating ontologies, without modifying them, or,
		  on the contrary, by focusing on building common knowledge.
		  Here, we consider agents adapting ontologies learned from
		  the environment in order to agree with each other when
		  cooperating. In this scenario, fundamental questions arise:
		  Do they achieve successful interaction? Can this process
		  improve knowledge correctness? Do all agents end up with
		  the same ontology? To answer these questions, we design a
		  two-stage experiment. First, agents learn to take decisions
		  about the environment by classifying objects and the
		  learned classifiers are turned into ontologies. In the
		  second stage, agents interact with each other to agree on
		  the decisions to take and modify their ontologies
		  accordingly. We show that agents indeed reduce interaction
		  failure, most of the time they improve the accuracy of
		  their knowledge about the environment, and they do not
		  necessarily opt for the same ontology.},
  booktitle	= {Proceedings of the 20th International Conference on
		  Autonomous Agents and MultiAgent Systems},
  pages		= {242–250},
  numpages	= {9},
  keywords	= {knowledge diversity, multi-agent learning, multi-agent
		  social simulation, ontologies},
  location	= {Virtual Event, United Kingdom},
  series	= {AAMAS '21}
}

@InProceedings{	  10.1145/3704268.3742700,
  author	= {Hussein, Hassan and Oelen, Allarad and Auer, S\"{o}ren},
  title		= {A Hybrid, Neuro-symbolic Approach for Scholarly Knowledge
		  Organization},
  year		= {2025},
  isbn		= {9798400713514},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704268.3742700},
  doi		= {10.1145/3704268.3742700},
  abstract	= {The rapid development of generative AI leveraging neural
		  models, particularly with the introduction of large
		  language models (LLMs), has fundamentally advanced natural
		  language processing and generation. However, such neural
		  models are non-deterministic, opaque, and tend to
		  confabulate. Knowledge Graphs (KGs) on the other hand
		  contain factual information represented in a symbolic way
		  for humans and machines following formal knowledge
		  representation formalisms. However, the creation and
		  curation of KGs is time-consuming, cumbersome, and
		  resource-demanding. A key research challenge now is how to
		  synergistically combine both formalisms with the human in
		  the loop (Hybrid AI) to obtain structured and
		  machine-processable knowledge in a scalable way. We
		  introduce an approach for a tight integration of Humans,
		  Neural Models (LLM), and Symbolic Representations (KG) for
		  the semiautomatic creation and curation of Scholarly
		  Knowledge Graphs. Our approach, while demonstrated in the
		  scholarly context, establishes generalizable principles for
		  neuro-symbolic integration that can be adapted to other
		  domains. We implement and integrate our approach comprising
		  an intelligent user interface and prompt templates for
		  interaction with an LLM in the Open Research Knowledge
		  Graph. We perform a thorough analysis of our approach and
		  implementation with a user evaluation to assess the merits
		  of the neuro-symbolic, hybrid approach for organizing
		  scholarly knowledge.},
  booktitle	= {Proceedings of the 2025 ACM Symposium on Document
		  Engineering},
  articleno	= {16},
  numpages	= {10},
  location	= {Nottingham, United Kingdom},
  series	= {DocEng '25}
}

@Article{	  10.14778/3565838.3565850,
  author	= {Bellomarini, Luigi and Benedetto, Davide and Brandetti,
		  Matteo and Sallinger, Emanuel},
  title		= {Exploiting the Power of Equality-Generating Dependencies
		  in Ontological Reasoning},
  year		= {2022},
  issue_date	= {September 2022},
  publisher	= {VLDB Endowment},
  volume	= {15},
  number	= {13},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3565838.3565850},
  doi		= {10.14778/3565838.3565850},
  abstract	= {Equality-generating dependencies (EGDs) allow to fully
		  exploit the power of existential quantification in
		  ontological reasoning settings modeled via Tuple-Generating
		  Dependencies (TGDs), by enabling value-assignment or
		  forcing the equivalence of fresh symbols. These
		  capabilities are at the core of many common reasoning
		  tasks, including graph traversals, clustering, data
		  matching and data fusion, and many more related real-world
		  scenarios.However, the interplay of TGDs and EGDs is known
		  to lead to undecidability or intractability of query
		  answering in tractable Datalog+/- fragments, like Warded
		  Datalog+/-, for which, in the sole presence of TGDs, query
		  answering is PTIME in data complexity. Restrictions of
		  equality constraints, like separable EGDs, have been
		  studied, but all achieve decidability at the cost of
		  limited expressive power, which makes them unsuitable for
		  the mentioned tasks.This paper introduces the class of
		  "harmless" EGDs, that subsume separable EGDs and allow to
		  model a very broad class of tasks. We contribute a
		  sufficient syntactic condition for testing harmlessness, an
		  undecidable task in general. We argue that in Warded
		  Datalog+/- with harmless EGDs, ontological reasoning is
		  decidable and PTIME. From such theoretical underpinnings,
		  we develop novel chase-based techniques for reasoning with
		  harmless EGDs and present an implementation within the
		  Vadalog system, a state-of-the-art Datalog-based reasoner.
		  We provide full-scale experimental evaluation and
		  comparative analysis.},
  journal	= {Proc. VLDB Endow.},
  month		= sep,
  pages		= {3976–3988},
  numpages	= {13}
}

@InProceedings{	  10.1145/3357766.3359541,
  author	= {Seifer, Philipp and H\"{a}rtel, Johannes and Leinberger,
		  Martin and L\"{a}mmel, Ralf and Staab, Steffen},
  title		= {Empirical study on the usage of graph query languages in
		  open source Java projects},
  year		= {2019},
  isbn		= {9781450369817},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357766.3359541},
  doi		= {10.1145/3357766.3359541},
  abstract	= {Graph data models are interesting in various domains, in
		  part because of the intuitiveness and flexibility they
		  offer compared to relational models. Specialized query
		  languages, such as Cypher for property graphs or SPARQL for
		  RDF, facilitate their use. In this paper, we present an
		  empirical study on the usage of graph-based query languages
		  in open-source Java projects on GitHub. We investigate the
		  usage of SPARQL, Cypher, Gremlin and GraphQL in terms of
		  popularity and their development over time. We select
		  repositories based on dependencies related to these
		  technologies and employ various popularity and source-code
		  based filters and ranking features for a targeted selection
		  of projects. For the concrete languages SPARQL and Cypher,
		  we analyze the activity of repositories over time. For
		  SPARQL, we investigate common application domains, query
		  use and existence of ontological data modeling in
		  applications that query for concrete instance data. Our
		  results show, that the usage of graph query languages in
		  open-source projects increased over the last years, with
		  SPARQL and Cypher being by far the most popular. SPARQL
		  projects are more active in terms of query related artifact
		  changes and unique developers involved, but Cypher is
		  catching up. Relatively few applications use SPARQL to
		  query for concrete instance data: A majority of those
		  applications employ multiple different ontologies,
		  including project and domain specific ones. Common
		  application domains are management systems and data
		  visualization tools.},
  booktitle	= {Proceedings of the 12th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  pages		= {152–166},
  numpages	= {15},
  keywords	= {Cypher, Empirical Study, GitHub, GraphQL, Graphs, Gremlin,
		  Query Languages, SPARQL},
  location	= {Athens, Greece},
  series	= {SLE 2019}
}

@Article{	  10.1145/3549553,
  author	= {Kirchhof, J\"{o}rg Christian and Kleiss, Anno and Rumpe,
		  Bernhard and Schmalzing, David and Schneider, Philipp and
		  Wortmann, Andreas},
  title		= {Model-driven Self-adaptive Deployment of Internet of
		  Things Applications with Automated Modification Proposals},
  year		= {2022},
  issue_date	= {November 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {4},
  url		= {https://doi.org/10.1145/3549553},
  doi		= {10.1145/3549553},
  abstract	= {Today’s Internet of Things (IoT) applications are mostly
		  developed as a bundle of hardware and associated software.
		  Future cross-manufacturer app stores for IoT applications
		  will require that the strong coupling of hardware and
		  software is loosened. In the resulting IoT applications, a
		  quintessential challenge is the effective and efficient
		  deployment of IoT software components across variable
		  networks of heterogeneous devices. Current research focuses
		  on computing whether deployment requirements fit the
		  intended target devices instead of assisting users in
		  successfully deploying IoT applications by suggesting
		  deployment requirement relaxations or hardware
		  alternatives. This can make successfully deploying
		  large-scale IoT applications a costly trial-and-error
		  endeavor. To mitigate this, we have devised a method for
		  providing such deployment suggestions based on search and
		  backtracking. This can make deploying IoT applications more
		  effective and more efficient, which, ultimately, eases
		  reducing the complexity of deploying the software
		  surrounding us.},
  journal	= {ACM Trans. Internet Things},
  month		= sep,
  articleno	= {30},
  numpages	= {30},
  keywords	= {Internet of Things, deployment, model-driven engineering,
		  architecture description languages}
}

@InProceedings{	  10.1145/3373722.3373782,
  author	= {Vinogradov, Andrei and Kurshev, Evgeny and Vlasova,
		  Natalia and Podobryaev, Alexey},
  title		= {Information extraction tasks in public administration
		  domain: ISIDA-T natural language processing system},
  year		= {2020},
  isbn		= {9781450376709},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3373722.3373782},
  doi		= {10.1145/3373722.3373782},
  abstract	= {This article represents approaches of the artificial
		  intelligence methods, used in public administration. An
		  overview of various technologies of artificial intelligence
		  applications in the field of public administration and
		  related fields is given. All of these research directions
		  are particularly relevant to the task of digital
		  technologies (including artificial intelligence) growth to
		  create an efficient and competitive digital economics in
		  Russia. Among the modern intellectual technologies that
		  allow solving the widest range of tasks, an important role
		  plays technologies related to natural language text
		  processing - nonstructured NL-texts are essential segment
		  of data, used for analysis and decision-making tasks (in
		  terms of data volume, of course, video data have a leading
		  position, however, they are usually suitable for solving
		  tactical but not strategic management tasks). The article
		  provides an overview of the existing methods of natural
		  language processing and their practical application to the
		  tasks of public administration. An integrated approach to
		  the natural language processing tools using for solving
		  practical problems in the field of public administration is
		  considered on the example of the ISIDA-T system for
		  extracting information from natural language texts,
		  developed at the Artificial Intelligence Research Center
		  PSI RAS. The system under consideration is distinguished by
		  a modular approach to the pre-processing of unstructured
		  text and the possibility of manual adjustment for a
		  specific extraction task. This technological solution gives
		  the necessary flexibility and ease of use. The system
		  consists of configurable text preprocessing, linguistic
		  analysis, target information retrieval and output of the
		  results in user-friendly form modules. One of the important
		  components of the system is an integrated knowledge
		  resource that allows quickly and efficiently adjust the
		  system to the specifics of the relevant subject area. An
		  approach to the application of the ISIDA-T system to the
		  task of fact information extraction (as the most important
		  for analyzing the situation and subsequent management
		  decisions) is proposed using the example of information
		  extraction about resignations and appointments from news
		  feeds.},
  booktitle	= {Proceedings of the XI International Scientific Conference
		  Communicative Strategies of the Information Society},
  articleno	= {15},
  numpages	= {10},
  keywords	= {Artificial intelligence, Digital economy, Information
		  extraction, Natural language processing, Public
		  administration},
  location	= {St. Petersburg, Russian Federation},
  series	= {CSIS'2019}
}

@InProceedings{	  10.1145/3372782.3406258,
  author	= {McGill, Monica M. and Decker, Adrienne},
  title		= {Construction of a Taxonomy for Tools, Languages, and
		  Environments across Computing Education},
  year		= {2020},
  isbn		= {9781450370929},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3372782.3406258},
  doi		= {10.1145/3372782.3406258},
  abstract	= {The sheer number of tools, languages, and environments
		  (TLEs) used in computing education has proliferated in the
		  last few years as more tools are developed to meet new
		  demands of the growing amount of K-12 computing education
		  that has been undertaken. However, there is little
		  formalized language at either the K-12 or post-secondary
		  level that provides for a way to classify these TLEs for
		  discussing research and for classifying in databases.In
		  this research study, we step through a formal process for
		  building a taxonomy for TLEs. As part of the supporting
		  research, we first discuss the importance of taxonomies and
		  classification systems in computing education, provide a
		  formal method for building a taxonomy, and provide working
		  definitions of TLEs based on previous literature. This is
		  followed by a systematic literature review using a
		  widely-accepted methodology for finding articles that have
		  examined TLEs in primary, secondary, and post-secondary
		  computing education. This literature review focuses on
		  studies that looked at multiple TLEs and specifically
		  attempted to classify or categorize them. We then propose a
		  new taxonomy for classifying TLEs and provide definitions
		  and samples for each category. This is followed by a
		  discussion of the next steps in vetting the taxonomy and
		  the challenges and issues that need to be considered when
		  evaluating it for classifying TLEs in computing
		  education.},
  booktitle	= {Proceedings of the 2020 ACM Conference on International
		  Computing Education Research},
  pages		= {124–135},
  numpages	= {12},
  keywords	= {classification, computing, education, environments, k-12,
		  languages, literature review, ontology, post-secondary,
		  primary, secondary, taxonomy, tools},
  location	= {Virtual Event, New Zealand},
  series	= {ICER '20}
}

@InProceedings{	  10.5555/3522802.3522970,
  author	= {Shuttleworth, David and Padilla, Jose J.},
  title		= {Towards semi-automatic model specification},
  year		= {2022},
  publisher	= {IEEE Press},
  abstract	= {This paper presents a natural language understanding (NLU)
		  approach to transition a description of a phenomenon
		  towards a simulation specification. As multidisciplinary
		  endeavors using simulations increase, the need for teams to
		  better communicate and make non-modelers active
		  participants on the process increases. We focus on
		  semi-automating the model conceptualization process towards
		  the creation of a specification as it is one of the most
		  challenging steps in collaborations. The approach relies on
		  NLU processing of narratives, create a model that captures
		  concepts and relationships, and finally provide a
		  specification of a simulation implementation. An initial
		  definition set and grammatical rules are proposed to
		  formalize this process. These are followed by a Design of
		  Experiments (DoE) to test the NLU model accuracy and a test
		  case that generates Agent-Based Model (ABM)
		  conceptualizations and specifications. We provide a
		  discussion on the advantages and limitations of using NLUs
		  for model conceptualization and specification processes.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  articleno	= {199},
  numpages	= {12},
  location	= {Phoenix, Arizona},
  series	= {WSC '21}
}

@InProceedings{	  10.1145/3651671.3651773,
  author	= {Sore, Safiatou S. and Ouedraogo, T. Frederic T.Fr\'{e}dric
		  and Bikienga, Moustapha M. and Traore, Yaya Y.},
  title		= {Towards a More Generic and Elastic Metadata Management
		  Model in a Data Lake Environment},
  year		= {2024},
  isbn		= {9798400709234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3651671.3651773},
  doi		= {10.1145/3651671.3651773},
  abstract	= {The evolution of the vast amount of heterogeneous data
		  sources is leading to the emergence of several new
		  concepts. One of the best-known concepts that is emerging
		  as a new and trending topic in the big data space is the
		  data lake. This is a central repository that stores
		  heterogeneous data sources in their native format, without
		  any predefined schema. In the absence of an enforced
		  schema, effective metadata management based on metadata
		  models remains an active research topic to address the
		  problems associated with the data lake: the "data swamp".
		  The analysis of existing metadata models shows that there
		  is no comprehensive model among them. In this paper, we
		  present a generic and scalable metadata model, which refers
		  to the ability to dynamically provision computing resources
		  based on demand and to resize resources as needed during
		  metadata integration. Our approach will be based on a
		  functional architecture of the data lake, along with a set
		  of features that promote the generality of the metadata
		  model.CCS CONCEPTS: Information systems→ Data management
		  systems→ Information integration → Entity resolution},
  booktitle	= {Proceedings of the 2024 16th International Conference on
		  Machine Learning and Computing},
  pages		= {44–51},
  numpages	= {8},
  keywords	= {data lake, elasticity, metadata, scalability},
  location	= {Shenzhen, China},
  series	= {ICMLC '24}
}

@InProceedings{	  10.1145/3404835.3462904,
  author	= {Zhang, Chiqun and Evans, Michael R. and Lepikhin, Max and
		  Yankov, Dragomir},
  title		= {Fast Attention-based Learning-To-Rank Model for Structured
		  Map Search},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3462904},
  doi		= {10.1145/3404835.3462904},
  abstract	= {Recent works show that Transformer-based learning-to-rank
		  (LTR) approaches can outperform previous well-established
		  ranking methods, such as gradient-boosted decision trees
		  (GBDT), on document and passage re-ranking problems. A
		  common assumption in these works is that the query and the
		  result documents are comprised of purely textual
		  information without explicit structure. In map search, the
		  relevance of results is determined based on rich
		  heterogeneous features - textual features derived from the
		  query and the results, geospatial features such as
		  proximity of a result to the user, structured features
		  reflecting the address format of the result, and the
		  perceived structure of the query. In this work, we propose
		  a novel deep neural network LTR architecture, capable of
		  seamlessly handling heterogeneous inputs, similar to
		  GBDT-based methods. At the same time, unlike GBDT, the
		  architecture does not require human input via (numerous)
		  carefully-crafted features. Instead, features are inferred
		  through a self-attention mechanism. Our model implements
		  two lightweight attention layers optimized for ranking: the
		  first layer computes query-result similarities, the second
		  implements listwise ranking inference. We perform
		  evaluation on several single language and one multilingual
		  dataset. Our model outperforms by a wide margin other
		  Transformer-based ranking architectures and has equal or
		  better performance than GBDT models. Equally important,
		  runtime inference is orders of magnitude faster than other
		  Transformer architectures, significantly reducing hardware
		  serving costs. The model is a low-cost alternative suitable
		  to power ranking in industrial map search engines across a
		  variety of languages and markets.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {942–951},
  numpages	= {10},
  keywords	= {geocoding, information retrieval, language understanding,
		  map search, ranking system},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3543873.3587659,
  author	= {Donald, Andy and Galanopoulos, Apostolos and Curry, Edward
		  and Mu\~{n}oz, Emir and Ullah, Ihsan and Waskow, M. A. and
		  Dabrowski, Maciej and Kalra, Manan},
  title		= {Towards a Semantic Approach for Linked Dataspace, Model
		  and Data Cards},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587659},
  doi		= {10.1145/3543873.3587659},
  abstract	= {The vast majority of artificial intelligence practitioners
		  overlook the importance of documentation when building and
		  publishing models and datasets. However, due to the recent
		  trend in the explainability and fairness of AI models,
		  several frameworks have been proposed such as Model Cards,
		  and Data Cards, among others, to help in the appropriate
		  re-usage of those models and datasets. In addition, because
		  of the introduction of the dataspace concept for similar
		  datasets in one place, there is potential that similar
		  Model Cards, Data Cards, Service Cards, and Dataspace Cards
		  can be linked to extract helpful information for better
		  decision-making about which model and data can be used for
		  a specific application. This paper reviews the case for
		  considering a Semantic Web approach for exchanging
		  Model/Data Cards as Linked Data or knowledge graphs in a
		  dataspace, making them machine-readable. We discuss the
		  basic concepts and propose a schema for linking Data Cards
		  and Model Cards within a dataspace. In addition, we
		  introduce the concept of a dataspace card which can be a
		  starting point for extracting knowledge about models and
		  datasets in a dataspace. This helps in building trust and
		  reuse of models and data among companies and individuals
		  participating as publishers or consumers of such assets.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {1468–1473},
  numpages	= {6},
  keywords	= {AI Documentation, Data Cards, Dataspace Cards, Model
		  Cards, Semantic Web, Service Cards},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3550356.3561581,
  author	= {Vanommeslaeghe, Yon and Ceulemans, David and van Acker,
		  Bert and Denil, Joachim and Derammelaere, Stijn and De
		  Meulenaere, Paul},
  title		= {Validation and uncertainty in model-based design space
		  exploration: an experience report},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561581},
  doi		= {10.1145/3550356.3561581},
  abstract	= {Model-based systems engineering (MBSE) techniques can help
		  manage the growing complexity in the design and development
		  of cyber-physical systems, and can even allow for the
		  optimization of a system under design in simulation.
		  However, models are always an abstraction of the real-world
		  systems they represent. This introduces uncertainty at the
		  model level, which affects the validity of simulation
		  results, and thus also the results of the optimization.
		  This, together with variations in real-world system
		  parameters, significantly complicates the validation of
		  simulation and optimization results. In this experience
		  report, we first use a descriptive process model to
		  describe our efforts to validate the results of a
		  model-based design space exploration (DSE) process given
		  this uncertainty. After this, we discuss lessons learned
		  and insights gained, and identify future challenges. We
		  present a possible prescriptive process model for future
		  validation efforts, which specifically takes into account
		  uncertainty.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {702–711},
  numpages	= {10},
  keywords	= {cyber-physical systems, design space exploration,
		  model-based systems engineering, uncertainty, validation},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3412841.3441942,
  author	= {Teixeira, Milene Santos and Maran, Vin\'{\i}cius and
		  Dragoni, Mauro},
  title		= {The interplay of a conversational ontology and AI planning
		  for health dialogue management},
  year		= {2021},
  isbn		= {9781450381048},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3412841.3441942},
  doi		= {10.1145/3412841.3441942},
  abstract	= {Health dialogue systems are required to respect some
		  special requirements such as predictability and
		  reliability. While knowledge based approaches still seem to
		  be the most appropriate for these systems, the automated
		  generation of reliable policies remains an open problem.
		  This work proposes an approach that integrates a
		  conversational ontology (Convology) and Artificial
		  Intelligence planning with the aim of automating the
		  generation of a dialogue manager capable of handling
		  goal-oriented dialogues for the health domain. The
		  resulting dialogue manager is aimed to be integrated into a
		  suitable architecture that provides the natural language
		  components. We illustrate our approach by describing how it
		  has been implemented into PuffBot, a multi-turn
		  goal-oriented conversational agent for supporting patients
		  affected by asthma.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on Applied
		  Computing},
  pages		= {611–619},
  numpages	= {9},
  keywords	= {automated planning, conversational ontology, dialogue
		  management, health dialogue},
  location	= {Virtual Event, Republic of Korea},
  series	= {SAC '21}
}

@InProceedings{	  10.1145/3366030.3366111,
  author	= {Jirkovsk\'{y}, V\'{a}clav and \v{S}ebek, Ond\v{r}ej and
		  Kadera, Petr and Burget, Pavel and Knoch, S\"{o}nke and
		  Becker, Tilman},
  title		= {Facilitation of Domain-Specific Data Models Design using
		  Semantic Web Technologies for Manufacturing},
  year		= {2020},
  isbn		= {9781450371797},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366030.3366111},
  doi		= {10.1145/3366030.3366111},
  abstract	= {Modern manufacturing faces a challenge of integrating data
		  models from various sources/domains which may differ both
		  semantically and technically when particular domain
		  specific data models are designed by different users and
		  stored in different formats. This paper introduces an
		  approach for facilitating the design of domain-specific
		  data models using semantic web technologies. In this
		  approach, all the information required for managing the
		  production (including a description of a product, processes
		  involved in the production, and existing resources and
		  their specifications) is captured in an ontology. The
		  proposed Product, Process, and Resource (PPR) ontology
		  defines fundamental conceptualization of the production
		  that can be easily applied to the arbitrary domain.
		  Application of the PPR ontology is demonstrated in the case
		  of simple truck assembling by means of robots. Capturing
		  the knowledge in the form of ontology provides the
		  advantage of employing supporting tools such as reasoners
		  for consistency checking or query languages for information
		  extraction. The paper demonstrates the utilization of SQWRL
		  for searching resources suitable to manipulate given truck
		  parts on the basis of semantic matching between properties
		  of particular elements.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {649–653},
  numpages	= {5},
  keywords	= {Data Model Design, Industrial Automation, Ontology,
		  Semantic Matchmaking},
  location	= {Munich, Germany},
  series	= {iiWAS2019}
}

@InProceedings{	  10.1145/3437120.3437311,
  author	= {E. Samaridi, Nikoletta and N. Karanikolas, Nikitas and C.
		  Papakitsos, Evangelos and Papoutsidakis, Michail},
  title		= {Designing a Greek Electronic Dictionary based on
		  Ontology},
  year		= {2021},
  isbn		= {9781450388979},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3437120.3437311},
  doi		= {10.1145/3437120.3437311},
  abstract	= {In this paper we examine the design of a conceptual
		  dictionary of the modern Greek language with intelligent
		  features, which will offer possibilities of semantic
		  integration and interoperability in an automatic and secure
		  way, connecting heterogeneous systems and approaches in the
		  field of engineering and technologies under the light of a
		  standard and standardized organization and coding of data
		  that come from structured and semi-structured information
		  sources.},
  booktitle	= {Proceedings of the 24th Pan-Hellenic Conference on
		  Informatics},
  pages		= {223–225},
  numpages	= {3},
  keywords	= {binary relations, concepts, dictionary, hierarchy,
		  inheritance, interoperability, ontology, semantic
		  integration, taxonomy},
  location	= {Athens, Greece},
  series	= {PCI '20}
}

@Article{	  10.1145/3557890,
  author	= {Cunningham, Jay and Benabdallah, Gabrielle and Rosner,
		  Daniela and Taylor, Alex},
  title		= {On the Grounds of Solutionism: Ontologies of Blackness and
		  HCI},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {30},
  number	= {2},
  issn		= {1073-0516},
  url		= {https://doi.org/10.1145/3557890},
  doi		= {10.1145/3557890},
  abstract	= {Why is the solution the end point to a problem? While many
		  in HCI and design have examined the impulse to solve
		  problems–the solutionist or techno-solutionist
		  mindset–we examine the logic that binds the solution and
		  the problem together as a pair. Focusing on the timely and
		  consequential problem of systemic racial injustice, we
		  think through the paradoxical possibility that the pairing
		  of the problem and solution (so often treated as the
		  default in design and HCI) perpetuates the very conditions
		  we seek to improve. With Calvin Warren’s profound
		  Afro-pessimism, we recognize how the tools used to solve
		  structural inequities around Black life are constructed
		  with inequities themselves. The problem-solution,
		  therefore, is a dead end. We use this paradox as an
		  invitation to rethink ongoing efforts to seek equity and
		  justice more broadly, setting out a fragile but hopeful
		  path for HCI and design.},
  journal	= {ACM Trans. Comput.-Hum. Interact.},
  month		= apr,
  articleno	= {20},
  numpages	= {17},
  keywords	= {Solutionism, solutions, design problems, theory}
}

@Article{	  10.1145/3402440,
  author	= {Goy, Annamaria and Colla, Davide and Magro, Diego and
		  Accornero, Cristina and Loreto, Fabrizio and Radicioni,
		  Daniele Paolo},
  title		= {Building Semantic Metadata for Historical Archives through
		  an Ontology-driven User Interface},
  year		= {2020},
  issue_date	= {October 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3402440},
  doi		= {10.1145/3402440},
  abstract	= {Historical archives represent an immense wealth, the
		  potential of which is endangered by the lack of effective
		  management and access tools. We believe that this issue can
		  be faced by providing archive catalogs with a semantic
		  layer, containing rich semantic metadata, representing the
		  content of documents in a full-fledged formal
		  machine-readable format. In this article, we present the
		  contribution offered in this direction by the PRiSMHA
		  project, in which the conceptual vocabulary of the semantic
		  layer is represented by computational ontologies. However,
		  acquiring semantic knowledge represents a well-known
		  bottleneck for knowledge-based systems; to solve this
		  problem, PRiSMHA relies on a crowdsourcing collaborative
		  model, i.e., an online community of users who collaborate
		  in building semantic representations of the content of
		  archival documents. In this perspective, this article aims
		  at answering the following research question: Starting from
		  the axioms characterizing concepts in the computational
		  ontology underlying the system, how can we derive a user
		  interface enabling users to formally represent the content
		  of archival documents by exploiting the conceptual
		  vocabulary provided by the ontology?Our solution includes
		  the following steps: (a) a manually defined configuration,
		  acting as a pre-filter, to hide “unsuited” classes,
		  properties, and relations; (b) an algorithm, combining
		  heuristics and reasoning, which extracts from the ontology
		  all and only the “compatible” properties and relations,
		  given an entity (event) type; and (c) a set of strategies
		  to rank, group, and present the entity (event) properties
		  and relations, based on the results of a study with users.
		  This integrated solution enabled us to design an
		  ontology-driven user interface enabling users to
		  characterize entities, and in particular (historical)
		  events, on the basis of the vocabulary provided by the
		  ontology.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {25},
  numpages	= {36},
  keywords	= {Ontology-driven user interfaces, computational ontologies,
		  crowdsourcing platform, historical archives}
}

@InProceedings{	  10.1145/3410566.3410610,
  author	= {Zorgati, Hela and Djemaa, Raoudha Ben and Amor, Ikram
		  Amous Ben and Sedes, Florence},
  title		= {QoC enhanced semantic IoT model},
  year		= {2020},
  isbn		= {9781450375030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3410566.3410610},
  doi		= {10.1145/3410566.3410610},
  abstract	= {The miniaturization of computers, coupled with a constant
		  increase in computing power, led to the emergence of new
		  sources of context information. We are facing a new
		  paradigm, the Internet of Things (IoT). Today, this latter
		  improves the quality of life in multiple areas. However,
		  the heterogeneity of objects used in such environments
		  makes their interoperability difficult. In addition, the
		  observations produced by context providers (connected
		  objects) are generated with different vocabularies and data
		  formats. This heterogeneity of technologies in the IoT
		  world makes it necessary to adopt generic solutions.
		  Therefore, it is important to transform the raw data from
		  these context producers into knowledge and information
		  based on ontologies. The use of ontologies solves the
		  challenges of heterogeneity and interoperability of IoT
		  systems. In this paper, we propose a semantic IoT model
		  that aims to overcome the semantic interoperability
		  challenges introduced by the variety of objects potentially
		  used in IoT systems. Furthermore, we enhanced this ontology
		  with quality of context meta-data. These meta-data helps in
		  dealing with imperfection and inconsistency of the
		  collected IoT data.},
  booktitle	= {Proceedings of the 24th Symposium on International
		  Database Engineering \&amp; Applications},
  articleno	= {5},
  numpages	= {7},
  keywords	= {QoC meta-data, internet of things, ontology reuse,
		  semantic model},
  location	= {Seoul, Republic of Korea},
  series	= {IDEAS '20}
}

@InProceedings{	  10.5555/3373669.3373686,
  author	= {Finidori, Helene},
  title		= {Configuring patterns and pattern languages for systemic
		  inquiry and design},
  year		= {2020},
  publisher	= {The Hillside Group},
  address	= {USA},
  abstract	= {This paper builds on work relating to pattern languages
		  for social change, such as in the papers titled Fourth
		  generation pattern languages - patterns as epistemic
		  threads for systemic orientation, and Pattern Literacy in
		  support of Systems Literacy presented to the Systems
		  Science and Pattern Language communities between 2015 and
		  2017.It is part of an endeavor to bring pattern thinking
		  and systems thinking, or pattern science and systems
		  science, closer to each other, in order to further
		  introduce pattern thinking and pattern language in the
		  design, assessment and orientation of our
		  socio-technological and socioenvironmental systems, large
		  or small, to better address the societal issues of our
		  time. It complements several initiatives to put pattern
		  languages at the service of sustainability and societal
		  change, and to introduce pattern thinking and pattern
		  language into systems thinking and systemic design.My
		  broader aim is to enhance the innate patterning capability
		  of human beings and thus an overall pattern literacy in
		  support of systems literacy. Pattern literacy manifests our
		  ability to grasp, learn, assemble, represent and mobilize
		  patterns to make-sense of, converse about and shape our
		  world(s). Systems literacy manifests our ability to
		  interrogate and attempt to understand the relationships
		  among systems wholes and parts, and the mechanisms that
		  affect and shape our world(s), in part or as a whole.In
		  this paper, I explore how a systemic approach to patterns
		  and pattern language could support systemic inquiry and
		  systemic design, and more generally the advancement of
		  pattern language.In particular, I discuss the extension of
		  the act of design to encompass the systemic inquiry that
		  motivates a design and the on-going monitoring of the
		  fitness of a design to its intended purpose. I examine the
		  multiple facets and understandings of the concept of
		  pattern and show how they can be reconciled to include both
		  the inquiry or observational/informational aspects and the
		  design aspects of patterns in a larger systems framework.
		  In this light, I reexamine the appropriateness of the
		  pattern expressed in problem-solution form in the context
		  of complex systems, and the notion of generativity, and I
		  propose ways forward for extended definitions and pattern
		  forms.},
  booktitle	= {Proceedings of the 25th Conference on Pattern Languages of
		  Programs},
  articleno	= {18},
  numpages	= {32},
  keywords	= {action research, boundary objects, complex adaptive
		  modeling, complex systems, participatory inquiry, pattern
		  language, pattern literacy, semiotics},
  location	= {Portland, Oregon},
  series	= {PLoP '18}
}

@Article{	  10.1145/3641552,
  author	= {Tseng, Tiffany and Davidson, Matt J. and Morales-Navarro,
		  Luis and Chen, Jennifer King and Delaney, Victoria and
		  Leibowitz, Mark and Beason, Jazbo and Shapiro, R.
		  Benjamin},
  title		= {Co-ML: Collaborative Machine Learning Model Building for
		  Developing Dataset Design Practices},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {2},
  url		= {https://doi.org/10.1145/3641552},
  doi		= {10.1145/3641552},
  abstract	= {Machine learning (ML) models are fundamentally shaped by
		  data, and building inclusive ML systems requires
		  significant considerations around how to design
		  representative datasets. Yet, few novice-oriented ML
		  modeling tools are designed to foster hands-on learning of
		  dataset design practices, including how to design for data
		  diversity and inspect for data quality. To this end, we
		  outline a set of four data design practices (DDPs) for
		  designing inclusive ML models and share how we designed a
		  tablet-based application called Co-ML to foster learning of
		  DDPs through a collaborative ML model building experience.
		  With Co-ML, beginners can build image classifiers through a
		  distributed experience where data is synchronized across
		  multiple devices, enabling multiple users to iteratively
		  refine ML datasets in discussion and coordination with
		  their peers. We deployed Co-ML in a 2-week-long educational
		  AIML Summer Camp, where youth ages 13–18 worked in groups
		  to build custom ML-powered mobile applications. Our
		  analysis reveals how multi-user model building with Co-ML,
		  in the context of student-driven projects created during
		  the summer camp, supported development of DDPs including
		  incorporating data diversity, evaluating model performance,
		  and inspecting for data quality. Additionally, we found
		  that students’ attempts to improve model performance
		  often prioritized learnability over class balance. Through
		  this work, we highlight how the combination of
		  collaboration, model testing interfaces, and student-driven
		  projects can empower learners to actively engage in
		  exploring the role of data in ML systems.},
  journal	= {ACM Trans. Comput. Educ.},
  month		= apr,
  articleno	= {25},
  numpages	= {37},
  keywords	= {Machine learning, collaboration, computing education, data
		  science}
}

@InProceedings{	  10.1145/3275245.3275269,
  author	= {Renault, Laylla D.C. and Barcellos, Monalessa Perini and
		  de Almeida Falbo, Ricardo},
  title		= {Using an Ontology-based Approach for Integrating
		  Applications to support Software Processes},
  year		= {2018},
  isbn		= {9781450365659},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3275245.3275269},
  doi		= {10.1145/3275245.3275269},
  abstract	= {Software organizations use several applications to support
		  their software processes. To properly support the software
		  processes, applications should be integrated at different
		  layers (data, service, and process). Moreover, the
		  integration should cover semantic aspects. Therefore, an
		  approach that provides guidelines on how to perform
		  integration at different layers addressing semantic aspects
		  can be helpful. This paper presents an extension of the
		  Ontology-based Approach for Semantic Integration (OBA-SI),
		  focusing on semantic integration at process layer. This
		  extension establishes relationships between integration at
		  data, service and process layers, and uses task ontologies
		  and a process ontology to guide integration at process
		  layer. It was used to provide an integrated solution
		  involving applications supporting the Issue Management and
		  Software Configuration Management processes.},
  booktitle	= {Proceedings of the XVII Brazilian Symposium on Software
		  Quality},
  pages		= {220–229},
  numpages	= {10},
  keywords	= {Ontology, process integration, semantics, system
		  integration},
  location	= {Curitiba, Brazil},
  series	= {SBQS '18}
}

@InProceedings{	  10.1145/3703187.3703290,
  author	= {Ma, Xiangfei and Li, Lin},
  title		= {Geological Disaster Named Entity Recognition with Small
		  Samples Based on Data Augmentation and Prompt Engineering},
  year		= {2024},
  isbn		= {9798400707254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703187.3703290},
  doi		= {10.1145/3703187.3703290},
  abstract	= {This paper uses a large language model to perform
		  generative data enhancement on the original small sample
		  data by performing random synonym replacement and random
		  mask filling operations. In accordance with the reasoning
		  logic of the large language model, three prompt templates
		  are designed and the reasons are explored. Experiments show
		  that when the parameters remain unchanged, the data
		  enhanced by this method has been greatly improved under the
		  three prompt templates, alleviating the difficulty of low
		  resources of geological disaster data. And by comparing the
		  performance of different instructions under different
		  learning rates, the fine-tuning learning rate range
		  suitable for the field of geological disasters is
		  summarized. The limitation is that it is constrained by
		  local computing resources, which reduces the parameter
		  scale of LLM, and the recognition performance is low for
		  extremely long or complex texts.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Computer Information Science and Artificial Intelligence},
  pages		= {613–617},
  numpages	= {5},
  keywords	= {Data Augmentation, Geological Disasters, LLMs, Named
		  Entity Recognition, Prompt Engineering},
  location	= { },
  series	= {CISAI '24}
}

@InProceedings{	  10.1145/3347317.3357245,
  author	= {Lagrue, Sylvain and Chetcuti-Sperandio, Nathalie and
		  Delorme, Fabien and Thi, Chau Ma and Thi, Duyen Ngo and
		  Tabia, Karim and Benferhat, Salem},
  title		= {An Ontology Web Application-based Annotation Tool for
		  Intangible Culture Heritage Dance Videos},
  year		= {2019},
  isbn		= {9781450369107},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3347317.3357245},
  doi		= {10.1145/3347317.3357245},
  abstract	= {Collecting dance videos, preserving and promoting them
		  after enriching the collected data has been significant
		  actions in preserving Intangible culture heritage in
		  South-East Asia. Whereas techniques for the conceptual
		  modeling of the expressive semantics of dance videos are
		  very complex, they are crucial to exploit effectively the
		  video semantics. This paper proposes an ontology web-based
		  dance video annotation system for representing the
		  semantics of dance videos at different granularity levels.
		  Especially, the system incorporates both syntactic and
		  semantic features of pre-built dance ontology system in
		  order to not only use the available semantic web system but
		  also to create unity for users when annotating videos to
		  minimize conflicts.},
  booktitle	= {Proceedings of the 1st Workshop on Structuring and
		  Understanding of Multimedia HeritAge Contents},
  pages		= {75–81},
  numpages	= {7},
  keywords	= {inconsistency-tolerant query answering, knowledge
		  representation, ontologies, video annotation},
  location	= {Nice, France},
  series	= {SUMAC '19}
}

@InProceedings{	  10.1145/3712716.3712719,
  author	= {Skipanes, Mads and Pratama, Nardiena and Porter, Kyle and
		  Demartini, Gianluca},
  title		= {Fast Synthetic Data Generation for Case-Specific Entity
		  Extraction in Criminal Investigations},
  year		= {2025},
  isbn		= {9798400710766},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3712716.3712719},
  doi		= {10.1145/3712716.3712719},
  abstract	= {In major criminal investigations, the manual analysis of
		  police reports for the categorization of entities is a
		  resource-intensive task prone to human error. Recent
		  advances in Named Entity Recognition (NER) models offer
		  promising solutions for automating this process,
		  potentially reducing both time and error rates.This paper
		  demonstrates the effectiveness of fine-tuning a NER model
		  using a publicly shared synthetic dataset inspired by real
		  case files. Notably, we leverage a large language model
		  (LLM) for generating both the synthetic data and the
		  annotations used for training. This approach enables
		  investigators to rapidly develop case-specific models
		  tailored to ongoing investigations. To structure this
		  effort, we propose an ontology for entity extraction in
		  criminal cases, focusing on key entities, such as persons,
		  seized items, communication profiles, vehicles, locations,
		  organizations, and financial profiles. Our model achieves
		  an average weighted F1-score of 94.2\% on the synthetic
		  dataset.For further validation, we manually annotated a
		  small dataset of confidential data from two homicide cases,
		  achieving an average weighted F1-score of 81.6\%. Our
		  results demonstrate that our approach can at times
		  generalize well to real case files.},
  booktitle	= {Proceedings of the Digital Forensics Doctoral Symposium},
  articleno	= {2},
  numpages	= {8},
  keywords	= {Criminal Investigation, Artificial Intelligence, Named
		  Entity Recognition, Large Language Models, Synthetic Data,
		  Data Management, Information Analysis.},
  location	= { },
  series	= {DFDS '25}
}

@InProceedings{	  10.1145/3307363.3307381,
  author	= {Liu, Bin and Wu, Junfeng and Yao, Li and Ding, Zheyuan},
  title		= {Ontology-based Fault Diagnosis: A Decade in Review},
  year		= {2019},
  isbn		= {9781450366199},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3307363.3307381},
  doi		= {10.1145/3307363.3307381},
  abstract	= {Fault diagnosis is a critical activity in the
		  comprehensive support for equipment operation. Relevant
		  research about it is becoming more and more popular.
		  Ontology-based fault diagnosis is an important method for
		  diagnosing faults. Due to the expressive ability, knowledge
		  sharing and knowledge reuse based on deep semantics, and
		  supporting for logical reasoning, ontology-based fault
		  diagnosis has received more and more attention from
		  researchers in the past decade. The fault diagnosis
		  ontology describes the core concepts involved in diagnosing
		  faults and the relationship among the concepts. Its quality
		  and usage determine the efficiency and effectiveness of
		  fault diagnosis, thus has a great impact on fault
		  diagnosis. However, its knowledge source and usage have not
		  been analyzed comprehensively to give us a deep insight
		  into this area yet. This paper investigates and studies the
		  ontology-based fault diagnosis during the past decade, from
		  the perspective of knowledge source and usage of fault
		  diagnosis ontology. In summary, it can be seen that
		  combining ontology with other methods improves the
		  efficiency and the effect of fault diagnosis, and
		  ontology-based fault diagnosis needs to be integrated into
		  the whole process of fault diagnosis. To make knowledge of
		  fault diagnosis complete, machine learning shall be used
		  for the ontological engineering.},
  booktitle	= {Proceedings of the 11th International Conference on
		  Computer Modeling and Simulation},
  pages		= {112–116},
  numpages	= {5},
  keywords	= {Fault diagnosis, Knowledge-based system, Ontology},
  location	= {North Rockhampton, QLD, Australia},
  series	= {ICCMS '19}
}

@Article{	  10.1145/3665252.3665263,
  author	= {Fan, Ju and Tu, Jianhong and Li, Guoliang and Wang, Peng
		  and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song and Tang,
		  Nan},
  title		= {Unicorn: A Unified Multi-Tasking Matching Model},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {1},
  issn		= {0163-5808},
  url		= {https://doi.org/10.1145/3665252.3665263},
  doi		= {10.1145/3665252.3665263},
  abstract	= {Data matching, which decides whether two data elements
		  (e.g., string, tuple, column, or knowledge graph entity)
		  are the "same" (a.k.a. a match), is a key concept in data
		  integration. The widely used practice is to build
		  task-specific or even dataset-specific solutions, which are
		  hard to generalize and disable the opportunities of
		  knowledge sharing that can be learned from different
		  datasets and multiple tasks. In this paper, we propose
		  Unicorn, a unified model for generally supporting common
		  data matching tasks. Building such a unified model is
		  challenging due to heterogeneous formats of input data
		  elements and various matching semantics of multiple tasks.
		  To address the challenges, Unicorn employs one generic
		  Encoder that converts any pair of data elements (a, b) into
		  a learned representation, and uses a Matcher, which is a
		  binary classifier, to decide whether a matches b. To align
		  matching semantics of multiple tasks, Unicorn adopts a
		  mixture-of-experts model that enhances the learned
		  representation into a better representation. We conduct
		  extensive experiments using 20 datasets on 7 well-studied
		  data matching tasks, and find that our unified model can
		  achieve better performance on most tasks and on average,
		  compared with the state-of-the-art specific models trained
		  for ad-hoc tasks and datasets separately. Moreover, Unicorn
		  can also well serve new matching tasks with zero-shot
		  learning.},
  journal	= {SIGMOD Rec.},
  month		= may,
  pages		= {44–53},
  numpages	= {10}
}

@Article{	  10.1145/3453475,
  author	= {Dwivedi, Vimal and Pattanaik, Vishwajeet and Deval, Vipin
		  and Dixit, Abhishek and Norta, Alex and Draheim, Dirk},
  title		= {Legally Enforceable Smart-Contract Languages: A Systematic
		  Literature Review},
  year		= {2021},
  issue_date	= {June 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {5},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3453475},
  doi		= {10.1145/3453475},
  abstract	= {Smart contracts are a key component of today’s
		  blockchains. They are critical in controlling decentralized
		  autonomous organizations (DAO). However, smart contracts
		  are not yet legally binding nor enforceable; this makes it
		  difficult for businesses to adopt the DAO paradigm.
		  Therefore, this study reviews existing Smart Contract
		  Languages (SCL) and identifies properties that are critical
		  to any future SCL for drafting legally binding contracts.
		  This is achieved by conducting a Systematic Literature
		  Review (SLR) of white- and grey literature published
		  between 2015 and 2019. Using the SLR methodology, 45
		  Selected and 28 Supporting Studies detailing 45
		  state-of-the-art SCLs are selected. Finally, 10 SCL
		  properties that enable legally compliant DAOs are
		  discovered, and specifications for developing SCLs are
		  explored.},
  journal	= {ACM Comput. Surv.},
  month		= jun,
  articleno	= {110},
  numpages	= {34},
  keywords	= {Blockchain, decentralized autonomous organization,
		  expressiveness, smart contract language, suitability,
		  systematic literature review}
}

@Article{	  10.1145/3517336,
  author	= {Kumar, Amit and Esmaili, Nazanin and Piccardi, Massimo},
  title		= {Neural Topic Model Training with the REBAR Gradient
		  Estimator},
  year		= {2022},
  issue_date	= {September 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3517336},
  doi		= {10.1145/3517336},
  abstract	= {Topic modelling is an important approach of unsupervised
		  machine learning that allows automatically extracting the
		  main “topics” from large collections of documents. In
		  addition, topic modelling is able to identify the topic
		  proportions of each individual document, which can be
		  helpful for organizing the collections. Many topic
		  modelling algorithms have been proposed to date, including
		  several that leverage advanced techniques such as
		  variational inference and deep autoencoders. However, to
		  date topic modelling has made limited use of reinforcement
		  learning, a framework that has obtained vast success in
		  many other unsupervised learning tasks. For this reason, in
		  this article we propose training a neural topic model using
		  a reinforcement learning objective and minimizing the
		  objective with the recently-proposed REBAR gradient
		  estimator. Experiments performed over two probing datasets
		  have shown that the proposed model has achieved
		  improvements over all the compared models in terms of both
		  model perplexity and topic coherence, and produced topics
		  that appear qualitatively informative and consistent.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {107},
  numpages	= {18},
  keywords	= {Topic models, deep neural networks, variational
		  autoencoders, variational-autoencoder topic models,
		  reinforcement learning, REBAR}
}

@InProceedings{	  10.1145/3539618.3591753,
  author	= {Jiang, Hao and Li, Chuanzhen and Cai, Juanjuan and Wang,
		  Jingling},
  title		= {RCENR: A Reinforced and Contrastive Heterogeneous Network
		  Reasoning Model for Explainable News Recommendation},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591753},
  doi		= {10.1145/3539618.3591753},
  abstract	= {Existing news recommendation methods suffer from sparse
		  and weak interaction data, leading to reduced effectiveness
		  and explainability. Knowledge reasoning, which explores
		  inferential trajectories in the knowledge graph, can
		  alleviate data sparsity and provide explicitly recommended
		  explanations. However, brute-force pre-processing
		  approaches used in conventional methods are not suitable
		  for fast-changing news recommendation. Therefore, we
		  propose an explainable news recommendation model: the
		  Reinforced and Contrastive Heterogeneous Network Reasoning
		  Model for Explainable News Recommendation (RCENR),
		  consisting of NHN-R2 and MR&amp;CO frameworks. The NHN-R2
		  framework generates user/news subgraphs to enhance
		  recommendation and extend the dimensions and diversity of
		  reasoning. The MR&amp;CO framework incorporates contrastive
		  learning with a reinforcement-based strategy for
		  self-supervised and efficient model training. Experiments
		  on the MIND dataset show that RCENR is able to improve
		  recommendation accuracy and provide diverse and credible
		  explanations.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1710–1720},
  numpages	= {11},
  keywords	= {contrastive learning, explainable recommendation,
		  knowledge reasoning, markov decision process, news
		  recommendation},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Article{	  10.1145/3274410,
  author	= {Piscopo, Alessandro and Simperl, Elena},
  title		= {Who Models the World? Collaborative Ontology Creation and
		  User Roles in Wikidata},
  year		= {2018},
  issue_date	= {November 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {CSCW},
  url		= {https://doi.org/10.1145/3274410},
  doi		= {10.1145/3274410},
  abstract	= {Wikidata is a collaborative knowledge graph which is
		  central to many academic and industry IT projects. Its
		  users are responsible for maintaining the schema that
		  organises this knowledge into classes, properties, and
		  attributes, which together form the Wikidata 'ontology'. In
		  this paper, we study the relationship between different
		  Wikidata user roles and the quality of the Wikidata
		  ontology. To do so we first propose a framework to evaluate
		  the ontology as it evolves. We then cluster editing
		  activities to identify user roles in monthly time frames.
		  Finally, we explore how each role impacts the ontology. Our
		  analysis shows that the Wikidata ontology has uneven
		  breadth and depth. We identified two user roles:
		  contributors and leaders. The second category is positively
		  associated to ontology depth, with no significant effect on
		  other features. Further work should investigate other
		  dimensions to define user profiles and their influence on
		  the knowledge graph.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= nov,
  articleno	= {141},
  numpages	= {18},
  keywords	= {collaborative knowledge engineering, ontologies, user
		  roles, wikidata}
}

@Proceedings{	  10.1145/3624486,
  title		= {eSAAM '23: Proceedings of the 3rd Eclipse Security, AI,
		  Architecture and Modelling Conference on Cloud to Edge
		  Continuum},
  year		= {2023},
  isbn		= {9798400708350},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Ludwigsburg, Germany}
}

@InProceedings{	  10.1145/3543873.3587645,
  author	= {Meyer zum Felde, Hendrik and Kollenstart, Maarten and
		  Bellebaum, Thomas and Dalmolen, Simon and Brost, Gerd},
  title		= {Extending Actor Models in Data Spaces},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587645},
  doi		= {10.1145/3543873.3587645},
  abstract	= {In today’s internet almost any party can share sets of
		  data with each other. However, creating frameworks and
		  regulated realms for the sharing of data is very complex
		  when multiple parties are involved and complicated
		  regulation comes into play. As solution data spaces were
		  introduced to enable participating parties to share data
		  among themselves in an organized, regulated and
		  standardized way. However, contract data processors, acting
		  as data space participants, are currently unable to execute
		  data requests on behalf of their contract partners. Here we
		  show that an on-behalf-of actor model can be easily added
		  to existing data spaces. We demonstrate how this extension
		  can be realized using verifiable credentials. We provide a
		  sample use case, a detailed sequence diagram and discuss
		  necessary architectural adaptations and additions to
		  established protocols. Using the extensions explained in
		  this work numerous real life use cases which previously
		  could technically not be realized can now be covered. This
		  enables future data spaces to provide more dynamic and
		  complex real world use cases.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {1447–1451},
  numpages	= {5},
  keywords	= {Actor Models, Contract Data Processing, Data Spaces,
		  Gaia-X, International Data Spaces, On-behalf-of Model,
		  Self-Sovereign Identities},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@Article{	  10.1145/3696427,
  author	= {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla,
		  Sandeep Kumar},
  title		= {TTPXHunter: Actionable Threat Intelligence Extraction as
		  TTPs from Finished Cyber Threat Reports},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {4},
  url		= {https://doi.org/10.1145/3696427},
  doi		= {10.1145/3696427},
  abstract	= {Understanding the modus operandi of adversaries aids
		  organizations to employ efficient defensive strategies and
		  share intelligence in the community. This knowledge is
		  often present in unstructured natural language text within
		  threat analysis reports. A translation tool is needed to
		  interpret the modus operandi explained in the sentences of
		  the threat report and convert it into a structured format.
		  This research introduces a methodology named TTPXHunter for
		  automated extraction of threat intelligence in terms of
		  Tactics, Techniques, and Procedures (TTPs) from finished
		  cyber threat reports. It leverages cyber domain-specific
		  state-of-the-art natural language model to augment
		  sentences for minority class TTPs and refine pinpointing
		  the TTPs in threat analysis reports significantly. We
		  create two datasets: an augmented sentence-TTP dataset of
		  (39,296) sentence samples and a (149) real-world cyber
		  threat intelligence report-to-TTP dataset. Further, we
		  evaluate TTPXHunter on the augmented sentence and report
		  datasets. The TTPXHunter achieves the highest performance
		  of (92.42\%) f1-score on the augmented dataset, and it also
		  outperforms existing state-of-the-art TTP extraction method
		  by achieving an f1-score of (97.09\%) when evaluated over
		  the report dataset. TTPXHunter significantly improves
		  cybersecurity threat intelligence by offering quick,
		  actionable insights into attacker behaviors. This
		  advancement automates threat intelligence analysis and
		  provides a crucial tool for cybersecurity professionals to
		  combat cyber threats.},
  journal	= {Digital Threats},
  month		= dec,
  articleno	= {37},
  numpages	= {19},
  keywords	= {Threat Intelligence, TTP Extraction, MITRE ATT&amp;CK,
		  Natural Language Processing, Threat Intelligence
		  Extraction, TTP Classification, Cyber Security and AI,
		  Cyber Security Threats, NLP, Cybersecurity}
}

@Article{	  10.1109/taslp.2023.3267832,
  author	= {Thulke, David and Daheim, Nico and Dugast, Christian and
		  Ney, Hermann},
  title		= {Task-Oriented Document-Grounded Dialog Systems by
		  HLTPR@RWTH for DSTC9 and DSTC10},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3267832},
  doi		= {10.1109/TASLP.2023.3267832},
  abstract	= {This paper summarizes our contributions to the
		  document-grounded dialog tasks at the 9th and 10th Dialog
		  System Technology Challenges (DSTC9 and DSTC10). In both
		  iterations the task consists of three subtasks: first
		  detect whether the current turn is knowledge seeking,
		  second select a relevant knowledge document, and third
		  generate a response grounded on the selected document. For
		  DSTC9 we proposed different approaches to make the
		  selection task more efficient. The best method,
		  Hierarchical Selection, actually improves the results
		  compared to the original baseline and gives a speedup of
		  24x. In the DSTC10 iteration of the task, the challenge was
		  to adapt systems trained on written dialogs to perform well
		  on noisy automatic speech recognition transcripts.
		  Therefore, we proposed data augmentation techniques to
		  increase the robustness of the models as well as methods to
		  adapt the style of generated responses to fit well into the
		  proceeding dialog. Additionally, we proposed a noisy
		  channel model that allows for increasing the factuality of
		  the generated responses. In addition to summarizing our
		  previous contributions, in this work, we also report on a
		  few small improvements and reconsider the automatic
		  evaluation metrics for the generation task which have shown
		  a low correlation to human judgments.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= apr,
  pages		= {733–741},
  numpages	= {9}
}

@InProceedings{	  10.1145/3335656.3335701,
  author	= {Pan, Jiabin and Mou, Naixia and Liu, Wenbao},
  title		= {Emotion Analysis of Tourists Based on Domain Ontology},
  year		= {2019},
  isbn		= {9781450360906},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3335656.3335701},
  doi		= {10.1145/3335656.3335701},
  abstract	= {The big data of tourism has exploded with the rapid
		  development of social media, providing a new data source
		  for the emotion analysis of tourism. Based on online
		  comments, this paper proposes an emotion analysis model
		  that combines tourism domain ontology and semantic-based
		  method to mine the fine-grained emotion of tourists and
		  designs specific formulas to quantify the emotion of
		  tourists. Finally, the Palace Museum is used as an example
		  to verify the validity of the model. The analysis results
		  show that: 1) Tourists pay more attention to the attributes
		  such as "scenery", "tourist flow", "ticket", etc. in their
		  travel activities. 2) The emotional score of the attributes
		  such as "lodging environment", "scenery", "culture",
		  "environment quality", etc. are higher, but the attributes
		  such as "safety", "tourist flow", "toilet" and cost-related
		  attributes are lower. The main reasons are: "low security",
		  "massive tourists", "less and small toilets" and "high
		  costs". 3) Due to the excessive number of tourists during
		  the holiday, which leads poor travel experience to the
		  tourists, the emotional score of tourists are lower in the
		  5th, 7th, 8th and 10th months. The analysis results can
		  provide reference for tourists' travel decisions and the
		  development and optimization of tourism.},
  booktitle	= {Proceedings of the 2019 International Conference on Data
		  Mining and Machine Learning},
  pages		= {146–150},
  numpages	= {5},
  keywords	= {Domain ontology, Emotion analysis, Online comment,
		  Tourism, the Palace Museum},
  location	= {Hong Kong, Hong Kong},
  series	= {ICDMML 2019}
}

@InProceedings{	  10.1145/3584371.3612950,
  author	= {Le, Nguyen Quoc Khanh and Kha, Quang Hien},
  title		= {A Sequence-Based Prediction Model of Vesicular Transport
		  Proteins Using Ensemble Deep Learning},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3612950},
  doi		= {10.1145/3584371.3612950},
  abstract	= {This study aims to employ computational methods for the
		  accurate identification of vesicular transport proteins.
		  The identification of these proteins holds great
		  significance in enhancing our understanding of their
		  protein family structure, thereby enabling the design of
		  more effective drug targets for individuals afflicted with
		  endocrine disorders. In recent times, researchers in the
		  field of biology have increasingly sought to leverage deep
		  learning techniques to address this challenge. In order to
		  further enhance the classification performance, we
		  investigated the following models incorporating distinct
		  features: (1) We devised a novel protein feature called
		  AAC_PSSM by amalgamating amino acid composition (AAC) and
		  position-specific scoring matrix (PSSM) features.
		  Subsequently, a gated recurrent unit (GRU) model was
		  employed to learn such features; (2) An ensemble model was
		  constructed by combining the existing GRU model with the
		  model of a neural network featuring the AAC feature; (3)
		  Random forest analysis was conducted using the pseudo-amino
		  acid composition (PseAAC) feature; (4) Furthermore, we
		  explored a natural language processing (NLP) approach by
		  considering the protein sequence as a natural language and
		  applying various neural network architectures. Upon
		  analyzing the results obtained from the different models,
		  it was observed that the ensemble model incorporating PSSM
		  and AAC features exhibited the highest sensitivity of
		  81.03\% and accuracy of 82.43\%. Notably, our proposed
		  model surpassed the performance of state-of-the-art models
		  addressing the same problem and datasets, thus establishing
		  its superiority.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {106},
  numpages	= {6},
  keywords	= {deep learning, nesemble learning, gate recurrent unit,
		  position-specific scoring matrix, protein sequence,
		  vesicular transport},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1145/3584371.3613056,
  author	= {Kalbasi, Reza and Nickerson, David and Atalag, Koray},
  title		= {PhysioMed: A Semantic Web based Framework for Linking
		  Healthcare Information with Computational Physiology
		  Models},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3613056},
  doi		= {10.1145/3584371.3613056},
  abstract	= {This research focuses on the integration of computational
		  physiology models and Electronic Health Records (EHR) to
		  improve the accuracy and applicability of computational
		  models in clinical settings. Computational physiology
		  models provide quantified insights into biological
		  processes, while EHRs store digital health information. By
		  combining these two domains, the research aims to create a
		  foundation for personalized and predictive clinical
		  decision support systems.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {85},
  numpages	= {1},
  keywords	= {semantic web, health informatics, biomedical ontology,
		  collaborative ontology development},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1145/3412841.3442057,
  author	= {Vsesviatska, Oleksandra and Tietz, Tabea and Hoppe, Fabian
		  and Sprau, Mirjam and Meyer, Nils and Dess\`{\i}, Danilo
		  and Sack, Harald},
  title		= {ArDO: an ontology to describe the dynamics of multimedia
		  archival records},
  year		= {2021},
  isbn		= {9781450381048},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3412841.3442057},
  doi		= {10.1145/3412841.3442057},
  abstract	= {Cultural heritage institutions store and digitize large
		  amounts of multimedia data inside archives to make archival
		  records findable by archivists, scientists, and general
		  public. Cataloging standards vary from archive to archive
		  and, therefore, the sharing and use of this data are
		  limited. To solve this issue, linked open data (LOD) is
		  rising as an essential paradigm to open and provide access
		  to the archival resources. Archives which are opened to the
		  world knowledge benefit from external connections by
		  enabling the application of automated approaches to process
		  archival records, helping all stakeholders to gain valuable
		  insights. In this paper, we present the Archive Dynamics
		  Ontology (ArDO) - an ontology designed for describing the
		  hierarchical nature of archival multimedia data, as well as
		  its application on the example of archival resources about
		  the Weimar Republic. Furthermore, ArDO semantically
		  organizes multimedia archival resources in form of texts,
		  images, audios, and videos by representing the dynamics
		  related to their classification over time. ArDO tracks the
		  changes of a specific hierarchical classification schema
		  referred to as systematics adopted to organize archival
		  resources under semantically defined keywords.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on Applied
		  Computing},
  pages		= {1855–1863},
  numpages	= {9},
  keywords	= {multimedia archive, ontology design, ontology dynamics},
  location	= {Virtual Event, Republic of Korea},
  series	= {SAC '21}
}

@InProceedings{	  10.1145/3503162.3503170,
  author	= {Mor, Annu and Kumar, Mukesh and Chaudhury, Santanu},
  title		= {Smart City Umbrella Ontology :Context -Driven Framework
		  For Traffic Planning},
  year		= {2022},
  isbn		= {9781450395960},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503162.3503170},
  doi		= {10.1145/3503162.3503170},
  abstract	= {Presently, huge public and private data sets are available
		  from different government sources regarding transportation
		  services in modern cities.The heterogeneous data-sets
		  address day- to- day operations in transportation
		  research,and these challenges can be effectively done with
		  assistance of ontologies. Ontology is used for accessing
		  and exploiting data from sensors to surveys in a
		  semantically inter-operable way is demanding,in recent
		  years. In this paper, an ontological framework is designed
		  for smart cities by integration of entities such as surface
		  transport network,road geometry, topology,and traffic
		  monitoring sensors.The Smart City Umbrella Ontology (SCUO)
		  is designed using standardized traffic guidelines in the
		  context of Indian surface transportation.The paper focuses
		  on ontology design by argumentation of road network
		  nomenclature and relationship among entities.A mechanism
		  for linking ontologies is implemented through Resource
		  Description Framework(RDF) with a set of relations.The
		  proposed framework can be used to provide services to
		  commuters via specific applications in intelligent system
		  of transportation by administration,and city stakeholders
		  for design,policy-making purposes of traffic planning.},
  booktitle	= {Proceedings of the 13th Annual Meeting of the Forum for
		  Information Retrieval Evaluation},
  pages		= {83–90},
  numpages	= {8},
  keywords	= {Transportation ontologies, linking ontologies, resource
		  description framework, smart city umbrella ontology},
  location	= {Virtual Event, India},
  series	= {FIRE '21}
}

@InProceedings{	  10.1145/3417990.3418744,
  author	= {Gogolla, Martin and Selic, Bran},
  title		= {On teaching descriptive and prescriptive modeling},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3418744},
  doi		= {10.1145/3417990.3418744},
  abstract	= {Models may be used for purposes relating (a) to
		  understanding, predicting, and communicating model aspects,
		  and (b) to implementing the model and capturing the design
		  intent. Models that are primarily used for understanding,
		  predicting and communicating are referred to as descriptive
		  models, whereas models mainly used for implementation are
		  called prescriptive models. This contribution focuses on
		  teaching both the common and the distinguishing aspects of
		  the two model categories. We start with an example for a
		  general descriptive and prescriptive model, independent of
		  particular software modeling languages, and continue to
		  discuss an example demonstrating how UML and OCL can be
		  applied for specifying both a descriptive and a
		  prescriptive model. Finally, we discuss essentials to be
		  learned from this teaching venture.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {23},
  numpages	= {9},
  keywords	= {UML and OCL model, descriptive model, prescriptive model},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@InProceedings{	  10.1145/3391274.3393636,
  author	= {Manda, Prashanti and SayedAhmed, Saed and Mohanty, Somya
		  D.},
  title		= {Automated ontology-based annotation of scientific
		  literature using deep learning},
  year		= {2020},
  isbn		= {9781450379748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3391274.3393636},
  doi		= {10.1145/3391274.3393636},
  abstract	= {Representing scientific knowledge using ontologies enables
		  data integration, consistent machine-readable data
		  representation, and allows for large-scale computational
		  analyses. Text mining approaches that can automatically
		  process and annotate scientific literature with ontology
		  concepts are necessary to keep up with the rapid pace of
		  scientific publishing. Here, we present deep learning
		  models (Gated Recurrent Units (GRU) and Long Short Term
		  Memory (LSTM)) combined with different input encoding
		  formats for automated Named Entity Recognition (NER) of
		  ontology concepts from text. The Colorado Richly Annotated
		  Full Text (CRAFT) gold standard corpus was used to train
		  and test our models. Precision, Recall, F-1, and Jaccard
		  semantic similarity were used to evaluate the performance
		  of the models. We found that GRU-based models outperform
		  LSTM models across all evaluation metrics. Surprisingly,
		  considering the top two probabilistic predictions of the
		  model for each instance instead of the top one resulted in
		  a substantial increase in accuracy. Inclusion of ontology
		  semantics via subsumption reasoning yielded modest
		  performance improvement.},
  booktitle	= {Proceedings of The International Workshop on Semantic Big
		  Data},
  articleno	= {5},
  numpages	= {6},
  keywords	= {automated annotation, deep learning, named entity
		  recognition, ontologies},
  location	= {Portland, Oregon},
  series	= {SBD '20}
}

@InProceedings{	  10.1145/3486622.3493933,
  author	= {Rajaonarivo, Landy and Mine, Tsunenori and Arakawa,
		  Yutaka},
  title		= {Automatic Generation of Event Ontology from Social Network
		  and Mobile Positioning Data},
  year		= {2022},
  isbn		= {9781450391153},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486622.3493933},
  doi		= {10.1145/3486622.3493933},
  abstract	= {The study of mobile positioning data makes it possible to
		  detect whether an event has happened at a particular place
		  during a given period. However, determining the nature and
		  details of the event is a challenge, especially if the
		  event is not widely known, as is the case for local events.
		  We propose an approach to determining the nature of local
		  events by generating an ontology in a completely automatic
		  way from social network data and data on people’s
		  movements and by querying this generated ontology. This
		  approach uses entity discovery techniques, filtering
		  systems and information enrichment via Open Data, as well
		  as a system for matching discovered entities and ontology
		  elements. Evaluation via a survey allowed us to validate
		  approximately that the information presented in the
		  ontology is reliable, makes sense and answers our
		  questions.},
  booktitle	= {IEEE/WIC/ACM International Conference on Web Intelligence
		  and Intelligent Agent Technology},
  pages		= {87–94},
  numpages	= {8},
  keywords	= {automatic ontology generation, data mining, event
		  ontology, recommendation engine, social network data},
  location	= {Melbourne, VIC, Australia},
  series	= {WI-IAT '21}
}

@InProceedings{	  10.1145/3318396.3318442,
  author	= {Ul Haq, Sami and Qamar, Usman},
  title		= {Ontology Based Test Case Generation for Black Box
		  Testing},
  year		= {2019},
  isbn		= {9781450362672},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318396.3318442},
  doi		= {10.1145/3318396.3318442},
  abstract	= {Software systems are not considered complete unless
		  properly tested and verified. In existing literature, a
		  growing interest on establishment of automated testing
		  techniques has been observed. However, tedious manual
		  process of test case generation largely depends upon domain
		  knowledge and formalized representation of user
		  requirements. The advent of semantic web engineering has
		  led the foundation for developing ontologies as a mean to
		  express information and knowledge semantics regarding
		  particular domain efficiently. In software testing,
		  ontologies can be significantly helpful to automate testing
		  phase as they encode domain knowledge in machine
		  interpretable format. We have proposed automatic test case
		  generation framework that involves ontology-based
		  requirement specification and learning based methods for
		  conducting black box testing. Our approach integrates
		  knowledge-based system (ontology) with learning-based
		  testing algorithm to automate: generation of test cases,
		  test execution and test verdict construction. Proposed
		  framework includes, requirement ontology to formalize
		  requirement specification, Dialogue Manager that enables
		  selection of available test cases and Learning Based
		  Testing to generate counter examples of test cases through
		  system learning. The contribution of this paper is to
		  enable 1) requirement elicitation and specification using
		  ontologies 2) test data selection from existing ontologies
		  and 3) automatic test case generation from existing test
		  cases. To represent the applicability of this research,
		  ontology for requirement elicitation and specification is
		  developed. Framework proposed in this research paper is an
		  effort to provide software testing tools to save time, cost
		  and efforts during test design phase.},
  booktitle	= {Proceedings of the 2019 8th International Conference on
		  Educational and Information Technology},
  pages		= {236–241},
  numpages	= {6},
  keywords	= {Black box testing, Learning based testing, Ontologies,
		  Requirement Engineering, Test case generation},
  location	= {Cambridge, United Kingdom},
  series	= {ICEIT 2019}
}

@Article{	  10.1145/3354584,
  author	= {Lilis, Yannis and Savidis, Anthony},
  title		= {A Survey of Metaprogramming Languages},
  year		= {2019},
  issue_date	= {November 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3354584},
  doi		= {10.1145/3354584},
  abstract	= {Metaprogramming is the process of writing computer
		  programs that treat programs as data, enabling them to
		  analyze or transform existing programs or generate new
		  ones. While the concept of metaprogramming has existed for
		  several decades, activities focusing on metaprogramming
		  have been increasing rapidly over the past few years, with
		  most languages offering some metaprogramming support and
		  the amount of metacode being developed growing
		  exponentially. In this article, we introduce a taxonomy of
		  metaprogramming languages and present a survey of
		  metaprogramming languages and systems based on the
		  taxonomy. Our classification is based on the
		  metaprogramming model adopted by the language, the phase of
		  the metaprogram evaluation, the metaprogram source
		  location, and the relation between the metalanguage and the
		  object language.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {113},
  numpages	= {39},
  keywords	= {Metaprogramming, aspect-oriented programming, generative
		  programming, macro systems, meta-object protocols,
		  multistage languages, reflection}
}

@InProceedings{	  10.1145/3711896.3737221,
  author	= {Shi, Binpeng and Luo, Yu and Wang, Jingya and Zhao,
		  Yongxin and Zhang, Shenglin and Hao, Bowen and Zhao, Chenyu
		  and Sun, Yongqian and Zhang, Zhi and Sun, Ronghua and Li,
		  Haihua and Song, Wei and Chen, Xiaolong and Miao, Jingbo
		  and Pei, Dan},
  title		= {FlowXpert: Expertizing Troubleshooting Workflow
		  Orchestration with Knowledge Base and Multi-Agent
		  Coevolution},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3737221},
  doi		= {10.1145/3711896.3737221},
  abstract	= {Incident management remains a critical yet challenging
		  task for large-scale cloud services. Most cloud service
		  providers abstract troubleshooting into predefined
		  workflows for different incidents, offering step-by-step
		  guidance. However, manually crafting workflows is
		  resource-consuming and knowledge-intensive, hindering
		  large-scale deployment. Most automated techniques for
		  workflow orchestration rely on large language models (LLMs)
		  to handle complex tasks but overlook key aspects of
		  troubleshooting, including complex expertise, domain
		  requirements, and the reliability of AI feedback. These
		  limitations undermine workflow quality. Therefore, we
		  propose FlowXpert, a novel framework for troubleshooting
		  workflow orchestration. Leveraging LLMs, it first builds a
		  knowledge base centered on incident-aware nodes to
		  precisely depict expertise. Then, fed into AI feedback and
		  synthetic preference data, reinforcement learning is
		  applied to refine the workflow generator and evaluator. To
		  assess troubleshooting workflows, we introduce OpsFlowBench
		  based on Huawei Cloud's datacenter switch operation
		  documents. Benchmark tests under the tailored STEPScore
		  metric validate its effectiveness. Furthermore, during a
		  10-week deployment in Huawei Cloud's datacenter network,
		  FlowXpert provided valuable support to both on-call
		  engineers and AI executors, as evidenced by empirical data
		  and case study.},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {4839–4850},
  numpages	= {12},
  keywords	= {incident management, large language model,
		  troubleshooting, workflow orchestration},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@InProceedings{	  10.1145/3276604.3276623,
  author	= {Coulon, Fabien and Degueule, Thomas and van der Storm,
		  Tijs and Combemale, Benoit},
  title		= {Shape-diverse DSLs: languages without borders (vision
		  paper)},
  year		= {2018},
  isbn		= {9781450360296},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3276604.3276623},
  doi		= {10.1145/3276604.3276623},
  abstract	= {Domain-Specific Languages (DSLs) manifest themselves in
		  remarkably diverse shapes, ranging from internal DSLs
		  embedded as a mere fluent API within a programming
		  language, to external DSLs with dedicated syntax and tool
		  support. Although different shapes have different pros and
		  cons, combining them for a single language is
		  problematic:&nbsp;language designers usually commit to a
		  particular shape early in the design process, and it is
		  hard to reconsider this choice later. In this new ideas
		  paper, we envision a language engineering approach enabling
		  (i) language users to manipulate language constructs in the
		  most appropriate shape according to the task at hand, and
		  (ii) language designers to combine the strengths of
		  different technologies for a single DSL. We report on early
		  experiments and lessons learned building , our prototype
		  approach to this problem. We illustrate its applicability
		  in the engineering of a simple shape-diverse DSL
		  implemented conjointly in Rascal, EMF, and Java. We hope
		  that our initial contribution will raise the awareness of
		  the community and encourage future research.},
  booktitle	= {Proceedings of the 11th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  pages		= {215–219},
  numpages	= {5},
  keywords	= {domain-specific language, shape-diverse dsl},
  location	= {Boston, MA, USA},
  series	= {SLE 2018}
}

@InProceedings{	  10.1145/3292500.3330838,
  author	= {Hao, Junheng and Chen, Muhao and Yu, Wenchao and Sun,
		  Yizhou and Wang, Wei},
  title		= {Universal Representation Learning of Knowledge Bases by
		  Jointly Embedding Instances and Ontological Concepts},
  year		= {2019},
  isbn		= {9781450362016},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3292500.3330838},
  doi		= {10.1145/3292500.3330838},
  abstract	= {Many large-scale knowledge bases simultaneously represent
		  two views of knowledge graphs (KGs): an ontology view for
		  abstract and commonsense concepts, and an instance view for
		  specific entities that are instantiated from ontological
		  concepts. Existing KG embedding models, however, merely
		  focus on representing one of the two views alone. In this
		  paper, we propose a novel two-view KG embedding model,
		  JOIE, with the goal to produce better knowledge embedding
		  and enable new applications that rely on multi-view
		  knowledge. JOIE employs both cross-view and intra-view
		  modeling that learn on multiple facets of the knowledge
		  base. The cross-view association model is learned to bridge
		  the embeddings of ontological concepts and their
		  corresponding instance-view entities. The intra-view models
		  are trained to capture the structured knowledge of instance
		  and ontology views in separate embedding spaces, with a
		  hierarchy-aware encoding technique enabled for ontologies
		  with hierarchies. We explore multiple representation
		  techniques for the two model components and investigate
		  with nine variants of JOIE. Our model is trained on
		  large-scale knowledge bases that consist of massive
		  instances and their corresponding ontological concepts
		  connected via a (small) set of cross-view links.
		  Experimental results on public datasets show that the best
		  variant of JOIE significantly outperforms previous models
		  on instance-view triple prediction task as well as ontology
		  population on ontology-view KG. In addition, our model
		  successfully extends the use of KG embeddings to entity
		  typing with promising performance.},
  booktitle	= {Proceedings of the 25th ACM SIGKDD International
		  Conference on Knowledge Discovery \&amp; Data Mining},
  pages		= {1709–1719},
  numpages	= {11},
  keywords	= {knowledge graph, ontology learning, relational
		  embeddings},
  location	= {Anchorage, AK, USA},
  series	= {KDD '19}
}

@InProceedings{	  10.1145/3442391.3442407,
  author	= {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o},
		  Robert},
  title		= {Validating Feature Models With Respect to Textual Product
		  Line Specifications},
  year		= {2021},
  isbn		= {9781450388245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442391.3442407},
  doi		= {10.1145/3442391.3442407},
  abstract	= {Feature models (FM) are a valuable resource in the
		  analysis of software product lines (SPL). They provide a
		  visual abstraction of the variation points in a family of
		  related software products. FMs can be manually created by
		  domain experts or extracted (semi-) automatically from
		  textual documents such as product descriptions or
		  requirements specifications. Nevertheless, there is no way
		  to measure the accuracy of a FM with respect to the
		  information described in the source documents. This paper
		  proposes a method to quantify and visualize whether the
		  elements in a FM (features and relationships) conform to
		  the information available in a set of specification
		  documents. Both the correctness (choice of representative
		  elements) and completeness (no missing elements) of the FM
		  are considered. Designers can use this feedback to fix
		  defects in the FM or to detect incomplete or inconsistent
		  information in the source documents.},
  booktitle	= {Proceedings of the 15th International Working Conference
		  on Variability Modelling of Software-Intensive Systems},
  articleno	= {15},
  numpages	= {10},
  keywords	= {Feature Model Validation, Machine Learning, Natural
		  Language Processing, Requirements Engineering, Software
		  Product Line},
  location	= {Krems, Austria},
  series	= {VaMoS '21}
}

@InProceedings{	  10.1145/3677052.3698671,
  author	= {Sarmah, Bhaskarjit and Mehta, Dhagash and Hall, Benika and
		  Rao, Rohan and Patel, Sunil and Pasquali, Stefano},
  title		= {HybridRAG: Integrating Knowledge Graphs and Vector
		  Retrieval Augmented Generation for Efficient Information
		  Extraction},
  year		= {2024},
  isbn		= {9798400710810},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677052.3698671},
  doi		= {10.1145/3677052.3698671},
  abstract	= {Extraction and interpretation of intricate information
		  from unstructured text data arising in financial
		  applications, such as earnings call transcripts, present
		  substantial challenges to large language models (LLMs) even
		  using the current best practices to use Retrieval Augmented
		  Generation (RAG) (referred to as VectorRAG techniques which
		  utilize vector databases for information retrieval) due to
		  challenges such as domain specific terminology and complex
		  formats of the documents. We introduce a novel approach
		  based on a combination, called HybridRAG, of the Knowledge
		  Graphs (KGs) based RAG techniques (called GraphRAG) and
		  VectorRAG techniques to enhance question-answer (Q&amp;A)
		  systems for information extraction from financial documents
		  that is shown to be capable of generating accurate and
		  contextually relevant answers. Using experiments on a set
		  of financial earning call transcripts documents which come
		  in the form of Q&amp;A format, and hence provide a natural
		  set of pairs of ground-truth Q&amp;As, we show that
		  HybridRAG which retrieves context from both vector database
		  and KG outperforms both traditional VectorRAG and GraphRAG
		  individually when evaluated at both the retrieval and
		  generation stages in terms of retrieval accuracy and answer
		  generation. The proposed technique has applications beyond
		  the financial domain.},
  booktitle	= {Proceedings of the 5th ACM International Conference on AI
		  in Finance},
  pages		= {608–616},
  numpages	= {9},
  location	= {Brooklyn, NY, USA},
  series	= {ICAIF '24}
}

@InProceedings{	  10.1145/3573428.3573599,
  author	= {Zhang, Luyi and Li, Ren and Xiao, Qiao},
  title		= {A Prompt-based Few-shot Machine Reading Comprehension
		  Model for Intelligent Bridge Management},
  year		= {2023},
  isbn		= {9781450397148},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3573428.3573599},
  doi		= {10.1145/3573428.3573599},
  abstract	= {Bridge inspection reports are an important data source in
		  the bridge management process, and they contain a large
		  amount of fine-grained information. However, the research
		  on machine reading comprehension (MRC) methods for this
		  field is insufficient, and annotating large scale
		  domain-specific corpus is time-consuming. This paper
		  presented a novel prompt-based few-shot MRC approach for
		  intelligent bridge management. The proposed model uses the
		  pretrained model MacBERT as backbone. The prompt templates
		  are designed based on some domain-specific heuristic rules.
		  The experimental results show that our model outperforms
		  the baseline models in different few-shot settings. The
		  proposed model can provide technical support for the
		  construction of automatic question answering system in the
		  field of bridge management.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {946–950},
  numpages	= {5},
  keywords	= {Bridge inspection, Few-shot, Machine reading
		  comprehension, Prompt},
  location	= {Xiamen, China},
  series	= {EITCE '22}
}

@InProceedings{	  10.1109/jcdl.2019.00066,
  author	= {Wu, Mei Mei and Liu, Ying-Hsang},
  title		= {Exploring ontologies for collection protection in second
		  sino-japanese war},
  year		= {2020},
  isbn		= {9781728115474},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL.2019.00066},
  doi		= {10.1109/JCDL.2019.00066},
  abstract	= {The actions are usually trivial, implicit yet significant
		  for the protection of collection during the wartime.
		  Records of those actions, events, and particularly, the
		  related persons, organizations may be scattered but deserve
		  further attention since the records are precious to human
		  knowledge protection efforts. Searching, identifying and
		  collecting the related resources from the scattered data
		  sets is not as complicated as developing an ontology for
		  this domain of collection protection during the wartime
		  period. This study has selected 1939--41, in the Second
		  Sino-Japanese War during WWII when there were figures
		  working on the collection protection projects that have
		  been documented in the historical archives and many other
		  resources, including oral history, journals, digital
		  documentation, conference papers, literature reviews, etc.
		  The preliminary ontology models for the protection of
		  wartime period collection have been developed based on one
		  of the oral history. The current research has further tuned
		  and finalized the ontology of collection protection during
		  the wartime period by analyzing the multi-sets of data
		  resources. The implications of the research results are two
		  folds: firstly, it could shed lights on the classification
		  system for a digital library of this domain, and secondly,
		  it could benefit the historical researchers for providing
		  new clues in this line of collection protection during the
		  wartime period research.},
  booktitle	= {Proceedings of the 18th Joint Conference on Digital
		  Libraries},
  pages		= {351–352},
  numpages	= {2},
  keywords	= {collection protection, cultural heritage, digital
		  scholarship, ontologies},
  location	= {Champaign, Illinois},
  series	= {JCDL '19}
}

@Article{	  10.1145/3660639,
  author	= {Sharma, Mandar and Gogineni, Ajay Kumar and Ramakrishnan,
		  Naren},
  title		= {Neural Methods for Data-to-text Generation},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {5},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3660639},
  doi		= {10.1145/3660639},
  abstract	= {The neural boom that has sparked natural language
		  processing (NLP) research throughout the last decade has
		  similarly led to significant innovations in data-to-text
		  (D2T) generation. This survey offers a consolidated view
		  into the neural D2T paradigm with a structured examination
		  of the approaches, benchmark datasets, and evaluation
		  protocols. This survey draws boundaries separating D2T from
		  the rest of the natural language generation (NLG)
		  landscape, encompassing an up-to-date synthesis of the
		  literature, and highlighting the stages of technological
		  adoption from within and outside the greater NLG umbrella.
		  With this holistic view, we highlight promising avenues for
		  D2T research that focus not only on the design of
		  linguistically capable systems but also on systems that
		  exhibit fairness and accountability.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  articleno	= {89},
  numpages	= {46},
  keywords	= {Narration, data-to-text, data-to-text generation, natural
		  language generation}
}

@InProceedings{	  10.1145/3366030.3366097,
  author	= {Stenzer, Alexander},
  title		= {Query Relaxation using Spreading-Activation and
		  SKOS-Ontologies},
  year		= {2020},
  isbn		= {9781450371797},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366030.3366097},
  doi		= {10.1145/3366030.3366097},
  abstract	= {Digital libraries and archives adopting the Linked Open
		  Data (LOD) approach add descriptive metadata to the objects
		  stored in their inventory in order to facilitate searching
		  and sorting. In many cases the available metadata terms are
		  organized as a controlled vocabulary. Technically, the
		  controlled vocabularies are often provided in the form of
		  SKOS ontologies thus allowing to apply semantic web
		  technologies to establish inter-vocabulary relations and
		  ask queries.However, how can a search over the contents of
		  different digital libraries each of which relies on their
		  own vocabulary be performed without sacrificing recall or
		  having to align the underlying ontologies beforehand?In
		  this paper we present an approach based on query relaxation
		  to solving this problem. Considering the graph nature of
		  ontologies for controlled vocabularies we propose to use a
		  spreading activation algorithm to relax and subsequently
		  transform SPARQL queries in a way that makes them suitable
		  for other vocabularies.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {330–334},
  numpages	= {5},
  keywords	= {Query, Relaxation, SKOS, Spreading-Activation,
		  Transformation},
  location	= {Munich, Germany},
  series	= {iiWAS2019}
}

@InProceedings{	  10.1145/3243082.3243106,
  author	= {Sacenti, Juarez A. P. and Willrich, Roberto and Fileto,
		  Renato},
  title		= {Hybrid Recommender System Based on Multi-Hierarchical
		  Ontologies},
  year		= {2018},
  isbn		= {9781450358675},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3243082.3243106},
  doi		= {10.1145/3243082.3243106},
  abstract	= {Recommender Systems (RSs) are usually based in User
		  Profiles (UP) to identify items of interest to a user,
		  among the items of a usually vast collection. Traditional
		  RSs are mostly based on ratings of items made by users and
		  do not attempt to estimate the reasons that led the user to
		  access these items. Furthermore, such systems may suffer
		  from the lack of rating data, the so-called data sparsity.
		  This paper proposes a hybrid recommender system that
		  considers, besides the ratings of the users, a feature
		  description analysis of the items accessed by the users.
		  This analysis is based on ontological UP, described in
		  accordance with a set of ontologies, one per feature. The
		  use of ontologies provides a weak coupling between the
		  proposed RS and the domain of the item to be recommended.
		  The effectiveness of our proposal is demonstrated and
		  evaluated in the movie domain using the MovieLens dataset.
		  The experiments demonstrated an improvement in the quality
		  of the recommendations and a greater tolerance to the data
		  sparsity, compared to state-of-art systems.},
  booktitle	= {Proceedings of the 24th Brazilian Symposium on Multimedia
		  and the Web},
  pages		= {149–156},
  numpages	= {8},
  keywords	= {Hibrid filtering, Ontology, Recommender systems},
  location	= {Salvador, BA, Brazil},
  series	= {WebMedia '18}
}

@InProceedings{	  10.1145/3447568.3448553,
  author	= {Djebouri, Djamila and Keskes, Nabil},
  title		= {Exploitation of ontological approaches in Big Data: A
		  State of the Art},
  year		= {2021},
  isbn		= {9781450376556},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447568.3448553},
  doi		= {10.1145/3447568.3448553},
  abstract	= {The emergence of web technologies is generating a data
		  deluge called Big Data. All this data is in fact a gold
		  mine to be exploited. However, we are confronted with huge
		  volumes of heterogeneous data (various formats) and varied
		  data (various sources) and in continuous expansion. To deal
		  with this, some research works have introduced ontologies:
		  this is the purpose of this paper. We present the Big Data
		  concept on the one hand and the ontology concept on the
		  other. We first recalled the definitions of Big Data, its
		  main dimensions known by the 3 V (volume, velocity,
		  variety), the fields of application as well as the various
		  problems related to it. We reviewed the different solutions
		  proposed as well as the existing tools by using the NoSQL
		  and the Map-Reduce paradigm implemented in Hadoop and
		  Spark.We then looked at the concept of ontology, starting
		  by recalling the definition of ontology, so an ontology is
		  a conceptual model to represent reality and on which it is
		  possible to develop systems that can be shared and reused.
		  Ontologies are used to represent a domain and reason about
		  its entities.Finally, we presented and discussed some
		  research works that combined ontologies and Big Data. We
		  have found that there is a very abundant literature that
		  deals with big data and ontologies separately, but few
		  studies combine the two concepts together. We will
		  therefore focus on the latter in order to enrich the
		  scientific literature in the domain.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Information Systems and Technologies},
  articleno	= {45},
  numpages	= {6},
  keywords	= {Big Data, HADOOP, Knowledge Base, Map-Reduce, Semantic
		  Web, Spark, ontology},
  location	= {Lecce, Italy},
  series	= {ICIST '20}
}

@Book{		  10.1145/3382097,
  author	= {Allemang, Dean and Hendler, Jim and Gandon, Fabien},
  title		= {Semantic Web for the Working Ontologist: Effective
		  Modeling for Linked Data, RDFS, and OWL},
  year		= {2020},
  isbn		= {9781450376174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {3},
  volume	= {33},
  abstract	= {Enterprises have made amazing advances by taking advantage
		  of data about their business to provide predictions and
		  understanding of their customers, markets, and products.
		  But as the world of business becomes more interconnected
		  and global, enterprise data is no long a monolith; it is
		  just a part of a vast web of data. Managing data on a
		  world-wide scale is a key capability for any business
		  today.The Semantic Web treats data as a distributed
		  resource on the scale of the World Wide Web, and
		  incorporates features to address the challenges of massive
		  data distribution as part of its basic design. The aim of
		  the first two editions was to motivate the Semantic Web
		  technology stack from end-to-end; to describe not only what
		  the Semantic Web standards are and how they work, but also
		  what their goals are and why they were designed as they
		  are. It tells a coherent story from beginning to end of how
		  the standards work to manage a world-wide distributed web
		  of knowledge in a meaningful way.The third edition builds
		  on this foundation to bring Semantic Web practice to
		  enterprise. Fabien Gandon joins Dean Allemang and Jim
		  Hendler, bringing with him years of experience in global
		  linked data, to open up the story to a modern view of
		  global linked data. While the overall story is the same,
		  the examples have been brought up to date and applied in a
		  modern setting, where enterprise and global data come
		  together as a living, linked network of data. Also included
		  with the third edition, all of the data sets and queries
		  are available online for study and experimentation at
		  data.world/swwo.}
}

@InProceedings{	  10.1145/3688671.3688734,
  author	= {Kivrakidis, Ioannis and Rigas, Emmanouil and Bassiliades,
		  Nick},
  title		= {Towards Enriching the Electric Vehicle Knowledge Graph by
		  Linking it to DBpedia},
  year		= {2024},
  isbn		= {9798400709821},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3688671.3688734},
  doi		= {10.1145/3688671.3688734},
  abstract	= {The automotive industry is focusing on Electric Vehicles
		  (EVs) for their efficiency in reducing oil consumption and
		  emissions. However, the EV market’s diversity in battery
		  capacities, classifications, and connector types creates a
		  lack of standardization. Researchers are exploring advanced
		  data and knowledge management methods, with Knowledge
		  Graphs (KGs) emerging as a promising solution. KGs
		  represent data in a way that reflects human understanding,
		  promoting natural human-machine interactions and enhancing
		  AI’s insights. Their structure and constraints are
		  defined by a vocabulary or ontology. This paper presents
		  ongoing work towards enriching an existing Electric Vehicle
		  Knowledge Graph (EVKG), which is specified by the Electric
		  Vehicle Ontology (EVO). To achieve this, we utilized the
		  Python programming language to create labels for each
		  resource in the EVKG. These labels were then used to match
		  and link these resources to their corresponding entries in
		  DBpedia through the use of the owl:sameAs and rdfs:seeAlso
		  properties. To ensure the matching was as accurate as
		  possible, we developed two algorithms: one employing string
		  matching and the other using word vectorization and
		  distance techniques.},
  booktitle	= {Proceedings of the 13th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {38},
  numpages	= {4},
  keywords	= {Linked Data, Ontology, Knowledge Graph, EVO Ontology,
		  Electric Vehicles},
  location	= { },
  series	= {SETN '24}
}

@InProceedings{	  10.1145/3243250.3243253,
  author	= {Telli, Abdelmoutia and Chau, Ma Thi and Bourahla, Mustapha
		  and Tabia, Karim and Benferhat, Salem},
  title		= {An Ontology for Classifying Vietnamese Dance Movements},
  year		= {2018},
  isbn		= {9781450364829},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3243250.3243253},
  doi		= {10.1145/3243250.3243253},
  abstract	= {This paper proposes an OWL ontology called "VDM"
		  (Vietnamese Dance Movements), to define taxonomy of dance
		  movement classes and their relationships for the
		  traditional Vietnamese dances taking into account the
		  semantics of its art and its cultural anthropologists. The
		  "VDM" terminology can be used to describe elementary
		  movements (poses) as a dataset ontology importing the
		  ontology "VDM". These poses are results of dance sequences
		  segmentation (using segmentation techniques). The ontology
		  "VDM" is supported by classification rules, which are
		  developed with the OWL complementary language SWRL
		  (Semantic Web Rule Language) to entail movement phrases,
		  which are basic movements with complete meaning. The
		  dataset ontology containing pose descriptions can be
		  queried using the query language SQWRL (Semantic Query
		  Web-enhanced Rule Language).},
  booktitle	= {Proceedings of the International Conference on Pattern
		  Recognition and Artificial Intelligence},
  pages		= {23–29},
  numpages	= {7},
  keywords	= {Dance Labanotation, Description Logics, Ontology, Semantic
		  Web Technologies, Traditional Vietnamese Dance},
  location	= {Union, NJ, USA},
  series	= {PRAI 2018}
}

@Article{	  10.1145/3375628,
  author	= {Hernich, Andr\'{e} and Lutz, Carsten and Papacchini, Fabio
		  and Wolter, Frank},
  title		= {Dichotomies in Ontology-Mediated Querying with the Guarded
		  Fragment},
  year		= {2020},
  issue_date	= {July 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {3},
  issn		= {1529-3785},
  url		= {https://doi.org/10.1145/3375628},
  doi		= {10.1145/3375628},
  abstract	= {We study ontology-mediated querying in the case where
		  ontologies are formulated in the guarded fragment of
		  first-order logic (GF) or extensions thereof with counting
		  and where the actual queries are (unions of) conjunctive
		  queries. Our aim is to classify the data complexity and
		  Datalog rewritability of query evaluation depending on the
		  ontology O, where query evaluation w.r.t. O is in PTime
		  (resp. Datalog rewritable) if all queries can be evaluated
		  in PTime w.r.t. O (resp. rewritten into Datalog under O),
		  and coNP-hard if at least one query is coNP-hard w.r.t. O.
		  We identify several fragments of GF that enjoy a dichotomy
		  between Datalog-rewritability (which implies PTime) and
		  coNP-hardness as well as several other fragments that enjoy
		  a dichotomy between PTime and coNP-hardness, but for which
		  PTime does not imply Datalog-rewritability. For the latter,
		  we establish and exploit a connection to constraint
		  satisfaction problems. We also identify fragments for which
		  there is no dichotomy between PTime and coNP. To prove
		  this, we establish a non-trivial variation of Ladner’s
		  theorem on the existence of NP-intermediate problems.
		  Finally, we study the decidability of whether a given
		  ontology enjoys PTime query evaluation, presenting both
		  positive and negative results, depending on the fragment.},
  journal	= {ACM Trans. Comput. Logic},
  month		= feb,
  articleno	= {20},
  numpages	= {47},
  keywords	= {Ontology-based data access, dichotomies, query
		  evaluation}
}

@InProceedings{	  10.1145/3472307.3484166,
  author	= {Ayimdji Tekemetieu, Armel and Pigot, H\'{e}l\`{e}ne and
		  Bottari, Carolina and Gagnon-Roy, Mireille and Giroux,
		  Sylvain},
  title		= {Modeling an Adaptive Resident-System Interaction for
		  Cognitive Assistance in Ambient Assisted Living},
  year		= {2021},
  isbn		= {9781450386203},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3472307.3484166},
  doi		= {10.1145/3472307.3484166},
  abstract	= {In the last decade, advances in the field of ambient
		  assisted living (AAL) have changed the way services can be
		  provided in smart homes. New possibilities are now offered
		  for addressing complex interaction problems between the
		  technology and users with special needs. Within this
		  context, this study addresses human-computer interactions
		  for cognitive assistance to people with Traumatic Brain
		  Injury (TBI). Interdisciplinary research combining computer
		  science and occupational therapy was conducted to model the
		  interaction between an AAL system (AALS) and a person with
		  cognitive impairments due to TBI residing in an AAL
		  environment. Cognitive assistance is modelled as an
		  interactive exchange where an AALS spreads assistance cues
		  that should induce appropriate behaviours from an assisted
		  person as responses. After an assistance cue is delivered,
		  the AALS should evaluate the user reaction and, stop the
		  interaction if the intended reaction is observed or resume
		  by adapting the assistance. To do so, evidence-based
		  cognitive rehabilitation and speech acts are used to model
		  the interaction content i.e., assistive cues and user
		  feedback, while an ontology formalizes the semantics of
		  this knowledge in a computer readable format. A behavioural
		  model based on behaviour trees informed by the ontological
		  model is then used to enable the cognitive assistant to
		  plan the sequence of cues to be delivered adaptively
		  depending on the circumstances, the assistance goal and the
		  user's behaviour. To show that the proposed interaction
		  model can help an AALS to provide adaptive assistance to
		  people with cognitive impairments, it is exemplified on a
		  cooking safety assistant designed for people with TBI.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Human-Agent Interaction},
  pages		= {183–192},
  numpages	= {10},
  keywords	= {Ambient intelligence, Cognitive assistance, Ontology-based
		  decision support, Speech acts, Traumatic Brain Injury,
		  interactive information retrieval},
  location	= {Virtual Event, Japan},
  series	= {HAI '21}
}

@InProceedings{	  10.1109/asonam55673.2022.10068618,
  author	= {Al-Shareef, Sarah and Alharbi, Rahaf and Alharbi, Rawan
		  and Almfarriji, Raghad and Alsharif, Maram and Alharthi,
		  Rasha and Althaqafi, Lamia},
  title		= {Investigating Community Detection in Arabic Scholarly
		  Network Using Ontology-Based Semantic Expansion},
  year		= {2023},
  isbn		= {9781665456616},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASONAM55673.2022.10068618},
  doi		= {10.1109/ASONAM55673.2022.10068618},
  abstract	= {Clustering researchers in communities is an important task
		  to support a range of techniques for analyzing and making
		  sense of the research environment and helps researchers
		  find people in the same field of interest to collaborate.
		  In computer science, ontology is commonly used to capture
		  knowledge about a particular area using relevant concepts
		  and relations. This study investigates the use of
		  overlapping community detection algorithms on a
		  multilayered Arabic scholarly network to detect communities
		  of researchers who share their research interests. Two
		  researchers can share an interest if they co-authored a
		  publication or share some keywords in their publications.
		  The set of keywords is expanded via semantic search within
		  a cross-domain ontology, e.g. DBpedia, allowing more
		  researchers with indirect relationships to be connected. A
		  2-layer scholarly network was constructed by retrieving the
		  scholarly data of faculty members from three colleges at
		  Umm AlQura University (UQU) with rich Arabic publications.
		  Four versions of this network were tested: unweighted,
		  weighted, semantically expanded, and reduced semantically
		  expanded. It was found that weights have an insignificant
		  role in community detection within this study. In addition,
		  a semantically expanded network does have better clustering
		  potentials but only if was performed selectively.
		  Otherwise, the expanded network might suffer from generic
		  and non-discriminative keywords, making the community
		  detection task more challenging. To our knowledge, this is
		  the first investigation into detecting communities within
		  an Arabic scholarly network.},
  booktitle	= {Proceedings of the 2022 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {96–103},
  numpages	= {8},
  keywords	= {community detection, semantic annotation, Arabic scholarly
		  data, overlapping community detection, multilayered complex
		  network, social network analysis, DBpedia},
  location	= {Istanbul, Turkey},
  series	= {ASONAM '22}
}

@InProceedings{	  10.1145/3696500.3696562,
  author	= {Ke, Lanxin and Jian, Li and Liao, Wang and Chen, Yibin and
		  Cai, Yangqi and Ye, Linmei},
  title		= {Hierarchical Multi-label Classification Model Research for
		  Scenic Area Tourism Resources},
  year		= {2024},
  isbn		= {9798400710278},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696500.3696562},
  doi		= {10.1145/3696500.3696562},
  abstract	= {Research on tourism resource demand offers critical
		  decision support for the protection, development, and
		  marketing of tourism resources. It also improves
		  personalized experiences and satisfaction for tourists,
		  thus fostering the advancement of the tourism industry into
		  broader and higher realms. It also improves personalized
		  experiences and satisfaction for tourists, thus fostering
		  the advancement of the tourism industry into broader and
		  higher realms. The objective of this study is to extract
		  characteristics of 3A-level and above scenic spots
		  nationwide, facilitating the integration and utilization of
		  tourism resources in the country. The objective of this
		  study is to extract characteristics of 3A-level and above
		  scenic spots nationwide, facilitating the integration and
		  utilization of tourism information. This facilitates the
		  automation and efficiency of tourism resource
		  classification and provides suggestions for improving
		  services at these scenic areas. In response to the abundant
		  resources of major scenic areas across the country, a new
		  approach has been developed. In response to the abundant
		  resources of major scenic areas across the country, this
		  study introduces a hierarchical multi-label classification
		  model for tourism resources. tourism resource
		  classification theme system issued by the Ministry of
		  Culture and Tourism, and leveraging FastText for
		  pre-training on scenic area introductory texts, this
		  research combines the advantages of the hierarchical
		  multi-label classification model with the theme system of
		  the Ministry of Culture and Tourism. introductory texts,
		  this research combines the traditional LSTM model with an
		  attention-based Transformer model. Additionally, a
		  Graph-Convolutional Network (GCN) is used to classify
		  tourism resources. Convolutional Network (GCN) is employed
		  as a hierarchical structure-aware encoder to construct the
		  MiLCT, a hierarchical multi-label classification model,
		  enabling sophisticated multi-label classification to be
		  applied to the text. model, enabling sophisticated
		  multi-label classification of tourism scenic area resource
		  data. Experimental comparisons demonstrate that, with
		  increasing classification levels, the proposed model
		  outperforms those lacking GCN and Transformer components in
		  terms of micro-Precision, micro-Recision, micro-Recision,
		  and micro-Recision. micro-Precision, micro-Recall, and
		  micro-F1 scores, this indicates that the hierarchical
		  structural information of the model can significantly
		  enhance its performance. In comparison to the hierarchical
		  multi-label correlation model, the proposed model
		  demonstrates enhanced performance across the evaluated
		  metrics, indicating its efficacy in integrating multimodal
		  features and providing more comprehensive and accurate data
		  characterization.},
  booktitle	= {Proceedings of the 2024 International Conference on Big
		  Data and Digital Management},
  pages		= {369–379},
  numpages	= {11},
  location	= {Shanghai, China},
  series	= {ICBDDM '24}
}

@InProceedings{	  10.1145/3297662.3365824,
  author	= {Orozco, Adrian Taboada and Mouakher, Amira and Ben Sassi,
		  Imen and Nicolle, Christophe},
  title		= {An Ontology-Based Thermal Comfort Management System In
		  Smart Buildings},
  year		= {2020},
  isbn		= {9781450362382},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297662.3365824},
  doi		= {10.1145/3297662.3365824},
  abstract	= {Achieving thermal comfort for occupants in buildings has
		  been the main focus of several studies in recent years. The
		  challenging issue of the building envelope is to save
		  energy and achieve a high comfortable environment
		  simultaneously. To calculate the thermal comfort level in a
		  living space, environmental factors such as indoor air
		  temperature, mean radiant temperature, air velocity, and
		  humidity are needed. The latter parameters are aggregated
		  through the well known PMV index. In this paper, we
		  introduce a wireless sensor network (WSN)-based comfort
		  measurement approach, called OnCom, using a dedicated
		  ontology and the emotional state analysis of the occupant
		  to reach the "adequate" indoor thermal comfort. The main
		  thrust of OnCom stands on the smooth connection of human
		  emotions with the thermal sensations. Carried out
		  experiments showed that emotions, unveiled from tweets,
		  have been efficiently used to mitigate user thermal
		  discomfort.},
  booktitle	= {Proceedings of the 11th International Conference on
		  Management of Digital EcoSystems},
  pages		= {300–307},
  numpages	= {8},
  keywords	= {Ontology, Sentiment Analysis, Smart Buildings, Thermal
		  Comfort, Wireless Sensor Networks (WSN)},
  location	= {Limassol, Cyprus},
  series	= {MEDES '19}
}

@Article{	  10.1613/jair.1.12631,
  author	= {Silverman, Greg M. and Sahoo, Himanshu S. and Ingraham,
		  Nicholas E. and Lupei, Monica and Puskarich, Michael A. and
		  Usher, Michael and Dries, James and Finzel, Raymond L. and
		  Murray, Eric and Sartori, John and Simon, Gyorgy and Zhang,
		  Rui and Melton, Genevieve B. and Tignanelli, Christopher J.
		  and Pakhomov, Serguei VS},
  title		= {NLP Methods for Extraction of Symptoms from Unstructured
		  Data for Use in Prognostic COVID-19 Analytic Models},
  year		= {2022},
  issue_date	= {Jan 2022},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {72},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.12631},
  doi		= {10.1613/jair.1.12631},
  abstract	= {Statistical modeling of outcomes based on a patient's
		  presenting symptoms (symptomatology) can help deliver high
		  quality care and allocate essential resources, which is
		  especially important during the COVID-19 pandemic. Patient
		  symptoms are typically found in unstructured notes, and
		  thus not readily available for clinical decision making. In
		  an attempt to fill this gap, this study compared two
		  methods for symptom extraction from Emergency Department
		  (ED) admission notes. Both methods utilized a lexicon
		  derived by expanding The Center for Disease Control and
		  Prevention's (CDC) Symptoms of Coronavirus list. The first
		  method utilized a word2vec model to expand the lexicon
		  using a dictionary mapping to the Uni ed Medical Language
		  System (UMLS). The second method utilized the expanded
		  lexicon as a rule-based gazetteer and the UMLS. These
		  methods were evaluated against a manually annotated
		  reference (f1-score of 0.87 for UMLS-based ensemble; and
		  0.85 for rule-based gazetteer with UMLS). Through analyses
		  of associations of extracted symptoms used as features
		  against various outcomes, salient risks among the
		  population of COVID-19 patients, including increased risk
		  of in-hospital mortality (OR 1.85, p-value &lt; 0.001),
		  were identified for patients presenting with dyspnea.
		  Disparities between English and non-English speaking
		  patients were also identified, the most salient being a
		  concerning finding of opposing risk signals between fatigue
		  and in-hospital mortality (non-English: OR 1.95, p-value =
		  0.02; English: OR 0.63, p-value = 0.01). While use of
		  symptomatology for modeling of outcomes is not unique,
		  unlike previous studies this study showed that models built
		  using symptoms with the outcome of in-hospital mortality
		  were not significantly different from models using data
		  collected during an in-patient encounter (AUC of 0.9 with
		  95\% CI of [0.88, 0.91] using only vital signs; AUC of 0.87
		  with 95\% CI of [0.85, 0.88] using only symptoms). These
		  findings indicate that prognostic models based on
		  symptomatology could aid in extending COVID-19 patient care
		  through telemedicine, replacing the need for in-person
		  options. The methods presented in this study have potential
		  for use in development of symptomatology-based models for
		  other diseases, including for the study of Post-Acute
		  Sequelae of COVID-19 (PASC).},
  journal	= {J. Artif. Int. Res.},
  month		= jan,
  pages		= {429–474},
  numpages	= {46},
  keywords	= {Natural language processing, COVID-19, Information
		  extraction, UMLS}
}

@Article{	  10.1145/3423166,
  author	= {Varela-Vaca, \'{A}ngel Jes\'{u}s and Quintero, Antonia M.
		  Reina},
  title		= {Smart Contract Languages: A Multivocal Mapping Study},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3423166},
  doi		= {10.1145/3423166},
  abstract	= {Blockchain is a disruptive technology that has attracted
		  the attention of the scientific community and companies, as
		  proven by the exponential growth of publications on this
		  topic in recent years. This growing interest is mainly due
		  to the promise that the use of blockchain enables it to be
		  verified, without including any trusted intermediaries,
		  that the information received from the network is authentic
		  and up-to-date. In this respect, blockchain is a
		  distributed database that can be seen as a ledger that
		  records all transactions that have ever been executed. In
		  this context, smart contracts are pieces of software used
		  to facilitate, verify, and enforce the negotiation of a
		  transaction on a blockchain platform. These pieces of
		  software are implemented by using programming languages,
		  which are sometimes provided by the blockchain platforms
		  themselves. This study aims to (1) identify and categorise
		  the state-of-the-art related to smart contract languages,
		  in terms of the existing languages and their main features,
		  and (2) identify new research opportunities. The review has
		  been conducted as a multivocal mapping study that follows
		  the guidelines proposed by Garousi et&nbsp;al. for
		  conducting multivocal literature reviews, as well as the
		  guidelines proposed by Kitchenham and Charters for
		  conducting mapping studies. As a result of the
		  implementation of the review protocol, 4,119 papers were
		  gathered, and 109 of them were selected for extraction. The
		  contributions of this article are twofold: (1) 101
		  different smart contract languages have been identified and
		  classified according to a variety of criteria; (2) a
		  discussion on the findings and their implications for
		  future research have been outlined. As a conclusion, it
		  could be stated that a rigorous and replicable overview of
		  the state-of-the-art of smart contract languages has been
		  provided that can benefit not only researchers but also
		  practitioners in the field, thanks to its multivocal
		  nature.},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  articleno	= {3},
  numpages	= {38},
  keywords	= {Smart contract language, blockchain, multivocal literature
		  mapping study, systematic literature review}
}

@InProceedings{	  10.1145/3331076.3331103,
  author	= {Mansour, Elio and Chbeir, Richard and Arnould, Philippe},
  title		= {EQL-CE: an event query language for connected
		  environments},
  year		= {2019},
  isbn		= {9781450362498},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331076.3331103},
  doi		= {10.1145/3331076.3331103},
  abstract	= {Recent advances in sensor technology and information
		  processing have allowed connected environments to impact
		  various application domains. In order to detect events in
		  these environments, existing works rely on the sensed data.
		  However, these works are not re-usable since they
		  statically define the targeted events (i.e., the
		  definitions are hard to modify when needed). Here, we
		  present a generic framework for event detection composed of
		  (i) a representation of the environment; (ii) an event
		  detection mechanism; and (iii) an Event Query Language
		  (EQL) for user/framework interaction. This paper focuses on
		  detailing the EQL which allows the definition of the data
		  model components, handles instances of each component,
		  protects the security/privacy of data/users, and
		  defines/detects events. We also propose a query optimizer
		  in order to handle the dynamicity of the environment and
		  spatial/temporal constraints. We finally illustrate the EQL
		  and conclude the paper with some future works.},
  booktitle	= {Proceedings of the 23rd International Database
		  Applications \&amp; Engineering Symposium},
  articleno	= {7},
  numpages	= {10},
  keywords	= {event query language, internet of things, sensor
		  networks},
  location	= {Athens, Greece},
  series	= {IDEAS '19}
}

@InProceedings{	  10.1145/3337722.3341871,
  author	= {Gandhi, Sagar and Harrison, Brent},
  title		= {Guided open story generation using probabilistic graphical
		  models},
  year		= {2019},
  isbn		= {9781450372176},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3337722.3341871},
  doi		= {10.1145/3337722.3341871},
  abstract	= {In this work, we present an approach for performing
		  computational storytelling in open domain based on Author
		  Goals. Author Goals are constraints placed on a story event
		  directed by the author of the system. There are two
		  challenges present in this type of story generation: (1)
		  automatically acquiring a model of story progression, and
		  (2) guiding the progress of story progression in light of
		  different goals. We propose a novel approach to story
		  generation based on probabilistic graphical models and
		  Loopy Belief Propagation (LBP) that addresses both of these
		  problems. We show the applicability of our technique
		  through a case study on the Visual Storytelling (VIST) 2017
		  dataset. We use image descriptions as author goals. This
		  empirical analysis suggests that our approach is able to
		  utilize goals information to better automatically generate
		  stories.},
  booktitle	= {Proceedings of the 14th International Conference on the
		  Foundations of Digital Games},
  articleno	= {79},
  numpages	= {7},
  keywords	= {belief propagation, computational storytelling, natural
		  language generation, probabilistic graphical models},
  location	= {San Luis Obispo, California, USA},
  series	= {FDG '19}
}

@InProceedings{	  10.1145/3444757.3485110,
  author	= {Khemiri, Ahmed and Drissi, Amani and Tissaoui, Anis and
		  Sassi, Salma and Chbeir, Richard},
  title		= {Learn2Construct: An automatic ontology construction based
		  on LDA from texual data},
  year		= {2021},
  isbn		= {9781450383141},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3444757.3485110},
  doi		= {10.1145/3444757.3485110},
  abstract	= {In recent years, the research on Ontology Learning has
		  become a hot topic among researchers because of the
		  exponential increase of the number of documents and textual
		  data not only on the web but also in digital libraries.
		  This has participated to the emergence of new computational
		  tools and methods to deal with the automatic organization,
		  representation, retrieval and exploration of large corpus
		  in order to have a good way of organizing and managing huge
		  volumes of data. LDA-based approaches have proven to
		  provide the best result [18][16] [4]. However, they suffers
		  to several limitations related to concept and relation
		  extraction, as well as coping with the corpus evolution. In
		  order to cope with these problems, we propose here a new
		  solution named Learn2Construct which is an automatic
		  ontology construction method based on topic modeling.
		  Experiments have been conducted to measure the
		  effectiveness of our solution and compare it to existing
		  ones. The results obtained are more than satisfactory.},
  booktitle	= {Proceedings of the 13th International Conference on
		  Management of Digital EcoSystems},
  pages		= {49–56},
  numpages	= {8},
  keywords	= {LDA, Latent Dirichlet Allocation, Ontology Learning,
		  Ontology based on topic modeling, Text classification,
		  Topic modeling},
  location	= {Virtual Event, Tunisia},
  series	= {MEDES '21}
}

@InProceedings{	  10.1145/3440749.3442607,
  author	= {Ismail, Qossay and Saleh, Osama and Hashayka, Mohammed and
		  Awad, Ahmed and Hawash, Amjad and Othman, Othman},
  title		= {Improve the Firewall Accuracy By using Dynamic Ontology},
  year		= {2021},
  isbn		= {9781450388863},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3440749.3442607},
  doi		= {10.1145/3440749.3442607},
  abstract	= {Data is considered an important asset for organizations,
		  companies, and even people. Crucial decisions depend mainly
		  on data. Exchanging data is essential in order to negotiate
		  ideas, thoughts, and decisions. Networks are the
		  communication channels of data exchange although data is
		  exposed to different attacks, threats, and loss. Because of
		  this, data security has become a key concern for different
		  parties through their daily data manipulation. There are
		  different ways to ensure data security. Paying attention to
		  network threats, data encryption, and using strong
		  passwords are all examples. However, a firewall represents
		  the first defense line against malicious traffic throughout
		  the network. Firewalls have a set of rules to be applied in
		  the time of data exchange between inside and outside of
		  data networks. Some of the firewalls apply such rules in a
		  sequential manner, which degrades the performance of the
		  firewall. In this work, we are utilizing a dynamic ontology
		  of different firewall rules managed by SPARQL queries, so
		  that the rules are applied faster, and thus, increasing the
		  firewall performance. Experimental results show that our
		  proposed methodology totally eliminates the anomalies in
		  the firewall rules as a result of conducting longest
		  matching with proper rules from the dynamically constructed
		  ontology.},
  booktitle	= {Proceedings of the 4th International Conference on Future
		  Networks and Distributed Systems},
  articleno	= {16},
  numpages	= {5},
  keywords	= {Anomalies, Correlation, DOM, False Negative, False
		  Positive, Firewall, Generalization, IP, Jena, Ontology,
		  Port, RDF, Redundancy, Rule, SPARQL, Shadowing, Software
		  Defined Network.},
  location	= {St.Petersburg, Russian Federation},
  series	= {ICFNDS '20}
}

@InProceedings{	  10.1145/3701716.3717809,
  author	= {Pellegrino, Maria Angela and Tuozzo, Gabriele},
  title		= {From Quality Reports to Knowledge Graphs: a Case Study on
		  CSV-to-KG Transformation},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3717809},
  doi		= {10.1145/3701716.3717809},
  abstract	= {The construction of Knowledge Graphs (KGs) often demands
		  substantial manual effort and domain expertise, especially
		  when converting structured data formats like CSV files into
		  KGs. Recent advancements in Large Language Models (LLMs)
		  offer promising avenues to simplify this process through
		  prompt engineering.This study investigates various
		  prompting strategies-zero-shot, one-shot, prompt chaining,
		  and a hybrid approach-to enable LLMs to automate the
		  creation of KGs from CSV files. Using a dataset containing
		  quality metrics for 2,026 KGs generated by KGHeartBeat, the
		  paper assesses the performance of GPT-4o, GPT-o1 mini,
		  Claude 3.5 Sonnet, and Gemini 1.5 pro, across different
		  prompt configurations. The findings reveal that the hybrid
		  approach consistently produces the most accurate and
		  complete KGs, effectively addressing challenges related to
		  scalability and complexity.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {1626–1632},
  numpages	= {7},
  keywords	= {comparison, csv converter, data quality, empirical
		  investigation, knowledge graph, llms, prompt engineering,
		  quality assessment},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3487553.3527164,
  author	= {Verspoor, Karin},
  title		= {Why Bother Enabling Biomedical Literature Analysis with
		  Semantics?},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3527164},
  doi		= {10.1145/3487553.3527164},
  abstract	= {These days, ELMo&nbsp;[3], BERT&nbsp;[1], BART&nbsp;[2]
		  and other similarly cutely-named models appear to have
		  dramatically advanced the state of the art in basically
		  every problem in natural language processing and
		  information retrieval. It can leave a researcher wondering
		  whether there is more to language processing than deploying
		  or fine-tuning contextual word embeddings. What of formal
		  semantics and knowledge representation? What value do these
		  bring to text analysis, either in modelling or in task
		  definitions? In this talk, I will try to explore these
		  questions, from the perspective of my long-running
		  experiences in biomedical information extraction and
		  literature exploration. Perhaps we can shift the academic
		  conversation from a one-model-fits-all solution for
		  individual tasks to a more nuanced consideration of
		  complex, multi-faceted problems in which such models
		  certainly can play a critical role but aren’t necessarily
		  “all you need”&nbsp;[4].},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {822},
  numpages	= {1},
  keywords	= {knowledge representation, language models, ontologies,
		  semantics, text mining, word embeddings},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3701551.3703500,
  author	= {Yoon, Soojin and Ko, Sungho and Kim, Tongyoung and Kang,
		  SeongKu and Yeo, Jinyoung and Lee, Dongha},
  title		= {Unsupervised Robust Cross-Lingual Entity Alignment via
		  Neighbor Triple Matching with Entity and Relation Texts},
  year		= {2025},
  isbn		= {9798400713293},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701551.3703500},
  doi		= {10.1145/3701551.3703500},
  abstract	= {Cross-lingual entity alignment (EA) enables the
		  integration of multiple knowledge graphs (KGs) across
		  different languages, providing users with seamless access
		  to diverse and comprehensive knowledge. Existing methods,
		  mostly supervised, face challenges in obtaining labeled
		  entity pairs. To address this, recent studies have shifted
		  towards self-supervised and unsupervised frameworks.
		  Despite their effectiveness, these approaches have
		  limitations: (1) Relation passing: mainly focusing on the
		  entity while neglecting the semantic information of
		  relations, (2) Isomorphic assumption: assuming isomorphism
		  between source and target graphs, which leads to noise and
		  reduced alignment accuracy, and (3) Noise vulnerability:
		  susceptible to noise in the textual features, especially
		  when encountering inconsistent translations or
		  Out-of-Vocabulary (OOV) problems. In this paper, we propose
		  ERAlign, an unsupervised and robust cross-lingual EA
		  pipeline that jointly performs
		  &lt;u&gt;E&lt;/u&gt;ntity-level and
		  &lt;u&gt;R&lt;/u&gt;elation-level
		  &lt;u&gt;Align&lt;/u&gt;ment by neighbor triple matching
		  strategy using semantic textual features of relations and
		  entities. Its refinement step iteratively enhances results
		  by fusing entity-level and relation-level alignments based
		  on neighbor triple matching. The additional verification
		  step examines the entities' neighbor triples as the
		  linearized text. This Align-then-Verify pipeline rigorously
		  assesses alignment results, achieving near-perfect
		  alignment even in the presence of noisy textual features of
		  entities. Our extensive experiments demonstrate that the
		  robustness and general applicability of ERAlign improved
		  the accuracy and effectiveness of EA tasks, contributing
		  significantly to knowledge-oriented applications.},
  booktitle	= {Proceedings of the Eighteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {184–193},
  numpages	= {10},
  keywords	= {cross-lingual entity alignment, knowledge graph, neighbor
		  triple matching, optimal transport, pretrained language
		  models},
  location	= {Hannover, Germany},
  series	= {WSDM '25}
}

@InProceedings{	  10.1145/3460210.3493552,
  author	= {Sartini, Bruno and van Erp, Marieke and Gangemi, Aldo},
  title		= {Marriage is a Peach and a Chalice: Modelling Cultural
		  Symbolism on the Semantic Web},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493552},
  doi		= {10.1145/3460210.3493552},
  abstract	= {In this work, we fill the gap in the Semantic Web in the
		  context of Cultural Symbolism. Building upon earlier work
		  in citesartini_towards_2021, we introduce the Simulation
		  Ontology, an ontology that models the background knowledge
		  of symbolic meanings, developed by combining the concepts
		  taken from the authoritative theory of Simulacra and
		  Simulations of Jean Baudrillard with symbolic structures
		  and content taken from "Symbolism: a Comprehensive
		  Dictionary'' by Steven Olderr. We re-engineered the
		  symbolic knowledge already present in heterogeneous
		  resources by converting it into our ontology schema to
		  create HyperReal, the first knowledge graph completely
		  dedicated to cultural symbolism. A first experiment run on
		  the knowledge graph is presented to show the potential of
		  quantitative research on symbolism.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {201–208},
  numpages	= {8},
  keywords	= {knowledge graph, linked data, ontology, semantic web,
		  symbolism},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@Article{	  10.1145/3558000,
  author	= {Kleyko, Denis and Rachkovskij, Dmitri and Osipov, Evgeny
		  and Rahimi, Abbas},
  title		= {A Survey on Hyperdimensional Computing aka Vector Symbolic
		  Architectures, Part II: Applications, Cognitive Models, and
		  Challenges},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3558000},
  doi		= {10.1145/3558000},
  abstract	= {This is Part&nbsp;II of the two-part comprehensive survey
		  devoted to a computing framework most commonly known under
		  the names Hyperdimensional Computing and Vector Symbolic
		  Architectures (HDC/VSA). Both names refer to a family of
		  computational models that use high-dimensional distributed
		  representations and rely on the algebraic properties of
		  their key operations to incorporate the advantages of
		  structured symbolic representations and vector distributed
		  representations. Holographic Reduced
		  Representations&nbsp;[321, 326] is an influential HDC/VSA
		  model that is well known in the machine learning domain and
		  often used to refer to the whole family. However, for the
		  sake of consistency, we use HDC/VSA to refer to the
		  field.Part&nbsp;I of this survey&nbsp;[222] covered
		  foundational aspects of the field, such as the historical
		  context leading to the development of HDC/VSA, key elements
		  of any HDC/VSA model, known HDC/VSA models, and the
		  transformation of input data of various types into
		  high-dimensional vectors suitable for HDC/VSA. This second
		  part surveys existing applications, the role of HDC/VSA in
		  cognitive computing and architectures, as well as
		  directions for future work. Most of the applications lie
		  within the Machine Learning/Artificial Intelligence domain;
		  however, we also cover other applications to provide a
		  complete picture. The survey is written to be useful for
		  both newcomers and practitioners.},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  articleno	= {175},
  numpages	= {52},
  keywords	= {Artificial intelligence, machine learning, distributed
		  representations, cognitive architectures, cognitive
		  computing, applications, analogical reasoning,
		  hyperdimensional computing, vector symbolic architectures,
		  holographic reduced representations, tensor product
		  representations, matrix binding of additive terms, binary
		  spatter codes, multiply-add-permute, sparse binary
		  distributed representations, sparse block codes, modular
		  composite representations, geometric analogue of
		  holographic reduced representations}
}

@InProceedings{	  10.1145/3581783.3613434,
  author	= {Yang, Yuchen},
  title		= {Encoding and Decoding Narratives: Datafication and
		  Alternative Access Models for Audiovisual Archives},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3613434},
  doi		= {10.1145/3581783.3613434},
  abstract	= {Situated in the intersection of audiovisual archives,
		  computational methods, and immersive interactions, this
		  work probes the increasingly important accessibility issues
		  from a two-fold approach. Firstly, the work proposes an
		  ontological data model to handle complex descriptors
		  (metadata, feature vectors, etc.) with regard to user
		  interactions. Secondly, this work examines text-to-video
		  retrieval from an implementation perspective by proposing a
		  classifier-enhanced workflow to deal with complex and
		  hybrid queries and a training data augmentation workflow to
		  improve performance. This work serves as the foundation for
		  experimenting with novel public-facing access models to
		  large audiovisual archives.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {9355–9359},
  numpages	= {5},
  keywords	= {audiovisual archive, computational archive, experimental
		  museology, text-to-video encoding},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3167132.3167227,
  author	= {Bernard, Camille and Villanova-Oliver, Marl\`{e}ne and
		  Gensel, J\'{e}r\^{o}me and Dao, Hy},
  title		= {Modeling changes in territorial partitions over time:
		  ontologies TSN and TSN-change},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167227},
  doi		= {10.1145/3167132.3167227},
  abstract	= {Territories are governed, administered and observed from
		  partitions of space into territorial units. All these
		  territorial partitions change over time, for political or
		  administrative reasons. In this paper, we present two
		  innovative ontologies - Territorial Statistical
		  Nomenclature (TSN) and TSN-Change - for the modeling of
		  territorial partitions over time, adopting a Linked Data
		  (LD) approach understandable by both humans and machines.
		  These ontologies are innovative as they are generic,
		  enabling the publication of any territorial partition into
		  the LD Web. Above all, they allow for rich descriptions of
		  changes, from one partition version to another, through
		  Multi-Levels Change Graphs that link together changes that
		  happen at different territorial levels (e.g., major regions
		  or districts levels). These RDF graphs constitute a
		  knowledge base to extract patterns of change and simulate
		  scenarios.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {866–875},
  numpages	= {10},
  keywords	= {change ontology, space and time ontology, territorial
		  statistical nomenclature, web of linked data},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InProceedings{	  10.1145/3297280.3297367,
  author	= {Arruda, Mayke Ferreira and Bulc\~{a}o-Neto, Renato
		  Freitas},
  title		= {Toward a lightweight ontology for privacy protection in
		  IoT},
  year		= {2019},
  isbn		= {9781450359337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297280.3297367},
  doi		= {10.1145/3297280.3297367},
  abstract	= {The literature asserts that the design of an
		  ontology-based privacy model is an essential starting point
		  to address privacy risks in IoT, where connected devices
		  are increasingly capable of monitoring human activities.
		  Due to the omnipresence of data privacy concerns in IoT, we
		  highlight the need for privacy ontologies that combine an
		  expressive vocabulary with extension points but that do not
		  overload the processing of privacy policies data. This
		  paper presents IoT-Priv as a lightweight privacy layer upon
		  IoT basic concepts such as device, sensor, and service. We
		  introduce privacy requirements guiding the IoT-Priv
		  ontology design, match these requirements to the respective
		  privacy terms modeled, and show how to use IoT-Priv through
		  a usage scenario. Finally, we evaluate static metrics and
		  response times of spatial and temporal query filters over
		  instances of privacy policies. Results open the way for the
		  creation of scalable, privacy-enabled systems.},
  booktitle	= {Proceedings of the 34th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {880–888},
  numpages	= {9},
  keywords	= {evaluation, internet of things, ontology, privacy,
		  requirements},
  location	= {Limassol, Cyprus},
  series	= {SAC '19}
}

@InProceedings{	  10.1145/3589335.3651572,
  author	= {Hsu, Chi-Yang and Cox, Kyle and Xu, Jiawei and Tan, Zhen
		  and Zhai, Tianhua and Hu, Mengzhou and Pratt, Dexter and
		  Chen, Tianlong and Hu, Ziniu and Ding, Ying},
  title		= {Thought Graph: Generating Thought Process for Biological
		  Reasoning},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651572},
  doi		= {10.1145/3589335.3651572},
  abstract	= {We present the Thought Graph as a novel framework to
		  support complex reasoning and use gene set analysis as an
		  example to uncover semantic relationships between
		  biological processes. Our framework stands out for its
		  ability to provide a deeper understanding of gene sets,
		  significantly surpassing GSEA by 40.28\% and LLM baselines
		  by 5.38\% based on cosine similarity to human annotations.
		  Our analysis further provides insights into future
		  directions of biological processes naming, and implications
		  for bioinformatics and precision medicine.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {537–540},
  numpages	= {4},
  keywords	= {bioinformatics, biological process., gene ontology, large
		  language model, natural language processing, semantic web},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3494322.3494337,
  author	= {Hviid, Jakob and Johansen, Aslak and Fierro, Gabe and
		  Kj\ae{}rgaard, Mikkel Baun},
  title		= {Service Portability and Information Discovery in Building
		  Operating Systems using Semantic Modeling},
  year		= {2022},
  isbn		= {9781450385664},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3494322.3494337},
  doi		= {10.1145/3494322.3494337},
  abstract	= {To achieve cost-efficient IoT based Building Operating
		  Systems (BOS), portable building services are needed. Most
		  previous work has gone into hardware abstraction, while
		  service abstraction has been neglected. This paper presents
		  an information discovery mechanism for BOSs that is based
		  on an ontology which integrates with other semantic models
		  used in the space. The built environment is characterized
		  by extreme heterogeneity; no buildings are entirely alike.
		  Equipment is replaced or updated, control systems and
		  building functionality evolve, as applications, models,
		  forecasters, and controllers improve. For services deployed
		  in such settings to operate at a scale, they must be robust
		  to change. Describing service interfaces using a semantic
		  model, together with the physical context in a building,
		  enables applications to query for their service
		  dependencies. Applications then depend on an abstract
		  query, instead of specific services. For evaluation, nine
		  services running on models of the service ecosystems of
		  three concrete buildings, are implemented, demonstrated,
		  and discussed. Results show services have successfully been
		  made portable and adapts to the changing environments of
		  buildings. Merging and splitting services without code
		  changes to depending services also work as intended, as
		  well as increasing system resilience, by arbitrating
		  similar services.},
  booktitle	= {Proceedings of the 11th International Conference on the
		  Internet of Things},
  pages		= {110–117},
  numpages	= {8},
  keywords	= {Dependency Resolution., Information Discovery, Ontology,
		  Service Abstraction, Service Discovery},
  location	= {St.Gallen, Switzerland},
  series	= {IoT '21}
}

@InProceedings{	  10.1145/3397125.3397134,
  author	= {Alaa, Rana and Gawich, Mariam and Fern\'{a}ndez-Veiga,
		  Manuel},
  title		= {Personalized Recommendation for Online Retail Applications
		  Based on Ontology Evolution},
  year		= {2020},
  isbn		= {9781450377492},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397125.3397134},
  doi		= {10.1145/3397125.3397134},
  abstract	= {The upcoming generation of World Wide Web is signified in
		  semantic web technology that allows future applications to
		  grasp and connect with numerous knowledge bases. Due to its
		  exclusive function in modeling specific domain, Ontology
		  has been playing an essential role in semantic web
		  development. Recommender systems are an indispensable part
		  of online site, which makes their use of high value in
		  recommending items to users according to their interests.
		  The semantic recommender systems recently aim to accomplish
		  the website ontologies to generate semantic recommendations
		  for users' profiles. Therefore, ontology-based semantic
		  recommender systems are used to develop web recommendation.
		  In this paper a recommendation system architecture based on
		  ontology is proposed to give semantic recommendations for
		  each user profile. The proposed system architecture applies
		  two recommendation techniques, content-based filtering and
		  collaborative filtering.},
  booktitle	= {Proceedings of the 2020 6th International Conference on
		  Computer and Technology Applications},
  pages		= {12–16},
  numpages	= {5},
  keywords	= {Semantic recommender system, intelligent personalization,
		  ontology, ontology evolution, reasoning},
  location	= {Antalya, Turkey},
  series	= {ICCTA '20}
}

@InProceedings{	  10.1145/3555776.3577657,
  author	= {Ichida, Alexandre Yukio and Meneguzzi, Felipe},
  title		= {Modeling a Conversational Agent using BDI Framework},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3577657},
  doi		= {10.1145/3555776.3577657},
  abstract	= {Building conversational agents to help humans in
		  domain-specific tasks is challenging since the agent needs
		  to understand the natural language and act over it while
		  accessing domain expert knowledge. Modern natural language
		  processing techniques led to an expansion of conversational
		  agents, with recent pretrained language models achieving
		  increasingly accurate language recognition results using
		  ever-larger open datasets. However, the black-box nature of
		  such pretrained language models obscures the agent's
		  reasoning and its motivations when responding, leading to
		  unexplained dialogues. We develop a belief-desire-intention
		  (BDI) agent as a task-oriented dialogue system to introduce
		  mental attitudes similar to humans describing their
		  behavior during a dialogue. We compare the resulting model
		  with a pipeline dialogue model by leveraging existing
		  components from dialogue systems and developing the agent's
		  intention selection as a dialogue policy. We show that
		  combining traditional agent modelling approaches, such as
		  BDI, with more recent learning techniques can result in
		  efficient and scrutable dialogue systems.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {856–863},
  numpages	= {8},
  keywords	= {belief-desire-intention, task-oriented dialogue systems,
		  autonomous agent, machine learning},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@InProceedings{	  10.1145/3627341.3630381,
  author	= {Hong, Sheng and Lai, Yizhong and Li, Yuzhou and You, Yang
		  and Ou, Shuai and Han, Fei and Wu, Tiejun},
  title		= {A Reference Model for Information Security Applications of
		  Generative Adversarial Network Variants},
  year		= {2023},
  isbn		= {9798400708701},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627341.3630381},
  doi		= {10.1145/3627341.3630381},
  abstract	= {Information security stemming from Generative Adversarial
		  Network (GAN) variants has garnered significant attention.
		  However, a complete reference model targeting this security
		  problem has yet to be established. This paper selects
		  several GAN variants as the research subject and proposes a
		  reference model framework for the information security
		  applications of adversarial generative network variants.
		  The proposed framework is derived using the NIST
		  information security reference model methodology. By
		  conducting a comprehensive analysis of the structure and
		  information security risks of GAN variants, this paper
		  classifies information security attacks on information
		  systems of GAN variants into three categories and maps them
		  onto the security target reference model. The resulting
		  security application reference model can serve as a basis
		  and reference for improving system confidentiality,
		  integrity, and availability, as well as facilitating the
		  design, analysis, and verification of security against
		  malicious attacks. Moreover, the research method employed
		  in this paper is also applicable to information security
		  research of other types of information systems. Therefore,
		  the proposed reference model framework can serve as a
		  valuable contribution to the field of information security
		  and advance the development of effective countermeasures
		  against adversarial generative network variants.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Computer, Vision and Intelligent Technology},
  articleno	= {7},
  numpages	= {8},
  keywords	= {Adversarial Generative Network Variant, Information
		  Security, Security Reference Model, System Security},
  location	= {Chenzhou, China},
  series	= {ICCVIT '23}
}

@Article{	  10.1145/3494572,
  author	= {Musso, Marta and Arnold, Kerstin and Nanni, Federico and
		  Cannelli, Beatrice},
  title		= {What Is in a &lt;unittitle&gt;? Cross-lingual Topic
		  Detection \&amp; Information Retrieval in Archives Portal
		  Europe},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3494572},
  doi		= {10.1145/3494572},
  abstract	= {Archives Portal Europe (APE, ) is the portal of European
		  archives, an aggregator that connects on a single research
		  point the catalogues and digitised archival material of all
		  archives in and about Europe. It currently hosts material
		  from more than 30 countries and from a variety of archival
		  institutions (such as State archives, city archives,
		  university and parish archives, private institutions, and
		  more). It is maintained by the Archives Portal Europe
		  Foundation, an international consortium of State archives
		  and other archival institutions that aim to connect the
		  archival material of single institutions into one digital
		  repository to allow universal access to the archival
		  heritage of Europe, promoting new forms of archival
		  research beyond national or local boundaries. One of the
		  research tools made available by Archives Portal Europe is
		  by topics; however, these are currently maintained manually
		  by the archivists, and the vast amount of archival material
		  ingested in the portal makes it impossible to have a
		  comprehensive body of topics that describe the whole of the
		  APE repository. Archives are traditionally not organised by
		  their subject content, but around the entity (person,
		  organization, body) that created and/or collected the
		  documents in the course of their activities. While this is
		  an undisputed pillar of archival management, the
		  availability of online digital repositories for archival
		  research requires new tools for digital archival research,
		  particularly when different archival traditions from
		  different countries and different types of institutions are
		  merged into a unique research portal. Topic detection
		  becomes a fundamental tool to guide archival research and
		  to allow archives to be accessible to potentially
		  world-wide users in a situation where national and
		  linguistics barriers blur or are re-defined. This article
		  presents the preliminary results and plan for future
		  iterations of an AI tool for automated topic detection in a
		  multi- lingual environment, where human-created taxonomies
		  act as bases for the algorithms to aggregate relevant
		  material around a specific topic. The development is based
		  on supervised machine learning, with a combination of human
		  inputs in different languages, and of the usage of
		  Wikipedia pages to model the relevant vocabulary and
		  entities.},
  journal	= {J. Comput. Cult. Herit.},
  month		= mar,
  articleno	= {25},
  numpages	= {23},
  keywords	= {Digital Archives, Digital Research, Automated topic
		  modelling, search engines, Archives Portal Europe}
}

@InProceedings{	  10.1145/3297280.3297507,
  author	= {Laadhar, Amir and Ghozzi, Faiza and Megdiche, Imen and
		  Ravat, Franck and Teste, Olivier and Gargouri, Faiez},
  title		= {Partitioning and local matching learning of large
		  biomedical ontologies},
  year		= {2019},
  isbn		= {9781450359337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297280.3297507},
  doi		= {10.1145/3297280.3297507},
  abstract	= {Conventional ontology matching systems are not
		  well-tailored to ensure sufficient quality alignments for
		  large ontology matching tasks. In this paper, we propose a
		  local matching learning strategy to align large and complex
		  biomedical ontologies. We define a novel partitioning
		  approach that breakups large ontology alignment task into a
		  set of local sub-matching tasks. We perform a machine
		  learning approach for each local sub-matching task. We
		  build a local machine learning model for each sub-matching
		  task without any user involvement. Each local matching
		  learning model automatically provides adequate matching
		  settings for each local sub-matching task. Our results show
		  that: (i) partitioning approach outperforms existing
		  techniques, (ii) local matching while using a specific
		  machine learning model for each sub-matching task yields to
		  promising results and (iii) the combination between
		  partitioning and machine learning increases the overall
		  result.},
  booktitle	= {Proceedings of the 34th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {2285–2292},
  numpages	= {8},
  keywords	= {machine learning, ontology matching, ontology
		  partitioning, semantic web},
  location	= {Limassol, Cyprus},
  series	= {SAC '19}
}

@Article{	  10.1145/3606702,
  author	= {Adamou, Alessandro and Picca, Davide and Hou, Yumeng and
		  Loreto Granados-Garc\'{\i}a, Paula},
  title		= {The Facets of Intangible Heritage in Southern Chinese
		  Martial Arts: Applying a Knowledge-driven Cultural Contact
		  Detection Approach},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3606702},
  doi		= {10.1145/3606702},
  abstract	= {Investigating the intangible nature of a cultural domain
		  can take multiple forms, addressing, for example, the
		  aesthetic, epistemic, and social dimensions of its
		  phenomenology. The context of Southern Chinese martial arts
		  is of particular significance, as it carries immaterial
		  components of all these aspects: The technical and
		  stylistic framework of a martial art system; the imagery
		  associated to movements; and the transmission of knowledge
		  orally, practically, or through influence, are but examples
		  of intangible characteristics that can and should be
		  captured, not unlike cultural artifacts. The latter
		  case—the one of formalizing cultural influence through
		  its various forms of evidenceis emblematic as well as
		  largely untrodden ground. A previous attempt at detecting
		  cultural influence computationally was made in the context
		  of Roman archaeology, though the binding of that early
		  effort with the domain model was tight; also, there has
		  hardly been any prior dedicated effort to model the martial
		  arts domain through ontologies. In this article, we present
		  the realization of the full cycle of a computational
		  approach to investigating cultural contact in Southern
		  Chinese martial arts. The entire approach is predicated
		  upon the usage of standards and techniques of the Semantic
		  Web and formal knowledge. Starting from a modular domain
		  ontology, which models martial arts independently of the
		  goal of capturing cultural influence, we perform knowledge
		  extraction from archival material from the Hong Kong
		  Martial Arts Living Archive and generate a dataset of the
		  results modeled after said ontology. Then, we combine the
		  resulting knowledge base with a rule model that represents
		  ways to infer knowledge of potential contact between
		  cultures based on the evidence present in the knowledge
		  base. The results offer an insight into how an
		  inference-based computational model can be applied to
		  detect interesting facts even in the as-yet underexplored
		  domain of intangible cultural heritage. The implemented
		  workflow shows that the full-cycle employment of semantic
		  technologies can offer the ground truth required for
		  largely different approaches, such as statistical and
		  machine learning ones, to operate.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {63},
  numpages	= {27},
  keywords	= {Intangible cultural heritage, digitization, Semantic Web,
		  embodied knowledge, knowledge representation, ontologies,
		  inferencing}
}

@Article{	  10.1145/3548458,
  author	= {Chen, Mu-Yen and Lai, Yi-Wei},
  title		= {Using Fuzzy Clustering with Deep Learning Models for
		  Detection of COVID-19 Disinformation},
  year		= {2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3548458},
  doi		= {10.1145/3548458},
  abstract	= {Since the beginning of 2020, the COVID-19 pandemic has
		  killed millions of people around the world, leading to a
		  worldwide panic that has fueled the rapid and widespread
		  dissemination of COVID-19-related disinformation on social
		  media. The phenomenon, described by the World Health
		  Organization (WHO) as an "indodemic" presents a serious
		  challenge to governments and public health authorities, but
		  the spread of misinformation has made human detection less
		  efficient than the rate of spread. While there have been
		  many studies developing automated detection techniques for
		  COVID-19 fake news, the results often refer to high
		  accuracy but rarely to model detection time. This research
		  uses fuzzy theory to extract features and uses multiple
		  deep learning model frameworks to detect Chinese and
		  English COVID-19 misinformation. With the reduction of text
		  features, the detection time of the model is significantly
		  reduced, and the model accuracy does not drop excessively.
		  This study designs two different feature extraction methods
		  based on fuzzy classification and compares the results with
		  different deep learning models. BiLSTM was found to provide
		  the best detection results for COVID-19 misinformation by
		  directly using deep learning models, with 99\% accuracy in
		  English and 86\% accuracy in Chinese. Applying fuzzy
		  clustering to English COVID-19 fake news data features
		  maintains 99\% accuracy while reducing detection time by
		  10\%. For Chinese misinformation, detection time is reduced
		  by 15\% at the cost of an 8\% drop in accuracy.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  keywords	= {Misinformation detection, COVID-19, Deep learning model,
		  Fuzzy clustering}
}

@Article{	  10.1145/3191832,
  author	= {Bienvenu, Meghyn and Kikot, Stanislav and Kontchakov,
		  Roman and Podolskii, Vladimir V. and Zakharyaschev,
		  Michael},
  title		= {Ontology-Mediated Queries: Combined Complexity and
		  Succinctness of Rewritings via Circuit Complexity},
  year		= {2018},
  issue_date	= {October 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {65},
  number	= {5},
  issn		= {0004-5411},
  url		= {https://doi.org/10.1145/3191832},
  doi		= {10.1145/3191832},
  abstract	= {We give solutions to two fundamental computational
		  problems in ontology-based data access with the W3C
		  standard ontology language OWL&nbsp;2&nbsp;QL: the
		  succinctness problem for first-order rewritings of
		  ontology-mediated queries (OMQs) and the complexity problem
		  for OMQ answering. We classify OMQs according to the shape
		  of their conjunctive queries (treewidth, the number of
		  leaves) and the existential depth of their ontologies. For
		  each of these classes, we determine the combined complexity
		  of OMQ answering and whether all OMQs in the class have
		  polynomial-size first-order, positive existential, and
		  nonrecursive datalog rewritings. We obtain the succinctness
		  results using hypergraph programs, a new computational
		  model for Boolean functions, which makes it possible to
		  connect the size of OMQ rewritings and circuit
		  complexity.},
  journal	= {J. ACM},
  month		= aug,
  articleno	= {28},
  numpages	= {51},
  keywords	= {Ontology-based data access, computational complexity,
		  ontology-mediated query, query rewriting, succinctness}
}

@InProceedings{	  10.1145/3579051.3579070,
  author	= {Gu, Zhenzhen and Lanti, Davide and Mosca, Alessandro and
		  Xiao, Guohui and Xiong, Jing and Calvanese, Diego},
  title		= {Ontology-based Data Federation},
  year		= {2023},
  isbn		= {9781450399876},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579051.3579070},
  doi		= {10.1145/3579051.3579070},
  abstract	= {Ontology-based data access (OBDA) is a well-established
		  approach to information management which facilitates the
		  access to a (single) relational data source through the
		  mediation of a high-level ontology, and the use of a
		  declarative mapping linking the data layer to the ontology.
		  We formally introduce here the notion of ontology-based
		  data federation (OBDF) to denote a framework that combines
		  OBDA with a data federation layer where multiple, possibly
		  heterogeneous sources are virtually exposed as a single
		  relational database. We discuss opportunities and
		  challenges of OBDF, and provide techniques to deliver
		  efficient query answering in an OBDF setting. Such
		  techniques are validated through an extensive experimental
		  evaluation based on the Berlin SPARQL Benchmark.},
  booktitle	= {Proceedings of the 11th International Joint Conference on
		  Knowledge Graphs},
  pages		= {10–19},
  numpages	= {10},
  keywords	= {Data federation, OBDA, Query optimization},
  location	= {Hangzhou, China},
  series	= {IJCKG '22}
}

@InProceedings{	  10.1145/3274192.3274204,
  author	= {Paim, Polianna and Prietch, Soraia and Duarte, Anderson},
  title		= {CoDesign in the Exploratory Phase of an Assistive
		  Technology product Design to support the Teaching-Learning
		  Process of Brazilian-Portuguese Language for Visual
		  Persons},
  year		= {2018},
  isbn		= {9781450366014},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3274192.3274204},
  doi		= {10.1145/3274192.3274204},
  abstract	= {Libras is a communication and expression language of many
		  Visual Person (VP) in Brazil. In National High School
		  Examination (ENEM), for candidates to a vacancy in higher
		  education institutions, the essay must be written in
		  Portuguese, not taking into account the first language of
		  the candidate. This paper presents an adapted framework
		  that groups concepts of CoDesign, technology adoption, HCI
		  life cycle and Semantic Numbers, and also shows the results
		  of first phase of research. This phase consists of a
		  contextual analysis of an Assistive Technology project for
		  Portuguese teaching as a second language for VP. The
		  adapted framework aims to reach objectives, working with
		  interested parts, to be accessible and potentially adopted
		  by them. As results we have defined Interested Parts
		  Diagram, discussed responses applied surveys and elaborated
		  Personas. Also, it was possible to observe the relation of
		  the PV with the Portuguese, their wishes and difficulties
		  with ENEM.},
  booktitle	= {Proceedings of the 17th Brazilian Symposium on Human
		  Factors in Computing Systems},
  articleno	= {12},
  numpages	= {9},
  keywords	= {Assistive Technology, CoDesign, Semantic Numbers, Visual
		  People},
  location	= {Bel\'{e}m, Brazil},
  series	= {IHC '18}
}

@InProceedings{	  10.1145/3360901.3364424,
  author	= {Chen, Jieying and Alghamdi, Ghadah and Schmidt, Renate A.
		  and Walther, Dirk and Gao, Yongsheng},
  title		= {Ontology Extraction for Large Ontologies via Modularity
		  and Forgetting},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364424},
  doi		= {10.1145/3360901.3364424},
  abstract	= {We are interested in the computation of ontology extracts
		  based on forgetting from large ontologies in real-world
		  scenarios. Such scenarios require nearly all of the terms
		  in the ontology to be forgotten, which poses a significant
		  challenge to forgetting tools. In this paper we show that
		  modularization and forgetting can be combined beneficially
		  in order to compute ontology extracts. While a module is a
		  subset of axioms of a given ontology, the solution of
		  forgetting (also known as a uniform interpolant) is a
		  compact representation of the ontology limited to a subset
		  of the signature. The approach introduced in this paper
		  uses an iterative workflow of four stages: (i)~extension of
		  the given signature and, if needed partitioning,
		  (ii)~modularization, (iii)~forgetting, and (iv)~evaluation
		  by domain expert. For modularization we use three kinds of
		  modules: locality-based, semantic and minimal subsumption
		  modules. For forgetting three tools are used: NUI, LETHE
		  and FAME. An evaluation on the SNOMED CT and NCIt
		  ontologies for standard concept name lists showed that
		  precomputing ontology modules reduces the number of terms
		  that need to be forgotten. An advantage of the presented
		  approach is high precision of the computed ontology
		  extracts.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {45–52},
  numpages	= {8},
  keywords	= {description logics, forgetting, knowledge abstraction,
		  knowledge management, knowledge representation and
		  reasoning, ontology abstraction, ontology modularity,
		  uniform interpolation},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.5555/3615924.3623629,
  author	= {Jiang, Hao and Tsiounis, Konstantinos and Kontogiannis,
		  Kostas},
  title		= {Digital Twin Models for Resource Oriented Service
		  Systems},
  year		= {2023},
  publisher	= {IBM Corp.},
  address	= {USA},
  abstract	= {Resource-oriented service computing has emerged as one of
		  the major paradigms for building large scale distributed
		  systems. These systems can grow very complex and require
		  access to diverse re-sources. In order to shorten the
		  development time of such systems, we can consider the use
		  of digital twin models. We propose a frame-work whereby a
		  resource-oriented service computing system can be
		  represented as a collection of models that denote three
		  major perspectives, namely structure, behavior and intent
		  of stakeholders’ goals. Furthermore, in order for the
		  digital twin model to be of practical use, the models
		  denoting the different perspectives have to be integrated.
		  In this respect, we propose an architecture which allows
		  these models to exchange information using open source
		  topic-based publish-subscribe systems, and discuss the
		  elements of its associated run-time engine.},
  booktitle	= {Proceedings of the 33rd Annual International Conference on
		  Computer Science and Software Engineering},
  pages		= {226–229},
  numpages	= {4},
  keywords	= {Resource Oriented Systems, Service Computing, REST,
		  Digital Twin Models, DevOps, Middleware.},
  location	= {Las Vegas, NV, USA},
  series	= {CASCON '23}
}

@InProceedings{	  10.1145/3638209.3638213,
  author	= {Glass, Ayse and Noennig, Jorg R. and Bek, Burak and Glass,
		  Roman and Menges, Eylul K. and Okhrin, Iryna and Baddam,
		  Pramod and Sanchez, Mariela Rossana and Senthil, Gunalan
		  and J\"{a}kel, Ren\'{e}},
  title		= {Innovative Urban Design Simulation: Utilizing Agent-Based
		  Modelling through Reinforcement Learning},
  year		= {2024},
  isbn		= {9798400709067},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638209.3638213},
  doi		= {10.1145/3638209.3638213},
  abstract	= {Data-driven design for cities is improving the quality of
		  everyday life of citizens and optimizes the usage of
		  resources. A new aspect is artificial intelligence, which
		  Smart Cities could greatly benefit from. A central problem
		  for urban designers is the unavailability of data to make
		  relevant decisions. Agent-based simulations enable a view
		  of the dynamic properties of the urban system, generating
		  data in its course. However, the simulation must remain
		  sufficiently simple to remain in the realm of
		  computability. The research question of this paper is: How
		  can we make agents behave more realistically to analyze
		  citizens’ mobility behavior? To solve this problem, we
		  first created a simulated virtual environment, where agents
		  can move freely in a small part of a city, the harbor area
		  in Hamburg, Germany. We assumed that happiness is a crucial
		  motivating factor for the movement of citizens. A survey of
		  130 citizens provided the weights that govern the simulated
		  environment and the happiness score assignation of places.
		  As an AI method, we then used Reinforcement Learning as a
		  general model and Q-learning as an algorithm to generate a
		  baseline. Through randomly traversing the model environment
		  a baseline was created. We are in the process of enhancing
		  Reinforcement Learning with a Deep Q-Network to make the
		  actors learn. Early experiments show a significant
		  improvement over a tabular Q-learning approach. This paper
		  contributes to the literature of urban planning, and
		  data-driven architectural design. The main contribution is
		  replacing the inefficient search for a global maximum of
		  the happiness function, with an efficient local solution
		  global maximum. This has implications for further research
		  in the generation of synthetic data through simulations.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Computational Intelligence and Intelligent Systems},
  pages		= {20–25},
  numpages	= {6},
  keywords	= {agent-based modeling, artificial intelligence, city
		  simulations, smart cities, synthetic data, urban design},
  location	= {Tokyo, Japan},
  series	= {CIIS '23}
}

@Article{	  10.1109/tcbb.2022.3181300,
  author	= {Tian, Zhen and Fang, Haichuan and Teng, Zhixia and Ye,
		  Yangdong},
  title		= {GOGCN: Graph Convolutional Network on Gene Ontology for
		  Functional Similarity Analysis of Genes},
  year		= {2022},
  issue_date	= {March-April 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {2},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3181300},
  doi		= {10.1109/TCBB.2022.3181300},
  abstract	= {The measurement of gene functional similarity plays a
		  critical role in numerous biological applications, such as
		  gene clustering, the construction of gene similarity
		  networks. However, most existing approaches still rely
		  heavily on traditional computational strategies, which are
		  not guaranteed to achieve satisfactory performance. In this
		  study, we propose a novel computational approach called
		  &lt;bold&gt;GOGCN&lt;/bold&gt; to measure gene functional
		  similarity by modeling the Gene Ontology
		  (&lt;bold&gt;GO&lt;/bold&gt;) through Graph Convolutional
		  Network (&lt;bold&gt;GCN&lt;/bold&gt;). GOGCN is a
		  graph-based approach that performs sufficient
		  representation learning for terms and relations in the GO
		  graph. First, GOGCN employs the GCN-based knowledge graph
		  embedding (KGE) model to learn vector representations
		  (i.e., embeddings) for all entities (i.e., terms). Second,
		  GOGCN calculates the semantic similarity between two terms
		  based on their corresponding vector representations.
		  Finally, GOGCN estimates gene functional similarity by
		  making use of the pair-wise strategy. During the
		  representation learning period, GOGCN promotes semantic
		  interaction between terms through GCN, thereby capturing
		  the rich structural information of the GO graph. Further
		  experimental results on various datasets suggest that GOGCN
		  is superior to the other state-of-the-art approaches, which
		  shows its reliability and effectiveness.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jun,
  pages		= {1053–1064},
  numpages	= {12}
}

@InProceedings{	  10.1145/3608251.3608255,
  author	= {Li, Jun and Zhou, Yong and Liu, Xin and Liu, Jinsong and
		  Li, Qing and Kong, Xiangjun and Niu, Guankai},
  title		= {Study on the Reference Architecture and Multi-Dimensional
		  Fusion Framework of Production Equipment Models},
  year		= {2023},
  isbn		= {9798400707919},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3608251.3608255},
  doi		= {10.1145/3608251.3608255},
  abstract	= {With the accelerated integration of the new generation
		  information technology and manufacturing industry, the
		  performance of production equipment continues to be
		  optimized, evolved and upgraded iteratively. The production
		  equipment model presents a multi-dimensional, multi-level,
		  full life cycle integration development trend, which has
		  become the key to improve the innovative capability and
		  digital management level of production equipment. Aiming at
		  the problems of multiple types of production equipment
		  models, such as complex structure, difficult integration
		  and low application efficiency, this study analyzed the
		  relationship and interaction mechanism among production
		  equipment models from the perspectives of life cycle,
		  hierarchical structure and multi-dimension, and built a
		  production equipment model architecture. Based on the model
		  architecture of production equipment, a general
		  multi-dimensional fusion framework of production equipment
		  models was proposed, which takes the mechanism model as the
		  core. This study can provide reference for the
		  construction, integration and application of production
		  equipment models.},
  booktitle	= {Proceedings of the 2023 15th International Conference on
		  Computer Modeling and Simulation},
  pages		= {172–183},
  numpages	= {12},
  location	= {Dalian, China},
  series	= {ICCMS '23}
}

@InProceedings{	  10.1145/3419604.3419619,
  author	= {Sebbaq, Hanane and el Faddouli, Nour-eddine and Bennani,
		  Samir},
  title		= {Recommender System to Support MOOCs Teachers: Framework
		  based on Ontology and Linked Data},
  year		= {2020},
  isbn		= {9781450377331},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3419604.3419619},
  doi		= {10.1145/3419604.3419619},
  abstract	= {The proliferation of Massive Open Online Courses (MOOCs)
		  has generated conflicting opinions about their quality. In
		  this paper, we aim at improving the quality of MOOCs
		  through assisting teachers and designers from the
		  initiation phase of MOOCs. For this purpose, we propose a
		  recommendation system Framework based on the knowledge
		  about teachers and MOOCs. Our approach aims to overcome the
		  problems of traditional recommendation systems, by using
		  and integrating different techniques: modeling via
		  ontologies, semantic web technologies, extracting and
		  integrating Linked Data from different sources, ontology
		  mapping and semantic similarity measures.},
  booktitle	= {Proceedings of the 13th International Conference on
		  Intelligent Systems: Theories and Applications},
  articleno	= {18},
  numpages	= {7},
  keywords	= {Linked Data, MOOC, Ontology, Ontology mapping, Recommender
		  System, Semantic Web, Semantic similarity},
  location	= {Rabat, Morocco},
  series	= {SITA'20}
}

@Article{	  10.1145/3637427,
  author	= {Atrey, Akanksha and Zakaria, Camellia and Balan, Rajesh
		  and Shenoy, Prashant},
  title		= {W4-Groups: Modeling the Who, What, When and Where of Group
		  Behavior via Mobility Sensing},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW1},
  url		= {https://doi.org/10.1145/3637427},
  doi		= {10.1145/3637427},
  abstract	= {Human social interactions occur in group settings of
		  varying sizes and locations, depending on the type of
		  social activity. The ability to distinguish group
		  formations based on their purposes transforms how group
		  detection mechanisms function. Not only should such tools
		  support the effective detection of serendipitous
		  encounters, but they can derive categories of relation
		  types among users. Determining who is involved, what
		  activity is performed, and when and where the activity
		  occurs are critical to understanding group processes in
		  greater depth, including supporting goal-oriented
		  applications (e.g., performance, productivity, and mental
		  health) that require sensing social factors. In this work,
		  we propose W4-Groups that captures the functional
		  perspective of variability and repeatability when
		  automatically constructing short-term and long-term groups
		  via multiple data sources (e.g., WiFi and location check-in
		  data). We design and implement W4-Groups to detect and
		  extract all four group features who-what-when-where from
		  the user's daily mobility patterns. We empirically evaluate
		  the framework using two real-world WiFi datasets and a
		  location check-in dataset, yielding an average of 92\%
		  overall accuracy, 96\% precision, and 94\% recall. Further,
		  we supplement two case studies to demonstrate the
		  application of W4-Groups for next-group activity prediction
		  and analyzing changes in group behavior at a longitudinal
		  scale, exemplifying short-term and long-term occurrences.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= apr,
  articleno	= {150},
  numpages	= {29},
  keywords	= {group modeling, next activity prediction, social
		  interactions, user mobility}
}

@InProceedings{	  10.1145/3629264.3629273,
  author	= {Xiao, Zheng and Cao, Haowei and Zheng, Dongwei},
  title		= {Model construction and implementation of digital twin data
		  for body workshop},
  year		= {2023},
  isbn		= {9798400700576},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3629264.3629273},
  doi		= {10.1145/3629264.3629273},
  abstract	= {Data is very important for Digital Twin Workshop (DTW).
		  Data in DTW is not only to store it, but also needs to be
		  associated with different physical information. Knowledge
		  graph (KG) can acquire, integrate and utilize the
		  information of the whole life of cycle process. This paper
		  proposes the construction of digital twin data (DTD) by
		  creation process and feature extraction methods of
		  knowledge graph. The DTD combines the actual production
		  data and the time-varying characteristics of geometry,
		  motion and behavior model of the body workshop. It is able
		  to realize the integration with DTW and graph database
		  storage.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Computing and Data Analysis},
  pages		= {1–7},
  numpages	= {7},
  keywords	= {Data model, Digital twin workshop, Knowledge graph},
  location	= {Guiyang, China},
  series	= {ICCDA '23}
}

@InBook{	  10.1145/3581906.3581912,
  author	= {Nikolaou, Charalampos},
  title		= {Geospatial Ontologies},
  year		= {2023},
  isbn		= {9798400707407},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  url		= {https://doi.org/10.1145/3581906.3581912},
  booktitle	= {Geospatial Data Science: A Hands-on Approach for Building
		  Geospatial Applications Using Linked Data Technologies},
  pages		= {67–84},
  numpages	= {18}
}

@InProceedings{	  10.1145/3447568.3448547,
  author	= {Zarri, Gian Piero},
  title		= {Using a High-Level Conceptual Model as a Support for the
		  Generalized World Entities (GWEs) Paradigm},
  year		= {2021},
  isbn		= {9781450376556},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447568.3448547},
  doi		= {10.1145/3447568.3448547},
  abstract	= {This paper concerns the Generalized World Entities (GWEs)
		  paradigm, an extension of the ordinary IoT/WoT (Internet of
		  Things/Web of Things) approach. GWEs offer a unified way to
		  seamlessly model i) conceptual representations of physical
		  objects, humans, robots and low-level events and ii) higher
		  level of abstractions corresponding to structured
		  situations/behaviors implying mutual relationships among
		  low level entities. The GWEs approach is currently
		  implemented in terms of NKRL, the "Narrative Knowledge
		  Representation Language", which is both a Knowledge
		  Representation language and a Computer Science environment.
		  It is expected to represent a significant contribution with
		  respect to bridge the gap between the recognition of
		  entities at sensor level and their description/management
		  at conceptual level.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Information Systems and Technologies},
  articleno	= {39},
  numpages	= {8},
  keywords	= {Generalized World Entities, IoT/WoT, high-level
		  abstraction entities, inferences, ontologies, physical
		  entities, seamless conceptual modelling, semantic
		  approach},
  location	= {Lecce, Italy},
  series	= {ICIST '20}
}

@InProceedings{	  10.1145/3167132.3167343,
  author	= {Besbes, Ghada and Baazaoui-Zghal, Hajer},
  title		= {Fuzzy ontologies for search results diversification:
		  application to medical data},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167343},
  doi		= {10.1145/3167132.3167343},
  abstract	= {Fuzzy ontologies offer an efficient representation of
		  uncertain information in natural language and this
		  representation allows a better interpretation of user
		  queries and documents. Integrating fuzzy ontologies in a
		  search results diversification process may improve the
		  quality of returned documents since diversification helps
		  covering the maximum of user's needs. In this context, we
		  propose an ontology based diversification approach for
		  search results applied to medical domain. The proposal
		  first analyses the query in order to extract medical
		  concepts. A contextual ontology fuzzification is then
		  applied in order to offer an understanding of the user's
		  information needs and finally a fuzzy search result
		  diversification is performed in order to improve the
		  ranking quality of returned documents. We perform a
		  thorough experimental evaluation of our proposal with CLEF
		  e-health 2016 topics. Evaluation results show a major
		  improvement in precision and ranking.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {1968–1975},
  numpages	= {8},
  keywords	= {fuzzy ontology, information retrieval, medical data,
		  search results diversification, semantic web},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InProceedings{	  10.1145/3503047.3503077,
  author	= {Nast, Benjamin and Sandkuhl, Kurt},
  title		= {Meta-Model and Tool Support for the Organizational Aspects
		  of Internet-of-Things Development Methods: Organizational
		  Aspects of IoT Development Methods},
  year		= {2022},
  isbn		= {9781450385862},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503047.3503077},
  doi		= {10.1145/3503047.3503077},
  abstract	= {The Internet-of-Things (IoT) has long become reality and
		  contributes to the digital transformation of many
		  industrial domains. IoT technologies are at the core of
		  industry 4.0 application scenarios, contribute to
		  cyber-physical system implementation, smart connected
		  products and new business models exploiting their
		  potential. There is plenty of work on how to specify,
		  design and implement IoT solutions, but a lot of
		  enterprises struggle to create business value from IoT
		  technology because they have difficulties to define the
		  organizational integration. Methodologies for model-driven
		  engineering (MDE) of IoT solutions should encompass both,
		  organizational and system development and integration, but
		  existing model-based approaches focus on the technical
		  perspective. The paper proposes a modeling approach
		  integrated into enterprise modeling techniques to
		  compensate for this lack. The enterprise modeling language
		  4EM is extended by adding a method component for IoT
		  modeling. The main contributions of this paper are (a) a
		  summary of the state-of-research in the field, (b) an
		  industrial case for model-based IoT development, and (c) a
		  meta-model and tool support for IoT modelling.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Advanced Information Science and System},
  articleno	= {27},
  numpages	= {6},
  keywords	= {Internet-of-Things, Modelling methodologies},
  location	= {Sanya, China},
  series	= {AISS '21}
}

@InProceedings{	  10.1145/3340531.3412768,
  author	= {Zhou, Lu and Shimizu, Cogan and Hitzler, Pascal and
		  Sheill, Alicia M. and Estrecha, Seila Gonzalez and Foley,
		  Catherine and Tarr, Duncan and Rehberger, Dean},
  title		= {The Enslaved Dataset: A Real-world Complex Ontology
		  Alignment Benchmark using Wikibase},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412768},
  doi		= {10.1145/3340531.3412768},
  abstract	= {Ontology alignment has taken a critical place for helping
		  heterogeneous resources to interoperate. It has been
		  studied for over a decade, and over that time many
		  alignment systems and methods have been developed by
		  researchers to find simple 1:1 equivalence matches between
		  two ontologies. However, very few alignment systems focus
		  on finding complex correspondences. Even if the complex
		  alignment systems are developed, the performance of finding
		  complex relations still has a lot of room for improvement.
		  One reason for this limitation may be that there are still
		  few applicable alignment benchmarks that contain such
		  complex relationships that can raise researchers'
		  interests. In this paper, we propose a real-world dataset
		  from the Enslaved project as a potential complex alignment
		  benchmark. The benchmark consists of two resources, the
		  Enslaved Ontology along with a Wikibase repository holding
		  a large number of instance data from the Enslaved project,
		  as well as a manually created reference alignment between
		  them. The alignment was developed in consultation with
		  domain experts in the digital humanities. The alignment not
		  only includes simple 1:1 equivalence correspondences, but
		  also more complex m:n equivalence and subsumption
		  correspondences and are provided in both Expressive and
		  Declarative Ontology Alignment Language (EDOAL) format and
		  rule syntax. The Enslaved benchmark has been incorporated
		  into the Ontology Alignment Evaluation Initiative (OAEI)
		  2020 and is completely free for public use to assist the
		  researchers in developing and evaluating their complex
		  alignment algorithms.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {3197–3204},
  numpages	= {8},
  keywords	= {benchmark, knowledge graph, ontology alignment, wikibase},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

###InProceedings{ 10.1145/3167132.3167343,
  author	= {Besbes, Ghada and Baazaoui-Zghal, Hajer},
  title		= {Fuzzy ontologies for search results diversification:
		  application to medical data},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167343},
  doi		= {10.1145/3167132.3167343},
  abstract	= {Fuzzy ontologies offer an efficient representation of
		  uncertain information in natural language and this
		  representation allows a better interpretation of user
		  queries and documents. Integrating fuzzy ontologies in a
		  search results diversification process may improve the
		  quality of returned documents since diversification helps
		  covering the maximum of user's needs. In this context, we
		  propose an ontology based diversification approach for
		  search results applied to medical domain. The proposal
		  first analyses the query in order to extract medical
		  concepts. A contextual ontology fuzzification is then
		  applied in order to offer an understanding of the user's
		  information needs and finally a fuzzy search result
		  diversification is performed in order to improve the
		  ranking quality of returned documents. We perform a
		  thorough experimental evaluation of our proposal with CLEF
		  e-health 2016 topics. Evaluation results show a major
		  improvement in precision and ranking.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {1968–1975},
  numpages	= {8},
  keywords	= {semantic web, search results diversification, medical
		  data, information retrieval, fuzzy ontology},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InProceedings{	  10.1145/3510455.3512771,
  author	= {Weyssow, Martin and Sahraoui, Houari and Liu, Bang},
  title		= {Better modeling the programming world with code concept
		  graphs-augmented multi-modal learning},
  year		= {2022},
  isbn		= {9781450392242},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3510455.3512771},
  doi		= {10.1145/3510455.3512771},
  abstract	= {The progress made in code modeling has been tremendous in
		  recent years thanks to the design of natural language
		  processing learning approaches based on state-of-the-art
		  model architectures. Nevertheless, we believe that the
		  current state-of-the-art does not focus enough on the full
		  potential that data may bring to a learning process in
		  software engineering. Our vision articulates on the idea of
		  leveraging multi-modal learning approaches to modeling the
		  programming world. In this paper, we investigate one of the
		  underlying idea of our vision whose objective based on
		  concept graphs of identifiers aims at leveraging high-level
		  relationships between domain concepts manipulated through
		  particular language constructs. In particular, we propose
		  to enhance an existing pretrained language model of code by
		  joint-learning it with a graph neural network based on our
		  concept graphs. We conducted a preliminary evaluation that
		  shows gain of effectiveness of the models for code search
		  using a simple joint-learning method and prompts us to
		  further investigate our research vision.},
  booktitle	= {Proceedings of the ACM/IEEE 44th International Conference
		  on Software Engineering: New Ideas and Emerging Results},
  pages		= {21–25},
  numpages	= {5},
  keywords	= {multi-modal learning, concept graphs, code search, code
		  modeling},
  location	= {Pittsburgh, Pennsylvania},
  series	= {ICSE-NIER '22}
}

@InProceedings{	  10.1145/3706598.3713491,
  author	= {Yeh, Catherine and Ren, Donghao and Assogba, Yannick and
		  Moritz, Dominik and Hohman, Fred},
  title		= {Exploring Empty Spaces: Human-in-the-Loop Data
		  Augmentation},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713491},
  doi		= {10.1145/3706598.3713491},
  abstract	= {Data augmentation is crucial to make machine learning
		  models more robust and safe. However, augmenting data can
		  be challenging as it requires generating diverse data
		  points to rigorously evaluate model behavior on edge cases
		  and mitigate potential harms. Creating high-quality
		  augmentations that cover these “unknown unknowns” is a
		  time- and creativity-intensive task. In this work, we
		  introduce Amplio, an interactive tool to help practitioners
		  navigate “unknown unknowns” in unstructured text
		  datasets and improve data diversity by systematically
		  identifying empty data spaces to explore. Amplio includes
		  three human-in-the-loop data augmentation techniques:
		  Augment with Concepts, Augment by Interpolation, and
		  Augment with Large Language Model. In a user study with 18
		  professional red teamers, we demonstrate the utility of our
		  augmentation methods in helping generate high-quality,
		  diverse, and relevant model safety prompts. We find that
		  Amplio enabled red teamers to augment data quickly and
		  creatively, highlighting the transformative potential of
		  interactive augmentation workflows.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {847},
  numpages	= {19},
  keywords	= {Human-in-the-loop data augmentation, interactive
		  visualization, data diversity, sparse autoencoders,
		  language models},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3649921.3650001,
  author	= {Zhou, Hongwei and Zhu, Jichen and Mateas, Michael and
		  Wardrip-Fruin, Noah},
  title		= {The Eyes, the Hands and the Brain: What can Text-to-Image
		  Models Offer for Game Design and Visual Creativity?},
  year		= {2024},
  isbn		= {9798400709555},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3649921.3650001},
  doi		= {10.1145/3649921.3650001},
  abstract	= {Text-to-image models such as DALL-E, Stable Diffusion, and
		  Midjourney have seen a boom in development and adoption in
		  both commercial and hobbyist spaces. This paper is a
		  theoretical analysis aimed at informing the development of
		  games that help improve critical literacy around
		  text-to-image models. It asks: what assumptions and
		  perspectives do text-to-image models have on visual
		  creativity, and how do we bring that out through games? We
		  propose a theory to differentiate between seeing an image
		  through the expression of color, shapes and lines, and
		  seeing an image through the recognition of concepts and
		  ideas. These two ways of seeing are two different ways of
		  orienting the player/user to their visual creativity. While
		  traditional painting mechanics emphasize the former,
		  text-to-image interfaces emphasize the latter. We deploy
		  this perspective to study games with traditional painting
		  interactions and games with text-to-image interactions.
		  This paper hopes to contribute to design both broadly for
		  games about visual creativity, and narrowly for gameplay
		  with text-to-image models — specifically, how the latter
		  fosters a different type of visual creativity than
		  traditional painting interactions.},
  booktitle	= {Proceedings of the 19th International Conference on the
		  Foundations of Digital Games},
  articleno	= {23},
  numpages	= {13},
  keywords	= {Game Design, Painting, Stable Diffusion, Text-to-Image,
		  Visual Creativity},
  location	= {Worcester, MA, USA},
  series	= {FDG '24}
}

@InProceedings{	  10.1145/3417990.3418741,
  author	= {Boubekeur, Younes and Mussbacher, Gunter and McIntosh,
		  Shane},
  title		= {Automatic assessment of students' software models using a
		  simple heuristic and machine learning},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3418741},
  doi		= {10.1145/3417990.3418741},
  abstract	= {Software models are increasingly popular. To educate the
		  next generation of software engineers, it is important that
		  they learn how to model software systems well, so that they
		  can design them effectively in industry. It is also
		  important that instructors have the tools that can help
		  them assess students' models more effectively. In this
		  paper, we investigate how a tool that combines a simple
		  heuristic with machine learning techniques can be used to
		  help assess student submissions in model-driven engineering
		  courses. We apply our proposed technique to first identify
		  submissions of high quality and second to predict
		  approximate letter grades. The results are comparable to
		  human grading and a complex rule-based technique for the
		  former and surprisingly accurate for the latter.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {20},
  numpages	= {10},
  keywords	= {heuristics, grading, domain modeling, assessment, Umple},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@InProceedings{	  10.1145/3613372.3614202,
  author	= {Luz, Carlos and Oliveirajr, Edson and Steinmacher, Igor},
  title		= {A Conceptual Model to Support Teaching of Software
		  Engineering Controlled (Quasi-)Experiments},
  year		= {2023},
  isbn		= {9798400707872},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613372.3614202},
  doi		= {10.1145/3613372.3614202},
  abstract	= {Throughout controlled experimentation, it is possible to
		  provide evidence of the software being developed. In the
		  academic environment, Experimentation in Software
		  Engineering (ESE) is essential to understanding
		  cause-effect relations, enabling a vision of the
		  development process, and taking action on actual events in
		  the software industry. As much as the experimentation
		  processes have been used in industry and academia, there is
		  a lack of formalization of the principles of ESE teaching
		  and artifacts that can be useful to support it in higher
		  education. One of the means to contribute to such a topic
		  would be the design of a conceptual model, which is widely
		  discussed in the literature, thus applying empirical
		  methods for a better understanding of the context and
		  representation of ESE teaching. Thus, in this paper, we
		  developed a conceptual model to support the teaching of
		  controlled experiments and quasi-experiments. To design the
		  conceptual model, we carried out an analysis of metadata
		  from controlled experiments and quasi-experiments in the
		  literature and conducted a survey to collect data from
		  instructors who teach ESE. We evaluated the model with the
		  Technology Acceptance Model (TAM). Results consist of a
		  feasible conceptual model aiming to standardize the basic
		  concepts of ESE and further support the production and
		  reuse of ESE materials.},
  booktitle	= {Proceedings of the XXXVII Brazilian Symposium on Software
		  Engineering},
  pages		= {236–245},
  numpages	= {10},
  keywords	= {Concepts, Conceptual Modeling, Controlled Experimentation,
		  TAM Model, Teaching of Controlled Experimentation},
  location	= {Campo Grande, Brazil},
  series	= {SBES '23}
}

@Proceedings{	  10.1145/3565472,
  title		= {UMAP '23: Proceedings of the 31st ACM Conference on User
		  Modeling, Adaptation and Personalization},
  year		= {2023},
  isbn		= {9781450399326},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Limassol, Cyprus}
}

@InProceedings{	  10.1145/3411764.3445522,
  author	= {Levy, Ariel and Agrawal, Monica and Satyanarayan, Arvind
		  and Sontag, David},
  title		= {Assessing the Impact of Automated Suggestions on Decision
		  Making: Domain Experts Mediate Model Errors but Take Less
		  Initiative},
  year		= {2021},
  isbn		= {9781450380966},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411764.3445522},
  doi		= {10.1145/3411764.3445522},
  abstract	= {Automated decision support can accelerate tedious tasks as
		  users can focus their attention where it is needed most.
		  However, a key concern is whether users overly trust or
		  cede agency to automation. In this paper, we investigate
		  the effects of introducing automation to annotating
		  clinical texts&nbsp;—&nbsp;a multi-step, error-prone task
		  of identifying clinical concepts (e.g., procedures) in
		  medical notes, and mapping them to labels in a large
		  ontology. We consider two forms of decision aid:
		  recommending which labels to map concepts to, and
		  pre-populating annotation suggestions. Through laboratory
		  studies, we find that 18 clinicians generally build
		  intuition of when to rely on automation and when to
		  exercise their own judgement. However, when presented with
		  fully pre-populated suggestions, these expert users exhibit
		  less agency: accepting improper mentions, and taking less
		  initiative in creating additional annotations. Our findings
		  inform how systems and algorithms should be designed to
		  mitigate the observed issues.},
  booktitle	= {Proceedings of the 2021 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {72},
  numpages	= {13},
  keywords	= {text tagging, ontology, mental model, human-AI teams,
		  clinical annotation, agency},
  location	= {Yokohama, Japan},
  series	= {CHI '21}
}

@InProceedings{	  10.1145/3726302.3730324,
  author	= {Dammu, Preetam Prabhu Srikar and Naidu, Himanshu and Shah,
		  Chirag},
  title		= {Dynamic-KGQA: A Scalable Framework for Generating Adaptive
		  Question Answering Datasets},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3730324},
  doi		= {10.1145/3726302.3730324},
  abstract	= {As question answering (QA) systems advance alongside the
		  rapid evolution of foundation models, the need for robust,
		  adaptable, and large-scale evaluation benchmarks becomes
		  increasingly critical. Traditional QA benchmarks are often
		  static and publicly available, making them susceptible to
		  data contamination and memorization by large language
		  models (LLMs). Consequently, static benchmarks may
		  overestimate model generalization and hinder a reliable
		  assessment of real-world performance. In this work, we
		  introduce Dynamic-KGQA, a scalable framework for generating
		  adaptive QA datasets from knowledge graphs (KGs), designed
		  to mitigate memorization risks while maintaining
		  statistical consistency across iterations. Unlike fixed
		  benchmarks, Dynamic-KGQA generates a new dataset variant on
		  every run while preserving the underlying distribution,
		  enabling fair and reproducible evaluations. Furthermore,
		  our framework provides fine-grained control over dataset
		  characteristics, supporting domain-specific and
		  topic-focused QA dataset generation. Additionally,
		  Dynamic-KGQA produces compact, semantically coherent
		  subgraphs that facilitate both training and evaluation of
		  KGQA models, enhancing their ability to leverage structured
		  knowledge effectively. To align with existing evaluation
		  protocols, we also provide static large-scale
		  train/test/validation splits, ensuring comparability with
		  prior methods. By introducing a dynamic, customizable
		  benchmarking paradigm, Dynamic-KGQA enables a more rigorous
		  and adaptable evaluation of QA systems.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3498–3508},
  numpages	= {11},
  keywords	= {benchmark, dynamic evaluation, kgqa, knowledge graphs,
		  large language models, question answering},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@InProceedings{	  10.1145/3307363.3307387,
  author	= {Ding, Zheyuan and Yao, Li and Liu, Bin and Wu, Junfeng},
  title		= {Review of the Application of Ontology in the Field of
		  Image Object Recognition},
  year		= {2019},
  isbn		= {9781450366199},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3307363.3307387},
  doi		= {10.1145/3307363.3307387},
  abstract	= {Image object recognition is an important research field in
		  computer vision. It has a wide application prospect and
		  practical significance in the information age. Although the
		  current image recognition technology has achieved high
		  accuracy in some tasks, the computer has many deficiencies
		  in the automatic recognition of images such as fine-grained
		  recognition, recognition of complex scenes. In these tasks,
		  some issues exist like insufficient precision, complex
		  high-level semantics which is difficult to identify and so
		  on. This paper reviews the application of ontology in image
		  object recognition. It is found that combining ontology
		  knowledge model and traditional image recognition
		  technology can improve recognition accuracy, enhance
		  high-level semantic recognition ability, reduce the demand
		  of the large number of training samples, and improve the
		  scalability of the image recognition system. Otherwise,
		  this paper also summarizes the frontier research of
		  ontology applied in the field of image object recognition
		  and the difficulties of deep integration of different
		  technologies and ontology.},
  booktitle	= {Proceedings of the 11th International Conference on
		  Computer Modeling and Simulation},
  pages		= {142–146},
  numpages	= {5},
  keywords	= {ontology, image object recognition},
  location	= {North Rockhampton, QLD, Australia},
  series	= {ICCMS '19}
}

@Article{	  10.1145/3473973,
  author	= {Xie, Qianqian and Zhu, Yutao and Huang, Jimin and Du, Pan
		  and Nie, Jian-Yun},
  title		= {Graph Neural Collaborative Topic Model for Citation
		  Recommendation},
  year		= {2021},
  issue_date	= {July 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {3},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3473973},
  doi		= {10.1145/3473973},
  abstract	= {Due to the overload of published scientific articles,
		  citation recommendation has long been a critical research
		  problem for automatically recommending the most relevant
		  citations of given articles. Relational topic models (RTMs)
		  have shown promise on citation prediction via joint
		  modeling of document contents and citations. However,
		  existing RTMs can only capture pairwise or direct
		  (first-order) citation relationships among documents. The
		  indirect (high-order) citation links have been explored in
		  graph neural network–based methods, but these methods
		  suffer from the well-known explainability problem. In this
		  article, we propose a model called Graph Neural
		  Collaborative Topic Model that takes advantage of both
		  relational topic models and graph neural networks to
		  capture high-order citation relationships and to have
		  higher explainability due to the latent topic semantic
		  structure. Experiments on three real-world citation
		  datasets show that our model outperforms several
		  competitive baseline methods on citation recommendation. In
		  addition, we show that our approach can learn better topics
		  than the existing approaches. The recommendation results
		  can be well explained by the underlying topics.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= nov,
  articleno	= {48},
  numpages	= {30},
  keywords	= {collaborative filtering, relational topic models, graph
		  neural networks, Explainable citation recommendation}
}

@InProceedings{	  10.1145/3463677.3463750,
  author	= {Panchal, Ronak and Swaminarayan, Priya and Tiwari, Sanju
		  and Ortiz-Rodriguez, Fernando},
  title		= {AISHE-Onto: A Semantic Model for Public Higher Education
		  Universities},
  year		= {2021},
  isbn		= {9781450384926},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3463677.3463750},
  doi		= {10.1145/3463677.3463750},
  abstract	= {The Electronic Government is a challenging field for the
		  Semantic Web and the ontologies play a key role in the
		  development of the Semantic Web. This paper explains the
		  terms of the university through university ontology. We
		  will focus on creating a university ontology. Here an
		  ontology-based case study has been implemented for Public
		  Higher Education (AISHE-Onto). SPARQL queries have been
		  applied to make reasoning with the proposed ontology. As a
		  result, a successful query interface has been provided to
		  search academic data by the AISHE-Onto semantic portal.},
  booktitle	= {Proceedings of the 22nd Annual International Conference on
		  Digital Government Research},
  pages		= {545–547},
  numpages	= {3},
  keywords	= {Egovernment, Ontologies, Public Administration, Semantic
		  Web},
  location	= {Omaha, NE, USA},
  series	= {dg.o '21}
}

@Article{	  10.1145/3412843,
  author	= {van Rozen, Riemer},
  title		= {Languages of Games and Play: A Systematic Mapping Study},
  year		= {2021},
  issue_date	= {November 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3412843},
  doi		= {10.1145/3412843},
  abstract	= {Digital games are a powerful means for creating enticing,
		  beautiful, educational, and often highly addictive
		  interactive experiences that impact the lives of billions
		  of players worldwide. We explore what informs the design
		  and construction of good games to learn how to speed-up
		  game development. In particular, we study to what extent
		  languages, notations, patterns, and tools, can offer
		  experts theoretical foundations, systematic techniques, and
		  practical solutions they need to raise their productivity
		  and improve the quality of games and play. Despite the
		  growing number of publications on this topic there is
		  currently no overview describing the state-of-the-art that
		  relates research areas, goals, and applications. As a
		  result, efforts and successes are often one-off, lessons
		  learned go overlooked, language reuse remains minimal, and
		  opportunities for collaboration and synergy are lost. We
		  present a systematic map that identifies relevant
		  publications and gives an overview of research areas and
		  publication venues. In addition, we categorize research
		  perspectives along common objectives, techniques, and
		  approaches, illustrated by summaries of selected languages.
		  Finally, we distill challenges and opportunities for future
		  research and development.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {123},
  numpages	= {37},
  keywords	= {tools, systematic map, patterns, notations, languages,
		  game design, Game development}
}

@InProceedings{	  10.1145/3485447.3511949,
  author	= {Fan, Lu and Li, Qimai and Liu, Bo and Wu, Xiao-Ming and
		  Zhang, Xiaotong and Lv, Fuyu and Lin, Guli and Li, Sen and
		  Jin, Taiwei and Yang, Keping},
  title		= {Modeling User Behavior with Graph Convolution for
		  Personalized Product Search},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511949},
  doi		= {10.1145/3485447.3511949},
  abstract	= {User preference modeling is a vital yet challenging
		  problem in personalized product search. In recent years,
		  latent space based methods have achieved state-of-the-art
		  performance by jointly learning semantic representations of
		  products, users, and text tokens. However, existing methods
		  are limited in their ability to model user preferences.
		  They typically represent users by the products they visited
		  in a short span of time using attentive models and lack the
		  ability to exploit relational information such as
		  user-product interactions or item co-occurrence relations.
		  In this work, we propose to address the limitations of
		  prior arts by exploring local and global user behavior
		  patterns on a user successive behavior graph, which is
		  constructed by utilizing short-term actions of all users.
		  To capture implicit user preference signals and
		  collaborative patterns, we use an efficient jumping graph
		  convolution to explore high-order relations to enrich
		  product representations for user preference modeling. Our
		  approach can be seamlessly integrated with existing latent
		  space based methods and be potentially applied in any
		  product retrieval method that uses purchase history to
		  model user preferences. Extensive experiments on eight
		  Amazon benchmarks demonstrate the effectiveness and
		  potential of our approach. The source code is available at
		  https://github.com/floatSDSDS/SBG .},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {203–212},
  numpages	= {10},
  keywords	= {Graph Convolution, Personalized Product Search, User
		  Preference Modeling},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3377713.3377755,
  author	= {Sun, Xuan and Jiang, Longquan and Zhang, Minghuan and
		  Wang, Cheng and Chen, Ying},
  title		= {Unsupervised Learning for Product Ontology from Textual
		  Reviews on E-Commerce Sites},
  year		= {2020},
  isbn		= {9781450372619},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3377713.3377755},
  doi		= {10.1145/3377713.3377755},
  abstract	= {On modern e-commerce sites, textual reviews are rich in
		  sentiment and opinions about a product, in particular, its
		  specific attributes. Product ontologies, consisting of a
		  taxonomic categorization of lists of such attributes and
		  product types, are useful for a wide range of applications,
		  such as opinion mining and sentiment analysis.
		  Unfortunately, with a paucity of fine-grained hierarchical
		  categorization of products, many smaller sites feature only
		  coarse high-level categories. We present the Iterative
		  Bootstrapping Process (IBP), an unsupervised learning
		  method for such fine-grained hierarchical categorization of
		  products using coarse categories from Chinese product
		  textual reviews. Results show that our model can extract
		  useful product attributes and still can achieve a high
		  accuracy on the task for categorizing unseen products.},
  booktitle	= {Proceedings of the 2019 2nd International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  pages		= {260–264},
  numpages	= {5},
  keywords	= {taxonomy, machine learning, cluster, Ontology},
  location	= {Sanya, China},
  series	= {ACAI '19}
}

@InProceedings{	  10.1145/3434780.3436609,
  author	= {Ferjaoui, Dhekra and Cheniti Belcadhi, Lilia},
  title		= {A Conceptual Model for Personalized Learning based on
		  Educational Robots},
  year		= {2021},
  isbn		= {9781450388504},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3434780.3436609},
  doi		= {10.1145/3434780.3436609},
  abstract	= {Over a previous couple of years, Distance learning has
		  successfully overcome the shortcomings of traditional
		  methods of teaching and learning, likewise increases
		  student interaction and diversities of opinion, online
		  instructors could also be from any location across the
		  world. So students have the chance to settle on a learning
		  strategy most suited to their abilities, while education is
		  streamlined to satisfy the requirements of the individual
		  in question. Due to the on-going technological change, we
		  are witnessing, Robots are getting an integral component of
		  our society and have great potential in being utilized as
		  an academic technology by providing students with a highly
		  interactive and hands-on learning experience. Indeed,
		  Robotics promises to inspire a replacement generation of
		  learning. With the aim of understanding how students can
		  use robots to review, we created and implemented a learning
		  scenario through an ontological conceptual Model for
		  Personalized Learning supported Educational Robots. This
		  model enables us to supply inferences over learning data
		  and supply personalized learning resources, adapted to the
		  progress of the scholar within the learning process.},
  booktitle	= {Eighth International Conference on Technological
		  Ecosystems for Enhancing Multiculturality},
  pages		= {29–33},
  numpages	= {5},
  keywords	= {robots, personalized learning, ontological model, learning
		  scenario, Distance learning},
  location	= {Salamanca, Spain},
  series	= {TEEM'20}
}

@Article{	  10.1145/3657631,
  author	= {Biancofiore, Giovanni Maria and Deldjoo, Yashar and Noia,
		  Tommaso Di and Di Sciascio, Eugenio and Narducci,
		  Fedelucio},
  title		= {Interactive Question Answering Systems: Literature
		  Review},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3657631},
  doi		= {10.1145/3657631},
  abstract	= {Question-answering systems are recognized as popular and
		  frequently effective means of information seeking on the
		  web. In such systems, information seekers can receive a
		  concise response to their queries by presenting their
		  questions in natural language. Interactive question
		  answering is a recently proposed and increasingly popular
		  solution that resides at the intersection of question
		  answering and dialogue systems. On the one hand, the user
		  can ask questions in normal language and locate the actual
		  response to her inquiry; on the other hand, the system can
		  prolong the question-answering session into a dialogue if
		  there are multiple probable replies, very few, or
		  ambiguities in the initial request. By permitting the user
		  to ask more questions, interactive question answering
		  enables users to interact with the system and receive more
		  precise results dynamically.This survey offers a detailed
		  overview of the interactive question-answering methods that
		  are prevalent in current literature. It begins by
		  explaining the foundational principles of
		  question-answering systems, hence defining new notations
		  and taxonomies to combine all identified works inside a
		  unified framework. The reviewed published work on
		  interactive question-answering systems is then presented
		  and examined in terms of its proposed methodology,
		  evaluation approaches, and dataset/application domain. We
		  also describe trends surrounding specific tasks and issues
		  raised by the community, so shedding light on the future
		  interests of scholars. Our work is further supported by a
		  GitHub page synthesizing all the major topics covered in
		  this literature study.},
  journal	= {ACM Comput. Surv.},
  month		= may,
  articleno	= {239},
  numpages	= {38},
  keywords	= {Question answering, natural language processing,
		  interactive systems, human-computer interaction, artificial
		  intelligence, large language model}
}

@InProceedings{	  10.1145/3555962.3555966,
  author	= {Cherian, Sherimon Puliprathu and Sherimon, Vinu and Agith,
		  Align Elsa and Thomas, Anu Achankunju},
  title		= {Design and Development of Alzheimer Disease Ontology},
  year		= {2022},
  isbn		= {9781450396578},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555962.3555966},
  doi		= {10.1145/3555962.3555966},
  abstract	= {Alzheimer is one of the common causes of dementia, which
		  is affecting millions of people around the world. This
		  research study proposes a framework for the knowledge
		  representation of Alzheimer disease using Ontology. The
		  case study is conducted in Sultanate of Oman. In Oman, it
		  is suspected that around 5000 people are affected by this
		  disease. The correct statistics is still unknown as public
		  are not fully aware of the symptoms of this disease yet.
		  So, the reported cases in the hospitals are less, even
		  though it is believed that the actual number of patients is
		  quite large. In this research study, Ontologies are used
		  for knowledge representation of Alzheimer disease.
		  Ontologies are best for semantically representing the
		  knowledge. It is defined as a formal, explicit
		  specification of a shared conceptualization. To develop
		  efficient clinical decision support systems, Ontologies can
		  be used to integrate the patient health record with data
		  from different sources. Ontologies manage and specify the
		  knowledge from the medical domain. Preliminary design of
		  the ontology is given in this paper.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Cloud and Big Data Computing},
  pages		= {19–23},
  numpages	= {5},
  location	= {Birmingham, United Kingdom},
  series	= {ICCBDC '22}
}

@InProceedings{	  10.1145/3529372.3530957,
  author	= {Koch, In\^{e}s},
  title		= {Integration of models for linked data in cultural heritage
		  and contributions to the FAIR principles},
  year		= {2022},
  isbn		= {9781450393454},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3529372.3530957},
  doi		= {10.1145/3529372.3530957},
  abstract	= {Incorporating linked data-based models into the process of
		  describing cultural objects is increasingly important for
		  cultural heritage. Communities such as libraries, archives,
		  and museums have developed and adopted models specific to
		  their contexts. Without a trivial solution, choosing models
		  to support more general applications is challenging. This
		  Ph.D. aims to analyze existing solutions and practices in
		  these domains and propose validated solutions for the
		  discovery, access, interoperability, and reuse of cultural
		  objects, following the FAIR principles. Transversal to the
		  base models used, this research intends to adopt solutions
		  that balance the simplicity of the models with the
		  satisfaction of the requirements.},
  booktitle	= {Proceedings of the 22nd ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {51},
  numpages	= {2},
  keywords	= {semantic web, linked open data, data integration, cultural
		  heritage, FAIR principles},
  location	= {Cologne, Germany},
  series	= {JCDL '22}
}

@Proceedings{	  10.1145/3608251,
  title		= {ICCMS '23: Proceedings of the 2023 15th International
		  Conference on Computer Modeling and Simulation},
  year		= {2023},
  isbn		= {9798400707919},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dalian, China}
}

@InProceedings{	  10.1145/3579051.3579060,
  author	= {Alrabbaa, Christian and Hieke, Willi},
  title		= {Explaining Non-Entailment by Model Transformation for the
		  Description Logic EL},
  year		= {2023},
  isbn		= {9781450399876},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579051.3579060},
  doi		= {10.1145/3579051.3579060},
  abstract	= {Reasoning results computed by description logic systems
		  can be hard to comprehend. When an ontology does not entail
		  an expected subsumption relationship, generating an
		  explanation of this non-entailment becomes necessary. In
		  this paper, we use countermodels to explain
		  non-entailments. More precisely, we devise relevant parts
		  of canonical models of ontologies that serve as
		  explanations and discuss the computational complexity of
		  extracting these parts by means of model transformations.
		  Furthermore, we provide an implementation of these
		  transformations and evaluate it using real ontologies.},
  booktitle	= {Proceedings of the 11th International Joint Conference on
		  Knowledge Graphs},
  pages		= {1–9},
  numpages	= {9},
  keywords	= {Model Transformation, Explainable AI, Description Logics},
  location	= {Hangzhou, China},
  series	= {IJCKG '22}
}

@Proceedings{	  10.1145/3701551,
  title		= {WSDM '25: Proceedings of the Eighteenth ACM International
		  Conference on Web Search and Data Mining},
  year		= {2025},
  isbn		= {9798400713293},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 18th ACM International Conference on Web
		  and Data Mining, this time taking place in Hannover,
		  Germany.Many volunteers have helped to provide excellent
		  content for the conference, most of all the 3 PC Chairs,
		  Meeyoung Cha (Max Planck Institute for Security and Privacy
		  in Germany), Francine Moens (KU Leuven, Belgium) and Marc
		  Najork (Google DeepMind, USA), who selected more than 100
		  papers from over 600 submissions for presentation at the
		  conference.This exciting WSDM 2025 main conference program
		  was extended by interesting demonstrations like Lightning
		  IR for fine-tuning and inference of transformer-based
		  language models for information retrieval, WildlifeLookup,
		  a chatbot designed to facilitate wildlife management and
		  Ventana a la Verdad, a chatbot for navigating Colombian
		  civil conflict archives. Several workshops ranging from
		  language models to trust and verification on the Web
		  complemented this program and provided important
		  opportunities for the discussion of exciting new ideas and
		  approaches.WSDM Cup 2025 challenged more than 1000
		  competitors to create useful and efficient auto-rater
		  models that can accurately reflect human evaluations. This
		  competition focused on the creation of multilingual
		  auto-rater models using chatbot arena data, with exciting
		  solutions. The WSDM 2025 Industry Day featured talks from
		  leading companies on practical applications of web search
		  and data mining. Some notable examples included zero-shot
		  image moderation in Google Ads, advancing Voice AI for
		  E-commerce at Amazon, and Fact-checking of multilingual
		  podcasts. Finally, WSDM Day talks focused on medical
		  research questions and topics, applying language models,
		  knowledge graphs and other AI based approaches.For
		  participants aiming to enter new research areas, the WSDM
		  2025 tutorials covered a broad range of topics, including
		  Robust Information Retrieval, Building Trustworthy AI
		  Models for Medicine, Unifying Bias and Unfairness in
		  Information Retrieval in the LLM Era, Advances in Vector
		  Search and quite a few more. And finally, three excellent
		  invited speakers (Slav Petrov from Google DeepMind and
		  co-lead on Gemini, Roberto Navigli from Sapienza University
		  and Babelscape and Mario Fritz from CISPA) provided
		  exciting insights into the state of AI, about what large
		  language models really understand, as well as privacy and
		  security aspects of neural models.},
  location	= {Hannover, Germany}
}

@InProceedings{	  10.1145/3407982.3407989,
  author	= {Ivanova, Tatyana and Popov, Miroslav},
  title		= {Ontology Evaluation and Multilingualism},
  year		= {2020},
  isbn		= {9781450377683},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3407982.3407989},
  doi		= {10.1145/3407982.3407989},
  abstract	= {Many ontologies have been developed recently, and
		  evaluation is important for researchers or users when
		  searching ontologies needed in their work. Ontology
		  evaluation also is important phase of ontology development,
		  refinement or evolution process. Automatically-developed
		  ontologies as a result of ontology learning also need from
		  evaluation. So, ontology evaluation is important both for
		  selecting ontologies, meeting some requirements, and as a
		  part of ontology life cycle. In this paper we propose deep
		  analysis of ontology evaluation and discuss how
		  multilingualism affects ontology evaluation. We classify
		  ontology evaluation approaches, methods and metrics
		  according to several dimensions. Our analysis and
		  conclusions will be helpful for development of good
		  ontologies and for finding or selection of needed
		  ontologies.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Computer Systems and Technologies},
  pages		= {215–222},
  numpages	= {8},
  keywords	= {Ontology evaluation approaches, Ontology evaluation,
		  Ontology assessment, Ontology, Multilingualism},
  location	= {Ruse, Bulgaria},
  series	= {CompSysTech '20}
}

@InProceedings{	  10.1145/3368691.3368697,
  author	= {U., Sreeja M. and Kovoor, Binsu C.},
  title		= {Object driven semantic multi-video summarisation based on
		  ontology},
  year		= {2019},
  isbn		= {9781450372848},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368691.3368697},
  doi		= {10.1145/3368691.3368697},
  abstract	= {Multi-Video summarisation frameworks aim to extract
		  representative frames from a collection of videos. In the
		  proposed framework, a multi-video summarisation model is
		  implemented that detects key frames from a collection of
		  related videos on the basis of user query object and
		  ontology inference approach. The framework also develops a
		  novel large-scale ontology for video genre identification
		  based on the characteristics of genre-specific videos. The
		  presence of ontologies aids in generating semantically
		  relevant summaries compared to traditional approaches.
		  Quantitative evaluation ensures that maximum information
		  from the video collection on the basis of query is
		  retrieved. Additionally, the ontology-based query inference
		  approach reduces the computation time significantly.
		  Qualitative results prove that the summary generated is
		  concise, informative and semantically relevant.},
  booktitle	= {Proceedings of the Second International Conference on Data
		  Science, E-Learning and Information Systems},
  articleno	= {6},
  numpages	= {6},
  keywords	= {semantic, query, ontology, multi-video, genre-specific},
  location	= {Dubai, United Arab Emirates},
  series	= {DATA '19}
}

@InProceedings{	  10.1145/3417990.3418742,
  author	= {Boubekeur, Younes and Mussbacher, Gunter},
  title		= {Towards a better understanding of interactions with a
		  domain modeling assistant},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3418742},
  doi		= {10.1145/3417990.3418742},
  abstract	= {The enrolment of software engineering students has
		  increased rapidly in the past few years following industry
		  demand. At the same time, model-driven engineering (MDE)
		  continues to become relevant to more domains like embedded
		  systems and machine learning. It is therefore important to
		  teach students MDE skills in an effective manner to prepare
		  them for future careers in academia and industry. The use
		  of interactive online tools can help instructors deliver
		  course material to more students in a more efficient
		  manner, allowing them to offload repetitive or tedious
		  tasks to these systems and focus on other teaching
		  activities that cannot be easily automated. Interactive
		  online tools can provide students with a more engaging
		  learning experience than static resources like books or
		  written exercises. Domain modeling with class diagrams is a
		  fundamental modeling activity in MDE. While there exist
		  multiple modeling tools that allow students to build a
		  domain model, none of them offer an interactive learning
		  experience. In this paper, we explore the interactions
		  between a student modeler and an interactive domain
		  modeling assistant with the aim of better understanding the
		  required interaction. We illustrate desired interactions
		  with three examples and then formalize them in a metamodel.
		  Based on the metamodel, we explain how to form a corpus of
		  learning material that supports the assistant
		  interactions.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {21},
  numpages	= {10},
  keywords	= {learning corpus, feedback, domain model, class diagram,
		  chatbot},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@Proceedings{	  10.1145/3551902,
  title		= {EuroPLop '22: Proceedings of the 27th European Conference
		  on Pattern Languages of Programs},
  year		= {2022},
  isbn		= {9781450395946},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Irsee, Germany}
}

@InProceedings{	  10.1145/3573128.3609346,
  author	= {Sotudeh, Sajad and Goharian, Nazli},
  title		= {OntG-Bart: Ontology-Infused Clinical Abstractive
		  Summarization},
  year		= {2023},
  isbn		= {9798400700279},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3573128.3609346},
  doi		= {10.1145/3573128.3609346},
  abstract	= {Automating the process of clinical text summarization
		  could save clinicians' reading time and reduce their
		  fatigue, acknowledging the necessity of human professionals
		  in the loop. This paper addresses clinical text
		  summarization, aiming to incorporate ontology concept
		  relationships via a Graph Neural Network (GNN) into the
		  summarization process. Specifically, we propose a model,
		  extending Bart's encoder-decoder framework with GNN encoder
		  and multi-head attentional layers for decoder, producing
		  ontology-aware summaries. This GNN interacts with the
		  textual encoder, influencing their mutual representations.
		  The model's effectiveness is validated on two real-world
		  radiology datasets. We also present an ablation study to
		  elucidate the impact of varied graph configurations and an
		  error analysis aimed at pinpointing potential areas for
		  future improvements.},
  booktitle	= {Proceedings of the ACM Symposium on Document Engineering
		  2023},
  articleno	= {17},
  numpages	= {4},
  keywords	= {abstractive summarization, clinical text summarization,
		  neural networks, text summarization},
  location	= {Limerick, Ireland},
  series	= {DocEng '23}
}

@InProceedings{	  10.1145/3401832.3402681,
  author	= {Lembo, Domenico and Li, Yunyao and Popa, Lucian and
		  Scafoglieri, Federico Maria},
  title		= {Ontology mediated information extraction in financial
		  domain with Mastro System-T},
  year		= {2020},
  isbn		= {9781450380300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3401832.3402681},
  doi		= {10.1145/3401832.3402681},
  abstract	= {Information extraction (IE) refers to the task of turning
		  text documents into a structured form, in order to make the
		  information contained therein automatically processable.
		  Ontology Mediated Information Extraction (OMIE) is a new
		  paradigm for IE that seeks to exploit the semantic
		  knowledge expressed in ontologies to improve query
		  answering over unstructured data (properly raw text). In
		  this paper we present Mastro System-T, an OMIE tool born
		  from a joint collaboration between the University of Rome
		  "La Sapienza" and IBM Research Almaden and its first
		  application in a financial domain, namely to facilitate the
		  access to and the sharing of data extracted from the EDGAR
		  system.},
  booktitle	= {Proceedings of the Sixth International Workshop on Data
		  Science for Macro-Modeling},
  articleno	= {3},
  numpages	= {6},
  keywords	= {ontology mediated information extraction, ontology based
		  data access, ontology, information extraction, financial
		  domain},
  location	= {Portland, Oregon},
  series	= {DSMM '20}
}

@InProceedings{	  10.1145/3318236.3318257,
  author	= {Shah, Unnati and Patel, Sankita and Jinwala, Devesh},
  title		= {An Ontological Approach to Specify Conflicts among
		  Non-Functional Requirements},
  year		= {2019},
  isbn		= {9781450362450},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318236.3318257},
  doi		= {10.1145/3318236.3318257},
  abstract	= {It is a usual practice for a user to narrate the
		  Non-Functional Requirements (NFRs) in natural language and
		  the requirements engineers manually try to express the
		  same, using semi-formal or formal language notations.
		  However, inaccurate and the laborious manual approach may
		  fail to detect all potential NFRs and conflicts among them.
		  Existing solutions for specifying NFRs are based on a
		  graphical representation that requires manual efforts.
		  Furthermore, they do not take into account the
		  classification of the types of conflicting NFRs that helps
		  to prioritize NFRs. In addition, these approaches are not
		  used in industrial practice due to three main reasons viz.
		  1) High manual inference 2) Sharing and reusing work can be
		  difficult and 3) No support for machine understanding.
		  Therefore, the aim of our research is to formally specify
		  conflicting NFRs from available natural language NFRs by
		  means of ontological representation that helps requirements
		  analysts prioritize the NFRs at an early stage of
		  requirements engineering.},
  booktitle	= {Proceedings of the 2019 2nd International Conference on
		  Geoinformatics and Data Analysis},
  pages		= {145–149},
  numpages	= {5},
  keywords	= {Specification, Requirements Engineering, Ontology,
		  Non-functional Requirements, Conflict},
  location	= {Prague, Czech Republic},
  series	= {ICGDA '19}
}

@InProceedings{	  10.1145/3194104.3194108,
  author	= {Blersch, Martin and Landh\"{a}u\ss{}er, Mathias and Mayer,
		  Thomas},
  title		= {Semi-automatic generation of active ontologies from web
		  forms for intelligent assistants},
  year		= {2018},
  isbn		= {9781450357234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3194104.3194108},
  doi		= {10.1145/3194104.3194108},
  abstract	= {Intelligent assistants are becoming widespread. A popular
		  method for creating intelligent assistants is modeling the
		  domain (and thus the assistant's capabilities) as Active
		  Ontology. Adding new functionality requires extending the
		  ontology or building new ones; as of today, this process is
		  manual.We describe an automated method for creating Active
		  Ontologies for arbitrary web forms. Our approach leverages
		  methods from natural language processing and data mining to
		  synthesize the ontologies. Furthermore, our tool generates
		  the code needed to process user input.We evaluate the
		  generated Active Ontologies in three case studies using web
		  forms from the UIUC Web Integration Repository, namely from
		  the domains airfare, automobile, and book search. First, we
		  examine how much of the generation process can be automated
		  and how well the approach identifies domain concepts and
		  their relations. Second, we test how well the generated
		  Active Ontologies handle end-user input to perform the
		  desired actions. In our evaluation, Easier automatically
		  generates 65\% of the Active Ontologies' sensor nodes; the
		  generated ontology for airfare search correctly answers
		  70\% of the queries.},
  booktitle	= {Proceedings of the 6th International Workshop on Realizing
		  Artificial Intelligence Synergies in Software Engineering},
  pages		= {28–34},
  numpages	= {7},
  location	= {Gothenburg, Sweden},
  series	= {RAISE '18}
}

@InProceedings{	  10.1145/3447568.3448533,
  author	= {Alnahdi, Amany},
  title		= {Mobile Application Development Ontology},
  year		= {2021},
  isbn		= {9781450376556},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447568.3448533},
  doi		= {10.1145/3447568.3448533},
  abstract	= {Ontologies are structural knowledge components constructed
		  to clarify concepts in a specific domain of knowledge. They
		  are formed to represent the concepts and the relationships
		  between these concepts in that domain. This paper proposes
		  a Mobile Application Development Ontology that
		  conceptualizes the knowledge in the domain of mobile
		  application development. The use of mobile applications is
		  flourishing as mobile devices use is pervasive. The
		  proposed ontology aims to provide a terminology glossary
		  reference for stakeholders in the domain of mobile
		  application development; students, researchers, educators,
		  mobile application analysts, and mobile application
		  developers. The constructed ontology was visualized using a
		  D3 library visualization tool. Applications used for the
		  built ontology include educational purposes and glossaries
		  for classifying concepts in the domain of mobile
		  application development.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Information Systems and Technologies},
  articleno	= {25},
  numpages	= {6},
  keywords	= {Semantic Web, Pedagogy, Ontology, Mobile Applications},
  location	= {Lecce, Italy},
  series	= {ICIST '20}
}

@InProceedings{	  10.1145/3650105.3652291,
  author	= {Khakzad Shahandashti, Kimya and Sivakumar, Mithila and
		  Mohajer, Mohammad Mahdi and Boaye Belle, Alvine and Wang,
		  Song and Lethbridge, Timothy},
  title		= {Assessing the Impact of GPT-4 Turbo in Generating
		  Defeaters for Assurance Cases},
  year		= {2024},
  isbn		= {9798400706097},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3650105.3652291},
  doi		= {10.1145/3650105.3652291},
  abstract	= {Assurance cases (ACs) are structured arguments that allow
		  verifying the correct implementation of the created
		  systems' non-functional requirements (e.g., safety,
		  security). This allows for preventing system failure. The
		  latter may result in catastrophic outcomes (e.g., loss of
		  lives). ACs support the certification of systems in
		  compliance with industrial standards, e.g., DO-178C and ISO
		  26262. Identifying defeaters ---arguments that challenge
		  these ACs --- is crucial for enhancing ACs' robustness and
		  confidence. To automatically support that task, we propose
		  a novel approach that explores the potential of GPT-4
		  Turbo, an advanced Large Language Model (LLM) developed by
		  OpenAI, in identifying defeaters within ACs formalized
		  using the Eliminative Argumentation (EA) notation. Our
		  preliminary evaluation assesses the model's ability to
		  comprehend and generate arguments in this context and the
		  results show that GPT-4 turbo is very proficient in EA
		  notation and can generate different types of defeaters.},
  booktitle	= {Proceedings of the 2024 IEEE/ACM First International
		  Conference on AI Foundation Models and Software
		  Engineering},
  pages		= {52–56},
  numpages	= {5},
  keywords	= {large language models, assurance cases, assurance
		  defeaters, system certification, FM for Requirement
		  Engineering},
  location	= {Lisbon, Portugal},
  series	= {FORGE '24}
}

@Article{	  10.1145/3736164,
  author	= {Albilali, Eman and Al-Twairesh, Nora and Hosny, Manar},
  title		= {AKER: Arabic Knowledge-enriched Reader for Machine Reading
		  Comprehension},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3736164},
  doi		= {10.1145/3736164},
  abstract	= {Machine reading comprehension aims at understanding a
		  passage and answer a given question by selecting a span
		  from the passage. Recently, pre-trained language models
		  achieved state-of-the-art results on Arabic machine reading
		  comprehension, yet a broad body of works suggests that
		  BERT-based variant models fail to encode and associate
		  common sense facts and world knowledge. To alleviate this
		  weakness, we propose an Arabic knowledge enriched reader
		  model, which fuses external knowledge into contextual
		  representation using an attention and gating mechanism. We
		  learn and generate Arabic knowledge graph embeddings that
		  represent information from Arabic Wikidata and utilize this
		  representation when fusing knowledge. We adopted a
		  knowledge graph embedding scoring function to select the
		  most relevant concepts to the context from the knowledge
		  graph. We evaluated our approach on multiple Arabic machine
		  reading comprehension datasets. Despite leveraging a
		  comparatively smaller pre-trained language model, our
		  approach significantly outperforms large language models in
		  Arabic machine reading comprehension across multiple
		  benchmark datasets, achieving substantial gains in both EM
		  and F1 scores.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {62},
  numpages	= {29},
  keywords	= {Machine reading comprehension, question answering, large
		  language model, knowledge base}
}

@Article{	  10.1109/taslp.2023.3268730,
  author	= {Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang,
		  Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
  title		= {Diffsound: Discrete Diffusion Model for Text-to-Sound
		  Generation},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3268730},
  doi		= {10.1109/TASLP.2023.3268730},
  abstract	= {Generating sound effects that people want is an important
		  topic. However, there are limited studies in this area for
		  sound generation. In this study, we investigate generating
		  sound conditioned on a text prompt and propose a novel
		  text-to-sound generation framework that consists of a text
		  encoder, a Vector Quantized Variational Autoencoder
		  (VQ-VAE), a token-decoder, and a vocoder. The framework
		  first uses the token-decoder to transfer the text features
		  extracted from the text encoder to a mel-spectrogram with
		  the help of VQ-VAE, and then the vocoder is used to
		  transform the generated mel-spectrogram into a waveform. We
		  found that the token-decoder significantly influences the
		  generation performance. Thus, we focus on designing a good
		  token-decoder in this study. We begin with the traditional
		  autoregressive (AR) token-decoder. However, the AR
		  token-decoder always predicts the mel-spectrogram tokens
		  one by one in order, which may introduce the unidirectional
		  bias and accumulation of errors problems. Moreover, with
		  the AR token-decoder, the sound generation time increases
		  linearly with the sound duration. To overcome the
		  shortcomings introduced by AR token-decoders, we propose a
		  non-autoregressive token-decoder based on the discrete
		  diffusion model, named Diffsound. Specifically, the
		  Diffsound model predicts all of the mel-spectrogram tokens
		  in one step and then refines the predicted tokens in the
		  next step, so the best-predicted results can be obtained by
		  iteration. Our experiments show that our proposed Diffsound
		  model not only produces better generation results when
		  compared with the AR token-decoder but also has a faster
		  generation speed, &lt;italic&gt;i.e.&lt;/italic&gt;, MOS:
		  3.56 &lt;italic&gt;v.s&lt;/italic&gt; 2.786.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= apr,
  pages		= {1720–1733},
  numpages	= {14}
}

@InProceedings{	  10.1145/3550355.3552426,
  author	= {Lathouwers, Sophie and Zaytsev, Vadim},
  title		= {Modelling program verification tools for software
		  engineers},
  year		= {2022},
  isbn		= {9781450394666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550355.3552426},
  doi		= {10.1145/3550355.3552426},
  abstract	= {In software engineering, models are used for many
		  different things. In this paper, we focus on program
		  verification, where we use models to reason about the
		  correctness of systems. There are many different types of
		  program verification techniques which provide different
		  correctness guarantees. We investigate the domain of
		  program verification tools, and present a concise megamodel
		  to distinguish these tools. We also present a data set of
		  almost 400 program verification tools. This data set
		  includes the category of verification tool according to our
		  megamodel, practical information such as input/output
		  format, repository links, and more. The categorisation
		  enables software engineers to find suitable tools,
		  investigate similar alternatives and compare them. We also
		  identify trends for each level in our megamodel based on
		  the categorisation. Our data set, publicly available at
		  https://doi.org/10.4121/20347950, can be used by software
		  engineers to enter the world of program verification and
		  find a verification tool based on their requirements.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems},
  pages		= {98–108},
  numpages	= {11},
  keywords	= {program verification, megamodelling, formal methods},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@Article{	  10.1145/3485466,
  author	= {Ronzino, Paola and Toth, Anna and Falcidieno, Bianca},
  title		= {Documenting the Structure and Adaptive Reuse of Roman
		  Amphitheatres through the CIDOC CRMba Model},
  year		= {2022},
  issue_date	= {June 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3485466},
  doi		= {10.1145/3485466},
  abstract	= {This article addresses an important aspect of the built
		  heritage documentation, which concerns encoding information
		  about a building in a formal way, making it available for
		  reuse by the research community. Formal ontologies allow
		  structuring and integrating information from heterogeneous
		  sources without loss of semantic information. In the field
		  of Cultural Heritage (CH), the CIDOC Conceptual Reference
		  Model (CRM) ontology is well known and widely accepted as
		  it provides definitions and a formal structure to describe
		  the implicit and explicit concepts and relationships used
		  in the CH documentation. One of its extensions, the CRMba
		  model, has been specifically designed to document
		  information on a built structure and its components. In
		  this work, we have applied the CRMba model to the
		  documentation of Roman architectures, in particular, Roman
		  amphitheatres, demonstrating how the semantic model allows
		  encoding information about the structure of the building
		  and its evolution over time and space, stressing on the
		  concepts of “empty spaces” and “functional spaces”
		  defined by form, and focusing on the relationship between
		  form and function. The aim of the work is to explore the
		  potentiality of the model and to provide, through a series
		  of examples supported by graphs, standard encoding
		  procedures to be reused by scholars dealing with similar
		  case studies.},
  journal	= {J. Comput. Cult. Herit.},
  month		= apr,
  articleno	= {36},
  numpages	= {23},
  keywords	= {CIDOC CRM, amphitheatre, Buildings archaeology}
}

@InProceedings{	  10.1145/3447568.3448526,
  author	= {Hadj-Mbarek, Asma and Maalel, Ahmed},
  title		= {Towards a Collection Data Approach to Crisis Situation
		  Based on Ontologies},
  year		= {2021},
  isbn		= {9781450376556},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447568.3448526},
  doi		= {10.1145/3447568.3448526},
  abstract	= {Disasters bring unforeseen situations to the public,
		  requiring a quick and immediate response from official
		  organizations. During these periods, access to rapidly
		  changing information plays an important role in
		  decision-making. However, the transmissions of information
		  hinder this task and ultimately delay response operations.
		  Social networks such as Twitter and Facebook create new
		  opportunities to address this type of real-world problems.
		  The involved actors use these channels at the time of
		  crisis to share various information, whether they are
		  requesting assistance, or describing an event. In this
		  article, we will present the first steps of validation of a
		  collect data approach from Twitter. This approach is based
		  on a domain ontology.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Information Systems and Technologies},
  articleno	= {18},
  numpages	= {6},
  keywords	= {Twitter, Ontology, Data collection, Crisis situation},
  location	= {Lecce, Italy},
  series	= {ICIST '20}
}

@Article{	  10.1145/3453172,
  author	= {Gil, Yolanda and Garijo, Daniel and Khider, Deborah and
		  Knoblock, Craig A. and Ratnakar, Varun and Osorio,
		  Maximiliano and Vargas, Hern\'{a}n and Pham, Minh and
		  Pujara, Jay and Shbita, Basel and Vu, Binh and Chiang,
		  Yao-Yi and Feldman, Dan and Lin, Yijun and Song, Hayley and
		  Kumar, Vipin and Khandelwal, Ankush and Steinbach, Michael
		  and Tayal, Kshitij and Xu, Shaoming and Pierce, Suzanne A.
		  and Pearson, Lissa and Hardesty-Lewis, Daniel and Deelman,
		  Ewa and Silva, Rafael Ferreira Da and Mayani, Rajiv and
		  Kemanian, Armen R. and Shi, Yuning and Leonard, Lorne and
		  Peckham, Scott and Stoica, Maria and Cobourn, Kelly and
		  Zhang, Zeya and Duffy, Christopher and Shu, Lele},
  title		= {Artificial Intelligence for Modeling Complex Systems:
		  Taming the Complexity of Expert Models to Improve Decision
		  Making},
  year		= {2021},
  issue_date	= {June 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {11},
  number	= {2},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3453172},
  doi		= {10.1145/3453172},
  abstract	= {Major societal and environmental challenges involve
		  complex systems that have diverse multi-scale interacting
		  processes. Consider, for example, how droughts and water
		  reserves affect crop production and how agriculture and
		  industrial needs affect water quality and availability.
		  Preventive measures, such as delaying planting dates and
		  adopting new agricultural practices in response to changing
		  weather patterns, can reduce the damage caused by natural
		  processes. Understanding how these natural and human
		  processes affect one another allows forecasting the effects
		  of undesirable situations and study interventions to take
		  preventive measures. For many of these processes, there are
		  expert models that incorporate state-of-the-art theories
		  and knowledge to quantify a system's response to a
		  diversity of conditions. A major challenge for efficient
		  modeling is the diversity of modeling approaches across
		  disciplines and the wide variety of data sources available
		  only in formats that require complex conversions. Using
		  expert models for particular problems requires integration
		  of models with third-party data as well as integration of
		  models across disciplines. Modelers face significant
		  heterogeneity that requires resolving semantic,
		  spatiotemporal, and execution mismatches, which are largely
		  done by hand today and may take more than 2 years of
		  effort.We are developing a modeling framework that uses
		  artificial intelligence (AI) techniques to reduce modeling
		  effort while ensuring utility for decision making. Our work
		  to date makes several innovative contributions: (1) an
		  intelligent user interface that guides analysts to frame
		  their modeling problem and assists them by suggesting
		  relevant choices and automating steps along the way; (2)
		  semantic metadata for models, including their modeling
		  variables and constraints, that ensures model relevance and
		  proper use for a given decision-making problem; and (3)
		  semantic representations of datasets in terms of modeling
		  variables that enable automated data selection and data
		  transformations. This framework is implemented in the MINT
		  (Model INTegration) framework, and currently includes data
		  and models to analyze the interactions between natural and
		  human systems involving climate, water availability,
		  agricultural production, and markets. Our work to date
		  demonstrates the utility of AI techniques to accelerate
		  modeling to support decision-making and uncovers several
		  challenging directions for future work.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= jul,
  articleno	= {11},
  numpages	= {49},
  keywords	= {remote sensing data, regional-level decision-making, model
		  metadata, integrated modeling, Intelligent user
		  interfaces}
}

@InProceedings{	  10.1145/3417990.3421410,
  author	= {Jeusfeld, Manfred A. and Almeida, Jo\~{a}o Paulo A. and
		  Carvalho, Victorio A. and Fonseca, Claudenir M. and
		  Neumayr, Bernd},
  title		= {Deductive reconstruction of MLT* for multi-level
		  modeling},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3421410},
  doi		= {10.1145/3417990.3421410},
  abstract	= {In the last two decades, about a dozen proposals were made
		  to extend object-oriented modeling by multiple abstraction
		  levels. One group of proposals designates explicit levels
		  to objects and classes. The second group uses the powertype
		  pattern to implicitly establish levels. From this group, we
		  consider two proposals, DeepTelos and MLT*. Both have been
		  defined via axioms and both give a central role to the
		  powertype pattern. In this paper, we reconstruct MLT* with
		  the deductive axiomatization style used for DeepTelos. The
		  resulting specification is executed in a deductive database
		  to check MLT* multi-level models for errors and complete
		  them with derived facts that do not have to be explicitly
		  asserted by modelers. This leverages the rich rules of MLT*
		  with the deductive approach underlying DeepTelos. The
		  effort also allows us to clearly establish the relation
		  between DeepTelos and MLT*, in an attempt to clarify the
		  relations between approaches in this research domain. As a
		  byproduct, we supply MLT-Telos as a fully operational
		  deductive implementation of MLT* to the research
		  community.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {83},
  numpages	= {10},
  keywords	= {powertype, object-oriented modeling, multi-level modeling,
		  deeptelos, datalog, conceptbase, MLT*},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@Article{	  10.1145/3747290,
  author	= {Dahou, Abdelghani and Dahou, Abdelhalim Hafedh and
		  Cheragui, Mohamed Amine and Abdedaiem, Amin and Al-Qaness,
		  Mohammed A. A. and Abd Elaziz, Mohamed and Ewees, Ahmed A.
		  and Zheng, Zhonglong},
  title		= {A Survey on Dialect Arabic Processing and Analysis: Recent
		  Advances and Future Trends},
  year		= {2025},
  issue_date	= {August 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {8},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3747290},
  doi		= {10.1145/3747290},
  abstract	= {Advances in language models have enabled significant
		  strides in developing language technologies tailored for
		  analyzing and processing Dialectical Arabic (DA), which
		  exhibits unique linguistic features and variations compared
		  to standard Arabic. This progress has sparked a surge of
		  interest in various research tasks within the Arabic
		  Natural Language Processing (ANLP) domain, encompassing
		  areas such as sentiment analysis, dialect identification,
		  normalization and classification, fake news detection, and
		  part-of-speech tagging. The primary objective of this
		  survey paper is to provide a comprehensive overview of the
		  advancements made in dialectical ANLP from 2014 to 2024. A
		  thorough analysis is undertaken, covering a corpus of
		  approximately 200 research papers, to offer insights into
		  the latest developments, resources, and applications
		  concerning dialectical Arabic. By identifying and
		  discussing the challenges and opportunities for future
		  research, this study aspires to serve as a valuable
		  reference for researchers, practitioners, and enthusiasts
		  interested in the subject matter. Central to the
		  investigation are the recent strides in natural language
		  processing techniques that pertain to dialectical Arabic,
		  namely DA sentiment analysis, DA identification, DA
		  classification, DA normalization, DA part-of-speech
		  tagging, and the role of DA in fake news detection, among
		  other applications. Each research category is meticulously
		  examined, providing a comprehensive understanding of their
		  respective contributions, significance, encountered
		  challenges, and the availability of pertinent datasets.
		  This exhaustive survey paper encompasses existing studies
		  within dialectical Arabic research categories. As a result,
		  readers are presented with a detailed reference source in
		  pursuing advancements and innovations within this field.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  articleno	= {84},
  numpages	= {45},
  keywords	= {Deep learning, arabic dialect, sentiment analysis,
		  classification, POS tag, datasets, fake news}
}

@Article{	  10.1145/3569458,
  author	= {Angermeier, Daniel and Wester, Hannah and Beilke, Kristian
		  and Hansch, Gerhard and Eichler, J\"{o}rn},
  title		= {Security Risk Assessments: Modeling and Risk Level
		  Propagation},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {1},
  issn		= {2378-962X},
  url		= {https://doi.org/10.1145/3569458},
  doi		= {10.1145/3569458},
  abstract	= {Security risk assessment is an important task in systems
		  engineering. It is used to derive security requirements for
		  a secure system design and to evaluate design alternatives
		  as well as vulnerabilities. Security risk assessment is
		  also a complex and interdisciplinary task, where experts
		  from the application domain and the security domain have to
		  collaborate and understand each other. Automated and
		  tool-supported approaches are desired to help manage the
		  complexity. However, the models used for system engineering
		  usually focus on functional behavior and lack
		  security-related aspects. Therefore, we present our
		  modeling approach that alleviates communication between the
		  involved experts and features steps of computer-aided
		  modeling to achieve consistency and avoid omission errors.
		  We demonstrate our approach with an example. We also
		  describe how to model impact rating and attack feasibility
		  estimation in a modular fashion, along with the propagation
		  and aggregation of these estimations through the model. As
		  a result, experts can make local decisions or changes in
		  the model, which in turn provides the impact of these
		  decisions or changes on the overall risk profile. Finally,
		  we discuss the advantages of our model-based method.},
  journal	= {ACM Trans. Cyber-Phys. Syst.},
  month		= feb,
  articleno	= {8},
  numpages	= {25},
  keywords	= {threat modeling, secure design, model-based, security
		  engineering, risk analysis, Security risk assessment}
}

@Article{	  10.1145/3610896,
  author	= {Boovaraghavan, Sudershan and Patidar, Prasoon and Agarwal,
		  Yuvraj},
  title		= {TAO: Context Detection from Daily Activity Patterns Using
		  Temporal Analysis and Ontology},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {3},
  url		= {https://doi.org/10.1145/3610896},
  doi		= {10.1145/3610896},
  abstract	= {Translating fine-grained activity detection (e.g., phone
		  ring, talking interspersed with silence and walking) into
		  semantically meaningful and richer contextual information
		  (e.g., on a phone call for 20 minutes while exercising) is
		  essential towards enabling a range of healthcare and
		  human-computer interaction applications. Prior work has
		  proposed building ontologies or temporal analysis of
		  activity patterns with limited success in capturing complex
		  real-world context patterns. We present TAO, a hybrid
		  system that leverages OWL-based ontologies and temporal
		  clustering approaches to detect high-level contexts from
		  human activities. TAO can characterize sequential
		  activities that happen one after the other and activities
		  that are interleaved or occur in parallel to detect a
		  richer set of contexts more accurately than prior work. We
		  evaluate TAO on real-world activity datasets (Casas and
		  Extrasensory) and show that our system achieves, on
		  average, 87\% and 80\% accuracy for context detection,
		  respectively. We deploy and evaluate TAO in a real-world
		  setting with eight participants using our system for three
		  hours each, demonstrating TAO's ability to capture
		  semantically meaningful contexts in the real world.
		  Finally, to showcase the usefulness of contexts, we
		  prototype wellness applications that assess productivity
		  and stress and show that the wellness metrics calculated
		  using contexts provided by TAO are much closer to the
		  ground truth (on average within 1.1\%), as compared to the
		  baseline approach (on average within 30\%).},
  journal	= {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month		= sep,
  articleno	= {87},
  numpages	= {32},
  keywords	= {Behavioral context recognition, activity recognition, deep
		  Learning, ontology}
}

@InProceedings{	  10.1145/3302425.3302495,
  author	= {Liu, Xiao and Gao, Feng},
  title		= {An Approach for Learning Ontology from Relational
		  Database},
  year		= {2018},
  isbn		= {9781450366250},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3302425.3302495},
  doi		= {10.1145/3302425.3302495},
  abstract	= {As a conceptual model and modeling tool, ontology can
		  describe knowledge systems at the semantic level, it can
		  also effectively solve the problem of information
		  integration and sharing. In the research of information
		  integration based on ontology, how to transform relational
		  database data into ontology is an important research
		  direction. Since relational databases are widely used to
		  store data, this paper proposes a new method(WN_Graph) for
		  learning ontology from relational database. Compared with
		  the existing methods, WN_Graph combines intermediate
		  conceptual graph model with WordNet to get more
		  hierarchical relationships of concepts. Therefore, more
		  rich semantic relationships of relational databases are
		  extracted. We use data set from the medical field to verify
		  and analyze the proposed method.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  articleno	= {58},
  numpages	= {6},
  keywords	= {WordNet, Relational Database, Ontology, Graph Model},
  location	= {Sanya, China},
  series	= {ACAI '18}
}

@Article{	  10.1109/taslp.2022.3153255,
  author	= {Gao, Silin and Takanobu, Ryuichi and Bosselut, Antoine and
		  Huang, Minlie},
  title		= {End-to-End Task-Oriented Dialog Modeling With
		  Semi-Structured Knowledge Management},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3153255},
  doi		= {10.1109/TASLP.2022.3153255},
  abstract	= {Current task-oriented dialog (TOD) systems mostly manage
		  structured knowledge (e.g. databases and tables) to guide
		  the goal-oriented conversations. However, they fall short
		  of handling dialogs which also involve unstructured
		  knowledge (e.g. reviews and documents). In this article, we
		  formulate a task of modeling TOD grounded on a fusion of
		  structured and unstructured knowledge. To address this
		  task, we propose a TOD system with semi-structured
		  knowledge management, SeKnow, which extends the belief
		  state to manage knowledge with both structured and
		  unstructured contents. Furthermore, we introduce two
		  implementations of SeKnow based on a non-pretrained
		  sequence-to-sequence model and a pretrained language model,
		  respectively. Both implementations use the end-to-end
		  manner to jointly optimize dialog modeling grounded on
		  structured and unstructured knowledge. We conduct
		  experiments on a modified version of MultiWOZ 2.1 dataset,
		  Mod-MultiWOZ 2.1, where dialogs are processed to involve
		  semi-structured knowledge. Experimental results show that
		  SeKnow has strong performances in both end-to-end dialog
		  and intermediate knowledge management, compared to existing
		  TOD systems and their extensions with pipeline knowledge
		  management schemes.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= feb,
  pages		= {2173–2187},
  numpages	= {15}
}

@InProceedings{	  10.1145/3307339.3342148,
  author	= {Lu, Qiuhao and de Silva, Nisansa and Kafle, Sabin and Cao,
		  Jiazhen and Dou, Dejing and Nguyen, Thien Huu and Sen,
		  Prithviraj and Hailpern, Brent and Reinwald, Berthold and
		  Li, Yunyao},
  title		= {Learning Electronic Health Records through Hyperbolic
		  Embedding of Medical Ontologies},
  year		= {2019},
  isbn		= {9781450366663},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3307339.3342148},
  doi		= {10.1145/3307339.3342148},
  abstract	= {Unplanned intensive care units (ICU) readmissions and
		  in-hospital mortality of patients are two important metrics
		  for evaluating the quality of hospital care. Identifying
		  patients with higher risk of readmission to ICU or of
		  mortality can not only protect those patients from
		  potential dangers, but also reduce the high costs of
		  healthcare. In this work, we propose a new method to
		  incorporate information from the Electronic Health Records
		  (EHRs) of patients and utilize hyperbolic embeddings of a
		  medical ontology (i.e., ICD-9) in the prediction model. The
		  results prove the effectiveness of our method and show that
		  hyperbolic embeddings of ontological concepts give
		  promising performance.},
  booktitle	= {Proceedings of the 10th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  pages		= {338–346},
  numpages	= {9},
  keywords	= {readmission prediction, mortality prediction, medical
		  ontology, graph embedding},
  location	= {Niagara Falls, NY, USA},
  series	= {BCB '19}
}

@InProceedings{	  10.1145/3297280.3297631,
  author	= {Fathalla, Said and Vahdati, Sahar and Auer, S\"{o}ren and
		  Lange, Christoph},
  title		= {The scientific events ontology of the OpenResearch.org
		  curation platform},
  year		= {2019},
  isbn		= {9781450359337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297280.3297631},
  doi		= {10.1145/3297280.3297631},
  abstract	= {Scholarly events, such as conferences play a key role in
		  scholarly communication from many research fields, such as
		  computer science. We describe a systematic redesign of the
		  OpenResearch Scientific Events Ontology (OR-SEO) that is
		  used as a schema for the event pages on OpenResearch.org
		  curation platform. OR-SEO is now in use in thousands of
		  event pages on OpenResearch, which enables users to create
		  events wiki-pages without going into the details of the
		  implementation of the ontology. We syntactically and
		  semantically validated OR-SEO to conform to the W3C
		  standards. It has been published through a persistent URL
		  following W3C best practices for publishing Linked data and
		  has been registered at Linked Open Vocabularies.},
  booktitle	= {Proceedings of the 34th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {2311–2313},
  numpages	= {3},
  keywords	= {knowledge engineering, linked data, scholarly
		  communication, scientific events modeling, semantic
		  MediaWiki},
  location	= {Limassol, Cyprus},
  series	= {SAC '19}
}

@Article{	  10.1145/3471907,
  author	= {Lano, K. and Kolahdouz-Rahimi, S. and Fang, S.},
  title		= {Model Transformation Development Using Automated
		  Requirements Analysis, Metamodel Matching, and
		  Transformation by Example},
  year		= {2021},
  issue_date	= {April 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {31},
  number	= {2},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3471907},
  doi		= {10.1145/3471907},
  abstract	= {In this article, we address how the production of model
		  transformations (MT) can be accelerated by automation of
		  transformation synthesis from requirements, examples, and
		  metamodels. We introduce a synthesis process based on
		  metamodel matching, correspondence patterns between
		  metamodels, and completeness and consistency analysis of
		  matches. We describe how the limitations of metamodel
		  matching can be addressed by combining matching with
		  automated requirements analysis and model transformation by
		  example (MTBE) techniques.We show that in practical
		  examples a large percentage of required transformation
		  functionality can usually be constructed automatically,
		  thus potentially reducing development effort. We also
		  evaluate the efficiency of synthesised transformations.Our
		  novel contributions are: The concept of correspondence
		  patterns between metamodels of a
		  transformation.Requirements analysis of transformations
		  using natural language processing (NLP) and machine
		  learning (ML).Symbolic MTBE using “predictive
		  specification” to infer transformations from
		  examples.Transformation generation in multiple MT languages
		  and in Java, from an abstract intermediate language.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= nov,
  articleno	= {18},
  numpages	= {71},
  keywords	= {requirements engineering, automated software engineering,
		  model-driven engineering, Model transformations}
}

@InProceedings{	  10.1145/3643664.3648211,
  author	= {Frattini, Julian and Fucci, Davide and Torkar, Richard and
		  Mendez, Daniel},
  title		= {A Second Look at the Impact of Passive Voice Requirements
		  on Domain Modeling: Bayesian Reanalysis of an Experiment},
  year		= {2024},
  isbn		= {9798400705670},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643664.3648211},
  doi		= {10.1145/3643664.3648211},
  abstract	= {The quality of requirements specifications may impact
		  subsequent, dependent software engineering (SE) activities.
		  However, empirical evidence of this impact remains scarce
		  and too often superficial as studies abstract from the
		  phenomena under investigation too much. Two of these
		  abstractions are caused by the lack of frameworks for
		  causal inference and frequentist methods which reduce
		  complex data to binary results. In this study, we aim to
		  demonstrate (1) the use of a causal framework and (2)
		  contrast frequentist methods with more sophisticated
		  Bayesian statistics for causal inference. To this end, we
		  reanalyze the only known controlled experiment
		  investigating the impact of passive voice on the subsequent
		  activity of domain modeling. We follow a framework for
		  statistical causal inference and employ Bayesian data
		  analysis methods to re-investigate the hypotheses of the
		  original study. Our results reveal that the effects
		  observed by the original authors turned out to be much less
		  significant than previously assumed. This study supports
		  the recent call to action in SE research to adopt Bayesian
		  data analysis, including causal frameworks and Bayesian
		  statistics, for more sophisticated causal inference.},
  booktitle	= {Proceedings of the 1st IEEE/ACM International Workshop on
		  Methodological Issues with Empirical Studies in Software
		  Engineering},
  pages		= {27–33},
  numpages	= {7},
  keywords	= {requirements engineering, requirements quality, controlled
		  experiment, bayesian data analysis},
  location	= {Lisbon, Portugal},
  series	= {WSESE '24}
}

@Article{	  10.1109/taslp.2022.3224285,
  author	= {Wang, Zhong-Qiu and Wichern, Gordon and Watanabe, Shinji
		  and Le Roux, Jonathan},
  title		= {STFT-Domain Neural Speech Enhancement With Very Low
		  Algorithmic Latency},
  year		= {2022},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3224285},
  doi		= {10.1109/TASLP.2022.3224285},
  abstract	= {Deep learning based speech enhancement in the short-time
		  Fourier transform (STFT) domain typically uses a large
		  window length such as 32 ms. A larger window can lead to
		  higher frequency resolution and potentially better
		  enhancement. This however incurs an algorithmic latency of
		  32 ms in an online setup, because the overlap-add algorithm
		  used in the inverse STFT (iSTFT) is also performed using
		  the same window size. To reduce this inherent latency, we
		  adapt a conventional dual-window-size approach, where a
		  regular input window size is used for STFT but a shorter
		  output window is used for overlap-add, for STFT-domain deep
		  learning based frame-online speech enhancement. Based on
		  this STFT-iSTFT configuration, we employ complex spectral
		  mapping for frame-online enhancement, where a deep neural
		  network (DNN) is trained to predict the real and imaginary
		  (RI) components of target speech from the mixture RI
		  components. In addition, we use the DNN-predicted RI
		  components to conduct frame-online beamforming, the results
		  of which are used as extra features for a second DNN to
		  perform frame-online postfiltering. The frequency-domain
		  beamformer can be easily integrated with our DNNs and is
		  designed to not incur any algorithmic latency.
		  Additionally, we propose a future-frame prediction
		  technique to further reduce the algorithmic latency.
		  Evaluation on noisy-reverberant speech enhancement shows
		  the effectiveness of the proposed algorithms. Compared with
		  Conv-TasNet, our STFT-domain system can achieve better
		  enhancement performance for a comparable amount of
		  computation, or comparable performance with less
		  computation, maintaining strong performance at an
		  algorithmic latency as low as 2 ms.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {397–410},
  numpages	= {14}
}

@InProceedings{	  10.1145/3167132.3167298,
  author	= {Belmonte, Javier and Dugerdil, Philippe},
  title		= {Program understanding using ontologies and dynamic
		  analysis},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167298},
  doi		= {10.1145/3167132.3167298},
  abstract	= {No maintenance activity can be performed without
		  understanding at least the part of the program that needs
		  to be modified. Therefore, considering its cost, helping
		  developers to understand programs is a must. Consequently,
		  our research aims at building a business-related model of
		  the program semantics, which is grounded in Perkinsfi
		  research in psychology. After a short reminder of our
		  model, whose performance in helping developers to
		  understand programs has been presented elsewhere, this
		  paper presents the automatic instantiation of the model.
		  This rests on the ontology technology as well as on an
		  innovative dynamic analysis technique. We present a use
		  case to evaluate the performance of our technique.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {1552–1559},
  numpages	= {8},
  keywords	= {reverse engineering, program understanding, ontology,
		  dynamic analysis},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InProceedings{	  10.1145/3550356.3561539,
  author	= {Acosta, Maribel and Hahner, Sebastian and Koziolek, Anne
		  and K\"{u}hn, Thomas and Mirandola, Raffaela and Reussner,
		  Ralf},
  title		= {Uncertainty in coupled models of cyber-physical systems},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561539},
  doi		= {10.1145/3550356.3561539},
  abstract	= {The development of cyber-physical systems typically
		  involves the association between multiple coupled models
		  that capture different aspects of the system and the
		  environment where it operates. Due to the dynamic aspect of
		  the environment, unexpected conditions and uncertainty may
		  impact the system. In this work, we tackle this problem and
		  propose a taxonomy for characterizing uncertainty in
		  coupled models. Our taxonomy extends existing proposals to
		  cope with the particularities of coupled models in
		  cyber-physical systems. In addition, our taxonomy discusses
		  the notion of uncertainty propagation to other parts of the
		  system. This allows for studying and (in some cases)
		  quantifying the effects of uncertainty on other models in a
		  system even at design time. We show the applicability of
		  our uncertainty taxonomy in real use cases motivated by our
		  envisioned scenario of automotive development.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {569–578},
  numpages	= {10},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3701716.3716841,
  author	= {Kejriwal, Mayank and McGuinness, Deborah L. and Lieberman,
		  Henry},
  title		= {Commonsense AI in the History of the Web},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3716841},
  doi		= {10.1145/3701716.3716841},
  abstract	= {Machine common sense (MCS)-the challenge of enabling
		  computers to grasp everyday human knowledge-has been a
		  grand challenge in Artificial Intelligence (AI) since the
		  1950s. While recent advances in large language models have
		  led to impressive progress, there is still no consensus on
		  how much common sense today's AI actually possesses. In
		  this brief review, we revisit the historical development of
		  MCS in the context of the Web, examining how the Web's
		  evolution-from early knowledge representation efforts to
		  knowledge graphs, the Semantic Web, and crowdsourcing-has
		  shaped MCS research. We argue that key breakthroughs in Web
		  technologies were instrumental in addressing longstanding
		  challenges of scale and coverage in commonsense reasoning.
		  At the same time, MCS research has influenced the
		  development of core Web applications, including intelligent
		  agents, plausibility-based reasoning, and robust evaluation
		  of black-box AI systems.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {837–840},
  numpages	= {4},
  keywords	= {conceptnet, cyc, llms, machine common sense},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3318299.3318365,
  author	= {Benarab, Achref and Rafique, Fahad and Sun, Jianguo},
  title		= {An Ontology Embedding Approach Based on Multiple Neural
		  Networks},
  year		= {2019},
  isbn		= {9781450366007},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318299.3318365},
  doi		= {10.1145/3318299.3318365},
  abstract	= {In this paper, we present a low-dimensional vector
		  representation method for the concepts and instances of an
		  ontology. The main idea is to transform the ontological
		  entities into digestible data for machine learning and deep
		  learning algorithms that only use digital inputs. The
		  generated vectors will represent the semantics contained in
		  the source ontology. We use the semantic relationships
		  connecting the concepts as a landmark to train expert
		  neural networks using the noise contrastive estimation
		  technique to project them into a vector space specific to
		  this relationship with weightings dependent on their
		  frequency. The resulting vectors are then combined and fed
		  into an autoencoder to generate a denser representation.
		  The generated representation vectors can be used to find
		  the semantically similar ontology entities, allowing
		  creating a semantic network automatically. Thus,
		  semantically similar ontology entities will have relatively
		  close corresponding vector representations in the
		  projection space.},
  booktitle	= {Proceedings of the 2019 11th International Conference on
		  Machine Learning and Computing},
  pages		= {186–190},
  numpages	= {5},
  keywords	= {multiple neural networks, feature representation,
		  continuous vector representations, concept embeddings,
		  autoencoders, Ontology embeddings},
  location	= {Zhuhai, China},
  series	= {ICMLC '19}
}

@InProceedings{	  10.1145/3585967.3585994,
  author	= {Jiang, Haoyu},
  title		= {Short-Text Semantic Similarity Model of BERT-Based Siamese
		  Network},
  year		= {2023},
  isbn		= {9781450398466},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3585967.3585994},
  doi		= {10.1145/3585967.3585994},
  abstract	= {People convey their emotions and thoughts through words,
		  the medium of human thoughts. Up against the vigorous
		  development of streaming media, the calculation of text
		  similarity is imperative in the field of natural language
		  processing. Any text-related field is inseparable from text
		  semantic similarity. The calculation of text semantic
		  similarity plays a key role in document management,
		  document classification, and document relevance. Besides,
		  popular natural language processing tasks in some trendy
		  fields, such as artificial intelligence, human-machine
		  translation, problem system, intelligent chat system, and
		  nomenclature recognition, are intertwined with text
		  semantic similarity calculation. In recent years, many
		  excellent researchers have studied the algorithms and
		  models of text semantic similarity from different
		  dimensions. In this paper, a new short-text cosine
		  similarity calculation model of the BERT-based Siamese
		  network is proposed.},
  booktitle	= {Proceedings of the 2023 10th International Conference on
		  Wireless Communication and Sensor Networks},
  pages		= {145–149},
  numpages	= {5},
  keywords	= {Siamese Network, Short-Text Semantic Similarity, Cosine
		  Similarity, BERT},
  location	= {Chengdu, China},
  series	= {icWCSN '23}
}

@InProceedings{	  10.1145/3291280.3291786,
  author	= {Jearanaiwongkul, Watanee and Anutariya, Chutiporn and
		  Andres, Frederic},
  title		= {An Ontology-based Approach to Plant Disease Identification
		  System},
  year		= {2018},
  isbn		= {9781450365680},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3291280.3291786},
  doi		= {10.1145/3291280.3291786},
  abstract	= {Disease identification in plants is an important issue for
		  farmers in terms of plant production and the reduction of
		  losses in crop field. To deal with this issue, a number of
		  systems and techniques for identifying plant diseases have
		  been proposed. However, most of them mainly concentrate on
		  image-based pattern recognition rather than focusing on the
		  observed abnormalities of plant diseases. In other words,
		  they have not employed ontology for semantic detection of
		  plant disease. Current systems do not support farmers to
		  find disease names w.r.t. the semantics of an infected
		  plant. In this work, we proposed an ontology-based approach
		  to modeling plant diseases and demonstrate our approach by
		  developing a rice disease ontology. The ontology helps
		  identifying plant diseases from existing symptoms on
		  plants. To construct the ontology, we reused the domain
		  knowledge related to symptoms of plant diseases from
		  reliable sources. The proposed ontology is modeled in Web
		  Ontology Language (OWL). We also develope a system
		  architecture compatible with the modeled ontology. Finally,
		  we illustrate the usage of our ontology in DL Query for
		  disease retrieval, given that a farmer's observation has
		  already been transformed into an OWL concept. The retrieved
		  results of DL Query make use of subsumption relationship
		  among concepts and properties defined in the ontology.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Advances in Information Technology},
  articleno	= {20},
  numpages	= {8},
  keywords	= {Knowledge representation, Ontology, Plant disease
		  identification},
  location	= {Bangkok, Thailand},
  series	= {IAIT '18}
}

@InProceedings{	  10.1145/3644523.3644561,
  author	= {Wang, Linyao and Zhao, Yan and Mao, Yinxuan and Lu,
		  Zhiang},
  title		= {Evaluation Index System for SysML System Design Model},
  year		= {2024},
  isbn		= {9798400709517},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644523.3644561},
  doi		= {10.1145/3644523.3644561},
  abstract	= {Model-based Systems Engineering (MBSE) is an advanced
		  approach to complex product design and development. The
		  fundamental concept of MBSE is to develop and use
		  consistent models of the system under development, referred
		  to as named system models, as a cross-sectional and
		  cross-discipline platform for information transfer. With
		  the implementation and popularization of MBSE, the industry
		  has put forward higher requirements for the quality of
		  system models. Due to the lack of system model evaluation
		  theory, the quality of the model is opaque, and different
		  stakeholders have different evaluation results for the same
		  model product. This defeats the purpose of using models
		  instead of documents, which is to ensure a consistent
		  exchange of information, and then seriously hinders the use
		  and delivery of the model. Therefore, this study analyzes
		  the characteristics of the system and model and the
		  application of the system model, forming an evaluation
		  index system for SysML system models. It provides
		  practitioners using SysML with systematic theory and
		  practical general methods to solve these problems, and
		  provides a reference for practitioners using different
		  languages to evaluate the quality of MBSE models.},
  booktitle	= {Proceedings of the 2023 4th International Conference on
		  Computer Science and Management Technology},
  pages		= {200–206},
  numpages	= {7},
  location	= {Xi'an, China},
  series	= {ICCSMT '23}
}

@InProceedings{	  10.1145/3318396.3318422,
  author	= {Tapia-Leon, Mariela and Santana-Perez, Idafen and
		  Poveda-Villal\'{o}n, Mar\'{\i}a and Espinoza-Arias, Paola
		  and Chicaiza, Janneth and Corcho, Oscar},
  title		= {Extension of the BiDO Ontology to Represent Scientific
		  Production},
  year		= {2019},
  isbn		= {9781450362672},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318396.3318422},
  doi		= {10.1145/3318396.3318422},
  abstract	= {The SPAR Ontology Network is a suite of complementary
		  ontology modules to describe the scholarly publishing
		  domain. BiDO Standard Bibliometric Measures is part of its
		  set of ontologies. It allows describing of numerical and
		  categorical bibliometric data such as h-index, author
		  citation count, journal impact factor. These measures may
		  be used to evaluate scientific production of researchers.
		  However, they are not enough. In a previous study, we
		  determined the lack of some terms to provide a more
		  complete representation of scientific production. Hence, we
		  have built an extension using the NeOn Methodology to
		  restructure the BiDO ontology. With this extension, it is
		  possible to represent and measure the number of documents
		  from research, the number of citations from a paper and the
		  number of publications in high impact journals according to
		  its area and discipline.},
  booktitle	= {Proceedings of the 2019 8th International Conference on
		  Educational and Information Technology},
  pages		= {166–172},
  numpages	= {7},
  keywords	= {BiDO, Ontology, RDF, SPAR Ontology Network, SPARQL,
		  Scholarly Publishing, Scientific Production},
  location	= {Cambridge, United Kingdom},
  series	= {ICEIT 2019}
}

@InProceedings{	  10.1145/3606305.3606320,
  author	= {Ivanova, Tatyana Ivanova},
  title		= {Collaborative methodology for semantic modeling of
		  learning domain knowledge},
  year		= {2023},
  isbn		= {9798400700477},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3606305.3606320},
  doi		= {10.1145/3606305.3606320},
  abstract	= {Technologies for structured representation of knowledge
		  are very useful for learning and for organization of
		  personalized tutoring. Concept maps (CMs) have been used in
		  many different ways to support learning. They are good tool
		  for regular note-taking, visualization of the structure of
		  learning content, or assisting in discussions and
		  problem-solving activities. On the other hand, ontologies
		  can benefit personalization and adaption of the tutoring
		  process to the needs of specific learners or groups. This
		  research discusses benefits of joint usage of both concept
		  maps and ontologies in e-learning. We propose a
		  collaborative methodology for semantic modeling of learning
		  domain knowledge, showing how to use both CMs and
		  ontologies to improve learning.},
  booktitle	= {Proceedings of the 24th International Conference on
		  Computer Systems and Technologies},
  pages		= {174–179},
  numpages	= {6},
  location	= {Ruse, Bulgaria},
  series	= {CompSysTech '23}
}

@InProceedings{	  10.1145/3284557.3284714,
  author	= {Brecher, Christian and Kusmenko, Evgeny and Lindt, Achim
		  and Rumpe, Bernhard and Storms, Simon and Wein, Stephan and
		  von Wenckstern, Michael and Wortmann, Andreas},
  title		= {Multi-Level Modeling Framework for Machine as a Service
		  Applications Based on Product Process Resource Models},
  year		= {2018},
  isbn		= {9781450366281},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3284557.3284714},
  doi		= {10.1145/3284557.3284714},
  abstract	= {At present, manufacturing processes are highly tailored to
		  a specific product. Changes in product requirements
		  therefore lead to big manual efforts for adapting the
		  manufacturing process and reconfiguring production
		  resources accordingly. Existing approaches do not cope well
		  with this complexity. This hinders agile, customer-oriented
		  manufacturing. A promising approach for automated
		  assembling processes is the Machine as a Service paradigm,
		  which aims for providing production resources on demand.
		  This requires a consistent and pervasive formalization of
		  product specifications, the corresponding manufacturing
		  resources and their interdependencies. Thus, our first
		  contribution is a generic and extensible multi-level and
		  modular modeling framework to formalize products and
		  available resources. Our framework is scalable for large
		  companies and enables reuse for cross-company collaboration
		  and supplier integration. Thereby, the static relationship
		  between product, process and resource is avoided by
		  describing product features and resource skills in separate
		  models. Our framework uses the standardized SysML/UML. Our
		  second contribution is the ability of our framework to
		  integrate different standards. For demonstration, we apply
		  our multi-level approach to a flexible assembly of terminal
		  boxes for transmission gears and show the integration of
		  standards by embedding the eCl@ss classification.},
  booktitle	= {Proceedings of the 2nd International Symposium on Computer
		  Science and Intelligent Control},
  articleno	= {4},
  numpages	= {9},
  keywords	= {Asset Administration Shell, Industry 4.0 Components,
		  Integration of Ontologies, Language Adaption and
		  Aggregation, Machine as a Service, Multi-level Modeling,
		  PPR Models},
  location	= {Stockholm, Sweden},
  series	= {ISCSIC '18}
}

@InProceedings{	  10.1145/3297280.3297508,
  author	= {Pikus, Yevgen and Wei\ss{}enberg, Norbert and Holtkamp,
		  Bernhard and Otto, Boris},
  title		= {Semi-automatic ontology-driven development documentation:
		  generating documents from RDF data and DITA templates},
  year		= {2019},
  isbn		= {9781450359337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297280.3297508},
  doi		= {10.1145/3297280.3297508},
  abstract	= {For a data-driven economy, digitization of product
		  information throughout the entire product lifecycle is key
		  to agility and efficiency of product-related processes.
		  Documenting products and their development, e.g., creating
		  requirement specifications, is an indispensable,
		  time-consuming and resource-intensive activity in large
		  organizations. A vast amount of related information often
		  emerges across several siloing lifecycle tools, and only a
		  portion of it is available in the post-hoc documentation.
		  Additionally, numerous product lines and versions
		  additionally increase the documentation effort. To tackle
		  these issues in a research project, we developed a
		  semi-automatic end-to-end documentation system, able to
		  generate documents based on templates and structured data.
		  As a use case for document generation, we employ the
		  RDF-based lifecycle tool integration standard OSLC and add
		  extended publishing information. In order to generate
		  target documents, we leverage DITA, an established digital
		  publishing standard. A pilot implementation demonstrates
		  that the approach is able to extract distributed lifecycle
		  data and to generate several types of documents in multiple
		  formats. Since the method can also be used to generate
		  documents from arbitrary RDF graphs, the results can be
		  generalized to other domains beyond software development.
		  We believe that the results support the change from a
		  document-driven to a data-driven documentation paradigm in
		  large organizations.},
  booktitle	= {Proceedings of the 34th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {2293–2302},
  numpages	= {10},
  keywords	= {digital publishing, linked data, ontology, product
		  documentation, tool integration},
  location	= {Limassol, Cyprus},
  series	= {SAC '19}
}

@InProceedings{	  10.1145/3341105.3374230,
  author	= {Ojino, Ronald Ochieng},
  title		= {Towards an ontology for personalized hotel room
		  recommendation: student research abstract},
  year		= {2020},
  isbn		= {9781450368667},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341105.3374230},
  doi		= {10.1145/3341105.3374230},
  abstract	= {This paper presents the design of an ontology based on
		  user profile that allows personalizing guests' hotel rooms
		  and services. The ontology being developed using NeON
		  methodology, takes into consideration the maximum number of
		  concepts associated with hotel guest profile and hotel
		  room. The ontology will also provide a sound representation
		  of comfort metrics for hotel rooms to support
		  recommendation.},
  booktitle	= {Proceedings of the 35th Annual ACM Symposium on Applied
		  Computing},
  pages		= {2060–2063},
  numpages	= {4},
  keywords	= {NeON methodology, comfort metrics, ontology, semantics},
  location	= {Brno, Czech Republic},
  series	= {SAC '20}
}

@InProceedings{	  10.1145/3351556.3351581,
  author	= {Stewart, Radovesta and Simeonov, Stanislav and Pavlov,
		  Radoslav},
  title		= {Development of base ontology for a digital library of the
		  Bulgarian museums' collections},
  year		= {2019},
  isbn		= {9781450371933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3351556.3351581},
  doi		= {10.1145/3351556.3351581},
  abstract	= {This paper gives a further look into the process of
		  ontology engineering for the needs of the Bulgarian
		  museums' digital collections. The representation of the
		  data model and a skeleton for a digital library offers a
		  universal solution that can be used for the digitalization
		  of movable cultural heritage ensuring its compatibility
		  with the existing legislation in the domain. The main
		  purpose of the base ontology is to unify and extend the
		  usability of accumulated knowledge stored in the museum
		  collection as well as information retrieval and query
		  processing. The development of its structure has been
		  proceeded following the bottom-up model due to the
		  requirements of the front and backend users forming an
		  important step for the standardization of I.T. solutions in
		  the work of Bulgarian museums.},
  booktitle	= {Proceedings of the 9th Balkan Conference on Informatics},
  articleno	= {5},
  numpages	= {4},
  keywords	= {Digital libraries, Digitalization, Museum database,
		  Ontologies},
  location	= {Sofia, Bulgaria},
  series	= {BCI'19}
}

@InProceedings{	  10.1145/3672608.3707798,
  author	= {Ehl, Marco and Ahmadian, Amir Shayan and Gro\ss{}er,
		  Katharina and Elsofi, Duaa Adel Ali and Herrmann, Marc and
		  Specht, Alexander and Schneider, Kurt and J\"{u}rjens, Jan},
  title		= {Supporting Software Engineers in IT Security and Privacy
		  through Automated Knowledge Discovery},
  year		= {2025},
  isbn		= {9798400706295},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672608.3707798},
  doi		= {10.1145/3672608.3707798},
  abstract	= {Security and privacy are increasingly essential concepts
		  in software engineering. New threats and corresponding
		  countermeasures are continuously discovered. Concurrently,
		  projects are becoming more complex and are exposed to a
		  greater number of threats. This presents a significant
		  challenge for software engineers. As a result, security and
		  privacy are often neglected due to a lack of knowledge,
		  limited time, and financial constraints. While systematic
		  literature reviews exist to address the increasing volume
		  of publications, software engineers still require
		  up-to-date knowledge of current threats and measures. This
		  paper presents an automated, time-efficient, and
		  cost-effective method for discovering knowledge from
		  state-of-the-art literature and project artifacts, such as
		  design documents. The presented method utilizes Large
		  Language Models (LLMs) for data extraction and is
		  demonstrated through a prototypical implementation and
		  evaluation. This evaluation involves security and privacy
		  in open-access scientific publications and project
		  documentation from European Union research and development
		  projects. The extracted knowledge is used to populate a
		  quality model that is specifically designed to provide
		  software engineers with information that helps them apply
		  the findings. This quality model offers software engineers
		  valuable, up-to-date insights into security and privacy,
		  bridging the gap between scientific research and practical
		  applications.},
  booktitle	= {Proceedings of the 40th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1647–1656},
  numpages	= {10},
  keywords	= {security, privacy, quality model, knowledge discovery,
		  large language model},
  location	= {Catania International Airport, Catania, Italy},
  series	= {SAC '25}
}

@InProceedings{	  10.1145/3706598.3714255,
  author	= {Meng, Han and Zhang, Renwen and Wang, Ganyi and Yang,
		  Yitian and Qin, Peinuan and Lee, Jungup and Lee, Yi-Chieh},
  title		= {Deconstructing Depression Stigma: Integrating AI-driven
		  Data Collection and Analysis with Causal Knowledge Graphs},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3714255},
  doi		= {10.1145/3706598.3714255},
  abstract	= {Mental-illness stigma is a persistent social problem,
		  hampering both treatment-seeking and recovery. Accordingly,
		  there is a pressing need to understand it more clearly, but
		  analyzing the relevant data is highly labor-intensive.
		  Therefore, we designed a chatbot to engage participants in
		  conversations; coded those conversations qualitatively with
		  AI assistance; and, based on those coding results, built
		  causal knowledge graphs to decode stigma. The results we
		  obtained from 1,002 participants demonstrate that
		  conversation with our chatbot can elicit rich information
		  about people’s attitudes toward depression, while our
		  AI-assisted coding was strongly consistent with
		  human-expert coding. Our novel approach combining large
		  language models (LLMs) and causal knowledge graphs
		  uncovered patterns in individual responses and illustrated
		  the interrelationships of psychological constructs in the
		  dataset as a whole. The paper also discusses these
		  findings’ implications for HCI researchers in developing
		  digital interventions, decomposing human psychological
		  constructs, and fostering inclusive attitudes.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {641},
  numpages	= {21},
  keywords	= {Social Stigma, Depression, Causal Knowledge Graph,
		  AI-assisted Coding, Chatbot, Large Language Model},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3589335.3641296,
  author	= {Todorov, Konstantin and Fafalios, Pavlos and Dietze,
		  Stefan and Dimitrov, Dimitar},
  title		= {Beyond Facts: 4th International Workshop on Computational
		  Methods for Online Discourse Analysis},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3641296},
  doi		= {10.1145/3589335.3641296},
  abstract	= {Expressing opinions and interacting with others on the Web
		  has led to the production of an abundance of online
		  discourse data, such as claims and viewpoints on
		  controversial topics, their sources and contexts (events,
		  entities). This data constitutes a valuable source of
		  insights for studies into misinformation spread, bias
		  reinforcement, echo chambers or political agenda setting.
		  Computational methods, mostly from the field of NLP, have
		  emerged that tackle a wide range of tasks in this context,
		  including argument and opinion mining, claim detection,
		  checkworthiness detection, stance detection or fact
		  verification. However, computational models require robust
		  definitions of classes and concepts under investigation.
		  Thus, these computational tasks require a strong
		  interdisciplinary and epistemological foundation,
		  specifically with respect to the underlying definitions of
		  key concepts such as claims, arguments, stances,
		  check-worthiness or veracity. This requires a highly
		  interdisciplinary approach combining expertise from fields
		  such as communication studies, computational linguistics
		  and computer science. As opposed to facts, claims are
		  inherently more complex. Their interpretation strongly
		  depends on the context and a variety of intentional or
		  unintended meanings, where terminology and conceptual
		  understandings strongly diverge across communities. From a
		  computational perspective, in order to address this
		  complexity, the synergy of multiple approaches, coming both
		  from symbolic (knowledge representation) and statistical AI
		  seem to be promising to tackle such challenges. This
		  workshop aims at strengthening the relations between these
		  communities, providing a forum for shared works on the
		  modeling, extraction and analysis of discourse on the Web.
		  It will address the need for a shared understanding and
		  structured knowledge about discourse data in order to
		  enable machine-interpretation, discoverability and reuse,
		  in support of scientific or journalistic studies into the
		  analysis of societal debates on the Web. Beyond research
		  into information and knowledge extraction, data
		  consolidation and modeling for knowledge graphs building,
		  the workshop targets communities focusing on the analysis
		  of online discourse, relying on methods from machine
		  learning, natural language processing, large language
		  models and Web data mining.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1418–1421},
  numpages	= {4},
  keywords	= {computational fact-checking, computational journalism,
		  intent detection, knowledge graphs, language models, mis-
		  and dis-information, online discourse analysis, social web
		  mining, stance and viewpoint discovery},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1145/3457608,
  author	= {Vera-Olivera, Harley and Guo, Ruizhe and Huacarpuma, Ruben
		  Cruz and Da Silva, Ana Paula Bernardi and Mariano, Ari Melo
		  and Holanda, Maristela},
  title		= {Data Modeling and NoSQL Databases - A Systematic Mapping
		  Review},
  year		= {2021},
  issue_date	= {July 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3457608},
  doi		= {10.1145/3457608},
  abstract	= {Modeling is one of the most important steps in developing
		  a database. In traditional databases, the Entity
		  Relationship (ER) and Unified Modeling Language (UML)
		  models are widely used. But how are NoSQL databases being
		  modeled? We performed a systematic mapping review to answer
		  three research questions to identify and analyze the levels
		  of representation, models used, and contexts where the
		  modeling process occurred in the main categories of NoSQL
		  databases. We found 54 primary studies where we identified
		  that conceptual and logical levels received more attention
		  than the physical level of representation. The UML, ER, and
		  new notation based on ER and UML were adapted to model
		  NoSQL databases, in the same way, formats such as JSON,
		  XML, and XMI were used to generate schemas through the
		  three levels of representation. New contexts such as
		  benchmark, evaluations, migration, and schema generation
		  were identified, as well as new features to be considered
		  for modeling NoSQL databases, such as the number of records
		  by entities, CRUD operations, and system requirements
		  (availability, consistency, or scalability). Additionally,
		  a coupling and co-citation analysis was carried out to
		  identify relevant works and researchers.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {116},
  numpages	= {26},
  keywords	= {Data modeling, NoSQL databases, systematic mapping}
}

@InProceedings{	  10.1145/3406865.3418595,
  author	= {Polovina, Simon and Polovina, Rubina and Kemp, Neil and
		  Pu, Ken},
  title		= {MOVE: Measuring Ontologies in Value-seeking Environments:
		  CSCW for Human Adaptation},
  year		= {2020},
  isbn		= {9781450380591},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3406865.3418595},
  doi		= {10.1145/3406865.3418595},
  abstract	= {The interest in sharing the
		  Data-Information-Knowledge-Wisdom (DIKW) continuum has been
		  amplified by the latest multi-scale social changes
		  including but not limited to pandemics, economic crises,
		  climate change, and racial issues. This workshop aims to
		  inspire research and discussion on measuring sharing of the
		  DIKW continuum, including through computer-mediated
		  methods, represented by its ontologies. The implied
		  suggestion is that there are ways to improve human
		  adaptation by social technologies that enable rapidly
		  finding solutions for complex global situations. We
		  therefore invite research on (1) ontologies as a medium
		  that enables comparing and measuring the DIKW continuum,
		  (2) ontologies and their convergence or divergence with the
		  values that motivate and determine DIKW sharing, (3)
		  properties and dynamics of ontologies shared via social
		  technologies in their relation to human adaptation.},
  booktitle	= {Companion Publication of the 2020 Conference on Computer
		  Supported Cooperative Work and Social Computing},
  pages		= {475–482},
  numpages	= {8},
  keywords	= {human adaptation, knowledge, knowledge sharing, ontology},
  location	= {Virtual Event, USA},
  series	= {CSCW '20 Companion}
}

@InProceedings{	  10.1145/3338906.3340446,
  author	= {Lohia, Pranay and Kannan, Kalapriya and Srivastava, Biplav
		  and Mehta, Sameep},
  title		= {Design diagrams as ontological source},
  year		= {2019},
  isbn		= {9781450355728},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3338906.3340446},
  doi		= {10.1145/3338906.3340446},
  abstract	= {beginabstract In custom software development projects, it
		  is frequently the case that the same type of software is
		  being built for different customers. The deliverables are
		  similar because they address the same market (e.g.,
		  Telecom, Banking) or have similar functions or both.
		  However, most organisations do not take advantage of this
		  similarity and conduct each project from scratch leading to
		  lesser margins and lower quality. Our key observation is
		  that the similarity among the projects alludes to the
		  existence of a veritable domain of discourse whose
		  ontology, if created, would make the similarity across the
		  projects explicit. Design diagrams are an integral part of
		  any commercial software project deliverables as they
		  document crucial facets of the software solution. We
		  propose an approach to extract ontological information from
		  UML design diagrams (class and sequence diagrams) and
		  represent it as domain ontology in a convenient
		  representation. This ontology not only helps in developing
		  a better understanding of the domain but also fosters
		  software reuse for future software projects in that domain.
		  Initial results on extracting ontology from thousands of
		  model from public repository show that the created
		  ontologies are accurate and help in better software reuse
		  for new solutions. endabstract},
  booktitle	= {Proceedings of the 2019 27th ACM Joint Meeting on European
		  Software Engineering Conference and Symposium on the
		  Foundations of Software Engineering},
  pages		= {863–873},
  numpages	= {11},
  keywords	= {Ontology Extraction, Semantic Representation, Software
		  Re-use},
  location	= {Tallinn, Estonia},
  series	= {ESEC/FSE 2019}
}

@Article{	  10.1145/3588947,
  author	= {Balsebre, Pasquale and Yao, Dezhong and Cong, Gao and
		  Huang, Weiming and Hai, Zhen},
  title		= {Mining Geospatial Relationships from Text},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {1},
  url		= {https://doi.org/10.1145/3588947},
  doi		= {10.1145/3588947},
  abstract	= {A geospatial Knowledge Graph (KG) is a heterogeneous
		  information network, capable of representing relationships
		  between spatial entities in a machine-interpretable format,
		  and has tremendous applications in logistics and social
		  networks. Existing efforts to build a geospatial KG, have
		  mainly used sparse spatial relationships, e.g., a district
		  located inside a city, which provide only marginal benefits
		  compared to a traditional database. In spite of the
		  substantial advances in the tasks of link prediction and
		  knowledge graph completion, identifying geospatial
		  relationships remains challenging, particularly due to the
		  fact that spatial entities are represented with
		  single-point geometries, and textual attributes are
		  frequently missing. In this study, we present GTMiner, a
		  novel framework capable of jointly modeling Geospatial and
		  Textual information to construct a knowledge graph, by
		  mining three useful spatial relationships from a geospatial
		  database, in an end-to-end fashion. The system is divided
		  into three components: (1) a Candidate Selection module, to
		  efficiently select a small number of candidate pairs; (2) a
		  Relation Prediction component to predict spatial
		  relationships between the entities; (3) a KG Refinement
		  procedure, to improve both coverage and correctness of a
		  geospatial knowledge graph. We carry out experiments on
		  four cities' geospatial databases, from publicly-available
		  sources and compare with existing algorithms for link
		  prediction and geospatial data integration. Finally, we
		  conduct an ablation study to motivate our design choices
		  and an efficiency analysis to show that the time required
		  by GTMiner for training and inference is comparable, or
		  even shorter, than existing solutions.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {93},
  numpages	= {26},
  keywords	= {area, geokg, geometry, geospatial, graph, information,
		  interest, kg, knowledge, language, learning, model, of,
		  ontology, poi, point, pois, prediction, relation,
		  relationship, relationships, representation, spatial,
		  text}
}

@InProceedings{	  10.1145/3570991.3571028,
  author	= {Kumar, Suresh and Kumar P, Sreenivasa},
  title		= {Using domain ontology to identify consistent and
		  inconsistent cases from LSTM-generated transfer type AWPs},
  year		= {2023},
  isbn		= {9781450397971},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570991.3571028},
  doi		= {10.1145/3570991.3571028},
  booktitle	= {Proceedings of the 6th Joint International Conference on
		  Data Science \&amp; Management of Data (10th ACM IKDD CODS
		  and 28th COMAD)},
  pages		= {289–290},
  numpages	= {2},
  location	= {Mumbai, India},
  series	= {CODS-COMAD '23}
}

@InProceedings{	  10.1145/3425329.3425332,
  author	= {Chen, Siming and Xiao, Liang and Cheng, Mo},
  title		= {A Semantic-based Multi-agent Dynamic Interaction Model},
  year		= {2020},
  isbn		= {9781450387873},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3425329.3425332},
  doi		= {10.1145/3425329.3425332},
  abstract	= {Due to the autonomy of agents and their ability to
		  perceive the environment, multi-agent systems have been
		  widely used in many fields. The design of multi-agent
		  systems requires the support of interactive models. The
		  traditional multi-agent interaction model has certain
		  feasibility in solving specific tasks. However, in a
		  distributed environment, a relatively static multi-agent
		  interaction model is not sufficient to support a
		  dynamically changing interaction process. Frequent data
		  interactions will also consume resources of multi-agent
		  systems, thereby reducing agent performance. In this study,
		  we propose a semantic-based multiagent dynamic interaction
		  model (MADIM). MADIM uses semantic ontology to map the
		  objects in the interaction model, and defines the
		  interaction protocol through the rule description language.
		  This model is attached with dynamically configurable
		  semantic templates and interaction rule base. We added a
		  reusable dynamic resolution engine component to MADIM to
		  provide dynamic resolution services for the semantic
		  information in the model. MADIM supports dynamic
		  interactive behavior and has good interoperability and
		  interpretability. Our model provides a flexible solution to
		  the multi-agent interaction process. Finally, we verified
		  the feasibility of the model design scheme through a simple
		  example.},
  booktitle	= {Proceedings of the 2nd World Symposium on Software
		  Engineering},
  pages		= {101–108},
  numpages	= {8},
  keywords	= {Grouping, Interaction model, Multi-agent system, Protocol,
		  Semantic rules},
  location	= {Chengdu, China},
  series	= {WSSE '20}
}

@InProceedings{	  10.1145/3491102.3501998,
  author	= {Cambo, Scott Allen and Gergle, Darren},
  title		= {Model Positionality and Computational Reflexivity:
		  Promoting Reflexivity in Data Science},
  year		= {2022},
  isbn		= {9781450391573},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3491102.3501998},
  doi		= {10.1145/3491102.3501998},
  abstract	= {Data science and machine learning provide indispensable
		  techniques for understanding phenomena at scale, but the
		  discretionary choices made when doing this work are often
		  not recognized. Drawing from qualitative research
		  practices, we describe how the concepts of positionality
		  and reflexivity can be adapted to provide a framework for
		  understanding, discussing, and disclosing the discretionary
		  choices and subjectivity inherent to data science work. We
		  first introduce the concepts of model positionality and
		  computational reflexivity that can help data scientists to
		  reflect on and communicate the social and cultural context
		  of a model’s development and use, the data annotators and
		  their annotations, and the data scientists themselves. We
		  then describe the unique challenges of adapting these
		  concepts for data science work and offer annotator
		  fingerprinting and position mining as promising solutions.
		  Finally, we demonstrate these techniques in a case study of
		  the development of classifiers for toxic commenting in
		  online communities.},
  booktitle	= {Proceedings of the 2022 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {572},
  numpages	= {19},
  keywords	= {Computational reflexivity, annotator fingerprinting,
		  critical data studies, data science, human-centered data
		  science, human-centered machine learning, model
		  positionality, position mining},
  location	= {New Orleans, LA, USA},
  series	= {CHI '22}
}

@InProceedings{	  10.1145/3607720.3607797,
  author	= {Iman, el kodssi and Sbai Hanae},
  title		= {A study of extending BPMN for IoT-aware process modeling},
  year		= {2023},
  isbn		= {9798400700194},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3607720.3607797},
  doi		= {10.1145/3607720.3607797},
  abstract	= {A fundamental obstacle to automatic business process
		  detection is the lack of modeling concepts that explicitly
		  express Internet of Things elements as components of a
		  business process model. In order to present a framework for
		  discovering business process models from sensor data, we
		  have studied and compared in our previous publications
		  associated with the modeling of processes in an intelligent
		  environment via the application of process mining. The
		  application of this technique is associated with process
		  modeling, which does not include IoT elements, namely IoT
		  information and devices as output. In this paper, we
		  presented a semantic framework built on our extended BPMN
		  ontology model for IoT.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Networking, Intelligent Systems \&amp; Security},
  articleno	= {68},
  numpages	= {4},
  location	= {Larache, Morocco},
  series	= {NISS '23}
}

@InProceedings{	  10.1145/3658644.3690306,
  author	= {Wen, Rui and Li, Zheng and Backes, Michael and Zhang,
		  Yang},
  title		= {Membership Inference Attacks Against In-Context Learning},
  year		= {2024},
  isbn		= {9798400706363},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3658644.3690306},
  doi		= {10.1145/3658644.3690306},
  abstract	= {Adapting Large Language Models (LLMs) to specific tasks
		  introduces concerns about computational efficiency,
		  prompting an exploration of efficient methods such as
		  In-Context Learning (ICL). However, the vulnerability of
		  ICL to privacy attacks under realistic assumptions remains
		  largely unexplored. In this work, we present the first
		  membership inference attack tailored for ICL, relying
		  solely on generated texts without their associated
		  probabilities. We propose four attack strategies tailored
		  to various constrained scenarios and conduct extensive
		  experiments on four popular large language models.
		  Empirical results show that our attacks can accurately
		  determine membership status in most cases, e.g., 95\%
		  accuracy advantage against LLaMA, indicating that the
		  associated risks are much higher than those shown by
		  existing probability-based attacks. Additionally, we
		  propose a hybrid attack that synthesizes the strengths of
		  the aforementioned strategies, achieving an accuracy
		  advantage of over 95\% in most cases. Furthermore, we
		  investigate three potential defenses targeting data,
		  instruction, and output. Results demonstrate combining
		  defenses from orthogonal dimensions significantly reduces
		  privacy leakage and offers enhanced privacy assurances.},
  booktitle	= {Proceedings of the 2024 on ACM SIGSAC Conference on
		  Computer and Communications Security},
  pages		= {3481–3495},
  numpages	= {15},
  keywords	= {in-context learning, large language models, membership
		  inference attacks},
  location	= {Salt Lake City, UT, USA},
  series	= {CCS '24}
}

@InProceedings{	  10.1145/3706598.3713375,
  author	= {Shen, Hanshu and Shen, Lyukesheng and Wu, Wenqi and Zhang,
		  Kejun},
  title		= {IdeationWeb: Tracking the Evolution of Design Ideas in
		  Human-AI Co-Creation},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713375},
  doi		= {10.1145/3706598.3713375},
  abstract	= {Due to the remarkable content generation capabilities,
		  large language models (LLMs) have demonstrated potential in
		  supporting early-stage conceptual design. However, current
		  interaction paradigms often struggle to effectively
		  facilitate multi-round idea exploration and selection,
		  leading to random outputs, unclear iterations, and
		  cognitive overload. To address these challenges, we propose
		  a human-AI co-ideation framework aimed at tracking the
		  evolution of design ideas. This framework leverages a
		  structured idea representation, an analogy-based reasoning
		  mechanism and interactive visualization techniques. It
		  guides both designers and AI to systematically explore
		  design spaces. We also develop a prototype system,
		  IdeationWeb, which integrates an intuitive, mind map-like
		  visual interface and interactive methods to support
		  co-ideation. Our user study validates the framework’s
		  feasibility, demonstrating enhanced collaboration and
		  creativity between humans and AI. Furthermore, we
		  identified collaborative design patterns from user
		  behaviors, providing valuable insights for future human-AI
		  interaction design.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {146},
  numpages	= {19},
  keywords	= {Human-AI co-ideation, Human-AI interaction, Creativity
		  support, Large language models, Design space},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3178248.3178257,
  author	= {Elstermann, Matthes and Krenn, Florian},
  title		= {The Semantic Exchange Standard for Subject-Oriented
		  Process Models},
  year		= {2018},
  isbn		= {9781450353601},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3178248.3178257},
  doi		= {10.1145/3178248.3178257},
  abstract	= {After the first general concept about using ontologies for
		  the exchange of subject-oriented business process models in
		  2014, a concrete proposal for using the Web Ontology
		  Language (OWL) and the semantic web technology framework as
		  a concrete and direct means was made in 2017 including a
		  proof of concept and proposal for a first standard. Based
		  upon this work a standardization committee has formed and
		  further developed the concept, brought in or reduced
		  definitions, and discussed and agreed on specific
		  controversial topics. In this paper we describe the
		  progress made in 2017 and the current state of the
		  OWL-Standard for the Subject-Oriented Parallel Activity
		  Specification Schema (PASS) modeling language. We present
		  the current state and argue for the made conventions.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Subject-Oriented Business Process Management},
  articleno	= {5},
  numpages	= {8},
  keywords	= {OWL, PASS, ontologies, subject-oriented process models},
  location	= {Linz, Austria},
  series	= {S-BPM One '18}
}

@Article{	  10.1145/3722231,
  author	= {Cimino, Gaetano and Deufemia, Vincenzo},
  title		= {SIGFRID: Unsupervised, Platform-Agnostic Interference
		  Detection in IoT Automation Rules},
  year		= {2025},
  issue_date	= {May 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {2},
  url		= {https://doi.org/10.1145/3722231},
  doi		= {10.1145/3722231},
  abstract	= {Smart home technology has profoundly changed modern living
		  by interconnecting devices, services, dataflows, and user
		  interactions into integrated, automated environments.
		  Homeowners can easily program smart devices using
		  conditional IF-THEN rules, where triggers prompt
		  corresponding actions. However, as smart homes incorporate
		  more multifunctional devices, conflicting trigger-action
		  rules can simultaneously control devices in inconsistent
		  ways, causing unexpected and potentially unsafe
		  interference situations. This article introduces Sigfrid, a
		  novel interference detection approach using scene
		  interaction graphs constructed through Large Language
		  Models (LLMs). To enhance LLM reasoning, we propose a new
		  prompt engineering methodology that integrates automated
		  and manual editing techniques to formulate queries for
		  deriving causal insights in the smart home domain.
		  Interferences are identified through efficient exploration
		  of the graph constructed from the extracted relations. We
		  evaluate Sigfrid on real-world If-This-Then-That (IFTTT)
		  and SmartThings rule sets, demonstrating its superiority
		  over state-of-the-art methods by more than 21\% in
		  F1-score.},
  journal	= {ACM Trans. Internet Things},
  month		= apr,
  articleno	= {13},
  numpages	= {33},
  keywords	= {IoT, trigger-action platforms, interference detection,
		  behavioral modeling, smart home}
}

@InProceedings{	  10.1145/3487553.3524927,
  author	= {Dupuy, Jean and Guille, Adrien and Jacques, Julien},
  title		= {Anchor Prediction: A Topic Modeling Approach},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524927},
  doi		= {10.1145/3487553.3524927},
  abstract	= {Networks of documents connected by hyperlinks, such as
		  Wikipedia, are ubiquitous. Hyperlinks are inserted by the
		  authors to enrich the text and facilitate the navigation
		  through the network. However, authors tend to insert only a
		  fraction of the relevant hyperlinks, mainly because this is
		  a time consuming task. In this paper we address an
		  annotation, which we refer to as anchor prediction. Even
		  though it is conceptually close to link prediction or
		  entity linking, it is a different task that require
		  developing a specific method to solve it. Given a source
		  document and a target document, this task consists in
		  automatically identifying anchors in the source document,
		  i.e words or terms that should carry a hyperlink pointing
		  towards the target document. We propose a contextualized
		  relational topic model, CRTM, that models directed links
		  between documents as a function of the local context of the
		  anchor in the source document and the whole content of the
		  target document. The model can be used to predict anchors
		  in a source document, given the target document, without
		  relying on a dictionary of previously seen mention or
		  title, nor any external knowledge graph. Authors can
		  benefit from CRTM, by letting it automatically suggest
		  hyperlinks, given a new document and the set of target
		  document to connect to. It can also benefit to readers, by
		  dynamically inserting hyperlinks between the documents
		  they’re reading. Experiments conducted on several
		  Wikipedia corpora (in English, Italian and German)
		  highlight the practical usefulness of anchor prediction and
		  demonstrate the relevancy of our approach.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {1310–1318},
  numpages	= {9},
  keywords	= {Anchor prediction, Annotation, Document network, Topic
		  modeling},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Article{	  10.1145/3744554,
  author	= {K\"{u}hn, Ramona and Mitrovi\'{c}, Jelena and Granitzer,
		  Michael},
  title		= {Computational Approaches to the Detection of Lesser-Known
		  Rhetorical Figures: A Systematic Survey and Research
		  Challenges},
  year		= {2025},
  issue_date	= {January 2026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {58},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3744554},
  doi		= {10.1145/3744554},
  abstract	= {Rhetorical figures play a major role in everyday
		  communication, making text and speech more interesting,
		  memorable, or persuasive through their association between
		  form and meaning. Computational detection of rhetorical
		  figures plays an important part in thorough understanding
		  of complex communication patterns. In this survey, we
		  provide a comprehensive overview of computational
		  approaches to lesser-known rhetorical figures. We explore
		  the linguistic and computational perspectives on rhetorical
		  figures and highlight their significance in the field of
		  Natural Language Processing. We present different figures
		  in detail and investigate datasets, definitions, rhetorical
		  functions, and detection approaches. We identify challenges
		  such as dataset scarcity, language limitations, and
		  reliance on rule-based methods.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {42},
  numpages	= {36},
  keywords	= {Rhetorical figures, computational rhetoric, rhetorical
		  figure detection, rhetorical datasets}
}

@InProceedings{	  10.1145/3167132.3167140,
  author	= {Proen\c{c}a, Diogo and Borbinha, Jos\'{e}},
  title		= {Using enterprise architecture model analysis and
		  description logics for maturity assessment},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167140},
  doi		= {10.1145/3167132.3167140},
  abstract	= {A Maturity Model represents a path towards an increasingly
		  organized and systematic way of doing business. It is
		  therefore a widely used technique valuable to assess
		  certain aspects of organizations, as for example business
		  processes. A maturity assessment can enable stakeholders to
		  clearly identify strengths and improvement points, and
		  prioritize actions in order to reach higher maturity
		  levels. Doing maturity assessments can range from simple
		  self-assessment questionnaires to full-blown assessment
		  methods, such as those recommended by the ISO/IEC 15504 or
		  the SEI CMMI. A main caveat of these assessments is the
		  resources they encompass. In addition, many times the lack
		  of automation renders benchmarks not possible. Assuming
		  that the wide spread of Enterprise Architecture practices
		  is making the modeling of business domains a fact, and
		  considering the recent state of the art on the
		  representation of those models as ontologies, this paper
		  proposes how existing semantic technology can be used to
		  automate maturity models assessment methods.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {102–109},
  numpages	= {8},
  keywords	= {OWL, description logics; archimate, enterprise
		  architecture, maturity model, ontology},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InProceedings{	  10.1145/3477314.3508383,
  author	= {Islam, Raisa and Cerny, Tomas and Shin, Dongwan},
  title		= {Ontology-based user privacy management in smart grid},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3508383},
  doi		= {10.1145/3477314.3508383},
  abstract	= {A smart grid system is one of the most complex
		  cyber-physical systems consisting of power generation,
		  distribution, consumption, customer domains, and millions
		  of connected end devices. The widespread implementation of
		  the smart grid raises concerns about the privacy of the
		  data it collects. Since users' personal and non-personal
		  data are going to be accessed by different entities
		  involved in the smart grid and by third parties, the
		  privacy concern can be a big obstacle in the adoption of
		  the smart grid among people. Hence, there is an urgent need
		  to provide the user with a privacy solution to support
		  selective sharing of their usage data with different
		  entities. In this paper, we propose an ontology-based user
		  privacy management approach that will enable the user to
		  release their data based on sensitivity and privacy
		  factors, thus making an informed privacy decision on their
		  usage data sharing. Green Button Initiative is a smart grid
		  application that allows users to download and share their
		  energy usage data with third parties. We present a
		  proof-of-concept implementation extending the Green Button
		  Initiative to test the feasibility of the proposed
		  approach. Lastly, we discuss the results of a user study to
		  investigate the effectiveness of our proposed approach.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {174–182},
  numpages	= {9},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@InProceedings{	  10.1145/3184558.3191533,
  author	= {Obeid, Charbel and Lahoud, Inaya and El Khoury, Hicham and
		  Champin, Pierre-Antoine},
  title		= {Ontology-based Recommender System in Higher Education},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3191533},
  doi		= {10.1145/3184558.3191533},
  abstract	= {Academic advising is limited in its ability to assist
		  students in identifying academic pathways. Selecting a
		  major and a university is a challenging process rife with
		  anxiety. Students at high school are not sure how to match
		  their interests with their working future or major.
		  Therefore, high school students need guidance and support.
		  Moreover, students need to filter, prioritize and
		  efficiently get appropriate information from the web in
		  order to solve the problem of information overload. This
		  paper represents an approach for developing ontology-based
		  recommender system improved with machine learning
		  techniques to orient students in higher education. The
		  proposed recommender system is an assessment tool for
		  students' vocational strengths and weaknesses, interests
		  and capabilities. The main objective of our ontology-based
		  recommender system is to identify the student requirements,
		  interests, preferences and capabilities to recommend the
		  appropriate major and university for each one.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {1031–1034},
  numpages	= {4},
  keywords	= {education, ontology-based, recommender system},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3615887.3627754,
  author	= {Rawsthorne, Helen Mair and Abadie, Nathalie and Kergosien,
		  Eric and Duch\^{e}ne, C\'{e}cile and Saux, \'{E}ric},
  title		= {Automatic Nested Spatial Entity and Spatial Relation
		  Extraction From Text for Knowledge Graph Creation: A
		  Baseline Approach and a Benchmark Dataset},
  year		= {2023},
  isbn		= {9798400703492},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3615887.3627754},
  doi		= {10.1145/3615887.3627754},
  abstract	= {Automatically extracting geographic information from text
		  is the key to harnessing the vast amount of spatial
		  knowledge that only exists in this unstructured form. The
		  fundamental elements of spatial knowledge include spatial
		  entities, their types and the spatial relations between
		  them. Structuring the spatial knowledge contained within
		  text as a geospatial knowledge graph, and disambiguating
		  the spatial entities, significantly facilitates its reuse.
		  The automatic extraction of geographic information from
		  text also allows the creation or enrichment of gazetteers.
		  We propose a baseline approach for nested spatial entity
		  and binary spatial relation extraction from text, a new
		  annotated French-language benchmark dataset on the maritime
		  domain that can be used to train algorithms for both
		  extraction tasks, and benchmark results for the two tasks
		  carried out individually and end-to-end. Our approach
		  involves applying the Princeton University Relation
		  Extraction system (PURE), made for flat, generic entity
		  extraction and generic binary relation extraction, to the
		  extraction of nested, spatial entities and spatial binary
		  relations. By extracting nested spatial entities and the
		  spatial relations between them, we have more information to
		  aid entity disambiguation. In our experiments we compare
		  the performance of a pretrained monolingual French BERT
		  language model with that of a pretrained multilingual BERT
		  language model, and study the effect of including
		  cross-sentence context. Our results reveal very similar
		  results for both models, although the multilingual model
		  performs slightly better in entity extraction, and the
		  monolingual model has slightly better relation extraction
		  and end-to-end performances. We observe that increasing the
		  amount of cross-sentence context improves the results for
		  entity extraction whereas it has the opposite effect on
		  relation extraction.},
  booktitle	= {Proceedings of the 7th ACM SIGSPATIAL International
		  Workshop on Geospatial Humanities},
  pages		= {21–30},
  numpages	= {10},
  keywords	= {binary spatial relation, deep learning, geographic
		  information, language model, maritime data, nested spatial
		  entity, neural network, spatial knowledge},
  location	= {Hamburg, Germany},
  series	= {GeoHumanities '23}
}

@InProceedings{	  10.1145/3239438.3239470,
  author	= {Su, Chuan-Jun and Huang, Shi-Feng and Li, Yi},
  title		= {Case Based Reasoning Driven Ontological Intelligent Health
		  Projection System},
  year		= {2018},
  isbn		= {9781450363891},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3239438.3239470},
  doi		= {10.1145/3239438.3239470},
  abstract	= {For the past few years, health-related issues are getting
		  more and more attention. With regular health screening,
		  disease detection and subsequent medical attention can
		  proceed with far lesser chance of missing the critical
		  period. The projection of health related issues from health
		  screening has been studied by using statistical analysis
		  methods. However, the results generally fail to alert the
		  subjects regarding the potential risks involved due to lack
		  of certainty. The potential health issues derived from
		  statistical analysis of health screening data and medical
		  general guidelines can only be presented to the subjects
		  with probability and lack of supportive hard evidence. The
		  efficacy of health screening consequently fails to be
		  realized. In order to confront the dilemma, we have
		  developed an Intelligent Health Projection System (IHPS)
		  for providing an evidential health status projection and a
		  more motivated health plan to the subjects. This study
		  focuses on modelling Type 2 Diabetes Mellitus (T2DM) and
		  associated factors as an example. Other cases can be
		  analogously implemented. The IHPS adaptively provides the
		  projection of potential risk of getting T2DM by exploring
		  the similarity between the subject's health screening data
		  and previous T2DM patients' cases. The main building
		  blocks, the cases that serve as a knowledge base for IHPS
		  are modelled using ontology technology. As the main
		  functionality of IHPS, the T2DM projection uses the traces
		  left by previous T2DM patients and works on top of
		  case-based reasoning mechanism. The proposed IHPS aims to
		  promote better self-health management by enhancing a
		  subject's comprehension on risks revealed in health
		  screening result. The cases retrieved can not only being
		  used for risk projection of a subject but also serving as
		  evidences for physicians to provide more accurate and
		  convincing health advice.},
  booktitle	= {Proceedings of the 2nd International Conference on Medical
		  and Health Informatics},
  pages		= {185–194},
  numpages	= {10},
  keywords	= {Analytic Hierarchy Process, Case-based Reasoning, Entropy,
		  Health Projection, Ontology, Type 2 Diabetes Mellitus},
  location	= {Tsukuba, Japan},
  series	= {ICMHI '18}
}

@InProceedings{	  10.1145/3358528.3358550,
  author	= {Liang, Yan and Wen, Zepeng and Liu, Li and Li, Gongliang
		  and Guo, Bing},
  title		= {Towards A Goal-driven Dynamic Business Process Ontology},
  year		= {2019},
  isbn		= {9781450371926},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3358528.3358550},
  doi		= {10.1145/3358528.3358550},
  abstract	= {Business process Modelling plays an important role in
		  supporting the whole life-cycle management of business
		  processes. With the frequent changes of the environment and
		  the urgent requirements of business collaboration, sharing,
		  inter-operation, there is an increasing need to dynamically
		  form business process models based on business goals and
		  the underlying criteria. The traditional modelling methods,
		  which focus on the activities with a fixed execution order,
		  are not suitable for the situation. In this paper, a
		  goal-driven dynamic business process ontology (GDBPO) is
		  introduced. The ontology consists of four parts, business
		  process ontology, goal ontology, business rule ontology and
		  decision-making ontology. It can support decomposing the
		  highlevel business goals into the refined operational level
		  activity goals which are aligned with business rules and
		  the available activities to dynamically construct the
		  business process. Moreover, the proposed ontology can
		  explicitly represent the knowledge within processes and be
		  employed as a knowledge base for reasoning and
		  decision-making.},
  booktitle	= {Proceedings of the 2nd International Conference on Big
		  Data Technologies},
  pages		= {295–299},
  numpages	= {5},
  keywords	= {Decision-making, Dynamic business process, Goal-driven,
		  Ontology},
  location	= {Jinan, China},
  series	= {ICBDT '19}
}

@InProceedings{	  10.1145/3529372.3533285,
  author	= {Oelen, Allard and Stocker, Markus and Auer, S\"{o}ren},
  title		= {TinyGenius: intertwining natural language processing with
		  microtask crowdsourcing for scholarly knowledge graph
		  creation},
  year		= {2022},
  isbn		= {9781450393454},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3529372.3533285},
  doi		= {10.1145/3529372.3533285},
  abstract	= {As the number of published scholarly articles grows
		  steadily each year, new methods are needed to organize
		  scholarly knowledge so that it can be more efficiently
		  discovered and used. Natural Language Processing (NLP)
		  techniques are able to autonomously process scholarly
		  articles at scale and to create machine readable
		  representations of the article content. However, autonomous
		  NLP methods are by far not sufficiently accurate to create
		  a high-quality knowledge graph. Yet quality is crucial for
		  the graph to be useful in practice. We present TinyGenius,
		  a methodology to validate NLP-extracted scholarly knowledge
		  statements using microtasks performed with crowdsourcing.
		  The scholarly context in which the crowd workers operate
		  has multiple challenges. The explainability of the employed
		  NLP methods is crucial to provide context in order to
		  support the decision process of crowd workers. We employed
		  TinyGenius to populate a paper-centric knowledge graph,
		  using five distinct NLP methods. In the end, the resulting
		  knowledge graph serves as a digital library for scholarly
		  articles.},
  booktitle	= {Proceedings of the 22nd ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {5},
  numpages	= {5},
  keywords	= {crowdsourcing microtasks, intelligent user interfaces,
		  knowledge graph validation, scholarly knowledge graphs},
  location	= {Cologne, Germany},
  series	= {JCDL '22}
}

@InProceedings{	  10.1145/3485447.3511946,
  author	= {Nguyen, Vinh and Yip, Hong Yung and Bajaj, Goonmeet and
		  Wijesiriwardene, Thilini and Javangula, Vishesh and
		  Parthasarathy, Srinivasan and Sheth, Amit and Bodenreider,
		  Olivier},
  title		= {Context-Enriched Learning Models for Aligning Biomedical
		  Vocabularies at Scale in the UMLS Metathesaurus},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511946},
  doi		= {10.1145/3485447.3511946},
  abstract	= {The Unified Medical Language System (UMLS) Metathesaurus
		  construction process mainly relies on lexical algorithms
		  and manual expert curation for integrating over 200
		  biomedical vocabularies. A lexical-based learning model
		  (LexLM) was developed to predict synonymy among
		  Metathesaurus terms and largely outperforms a rule-based
		  approach (RBA) that approximates the current construction
		  process. However, the LexLM has the potential for being
		  improved further because it only uses lexical information
		  from the source vocabularies, while the RBA also takes
		  advantage of contextual information. We investigate the
		  role of multiple types of contextual information available
		  to the UMLS editors, namely source synonymy (SS), source
		  semantic group (SG), and source hierarchical relations
		  (HR), for the UMLS vocabulary alignment (UVA) problem. In
		  this paper, we develop multiple variants of
		  context-enriched learning models (ConLMs) by adding to the
		  LexLM the types of contextual information listed above. We
		  represent these context types in context-enriched knowledge
		  graphs (ConKGs) with four variants ConSS, ConSG, ConHR, and
		  ConAll. We train these ConKG embeddings using seven KG
		  embedding techniques. We create the ConLMs by concatenating
		  the ConKG embedding vectors with the word embedding vectors
		  from the LexLM. We evaluate the performance of the ConLMs
		  using the UVA generalization test datasets with hundreds of
		  millions of pairs. Our extensive experiments show a
		  significant performance improvement from the ConLMs over
		  the LexLM, namely +5.0\% in precision (93.75\%), +0.69\% in
		  recall (93.23\%), +2.88\% in F1 (93.49\%) for the best
		  ConLM. Our experiments also show that the ConAll variant
		  including the three context types takes more time, but does
		  not always perform better than other variants with a single
		  context type. Finally, our experiments show that the pairs
		  of terms with high lexical similarity benefit most from
		  adding contextual information, namely +6.56\% in precision
		  (94.97\%), +2.13\% in recall (93.23\%), +4.35\% in F1
		  (94.09\%) for the best ConLM. The pairs with lower degrees
		  of lexical similarity also show performance improvement
		  with +0.85\% in F1 (96\%) for low similarity and +1.31\% in
		  F1 (96.34\%) for no similarity. These results demonstrate
		  the importance of using contextual information in the UVA problem.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {1037–1046},
  numpages	= {10},
  keywords	= {UMLS Metathesaurus, knowledge graph embeddings., neural
		  networks, scalability, supervised learning, vocabulary
		  alignment},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Article{	  10.1109/taslp.2024.3376984,
  author	= {Schmid, Florian and Koutini, Khaled and Widmer, Gerhard},
  title		= {Dynamic Convolutional Neural Networks as Efficient
		  Pre-Trained Audio Models},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3376984},
  doi		= {10.1109/TASLP.2024.3376984},
  abstract	= {The introduction of large-scale audio datasets, such as
		  AudioSet, paved the way for Transformers to conquer the
		  audio domain and replace CNNs as the state-of-the-art
		  neural network architecture for many tasks. Audio
		  Spectrogram Transformers are excellent at exploiting large
		  datasets, creating powerful pre-trained models that surpass
		  CNNs when fine-tuned on downstream tasks. However, current
		  popular Audio Spectrogram Transformers are demanding in
		  terms of computational complexity compared to CNNs.
		  Recently, we have shown that, by employing
		  Transformer-to-CNN Knowledge Distillation, efficient CNNs
		  can catch up with and even outperform Transformers on large
		  datasets. In this work, we extend this line of research and
		  increase the capacity of efficient CNNs by introducing
		  dynamic CNN blocks constructed of dynamic convolutions, a
		  dynamic ReLU activation function, and Coordinate Attention.
		  We show that these dynamic CNNs outperform traditional
		  efficient CNNs, such as MobileNets, in terms of the
		  performance–complexity trade-off at the task of audio
		  tagging on the large-scale AudioSet. Our experiments
		  further indicate that the proposed dynamic CNNs achieve
		  competitive performance with Transformer-based models for
		  end-to-end fine-tuning on downstream tasks while being much
		  more computationally efficient.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {2227–2241},
  numpages	= {15}
}

@Proceedings{	  10.1145/3558489,
  title		= {PROMISE 2022: Proceedings of the 18th International
		  Conference on Predictive Models and Data Analytics in
		  Software Engineering},
  year		= {2022},
  isbn		= {9781450398602},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our pleasure to welcome you to the 18th ACM
		  International Conference on Predictive Models and Data
		  Analytics in Software Engineering (PROMISE 2022), to be
		  held in hybrid mode (physically and virtually) on November
		  18th, 2022, co-located with the ACM Joint European Software
		  Engineering Conference and Symposium on the Foundations of
		  Software Engineering (ESEC/FSE 2022). PROMISE is an annual
		  forum for researchers and practitioners to present, discuss
		  and exchange ideas, results, expertise and experiences in
		  the construction and/or application of predictive models
		  and data analytics in software engineering. Such models and
		  analyses could be targeted at planning, design,
		  implementation, testing, maintenance, quality assurance,
		  evaluation, process improvement, management, decision
		  making, and risk assessment in software and systems
		  development. This year PROMISE received a total of 18 paper
		  submissions. The review process was double blind and each
		  paper was reviewed by at least three members of the program
		  committee. An online discussion was also held for 8 days.
		  Based on this procedure, we accepted a total of 10 full
		  papers, which will be presented in 3 technical sessions.
		  The acceptance criteria were entirely based on the quality
		  of the papers, without imposing any constraint on the
		  number of papers to be accepted. We are delighted to
		  announce an outstanding keynote: Release Engineering in the
		  AI World: How can Analytics Help? By Prof. Bram Adams,
		  Queen’s University, Canada We would like to thank all
		  authors for submitting high quality papers, and program
		  committee members for their timely and accurate reviewing
		  activity. Last, but not least, we would like to thank the
		  FSE 2022 organizers for hosting PROMISE 2022 as a
		  co-located event and for their logistic support in the
		  organization of the conference. We hope you will enjoy
		  PROMISE 2022. We certainly will! Many thanks from Shane
		  McIntosh (General Chair), Gema Rodriguez-Perez and Weiyi
		  Shang (Program Chairs).},
  location	= {Singapore, Singapore}
}

@InProceedings{	  10.1145/3241403.3241457,
  author	= {Schr\"{o}der, Sandra and Riebisch, Matthias},
  title		= {An ontology-based approach for documenting and validating
		  architecture rules},
  year		= {2018},
  isbn		= {9781450364836},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3241403.3241457},
  doi		= {10.1145/3241403.3241457},
  abstract	= {Architecture conformance checking is an important activity
		  of architecture enforcement where the architect ensures
		  that all architecture concepts are implemented correctly in
		  the source code. In order to support the architect, a lot
		  of tools for conformance checking are available that allow
		  to formalize the architecture in order to perform an
		  automated verification. Typically, the formalization uses a
		  rigid, tool-specific architecture concept language that may
		  strongly deviate from the project-specific architecture
		  concept language. In addition, a high level of formal
		  expertise is required in order to comprehend the created
		  formalization. We present an approach that uses a
		  controlled natural language for the formalization of
		  architecture concepts. This language allows to flexibly
		  express architecture rules directly with project-specific
		  concepts. Consequently, the resulting formalization is easy
		  to understand and might also be used as an architecture
		  documentation at the same time. Nevertheless, the
		  documentation can be automatically verified, since the
		  approach is based on powerful means of the semantic web,
		  i.e., ontologies and description logics. For the evaluation
		  of the approach, we use the real-world software system
		  TEAMMATES and show that architecture rules and concepts can
		  be flexibly designed and checked for conformance in order
		  to detect crucial architecture violations.},
  booktitle	= {Proceedings of the 12th European Conference on Software
		  Architecture: Companion Proceedings},
  articleno	= {52},
  numpages	= {7},
  keywords	= {architecture conformance checking, architecture
		  documentation, architecture erosion, description logics,
		  ontologies},
  location	= {Madrid, Spain},
  series	= {ECSA '18}
}

@InProceedings{	  10.1145/3726302.3730164,
  author	= {Li, Wing Yan and Wang, Zeqiang and Johnson, Jon and De,
		  Suparna},
  title		= {Are Information Retrieval Approaches Good at Harmonising
		  Longitudinal Surveys in Social Science?},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3730164},
  doi		= {10.1145/3726302.3730164},
  abstract	= {Automated detection of semantically equivalent questions
		  in longitudinal social science surveys is crucial for
		  long-term studies informing empirical research in the
		  social, economic, and health sciences. Retrieving
		  equivalent questions faces dual challenges: inconsistent
		  representation of theoretical constructs (i.e.
		  concept/sub-concept) across studies as well as between
		  question and response options, and the evolution of
		  vocabulary and structure in longitudinal text. To address
		  these challenges, our multi-disciplinary collaboration of
		  computer scientists and survey specialists presents a new
		  information retrieval (IR) task of identifying concept
		  (e.g. Housing, Job, etc.) equivalence across question and
		  response options to harmonise longitudinal population
		  studies. This paper investigates multiple unsupervised
		  approaches on a survey dataset spanning 1946-2020,
		  including probabilistic models, linear probing of language
		  models, and pre-trained neural networks specialised for IR.
		  We show that IR-specialised neural models achieve the
		  highest overall performance with other approaches
		  performing comparably. Additionally, the re-ranking of the
		  probabilistic model's results with neural models only
		  introduces modest improvements of 0.07 at most in F1-score.
		  Qualitative post-hoc evaluation by survey specialists shows
		  that models generally have a low sensitivity to questions
		  with high lexical overlap, particularly in cases where
		  sub-concepts are mismatched. Altogether, our analysis
		  serves to further research on harmonising longitudinal
		  studies in social science.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2946–2950},
  numpages	= {5},
  keywords	= {conceptual comparison, information retrieval, longitudinal
		  study, natural language processing},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@Article{	  10.1145/3588316,
  author	= {Lin, Jerry Chun-Wei and D\'{I}az, Vicente Garc\'{I}a and
		  Molinera, Juan Antonio Morente},
  title		= {Introduction to the Special Issue of Recent Advances in
		  Computational Linguistics for Asian Languages},
  year		= {2023},
  issue_date	= {March 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3588316},
  doi		= {10.1145/3588316},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {62},
  numpages	= {5}
}

@Article{	  10.1162/coli_a_00385,
  author	= {\v{Z}abokrtsk\'{y}, Zden\v{e}k and Zeman, Daniel and
		  \v{S}ev\v{c}\'{\i}kov\'{a}, Magda},
  title		= {Sentence Meaning Representations Across Languages: What
		  Can We Learn from Existing Frameworks?},
  year		= {2020},
  issue_date	= {September 2020},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA},
  volume	= {46},
  number	= {3},
  issn		= {0891-2017},
  url		= {https://doi.org/10.1162/coli_a_00385},
  doi		= {10.1162/coli_a_00385},
  abstract	= {This article gives an overview of how sentence meaning is
		  represented in eleven deep-syntactic frameworks, ranging
		  from those based on linguistic theories elaborated for
		  decades to rather lightweight NLP-motivated approaches. We
		  outline the most important characteristics of each
		  framework and then discuss how particular language
		  phenomena are treated across those frameworks, while trying
		  to shed light on commonalities as well as differences.},
  journal	= {Comput. Linguist.},
  month		= nov,
  pages		= {605–665},
  numpages	= {61}
}

@InProceedings{	  10.1145/3459637.3482097,
  author	= {Zhang, Zhiling and Zhou, Zelin and Tang, Haifeng and Li,
		  Guangwei and Wu, Mengyue and Zhu, Kenny Q.},
  title		= {Enriching Ontology with Temporal Commonsense for
		  Low-Resource Audio Tagging},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482097},
  doi		= {10.1145/3459637.3482097},
  abstract	= {Audio tagging aims at predicting sound events occurred in
		  a recording. Traditional models require enormous laborious
		  annotations, otherwise performance degeneration will be the
		  norm. Therefore, we investigate robust audio tagging models
		  in low-resource scenarios with the enhancement of knowledge
		  graphs. Besides existing ontological knowledge, we further
		  propose a semi-automatic approach that can construct
		  temporal knowledge graphs on diverse domain-specific label
		  sets. Moreover, we leverage a variant of relation-aware
		  graph neural network, D-GCN, to combine the strength of the
		  two knowledge types. Experiments on AudioSet and SONYC
		  urban sound tagging datasets suggest the effectiveness of
		  the introduced temporal knowledge, and the advantage of the
		  combined KGs with D-GCN over single knowledge source.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {3652–3656},
  numpages	= {5},
  keywords	= {audio tagging, graph neural network, knowledge graph,
		  low-resource},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3453892.3461349,
  author	= {Bifis, Aristeidis and Trigka, Maria and Dedegkika, Sofia
		  and Goula, Panagiota and Constantinopoulos, Constantinos
		  and Kosmopoulos, Dimitrios},
  title		= {A Hierarchical Ontology for Dialogue Acts in Psychiatric
		  Interviews},
  year		= {2021},
  isbn		= {9781450387927},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3453892.3461349},
  doi		= {10.1145/3453892.3461349},
  abstract	= {We present our work on modeling the context of an
		  interview during diagnostic sessions for patients with
		  mental health problems. The results are to be exploited by
		  translation system for telehealth services. More
		  specifically, we plan to use the context of the psychiatric
		  interview in order to set informative priors over the
		  vocabulary of the speaker. Therefore we have modelled the
		  context with a hierarchical ontology, and we use it to
		  classify the current state of the interview. The state is
		  extracted after the doctor asks a question, and allow us to
		  select a non-uniform prior regarding the vocabulary of the
		  patient.},
  booktitle	= {Proceedings of the 14th PErvasive Technologies Related to
		  Assistive Environments Conference},
  pages		= {330–337},
  numpages	= {8},
  keywords	= {depression, dialogue context, hierarchical classification,
		  stress},
  location	= {Corfu, Greece},
  series	= {PETRA '21}
}

@InProceedings{	  10.1145/3341620.3341640,
  author	= {Wang, Wei and Mu, Wenxin and Gou, Juanqiong},
  title		= {Spatial-temporal Data Association Based Ontology Alignment
		  Research in High Education Context},
  year		= {2019},
  isbn		= {9781450360913},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341620.3341640},
  doi		= {10.1145/3341620.3341640},
  abstract	= {In the process of practicing smart campus, student
		  behavior is highly concerned. Student activities produce
		  spatial-temporal data, which records the daily life of
		  students and contains the potential regulations of student
		  behavior. The complexity of these data brings challenges
		  for data collection and data analysis. The key to solve
		  these problems is data fusion. In addition, ontology
		  alignment is an important method for exploring the
		  association between different ontology in different fields.
		  It can solve the problem of data fusion in practice and
		  maximize the value of data. At present, the research
		  methods of ontology alignment are mostly mathematical
		  similarity algorithms, and not considering the uncertainty
		  of spatial-temporal data. Ontology is better way to deal
		  with spatial-temporal data. In order to solve this problem
		  effectively, this paper proposes a method based on
		  spatial-temporal data association, which establishes fuzzy
		  ontology and formulates a series of fuzzy rules for fuzzy
		  reasoning, and mines the relationship between data; then it
		  connects the different concepts between ontologies to
		  realize the alignment of ontologies. Finally, the fuzzy
		  ontology modeling and fuzzy reasoning are implemented and
		  tested by using high education context. The rationality and
		  effectiveness of the method are verified.},
  booktitle	= {Proceedings of the 2019 International Conference on Big
		  Data Engineering},
  pages		= {125–130},
  numpages	= {6},
  keywords	= {Data association, fuzzy ontology, fuzzy reasoning,
		  ontology alignment},
  location	= {Hong Kong, Hong Kong},
  series	= {BDE '19}
}

@Article{	  10.1145/3579615,
  author	= {Ferrier-Barbut, El\'{e}onore and Avellino, Ignacio and
		  Canlorbe, Geoffroy and Vitrani, Marie-Aude and Luengo,
		  Vanda},
  title		= {Learning With Pedagogical Models: Videos As Adjuncts to
		  Apprenticeship for Surgical Training},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {CSCW1},
  url		= {https://doi.org/10.1145/3579615},
  doi		= {10.1145/3579615},
  abstract	= {Videos are a powerful media to learn activities through
		  guided physical training such as surgery, especially when
		  they are produced following human learning models and not
		  as "how-to" videos. However, their success greatly depends
		  on how they are integrated into the extensive curricula of
		  domains where learning occurs through guided practice. In
		  this work, we investigate the impact of integrating video
		  as a learning tool into the learning curricula of surgery.
		  We created a pedagogical video on surgical hysterectomy
		  through a model based on the Conceptual Fields theory
		  (Vergnaud) and performed two rounds of interviews with
		  seven medical residents, who watched the video freely
		  during their residency in gynecology-obstetrics as they
		  trained with experts. We find that videos can complement
		  guided physical training, as they can provide the rationale
		  behind expert action, something that is difficult to
		  explicit during guided training. Still, their linear and
		  static nature limits their integration as true adjuncts. We
		  discuss our vision of moving towards interactive videos
		  created with an ontological approach, developed in a
		  workshop with four expert surgeons, which involves the
		  ability to navigate through levels of information and
		  layers of representations, so that experts can represent
		  information to learners according to pedagogical models
		  that complement their complex and extensive learning
		  curricula.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= apr,
  articleno	= {139},
  numpages	= {40},
  keywords	= {conceptual fields theory, guided practice, surgery,
		  video-based learning}
}

@InProceedings{	  10.1145/3696410.3714694,
  author	= {Ahmetaj, Shqiponja and Boneva, Iovka and Hidders, Jan and
		  Hose, Katja and Jakubowski, Maxime and Labra Gayo, Jose
		  Emilio and Martens, Wim and Mogavero, Fabio and Murlak,
		  Filip and Okulmus, Cem and Polleres, Axel and Savkovi\'{c},
		  Ognjen and \v{S}imkus, Mantas and Tomaszuk, Dominik},
  title		= {Common Foundations for SHACL, ShEx, and PG-Schema},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696410.3714694},
  doi		= {10.1145/3696410.3714694},
  abstract	= {Graphs have emerged as a foundation for a variety of
		  applications, including capturing factual knowledge,
		  semantic data integration, social networks, and informing
		  machine learning algorithms. Formalising properties of the
		  data and ensuring data quality requires describing schemas
		  of such graphs. Driven by diverse applications, the
		  Semantic Web and database communities developed not only
		  different graph data models-RDF and property graphs-but
		  also different graph schema languages-SHACL, ShEx, and
		  PG-Schema. Each language has its unique approach to
		  defining constraints and validating graph data, leaving
		  potential users in the dark about their commonalities and
		  differences. In this paper, we provide concise formal
		  definitions of the core components of these languages,
		  employ a uniform framework to facilitate a comprehensive
		  comparison between them, and identify a common set of
		  functionalities, shedding light on both overlapping and
		  distinctive features.},
  booktitle	= {Proceedings of the ACM on Web Conference 2025},
  pages		= {8–21},
  numpages	= {14},
  keywords	= {PG-schema, RDF, SHACL, ShEx, common data model, graph
		  databases, graph schema languages, property graphs},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3290511.3290546,
  author	= {Chen, Jizhi and Gu, Junzhong},
  title		= {Developing educational ontology: a case study in physics},
  year		= {2018},
  isbn		= {9781450365178},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3290511.3290546},
  doi		= {10.1145/3290511.3290546},
  abstract	= {Nowadays, e-learning systems are widely developed. With
		  e-learning, students can learn different subjects remotely,
		  and teachers can edit online teaching scripts and issue
		  online courseware. But complain is ceaseless, that is
		  either students or teachers are not satisfied with the
		  existing state of affairs. The reason is that they expect
		  that students can master knowledge architecture of required
		  subjects, but current scattered courseware lacks
		  systematicness. How to describe knowledge architectures of
		  subjects, e.g. in K12, and help students to master them?
		  Ontology can be used to efficiently present knowledge
		  architecture of different subjects, such as Physics. A big
		  challenge is how to construct educational ontology to
		  describe systematic knowledge of subjects automatically. In
		  this paper educational ontology is as a new topic
		  discussed. An approach to automatically constructing
		  educational ontology is proposed to convert textbook into a
		  corresponding Ontology, with High School Physics as an
		  example.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Education Technology and Computers},
  pages		= {201–206},
  numpages	= {6},
  keywords	= {e-learning, educational ontology, ontology, ontology
		  development},
  location	= {Tokyo, Japan},
  series	= {ICETC '18}
}

@InProceedings{	  10.1145/3377170.3377201,
  author	= {Ratsamano, Salintip and Chairungsee, Supaporn},
  title		= {Knowledge-Based Ontology Development for Folk Medicine},
  year		= {2020},
  isbn		= {9781450376631},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3377170.3377201},
  doi		= {10.1145/3377170.3377201},
  abstract	= {This research sought to develop a semantic ontology of
		  knowledge about the introduction of folk medicine. In this
		  research, the content of folk medicine is presented
		  comprehensively and the research goals or procedures are
		  defined as follows: (1) determine the purpose and scope of
		  the ontology; (2) develop the ontology by using the Hozo
		  Ontology Editor program (Osaka University, Osaka, Japan);
		  and (3) have an expert evaluate the ontology that is
		  developed. The purpose of this research is to clarify the
		  system-recommended treatment involving folk medicine that
		  can guide patients well. Ideally, the treatment selection
		  process will be accurate 100\% of the time and can be used
		  in related research.},
  booktitle	= {Proceedings of the 2019 7th International Conference on
		  Information Technology: IoT and Smart City},
  pages		= {70–74},
  numpages	= {5},
  keywords	= {Ontology, Thai medicine, folk medicine, semantic web},
  location	= {Shanghai, China},
  series	= {ICIT '19}
}

@InProceedings{	  10.1145/3580305.3599246,
  author	= {Zhang, Jiarui and Ilievski, Filip and Ma, Kaixin and
		  Kollaa, Aravinda and Francis, Jonathan and Oltramari,
		  Alessandro},
  title		= {A Study of Situational Reasoning for Traffic
		  Understanding},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599246},
  doi		= {10.1145/3580305.3599246},
  abstract	= {Intelligent Traffic Monitoring (ITMo) technologies hold
		  the potential for improving road safety/security and for
		  enabling smart city infrastructure. Understanding traffic
		  situations requires a complex fusion of perceptual
		  information with domain-specific and causal commonsense
		  knowledge. Whereas prior work has provided benchmarks and
		  methods for traffic monitoring, it remains unclear whether
		  models can effectively align these information sources and
		  reason in novel scenarios. To address this assessment gap,
		  we devise three novel text-based tasks for situational
		  reasoning in the traffic domain: i) BDD-QA, which evaluates
		  the ability of Language Models (LMs) to perform situational
		  decision-making, ii) TV-QA, which assesses LMs' abilities
		  to reason about complex event causality, and iii) HDT-QA,
		  which evaluates the ability of models to solve human
		  driving exams. We adopt four knowledge-enhanced methods
		  that have shown generalization capability across language
		  reasoning tasks in prior work, based on natural language
		  inference, commonsense knowledge-graph self-supervision,
		  multi-QA joint training, and dense retrieval of domain
		  information. We associate each method with a relevant
		  knowledge source, including knowledge graphs, relevant
		  benchmarks, and driving manuals. In extensive experiments,
		  we benchmark various knowledge-aware methods against the
		  three datasets, under zero-shot evaluation; we provide
		  in-depth analyses of model performance on data partitions
		  and examine model predictions categorically, to yield
		  useful insights on traffic understanding, given different
		  background knowledge and reasoning strategies.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3262–3272},
  numpages	= {11},
  keywords	= {language models, question answering, traffic
		  understanding, zero-shot evaluation},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3355402.3355406,
  author	= {Fong, A. C.M. and Hong, Guanyue},
  title		= {Ontology-Powered Hybrid Extensional-Intensional Learning},
  year		= {2019},
  isbn		= {9781450372282},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3355402.3355406},
  doi		= {10.1145/3355402.3355406},
  abstract	= {Deep learning has made headlines in the past few years due
		  to successes in tasks, such as self-driving vehicles and
		  board games, which were previously thought difficult or
		  impossible. The successes have generated much interest in
		  artificial intelligence among researchers and members of
		  the public. However, deep learning algorithms generally
		  require very large labelled data sets to work well and
		  large labelled data sets are not always readily available.
		  In addition, most machine learning techniques, including
		  deep learning, often perform well statistically but can
		  fail miserably when, for example, data are deliberately
		  perturbed in an adversarial attack. Another criticism of
		  deep learning techniques is a relative lack of
		  explainability. This paper proposes the use of intentional
		  learning to simultaneously address these issues.
		  Preliminary evaluation on the MNIST data set has shown
		  promising results. Specifically, by combing extensional and
		  intensional learning, it is possible to achieve similar
		  accuracy result as extensional learning only using only
		  one-sixth of the original training data set.},
  booktitle	= {Proceedings of the 2019 International Conference on
		  Information Technology and Computer Communications},
  pages		= {18–23},
  numpages	= {6},
  keywords	= {Machine learning paradigms, intension and extension of
		  concepts, neural networks, ontologies},
  location	= {Singapore, Singapore},
  series	= {ITCC '19}
}

@InProceedings{	  10.1145/3571884.3603756,
  author	= {Fischer, Joel E},
  title		= {Generative AI Considered Harmful},
  year		= {2023},
  isbn		= {9798400700149},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571884.3603756},
  doi		= {10.1145/3571884.3603756},
  abstract	= {The recent months have seen an explosion of interest,
		  hype, and concern about generative AI, driven by the
		  release of ChatGPT. In this article I seek to explicate
		  some potential and actual harms of the engineering and use
		  of generative AI such as ChatGPT. With this I also suggest
		  a reframing for researchers with an interest in
		  interaction. With this reframing I seek to provoke
		  researchers to consider studying the settings of ChatGPT
		  development and use as active sites of production. Research
		  should focus on the organisational, technological and
		  interactional practices and contexts in and through which
		  generative AI and its outputs—harmful and otherwise—are
		  produced, by whom, to what end, and with what consequences
		  on societies.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Conversational User Interfaces},
  articleno	= {7},
  numpages	= {5},
  keywords	= {ChatGPT, GPT-3, GPT-4, LLM, Large Language Models, NLG,
		  NLP, generative AI, natural language, text generation},
  location	= {Eindhoven, Netherlands},
  series	= {CUI '23}
}

@InProceedings{	  10.1145/3365438.3410951,
  author	= {Song, Hui and Dautov, Rustem and Ferry, Nicolas and
		  Solberg, Arnor and Fleurey, Franck},
  title		= {Model-based fleet deployment of edge computing
		  applications},
  year		= {2020},
  isbn		= {9781450370196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3365438.3410951},
  doi		= {10.1145/3365438.3410951},
  abstract	= {Edge computing brings software in close proximity to end
		  users and IoT devices. Given the increasing number of
		  distributed Edge devices with various contexts, as well as
		  the widely adopted continuous delivery practices, software
		  developers need to maintain multiple application versions
		  and frequently (re-)deploy them to a fleet of many devices
		  with respect to their contexts. Doing this correctly and
		  efficiently goes beyond manual capabilities and requires
		  employing an intelligent and reliable automated approach.
		  Accordingly this paper describes a joint research with a
		  Smart Healthcare application provider on a model-based
		  approach to automatically assigning multiple software
		  deployments to hundreds of Edge gateways. From a
		  Platform-Specific Model obtained from the existing Edge
		  computing platform, we extract a Platform-Independent Model
		  that describes a list of target devices and a pool of
		  available deployments. Next, we use constraint solving to
		  automatically assign deployments to devices at once, given
		  their specific contexts. The resulting solution is
		  transformed back to the PSM as to proceed with software
		  deployment accordingly. We validate the approach with a
		  Fleet Deployment prototype integrated into the DevOps
		  toolchain currently used by the application provider.
		  Initial experiments demonstrate the viability of the
		  approach and its usefulness in supporting DevOps in Edge
		  computing applications.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {132–142},
  numpages	= {11},
  keywords	= {DevOps, IoT, device fleet, model-based software
		  engineering, software deployment},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@InProceedings{	  10.1145/3397125.3397151,
  author	= {Voit, Nikolay and Kirillov, Sergey and Bochkov, Semen},
  title		= {Converting Diagram to a Timeline Ontology},
  year		= {2020},
  isbn		= {9781450377492},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397125.3397151},
  doi		= {10.1145/3397125.3397151},
  abstract	= {CAD systems design and development is not a simple
		  process. It consists of large amount of works, most of them
		  are interconnected and should be performed either
		  simultaneously or sequentially, some of them depend on
		  success of previous works etc. Design workflows allow to
		  describe works in visual form and calculate their
		  quantitative and qualitative parameters. They significantly
		  increase the design process efficiency and the product
		  quality due to the usage of participants interaction
		  language unification. However, modern workflow management
		  tools lack of some important functions especially in part
		  of temporal analysis and ontology-based timeline
		  diagrams.In this paper, we describe the novel method to
		  convert any diagram to a timeline structure like an
		  ontology. The method includes converting algorithm and
		  allows engineers to get the issue of workflows in which
		  they are involved thus giving them a help to design complex
		  CAD systems. It is shown that any diagram describing
		  complex system behavior may be converted into simple view
		  as a timeline ontology. An illustrated example is given in
		  the article.},
  booktitle	= {Proceedings of the 2020 6th International Conference on
		  Computer and Technology Applications},
  pages		= {80–86},
  numpages	= {7},
  keywords	= {analysis, business process, computer-aided design,
		  workflows},
  location	= {Antalya, Turkey},
  series	= {ICCTA '20}
}

@InProceedings{	  10.1145/3412841.3441939,
  author	= {Combi, Carlo and Galetto, Francesca and Nakawala,
		  Hirenkumar Chandrakant and Pozzi, Giuseppe and Zerbato,
		  Francesca},
  title		= {Enriching surgical process models by BPMN extensions for
		  temporal durations},
  year		= {2021},
  isbn		= {9781450381048},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3412841.3441939},
  doi		= {10.1145/3412841.3441939},
  abstract	= {Many surgical interventions are finding new techniques in
		  robot-assisted surgery, which allows surgeons to perform
		  surgery with the help of robotic arms. A formal
		  representation of robot-assisted surgery can provide
		  surgeons with an overview of the main stages of surgical
		  intervention and a detailed description of the different
		  steps, including all the possible emergencies that may
		  occur. Formalizing such kinds of interventions could also
		  help to train new surgeons. However, literature does not
		  consider formal representations and properties of
		  robot-assisted surgery properly.The Business Process Model
		  and Notation (BPMN) is a standard language allowing to
		  represent processes in a graphical and semi-formal way. In
		  this paper, we propose to use BPMN for representing the
		  processes and the guidelines underlying robot-assisted
		  surgery, considering the explicit modeling of temporal and
		  informational aspects: in detail, guidelines aim at
		  providing surgeons with high-level recommendations based on
		  the operational knowledge of expert users, delivering hints
		  on how to execute exploration tasks. As a real-world
		  application domain, we consider here the Robot-Assisted
		  Partial Nephrectomy (RAPN), which is the partial surgical
		  removal of a kidney to treat severe kidney diseases such as
		  cancer.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on Applied
		  Computing},
  pages		= {586–593},
  numpages	= {8},
  keywords	= {nephrectomy, process modelling (BPMN), surgical processes,
		  temporal durations},
  location	= {Virtual Event, Republic of Korea},
  series	= {SAC '21}
}

@InProceedings{	  10.1145/3463677.3463715,
  author	= {Palma, Ingrid and Ladeira, Marcelo and Reis, Ana Carla
		  Bittencourt},
  title		= {Machine Learning Predictive Model for the Passive
		  Transparency at the Brazilian Ministry of Mines and
		  Energy},
  year		= {2021},
  isbn		= {9781450384926},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3463677.3463715},
  doi		= {10.1145/3463677.3463715},
  abstract	= {This paper presents a case study based on the CRISP-DM
		  Model and the use of Text Mining tools and techniques to
		  automate the Passive Transparency process at the Brazilian
		  Ministry of Mines and Energy. Thus, a Machine Learning
		  Model is proposed to predict the class of the technical
		  unit responsible for the data/information requested by
		  citizens. Through the application of the algorithm LDA and
		  TF-IDF it was possible to map the topics of the most
		  relevant subjects for society. The stability of the model
		  was tested from the comparative analysis between 5 known
		  classification algorithms (Random Forest, Multinomial NB,
		  Linear SVC, Logistic Regression, XGBoost and Gradient
		  Boosting). XGBoost presented better performance and
		  precision in multiclass learning outcomes.},
  booktitle	= {Proceedings of the 22nd Annual International Conference on
		  Digital Government Research},
  pages		= {76–81},
  numpages	= {6},
  keywords	= {Machine Learning Algorithms, Multicriteria Decision
		  Making, Passive Transparency, Predictive Analisys and
		  XGBoost., Topic Modeling},
  location	= {Omaha, NE, USA},
  series	= {dg.o '21}
}

@Article{	  10.1145/3700639,
  author	= {Rao, Abishek and Aithal, Shivani and Singh, Sanjay},
  title		= {Single-Document Abstractive Text Summarization: A
		  Systematic Literature Review},
  year		= {2024},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3700639},
  doi		= {10.1145/3700639},
  abstract	= {Abstractive text summarization is a task in natural
		  language processing that automatically generates the
		  summary from the source document in a human-written form
		  with minimal loss of information. Research in text
		  summarization has shifted towards abstractive text
		  summarization due to its challenging aspects. This study
		  provides a broad systematic literature review of
		  abstractive text summarization on single-document
		  summarization to gain insights into the challenges, widely
		  used datasets, evaluation metrics, approaches, and methods.
		  This study reviews research articles published between 2011
		  and 2023 from popular electronic databases. In total, 226
		  journal and conference publications were included in this
		  review. The in-depth analysis of these papers helps
		  researchers understand the challenges, widely used
		  datasets, evaluation metrics, approaches, and methods. This
		  article identifies and discusses potential opportunities
		  and directions along with a generic conceptual framework
		  and guidelines on abstractive summarization models and
		  techniques for research in abstractive text
		  summarization.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {60},
  numpages	= {37},
  keywords	= {Text summarization, abstractive summarization, abstractive
		  text summarization, natural language processing}
}

@InProceedings{	  10.1145/3439961.3439993,
  author	= {Souza, \'{E}rica Ferreira de and Falbo, Ricardo de Almeida
		  and Specimille, Marcos S. and Coelho, Alexandre G. N. and
		  Vijaykumar, Nandamudi L. and Felizardo, Katia Romero and
		  Meinerz, Giovani Volnei},
  title		= {Experience Report on Developing an Ontology-based Approach
		  for Knowledge Management in Software Testing},
  year		= {2021},
  isbn		= {9781450389235},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3439961.3439993},
  doi		= {10.1145/3439961.3439993},
  abstract	= {Software testing is a knowledge intensive process. Thus,
		  Knowledge Management (KM) emerges as a means to manage
		  testing knowledge, and, consequently, to improve software
		  quality. However, there are only a few KM solutions
		  supporting software testing. This paper reports experiences
		  from the development of an approach, Ontology-based Testing
		  Knowledge Management (OntoT-KM), to assist in launching KM
		  initiatives in the software testing domain with the support
		  of Knowledge Management Systems (KMSs). OntoT-KM provides a
		  process guiding how to start applying KM in software
		  testing. OntoT-KM is based on the findings of a systematic
		  mapping on KM in software testing and the results of a
		  survey with testing practitioners. Moreover, OntoT-KM
		  considers the conceptualization established by a Reference
		  Ontology on Software Testing (ROoST). As a proof of
		  concept, OntoT-KM was applied to develop a KMS called
		  Testing KM Portal (TKMP), which was evaluated in terms of
		  usefulness, usability and functional correctness. Results
		  show that the developed KMS from OntoT-KM is a potential
		  system for managing knowledge in software testing, so, the
		  approach is able to guide KM initiatives in software
		  testing.},
  booktitle	= {Proceedings of the XIX Brazilian Symposium on Software
		  Quality},
  articleno	= {32},
  numpages	= {10},
  keywords	= {Knowledge Management, Knowledge Management System,
		  Software Testing, Testing Ontology},
  location	= {S\~{a}o Lu\'{\i}s, Brazil},
  series	= {SBQS '20}
}

@InProceedings{	  10.1145/3656766.3656776,
  author	= {Zhang, Xue and Wang, Li and Xu, Lianzheng and Fu, Deqian},
  title		= {A Distributed Logistics Data Security Sharing Model Based
		  on Semantics and CP-ABE},
  year		= {2024},
  isbn		= {9798400716478},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3656766.3656776},
  doi		= {10.1145/3656766.3656776},
  abstract	= {Effective sharing of logistics data in a secure manner is
		  vital for the growth and progress of the logistics
		  industry. With the development of logistics digitalization,
		  the demand for data sharing among logistics enterprises is
		  fiercely increasing. However, ensuring the safety and
		  security of logistics business data is a challenging task,
		  especially considering the importance of data sharing and
		  exchange for business collaboration. To address this issue,
		  we propose a distributed logistics data security sharing
		  model that adopts semantics and CP-ABE technology to solve
		  the semantic heterogeneity problem between logistics
		  enterprises and ensure data security. In addition, we
		  propose a semantic based access policy generation method,
		  which is integrated into CP-ABE to simplify the creation
		  process of access policies and improve the user
		  friendliness and practicality of the system.},
  booktitle	= {Proceedings of the 2023 3rd International Conference on
		  Big Data, Artificial Intelligence and Risk Management},
  pages		= {51–56},
  numpages	= {6},
  location	= {Chengdu, China},
  series	= {ICBAR '23}
}

@Article{	  10.1145/3565481,
  author	= {Rizzo, Stefano Giovanni and Brucato, Matteo and Montesi,
		  Danilo},
  title		= {Ranking Models for the Temporal Dimension of Text},
  year		= {2022},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3565481},
  doi		= {10.1145/3565481},
  abstract	= {Temporal features of text have been shown to improve
		  clustering and organization of documents, text
		  classification, visualization, and ranking. Temporal
		  ranking models consider the temporal expressions found in
		  text (e.g., “in 2021” or “last year”) as time
		  units, rather than as keywords, to define a temporal
		  relevance and improve ranking. This article introduces a
		  new class of ranking models called Temporal Metric Space
		  Models (TMSM), based on a new domain for representing
		  temporal information found in documents and queries, where
		  each temporal expression is represented as a time interval.
		  Furthermore, we introduce a new frequency-based baseline
		  called Temporal BM25 (TBM25). We evaluate the effectiveness
		  of each proposed metric against a purely textual baseline,
		  as well as several variations of the metrics themselves,
		  where we change the aggregate function, the time
		  granularity and the combination weight. Our extensive
		  experiments on five test collections show statistically
		  significant improvements of TMSM and TBM25 over
		  state-of-the-art temporal ranking models. Combining the
		  temporal similarity scores with the text similarity scores
		  always improves the results, when the combination weight is
		  between 2\% and 6\% for the temporal scores. This is true
		  also for test collections where only 5\% of queries contain
		  explicit temporal expressions.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= dec,
  articleno	= {49},
  numpages	= {34},
  keywords	= {Temporal information retrieval, Temporal Metric Space,
		  texto-temporal relevance, temporal ranking, timexes, time
		  similarity}
}

@InProceedings{	  10.1145/3341105.3373863,
  author	= {Motara, Yusuf Moosa},
  title		= {A structural modeling notation for the typed functional
		  paradigm},
  year		= {2020},
  isbn		= {9781450368667},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341105.3373863},
  doi		= {10.1145/3341105.3373863},
  abstract	= {Although typed functional programming is becoming
		  increasingly important for practical software development,
		  it remains inaccessible from a modeling perspective. This
		  paper develops and theoretically justifies an initial
		  best-practices notation for the typed functional paradigm.
		  A small case study explores how the same scenario is
		  modeled differently in the object-oriented and typed
		  functional paradigms, and it is argued that the notation
		  developed is a necessary step on the path to a more
		  comprehensive notation for modeling within the paradigm.},
  booktitle	= {Proceedings of the 35th Annual ACM Symposium on Applied
		  Computing},
  pages		= {1515–1522},
  numpages	= {8},
  keywords	= {fml, functional programming, modeling languages,
		  notation},
  location	= {Brno, Czech Republic},
  series	= {SAC '20}
}

@Article{	  10.1109/tcbb.2022.3167245,
  author	= {Di Persia, Leandro and Lopez, Tiago and Arce, Agustin and
		  Milone, Diego H. and Stegmayer, Georgina},
  title		= {exp2GO: Improving Prediction of Functions in the Gene
		  Ontology With Expression Data},
  year		= {2022},
  issue_date	= {March-April 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {2},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3167245},
  doi		= {10.1109/TCBB.2022.3167245},
  abstract	= {The computational methods for the prediction of gene
		  function annotations aim to automatically find associations
		  between a gene and a set of Gene Ontology (GO) terms
		  describing its functions. Since the hand-made curation
		  process of novel annotations and the corresponding wet
		  experiments validations are very time-consuming and costly
		  procedures, there is a need for computational tools that
		  can reliably predict likely annotations and boost the
		  discovery of new gene functions. This work proposes a novel
		  method for predicting annotations based on the inference of
		  GO similarities from expression similarities. The novel
		  method was benchmarked against other methods on several
		  public biological datasets, obtaining the best comparative
		  results. exp2GO effectively improved the prediction of GO
		  annotations in comparison to state-of-the-art methods.
		  Furthermore, the proposal was validated with a full genome
		  case where it was capable of predicting relevant and
		  accurate biological functions. The repository of this
		  project withh full data and code is available at
		  &lt;uri&gt;https://github.com/sinc-lab/exp2GO&lt;/uri&gt;.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= apr,
  pages		= {999–1008},
  numpages	= {10}
}

@InProceedings{	  10.1145/3677389.3702517,
  author	= {Yang, Can and Pereira Nunes, Bernardo and Rodr\'{\i}guez
		  M\'{e}ndez, Sergio and Chen, Yige and Manrique, Rub\'{e}n
		  and Casanova, Marco Antonio},
  title		= {Towards Comprehensive Artwork Representation: Motivations
		  and Challenges in Capturing Multi-Dimensional Art
		  Descriptions},
  year		= {2025},
  isbn		= {9798400710933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677389.3702517},
  doi		= {10.1145/3677389.3702517},
  abstract	= {This paper explores the motivations and challenges in
		  developing a comprehensive framework for artwork
		  representation. Current artwork-related ontologies and
		  description methods often fall short in capturing the
		  multi-faceted nature of artworks, focusing primarily on
		  basic metadata or specific aspects while neglecting others.
		  We propose a conceptual framework for an ontology that
		  integrates descriptive, contextual, and interpretive
		  aspects of artworks, addressing the limitations of existing
		  models. The paper discusses the potential benefits of this
		  holistic approach for art education, public appreciation,
		  and digital accessibility. Key challenges are identified,
		  including capturing complex visual elements, balancing
		  objectivity with subjectivity in interpretation, and
		  representing the multiple layers of meaning in artworks.
		  The proposed framework aims to enhance the quality and
		  depth of artwork representation, potentially facilitating
		  the development of automated systems for artwork analysis
		  and description.},
  booktitle	= {Proceedings of the 24th ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {15},
  numpages	= {5},
  keywords	= {contextual object model, descriptive object model,
		  ontology, captioning, artworks},
  location	= {Hong Kong, China},
  series	= {JCDL '24}
}

@InProceedings{	  10.5555/3522802.3522806,
  author	= {Wagner, Gerd},
  title		= {Business process modeling and simulation with DPMN:
		  processing activities},
  year		= {2022},
  publisher	= {IEEE Press},
  abstract	= {The Business Process Modeling Notation (BPMN) has been
		  established as a modeling standard in Business Process (BP)
		  Management. However, BPMN lacks several important elements
		  needed for BP simulation and is not well-aligned with the
		  Queueing Network paradigm of Operations Research and the
		  related BP simulation paradigm pioneered by the Discrete
		  Event Simulation (DES) languages/tools GPSS and
		  SIMAN/Arena. The Discrete Event Process Modeling Notation
		  (DPMN) proposed by Wagner (2018) is based on Event Graphs
		  (Schruben 1983), which capture the DES paradigm of
		  Event-Based Simulation. By allowing to make flowchart
		  models of queueing/processing networks with a precise
		  semantics, DPMN reconciles (the flowchart approach of) BPMN
		  with DES. DPMN is the first visual modeling language that
		  supports all important DES approaches: event-based
		  simulation, activity-based DES and Processing Network
		  models, providing a foundation for harmonizing and unifying
		  the many different terminologies/concepts and diagram
		  languages of established DES tools.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  articleno	= {4},
  numpages	= {15},
  location	= {Phoenix, Arizona},
  series	= {WSC '21}
}

@InProceedings{	  10.1145/3207677.3278056,
  author	= {Xian, Guojian and Li, Jiao and Kou, Yuantao and Luo,
		  Tingting and Huang, Yongwen},
  title		= {Construction and Application of Upper Country Ontology
		  Based on OWL and SKOS},
  year		= {2018},
  isbn		= {9781450365123},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3207677.3278056},
  doi		= {10.1145/3207677.3278056},
  abstract	= {The concept1 of country has been widely used in all kinds
		  of databases or systems. A commonly understood and unified
		  schema for country entity is needed to knowledge
		  organization, semantic association and deeply integration
		  of the multi-source and heterogeneous data resources about
		  country concepts. This paper proposes a computer
		  understandable and computable Upper Country Ontology based
		  on OWL and SKOS, with instances of 195 countries, 7
		  continents, 38 regions and more than 10 international
		  organizations. The proposed ontology is expected to be used
		  as knowledge middleware, supports identifying and indexing
		  country entities, publishing open linked dataset,
		  referencing country instance, SPARQL endpoint query and
		  semantic reasoning and so on.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Computer Science and Application Engineering},
  articleno	= {153},
  numpages	= {6},
  keywords	= {Jena, OWL, Prot\'{e}g\'{e}, SKOS, SPARQL, Upper Country
		  Ontology, WebVOWL},
  location	= {Hohhot, China},
  series	= {CSAE '18}
}

@InProceedings{	  10.1145/3357384.3358168,
  author	= {van Bremen, Timothy and Dries, Anton and Jung, Jean
		  Christoph},
  title		= {Ontology-Mediated Queries over Probabilistic Data via
		  Probabilistic Logic Programming},
  year		= {2019},
  isbn		= {9781450369763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357384.3358168},
  doi		= {10.1145/3357384.3358168},
  abstract	= {We study ontology-mediated querying over probabilistic
		  data for the case when the ontology is formulated in
		  EL(hdr), an expressive member of the EL family of
		  description logics. We leverage techniques that have been
		  developed (i) for classical ontology-mediated querying and
		  (ii) for probabilistic logic programming and provide an
		  implementation based on our findings. We include both
		  theoretical considerations and an experimental evaluation
		  of our approach.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2437–2440},
  numpages	= {4},
  keywords	= {ontology-mediated querying, probabilistic logic
		  programming, uncertainty},
  location	= {Beijing, China},
  series	= {CIKM '19}
}

@InProceedings{	  10.1145/3285957.3285987,
  author	= {Pawe\l{}oszek, Ilona and Korczak, Jerzy},
  title		= {Merging of Ontologies - Conceptual Design Issues},
  year		= {2018},
  isbn		= {9781450364898},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3285957.3285987},
  doi		= {10.1145/3285957.3285987},
  abstract	= {An approach to merging ontologies is presented that
		  integrates financial knowledge in Decision Support Systems.
		  The process of ontology merging is supported by an editor
		  Prot\'{e}g\'{e}, and illustrated by examples extracted from
		  financial information systems. It is shown how the
		  correspondences between concepts, properties, and relations
		  in the various ontologies can be processed. However not all
		  problems of ontology integration may be resolved
		  automatically. Therefore a number of cases of manager
		  involvement in the merging process are considered.},
  booktitle	= {Proceedings of the 2018 10th International Conference on
		  Information Management and Engineering},
  pages		= {59–63},
  numpages	= {5},
  keywords	= {Decision Support Systems, Financial ontology, Ontology
		  integration, Ontology merging},
  location	= {Salford, United Kingdom},
  series	= {ICIME 2018}
}

@InProceedings{	  10.1109/icse43902.2021.00040,
  author	= {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang,
		  Meng and Cleland-Huang, Jane},
  title		= {Traceability Transformed: Generating more Accurate Links
		  with Pre-Trained BERT Models},
  year		= {2021},
  isbn		= {9781450390859},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ICSE43902.2021.00040},
  doi		= {10.1109/ICSE43902.2021.00040},
  abstract	= {Software traceability establishes and leverages
		  associations between diverse development artifacts.
		  Researchers have proposed the use of deep learning trace
		  models to link natural language artifacts, such as
		  requirements and issue descriptions, to source code;
		  however, their effectiveness has been restricted by
		  availability of labeled data and efficiency at runtime. In
		  this study, we propose a novel framework called Trace BERT
		  (T-BERT) to generate trace links between source code and
		  natural language artifacts. To address data sparsity, we
		  leverage a three-step training strategy to enable trace
		  models to transfer knowledge from a closely related
		  Software Engineering challenge, which has a rich dataset,
		  to produce trace links with much higher accuracy than has
		  previously been achieved. We then apply the T-BERT
		  framework to recover links between issues and commits in
		  Open Source Projects. We comparatively evaluated accuracy
		  and efficiency of three BERT architectures. Results show
		  that a Single-BERT architecture generated the most accurate
		  links, while a Siamese-BERT architecture produced
		  comparable results with significantly less execution time.
		  Furthermore, by learning and transferring knowledge, all
		  three models in the framework outperform classical IR trace
		  models. On the three evaluated real-word OSS projects, the
		  best T-BERT stably outperformed the VSM model with average
		  improvements of 60.31\% measured using Mean Average
		  Precision (MAP). RNN severely underper-formed on these
		  projects due to insufficient training data, while T-BERT
		  overcame this problem by using pretrained language models
		  and transfer learning.},
  booktitle	= {Proceedings of the 43rd International Conference on
		  Software Engineering},
  pages		= {324–335},
  numpages	= {12},
  keywords	= {Software traceability, deep learning, language models},
  location	= {Madrid, Spain},
  series	= {ICSE '21}
}

@InProceedings{	  10.1145/3341105.3373848,
  author	= {de Kok, Sophie and Frasincar, Flavius},
  title		= {Using word embeddings for ontology-driven aspect-based
		  sentiment analysis},
  year		= {2020},
  isbn		= {9781450368667},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341105.3373848},
  doi		= {10.1145/3341105.3373848},
  abstract	= {Nowadays, the Web is the main platform to gather
		  information. The growing amount of freely available
		  unstructured data has increased the interest in sentiment
		  analysis, where the goal is to extract opinions from text.
		  In this paper we focus on review-level aspect-based
		  sentiment analysis, where we predict the sentiment of a
		  certain aspect in a review. We propose a two-stage
		  sentiment analysis algorithm. In the first stage a domain
		  ontology is utilized to predict the sentiment. If the
		  domain ontology stage is inconclusive, a back-up stage
		  based on an SVM bag-of-words model is employed.
		  Furthermore, the use of word embeddings to improve the
		  domain ontology coverage in the first stage by finding
		  semantically similar words is investigated. We find that
		  the two-stage approach significantly outperforms two
		  baseline methods and achieves competitive results for the
		  SemEval-2016 data. Furthermore, by not employing the
		  back-up stage, we still perform significantly better than
		  the baselines. Lastly, we find that employing word
		  embeddings improves the accuracy when the domain ontology
		  size is relatively small.},
  booktitle	= {Proceedings of the 35th Annual ACM Symposium on Applied
		  Computing},
  pages		= {834–842},
  numpages	= {9},
  keywords	= {aspect-based sentiment analysis, domain ontology,
		  review-level sentiment analysis, word embeddings},
  location	= {Brno, Czech Republic},
  series	= {SAC '20}
}

@InProceedings{	  10.1145/3678610.3678613,
  author	= {Bekmanova, Gulmira and Sairanbekova, Ayaulym and
		  Ongarbayev, Yerkin and Mukanova, Assel and Zulkhazhav,
		  Altanbek and Omarbekova, Assel and Ukenova, Aru},
  title		= {Intelligent question-answering system based on the public
		  political discourse knowledge},
  year		= {2024},
  isbn		= {9798400716799},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3678610.3678613},
  doi		= {10.1145/3678610.3678613},
  abstract	= {The article describes the implementation of a
		  question-answering system based on knowledge of public
		  political discourse. An ontology of the subject area was
		  developed, a program was developed to publish an answer to
		  a question on the topic of political discourse in the
		  Kazakh language.},
  booktitle	= {Proceedings of the 2024 10th International Conference on
		  E-Society, e-Learning and e-Technologies (ICSLT)},
  pages		= {14–19},
  numpages	= {6},
  keywords	= {Artificial intelligence, discourse, formalization, python,
		  owlready, sparql, OWL, knowledge base, ontology},
  location	= { },
  series	= {ICSLT '24}
}

@Article{	  10.1145/3715011,
  author	= {Doerr, Martin},
  title		= {Identifiable Individuals and Reality: Describing the Past
		  by&nbsp;Formal&nbsp;Propositions},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3715011},
  doi		= {10.1145/3715011},
  abstract	= {Data of historical and ideographic sciences, such as
		  cultural heritage studies, geography, geological evolution,
		  biodiversity, but also experimental data of nomothetic
		  natural sciences, are increasingly documented and published
		  in information systems compatible with predicate-logic that
		  refer to things in reality by unique identifiers (or
		  “keys” most likely to be unique in some context). This
		  can only work as a method of sharing and integrating
		  knowledge beyond the spatial and temporal context of local
		  projects, if the referred features or phenomena of reality
		  are distinct and can diachronically be identified by
		  independent observers in the same way and without a
		  clarifying dialogue between them. In this article, we argue
		  that only a smaller part of the features in our environment
		  are sufficiently distinct over a useful time-span in order
		  to form such “identifiable individuals”. Ontological
		  categories should each provide specific criteria for the
		  so-called ontological individuation, i.e., about how parts
		  of reality can be subdivided into identifiable individuals
		  that are useful for modelling their behaviour and
		  interactions in reality for answering specific scientific
		  questions, by obeying evidential constraints as a result of
		  applying these criteria in observations. We motivate by
		  several examples that there are always cases in which the
		  individuality of such an instance may be undecidable
		  basically within all such ontological categories and that
		  all ontological categories are more or less effective
		  approximations of reality. We argue that effective
		  knowledge sharing by information systems using formal
		  ontologies is only possible if these limitations of
		  applicability and precision to real world phenomena are
		  well understood and taken into account.},
  journal	= {J. Comput. Cult. Herit.},
  month		= may,
  articleno	= {27},
  numpages	= {32},
  keywords	= {Historical Sciences, Historical Data, Scientific realism,
		  Information integration, Ontological Individuation,
		  Epistemic Individuation}
}

@InProceedings{	  10.1145/3368756.3369090,
  author	= {Ihab, Moudhich and Soumaya, Loukili and Mohamed, Bahra and
		  Haytam, Hmami and Abdelhadi, Fennan},
  title		= {Ontology-based sentiment analysis and community detection
		  on social media: application to Brexit},
  year		= {2019},
  isbn		= {9781450362894},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368756.3369090},
  doi		= {10.1145/3368756.3369090},
  abstract	= {Sentiment Analysis and Community Detection are two of the
		  main methods used to analyze and comprehend human
		  interactions on social media. These domains expanded
		  immensely with the rise of social media, as it provided a
		  free and ever-increasing quantity of data. Domain
		  ontologies are of great assistance in collecting specific
		  data, as it describes the domain's features and their
		  existing relationships. Therefore, we utilize them in
		  collecting subject-specific data on social media. This
		  paper describes the framework we've designed in order to
		  understand, in depth, the impact of a subject on social
		  media users, and also to evaluate the difference between
		  the Lexicon Approach and the Machine Learning Approach, by
		  assessing the strengths and weaknesses of each. This
		  framework also aims to deeply understand the connections
		  that exist between users, depending on their point of view
		  on a particular subject. The resulting framework not only
		  analyzes textual data (by taking into account the negation
		  and sentence POS tags), but also visual one, such as
		  images. In order to test the framework, we chose to analyze
		  the Brexit phenomenon by collecting ontology-based data
		  from Twitter and Reddit, and it had some promising
		  results.},
  booktitle	= {Proceedings of the 4th International Conference on Smart
		  City Applications},
  articleno	= {103},
  numpages	= {7},
  keywords	= {classification, community detection, lexicon, machine
		  learning, ontology, opinion mining, sentiment analysis,
		  social media analysis},
  location	= {Casablanca, Morocco},
  series	= {SCA '19}
}

@InProceedings{	  10.1145/3419604.3419797,
  author	= {Khannat, Aicha and Sbai, Hanae and Kjiri, Laila},
  title		= {Towards Mining Semantically Enriched Configurable Process
		  Models},
  year		= {2020},
  isbn		= {9781450377331},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3419604.3419797},
  doi		= {10.1145/3419604.3419797},
  abstract	= {Providing configurable process model with high quality is
		  a primary objective to derive process variants with better
		  accuracy and facilitate process model reuse. For this
		  purpose, many research works have been interested in
		  configurable process mining techniques to discover and
		  configure processes from event logs. Moreover, to use the
		  knowledge captured by event logs when mining processes, the
		  concept of semantic process mining is introduced. It allows
		  for combining semantic technologies with process mining.
		  Despite the diversity of works in mining and customizing
		  configurable process models, the application of these
		  techniques is still limited to use semantics in minimizing
		  the complexity of discovered processes. However, it seems
		  to be pertinent to discover semantically enriched
		  configurable process models directly from event logs.
		  Consequently, this can facilitate using semantic in
		  configuring, verifying conformance or enhancing discovered
		  configurable processes. In this paper, we present a
		  comparative study of existing works that focus on mining
		  configurable process models with respect to semantic
		  technologies. Our aim is to propose a new framework to
		  automatically discover semantically enriched configurable
		  processes.},
  booktitle	= {Proceedings of the 13th International Conference on
		  Intelligent Systems: Theories and Applications},
  articleno	= {4},
  numpages	= {6},
  keywords	= {Configurable Process Model, Ontology, Process Mining,
		  Semantic Technologies, Variability},
  location	= {Rabat, Morocco},
  series	= {SITA'20}
}

@InProceedings{	  10.1145/3371647.3371666,
  author	= {Olabe, Juan Carlos and Basogain, Xabier and Olabe, Miguel
		  \'{A}ngel},
  title		= {Modern Education with a Computational Model of the Mind},
  year		= {2020},
  isbn		= {9781450372251},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3371647.3371666},
  doi		= {10.1145/3371647.3371666},
  abstract	= {We are witnessing a great effort on the part of the
		  educational systems of the world in modernizing the
		  curricular content of primary and secondary schools. One
		  example of these educational initiatives is the effort of
		  integrating aspects of engineering and technology with the
		  existing core subjects of sciences and mathematic in K-12
		  education. These efforts are often labeled as STEM or STEAM
		  (Science, Technology, Engineering, Mathematics, and Arts.)
		  These subjects, studied in an integral form, are considered
		  essential in the education of the citizens of a modern
		  society. A common obstacle encountered in the
		  implementation of these projects or initiatives is the lack
		  of consensus on the specific topics to be included in the
		  curriculum, the pedagogical methodology selected for the
		  classroom, and the means, computer-based or otherwise, to
		  be used by the teachers and the students. Often the lack of
		  consensus among the different constituencies in charge of
		  these projects finds its roots in the different assumptions
		  made by their participants. This paper addresses one of the
		  most acute set of differences present in these projects:
		  the teaching methods used in class, knowing the resources
		  and limitations of the human mind. In the last few decades
		  we have learned much of how the mind works; what tasks are
		  intrinsically easy or difficult for the human mind. Also,
		  with the extensive access to computing power in our
		  society, it is important to determine if a traditional task
		  was studied in school for its practical use or for its
		  value in developing the potential qualities of the mind. In
		  this paper we will use the word computation in its
		  traditional meaning of symbol manipulation. In that sense,
		  all processes of thinking, solving problems, and endeavors
		  of creation are processes of symbol manipulation, or
		  computation. In this paper we present a computational model
		  of the mind in order to provide a standard reference that
		  will help in finding answers to questions such as: when is
		  a task complex, what are the cognitive capabilities and
		  limitations of the mind, what teaching methodologies are
		  optimal, and why.},
  booktitle	= {Proceedings of the 2019 3rd International Conference on
		  Education and E-Learning},
  pages		= {41–45},
  numpages	= {5},
  keywords	= {Complex Systems, Computational Theory of the Mind,
		  Computing, Model of the Mind, New approaches to problem
		  solving, Ontology, STEM Education},
  location	= {Barcelona, Spain},
  series	= {ICEEL '19}
}

@InProceedings{	  10.1145/3640310.3674079,
  author	= {Sultan, Bastien and Apvrille, Ludovic},
  title		= {AI-Driven Consistency of SysML Diagrams},
  year		= {2024},
  isbn		= {9798400705045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640310.3674079},
  doi		= {10.1145/3640310.3674079},
  abstract	= {Graphical modeling languages, expected to simplify systems
		  analysis and design, present a challenge in maintaining
		  consistency across their varied views. Traditional
		  rule-based methods for ensuring consistency in languages
		  like UML often fall short in addressing complex semantic
		  dimensions. Moreover, the integration of Large Language
		  Models (LLMs) into Model Driven Engineering (MDE)
		  introduces additional consistency challenges, as LLM's
		  limited output contexts requires the integration of
		  responses. This paper presents a new framework that
		  automates the detection and correction of inconsistencies
		  across different views, leveraging formally defined rules
		  and incorporating OpenAI's GPT, as implemented in TTool.
		  Focusing on the consistency between use case and block
		  diagrams, the framework is evaluated through its
		  application to three case studies, highlighting its
		  potential to significantly enhance consistency management
		  in graphical modeling.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {149–159},
  numpages	= {11},
  location	= {Linz, Austria},
  series	= {MODELS '24}
}

@InProceedings{	  10.5555/3522802.3522925,
  author	= {Herding, Raphael and M\"{o}nch, Lars and Ehm, Hans},
  title		= {Design and application of an ontology for demand
		  fulfillment in semiconductor supply chains},
  year		= {2022},
  publisher	= {IEEE Press},
  abstract	= {Ensuring interoperability of different information systems
		  for planning and control is a challenging task in
		  semiconductor supply chains. This is partially caused by
		  the sheer size of the involved production facilities and
		  the supply chains in the semiconductor domain, the
		  permanent appearance of uncertainty, and the rapid
		  technological changes which lead to sophisticated planning
		  and control systems in this domain. Ontologies are a
		  promising approach to support interoperability among such
		  systems. Demand fulfillment is an important function in
		  semiconductor supply chains. However, at the same time, it
		  is a planning function that is not very well understood. In
		  the present paper, a domain- and task ontology for demand
		  fulfillment is designed based on a domain analysis. The
		  usage of the proposed ontology is illustrated by means of
		  an example.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  articleno	= {152},
  numpages	= {12},
  location	= {Phoenix, Arizona},
  series	= {WSC '21}
}

@InProceedings{	  10.1145/3508072.3508117,
  author	= {Hammoudeh, Mohammad and Adebisi, Bamidele and Unal, Devrim
		  and Laouid, Abdelkader},
  title		= {Bringing Coordination Languages Back to the Future Using
		  Blockchain Smart Contracts},
  year		= {2022},
  isbn		= {9781450387347},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3508072.3508117},
  doi		= {10.1145/3508072.3508117},
  abstract	= {This paper presents a blockchain extension of the run-time
		  Sensing as a Service SOA (3SOA) approach presented
		  in&nbsp;[5]. 3SOA defines a practical approach for
		  implementing service-oriented Internet of Things (IoT)
		  using coordination languages to integrate and program
		  individual IoT objects to compose into full IoT system. We
		  believe that the modularity, reuse, interoperability and
		  portability of this model has much to offer, but that there
		  exist some challenges in overcoming the performance issues
		  inherent in the approach, and extending the range of
		  applications to which it is suited. We are particularly
		  interested in applying the coordination languages to
		  decentralized systems. To this end, blockchain smart
		  contracts are proposed to offer a decentralized trustable
		  method to automatically verify compliance with pre-defined
		  conditions before executing a transaction involving
		  multiple parties. To validate our proposal, we demonstrate
		  a healthcare functional prototypes as a proof of concept.},
  booktitle	= {Proceedings of the 5th International Conference on Future
		  Networks and Distributed Systems},
  pages		= {299–304},
  numpages	= {6},
  keywords	= {Internet of Things, blockchain, service-oriented, smart
		  contracts},
  location	= {Dubai, United Arab Emirates},
  series	= {ICFNDS '21}
}

@InProceedings{	  10.1145/3580305.3599199,
  author	= {Gaur, Manas and Tsamoura, Efthymia and Sreedharan, Sarath
		  and Mittal, Sudip},
  title		= {KiL 2023 : 3rd International Workshop on
		  Knowledge-infused Learning},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599199},
  doi		= {10.1145/3580305.3599199},
  abstract	= {Recent prolific advances in artificial intelligence
		  through the incorporation of domain knowledge have
		  constituted a new paradigm for AI and data mining
		  communities. For example, the human feedback-based language
		  generation in ChatGPT (a large language model (LLM)), the
		  use of Protein Bank in DeepMind's AlphaFold, and the use of
		  23 rules of safety in DeepMind's Sparrow have demonstrated
		  the success of teaming human knowledge and AI. In addition,
		  the knowledge retrieval-guided language modeling methods
		  have strengthened the association between knowledge and AI.
		  However, translating research methods and resources into
		  practice presents a new challenge for the machine learning
		  and data/knowledge mining communities. For example, in
		  DARPA's Explainable AI seminar, the need for explainable
		  contextual adaptation is seen as the 3rd phase of AI,
		  facilitating the interplay between data and knowledge for
		  explainability, safety, and, eventually, trust. However,
		  policymakers and practitioners assert serious usability and
		  privacy concerns that constrain adoption, notably in
		  high-consequence domains, such as cybersecurity,
		  healthcare, and other social good domains. In addition,
		  limitations in output quality, measurement, and interactive
		  ability, including both the provision of explanations and
		  the acceptance of user preferences, result in low adoption
		  rates in such domains. This workshop aims to accelerate our
		  pace towards creating innovative methods for integrating
		  knowledge into contemporary AI and data science methods and
		  develop metrics for assessing performance in various
		  applications.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {5857–5858},
  numpages	= {2},
  keywords	= {explainable ai, games, knowledge-infused learning,
		  language models, neurosymbolic ai, programming languages,
		  safe ai},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3635059.3635062,
  author	= {Giarelis, Nikolaos and Mastrokostas, Charalampos and
		  Siachos, Ilias and Karacapilidis, Nikos},
  title		= {A Review of Greek NLP Technologies for Chatbot
		  Development},
  year		= {2024},
  isbn		= {9798400716263},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3635059.3635062},
  doi		= {10.1145/3635059.3635062},
  abstract	= {The advent of Generative AI has certainly boosted the
		  interest in developing innovative chatbot applications.
		  Despite a vast amount of machine learning (ML) and natural
		  language processing (NLP) research and English language
		  resources that greatly improve chatbot technology, the
		  corresponding research and resources for the Greek language
		  are limited. The contribution of this paper is twofold: (i)
		  it reports on the state-of-the-art research in Greek NLP,
		  as far as language resources, embeddings-based techniques,
		  deep learning models, and existing chatbot applications are
		  concerned; (ii) it offers a set of insights on current NLP
		  models and chatbot implementation methodologies, and
		  outlines a set of pending issues and future research
		  directions.},
  booktitle	= {Proceedings of the 27th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {15–20},
  numpages	= {6},
  keywords	= {Deep Learning, Greek Language, Large Language Models,
		  Review, Text Classification, Text Summarization, Word
		  Embeddings},
  location	= {Lamia, Greece},
  series	= {PCI '23}
}

@InProceedings{	  10.1145/3462757.3466149,
  author	= {Savelka, Jaromir and Westermann, Hannes and Benyekhlef,
		  Karim and Alexander, Charlotte S. and Grant, Jayla C. and
		  Amariles, David Restrepo and Hamdani, Rajaa El and
		  Mee\`{u}s, S\'{e}bastien and Troussel, Aurore and
		  Araszkiewicz, Micha\l{} and Ashley, Kevin D. and Ashley,
		  Alexandra and Branting, Karl and Falduti, Mattia and
		  Grabmair, Matthias and Hara\v{s}ta, Jakub and Novotn\'{a},
		  Tereza and Tippett, Elizabeth and Johnson, Shiwanni},
  title		= {Lex Rosetta: transfer of predictive models across
		  languages, jurisdictions, and legal domains},
  year		= {2021},
  isbn		= {9781450385268},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3462757.3466149},
  doi		= {10.1145/3462757.3466149},
  abstract	= {In this paper, we examine the use of multi-lingual
		  sentence embeddings to transfer predictive models for
		  functional segmentation of adjudicatory decisions across
		  jurisdictions, legal systems (common and civil law),
		  languages, and domains (i.e. contexts). Mechanisms for
		  utilizing linguistic resources outside of their original
		  context have significant potential benefits in AI \&amp;
		  Law because differences between legal systems, languages,
		  or traditions often block wider adoption of research
		  outcomes. We analyze the use of Language-Agnostic Sentence
		  Representations in sequence labeling models using Gated
		  Recurrent Units (GRUs) that are transferable across
		  languages. To investigate transfer between different
		  contexts we developed an annotation scheme for functional
		  segmentation of adjudicatory decisions. We found that
		  models generalize beyond the contexts on which they were
		  trained (e.g., a model trained on administrative decisions
		  from the US can be applied to criminal law decisions from
		  Italy). Further, we found that training the models on
		  multiple contexts increases robustness and improves overall
		  performance when evaluating on previously unseen contexts.
		  Finally, we found that pooling the training data from all
		  the contexts enhances the models' in-context performance.},
  booktitle	= {Proceedings of the Eighteenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {129–138},
  numpages	= {10},
  keywords	= {adjudicatory decisions, annotation, document segmentation,
		  domain adaptation, multi-lingual sentence embeddings,
		  transfer learning},
  location	= {S\~{a}o Paulo, Brazil},
  series	= {ICAIL '21}
}

@InProceedings{	  10.1145/3625007.3627514,
  author	= {Kim, Seonhyeong and Khan, Irshad and Kwon, Young-Woo},
  title		= {Realtime Disaster Detection Through GNN Models Using
		  Disaster Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400704093},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625007.3627514},
  doi		= {10.1145/3625007.3627514},
  abstract	= {In the context of the increasing scale and complexity of
		  disasters caused by rapid climate change, a comprehensive
		  understanding of disaster big data is essential for
		  effective detection and response. The disaster knowledge
		  graph proposed in this paper fills this gap by capturing
		  the connections between various disaster-related data
		  sources and their potential for growth across heterogeneous
		  datasets. We generate time-series disaster graphs every
		  minute using SNS data (e.g., Twitter) and public data,
		  specifically focusing on disasters. Then, we create
		  disaster knowledge graphs to represent the relationships
		  between various data sources and try to predict their
		  potential developments. We label and annotate knowledge
		  graphs and then detect sudden changes in time-series
		  disaster knowledge graphs for disaster detection. To that
		  end, we assess the effectiveness of three state-of-the-art
		  GNN models for graph-based event classification using Graph
		  Convolutional Network (GCN), Graph Attention Network (GAT),
		  and SageConv. In addition, we evaluate a simple clustering
		  model, K-means, for comparison. Our experiments show
		  promising results with approximately 87\% precision in
		  detecting disaster events using structural data and
		  connectivity patterns within disaster graphs. Finally, we
		  measure the result of disaster detection time with an
		  unseen dataset, showing positive results that about 70\%
		  detect a disaster in less than 3 minutes. To
		  comprehensively analyze real-time social media data and
		  understand the patterns of disaster to enhance disaster
		  management and response strategies, our approach combines
		  the strength of GNNs with a designed disaster knowledge
		  graph.},
  booktitle	= {Proceedings of the 2023 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {221–228},
  numpages	= {8},
  keywords	= {knowledge graphs, graph neural networks, disaster
		  detection},
  location	= {Kusadasi, Turkiye},
  series	= {ASONAM '23}
}

@InProceedings{	  10.1145/3707292.3707389,
  author	= {Li, Yanjun and Yang, Ruiting and Guo, Donghao and Song,
		  Yu},
  title		= {Research on the Construction of Digital Knowledge Graphs
		  Based on Resources of National First-Class Undergraduate
		  Programs},
  year		= {2025},
  isbn		= {9798400707308},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3707292.3707389},
  doi		= {10.1145/3707292.3707389},
  abstract	= {[Purpose/Significance]: The digitalization of education is
		  an essential path to advancing higher education. The
		  construction of knowledge graphs is a key approach to
		  achieving the digitalization and intelligence of education.
		  [Method/Process]: This paper leverages the rich video
		  resources of existing national first-class undergraduate
		  programs and, based on the teaching orientations of
		  different universities, independently designs customized
		  ontologies and extraction principles. These are then
		  integrated into the LLM knowledge graph builder to ensure
		  the hierarchical structure of the overall course framework.
		  The course video content is transformed into text form, and
		  large language models (LLMS) and word segmentation tools
		  are used for core content extraction, text cleaning, and
		  lexical analysis. The structured text is then converted
		  into SPO (Subject-Predicate-Object) triplets database.
		  [Results/Conclusions]: Finally, the database is imported
		  into the LLM knowledge graph builder, which is
		  pre-configured with extraction rules. It will automatically
		  generate the knowledge graph. After the text is imported
		  into the LLM knowledge graph builder, it will be manually
		  checked to ensure it better meets the actual needs of the
		  students. [Innovation/Limitations]: The research team plans
		  to apply the knowledge graph to train a specialized
		  knowledge-based Q&amp;A assistant. This will support
		  students' understanding and self-assessment of knowledge
		  points in an online learning community. Student feedback
		  will be used to improve and enrich the knowledge graph.
		  Compared to existing methods, this approach better aligns
		  with the constantly evolving digital teaching resources
		  available online, offering more comprehensive and
		  higher-level automation.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Artificial Intelligence and Intelligent Information
		  Processing},
  pages		= {353–359},
  numpages	= {7},
  keywords	= {Knowledge graph, course resources, intelligent Q&amp;A,
		  ontology construction, personalized learning},
  location	= { },
  series	= {AIIIP '24}
}

@Article{	  10.1145/3314948,
  author	= {Mohammadi, Majid and Hofman, Wout and Tan, Yao-Hua},
  title		= {Simulated Annealing-based Ontology Matching},
  year		= {2019},
  issue_date	= {March 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {10},
  number	= {1},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3314948},
  doi		= {10.1145/3314948},
  abstract	= {Ontology alignment is a fundamental task to reconcile the
		  heterogeneity among various information systems using
		  distinct information sources. The evolutionary algorithms
		  (EAs) have been already considered as the primary strategy
		  to develop an ontology alignment system. However, such
		  systems have two significant drawbacks: they either need a
		  ground truth that is often unavailable, or they utilize the
		  population-based EAs in a way that they require massive
		  computation and memory. This article presents a new
		  ontology alignment system, called SANOM, which uses the
		  well-known simulated annealing as the principal technique
		  to find the mappings between two given ontologies while no
		  ground truth is available. In contrast to population-based
		  EAs, the simulated annealing need not generate populations,
		  which makes it significantly swift and memory-efficient for
		  the ontology alignment problem. This article models the
		  ontology alignment problem as optimizing the fitness of a
		  state whose optimum is obtained by using the simulated
		  annealing. A complex fitness function is developed that
		  takes advantage of various similarity metrics including
		  string, linguistic, and structural similarities. A
		  randomized warm initialization is specially tailored for
		  the simulated annealing to expedite its convergence. The
		  experiments illustrate that SANOM is competitive with the
		  state-of-the-art and is significantly superior to other
		  EA-based systems.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= may,
  articleno	= {3},
  numpages	= {24},
  keywords	= {OAEI, Ontology alignment, SANOM, simulated annealing}
}

@InProceedings{	  10.1145/3368756.3369075,
  author	= {Hamim, Touria and Benabbou, Faouzia and Sael, Nawal},
  title		= {Student profile modeling: an overview model},
  year		= {2019},
  isbn		= {9781450362894},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368756.3369075},
  doi		= {10.1145/3368756.3369075},
  abstract	= {Today, provide a customized, personalized and adaptive
		  service is a big challenge that improve the services and
		  functionalities of systems. The knowledge of the user
		  profile is a vital phase in this process. Profile modeling
		  is an important field that aims to give an abstract
		  representation of some aspects related to the user
		  features. In the educational field, student profile
		  modeling can be a support for decision-making at different
		  levels like learning, orientation, and recommendation. It
		  can mainly offer the most exact description of students in
		  order to: be able to act in case of problems such as
		  failure, drop out; offer students the most appropriate
		  orientation and recommendation; and define the most
		  adaptive learning resources depending on their profile. In
		  this paper, we present an analytical and statistical study
		  on student profile modeling to propose a detailed
		  description of the student's profile obtained with
		  different techniques in different contexts of the
		  educational field. We aim to extract the different
		  categories of student's features that can be used alone or
		  combined for decision making in different fields.},
  booktitle	= {Proceedings of the 4th International Conference on Smart
		  City Applications},
  articleno	= {88},
  numpages	= {9},
  keywords	= {e-portfolio, learner profile, machine learning,
		  ontologies, profile modeling},
  location	= {Casablanca, Morocco},
  series	= {SCA '19}
}

@InProceedings{	  10.1145/3498851.3498942,
  author	= {Liu, Li and Li, Xuebo},
  title		= {Research and Construction of Classical Formulas Knowledge
		  Graph Based on Ontology},
  year		= {2022},
  isbn		= {9781450391870},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3498851.3498942},
  doi		= {10.1145/3498851.3498942},
  abstract	= {Classical formula is an important part and basis of
		  traditional Chinese prescriptions. The effective storage
		  and expression of classical formula knowledge in ancient
		  books and modern documents is a key issue for studying and
		  using classical formulas. This paper standardized and
		  normalized the contents of the classic formula in "Treatise
		  on Exogenous Febrile Disease" and "The Synopsis of the
		  Golden Chamber", collected and organized classical formulas
		  related documents from China National Knowledge
		  Infrastructure (CNKI). Ontology construction tool
		  Prot\'{e}g\'{e} is used to establish the domain ontology of
		  classical formulas, and Neo4j graph database is used to
		  construct the knowledge graph of ancient books and modern
		  CNKI documents. 296 ancient classical formula items and
		  11175 modern documents from CNKI are collected, normalized
		  and stored in database as the data source of this research.
		  On the basis of these work, constructed the ontology and
		  knowledge graph of classical formulas and built an
		  application system for knowledge query. Constructing
		  knowledge graph from top to bottom based on ontology can
		  express and visualize the related knowledge of classical
		  formulas accurately and efficiently. The construction
		  strategy mentioned in this paper has got a good result and
		  showed great potential in traditional Chinese medicine
		  knowledge domain.},
  booktitle	= {IEEE/WIC/ACM International Conference on Web Intelligence
		  and Intelligent Agent Technology},
  pages		= {140–143},
  numpages	= {4},
  keywords	= {Classical Formula, Knowledge Graph, TCM},
  location	= {Melbourne, VIC, Australia},
  series	= {WI-IAT '21}
}

@InProceedings{	  10.5555/3466184.3466211,
  author	= {Jose, Justin and Singh, Divye and Patel, Amit and
		  Hayatnagarkar, Harshal G.},
  title		= {Simulating re-configurable multi-rovers for planetary
		  exploration using behavior-based ontology},
  year		= {2021},
  isbn		= {9781728194998},
  publisher	= {IEEE Press},
  abstract	= {For planetary explorations, the space agencies have
		  usually sent single robotic rovers to complete missions. An
		  alternative approach is to send multiple rovers, which can
		  insure against failure of one or more rovers. Planning for
		  a multi-rover mission has its own challenges, and
		  simulations can aid in identifying and addressing such
		  challenges. In this paper, we present an ontology-based
		  approach to simulate a multi-rover planetary exploration
		  mission, with a focus on resilience, adaptation,
		  heterogeneity, and reconfigurability. We present an
		  ontology that describes multiple rovers along with an
		  inventory of their parts shipped with a lander. Our
		  approach shows that having the ontology-based simulations
		  help in complex scenarios such as to loan parts from
		  inventory, and salvaging a damaged rover for good parts.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {254–265},
  numpages	= {12},
  location	= {Orlando, Florida},
  series	= {WSC '20}
}

@InProceedings{	  10.1145/3234698.3234761,
  author	= {Hemam, Mounir and Djezzar, Meriem and Seghir, Zianou
		  Ahmed},
  title		= {Multi-Viewpoints Ontological Knowledge Representation: A
		  Fuzzy Description Logics Based Approach},
  year		= {2018},
  isbn		= {9781450363921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3234698.3234761},
  doi		= {10.1145/3234698.3234761},
  abstract	= {Description Logics (DLs) play a key role in the design of
		  ontologies. An ontology is a formal description of
		  important concepts in a particular domain. In practice, a
		  set of a specific-domain applications use different
		  representations of the same real world entity due to
		  various viewpoints, context, and specific interest. In this
		  paper, we are interested in the problem of representing an
		  ontology in a heterogeneous domain by taking into
		  consideration different viewpoints and different
		  terminologies of various users, groups or even communities
		  in the organisation. This type of ontology, called
		  multi-viewpoints ontology, confers to the same universe of
		  discourse, several partial descriptions, where each one is
		  relative to a particular viewpoint. Moreover, these partial
		  descriptions share at global level, fuzzy ontological
		  elements allowing the representation of vague/imprecise
		  knowledge between the various viewpoints. So, our goal is
		  to propose a fuzzy multi-viewpoints ontology Web language,
		  which is an extension of OWL-DL language, to allow the
		  multi-viewpoints ontologies representation.},
  booktitle	= {Proceedings of the Fourth International Conference on
		  Engineering \&amp; MIS 2018},
  articleno	= {63},
  numpages	= {6},
  keywords	= {Description Logics, Fuzziness, Knowledge Engineering,
		  Ontology, Semantic Web, Viewpoint},
  location	= {Istanbul, Turkey},
  series	= {ICEMIS '18}
}

@InProceedings{	  10.1109/models-c.2019.00095,
  author	= {Reynolds, Owen J.},
  title		= {Towards model-driven self-explanation for autonomous
		  decision-making systems},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00095},
  doi		= {10.1109/MODELS-C.2019.00095},
  abstract	= {The ability for systems to make decisions by themselves is
		  increasing with advances in different areas of AI such as
		  machine learning and optimisation techniques for autonomous
		  systems among other. Humans are handing over more decisions
		  to systems that provide no explanations for their
		  judgements unless they are enabled explicitly in their
		  design. Trust based on a program being well written and
		  tested correctly is not appropriate for AI-based autonomous
		  systems. Unlike traditional software, this new software
		  increasingly exhibit emergent behaviours making it
		  unpredictable due to unexpected situations.
		  Self-explanation is sometimes implemented, tracking
		  decisions to give explanations to users. A more consistent,
		  proven approach to self-explanation would be needed for
		  making trustable systems.The paper proposes a research
		  agenda to define an architecture to enable self-explanation
		  for autonomous decision-making systems. The approach will
		  be model-driven to facilitate reuse, the rapid development
		  of tools and suitable abstractions for demonstrating
		  concepts. The architecture will be informed by existing
		  research in provenance ontology and model version research.
		  The evaluation of the architecture is expected to be done
		  using two case studies. The first will implement
		  self-explanation as a primary concern in the building of a
		  system. The second case will attempt to fit
		  self-explanation to an existing system.},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {624–628},
  numpages	= {5},
  keywords	= {autonomous, decision-making, model-driven,
		  self-explanation},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.1145/3587259.3627555,
  author	= {Verkijk, Stella and Roothaert, Ritten and Pernisch, Romana
		  and Schlobach, Stefan},
  title		= {Do you catch my drift? On the usage of embedding methods
		  to measure concept shift in knowledge graphs},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627555},
  doi		= {10.1145/3587259.3627555},
  abstract	= {Automatically detecting and measuring differences between
		  evolving Knowledge Graphs (KGs) has been a topic of
		  investigation for years. With the rising popularity of
		  embedding methods, we investigate the possibility of using
		  embeddings to detect Concept Shift in evolving KGs.
		  Specifically, we go deeper into the usage of nearest
		  neighbour set comparison as the basis for a similarity
		  measure, and show why this approach is conceptually
		  problematic. As an alternative, we explore the possibility
		  of using clustering methods. This paper serves to (i)
		  inform the community about the challenges that arise when
		  using KG embeddings for the comparison of different
		  versions of a KG specifically, (ii) investigate how this is
		  supported by theories on knowledge representation and
		  semantic representation in NLP and (iii) take the first
		  steps into the direction of valuable representation of
		  semantics within KGs for comparison.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {70–74},
  numpages	= {5},
  keywords	= {Concept Shift, Knowledge Graph Embeddings, NLP,
		  Semantics},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3419804.3420262,
  author	= {Reynolds, Owen and Garc\'{\i}a-Dom\'{\i}nguez, Antonio and
		  Bencomo, Nelly},
  title		= {Towards automated provenance collection for runtime models
		  to record system history},
  year		= {2020},
  isbn		= {9781450381406},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3419804.3420262},
  doi		= {10.1145/3419804.3420262},
  abstract	= {In highly dynamic environments, systems are expected to
		  make decisions on the fly based on their observations that
		  are bound to be partial. As such, the reasons for its
		  runtime behaviour may be difficult to understand. In these
		  cases, accountability is crucial, and decisions by the
		  system need to be traceable. Logging is essential to
		  support explanations of behaviour, but it poses challenges.
		  Concerns about analysing massive logs have motivated the
		  introduction of structured logging, however, knowing what
		  to log and which details to include is still a challenge.
		  Structured logs still do not necessarily relate events to
		  each other, or indicate time intervals. We argue that
		  logging changes to a runtime model in a provenance graph
		  can mitigate some of these problems. The runtime model
		  keeps only relevant details, therefore reducing the volume
		  of the logs, while the provenance graph records causal
		  connections between the changes and the activities
		  performed by the agents in the system that have introduced
		  them. In this paper, we demonstrate a first version towards
		  a reusable infrastructure for the automated construction of
		  such a provenance graph. We apply it to a multithreaded
		  traffic simulation case study, with multiple concurrent
		  agents managing different parts of the simulation. We show
		  how the provenance graphs can support validating the system
		  behaviour, and how a seeded fault is reflected in the
		  provenance graphs.},
  booktitle	= {Proceedings of the 12th System Analysis and Modelling
		  Conference},
  pages		= {12–21},
  numpages	= {10},
  keywords	= {Provenance, multi-threading, runtime models,
		  self-adaptation, self-explanation},
  location	= {Virtual Event, Canada},
  series	= {SAM '20}
}

@Article{	  10.1007/s00165-021-00555-2,
  author	= {St\"{u}nkel, Patrick and K\"{o}nig, Harald and Lamo, Yngve
		  and Rutle, Adrian},
  title		= {Comprehensive Systems: A formal foundation for Multi-Model
		  Consistency Management},
  year		= {2021},
  issue_date	= {Dec 2021},
  publisher	= {Springer-Verlag},
  address	= {Berlin, Heidelberg},
  volume	= {33},
  number	= {6},
  issn		= {0934-5043},
  url		= {https://doi.org/10.1007/s00165-021-00555-2},
  doi		= {10.1007/s00165-021-00555-2},
  abstract	= {Model management is a central activity in Software
		  Engineering. The most challenging aspect of model
		  management is to keep inter-related models consistent with
		  each other while they evolve. As a consequence, there is a
		  lot of scientific activity in this area, which has produced
		  an extensive body of knowledge, methods, results and tools.
		  The majority of these approaches, however, are limited to
		  binary inter-model relations; i.e. the synchronisation of
		  exactly two models. Yet, not every multi-ary relation can
		  be factored into a family of binary relations. In this
		  paper, we propose and investigate a novel comprehensive
		  system construction, which is able to represent multi-ary
		  relations among multiple models in an integrated manner and
		  thus serves as a formal foundation for artefacts used in
		  consistency management activities involving multiple
		  models. The construction is based on the definition of
		  partial commonalities among a set of models using the same
		  language, which is used to denote the (local) models. The
		  main theoretical results of this paper are proofs of the
		  facts that comprehensive systems are an admissible
		  environment for (i) applying formal means of consistency
		  verification (diagrammatic predicate framework), (ii)
		  performing algebraic graph transformation (weak adhesive
		  HLR category), and (iii) that they generalise the
		  underlying setting of graph diagrams and triple graph
		  grammars.},
  journal	= {Form. Asp. Comput.},
  month		= dec,
  pages		= {1067–1114},
  numpages	= {48},
  keywords	= {Multi-modelling, Inter-model consistency, Consistency
		  verification, Consistency restoration, Model
		  synchronisation, Multi-directional transformations (MX),
		  Model merging, Model weaving, Graph diagrams, Triple graph
		  grammars, Category theory, Adhesive categories}
}

@InProceedings{	  10.1145/3722237.3722259,
  author	= {Zhao, Linlin and Liu, Zhansheng and Zhao, Xuefeng},
  title		= {Developing a Knowledge Graph for Intelligent Structural
		  Design Course},
  year		= {2025},
  isbn		= {9798400712692},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3722237.3722259},
  doi		= {10.1145/3722237.3722259},
  abstract	= {Knowledge graphs provide concept visualization and context
		  information across many applications. However, the process
		  of building a knowledge graph by transforming large and
		  intricate unstructured data into a new domain presents
		  numerous challenges. Learning from an Intelligent
		  Structural Design (ISD) course requires comprehension of
		  structural concepts, design techniques, AI technologies,
		  and the tackling of multifaceted real-project problems,
		  which require adaptive learning strategies and cognitive
		  engagement. This study introduces a bottom-up approach to
		  constructing a knowledge graph specifically designed for
		  the course. Furthermore, by employing a deep learning
		  model, a personalized learning path could be generated
		  based on a student's submitted assignments. To evaluate the
		  effectiveness of this novel educational paradigm, an
		  experiment was conducted involving two groups. The results
		  indicate that participants who used the Course Knowledge
		  Graph (CKG) attained higher scores than those instructed
		  through conventional teaching methods.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Artificial Intelligence and Education},
  pages		= {123–128},
  numpages	= {6},
  keywords	= {Course Knowledge Graph, Intelligent Structural Design,
		  Smart Construction, natural language processing (NLP),
		  ontology},
  location	= { },
  series	= {ICAIE '24}
}

@InProceedings{	  10.1109/models-c.2019.00015,
  author	= {Almeida, Jo\~{a}o Paulo A. and Rutle, Adrian and Wimmer,
		  Manuel},
  title		= {Preface to the 6th international workshop on multi-level
		  modelling (MULTI 2019)},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00015},
  doi		= {10.1109/MODELS-C.2019.00015},
  abstract	= {Multi-level modeling (MLM) represents a significant
		  extension to the traditional two-level object-oriented
		  paradigm with the potential to dramatically improve upon
		  the utility, reliability and complexity of models.
		  Different from conventional approaches, they allow for an
		  arbitrary number of classification levels and introduce
		  other concepts that foster expressiveness, reuse and
		  adaptability. A key aspect of the MLM paradigm is the use
		  of entities that are simultaneously types and instances, a
		  feature which has consequences for conceptual modeling,
		  language engineering and for the development of model-based
		  software systems.},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {64–65},
  numpages	= {2},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.1145/3227609.3227649,
  author	= {Flisar, Jernej and Podgorelec, Vili},
  title		= {Document Enrichment using DBPedia Ontology for Short Text
		  Classification},
  year		= {2018},
  isbn		= {9781450354899},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3227609.3227649},
  doi		= {10.1145/3227609.3227649},
  abstract	= {Every day, millions of short-texts are generated for which
		  effective tools for organization and retrieval are
		  required. Because of the short length of these documents
		  and of their extremely sparse representations, the
		  traditional text classification methods are not effective.
		  We propose a new approach that uses DBpedia Spotlight
		  annotation tools, to identify relevant entities in text and
		  enrich short text documents with concepts derived from
		  those entities, represented in DBpedia ontology. Our
		  experiments show that the proposed document enrichment
		  approach is beneficial for classification of short texts,
		  and is robust with respect to concept drifts and input
		  sources. We report experimental results in three
		  challenging collections, using a variety of classification
		  methods. The results show that the use of DBpedia ontology
		  significantly improves the classification performance of
		  classifiers in short-text classification.},
  booktitle	= {Proceedings of the 8th International Conference on Web
		  Intelligence, Mining and Semantics},
  articleno	= {8},
  numpages	= {9},
  keywords	= {DBPedia, ontology, short text classification, text
		  enrichment},
  location	= {Novi Sad, Serbia},
  series	= {WIMS '18}
}

@Article{	  10.1145/3447508,
  author	= {Gottlob, Georg and Hernich, Andr\'{e} and Kupke, Clemens
		  and Lukasiewicz, Thomas},
  title		= {Stable Model Semantics for Guarded Existential Rules and
		  Description Logics: Decidability and Complexity},
  year		= {2021},
  issue_date	= {October 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {68},
  number	= {5},
  issn		= {0004-5411},
  url		= {https://doi.org/10.1145/3447508},
  doi		= {10.1145/3447508},
  abstract	= {This work investigates the decidability and complexity of
		  database query answering under guarded existential rules
		  with nonmonotonic negation according to the classical
		  stable model semantics. In this setting, existential
		  quantification is interpreted via Skolem functions, and the
		  unique name assumption is adopted. As a first result, we
		  show the decidability of answering first-order queries
		  based on such rules by a translation into the
		  satisfiability problem for guarded second-order formulas
		  having the tree-model property. To obtain precise
		  complexity results for unions of conjunctive queries, we
		  transform the original problem in polynomial time into an
		  intermediate problem that is easier to analyze: query
		  answering for guarded disjunctive existential rules with
		  stratified negation. We obtain precise bounds for the
		  general setting and for various restricted settings. We
		  also consider extensions of the original formalism with
		  negative constraints, keys, and the possibility of negated
		  atoms in queries. Finally, we show how the above results
		  can be used to provide decidability and complexity results
		  for a natural adaptation of the stable model semantics to
		  description logics such as&nbsp;ELHI and the
		  DL-Lite&nbsp;family.},
  journal	= {J. ACM},
  month		= oct,
  articleno	= {35},
  numpages	= {87},
  keywords	= {Answer set programming, stable models, complexity,
		  decidability}
}

@InProceedings{	  10.1145/3339252.3341496,
  author	= {Doynikova, Elena and Fedorchenko, Andrey and Kotenko,
		  Igor},
  title		= {Ontology of Metrics for Cyber Security Assessment},
  year		= {2019},
  isbn		= {9781450371643},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3339252.3341496},
  doi		= {10.1145/3339252.3341496},
  abstract	= {Development of metrics that are valuable for assessing
		  security and decision making is an important element of
		  efficient counteraction to cyber threats. The paper
		  proposes an ontology of metrics for cyber security
		  assessment. The developed ontology is based on determining
		  the concepts and relations between primary features of
		  initial security data and forming a set of hierarchically
		  interconnected security metrics. The paper describes the
		  main classes of the proposed ontology, the revealed
		  relations, the involved security metrics, and the used data
		  sources. The publicly available sources of security data
		  are analyzed to get primary security metrics. Application
		  of the approach is shown on a case study. The main feature
		  of the proposed ontology is representation of security
		  metrics as separate instances of ontology. It allows using
		  the relations between the concepts of ontology for
		  calculating integral metrics reflecting the security
		  state.},
  booktitle	= {Proceedings of the 14th International Conference on
		  Availability, Reliability and Security},
  articleno	= {52},
  numpages	= {8},
  keywords	= {Security metrics, countering cyber attacks, intelligent
		  data analysis, ontology, security assessment, semantics},
  location	= {Canterbury, CA, United Kingdom},
  series	= {ARES '19}
}

@InProceedings{	  10.1145/3423390.3423395,
  author	= {Tsoutsa, Paraskevi and Ragos, Omiros},
  title		= {Towards an Ontology for Teamwork Enabled Services},
  year		= {2020},
  isbn		= {9781450377324},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3423390.3423395},
  doi		= {10.1145/3423390.3423395},
  abstract	= {Teamwork ability is a crucial aspect of humans, agents and
		  other intelligent systems. This study is dedicated to the
		  conceptual formal modeling of service teamworking by
		  considering concepts defined within a vast range of models
		  and theories of human and agent team development. We make
		  the conceptual based modeling by using ontologies to define
		  teams, roles, services and their modeling domains
		  abstracting the most important concepts of team development
		  whilst providing a decent level of formality and
		  unambiguity. An ontology named TrEWSOnto is proposed as a
		  model to assist the development process of teamwork enabled
		  services in tandem with the representation of their
		  composition process activity. This includes (i) the
		  definition of concepts that are used to improve the
		  teamwork ability of peer services, (ii) the description of
		  the main concepts are used for the role modeling
		  composition, and (iii) the description of situations that
		  teamwork roles should monitor to catch potential
		  "unhealthy" behavior happen during the service activity in
		  order to run proactively and avoid obstacles or collisions.
		  The result is a structure of five ontology sections
		  together forming a representation for TeamwoRk Enabled Web
		  (TrEW) Services and the environment they live.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Algorithms, Computing and Systems},
  pages		= {69–75},
  numpages	= {7},
  keywords	= {Composition, Microservices, Teamwork, Teamwork Ontology,
		  Web Service},
  location	= {Rabat, Morocco},
  series	= {ICACS '20}
}

@InProceedings{	  10.1145/3423334.3431448,
  author	= {Dassereto, Federico and Rocco, Laura Di and Shaw, Shanley
		  and Guerrini, Giovanna and Bertolotto, Michela},
  title		= {How to Tune Parameters in Geographical Ontologies
		  Embedding},
  year		= {2020},
  isbn		= {9781450381604},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3423334.3431448},
  doi		= {10.1145/3423334.3431448},
  abstract	= {Many Natural Language Processing (NLP) tasks, like
		  question answering or analyzing verbatim comments, have
		  started to use word embeddings due to their ability to
		  capture semantic relations between words. Recently,
		  embeddings have been also applied in the geospatial context
		  to represent geospatial ontologies, thanks to their ability
		  to capture semantic similarity. In this paper, we present
		  an analysis of a promising embedding technique particularly
		  suitable for representing hierarchical structures. We
		  conduct a deep technical evaluation of many parameters and
		  their impact on the quality of the representation.},
  booktitle	= {Proceedings of the 4th ACM SIGSPATIAL Workshop on
		  Location-Based Recommendations, Geosocial Networks, and
		  Geoadvertising},
  articleno	= {2},
  numpages	= {9},
  keywords	= {Embeddings, Geographic Information Retrieval, Geotagging,
		  Knowledge Bases},
  location	= {Seattle, WA, USA},
  series	= {LocalRec'20}
}

@InProceedings{	  10.1145/3726302.3730365,
  author	= {Jia, Pengyue and Cai, Qingpeng and Zhao, Xiangyu and Pan,
		  Ling and Xin, Xin and Huang, Jin and Zhang, Weinan and
		  Zhao, Li and Yin, Dawei and Yang, Grace Hui},
  title		= {AgentIR: 2nd Workshop on Agent-based Information
		  Retrieval},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3730365},
  doi		= {10.1145/3726302.3730365},
  abstract	= {Information retrieval (IR) systems are essential in modern
		  society, aiding users to efficiently locate relevant
		  information through query expansion, document retrieval,
		  ranking, and re-ranking. User feedback from ranked outputs
		  forms a dynamic interaction loop with IR systems, which can
		  be modeled as either one-time or sequential decision-making
		  problems. Over the past decade, deep reinforcement learning
		  (DRL) has emerged as a promising approach to
		  decision-making, leveraging the high model capacity of deep
		  learning for complex tasks. While significant research has
		  explored the application of DRL to IR tasks, several
		  fundamental challenges remain underexplored, including the
		  underlying information theory in DRL settings, the
		  limitations of reinforcement learning methods for
		  industrial IR applications, and the simulation of DRL-based
		  IR systems. Concurrently, the advent of large language
		  models (LLMs) has introduced new opportunities for
		  optimizing and simulating IR systems. Building on the
		  success of the Agent-based IR Workshop at SIGIR 2024, we
		  propose hosting the second Agent-based IR Workshop at SIGIR
		  2025. This workshop will continue to provide a platform for
		  researchers and practitioners from academia and industry to
		  present cutting-edge advances in DRL-based and LLM-based IR
		  systems from an agent-based perspective. By building on the
		  foundation laid in the first workshop, the 2025 edition
		  aims to delve deeper into emerging research challenges,
		  foster collaborations, and explore innovative applications.
		  Through engaging discussions and insightful presentations,
		  the workshop seeks to further expand the boundaries of IR
		  research and solidify its role as a premier venue for
		  advancing agent-based IR systems.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {4180–4183},
  numpages	= {4},
  keywords	= {agent-based information retrieval, drl, llm},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@InProceedings{	  10.1145/3472306.3478342,
  author	= {Antunes, Ana and Campos, Joana and Dias, Jo\~{a}o and
		  Santos, Pedro A. and Prada, Rui},
  title		= {EEG Model: Emotional Episode Generation for Social Sharing
		  of Emotions},
  year		= {2021},
  isbn		= {9781450386197},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3472306.3478342},
  doi		= {10.1145/3472306.3478342},
  abstract	= {Social sharing of emotions (SSE) occurs when one
		  communicates their feelings and reactions to a certain
		  event in the course of a social interaction. The phenomenon
		  is part of our social fabric and plays an important role in
		  creating empathetic responses and establishing rapport.
		  Intelligent social agents capable of SSE will have a
		  mechanism to create and build long-term interaction with
		  humans. In this paper, we present the Emotional Episode
		  Generation (EEG) model, a fine-tuned GPT-2 model capable of
		  generating emotional social talk regarding multiple event
		  tuples in a human-like manner. Human evaluation results
		  show that the model successfully translates one or more
		  event-tuples into emotional episodes, reaching quality
		  levels close to human performance. Furthermore, the model
		  clearly expresses one emotion in each episode as well as
		  humans. To train this model we used a public dataset and
		  built upon it using event extraction techniques1.},
  booktitle	= {Proceedings of the 21st ACM International Conference on
		  Intelligent Virtual Agents},
  pages		= {1–8},
  numpages	= {8},
  keywords	= {emotional text generation, event-to-text generation,
		  social agents},
  location	= {Virtual Event, Japan},
  series	= {IVA '21}
}

@InProceedings{	  10.1145/3487664.3487742,
  author	= {Mondal, Shalmoly and Hassani, Alireza and Jayaraman, Prem
		  Prakash and Delir Haghighi, Pari and Georgakopoulos,
		  Dimitrios},
  title		= {Modelling IoT Application Requirements for Benchmarking
		  IoT Middleware Platforms},
  year		= {2022},
  isbn		= {9781450395564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487664.3487742},
  doi		= {10.1145/3487664.3487742},
  abstract	= {The significant advances in the Internet of Things (IoT)
		  have led to IoT applications being widely used in various
		  scenarios ranging from smart city, smart farming, to
		  Industrial IoT (IIoT) solutions. With the explosion of IoT
		  application development, IoT middleware platforms are
		  increasingly being used for hosting such IoT applications.
		  This has given rise to the need for developing benchmarking
		  solutions to analyze and test the performance of different
		  middleware platforms that host these IoT applications. To
		  develop such benchmarks, there are a number of key
		  components that are needed. One of these components is an
		  IoT dataset. To generate such datasets, representing IoT
		  application requirements in a general and formal way is
		  important. In this paper, we propose a framework to model
		  the IoT Applications Requirements and enable Data
		  Generation(ARDG-IoT). The framework supports a formal way
		  to capture IoT application requirements and use these
		  requirements to generate IoT data that can be used to
		  create benchmarks for different IoT middleware platforms.
		  ARDG-IoT consists of our proposed model, IoTSySML, which
		  captures the application requirements, and an IoT data
		  simulator tool, which is used to generate IoT data. We
		  present an evaluation of the framework using a real world
		  Industrial IoT application case study.},
  booktitle	= {The 23rd International Conference on Information
		  Integration and Web Intelligence},
  pages		= {553–561},
  numpages	= {9},
  keywords	= {Benchmarking, IoT Application Requirements, IoT
		  Middleware, Requirements Engineering, Requirements
		  Modelling},
  location	= {Linz, Austria},
  series	= {iiWAS2021}
}

@Proceedings{	  10.1145/3571788,
  title		= {VaMoS '23: Proceedings of the 17th International Working
		  Conference on Variability Modelling of Software-Intensive
		  Systems},
  year		= {2023},
  isbn		= {9798400700019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Odense, Denmark}
}

@Article{	  10.1145/3484828,
  author	= {Amaral, Gabriel and Piscopo, Alessandro and Kaffee,
		  Lucie-aim\'{e}e and Rodrigues, Odinaldo and Simperl, Elena},
  title		= {Assessing the Quality of Sources in Wikidata Across
		  Languages: A Hybrid Approach},
  year		= {2021},
  issue_date	= {December 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {4},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3484828},
  doi		= {10.1145/3484828},
  abstract	= {Wikidata is one of the most important sources of
		  structured data on the web, built by a worldwide community
		  of volunteers. As a secondary source, its contents must be
		  backed by credible references; this is particularly
		  important, as Wikidata explicitly encourages editors to add
		  claims for which there is no broad consensus, as long as
		  they are corroborated by references. Nevertheless, despite
		  this essential link between content and references,
		  Wikidata's ability to systematically assess and assure the
		  quality of its references remains limited. To this end, we
		  carry out a mixed-methods study to determine the relevance,
		  ease of access, and authoritativeness of Wikidata
		  references, at scale and in different languages, using
		  online crowdsourcing, descriptive statistics, and machine
		  learning. Building on previous work of ours, we run a
		  series of microtasks experiments to evaluate a large corpus
		  of references, sampled from Wikidata triples with labels in
		  several languages. We use a consolidated, curated version
		  of the crowdsourced assessments to train several machine
		  learning models to scale up the analysis to the whole of
		  Wikidata. The findings help us ascertain the quality of
		  references in Wikidata and identify common challenges in
		  defining and capturing the quality of user-generated
		  multilingual structured data on the web. We also discuss
		  ongoing editorial practices, which could encourage the use
		  of higher-quality references in a more immediate way. All
		  data and code used in the study are available on GitHub for
		  feedback and further improvement and deployment by the
		  research community.},
  journal	= {J. Data and Information Quality},
  month		= oct,
  articleno	= {23},
  numpages	= {35},
  keywords	= {Wikidata, crowdsourcing, verifiability, data quality,
		  knowledge graphs}
}

@InProceedings{	  10.1145/3627673.3679228,
  author	= {Dew, Rebecca and Li, Mingzhao and Baratha Raj, Sandya},
  title		= {A Skill Proficiency Framework for Workforce Learning and
		  Development},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679228},
  doi		= {10.1145/3627673.3679228},
  abstract	= {Understanding the skills and proficiency levels required
		  for various roles is crucial for effective workforce
		  planning, learning and development. In this paper, we
		  propose a robust skill proficiency modeling framework that
		  offers a structured method to help describe, assess and
		  develop proficiency in key skills, facilitating
		  individuals' career pathways and aiding organizations in
		  talent management and adaptability. We first design a skill
		  proficiency description pipeline, which generates
		  statements describing the requirements at each proficiency
		  level of a skill. Following this, we build a skill
		  proficiency by occupation model using large-scale job ad
		  data to help organizations and individuals understand the
		  skill proficiency requirements for different roles.
		  Finally,we design a visual analytics system, based on a
		  real-world career pathway scenario, to demonstrate the
		  practical usefulness and effectiveness of our framework. A
		  demo video is available at
		  www.dropbox.com/scl/fi/nd0f3vi03n12g4y0sluaw/cikm24_demo.mp4?rlkey=55vya144q5ftai1uqqaubr5u5.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5210–5214},
  numpages	= {5},
  keywords	= {GPT, large language model, skill proficiency, visual
		  analytics},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3711896.3737011,
  author	= {Abdullahi, Tassallah and Gemou, Ioanna and Nayak, Nihal V.
		  and Murtaza, Ghulam and Bach, Stephen H. and Eickhoff,
		  Carsten and Singh, Ritambhara},
  title		= {K-Paths: Reasoning over Graph Paths for Drug Repurposing
		  and Drug Interaction Prediction},
  year		= {2025},
  isbn		= {9798400714542},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3711896.3737011},
  doi		= {10.1145/3711896.3737011},
  abstract	= {Biomedical knowledge graphs (KGs) encode rich, structured
		  information critical for drug discovery tasks, but
		  extracting meaningful insights from large-scale KGs remains
		  challenging due to their complex structure. Existing
		  biomedical subgraph retrieval methods are tailored for
		  graph neural networks (GNNs), limiting compatibility with
		  other paradigms, including large language models (LLMs). We
		  introduce K-Paths, a model-agnostic retrieval framework
		  that extracts structured, diverse, and biologically
		  meaningful multi-hop paths from dense biomedical KGs. These
		  paths enable prediction of unobserved drug-drug and
		  drug-disease interactions, including those involving
		  entities not seen during training, thus supporting
		  inductive reasoning. K-Paths is training-free and employs a
		  diversity-aware adaptation of Yen's algorithm to extract
		  the K shortest loopless paths between entities in a query,
		  prioritizing biologically relevant and relationally diverse
		  connections. These paths serve as concise, interpretable
		  reasoning chains that can be directly integrated with LLMs
		  or GNNs to improve generalization, accuracy, and enable
		  explainable inference. Experiments on benchmark datasets
		  show that K-Paths improves zero-shot reasoning across
		  state-of-the-art LLMs. For instance, Tx-Gemma 27B improves
		  by 19.8 and 4.0 F1 points on interaction severity
		  prediction and drug repurposing tasks, respectively. Llama
		  70B achieves gains of 8.5 and 6.2 points on the same tasks.
		  K-Paths also boosts the training efficiency of EmerGNN, a
		  state-of-the-art GNN, by reducing the KG size by 90\% while
		  maintaining predictive performance. Beyond efficiency,
		  K-Paths bridges the gap between KGs and LLMs, enabling
		  scalable and explainable LLM-augmented scientific
		  discovery. We release our code and the retrieved paths as a
		  benchmark for inductive reasoning.},
  booktitle	= {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining V.2},
  pages		= {5–16},
  numpages	= {12},
  keywords	= {drug discovery, explainability, gnns, inductive reasoning,
		  knowledge graph reasoning, llms},
  location	= {Toronto ON, Canada},
  series	= {KDD '25}
}

@InProceedings{	  10.1145/3220547.3220551,
  author	= {Fan, Liju and Flood, Mark D.},
  title		= {An Ontology of Ownership and Control Relations for Bank
		  Holding Companies},
  year		= {2018},
  isbn		= {9781450358835},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3220547.3220551},
  doi		= {10.1145/3220547.3220551},
  abstract	= {We consider the challenges and benefits of ontologies for
		  information management for regulatory reporting from bank
		  holding companies (BHCs). Many BHCs, especially the largest
		  and most complex firms, have multiple federal supervisors
		  who oversee a diverse array of subsidiaries. This creates a
		  federated data management problem that disperses
		  information across many firms and regulators. We prototype
		  an ontology for the Federal Reserve's public National
		  Information Center (NIC) database. The NIC identifies all
		  BHCs, their subsidiaries, and the ownership and control
		  relationships among them. It is a basic official source on
		  the structure of the industry. A formal ontology can
		  capture this expert-curated knowledge in a coherent,
		  structured format. This could assure data integrity and
		  enable non-experts to more readily integrate and analyze
		  data about complex organizations. We test the design and
		  development of federated prototype ontologies in OWL/RDF to
		  provide and integrate the NIC data with precise semantics
		  for transparency and consistency. Our preliminary results
		  indicate that this is feasible in practice for data search
		  and analysis, and that the ontologies can facilitate
		  semantic integration and improve the integrity of data and
		  metadata.},
  booktitle	= {Proceedings of the Fourth International Workshop on Data
		  Science for Macro-Modeling with Financial and Economic
		  Datasets},
  articleno	= {3},
  numpages	= {6},
  keywords	= {Financial regulation, bank holding companies, data
		  integration, data integrity, knowledge representation,
		  ontologies},
  location	= {Houston, TX, USA},
  series	= {DSMM'18}
}

@InProceedings{	  10.1145/3689187.3709607,
  author	= {Clear, Tony and Cajander, \r{A}sa and Clear, Alison and
		  McDermott, Roger and Daniels, Mats and Divitini, Monica and
		  Forshaw, Matthew and Humble, Niklas and Kasinidou, Maria
		  and Kleanthous, Styliani and Kultur, Can and Parvini,
		  Ghazaleh and Polash, Mohammad and Zhu, Tingting},
  title		= {AI Integration in the IT Professional Workplace: A Scoping
		  Review and Interview Study with Implications for Education
		  and Professional Competencies},
  year		= {2025},
  isbn		= {9798400712081},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689187.3709607},
  doi		= {10.1145/3689187.3709607},
  abstract	= {As Artificial Intelligence (AI) continues transforming
		  workplaces globally, particularly within the Information
		  Technology (IT) industry, understanding its impact on IT
		  professionals and computing curricula is crucial. This
		  research builds on joint work from two countries,
		  addressing concerns about AI's increasing influence in IT
		  sector workplaces and its implications for tertiary
		  education. The study focuses on AI technologies such as
		  generative AI (GenAI) and large language models (LLMs). It
		  examines how they are perceived and adopted and their
		  effects on workplace dynamics, task allocation, and
		  human-system interaction.IT professionals, noted as early
		  adopters of AI, offer valuable insights into the interplay
		  between AI and work engagement, highlighting the
		  significant competencies required for digital workplaces.
		  This study employs a dual-method approach, combining a
		  systematic and multi-vocal literature review and
		  qualitative research methods. These included a thematic
		  analysis of a set of 47 interviews conducted between March
		  and May of 2024 with IT professionals in two countries (New
		  Zealand and Sweden). The research aimed to understand the
		  implications for computing students, education curricula,
		  and the assessment of emerging professional
		  competencies.The literature review found insufficient
		  evidence addressing comprehensive AI practice
		  methodologies, highlighting the need to both develop and
		  regulate professional competencies for effective AI
		  integration. Key interview findings revealed diverse levels
		  of GenAI adoption, ranging from individual experimentation
		  to institutional integration. Participants generally
		  expressed positive attitudes toward the technology and were
		  actively pursuing self-learning despite some concerns. The
		  themes emerging from the interviews included AI's role in
		  augmenting human tasks, privacy and security concerns,
		  productivity enhancements, legal and ethical challenges,
		  and the evolving need for new competencies in the
		  workplace.The study underscores the critical role of
		  competency frameworks in guiding professional development
		  and ensuring preparedness for an AI-driven environment.
		  Additionally, it highlights the need for educational
		  institutions to adapt curricula to address these emerging
		  demands effectively},
  booktitle	= {2024 Working Group Reports on Innovation and Technology in
		  Computer Science Education},
  pages		= {34–67},
  numpages	= {34},
  keywords	= {artificial intelligence, computing competencies, computing
		  curricula, generative ai, it profession, large language
		  models},
  location	= {Milan, Italy},
  series	= {ITiCSE 2024}
}

@InProceedings{	  10.1145/3297280.3297525,
  author	= {Me\v{s}kelundefined, Donatas and Frasincar, Flavius},
  title		= {ALDONA: a hybrid solution for sentence-level aspect-based
		  sentiment analysis using a lexicalised domain ontology and
		  a neural attention model},
  year		= {2019},
  isbn		= {9781450359337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297280.3297525},
  doi		= {10.1145/3297280.3297525},
  abstract	= {Sentences containing several different polarity aspects
		  cause one of the main problems in sentiment analysis.
		  Depending on an aspect, the same context words can have
		  different effects on its sentiment value. Additionally, the
		  polarity can be influenced by the domain-specific
		  knowledge, showing the necessity to incorporate it into the
		  sentiment classification. In this paper we present a hybrid
		  solution for sentence-level aspect-based sentiment analysis
		  using A Lexicalised Domain Ontology and Neural Attention
		  (ALDONA) model to handle the problems mentioned above. To
		  measure the influence of each word in a given sentence on
		  an aspect's polarity, we introduce the bidirectional
		  context attention mechanism. Moreover, the classification
		  module is designed to handle the sentence's complex
		  structure. Finally, the manually created lexicalised domain
		  ontology (represented in OWL) is integrated to exploit the
		  field-specific knowledge. Computational results obtained on
		  a benchmark data set based on Web reviews have shown
		  ALDONA's ability to outperform several state-of-the-art
		  models and stress its contribution to aspect-based
		  sentiment classification.},
  booktitle	= {Proceedings of the 34th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {2489–2496},
  numpages	= {8},
  keywords	= {aspect-based sentiment classification, bidirectional gated
		  neural network, hybrid model, lexicalised domain ontology},
  location	= {Limassol, Cyprus},
  series	= {SAC '19}
}

@InProceedings{	  10.1145/3511808.3557365,
  author	= {Wang, Shuo and Zhang, Yifei and Lin, Bochen and Li,
		  Boxun},
  title		= {Interpretable Emotion Analysis Based on Knowledge Graph
		  and OCC Model},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557365},
  doi		= {10.1145/3511808.3557365},
  abstract	= {Sentiment analysis or opinion mining has been significant
		  for information extraction from the text. At the same time,
		  emotion psychology also proposed many appraisal theories
		  for emotional evaluations and concrete predictions. While
		  sentiment analysis focuses on identifying the polarity,
		  appraisal theories of emotion can define different emotions
		  and view emotions as process rather than states. In real
		  life, the mechanism of emotional generations and
		  interactions is complicated. Only plausible polarity can't
		  provide enough explanations for the emotional mechanism.
		  Hence an explainable model is in demand during emotion
		  inference and dynamical analysis. In this paper, an
		  analysis framework is constructed for interpreting casual
		  association based on the emotional logic. Knowledge graph
		  is introduced into the appraisal theories for inferring the
		  emotions and predicting the action tendency. The emotion
		  knowledge graph levels: concept level and case level. The
		  concept level can be built manually as an abstract based on
		  the appraisal model of Ortony, Clore \&amp; Collins (OCC
		  model). The inference and predictions can be implemented at
		  this level. The case level includes entities, objects,
		  events and cognitive relations between them that extract
		  from the text through the modular functions. The elements
		  in the case level can be linked to the abstract types in
		  the concept level for the emotional inference. We test this
		  emotional analysis framework on several datasets from the
		  appraisal theory and the text of drama works. The results
		  demonstrate that our framework can make better inferences
		  on emotions and good interpretability for human beings.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {2038–2045},
  numpages	= {8},
  keywords	= {analysis framework, appraisal theories, emotional logic,
		  knowledge graph, occ model},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3604951.3605514,
  author	= {Anuradha Nanomi Arachchige, Isuri and Ha, Le and Mitkov,
		  Ruslan and Steinert, Johannes-Dieter},
  title		= {Enhancing Named Entity Recognition for Holocaust
		  Testimonies through Pseudo Labelling and Transformer-based
		  Models},
  year		= {2023},
  isbn		= {9798400708411},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604951.3605514},
  doi		= {10.1145/3604951.3605514},
  abstract	= {The Holocaust was a tragic and catastrophic event in World
		  War II (WWII) history that resulted in the loss of millions
		  of lives. In recent years, the emergence of the field of
		  digital humanities has made the study of Holocaust
		  testimonies an important area of research for historians,
		  Holocaust educators, social scientists, and linguists. One
		  of the challenges in analysing Holocaust testimonies is the
		  recognition and categorisation of named entities such as
		  concentration camps, military officers, ships, and ghettos,
		  due to the scarcity of annotated data. This paper presents
		  a research study on a domain-specific hybrid named-entity
		  recognition model, which focuses on developing NER models
		  specifically tailored for the Holocaust domain. To overcome
		  the problem of data scarcity, we employed hybrid annotation
		  approach to training different transformer model
		  architectures in order to recognise the named entities.
		  Results show transformer models to have good performance
		  compared to other approaches.},
  booktitle	= {Proceedings of the 7th International Workshop on
		  Historical Document Imaging and Processing},
  pages		= {85–90},
  numpages	= {6},
  keywords	= {Holocaust Testimonies, NER, Pseudo Labelling,
		  Transformers},
  location	= {San Jose, CA, USA},
  series	= {HIP '23}
}

@Article{	  10.1145/3579353,
  author	= {Casadei, Roberto},
  title		= {Macroprogramming: Concepts, State of the Art, and
		  Opportunities of Macroscopic Behaviour Modelling},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {13s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3579353},
  doi		= {10.1145/3579353},
  abstract	= {Macroprogramming refers to the theory and practice of
		  expressing the macro(scopic) behaviour of a collective
		  system using a single program. Macroprogramming approaches
		  are motivated by the need of effectively capturing
		  global/system-level aspects and the collective behaviour of
		  multiple computational components, while abstracting over
		  low-level details. Previously, this programming style had
		  been primarily adopted to describe the data-processing
		  logic in sensor networks; recently, research forums on
		  spatial computing, collective systems, and the Internet of
		  Things have provided renewed interest in macro approaches.
		  However, related contributions are still fragmented and
		  lack conceptual consistency. Therefore, to foster
		  principled research, an integrated view of the field is
		  provided, together with opportunities and challenges.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {275},
  numpages	= {37},
  keywords	= {Macro programming, system-level programming, collective
		  intelligence}
}

@Article{	  10.14778/3407790.3407858,
  author	= {Sen, Jaydeep and Lei, Chuan and Quamar, Abdul and
		  \"{O}zcan, Fatma and Efthymiou, Vasilis and Dalmia, Ayushi
		  and Stager, Greg and Mittal, Ashish and Saha, Diptikalyan
		  and Sankaranarayanan, Karthik},
  title		= {ATHENA++: natural language querying for complex nested SQL
		  queries},
  year		= {2020},
  issue_date	= {August 2020},
  publisher	= {VLDB Endowment},
  volume	= {13},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3407790.3407858},
  doi		= {10.14778/3407790.3407858},
  abstract	= {Natural Language Interfaces to Databases (NLIDB) systems
		  eliminate the requirement for an end user to use complex
		  query languages like SQL, by translating the input natural
		  language (NL) queries to SQL automatically. Although a
		  significant volume of research has focused on this space,
		  most state-of-the-art systems can at best handle simple
		  select-project-join queries. There has been little to no
		  research on extending the capabilities of NLIDB systems to
		  handle complex business intelligence (BI) queries that
		  often involve nesting as well as aggregation. In this
		  paper, we present Athena++, an end-to-end system that can
		  answer such complex queries in natural language by
		  translating them into nested SQL queries. In particular,
		  Athena++ combines linguistic patterns from NL queries with
		  deep domain reasoning using ontologies to enable nested
		  query detection and generation. We also introduce a new
		  benchmark data set (FIBEN), which consists of 300 NL
		  queries, corresponding to 237 distinct complex SQL queries
		  on a database with 152 tables, conforming to an ontology
		  derived from standard financial ontologies (FIBO and FRO).
		  We conducted extensive experiments comparing Athena++ with
		  two state-of-the-art NLIDB systems, using both FIBEN and
		  the prominent Spider benchmark. Athena++ consistently
		  outperforms both systems across all benchmark data sets
		  with a wide variety of complex queries, achieving 88.33\%
		  accuracy on FIBEN benchmark, and 78.89\% accuracy on Spider
		  benchmark, beating the best reported accuracy results on
		  the dev set by 8\%.},
  journal	= {Proc. VLDB Endow.},
  month		= jul,
  pages		= {2747–2759},
  numpages	= {13}
}

@InProceedings{	  10.1145/3356395.3365545,
  author	= {Kurte, Kuldeep and Potnis, Abhishek and Durbha, Surya},
  title		= {Semantics-enabled Spatio-Temporal Modeling of Earth
		  Observation Data: An application to Flood Monitoring},
  year		= {2019},
  isbn		= {9781450369541},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3356395.3365545},
  doi		= {10.1145/3356395.3365545},
  abstract	= {Extreme events such as urban floods are dynamic in nature,
		  i.e. they evolve with time. The spatiotemporal analysis of
		  such disastrous events is important for understanding the
		  resiliency of an urban system during these events. Remote
		  Sensing (RS) data is one of the crucial earth observation
		  (EO) data sources that can facilitate such spatiotemporal
		  analysis due to its wide spatial coverage and high temporal
		  availability. In this paper, we propose a discrete
		  mereotopology (DM) based approach to enable representation
		  and querying of spatiotemporal information from a series of
		  multitemporal RS images that are acquired during a flood
		  disaster event. We represent this spatiotemporal
		  information using a semantic model called Dynamic Flood
		  Ontology (DFO). To establish the effectiveness and
		  applicability of the proposed approach, spatiotemporal
		  queries relevant during an urban flood scenario such as,
		  show me road segments that were partially flooded during
		  the time interval t1 have been demonstrated with promising
		  results.},
  booktitle	= {Proceedings of the 2nd ACM SIGSPATIAL International
		  Workshop on Advances on Resilient and Intelligent Cities},
  pages		= {41–50},
  numpages	= {10},
  keywords	= {discrete mereotopology, flood disaster, ontology,
		  semantics, spatial relations, spatiotemporal},
  location	= {Chicago, IL, USA},
  series	= {ARIC'19}
}

@InProceedings{	  10.1145/3307630.3342397,
  author	= {Shaaban, Abdelkader Magdy and Gruber, Thomas and
		  Schmittner, Christoph},
  title		= {Ontology-Based Security Tool for Critical Cyber-Physical
		  Systems},
  year		= {2019},
  isbn		= {9781450366687},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3307630.3342397},
  doi		= {10.1145/3307630.3342397},
  abstract	= {Industry 4.0 considers as a new advancement concept of the
		  industrial revolution, which introduces a full utilization
		  of Internet technologies. This concept aims to combine
		  diverse technological resources into the industry field,
		  which enables the communication between two worlds: the
		  physical and the cyber one. Cyber-physical Systems are one
		  of the special forces that integrate and build a variety of
		  existing technologies and components. The diversity of
		  components and technologies creates new security threats
		  that can exploit vulnerabilities to attack a critical
		  system. This work introduces an ontology-based security
		  tool-chain able to be integrated with the initial stages of
		  the development process of critical systems. The tool
		  detects the potential threats, and apply the suitable
		  security requirements which can address these threats.
		  Eventually, it uses the ontology approach to ensure that
		  the security requirements are fulfilled.},
  booktitle	= {Proceedings of the 23rd International Systems and Software
		  Product Line Conference - Volume B},
  pages		= {207–210},
  numpages	= {4},
  keywords	= {cyber-physical system, ontology, security, threats},
  location	= {Paris, France},
  series	= {SPLC '19}
}

@Article{	  10.1109/tcbb.2023.3322753,
  author	= {Hasan, Md. Al Mehedi and Maniruzzaman, Md. and Shin,
		  Jungpil},
  title		= {Gene Expression and Metadata Based Identification of Key
		  Genes for Hepatocellular Carcinoma Using Machine Learning
		  and Statistical Models},
  year		= {2023},
  issue_date	= {Nov.-Dec. 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2023.3322753},
  doi		= {10.1109/TCBB.2023.3322753},
  abstract	= {Biomarkers associated with hepatocellular carcinoma (HCC)
		  are of great importance to better understand biological
		  response mechanisms to internal or external intervention.
		  The study aimed to identify key candidate genes for HCC
		  using machine learning (ML) and statistics-based
		  bioinformatics models. Differentially expressed genes
		  (DEGs) were identified using limma and then selected their
		  common genes among DEGs identified from four datasets.
		  After that, protein-protein interaction networks were
		  constructed using STRING and then Cytoscape was used to
		  determine hub genes, significant modules, and their
		  associated genes. Simultaneously, three ML-based techniques
		  such as support vector machine (SVM), least absolute
		  shrinkage and selection operator-logistic regression
		  (LASSO-LR), and partial least squares-discriminant analysis
		  (PLS-DA) were implemented to determine the discriminative
		  genes of HCC from common DEGs. Moreover, metadata of hub
		  genes were formed by listing all hub genes from existing
		  studies to incorporate other findings in our analysis.
		  Finally, seven key candidate genes (ASPM, CCNB1, CDK1,
		  DLGAP5, KIF20 A, MT1X, and TOP2A) were identified by
		  intersecting common genes among hub genes, significant
		  modules genes, discriminative genes from SVM, LASSO-LR, and
		  PLS-DA, and meta hub genes from existing studies. Another
		  three independent test datasets were also used to validate
		  these seven key candidate genes using AUC, computed from
		  ROC.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= oct,
  pages		= {3786–3799},
  numpages	= {14}
}

@InProceedings{	  10.1145/3184558.3191548,
  author	= {Enea, Roberto and Pazienza, Maria Teresa and Turbati,
		  Andrea and Colantonio, Alessandro},
  title		= {How to Support Human Operator in "Uncertainty" Managing
		  during the Ontology Learning Process},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3191548},
  doi		= {10.1145/3184558.3191548},
  abstract	= {Creating ontologies is an essential while challenging task
		  to be performed by either a human or a system: on one hand
		  it is excessively burdensome for a human operator, on the
		  other it is very complex also for a machine due to the not
		  negligible amount of "uncertainty" that it must be able to
		  manage. In the last years, some attempts have been made to
		  automate this process, but at present, due to the large
		  number of aspects to be covered in the automatic creation
		  of an ontology (such as Domain terminology extraction,
		  Concept discovery, Concept hierarchy derivation, ")
		  satisfactory solutions have not been reached yet. In order
		  to produce efficient tools for both creation and enrichment
		  of ontologies, the participation of the human in such a
		  process still seems necessary. Our approach, that foresees
		  a broader framework for ontology learning, is based by
		  first on the automatic extraction of triples from
		  heterogeneous sources, then on the presentation of the most
		  reliable triples to the human operator for validation
		  purposes. The system provides the user with a series of
		  graphical representations that can give him an overview of
		  the level of uncertainty of the automatically generated
		  ontology. Then provides the user with the possibility to
		  perform SPARQL what-if queries, (i.e. assuming as true the
		  triples filtered according to the level of confidence, the
		  source and the structure of the triples).},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {1147–1154},
  numpages	= {8},
  keywords	= {human computer interaction, ontology learning, reasoning
		  on uncertain knowledge},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3550356.3561573,
  author	= {Prinz, Andreas and Xanthopoulou, Themis Dimitra and
		  Gj\o{}s\ae{}ter, Terje and M\o{}ller-Pedersen, Birger},
  title		= {On abstraction in the OMG hierarchy: systems, models, and
		  descriptions},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561573},
  doi		= {10.1145/3550356.3561573},
  abstract	= {The Model-Driven Architecture (MDA) uses a metadata
		  hierarchy with several layers that are placed on top of
		  each other. The traditional view is that the layers provide
		  abstractions related to models in languages defined by
		  meta-models. Over the years, it has been difficult to
		  define a consistent understanding of the layers. In this
		  paper, we propose such a consistent understanding by
		  clarifying the relations between the different elements in
		  the hierarchy. This is done based on the Scandinavian
		  approach to modelling that distinguishes between systems
		  and system descriptions. Systems can be physical, digital,
		  or even mental, while descriptions can be programs,
		  language descriptions, specifications, and diagrams. We
		  relate descriptions and systems by explaining where
		  semantics of objects originate and how they apply in the
		  hierarchy.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {322–330},
  numpages	= {9},
  keywords	= {abstraction, description, instantiation, model, semantics,
		  system},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3318236.3318239,
  author	= {Kechagioglou, Xeni and Lemmens, Rob and Retsios,
		  Vasilios},
  title		= {Sharing Geoprocessing Workflows with Business Process
		  Model and Notation (BPMN)},
  year		= {2019},
  isbn		= {9781450362450},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318236.3318239},
  doi		= {10.1145/3318236.3318239},
  abstract	= {Graphical geoprocessing workflows are often built visually
		  on interactive canvases of GIS software. Such workflows
		  cannot be shared among different software, due to
		  structural and semantical differences. This study
		  experiments with a workflow created for ILWIS software and
		  transforms it into a BPMN process model, exploiting XML
		  serialisations of the two workflows. Ultimately, it aims at
		  contributing to interoperability of geoprocessing
		  workflows, through an extended approach serving as a frame
		  around workflow conversion.},
  booktitle	= {Proceedings of the 2019 2nd International Conference on
		  Geoinformatics and Data Analysis},
  pages		= {56–60},
  numpages	= {5},
  keywords	= {Business Process Model and Notation (BPMN),
		  Geoinformatics, Geoprocessing, ILWIS, Interoperability,
		  Workflow, eXtensible Stylesheet Language Transformations
		  (XSLT)},
  location	= {Prague, Czech Republic},
  series	= {ICGDA '19}
}

@InProceedings{	  10.1145/3511808.3557179,
  author	= {Barret, Nelly and Manolescu, Ioana and Upadhyay, Prajna},
  title		= {Abstra: Toward Generic Abstractions for Data of Any
		  Model},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557179},
  doi		= {10.1145/3511808.3557179},
  abstract	= {Digital data sharing leads to unprecedented opportunities
		  to develop data-driven systems for supporting economic
		  activities, the social and political life, and science.
		  Many open-access datasets are RDF (Linked Data) graphs, but
		  others are JSON or XML documents, CSV files, Neo4J property
		  graphs, etc.Potential users need to understand a dataset in
		  order to decide if it is useful for their goal. While some
		  published datasets come with a schema and/or documentation,
		  this is not always the case.We demonstrate Abstra, a
		  dataset abstraction system, which applies on a large
		  variety of data models. Abstra computes a description meant
		  for humans, and integrates Information Extraction to
		  classify dataset content among a set of categories of
		  interest to the user. Our abstractions are conceptually
		  close to Entity-Relationship diagrams, but our entities can
		  have deeply nested structure.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {4803–4807},
  numpages	= {5},
  keywords	= {E-R schema, data integration, dataset discovery},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3641584.3641803,
  author	= {Peng, Xiangyu and Zhang, Yu and Hu, Wang},
  title		= {Data fusing driven graph embedding model for multi-source
		  heterogeneous knowledge graph},
  year		= {2024},
  isbn		= {9798400707674},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3641584.3641803},
  doi		= {10.1145/3641584.3641803},
  abstract	= {Graph embedding has been suggested as an efficient
		  approach for discovering substantial and valuable knowledge
		  in knowledge graphs. However, due to the vast amount and
		  complexity of knowledge graphs, it is challenging to obtain
		  reliable embeddings, which poses challenges in scalability
		  and property selection. Previous research has given little
		  attention to these challenges. In addressing this issue, a
		  knowledge graph embedding model known as the data fusing
		  driven graph embedding model (DFGE) is proposed, which
		  comprises three modules: node embedding module, attribute
		  embedding module, and merge module. Within DFGE, node
		  attribute information is filtered and combined with graph
		  structure information to calculate embeddings. Furthermore,
		  multi-source heterogeneous data fusion technology (MHDF) is
		  also proposed within this model to capture attribute
		  features, embed and fuse nodes' attributes, and enhance
		  DFGE's effectiveness. Extensive experiments are conducted
		  on both Cora and a new proposed Buildings Seismic Damage
		  dataset. Results confirm that DFGE delivers significant
		  performance improvements over state-of-the-art models.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Artificial Intelligence and Pattern Recognition},
  pages		= {1452–1457},
  numpages	= {6},
  keywords	= {graph embedding, heterogeneous data, knowledge graph},
  location	= {Xiamen, China},
  series	= {AIPR '23}
}

@InProceedings{	  10.1145/3486011.3486523,
  author	= {Jim\'{e}nez-Mac\'{\i}as, Alberto and Mu\~{n}oz-Merino,
		  Pedro J. and Delgado Kloos, Carlos},
  title		= {A model to characterize exercises using probabilistic
		  methods},
  year		= {2021},
  isbn		= {9781450390668},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486011.3486523},
  doi		= {10.1145/3486011.3486523},
  abstract	= {Many studies have been conducted on modeling learners in
		  education using probabilistic methods to infer different
		  indicators. However, little research has been done on
		  modeling content in education by estimating different
		  content characteristics using probabilistic methods. Based
		  on the existing gap in this area, this paper presents a
		  model for exercises using probabilistic methods with
		  interactions carried out by students to obtain content
		  characteristics and their relationship such as the number
		  of attempts, grade, and time spent related with student
		  efficiency. Simulations were performed to train the model
		  and find the different curves that best characterize the
		  exercises based on programmatically generated interactions.
		  This model allows the teacher to redesign the exercises to
		  improve the student’s learning process.},
  booktitle	= {Ninth International Conference on Technological Ecosystems
		  for Enhancing Multiculturality (TEEM'21)},
  pages		= {594–599},
  numpages	= {6},
  keywords	= {content modeling, learning analytics, smart content, smart
		  learning environments},
  location	= {Barcelona, Spain},
  series	= {TEEM'21}
}

@InProceedings{	  10.1145/3412841.3442059,
  author	= {Chakraborty, Jaydeep and Bansal, Srividya K. and Virgili,
		  Luca and Konar, Krishanu and Yaman, Beyza},
  title		= {OntoConnect: unsupervised ontology alignment with
		  recursive neural network},
  year		= {2021},
  isbn		= {9781450381048},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3412841.3442059},
  doi		= {10.1145/3412841.3442059},
  abstract	= {Ontology alignment is performed to combine or integrate
		  multiple knowledge bases at the elemental and structural
		  levels. The current state-of-the-art systems use many
		  different approaches to match semantics, syntax, and
		  terminologies of different ontological entities. However,
		  most of the ontology alignment systems depend on domain
		  knowledge, which makes the alignment process
		  domain-specific. To address this challenge, we aim at
		  developing an ontology alignment approach that is
		  independent of domain knowledge. To achieve this goal, an
		  ontology alignment approach is proposed which exploits an
		  unsupervised learning method using a recursive neural
		  network to align classes between different ontologies. In
		  particular, the proposed approach extracts structural
		  information of the classes in ontology to train the
		  unsupervised model. The proposed approach is tested against
		  a reference gold copy of the Anatomy data set in the
		  Ontology Alignment Evaluation Initiative. Our evaluation
		  results show that the proposed unsupervised neural network
		  approach using the meta information of ontological classes
		  yields satisfactory results with a precision of 95.66\% and
		  F-measure of 80.26\% for a similarity threshold of 0.96
		  with the 100-dimension input vector. Increasing the input
		  vector dimension to 300 results in improved precision of
		  97.71\% and F-measure of 80.38\% with a 0.96 threshold. The
		  significance of the proposed approach is that it can be
		  used for ontology alignment independent of domain expertise
		  and without the need for human intervention.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on Applied
		  Computing},
  pages		= {1874–1882},
  numpages	= {9},
  keywords	= {LSTM, ontology schema alignment, ontology schema matching,
		  recursive neural network, unsupervised learning},
  location	= {Virtual Event, Republic of Korea},
  series	= {SAC '21}
}

@InProceedings{	  10.5555/3470152.3470205,
  author	= {Barcel\'{o}, Pablo and Feier, Cristina and Lutz, Carsten
		  and Pieris, Andreas},
  title		= {When is ontology-mediated querying efficient?},
  year		= {2021},
  publisher	= {IEEE Press},
  abstract	= {In ontology-mediated querying, description logic (DL)
		  ontologies are used to enrich incomplete data with domain
		  knowledge which results in more complete answers to
		  queries. However, the evaluation of ontology-mediated
		  queries (OMQs) over relational databases is computationally
		  hard. This raises the question when OMQ evaluation is
		  efficient, in the sense of being tractable in combined
		  complexity or fixed-parameter tractable. We study this
		  question for a range of ontology-mediated query languages
		  based on several important and widely-used DLs, using
		  unions of conjunctive queries as the actual queries. For
		  the DL ELHI⊥, we provide a characterization of the
		  classes of OMQs that are fixed-parameter tractable. For its
		  fragment ELHdr⊥, which restricts the use of inverse
		  roles, we provide a characterization of the classes of OMQs
		  that are tractable in combined complexity. Both results are
		  in terms of equivalence to OMQs of bounded tree width and
		  rest on a reasonable assumption from parameterized
		  complexity theory. They are similar in spirit to Grohe's
		  seminal characterization of the tractable classes of
		  conjunctive queries over relational databases. We further
		  study the complexity of the meta problem of deciding
		  whether a given OMQ is equivalent to an OMQ of bounded tree
		  width, providing several completeness results that range
		  from NP to 2ExpTime, depending on the DL used. We also
		  consider the DL-Lite family of DLs, including members that,
		  unlike ELHI⊥, admit functional roles.},
  booktitle	= {Proceedings of the 34th Annual ACM/IEEE Symposium on Logic
		  in Computer Science},
  articleno	= {53},
  numpages	= {13},
  location	= {Vancouver, Canada},
  series	= {LICS '19}
}

@Proceedings{	  10.1145/3731120,
  title		= {ICTIR '25: Proceedings of the 2025 International ACM SIGIR
		  Conference on Innovative Concepts and Theories in
		  Information Retrieval (ICTIR)},
  year		= {2025},
  isbn		= {9798400718618},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to ACM ICTIR 2025, the 11th ACM SIGIR / the 15th
		  International Conference on Innovative Concepts and
		  Theories in Information Retrieval. ICTIR went through a
		  number of major developments in 2025 that are briefly
		  described below.A new chapter in the history of ICTIR: Ten
		  years ago, ICTIR 2015 was established under the ACM SIGIR
		  umbrella as the premier forum for presenting and discussing
		  research on theoretical and foundational aspects of
		  Information Retrieval. Given the evolving nature of the
		  Information Retrieval field and the needs of the SIGIR
		  community and the conference, ACM ICTIR, formerly known as
		  the "ACM SIGIR International Conference on the Theory of
		  Information Retrieval", is renamed to ACM SIGIR
		  International Conference on Innovative Concepts and
		  Theories in Information Retrieval.This new branding
		  highlights and clarifies the focus of the conference for
		  the authors, the reviewers, and the organizers. The
		  conference aims to provide a forum for the presentation and
		  discussion of research related to the foundational aspects
		  of Information Retrieval (IR), including, for example, new
		  or improved models of relevance, ranking, representation,
		  information needs, and evaluation. The conference also
		  welcomes interdisciplinary research that connects
		  information retrieval with other research disciplines that
		  are theoretically motivated. We use the definition of
		  foundation used by ICTIR founders as a scientific result
		  that others can build upon and use for their own research.
		  While IR research traditionally relies heavily on rigorous
		  experimentation, ICTIR does not see this as a must.SIGIR
		  Revise-and-Resubmit for ICTIR: For the first time, ICTIR
		  2025 accepts two types of submissions: regular submissions
		  and SIGIR Revise-and-Resubmit (SIGIR-RR) submissions.
		  SIGIRRR submissions are for revised manuscripts of papers
		  that were submitted to but not accepted by the SIGIR 2025
		  conference (only full and short paper tracks). Authors can
		  use this option to address the issues raised in the SIGIR
		  2025 reviews and revise the paper accordingly.Special
		  Theme: Even though ICTIR 2025 welcomes papers on all areas
		  of Information Retrieval as detailed above, its program
		  especially welcomes research papers related to the
		  following theme: "LLMs + IR, what could possibly go wrong?"
		  This theme stands on the long-term relationship between
		  large language models and information retrieval. In order
		  to foster a lively discussion on this theme, we seek
		  innovative and foundational technical research papers and
		  discussion papers that methodologically studies this theme
		  by providing impactful perspectives, opinions, or positions
		  on the matter.},
  location	= {Padua, Italy}
}

@InProceedings{	  10.1145/3570236.3570256,
  author	= {Zeli, Cen},
  title		= {The Establishment and Research of an Evaluation Model
		  Based on Network Analytic Hierarchy Process},
  year		= {2023},
  isbn		= {9781450396714},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570236.3570256},
  doi		= {10.1145/3570236.3570256},
  abstract	= {AHP is a decision-making method that decomposes the
		  elements that are always related to decision-making into
		  the levels of goals, criteria, and plans, and conducts
		  qualitative and quantitative analysis on this basis. By
		  analyzing a series of factors that affect the target,
		  comparing their relative importance, and finally selecting
		  the scheme with the highest score is the optimal scheme.
		  This paper makes use of this characteristic of its
		  analysis, establishes a set of evaluation model system, and
		  applies the evaluation model to Wuhan Business School.
		  Finally, the model is scientific and reasonable, which
		  provides a reference for other researches.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Intelligent Information Processing},
  articleno	= {19},
  numpages	= {7},
  keywords	= {AHP, evaluation model, evaluation system, model analysis},
  location	= {Bucharest, Romania},
  series	= {ICIIP '22}
}

@InProceedings{	  10.1145/3719160.3736609,
  author	= {Yang, Boyin and Dudley, John J and Kristensson, Per Ola},
  title		= {Design Activity Simulation: Opportunities and Challenges
		  in Using Multiple Communicative AI Agents to Tackle Design
		  Problems},
  year		= {2025},
  isbn		= {9798400715273},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3719160.3736609},
  doi		= {10.1145/3719160.3736609},
  abstract	= {Large Language Models (LLMs) can enhance structured design
		  thinking, yet existing copilot approaches integrate them
		  into human workflows rather than exploring their autonomous
		  potential. This paper investigates how LLM-based
		  communicative AI agents can independently tackle open-ended
		  design problems and how their strengths and limitations
		  inform human-AI collaboration. We iteratively design a
		  system where AI agents play different roles and simulate
		  human design activity through conversational turns. The
		  agents investigate user needs, identify design constraints,
		  and explore the design space, with useful insights emerging
		  from their interactions. To assess reasoning quality, we
		  conducted a human jury evaluation with five HCI researchers
		  and explored potential applications through a contextual
		  inquiry with seven professionals. Our findings demonstrate
		  that integrating human design thinking techniques enhances
		  AI reasoning. AI agents effectively tackle design problems,
		  generating low-novelty yet well-grounded and practical
		  solutions that meet key design requirements.},
  booktitle	= {Proceedings of the 7th ACM Conference on Conversational
		  User Interfaces},
  articleno	= {46},
  numpages	= {19},
  keywords	= {Generative AI; End-user interaction with LLMs and
		  Multimodal models},
  location	= { },
  series	= {CUI '25}
}

@InProceedings{	  10.1145/3219788.3219797,
  author	= {Lin, Shuo and Han, Jun and Kumar, Kuldeep and Wang,
		  Jiping},
  title		= {Generating Domain Ontology from Chinese Customer Reviews
		  to Analysis Fine-gained Product Quality Risk},
  year		= {2018},
  isbn		= {9781450363938},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3219788.3219797},
  doi		= {10.1145/3219788.3219797},
  abstract	= {With the rapid development of E-commerce in China, quality
		  of the products on online shopping platforms has caused
		  wide concern. Customer reviews, which commented by people
		  who bought the very product, now have been one of the most
		  important resources for analyzing product's quality risk.
		  We can get fine-gained, aspect-oriented risk information of
		  a product by mining its reviews. Unfortunately, people tend
		  to write reviews with casual grammar or just omit parts of
		  components of a sentence. Both these features will cause
		  negative impacts when parsing the raw customer reviews
		  directly. Thus a knowledge base which is built totally
		  beyond the reviews could be used to analyze it despite the
		  drawbacks above. In this paper, we generate a domain
		  ontology from raw text in the online encyclopedia. It can
		  be viewed as a graph whose nodes represent domain concepts
		  and edges represent the relations between these concepts.
		  In our work, we integrate syntactic tree structure in
		  linear-chain CRFs for recognizing domain concepts and train
		  SVMs and MaxEnt models on elaborate features for clarifying
		  three types of relationship, namely "Attribute-of",
		  "Part-of" and "Instance-of". Once the ontology has been
		  built, product properties with potential risk will be
		  extracted by our matching method. Experiment show that our
		  approach achieves 64.4\% precision and 82.4\% recall on
		  risky property extraction task.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Computing and Data Engineering},
  pages		= {73–77},
  numpages	= {5},
  keywords	= {domain ontology, opinion mining, product quality risk},
  location	= {Shanghai, China},
  series	= {ICCDE '18}
}

@InProceedings{	  10.1145/3659677.3659742,
  author	= {Babaalla, Zakaria and Jakimi, Abdeslam and Oualla, Mohamed
		  and Saadane, Rachid and Chehri, Abdellah},
  title		= {Towards an Automatic Extracting UML Class Diagram from
		  System's Textual Specification},
  year		= {2024},
  isbn		= {9798400709296},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3659677.3659742},
  doi		= {10.1145/3659677.3659742},
  abstract	= {Developing a software system from natural language
		  requirements is a complex and delicate task that requires a
		  high level of design and programming expertise. Increasing
		  the level of abstraction used to describe these
		  requirements is the most natural solution. Model-Driven
		  Engineering (MDE) also takes this route, using abstract
		  models as primary entities to generate source code
		  automatically or semi-automatically. Among these models,
		  the UML class diagram occupies a privileged place in
		  object-oriented systems because it not only serves as a
		  basis for communication between developers but also
		  provides a closely aligned static representation of the
		  system implementation. However, creating a UML class
		  diagram from a textual system specification poses a
		  significant challenge due to the inherent imprecision and
		  ambiguity commonly found in natural language expressions.
		  In this paper, we propose a model-centric approach based on
		  deep learning for the automatic extraction of UML class
		  diagrams from textual requirements.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Networking, Intelligent Systems and Security},
  articleno	= {36},
  numpages	= {5},
  keywords	= {Class diagram, Deep learning, Machine learning, Natural
		  Language Processing, UML},
  location	= {Meknes, AA, Morocco},
  series	= {NISS '24}
}

@InProceedings{	  10.1145/3460231.3473901,
  author	= {Olufisayo Dahunsi, Bolanle},
  title		= {An Ontology-based Knowledgebase for User Profile and
		  Garment Features in Apparel Recommender Systems},
  year		= {2021},
  isbn		= {9781450384582},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460231.3473901},
  doi		= {10.1145/3460231.3473901},
  abstract	= {This research proposes the development of an
		  ontology-based knowledgebase for Apparel recommender
		  systems. User and garment attribute definitions and expert
		  style rules will be extracted from expert literature and
		  the most important accuracy-driving user and garment
		  features will be parsed from it. The features will then be
		  used to define the classes and slots of the knowledgebase
		  with constraints defined by the style rules from the
		  experts. This knowledgebase will be made publicly available
		  for modification and reuse in future apparel recommender
		  systems design.},
  booktitle	= {Proceedings of the 15th ACM Conference on Recommender
		  Systems},
  pages		= {851–854},
  numpages	= {4},
  location	= {Amsterdam, Netherlands},
  series	= {RecSys '21}
}

@InProceedings{	  10.1145/3281375.3281386,
  author	= {Rinaldi, Antonio M. and Russo, Cristiano},
  title		= {A semantic-based model to represent multimedia big data},
  year		= {2018},
  isbn		= {9781450356220},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3281375.3281386},
  doi		= {10.1145/3281375.3281386},
  abstract	= {The use of formal representation is a key task in the era
		  of big data. In the context of multimedia big data this
		  issue is stressed due to the intrinsic complexity nature of
		  this kind of data. Moreover, the relations among objects
		  should be clearly expressed and formalized to give the
		  right meaning of data correlation. For this reason the
		  design of formal models to represent and manage information
		  is a necessary task to implement intelligent information
		  systems. In this latter some approaches related to the
		  semantic web could be used to improve the data models which
		  underlie the implementation of big data applications. Using
		  these models the visualization of data and information
		  become an intrinsic and strategic task for the analysis and
		  exploration of multimedia BigData. In this paper we propose
		  the use of a semantic approach to formalize the model
		  structure of multimedia BigData. In addition, the
		  recognition of multimodal features to represent concepts
		  and linguistic properties to relate them are an effective
		  way to bridge the gap between the target semantic classes
		  and the available low-level multimedia descriptors. The
		  proposed model has been implemented in a NoSQL graph
		  database populated from different knowledge sources and a
		  visualization of this very large knowledge base has been
		  presented and discussed as a case study.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Management of Digital EcoSystems},
  pages		= {31–38},
  numpages	= {8},
  keywords	= {multimedia ontologies, semantic bigdata, semantics},
  location	= {Tokyo, Japan},
  series	= {MEDES '18}
}

@InProceedings{	  10.1145/3584371.3613001,
  author	= {Wang, Zifeng and Xiao, Cao and Sun, Jimeng},
  title		= {SPOT: Sequential Predictive Modeling of Clinical Trial
		  Outcome with Meta-Learning},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3613001},
  doi		= {10.1145/3584371.3613001},
  abstract	= {Clinical trials are essential to drug development but
		  time-consuming, costly, and prone to failure. Accurate
		  trial outcome prediction based on historical trial data
		  promises better trial investment decisions and more trial
		  success. Existing trial outcome prediction models were not
		  designed to model the relations among similar trials,
		  capture the progression of features and designs of similar
		  trials, or address the skewness of trial data which causes
		  inferior performance for less common trials.To fill the gap
		  and provide accurate trial outcome prediction, we propose
		  Sequential Predictive mOdeling of clinical Trial outcome
		  (SPOT) that first identifies trial topics to cluster the
		  multisourced trial data into relevant trial topics. It then
		  generates trial embeddings and organizes them by topic and
		  time to create clinical trial sequences. With the
		  consideration of each trial sequence as a task, it uses a
		  meta-learning strategy to achieve a point where the model
		  can rapidly adapt to new tasks with minimal updates. In
		  particular, the topic discovery module enables a deeper
		  understanding of the underlying structure of the data,
		  while sequential learning captures the evolution of trial
		  designs and outcomes. This results in predictions that are
		  not only more accurate but also more interpretable, taking
		  into account the temporal patterns and unique
		  characteristics of each trial topic. We demonstrate that
		  SPOT wins over the prior methods by a significant margin on
		  trial outcome benchmark data: with a 21.5\% lift on phase
		  I, an 8.9\% lift on phase II, and a 5.5\% lift on phase III
		  trials in the metric of the area under precision-recall
		  curve (PR-AUC). Code is available at
		  https://github.com/RyanWangZf/PyTrial.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {53},
  numpages	= {11},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1145/3278681.3278687,
  author	= {Motara, Yusuf Moosa and van der Schyff, Karl},
  title		= {A functional ontology},
  year		= {2018},
  isbn		= {9781450366472},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3278681.3278687},
  doi		= {10.1145/3278681.3278687},
  abstract	= {The ontology of information systems --- the way in which
		  knowledge claims, and thus theories, are conceptualised and
		  represented --- is of particular importance in the
		  information systems field, due to its reliance on relations
		  between entities. This work proposes, demonstrates, and
		  evaluates an alternative ontology for theory description
		  which is arguably more powerful and more expressive than
		  the dominant ontological model.},
  booktitle	= {Proceedings of the Annual Conference of the South African
		  Institute of Computer Scientists and Information
		  Technologists},
  pages		= {49–54},
  numpages	= {6},
  keywords	= {functional design, ontology, theory building, theory of
		  planned behaviour},
  location	= {Port Elizabeth, South Africa},
  series	= {SAICSIT '18}
}

@InProceedings{	  10.1145/3286606.3286825,
  author	= {El Yamami, Abir and Mansouri, Khalifa and Qbadou, Mohammed
		  and Illoussamen, Elhossein and Laaziri, Majida and
		  Benmoussa, Khaoula},
  title		= {An Ontological Representation of PMBOK Framework Knowledge
		  Areas},
  year		= {2018},
  isbn		= {9781450365628},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3286606.3286825},
  doi		= {10.1145/3286606.3286825},
  abstract	= {Considering the problematic of IT Governance frameworks
		  implementation, since it is difficult to apply a common
		  framework to all organizations, this paper intended to
		  address issues related to the adoption of IT project
		  governance practices in organizations. It provides a common
		  representation of its knowledge areas (Scope management,
		  Schedule management, Cost management, Quality management
		  and Risk Management) through ontological approach, relying
		  on PMBOK framework best practice. The goal is to provide a
		  machine-readable document for modeling IT projects
		  governance domain. The results of this paper constitute a
		  considerable contribution for practitioners, by
		  participating in the development of IT governance
		  approaches, to be used by non-specialists PMBOK in
		  organizations, and by limiting as much as possible the
		  bureaucracy of the information systems frameworks.},
  booktitle	= {Proceedings of the 3rd International Conference on Smart
		  City Applications},
  articleno	= {48},
  numpages	= {6},
  keywords	= {IT Governance, IT Project management, Ontology, PMBOK,
		  Prot\'{e}g\'{e}},
  location	= {Tetouan, Morocco},
  series	= {SCA '18}
}

@InProceedings{	  10.1145/3623509.3635326,
  author	= {Rayzhekov, Antoni and Murer, Martin},
  title		= {Between This and That is It: Embodied Semantic Space at
		  the Edge},
  year		= {2024},
  isbn		= {9798400704024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3623509.3635326},
  doi		= {10.1145/3623509.3635326},
  abstract	= {This paper describes the interactive artwork “Between
		  This and That is It”: An AI-augmented typewriter that
		  blends several decades of computerized optimization of text
		  production. The artwork employs an offline processing
		  machine learning-based language model embedded in a typical
		  office typewriter from the 1980s. Deliberately diverting
		  from the pervasive conversational user interface, the
		  interaction style is based on a well-defined minimalist
		  pattern of complementing two user-supplied words with a
		  third word that – in the model – lies in the middle of
		  the other two. This constraint interaction invites to
		  explore the limits of the semantic space of language models
		  and poses questions related to the topology of meaning,
		  with respect to truthfulness, biases, and cliches, by
		  creating a semi-intelligent poetic co-performance involving
		  the audience. This project seeks to foster a discussion on
		  the creative collaboration between humans and AI within the
		  constraints of machine learning technologies embedded into
		  objects from the near past. The experience and the
		  perception of the interaction are shaped by a hybrid space
		  shared between the audience members and the AI, the
		  mechanical limitations of the typewriter, the embedded
		  mini-computer’s computational capacity, and the language
		  model itself.},
  booktitle	= {Proceedings of the Eighteenth International Conference on
		  Tangible, Embedded, and Embodied Interaction},
  articleno	= {101},
  numpages	= {4},
  keywords	= {AI, AI edge computing, Interactive Art, Language models,
		  graspable AI},
  location	= {Cork, Ireland},
  series	= {TEI '24}
}

@InProceedings{	  10.1145/3719160.3736616,
  author	= {Samimi, Reza and Bhattacharya, Aditya and Gosak, Lucija
		  and Stiglic, Gregor and Verbert, Katrien},
  title		= {Visual-Conversational Interface for Evidence-Based
		  Explanation of Diabetes Risk Prediction},
  year		= {2025},
  isbn		= {9798400715273},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3719160.3736616},
  doi		= {10.1145/3719160.3736616},
  abstract	= {Healthcare professionals need effective ways to use,
		  understand, and validate AI-driven clinical decision
		  support systems. Existing systems face two key limitations:
		  complex visualizations and lack of grounding in scientific
		  evidence. We present an integrated Decision Support System
		  that combines interactive visualizations with a
		  conversational agent for explaining diabetes risk
		  assessments. We propose a hybrid prompt handling approach
		  combining fine-tuned language models for analytical queries
		  with general Large Language Models (LLMs) for broader
		  medical questions, a methodology for grounding AI
		  explanations in scientific evidence and a feature range
		  analysis technique to support deeper understanding of
		  feature contributions. We conducted a mixed-methods study
		  with 30 healthcare professionals and found that the
		  conversational interactions helped healthcare professionals
		  build a clear understanding of model assessments, while the
		  integration of scientific evidence calibrated trust in the
		  system’s decisions. Most participants reported that the
		  system supported both patient risk evaluation and
		  recommendation.},
  booktitle	= {Proceedings of the 7th ACM Conference on Conversational
		  User Interfaces},
  articleno	= {52},
  numpages	= {18},
  keywords	= {Clinical Decision Support Systems, Explainable AI,
		  Human-centered AI, Conversational AI},
  location	= { },
  series	= {CUI '25}
}

@Article{	  10.1145/3486250,
  author	= {Guo, Jiafeng and Cai, Yinqiong and Fan, Yixing and Sun,
		  Fei and Zhang, Ruqing and Cheng, Xueqi},
  title		= {Semantic Models for the First-Stage Retrieval: A
		  Comprehensive Review},
  year		= {2022},
  issue_date	= {October 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3486250},
  doi		= {10.1145/3486250},
  abstract	= {Multi-stage ranking pipelines have been a practical
		  solution in modern search systems, where the first-stage
		  retrieval is to return a subset of candidate documents and
		  latter stages attempt to re-rank those candidates. Unlike
		  re-ranking stages going through quick technique shifts over
		  the past decades, the first-stage retrieval has long been
		  dominated by classical term-based models. Unfortunately,
		  these models suffer from the vocabulary mismatch problem,
		  which may block re-ranking stages from relevant documents
		  at the very beginning. Therefore, it has been a long-term
		  desire to build semantic models for the first-stage
		  retrieval that can achieve high recall efficiently.
		  Recently, we have witnessed an explosive growth of research
		  interests on the first-stage semantic retrieval models. We
		  believe it is the right time to survey current status,
		  learn from existing methods, and gain some insights for
		  future development. In this article, we describe the
		  current landscape of the first-stage retrieval models under
		  a unified framework to clarify the connection between
		  classical term-based retrieval methods, early semantic
		  retrieval methods, and neural semantic retrieval methods.
		  Moreover, we identify some open challenges and envision
		  some future directions, with the hope of inspiring more
		  research on these important yet less investigated topics.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= mar,
  articleno	= {66},
  numpages	= {42},
  keywords	= {Semantic retrieval models, information retrieval, survey}
}

@Article{	  10.1109/tcbb.2019.2951137,
  author	= {Liu, Jian and Qu, Zhi and Yang, Mo and Sun, Jialiang and
		  Su, Shuhui and Zhang, Lei},
  title		= {Jointly Integrating VCF-Based Variants and OWL-Based
		  Biomedical Ontologies in MongoDB},
  year		= {2020},
  issue_date	= {Sept.-Oct. 2020},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {17},
  number	= {5},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2019.2951137},
  doi		= {10.1109/TCBB.2019.2951137},
  abstract	= {The development of the next-generation sequencing (NGS)
		  technologies has led to massive amounts of VCF (Variant
		  Call Format) files, which have been the standard formats
		  developed with 1000 Genomes Project. At the same time, with
		  the widespread use of biomedical ontologies in the
		  biomedical community, more and more applications have
		  accepted the Web Ontology Language (OWL) as the dominant
		  data format for the specifications of biomedical ontology
		  descriptions, leading to the rapid growth of OWL-based
		  biomedical ontology scale. In this paper, we seek to
		  explore an effective method for the management of VCF-based
		  genetic variants and OWL-based biological ontologies using
		  the MongoDB database. Considering many current applications
		  (such as the short genetic variations database dbSNP, etc.)
		  are transitioning to the new design by using JSON
		  (JavaScript Object Notation) to support future massive data
		  expansion and interchanges. We firstly propose a series of
		  rules for the mapping from VCF and OWL files to JSON files,
		  and then present rule-based algorithms for transforming
		  VCF-based genetic variants and OWL-based biological
		  ontologies into JSON objects. On this basis, we introduce
		  effective approaches of integrating the mapped JSON files
		  in MongoDB. Finally, we complement this work with a set of
		  experiments to show the performance of our proposed
		  approaches. The source code of the proposed approaches
		  could be freely available at
		  https://github.com/lyotvincent/AJIA.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= oct,
  pages		= {1504–1515},
  numpages	= {12}
}

@InProceedings{	  10.1145/3442442.3451386,
  author	= {Goel, Tushar and Chauhan, Vipul and Verma, Ishan and
		  Dasgupta, Tirthankar and Dey, Lipika},
  title		= {TCS_WITM_2021 @FinSim-2: Transformer based Models for
		  Automatic Classification of Financial Terms},
  year		= {2021},
  isbn		= {9781450383134},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442442.3451386},
  doi		= {10.1145/3442442.3451386},
  abstract	= {Recent advancement in neural network architectures has
		  provided several opportunities to develop systems to
		  automatically extract and represent information from domain
		  specific unstructured text sources. The Finsim-2021 shared
		  task, collocated with the FinNLP workshop, offered the
		  challenge to automatically learn effective and precise
		  semantic models of financial domain concepts. Building such
		  semantic representations of domain concepts requires
		  knowledge about the specific domain. Such a thorough
		  knowledge can be obtained through the contextual
		  information available in raw text documents on those
		  domains. In this paper, we proposed a transformer-based
		  BERT architecture that captures such contextual information
		  from a set of domain specific raw documents and then
		  perform a classification task to segregate domain terms
		  into fixed number of class labels. The proposed model not
		  only considers the contextual BERT embeddings but also
		  incorporates a TF-IDF vectorizer that gives a word level
		  importance to the model. The performance of the model has
		  been evaluated against several baseline architectures.},
  booktitle	= {Companion Proceedings of the Web Conference 2021},
  pages		= {311–315},
  numpages	= {5},
  keywords	= {Automatic Classification of Financial Term, Ontology,
		  TFIDF Vectors, Text Classification, Transformers},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3319008.3319017,
  author	= {Wen, Shao-Fang and Katt, Basel},
  title		= {Preliminary Evaluation of an Ontology-Based Contextualized
		  Learning System for Software Security},
  year		= {2019},
  isbn		= {9781450371452},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3319008.3319017},
  doi		= {10.1145/3319008.3319017},
  abstract	= {Learning software security is a big challenging task in
		  the information technology sector due to the vast amount of
		  security knowledge and the difficulties in understanding
		  the practical applications. The traditional teaching and
		  learning materials, which are usually organized topically
		  and security-centric, have fewer linkages with learners'
		  experience and prior knowledge that they bring to the
		  learning sessions. Learners often do not associate
		  vulnerabilities or coding practices with programs similar
		  to what they were writing in their previous time.
		  Consequently, their motivation for learning is not touched
		  by conventional methods. The aim of this paper is the
		  presentation of an ontology-based learning system for
		  software security with contextualized learning approaches,
		  and of the results of an initial evaluation using a
		  controlled quasi-experiment in a university learning
		  environment. This system facilitates the contextual
		  learning process by providing contextualized access to
		  security knowledge via real software application scenarios,
		  in which learners can explore and relate the security
		  knowledge to the context they are already familiar with.
		  The experiment results show that the prototyped system with
		  the proposed learning approach not only yields significant
		  knowledge gain compared to the conventional learning
		  approach but also gains better learning satisfaction of
		  students.},
  booktitle	= {Proceedings of the 23rd International Conference on
		  Evaluation and Assessment in Software Engineering},
  pages		= {90–99},
  numpages	= {10},
  keywords	= {Software security, context-based knowledge, learning
		  system, ontology},
  location	= {Copenhagen, Denmark},
  series	= {EASE '19}
}

@Article{	  10.1145/3238304,
  author	= {Arenas, Marcelo and Gottlob, Georg and Pieris, Andreas},
  title		= {Expressive Languages for Querying the Semantic Web},
  year		= {2018},
  issue_date	= {September 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {3},
  issn		= {0362-5915},
  url		= {https://doi.org/10.1145/3238304},
  doi		= {10.1145/3238304},
  abstract	= {The problem of querying RDF data is a central issue for
		  the development of the Semantic Web. The query language
		  SPARQL has become the standard language for querying RDF
		  since its W3C standardization in 2008. However, the 2008
		  version of this language missed some important
		  functionalities: reasoning capabilities to deal with RDFS
		  and OWL vocabularies, navigational capabilities to exploit
		  the graph structure of RDF data, and a general form of
		  recursion much needed to express some natural queries. To
		  overcome these limitations, a new version of SPARQL, called
		  SPARQL 1.1, was released in 2013, which includes entailment
		  regimes for RDFS and OWL vocabularies, and a mechanism to
		  express navigation patterns through regular expressions.
		  Unfortunately, there are a number of useful navigation
		  patterns that cannot be expressed in SPARQL 1.1, and the
		  language lacks a general mechanism to express recursive
		  queries. To the best of our knowledge, no efficient RDF
		  query language that combines the above functionalities is
		  known. It is the aim of this work to fill this gap. To this
		  end, we focus on a core fragment of the OWL 2 QL profile of
		  OWL 2 and show that every SPARQL query enriched with the
		  above features can be naturally translated into a query
		  expressed in a language that is based on an extension of
		  Datalog, which allows for value invention and stratified
		  negation. However, the query evaluation problem for this
		  language is highly intractable, which is not surprising
		  since it is expressive enough to encode some inherently
		  hard queries. We identify a natural fragment of it, and we
		  show it to be tractable and powerful enough to define
		  SPARQL queries enhanced with the desired functionalities.},
  journal	= {ACM Trans. Database Syst.},
  month		= nov,
  articleno	= {13},
  numpages	= {45},
  keywords	= {Datalog-based languages, RDF, SPARQL, Semantic web, query
		  answering}
}

@InProceedings{	  10.1145/3286606.3286846,
  author	= {Pahal, Nisha and Mallik, Anupama and Chaudhury, Santanu},
  title		= {An Ontology-based Context-aware IoT Framework for Smart
		  Surveillance},
  year		= {2018},
  isbn		= {9781450365628},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3286606.3286846},
  doi		= {10.1145/3286606.3286846},
  abstract	= {In this paper, we have proposed an ontology-based
		  context-aware framework for providing intelligent services
		  such as smart surveillance, which employ IoT technologies
		  to ensure better quality of life in a smart city. An IoT
		  network such as a smart surveillance system combines the
		  working of Closed-circuit television (CCTV) cameras and
		  various sensors to perform real-time computation for
		  identifying threats and critical situations with the help
		  of valuable context information. This information is
		  perceptual in nature and needs to be converted into
		  higher-level abstractions that can further be used for
		  reasoning to recognize situations. Semantic abstractions
		  for perceptual inputs are possible with the use of a
		  multimedia ontology encoded using Multimedia Web Ontology
		  Language (MOWL) that helps to define concepts, properties
		  and structure of a possible environment. MOWL also allows
		  for a dynamic modeling of real-time situations by employing
		  Dynamic Bayesian networks (DBN), which suits the
		  requirements of a intelligent IoT system. In this paper, we
		  show the application of this framework in a smart
		  surveillance system. Surveillance is enhanced by not only
		  helping to analyze past events, but by predicting dangerous
		  situations for which preventive actions can be taken. In
		  our proposed approach, continuous video stream of data
		  captured by CCTV cameras can be processed on the fly to
		  give real-time alerts to concerned authorities. These
		  alerts can be disseminated using e-mail, text messaging,
		  on-screen alerts and alarms.},
  booktitle	= {Proceedings of the 3rd International Conference on Smart
		  City Applications},
  articleno	= {69},
  numpages	= {7},
  keywords	= {Dynamic Bayesian Network (DBN), Internet of Things (IoT),
		  Multimedia ontology, Smart Surveillance},
  location	= {Tetouan, Morocco},
  series	= {SCA '18}
}

@InProceedings{	  10.1145/3196959.3196963,
  author	= {Barcelo, Pablo and Berger, Gerald and Pieris, Andreas},
  title		= {Containment for Rule-Based Ontology-Mediated Queries},
  year		= {2018},
  isbn		= {9781450347068},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3196959.3196963},
  doi		= {10.1145/3196959.3196963},
  abstract	= {Many efforts have been dedicated to identifying
		  restrictions on ontologies expressed as tuple-generating
		  dependencies (tgds), a.k.a. existential rules, that lead to
		  the decidability of answering ontology-mediated queries
		  (OMQs). This has given rise to three families of
		  formalisms: guarded, non-recursive, and sticky sets of
		  tgds. We study the containment problem for OMQs expressed
		  in such formalisms, which is a key ingredient for solving
		  static analysis tasks associated with them. Our main
		  contribution is the development of specially tailored
		  techniques for OMQ containment under the classes of tgds
		  stated above. This enables us to obtain sharp complexity
		  bounds for the problems at hand.},
  booktitle	= {Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium
		  on Principles of Database Systems},
  pages		= {267–279},
  numpages	= {13},
  keywords	= {computational complexity, conjunctive queries,
		  ontology-mediated queries, query containment,
		  tuple-generating dependencies},
  location	= {Houston, TX, USA},
  series	= {PODS '18}
}

@InProceedings{	  10.1145/3706598.3713083,
  author	= {Krau\ss{}, Veronika and McGill, Mark and Kosch, Thomas and
		  Thiel, Yolanda Maira and Sch\"{o}n, Dominik and
		  Gugenheimer, Jan},
  title		= {"Create a Fear of Missing Out" - ChatGPT Implements
		  Unsolicited Deceptive Designs in Generated Websites Without
		  Warning},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713083},
  doi		= {10.1145/3706598.3713083},
  abstract	= {With the recent advancements in Large Language Models
		  (LLMs), web developers increasingly apply their
		  code-generation capabilities to website design. However,
		  since these models are trained on existing designerly
		  knowledge, they may inadvertently replicate bad or even
		  illegal practices, especially deceptive designs (DD). This
		  paper examines whether users can accidentally create DD for
		  a fictitious webshop using GPT-4. We recruited 20
		  participants, asking them to use ChatGPT to generate
		  functionalities (product overview or checkout) and then
		  modify these using neutral prompts to meet a business goal
		  (e.g., “increase the likelihood of us selling our
		  product”). We found that all 20 generated websites
		  contained at least one DD pattern (mean: 5, max: 9), with
		  GPT-4 providing no warnings. When reflecting on the
		  designs, only 4 participants expressed concerns, while most
		  considered the outcomes satisfactory and not morally
		  problematic, despite the potential ethical and legal
		  implications for end-users and those adopting ChatGPT’s
		  recommendations.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {857},
  numpages	= {20},
  keywords	= {ChatGPT, LLM, Deceptive Design, Dark Patterns, Design
		  Inspiration},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3383219.3383292,
  author	= {Alenezi, Mamdouh and Basit, Hamid Abdul and Khan, Faraz
		  Idris and Beg, Maham Anwar},
  title		= {A Comparison Study of Available Sofware Security
		  Ontologies},
  year		= {2020},
  isbn		= {9781450377317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383219.3383292},
  doi		= {10.1145/3383219.3383292},
  abstract	= {A rising number of software and services malfunctioning
		  due to security flaws has increased the importance of
		  software security and resulted in numerous knowledge
		  sources of the domain. Building secure software systems
		  require the understanding and extraction of the available
		  knowledge, and a standard knowledge management platform is
		  needed. Ontologies form an integral part of knowledge
		  management platforms as they capture and structure the
		  given knowledge. Various software security ontologies have
		  been proposed previously, either stand-alone or as part of
		  some bigger ontology like a computer or information
		  security. However, these ontologies do not cover the entire
		  domain and cannot be used as a standard ontology for
		  software security in its current form. In this paper, we
		  have identified and evaluated the existing ontologies that
		  specifically capture software security knowledge, both
		  qualitatively and quantitatively with the help of ontology
		  evaluation tools, in order to select the best ontology that
		  can be extended to prepare the standard ontology for the
		  software security domain.},
  booktitle	= {Proceedings of the 24th International Conference on
		  Evaluation and Assessment in Software Engineering},
  pages		= {499–504},
  numpages	= {6},
  keywords	= {Security Ontology, Software Quality Assurance, Software
		  Quality Management, Software Security, Software Security
		  Knowledge Management},
  location	= {Trondheim, Norway},
  series	= {EASE '20}
}

@InProceedings{	  10.1145/3193063.3193065,
  author	= {Yeh, Jian-hua},
  title		= {Towards a Biographic Knowledge-based Story Ontology
		  System},
  year		= {2018},
  isbn		= {9781450363785},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3193063.3193065},
  doi		= {10.1145/3193063.3193065},
  abstract	= {In this article, we illustrate some of the semantic
		  web-related technologies and design a set of ontology
		  knowledge structures based on biographical history, using
		  the OWL markup language, which we call BKOnto. This is an
		  official framework for processing biographical
		  history-related messages on the semantic web, including
		  biographical events, time and space relationships, related
		  personal messages, and more. We elaborate on this ontology
		  knowledge architecture and explain how to use BKOnto as a
		  basis for more domain-specific knowledge representation. In
		  BKOnto, we use the OWL language to define the main
		  components of the cognitive structure of the historical
		  body of biography, namely the Storyline of the biography
		  and the historical event of the biography. The so-called
		  biographical story line, which is used to organize the
		  history of multiple biographical superstructure, can be
		  used to describe the biography of a particular person. The
		  so-called biographical historical events, based on the
		  historical data can be based on the description of the
		  content and related space-time factor description of the
		  basic unit. BKOnto's design was based on the StoryLine and
		  Event infrastructure, and then we developed the ontology
		  knowledge building system based on this ontology awareness
		  architecture. Therefore, we also developed a set of
		  ontology knowledge building system based on BKOnto, which
		  is called StoryTeller. The StoryTeller system can be used
		  to construct relevant knowledge of human things in the
		  history of the biography and form a complete biographical
		  story. StoryTeller system, mainly based on the story line
		  organized by the timeline, which contains a number of types
		  and events related to multiple human things as the basic
		  unit to build the story line. The event unit not only
		  describes the description of related human affairs, but
		  also contains the description of time factor and space
		  factor, which is used to construct the space-time
		  information of the unit in the story line. As a result, in
		  a story line with multiple event units, you will be able to
		  present a wealth of information about people and things
		  with their associated spatiotemporal features. In addition,
		  based on the idea of supporting the digital collection
		  system, we also linked up individual event units with the
		  digital collection system of their information sources so
		  that more diverse digital collections could be presented in
		  the future. The empirical study also uses the Mackay
		  Digital Archives Project (http://dlm.csie.au.edu.tw/) as a
		  source of information to demonstrate the ontology knowledge
		  building process of Mackay's biographical stories, as well
		  as related Digital collection of information.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Intelligent Information Technology},
  pages		= {33–38},
  numpages	= {6},
  keywords	= {Biographical knowledge, OWL, ontology, ontology
		  composition system, semantic web, temporal event},
  location	= {Ha Noi, Viet Nam},
  series	= {ICIIT '18}
}

@InProceedings{	  10.1145/3591106.3592258,
  author	= {Deng, Jiaxin and Shen, Dong and Pan, Haojie and Wu,
		  Xiangyu and Liu, Ximan and Meng, Gaofeng and Yang, Fan and
		  Gao, Tingting and Fu, Ruiji and Wang, Zhongyuan},
  title		= {A Unified Model for Video Understanding and Knowledge
		  Embedding with Heterogeneous Knowledge Graph Dataset},
  year		= {2023},
  isbn		= {9798400701788},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3591106.3592258},
  doi		= {10.1145/3591106.3592258},
  abstract	= {Video understanding is an important task in short video
		  business platforms and it has a wide application in video
		  recommendation and classification. Most of the existing
		  video understanding works only focus on the information
		  that appeared within the video content, including the video
		  frames, audio and text. However, introducing common sense
		  knowledge from the external Knowledge Graph (KG) dataset is
		  essential for video understanding when referring to the
		  content which is less relevant to the video. Owing to the
		  lack of video knowledge graph dataset, the work which
		  integrates video understanding and KG is rare. In this
		  paper, we propose a heterogeneous dataset that contains the
		  multi-modal video entity and fruitful common sense
		  relations. This dataset also provides multiple novel video
		  inference tasks like the Video-Relation-Tag (VRT) and
		  Video-Relation-Video (VRV) tasks. Furthermore, based on
		  this dataset, we propose an end-to-end model that jointly
		  optimizes the video understanding objective with knowledge
		  graph embedding, which can not only better inject factual
		  knowledge into video understanding but also generate
		  effective multi-modal entity embedding for KG.
		  Comprehensive experiments indicate that combining video
		  understanding embedding with factual knowledge benefits the
		  content-based video retrieval performance. Moreover, it
		  also helps the model generate better knowledge graph
		  embedding which outperforms traditional KGE-based methods
		  on VRT and VRV tasks with at least 42.36\% and 17.73\%
		  improvement in HITS@10.},
  booktitle	= {Proceedings of the 2023 ACM International Conference on
		  Multimedia Retrieval},
  pages		= {95–104},
  numpages	= {10},
  keywords	= {knowledge graph, multi-modal learning, video inference,
		  video understanding},
  location	= {Thessaloniki, Greece},
  series	= {ICMR '23}
}

@InProceedings{	  10.1145/3443279.3443291,
  author	= {Abri, Sara and Abri, Rayan and Cetin, Salih},
  title		= {A Classification on Different Aspects of User Modelling in
		  Personalized Web Search},
  year		= {2021},
  isbn		= {9781450377607},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3443279.3443291},
  doi		= {10.1145/3443279.3443291},
  abstract	= {In the context of personalization has recently been doing
		  a lot of researches and applications. A common component of
		  all research in the field of personalization is user
		  modeling that also called user profiling. The main work of
		  the user modeling in the field of personalization in the
		  first step is capturing information about users and in the
		  next step is to identify the user's preferences and
		  interests and efficient use this information for increasing
		  the retrieval performance. How to collect information about
		  the user, user model structures and the used techniques to
		  create a user model is different in each of personalized
		  applications. In the previous studies, there was not a
		  complete classification on the major dimensions of user
		  models. In this research, we present an appropriate
		  classification on the major dimensions of user models. We
		  aim to present a survey on applications and techniques of
		  user modeling and make a classification of user modeling by
		  considering the existing literature and research and we
		  hope can help to researchers in better-developing on the
		  area.},
  booktitle	= {Proceedings of the 4th International Conference on Natural
		  Language Processing and Information Retrieval},
  pages		= {194–199},
  numpages	= {6},
  keywords	= {User Profiling, Recommendation Systems, Personalized
		  Search},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '20}
}

@InProceedings{	  10.1145/3652620.3688222,
  author	= {Exelmans, Joeri and Pietron, Jakob and Raschke, Alexander
		  and Vangheluwe, Hans},
  title		= {A Virtual Global Monorepo of Immutable Linked Data},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688222},
  doi		= {10.1145/3652620.3688222},
  abstract	= {The data layer of today's model management solutions often
		  is either centralized or Git-based. We point out a number
		  of limitations of current approaches, such as poor
		  replicability, manually configured access control,
		  centralization, hard-coded 'meta-data', and inflexible
		  encodings. We argue for a set of fundamental features /
		  restrictions (most importantly immutability and
		  capability-based security) for decentralized model
		  management systems to adapt, to solve these problems at
		  their root. We distinguish a fundamental core from
		  non-fundamental applications (such as versioning), that can
		  be built on top.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {1000–1004},
  numpages	= {5},
  keywords	= {model management, capability-based security, versioning},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3462757.3466104,
  author	= {Wehnert, Sabine and Sudhi, Viju and Dureja, Shipra and
		  Kutty, Libin and Shahania, Saijal and De Luca, Ernesto W.},
  title		= {Legal norm retrieval with variations of the bert model
		  combined with TF-IDF vectorization},
  year		= {2021},
  isbn		= {9781450385268},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3462757.3466104},
  doi		= {10.1145/3462757.3466104},
  abstract	= {In this work, we examine variations of the BERT model on
		  the statute law retrieval task of the COLIEE competition.
		  This includes approaches to leverage BERT's contextual word
		  embeddings, fine-tuning the model, combining it with TF-IDF
		  vectorization, adding external knowledge to the statutes
		  and data augmentation. Our ensemble of Sentence-BERT with
		  two different TF-IDF representations and document
		  enrichment exhibits the best performance on this task
		  regarding the F2 score. This is followed by a fine-tuned
		  LEGAL-BERT with TF-IDF and data augmentation and our third
		  approach with the BERTScore. As a result, we show that
		  there are significant differences between the chosen BERT
		  approaches and discuss several design decisions in the
		  context of statute law retrieval.},
  booktitle	= {Proceedings of the Eighteenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {285–294},
  numpages	= {10},
  keywords	= {contextual word embeddings, data augmentation, document
		  enrichment, legal information retrieval},
  location	= {S\~{a}o Paulo, Brazil},
  series	= {ICAIL '21}
}

@InProceedings{	  10.1145/3478676,
  author	= {Hormozdiari, Fereydoun},
  title		= {Session details: Ontologies \&amp; databases},
  year		= {2021},
  isbn		= {9781450384506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3478676},
  doi		= {10.1145/3478676},
  booktitle	= {Proceedings of the 12th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  location	= {Gainesville, Florida},
  series	= {BCB '21}
}

@InProceedings{	  10.1145/3706598.3713502,
  author	= {Choi, Jiin and Lee, Seung Won and Hyun, Kyung Hoon},
  title		= {GenPara: Enhancing the 3D Design Editing Process by
		  Inferring Users' Regions of Interest with Text-Conditional
		  Shape Parameters},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713502},
  doi		= {10.1145/3706598.3713502},
  abstract	= {In 3D design, specifying design objectives and visualizing
		  complex shapes through text alone proves to be a
		  significant challenge. Although advancements in 3D GenAI
		  have significantly enhanced part assembly and the creation
		  of high-quality 3D designs, many systems still to
		  dynamically generate and edit design elements based on the
		  shape parameters. To bridge this gap, we propose GenPara,
		  an interactive 3D design editing system that leverages
		  text-conditional shape parameters of part-aware 3D designs
		  and visualizes design space within the Exploration Map and
		  Design Versioning Tree. Additionally, among the various
		  shape parameters generated by LLM, the system extracts and
		  provides design outcomes within the user’s regions of
		  interest based on Bayesian inference. A user study (N = 16)
		  revealed that GenPara enhanced the comprehension and
		  management of designers with text-conditional shape
		  parameters, streamlining design exploration and
		  concretization. This improvement boosted efficiency and
		  creativity of the 3D design process.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {4},
  numpages	= {21},
  keywords	= {3D Generative AI, Design Space, Large Language Models
		  (LLMs), Bayesian Inference, Human-AI Interaction},
  location	= { },
  series	= {CHI '25}
}

@Article{	  10.1145/3588938,
  author	= {Tu, Jianhong and Fan, Ju and Tang, Nan and Wang, Peng and
		  Li, Guoliang and Du, Xiaoyong and Jia, Xiaofeng and Gao,
		  Song},
  title		= {Unicorn: A Unified Multi-tasking Model for Supporting
		  Matching Tasks in Data Integration},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {1},
  url		= {https://doi.org/10.1145/3588938},
  doi		= {10.1145/3588938},
  abstract	= {Data matching - which decides whether two data elements
		  (e.g., string, tuple, column, or knowledge graph entity)
		  are the "same" (a.k.a. a match) - is a key concept in data
		  integration, such as entity matching and schema matching.
		  The widely used practice is to build task-specific or even
		  dataset-specific solutions, which are hard to generalize
		  and disable the opportunities of knowledge sharing that can
		  be learned from different datasets and multiple tasks. In
		  this paper, we propose Unicorn, a unified model for
		  generally supporting common data matching tasks. Unicorn
		  can enable knowledge sharing by learning from multiple
		  tasks and multiple datasets, and can also support zero-shot
		  prediction for new tasks with zero labeled
		  matching/non-matching pairs. However, building such a
		  unified model is challenging due to heterogeneous formats
		  of input data elements and various matching semantics of
		  multiple tasks. To address the challenges, Unicorn employs
		  one generic Encoder that converts any pair of data elements
		  (a, b) into a learned representation, and uses a Matcher,
		  which is a binary classifier, to decide whether a matches
		  b. To align matching semantics of multiple tasks, Unicorn
		  adopts a mixture-of-experts model that enhances the learned
		  representation into a better representation. We conduct
		  extensive experiments using 20 datasets on seven
		  well-studied data matching tasks, and find that our unified
		  model can achieve better performance on most tasks and on
		  average, compared with the state-of-the-art specific models
		  trained for ad-hoc tasks and datasets separately. Moreover,
		  Unicorn can also well serve new matching tasks with
		  zero-shot learning.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {84},
  numpages	= {26},
  keywords	= {data integration, data matching, multi-task learning}
}

@InProceedings{	  10.1145/3688671.3688738,
  author	= {Papoutsoglou, Maria and Meditskos, Georgios and
		  Bassiliades, Nick and Kontopoulos, Efstratios and
		  Vrochidis, Stefanos},
  title		= {Mapping the Current Status of CTI Knowledge Graphs through
		  a Bibliometric Analysis},
  year		= {2024},
  isbn		= {9798400709821},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3688671.3688738},
  doi		= {10.1145/3688671.3688738},
  abstract	= {Bibliometric analysis in the field of cybersecurity and
		  Cyber Threat Intelligence (CTI) is crucial for identifying
		  research trends, key themes, and collaborative networks,
		  which can guide future research directions and policy
		  decisions. This paper presents a comprehensive bibliometric
		  analysis of the current status of research on knowledge
		  graphs in cybersecurity, highlighting significant trends
		  and thematic clusters. The analysis reveals a rapidly
		  growing interest in integrating knowledge graphs with
		  advanced machine learning and AI techniques, such as deep
		  learning and neural networks, to enhance cyber threat
		  intelligence and response strategies. Key findings include
		  the prominence of natural language processing, entity
		  recognition, and relation extraction as critical
		  methodologies in this field. Thematic evolution analysis
		  shows the adoption of large language models (LLMs) and an
		  ongoing focus on structured knowledge representation. The
		  study underscores the potential of knowledge graphs to
		  improve cybersecurity through better data organization,
		  threat detection, and intelligence extraction.},
  booktitle	= {Proceedings of the 13th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {42},
  numpages	= {6},
  keywords	= {Knowledge Graphs, CTI, Cybersecurity, Bibliometric
		  Analysis},
  location	= { },
  series	= {SETN '24}
}

@InProceedings{	  10.1145/3543882.3543891,
  author	= {Lewis, David and Shibata, Elisabete and Saccomano, Mark
		  and Rosendahl, Lisa and Kepper, Johannes and Hankinson,
		  Andrew and Siegert, Christine and Page, Kevin},
  title		= {A model for annotating musical versions and arrangements
		  across multiple documents and media},
  year		= {2022},
  isbn		= {9781450396684},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543882.3543891},
  doi		= {10.1145/3543882.3543891},
  abstract	= {We present a model for the annotation of musical works,
		  where the annotations are created with respect to a
		  conceptual abstraction of the music instead of directly to
		  concrete encodings. This supports musicologists in
		  constructing arguments about musical elements that occur in
		  multiple digital library sources (or other web resources),
		  that recur across a work, or that appear in different forms
		  in different arrangements. It provides a way of discussing
		  musical content without tying that discourse to the
		  location, notation or medium of the content, allowing
		  evidence from multiple libraries and in different formats
		  to be brought together to support musicological assertions.
		  This model is implemented in Linked Data and illustrated in
		  a prototype application in which musicologists annotate
		  vocal arrangements of the Allegretto from Beethoven’s
		  Seventh Symphony from multiple sources.},
  booktitle	= {Proceedings of the 9th International Conference on Digital
		  Libraries for Musicology},
  pages		= {10–18},
  numpages	= {9},
  keywords	= {FRBR, conceptual modelling, digital musicology, linked
		  data, music arrangements},
  location	= {Prague, Czech Republic},
  series	= {DLfM '22}
}

@InProceedings{	  10.1145/3316615.3316685,
  author	= {Wang, Bo and Luo, Jun and Zhu, Shuyuan},
  title		= {Research on Domain Ontology Automation Construction Based
		  on Chinese Texts},
  year		= {2019},
  isbn		= {9781450365734},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3316615.3316685},
  doi		= {10.1145/3316615.3316685},
  abstract	= {The main construction method of the current ontology is to
		  rely on ontology experts for manual construction. Because
		  manual construction requires a lot of manual participation,
		  manual construction has great limitations. Text data as one
		  of the main forms of data source, how to construct domain
		  ontology automatically from texts and how to provide
		  semantic retrieval support to text quickly by ontology is
		  the hotspot of ontology research at present. Aiming at the
		  above problems, an automatic construction method of domain
		  ontology based on knowledge graph and association rule
		  mining is presented, and it can extract the concepts,
		  hierarchies and non-hierarchies of domain ontology from
		  text, and finally form ontology by Jena. It also provides
		  semantic retrieval of text by associating text and concepts
		  in the process of ontology construction. Finally, the
		  effect of automatic ontology construction is verified by
		  the effect of text retrieval.},
  booktitle	= {Proceedings of the 2019 8th International Conference on
		  Software and Computer Applications},
  pages		= {425–430},
  numpages	= {6},
  keywords	= {Association Rule Mining, Jena, Knowledge Graph, Ontology
		  Construction, RDF},
  location	= {Penang, Malaysia},
  series	= {ICSCA '19}
}

@Article{	  10.1145/3210256,
  author	= {Florence, Spencer P. and Fetscher, Burke and Flatt,
		  Matthew and Temps, William H. and St-Amour, Vincent and
		  Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte
		  and Yarnold, Paul R. and Findler, Robert Bruce and Belknap,
		  Steven M.},
  title		= {POP-PL: A Patient-Oriented Prescription Programming
		  Language},
  year		= {2018},
  issue_date	= {September 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {3},
  issn		= {0164-0925},
  url		= {https://doi.org/10.1145/3210256},
  doi		= {10.1145/3210256},
  abstract	= {A medical prescription is a set of health care
		  instructions that govern the plan of care for an individual
		  patient, which may include orders for drug therapy, diet,
		  clinical assessment, and laboratory testing. Clinicians
		  have long used algorithmic thinking to describe and
		  implement prescriptions but without the benefit of a formal
		  programming language. Instead, medical algorithms are
		  expressed using a natural language patois, flowcharts, or
		  as structured data in an electronic medical record system.
		  The lack of a prescription programming language inhibits
		  expressiveness; results in prescriptions that are difficult
		  to understand, hard to debug, and awkward to reuse; and
		  increases the risk of fatal medical error.This article
		  reports on the design and evaluation of Patient-Oriented
		  Prescription Programming Language (POP-PL), a
		  domain-specific programming language designed for
		  expressing prescriptions. The language is based around the
		  idea that programs and humans have complementary strengths
		  that, when combined properly, can make for safer, more
		  accurate performance of prescriptions. Use of POP-PL
		  facilitates automation of certain low-level vigilance
		  tasks, freeing up human cognition for abstract thinking,
		  compassion, and human communication.We implemented this
		  language and evaluated its design attempting to write
		  prescriptions in the new language and evaluated its
		  usability by assessing whether clinicians can understand
		  and modify prescriptions written in the language. We found
		  that some medical prescriptions can be expressed in a
		  formal domain-specific programming language, and we
		  determined that medical professionals can understand and
		  correctly modify programs written in POP-PL. We also
		  discuss opportunities for refining and further developing
		  POP-PL.},
  journal	= {ACM Trans. Program. Lang. Syst.},
  month		= jul,
  articleno	= {10},
  numpages	= {37},
  keywords	= {DSL design, empirical evaluation, medical prescriptions,
		  medical programming languages}
}

@InProceedings{	  10.1145/3314074.3314091,
  author	= {Bekkali, Mohammed and Lachkar, Abdelmonaime},
  title		= {Arabic Sentiment Analysis based on Topic Modeling},
  year		= {2019},
  isbn		= {9781450361293},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3314074.3314091},
  doi		= {10.1145/3314074.3314091},
  abstract	= {Users of social media generate a huge volume of reviews
		  and comments. These reviews and comments express user's
		  opinions about different topics. As a result, there is a
		  great need to understand and classify these reviews.
		  Sentiment Analysis Systems is a good way to overcome this
		  problem. Reviews are considered as short texts and they are
		  different from traditional documents without enough
		  contextual information. To address this issue, we propose
		  an efficient representation for short text based on
		  concepts instead of terms, which transforms the data
		  representation into a shorter, more compact, and more
		  predictive one. However, for the Arabic language, the
		  majority of semantic resources are incomplete projects;
		  this may presents a serious problem about the coverage
		  ratio of the Arabic language compared with other Languages.
		  To overcome this problem and starting with the assumption
		  that terms belonging to same topic share many semantic
		  links in the same dataset, their corresponding concepts
		  will share the same semantics links in the same dataset. We
		  suggest integrating Topic Modeling as a tool to bring
		  together terms with the same semantic links. The proposed
		  method has been tested and evaluated using the Large Scale
		  Arabic Book Reviews Dataset and the obtained results
		  illustrate the interest and efficiency of our
		  contribution.},
  booktitle	= {Proceedings of the New Challenges in Data Sciences: Acts
		  of the Second Conference of the Moroccan Classification
		  Society},
  articleno	= {17},
  numpages	= {6},
  keywords	= {Arabic Language, Conceptualization, LDA, Sentiment
		  Analysis, Short Text Representation, Topic Modeling},
  location	= {Kenitra, Morocco},
  series	= {SMC '19}
}

@InProceedings{	  10.1145/3305160.3305212,
  author	= {Selvaraj, Suganya and Choi, Eunmi},
  title		= {A Study on Traditional Medicine Ontology},
  year		= {2019},
  isbn		= {9781450366427},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3305160.3305212},
  doi		= {10.1145/3305160.3305212},
  abstract	= {The traditional medical field needs to be studied more for
		  applying compound reasoning using ontology because
		  traditional medicine treats the patients by finding root
		  cause of the symptoms rather than treating for the symptoms
		  directly. Many countries have their own traditional
		  medicines like traditional Chinese medicine, traditional
		  Korean medicine, Siddha, Ayurveda and Unani. Already many
		  projects are started to work on the ontology development
		  for traditional medicines. In this paper, we analyze and
		  summarize the existing medical ontology researches. This
		  study is useful to understand the existing medical ontology
		  system and also provide an idea to develop and enhance the
		  traditional medicine ontology by reusing existing
		  resources. In this study, we also propose the
		  ontology-based medical support system to predict the
		  seasonal diseases by combining medical ontology with Big
		  data analysis.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Software Engineering and Information Management},
  pages		= {235–239},
  numpages	= {5},
  keywords	= {Medical ontology study, Ontology-based medical support
		  system, Traditional medicine ontology},
  location	= {Bali, Indonesia},
  series	= {ICSIM '19}
}

@InProceedings{	  10.1145/3276604.3276610,
  author	= {de Lara, Juan and Guerra, Esther and Kienzle, J\"{o}rg and
		  Hattab, Yanis},
  title		= {Facet-oriented modelling: open objects for model-driven
		  engineering},
  year		= {2018},
  isbn		= {9781450360296},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3276604.3276610},
  doi		= {10.1145/3276604.3276610},
  abstract	= {Model-driven engineering (MDE) promotes models as the
		  principal assets in software projects. Models are built
		  using a modelling language whose syntax is defined by a
		  metamodel. Hence, objects in models are typed by a
		  metamodel class, and this typing relation is static as it
		  is established at creation time and cannot be changed
		  later. This way, objects in MDE are closed and fixed with
		  respect to the type they conform to, the slots/properties
		  they have, and the constraints they should obey. This
		  hampers the reuse of model-related artefacts like model
		  transformations, as well as the opportunistic or dynamic
		  combination of metamodels. To alleviate this rigidity, we
		  propose making model objects open so that they can acquire
		  or drop so-called facets, each one contributing a type,
		  slots and constraints to the object. Facets are defined by
		  regular metamodels, hence being a lightweight extension of
		  standard metamodelling. Facet metamodels may declare usage
		  interfaces, and it is possible to specify laws that govern
		  how facets are to be assigned to the instances of a
		  metamodel. In this paper, we describe our proposal, report
		  on an implementation, and illustrate scenarios where facets
		  have advantages over other techniques.},
  booktitle	= {Proceedings of the 11th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  pages		= {147–159},
  numpages	= {13},
  keywords	= {Flexible Modelling, MetaDepth, Metamodelling, Model-Driven
		  Engineering, Reuse, Role-Based Modelling},
  location	= {Boston, MA, USA},
  series	= {SLE 2018}
}

@InProceedings{	  10.1145/3417990.3421412,
  author	= {K\"{u}hne, Thomas and Lange, Arne},
  title		= {Meaningful metrics for multi-level modelling},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3421412},
  doi		= {10.1145/3417990.3421412},
  abstract	= {One of the key enablers of further growth of multi-level
		  modeling will be the development of objective ways to allow
		  multi-level modeling approaches to be compared to one
		  another and to two-level modeling approaches. While
		  significant strides have been made regarding qualitative
		  comparisons, there is currently no adequate way to
		  quantitatively assess to what extent a multi-level model
		  may be preferable over another model with respect to
		  high-level qualities such as understandability,
		  maintainability, and control capacity. In this paper, we
		  propose deep metrics, as an approach to quantitatively
		  measure high-level model concerns of multi-level models
		  that are of interest to certain stakeholders. Beyond the
		  stated goals, we see deep metrics as furthermore supporting
		  the comparison of modeling styles and aiding modelers in
		  making individual design decisions. We discuss what makes a
		  metric "depth-aware" so that it can appropriately capture
		  multi-level model properties, and present two concrete
		  proposals for metrics that measure high-level multi-level
		  model qualities.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {85},
  numpages	= {9},
  keywords	= {metrics, model comparison, multi-level modeling},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@InProceedings{	  10.1145/3302425.3302482,
  author	= {Sowah, Edmund and Xu, Jianqiu},
  title		= {Edgebase: A Cooperative Query Answering Database System
		  With A Natural Language Interface},
  year		= {2018},
  isbn		= {9781450366250},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3302425.3302482},
  doi		= {10.1145/3302425.3302482},
  abstract	= {Traditional Database Management Systems (DBMS) require
		  users to meticulously construct and submit queries to
		  generate answers. The lack of query syntax flexibility in
		  traditional database systems make results in simple and
		  direct answers - queries retrieve precisely matched
		  elements stated in the given Boolean query. In this paper,
		  we propose a Cooperative Query Answering Database System
		  (CDBS) that provide answers to user queries in the same
		  manner as humans do, and not as machines.The method of
		  "Cooperative Query Answering (CQA)", emanated from the
		  perception that to provide adequate and "complete" answers
		  to queries, recognition of users' intentions is vital. Most
		  database systems require users to submit their queries
		  using SQL syntax. In addition to presenting answers to
		  queries in human-like manner, we present a cooperative
		  approach to query submission. By this, we present an
		  architecture that combines the rich features of html,
		  Natural Language (NL) with Query-By-Form (QBF) method and
		  MySQL to enable our proposed system accept user queries in
		  plain English language.To authenticate our approach and
		  proposed system, a set of thorough experiments were
		  conducted on two database systems using mysqlslap benchmark
		  and a comparative study with other methods is done.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  articleno	= {71},
  numpages	= {8},
  keywords	= {Cooperative Query Answering, Natural Language, Query
		  Language, Query syntax, Query-By-Form},
  location	= {Sanya, China},
  series	= {ACAI '18}
}

@InProceedings{	  10.1145/3377170.3377274,
  author	= {Pattar, Santosh and Sandhya, C. R. and Vala, Darshil and
		  Buyya, Rajkumar and Venugopal, K. R. and Iyenger, S. S. and
		  Patnaik, L. M.},
  title		= {SoCo-ITS: Service Oriented Context Ontology for
		  Intelligent Transport System},
  year		= {2020},
  isbn		= {9781450376631},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3377170.3377274},
  doi		= {10.1145/3377170.3377274},
  abstract	= {Intelligent Transport System (ITS) is a culmination of
		  technological and application systems that are contrived to
		  improve the performance of road transportation and upgrade
		  the commuter's experience. The integration of Internet of
		  Things (IoT) with the transport system has contributed to
		  the development of ITS. In this paper, we concentrate on
		  the commercial servitization standpoint of the application.
		  We structure and formulate an ontology called
		  Service-Oriented Context Ontology for Intelligent Transport
		  System: SoCo-ITS. This ontological framework abets in
		  identifying appropriate services required by the commuters
		  in transit based on their situation, predilection and ITS
		  environmental information. We discuss the detailed
		  implementation description and also accentuate its role in
		  ITS through a use case scenario and an exemplar application
		  portraying the importance of the proposed ontological
		  model.},
  booktitle	= {Proceedings of the 2019 7th International Conference on
		  Information Technology: IoT and Smart City},
  pages		= {503–508},
  numpages	= {6},
  keywords	= {Intelligent Transport Systems, Internet of Things,
		  Ontology, Service Discovery, User Centric Services},
  location	= {Shanghai, China},
  series	= {ICIT '19}
}

@InProceedings{	  10.1145/3194658.3194680,
  author	= {Alfaifi, Yousef and Grasso, Floriana and Tamma,
		  Valentina},
  title		= {An Ontology of Psychological Barriers to Support Behaviour
		  Change},
  year		= {2018},
  isbn		= {9781450364935},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3194658.3194680},
  doi		= {10.1145/3194658.3194680},
  abstract	= {Helping people to adopt and maintain healthier lifestyles
		  is a primary goal of behaviour change interventions.
		  Successful interventions need to account for different
		  barriers (informational, environmental, or psychological)
		  that prevent people from engaging in healthy behaviours.
		  Computational approaches to modelling these interventions
		  focus primarily on informational needs, or on persuasive
		  techniques. The study presented in this paper is
		  specifically aimed at creating a formal conceptual model of
		  the psychological notion of barriers to healthy behaviour,
		  by means of an ontology,i.e. an explicit and machine
		  readable specification of a conceptualisation shared by all
		  the stakeholders~citeStuder-et-al98. The model accounts for
		  other related patient concepts to understand patient
		  behaviour better. This machine-readable knowledge can
		  function as a background to finding the right interventions
		  for behaviour change. Whilst the model is generic and
		  expandable to include other diseases and behaviours, our
		  study uses type 2 diabetes to contextualise the problem of
		  behaviour change.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Digital Health},
  pages		= {11–15},
  numpages	= {5},
  keywords	= {type 2 diabetes, physical activity behaviour, behaviour
		  ontology, behaviour change ontology},
  location	= {Lyon, France},
  series	= {DH '18}
}

@InProceedings{	  10.1145/3622758.3622894,
  author	= {Wilczynski, Peter and Gregoire-Wright, Taylor and Jackson,
		  Daniel},
  title		= {Concept-Centric Software Development: An Experience
		  Report},
  year		= {2023},
  isbn		= {9798400703881},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3622758.3622894},
  doi		= {10.1145/3622758.3622894},
  abstract	= {Developers have long recognized the importance of the
		  concepts underlying the systems they build, and the primary
		  role that concepts play in shaping user experience. To
		  date, however, concepts have tended to be only implicit in
		  software design with development being organized instead
		  around more concrete artifacts (such as wireframes and code
		  modules). Palantir, a software company whose data analytics
		  products are widely used by major corporations, recently
		  reworked the internal representation of its software
		  development process to bring concepts to the fore, making
		  explicit the concepts underlying its products, including
		  how they are clustered together, used in applications, and
		  governed by teams. With a centralized repository of
		  concepts, Palantir engineers are able to align products
		  more closely based on shared concepts, evolve concepts in
		  response to user needs, and communicate more effectively
		  with non-engineering groups within the company. This paper
		  reports on Palantir's experiences to date, analyzing both
		  successes and challenges, and offers advice to other
		  organizations considering adopting a concept-centric
		  approach to software development.},
  booktitle	= {Proceedings of the 2023 ACM SIGPLAN International
		  Symposium on New Ideas, New Paradigms, and Reflections on
		  Programming and Software},
  pages		= {120–135},
  numpages	= {16},
  keywords	= {concepts, ontology, software design},
  location	= {Cascais, Portugal},
  series	= {Onward! 2023}
}

@Article{	  10.1145/3593224,
  author	= {Zen, Mathieu and Burny, Nicolas and Vanderdonckt, Jean},
  title		= {A Quality Model-based Approach for Measuring User
		  Interface Aesthetics with Grace},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {EICS},
  url		= {https://doi.org/10.1145/3593224},
  doi		= {10.1145/3593224},
  abstract	= {User interface aesthetics, a particular sub-characteristic
		  of the ISO 25010 software quality model, is correlated to
		  the perceived or actual usability of a graphical user
		  interface, its user experience, and trust. While many
		  measures, such as balance, symmetry, proportion, alignment,
		  regularity, and simplicity, can be computed, no consensus
		  exists today on which measure to adopt, which formula to
		  compute for each measure, and which interpretation to give
		  for each computed formula. To accommodate these variations
		  and to make the assessment explicit and interpretable, we
		  present a quality model-based approach for measuring
		  aesthetics that defines a quality function in terms of the
		  formula of the measures, their weights, their composition,
		  and their overall computation. This quality model is
		  transformed into a configuration used by GRACE, a web
		  application developed for this purpose. When the measures,
		  their formula, their weights, or the quality function
		  change, the quality model changes, which is re-computed to
		  compare it with any other model, thus making the
		  measurement process explicit and interpretable. We apply
		  this approach to a large dataset "Lab in the Wild'' to
		  investigate the correlations between aesthetic measures and
		  perceived visual complexity and colorfulness. We discuss
		  limitations through threats to validity.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jun,
  articleno	= {172},
  numpages	= {47},
  keywords	= {aesthetics, graphical user interfaces, software
		  measurement, usability engineering, user interface
		  evaluation, visual measures, visual techniques}
}

@InProceedings{	  10.1145/3587259.3627557,
  author	= {Blin, In\`{e}s and Stork, Lise and Spillner, Laura and
		  Santagiustina, Carlo},
  title		= {OKG: A Knowledge Graph for Fine-grained Understanding of
		  Social Media Discourse on Inequality},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627557},
  doi		= {10.1145/3587259.3627557},
  abstract	= {In recent years, social media platforms such as Twitter
		  have allowed people to voice their opinions by engaging in
		  online discussions. The availability of such discussions
		  has garnered interest amongst researchers in analyzing the
		  dynamics on critical topics, such as inequality. Most of
		  the current strategies are, however, limited with respect
		  to conveying the fine-grained opinions of users, focusing
		  on tasks such as sentiment analysis or topic modeling that
		  extract coarse categorizations. In this work, we address
		  this challenge by integrating a Twitter corpus with the
		  output of finer-grained semantic parsing for the analysis
		  of social media discourse. To do so, we first introduce the
		  OBservatory Integrated Ontology (OBIO) that integrates
		  social media metadata with various types of linguistic
		  knowledge. We then present the Observatory Knowledge Graph
		  (OKG), a knowledge graph in terms of the ontology,
		  populated with tweets on inequality. We lastly provide use
		  cases showing how the knowledge graph can be used as the
		  backbone of a social media observatory, to facilitate a
		  deeper understanding of social media discourse.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {166–174},
  numpages	= {9},
  keywords	= {Ontology Engineering and Population, Social Media
		  Discourse},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3696410.3714581,
  author	= {Jradeh, Chlo\'{e} Khadija and Raoufi, Ensiyeh and David,
		  J\'{e}r\^{o}me and Larmande, Pierre and Scharffe,
		  Fran\c{c}ois and Todorov, Konstantin and Trojahn, Cassia},
  title		= {Graph Embeddings Meet Link Keys Discovery for Entity
		  Matching},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696410.3714581},
  doi		= {10.1145/3696410.3714581},
  abstract	= {Entity Matching (EM) automates the discovery of identity
		  links between entities within different Knowledge Graphs
		  (KGs). Link keys are crucial for EM, serving as rules
		  allowing to identify identity links across different KGs,
		  possibly described using different ontologies. However, the
		  approach for extracting link keys struggles to scale on
		  large KGs. While embedding-based EM methods efficiently
		  handle large KGs they lack explainability. This paper
		  proposes a novel hybrid EM approach to guarantee the
		  scalability link key extraction approach and improve the
		  explainability of embedding-based EM methods. First,
		  embedding-based EM approaches are used to sample the KGs
		  based on the identity links they generate, thereby reducing
		  the search space to relevant sub-graphs for link key
		  extraction. Second, rules (in the form of link keys) are
		  extracted to explain the generation of identity links by
		  the embedding-based methods. Experimental results
		  demonstrate that the proposed approach allows link key
		  extraction to scale on large KGs, preserving the quality of
		  the extracted link keys. Additionally, it shows that link
		  keys can improve the explainability of the identity links
		  generated by embedding-methods, allowing for the
		  regeneration of 77\% of the identity links produced for a
		  specific EM task, thereby providing an approximation of the
		  reasons behind their generation.},
  booktitle	= {Proceedings of the ACM on Web Conference 2025},
  pages		= {3344–3353},
  numpages	= {10},
  keywords	= {embedding-based em, entity matching, graph embeddings,
		  hybrid ai, knowledge graphs, language models, link keys,
		  symbolic em},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@Article{	  10.1145/3606703,
  author	= {Murano, Francesca and Quochi, Valeria and Del Grosso,
		  Angelo Mario and Rigobianco, Luca and Zinzi, Mariarosaria},
  title		= {Describing Inscriptions of Ancient Italy. The ItAnt
		  Project and Its Information Encoding Process},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3606703},
  doi		= {10.1145/3606703},
  abstract	= {This article discusses the challenges addressed in the
		  digital scholarly encoding of the fragmentary texts of the
		  languages of Ancient Italy according to the TEI/EpiDoc
		  Guidelines in XML format. It describes the solutions and
		  customisations that have been adopted for dealing with the
		  peculiarities of our epigraphical documentation and with
		  the formalisation of epigraphical information deemed
		  interesting for data retrieval in a historical linguistic
		  perspective. The making of a digital corpus consisting of
		  new critical editions of selected inscriptions is a work
		  carried out in the context of the project “Languages and
		  Cultures of Ancient Italy. Historical Linguistics and
		  Digital Models”, which aims to investigate the languages
		  of Ancient Italy by combining the traditional methods,
		  proper to historical linguistics, with methods and
		  technology proper to the digital humanities and
		  computational lexicography. More specifically, the purpose
		  of the project is to create a set of interrelated digital
		  language resources which comprise: (1) a digital corpus of
		  texts editions; (2) a computational lexicon compliant with
		  the Web Semantic requirements; (3) a relevant bibliographic
		  reference dataset encoded according to the FRBRoo/LRMoo
		  specifications. Additionally, selected textual data and
		  scientific interpretations will be encoded using CIDOC CRM
		  and its extensions, namely CRMtex and CRMinf. The present
		  contribution thus tackles one of the main aspects of the
		  project, and proposes significant innovations in the
		  encoding of critical editions for epigraphic texts of
		  fragmentary languages, which will hopefully foster future
		  interoperability and integration with other external
		  datasets, a paramount concern of the project.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {53},
  numpages	= {14},
  keywords	= {Text encoding, ancient languages, digital epigraphy,
		  TEI/EpiDoc}
}

@InProceedings{	  10.1145/3477314.3507031,
  author	= {Liu, Xinglan and Hussain, Hussain and Razouk, Houssam and
		  Kern, Roman},
  title		= {Effective use of BERT in graph embeddings for sparse
		  knowledge graph completion},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3507031},
  doi		= {10.1145/3477314.3507031},
  abstract	= {Graph embedding methods have emerged as effective
		  solutions for knowledge graph completion. However, such
		  methods are typically tested on benchmark datasets such as
		  Freebase, but show limited performance when applied on
		  sparse knowledge graphs with orders of magnitude lower
		  density. To compensate for the lack of structure in a
		  sparse graph, low dimensional representations of textual
		  information such as word2vec or BERT embeddings have been
		  used. This paper proposes a BERT-based method (BERT-ConvE),
		  to exploit transfer learning of BERT in combination with a
		  convolutional network model ConvE. Comparing to existing
		  text-aware approaches, we effectively make use of the
		  context dependency of BERT embeddings through optimizing
		  the features extraction strategies. Experiments on
		  ConceptNet show that the proposed method outperforms strong
		  baselines by 50\% on knowledge graph completion tasks. The
		  proposed method is suitable for sparse graphs as also
		  demonstrated by empirical studies on ATOMIC and
		  sparsified-FB15k-237 datasets. Its effectiveness and
		  simplicity make it appealing for industrial applications.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {799–802},
  numpages	= {4},
  keywords	= {BERT, context aware embedding, knowledge graph embedding,
		  language model, sparse knowledge graph},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@Article{	  10.1109/tcbb.2021.3083150,
  author	= {Paul, Madhusudan and Anand, Ashish},
  title		= {A New Family of Similarity Measures for Scoring Confidence
		  of Protein Interactions Using Gene Ontology},
  year		= {2021},
  issue_date	= {Jan.-Feb. 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2021.3083150},
  doi		= {10.1109/TCBB.2021.3083150},
  abstract	= {The large-scale protein-protein interaction (PPI) data has
		  the potential to play a significant role in the endeavor of
		  understanding cellular processes. However, the presence of
		  a considerable fraction of false positives is a bottleneck
		  in realizing this potential. There have been continuous
		  efforts to utilize complementary resources for scoring
		  confidence of PPIs in a manner that false positive
		  interactions get a low confidence score. Gene Ontology
		  (GO), a taxonomy of biological terms to represent the
		  properties of gene products and their relations, has been
		  widely used for this purpose. We utilize GO to introduce a
		  new set of specificity measures: Relative Depth Specificity
		  (RDS), Relative Node-based Specificity (RNS), and Relative
		  Edge-based Specificity (RES), leading to a new family of
		  similarity measures. We use these similarity measures to
		  obtain a confidence score for each PPI. We evaluate the new
		  measures using four different benchmarks. We show that all
		  the three measures are quite effective. Notably, RNS and
		  RES more effectively distinguish true PPIs from false
		  positives than the existing alternatives. RES also shows a
		  robust set-discriminating power and can be useful for
		  protein functional clustering as well.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= may,
  pages		= {19–30},
  numpages	= {12}
}

@InProceedings{	  10.1145/3386723.3387874,
  author	= {El-Ansari, Anas and Beni-Hssane, Abderrahim and Saadi,
		  Mostafa},
  title		= {An improved modeling method for profile-based personalized
		  search},
  year		= {2020},
  isbn		= {9781450376341},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3386723.3387874},
  doi		= {10.1145/3386723.3387874},
  abstract	= {A Personalized search system aims to provide tailor-made
		  results for each user's query according to his preferences.
		  Building such a system depends mainly on creating profiles
		  that represent real user interests. In this paper, we
		  present a method for modeling users and creating accurate
		  profiles by implicitly tracking and collecting
		  user-browsing data. Furthermore, we examine techniques to
		  enhance profile accuracy through combining multiple
		  browsing data sources, distinguishing important concepts
		  from irrelevant ones in a user profile, and the concept
		  levels required from a reference ontology to describe
		  user's interests.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Networking, Information Systems \&amp; Security},
  articleno	= {55},
  numpages	= {6},
  keywords	= {Accuracy, Ontology, Personalized Search, User profile},
  location	= {Marrakech, Morocco},
  series	= {NISS '20}
}

@Article{	  10.1145/3557894,
  author	= {Liao, Junwei and Eskimez, Sefik and Lu, Liyang and Shi, Yu
		  and Gong, Ming and Shou, Linjun and Qu, Hong and Zeng,
		  Michael},
  title		= {Improving Readability for Automatic Speech Recognition
		  Transcription},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3557894},
  doi		= {10.1145/3557894},
  abstract	= {Modern Automatic Speech Recognition (ASR) systems can
		  achieve high performance in terms of recognition accuracy.
		  However, a perfectly accurate transcript still can be
		  challenging to read due to grammatical errors, disfluency,
		  and other noises common in spoken communication. These
		  readable issues introduced by speakers and ASR systems will
		  impair the performance of downstream tasks and the
		  understanding of human readers. In this work, we present a
		  task called ASR post-processing for readability (APR) and
		  formulate it as a sequence-to-sequence text generation
		  problem. The APR task aims to transform the noisy ASR
		  output into a readable text for humans and downstream tasks
		  while maintaining the semantic meaning of speakers. We
		  further study the APR task from the benchmark dataset,
		  evaluation metrics, and baseline models: First, to address
		  the lack of task-specific data, we propose a method to
		  construct a dataset for the APR task by using the data
		  collected for grammatical error correction. Second, we
		  utilize metrics adapted or borrowed from similar tasks to
		  evaluate model performance on the APR task. Lastly, we use
		  several typical or adapted pre-trained models as the
		  baseline models for the APR task. Furthermore, we fine-tune
		  the baseline models on the constructed dataset and compare
		  their performance with a traditional pipeline method in
		  terms of proposed evaluation metrics. Experimental results
		  show that all the fine-tuned baseline models perform better
		  than the traditional pipeline method, and our adapted
		  RoBERTa model outperforms the pipeline method by 4.95 and
		  6.63 BLEU points on two test sets, respectively. The human
		  evaluation and case study further reveal the ability of the
		  proposed model to improve the readability of ASR
		  transcripts.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {142},
  numpages	= {23},
  keywords	= {Automatic speech recognition, post-processing for
		  readability, data synthesis, pre-trained model}
}

@InProceedings{	  10.1145/3239372.3239376,
  author	= {Guerra, Esther and de Lara, Juan},
  title		= {On the Quest for Flexible Modelling},
  year		= {2018},
  isbn		= {9781450349499},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3239372.3239376},
  doi		= {10.1145/3239372.3239376},
  abstract	= {Modelling is a fundamental activity in Software
		  Engineering, and central to model-based engineering
		  approaches. It is used for different purposes, and so its
		  nature can range from informal (e.g., as a casual mechanism
		  for problem discussion and understanding) to fully formal
		  (e.g., to enable the automated processing of models by
		  model transformations). However, existing modelling tools
		  only serve one of these two extreme purposes: either to
		  create informal drawings or diagrams, or to build models
		  fully conformant to their modelling language. This lack of
		  reconciliation is hampering the adoption of model-based
		  techniques in practice, as they are deemed too imprecise in
		  the former case, and too rigid in the latter.In this new
		  ideas paper, we claim that modelling tools need further
		  flexibility covering different stages, purposes and
		  approaches to modelling. We detail requirements for such a
		  new generation of modelling tools, describe our first steps
		  towards their realization in the Kite metamodelling tool,
		  and showcase application scenarios.},
  booktitle	= {Proceedings of the 21th ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {23–33},
  numpages	= {11},
  keywords	= {Flexible modelling, Model-driven engineering, Modelling
		  process},
  location	= {Copenhagen, Denmark},
  series	= {MODELS '18}
}

@InProceedings{	  10.1145/3417990.3419503,
  author	= {Reynolds, Owen and Garc\'{\i}a-Dom\'{\i}nguez, Antonio and
		  Bencomo, Nelly},
  title		= {Automated provenance graphs for models@run.time},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3419503},
  doi		= {10.1145/3417990.3419503},
  abstract	= {Software systems are increasingly making decisions
		  autonomously by incorporating AI and machine learning
		  capabilities. These systems are known as self-adaptive and
		  autonomous systems (SAS). Some of these decisions can have
		  a life-changing impact on the people involved and
		  therefore, they need to be appropriately tracked and
		  justified: the system should not be taken as a black box.
		  It is required to be able to have knowledge about past
		  events and records of history of the decision making.
		  However, tracking everything that was going on in the
		  system at the time a decision was made may be unfeasible,
		  due to resource constraints and complexity. In this paper,
		  we propose an approach that combines the abstraction and
		  reasoning support offered by models used at runtime with
		  provenance graphs that capture the key decisions made by a
		  system through its execution. Provenance graphs relate the
		  entities, actors and activities that take place in the
		  system over time, allowing for tracing the reasons why the
		  system reached its current state. We introduce activity
		  scopes, which highlight the high-level activities taking
		  place for each decision, and reduce the cost of
		  instrumenting a system to automatically produce provenance
		  graphs of these decisions. We demonstrate a proof of
		  concept implementation of our proposal across two case
		  studies, and present a roadmap towards a reusable
		  provenance layer based on the experiments.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {53},
  numpages	= {10},
  keywords	= {PROV-DM, autonomous decision-making, provenance, runtime
		  models, self-explanation},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@InProceedings{	  10.1145/3194480.3194490,
  author	= {Gao, Mingxia and Chen, Furong and Wang, Rifeng},
  title		= {Improving Medical Ontology Based on Word Embedding},
  year		= {2018},
  isbn		= {9781450363488},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3194480.3194490},
  doi		= {10.1145/3194480.3194490},
  abstract	= {Medical ontology learning or improving is automatically
		  learning the knowledge in ontology format from medical
		  data, mainly text data. With the rise of the word vector
		  space, improving ontology using word embedding has become a
		  hot spot. Most of previous studies have focused on how to
		  acquire different ontological elements using all kinds of
		  learning technologies. Few studies focus on the prior
		  knowledge in a given ontology. In essence, ontology
		  learning or improving is still a learning process based on
		  existing samples. So, the type and number of knowledge
		  acquired is limited by existing samples in a given
		  ontology. This paper firstly formalizes several kinds of
		  prior knowledge for classes in a given ontology. Then we
		  propose a method, named improving medical ontology based on
		  word embeddings (IMO-WE), to enrich different types of
		  knowledge from medical text according to characteristics of
		  different types of prior knowledge. At last, the paper
		  collects the PubMed Central (PMC) data and the PHARE
		  ontology, and finishes a series of experiments to evaluate
		  the IMO-WE. The experimental results yield the following
		  conclusions. The first one is that the data-rich model can
		  achieve higher accuracy for the IMO-WE under same setting
		  in training progress. So, collecting and training big
		  medical data is a viable way to learn more useful
		  knowledge. The second one is that the IMO-WE can be used to
		  improving ontology knowledge when medical data is
		  sufficiently abundant and the ontology has appropriate
		  prior knowledge. Moreover, in the task of improving
		  synonymous labels through similarity distance, the accuracy
		  of IMO-WE is significantly better than that of the Random
		  indexing method.},
  booktitle	= {Proceedings of the 2018 6th International Conference on
		  Bioinformatics and Computational Biology},
  pages		= {121–127},
  numpages	= {7},
  keywords	= {medical ontology improving, prior knowledge, word
		  embedding},
  location	= {Chengdu, China},
  series	= {ICBCB 2018}
}

@InProceedings{	  10.1145/3230905.3230933,
  author	= {Bouihi, Bouchra and Bahaj, Mohamed},
  title		= {Moodle's Ontology Development from UML for Social Learning
		  Network Analysis},
  year		= {2018},
  isbn		= {9781450353045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3230905.3230933},
  doi		= {10.1145/3230905.3230933},
  abstract	= {The online learning called e-learning is a new learning
		  path that offers to learners to study at their own pace and
		  at the moments that suit them. It is in this perspective
		  that the semantic web has known its emergence in the field
		  of e-learning to offer platforms content more personalized
		  and more adapted to student's and teacher's needs. Since
		  Moodle is the most popular e-learning platform, we propose
		  in this paper to build its OWL ontology by exploring the
		  representative data that we collected from its UML class
		  diagram. The choice of UML class diagram as a basis for
		  data collection for the development of the ontology is
		  justified by the fact that the transition from UML to OWL
		  ontology brings ontology development process closer to the
		  wider software engineering population. The built ontology
		  brings also great benefit in the field of the Social
		  Learning Network Analysis. Because it gives the opportunity
		  to study the behavior of the platform users by giving
		  meaning to their relationships instead of modelling them
		  only as knots and edges.},
  booktitle	= {Proceedings of the International Conference on Learning
		  and Optimization Algorithms: Theory and Applications},
  articleno	= {41},
  numpages	= {6},
  keywords	= {E-learning, Moodle, OWL, Ontology, Semantic web, Social
		  Network Analysis, UML},
  location	= {Rabat, Morocco},
  series	= {LOPAL '18}
}

@Article{	  10.1613/jair.1.12414,
  author	= {De Giacomo, Giuseppe and Oriol, Xavier and Rosati,
		  Riccardo and Savo, Domenico Fabio},
  title		= {Instance-Level Update in DL-Lite Ontologies through
		  First-Order Rewriting},
  year		= {2021},
  issue_date	= {May 2021},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {70},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.12414},
  doi		= {10.1613/jair.1.12414},
  abstract	= {In this paper we study instance-level update in DL-LiteA ,
		  a well-known description logic that influenced the OWL 2 QL
		  standard. Instance-level update regards insertions and
		  deletions in the ABox of an ontology. In particular we
		  focus on formula-based approaches to instance-level update.
		  We show that DL-LiteA , which is well-known for enjoying
		  first-order rewritability of query answering, enjoys a
		  first-order rewritability property also for instance-level
		  update. That is, every update can be reformulated into a
		  set of insertion and deletion instructions computable
		  through a non-recursive Datalog program with negation. Such
		  a program is readily translatable into a first-order query
		  over the ABox considered as a database, and hence into SQL.
		  By exploiting this result, we implement an update component
		  for DL-LiteA-based systems and perform some experiments
		  showing that the approach works in practice.},
  journal	= {J. Artif. Int. Res.},
  month		= may,
  pages		= {1335–1371},
  numpages	= {37}
}

@InProceedings{	  10.1145/3477314.3507164,
  author	= {Corradini, Flavio and Fedeli, Arianna and Fornari,
		  Fabrizio and Polini, Andrea and Re, Barbara},
  title		= {X-IoT: a model-driven approach for cross-platform IoT
		  applications development},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3507164},
  doi		= {10.1145/3477314.3507164},
  abstract	= {Several heterogeneous IoT platforms have been proposed and
		  regularly used by enterprises and academies to support and
		  facilitate IoT software applications development. However,
		  IoT applications strongly depend on the functionalities
		  supported by the specific platform used. This affects the
		  development and portability of the developed applications
		  that may require significant changes, or a complete
		  re-design, for being migrated between platforms. This paper
		  presents X-IoT, an MDE approach for developing
		  cross-platform IoT applications. The approach implements a
		  Domain-Specific Modelling Language (DSML) based on emerging
		  IoT application requirements. A meta-model that
		  incorporates the main IoT platform characteristics has been
		  developed within the ADOxx platform, together with a
		  graphical notation. Through the DSML, it is possible to
		  model a platform-independent model of IoT applications that
		  are refined and deployed on specific IoT platforms.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1448–1451},
  numpages	= {4},
  keywords	= {IoT application development, IoT platform, cross-platform,
		  internet of things, model driven engineering},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@InProceedings{	  10.1145/3438872.3439101,
  author	= {Yang, JinXiong and Bai, Liang and Guo, Yanming},
  title		= {A survey of text classification models},
  year		= {2020},
  isbn		= {9781450388306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3438872.3439101},
  doi		= {10.1145/3438872.3439101},
  abstract	= {With the rapid development of artificial intelligence,
		  text classification method based on deep learning model has
		  surpassed traditional machine learning method in various
		  aspects. This paper introduces dozens of deep learning
		  models for text classification according to the different
		  network structures of the models. In addition, this paper
		  briefly introduces the evaluation indicators and
		  application scenarios of text classification, summarizes
		  and forecasts the current challenges and future development
		  trend of text classification.},
  booktitle	= {Proceedings of the 2020 2nd International Conference on
		  Robotics, Intelligent Control and Artificial Intelligence},
  pages		= {327–334},
  numpages	= {8},
  keywords	= {Deep learning, Model, Text classification},
  location	= {Shanghai, China},
  series	= {RICAI '20}
}

@InProceedings{	  10.1145/3320326.3320369,
  author	= {El Orche, Ahmed and Bahaj, Mohamed},
  title		= {Approach to use ontology based on electronic payment
		  system and machine learning to prevent Fraud},
  year		= {2019},
  isbn		= {9781450366458},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3320326.3320369},
  doi		= {10.1145/3320326.3320369},
  abstract	= {Machine learning is a field of study of artificial
		  intelligence that is based on statistical approaches to
		  give computers the ability to learn from data. An ontology
		  is the structured set of terms and concepts representing
		  the meaning of a field of information, whether by the
		  metadata of a namespace, or the elements of a domain of
		  knowledge. in this paper we propose an approach to combine
		  a machine learning with an ontology based on an electronic
		  payment system and another ontology generated automatically
		  to prevent and fight the risks of fraud.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Networking, Information Systems \&amp; Security},
  articleno	= {37},
  numpages	= {6},
  keywords	= {Electronic payment system, Fraud, Machine learning,
		  Ontology},
  location	= {Rabat, Morocco},
  series	= {NISS '19}
}

@InProceedings{	  10.1145/3368756.3369092,
  author	= {Kaoutar, Lamrani and Abderrahim, Ghadi},
  title		= {Global similarity based ontology to enhance the quality of
		  big and distributed RDF data},
  year		= {2019},
  isbn		= {9781450362894},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368756.3369092},
  doi		= {10.1145/3368756.3369092},
  abstract	= {Nowadays, the web content of e-commerce data is increasing
		  rapidly, which make the traditional techniques to querying
		  this resources not efficient, for that the researches focus
		  to how using the new technologies to provide a relevant and
		  complete answers to user query. Using Technologies of big
		  data and web semantic are two new fields that can be
		  exploiting to processes data semantically and to handle
		  with storage of this hug data.In recent works [1, 2], we
		  have proposed the techniques using in big data and we are
		  proposed in architecture that integrate the big RDF
		  (Resources Description Framework) data semantically by
		  exploiting HDFS (Hadoop Distributed File System) to store
		  Global RDF schema and Map Reduce to process the query, in
		  aims to give an infrastructure who give a complete and
		  pertinent answers to user query. In this paper we are
		  proposed a simple scenario to have a complete and pertinent
		  response to user query.},
  booktitle	= {Proceedings of the 4th International Conference on Smart
		  City Applications},
  articleno	= {105},
  numpages	= {4},
  keywords	= {big data, ontology, semantic web},
  location	= {Casablanca, Morocco},
  series	= {SCA '19}
}

@InProceedings{	  10.1145/3487553.3524675,
  author	= {Turki, Houcemeddine and Hadj Taieb, Mohamed Ali and
		  Piad-Morffis, Alejandro and Ben Aouicha, Mohamed and Bile,
		  Ren\'{e} Fabrice},
  title		= {Data Models for Annotating Biomedical Scholarly
		  Publications: the Case of CORD-19},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524675},
  doi		= {10.1145/3487553.3524675},
  abstract	= {Semantic text annotations have been a key factor for
		  supporting computer applications ranging from knowledge
		  graph construction to biomedical question answering. In
		  this systematic review, we provide an analysis of the data
		  models that have been applied to semantic annotation
		  projects for the scholarly publications available in the
		  CORD-19 dataset, an open database of the full texts of
		  scholarly publications about COVID-19. Based on Google
		  Scholar and the screening of specific research venues, we
		  retrieve seventeen publications on the topic mostly from
		  the United States of America. Subsequently, we outline and
		  explain the inline semantic annotation models currently
		  applied on the full texts of biomedical scholarly
		  publications. Then, we discuss the data models currently
		  used with reference to semantic annotation projects on the
		  CORD-19 dataset to provide interesting directions for the
		  development of semantic annotation models and projects.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {740–750},
  numpages	= {11},
  keywords	= {Annotation models, CORD-19, Named entity annotation,
		  Semantic annotations, Semantic relation annotation,
		  Semantic relations},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3378936.3378976,
  author	= {Selvaraj, Suganya and Choi, Eunmi},
  title		= {TKM Ontology Integration and Visualization},
  year		= {2020},
  isbn		= {9781450376907},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3378936.3378976},
  doi		= {10.1145/3378936.3378976},
  abstract	= {Ontology is the most efficient way of representing
		  knowledge and influence relationship about diseases,
		  symptoms, medications, and diagnosis in the traditional
		  medical field. Since integrating traditional medicine
		  ontologies with modern medical ontologies can benefit to
		  the treatment process for effectively using traditional
		  medicines, there is a need for integration and
		  visualization of traditional medicine ontology.
		  Furthermore, an effective ontology visualization is useful
		  to design, manage, and browse the traditional medicine
		  ontology successfully. In this paper, we construct the
		  traditional Korean medicine (TKM) ontology using TKM domain
		  knowledge with a few imported classes from modern medicine
		  ontology and traditional Chinese medicine ontology (TCM)
		  and also visualize the TKM using Web-based Visualization of
		  Ontologies (WebVowl).},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Software Engineering and Information Management},
  pages		= {146–149},
  numpages	= {4},
  keywords	= {TKM ontology integration, Traditional Korean medicine
		  ontology, medical ontology virtualization},
  location	= {Sydney, NSW, Australia},
  series	= {ICSIM '20}
}

@InProceedings{	  10.1145/3178248.3178253,
  author	= {Kaar, Claudia and Frysak, Josef and Stary, Christian and
		  Kannengiesser, Udo and M\"{u}ller, Harald},
  title		= {Resilient Ontology Support Facilitating Multi-Perspective
		  Process Integration in Industry 4.0},
  year		= {2018},
  isbn		= {9781450353601},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3178248.3178253},
  doi		= {10.1145/3178248.3178253},
  abstract	= {A major challenge for Industry 4.0 organizations is the
		  mutual alignment of automation and information technology
		  while increasing effectiveness and agility of processes.
		  From a technological view, it requires architectures and
		  systems coupling heterogeneous technologies, from an
		  operations perspective, it requires context-sensitive
		  representations. Ontologies do not only support alignment,
		  but also integration and development processes. For the
		  introduced ontology we utilize the multi-perspective
		  RAMI4.0 framework, as it provides several layers and
		  perspectives, including production and business processes.
		  We suggest using Subject-oriented Business Process
		  Management (S-BPM) models to represent executable
		  processes, as they allow encapsulating industry
		  standard-conform as well as stakeholder behavior. Thereby,
		  the ontology backs perspective specific knowledge, and can
		  be adapted as semantic baseline in a flexible way.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Subject-Oriented Business Process Management},
  articleno	= {9},
  numpages	= {10},
  keywords	= {Industry 4.0, Ontology engineering, RAMI4.0, process
		  integration, resilience, semantic interoperability},
  location	= {Linz, Austria},
  series	= {S-BPM One '18}
}

@InProceedings{	  10.1145/3322134.3322143,
  author	= {Demchenko, Yuri and Comminiello, Luca and Reali,
		  Gianluca},
  title		= {Designing Customisable Data Science Curriculum Using
		  Ontology for Data Science Competences and Body of
		  Knowledge},
  year		= {2019},
  isbn		= {9781450361866},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3322134.3322143},
  doi		= {10.1145/3322134.3322143},
  abstract	= {Importance of Data Science education and training is
		  growing with the emergence of data driven technologies and
		  organisational culture that intend to derive actionable
		  value for improving research process or enterprise business
		  using variety of enterprise data and widely available open
		  and social media data. Modern data driven research and
		  industry require new types of specialists that are capable
		  to support all stages of the data lifecycle from data
		  production and input to data processing and actionable
		  results delivery, visualisation and reporting, which can be
		  jointly defined as the Data Science professions family. The
		  education and training of Data Scientists requires
		  multi-disciplinary approach combining wide view of the Data
		  Science and Analytics foundation with deep practical
		  knowledge in domain specific areas. In modern conditions
		  with the fast technology change and strong skills demand,
		  the Data Science education and training should be
		  customizable and delivered in multiple form, also providing
		  sufficient data labs facilities for practical training.
		  This paper discusses approach to building customizable Data
		  Science curriculum for different types of learners based on
		  using the ontology of the EDISON Data Science Framework
		  (EDSF) developed in the EU funded Project EDISON and widely
		  used by universities and professional training
		  organisations.},
  booktitle	= {Proceedings of the 2019 International Conference on Big
		  Data and Education},
  pages		= {124–128},
  numpages	= {5},
  keywords	= {EDISON Data Science Framework (EDSF), Data Scientist
		  Professional, Data Science Ontology, Data Science Model
		  Curriculum, Data Science Competences Framework, Data
		  Science Body of Knowledge, Data Science, Big Data},
  location	= {London, United Kingdom},
  series	= {ICBDE '19}
}

@Article{	  10.1145/3746281,
  author	= {Hang, Tingting and Liu, Shuting and Feng, Jun and Djigal,
		  Hamza and Huang, Jun},
  title		= {Few-Shot Relation Extraction Based on Prompt Learning: A
		  Taxonomy, Survey, Challenges and Future Directions},
  year		= {2025},
  issue_date	= {January 2026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {58},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3746281},
  doi		= {10.1145/3746281},
  abstract	= {Relation extraction (RE) is critical in information
		  extraction (IE) and knowledge graph construction. RE aims
		  to identify the semantic relations between entities from
		  natural language texts. Traditional RE models often rely on
		  many manually annotated training samples, which are limited
		  when data is scarce. Therefore, exploring how to perform RE
		  under few-shot conditions has become a research focus.
		  Recently, prompt learning has attracted attention from
		  researchers due to its ability to fully activate the
		  potential of Pre-trained Language Models (PLMs), especially
		  making significant progress in Few-Shot RE (FSRE). This
		  article comprehensively reviews FSRE based on prompt
		  learning. We first introduce the fundamental concepts of
		  FSRE and prompt learning. Then, we systematically review
		  recent research advances in FSRE with prompt learning,
		  focusing on two perspectives: template construction and
		  model fine-tuning strategies. Next, we summarize the
		  benchmark datasets, evaluation metrics, and experimental
		  results of representative works in FSRE. Afterward, we
		  present practical applications of prompt-based FSRE in
		  specialized domains. Finally, we discuss the critical
		  challenges and future research directions of FSRE tasks
		  based on prompt learning.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {40},
  numpages	= {38},
  keywords	= {Few-shot relation extraction, prompt learning, template
		  construction, model fine-tuning strategies}
}

@InProceedings{	  10.1145/3368691.3372391,
  author	= {Zulfiya, Kaderkeyeva and Gulmira, Bekmanova and Altynbek,
		  Sharipbay and Assel, Omarbekova},
  title		= {A model and a method for assessing students' competencies
		  in e-learning system},
  year		= {2019},
  isbn		= {9781450372848},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368691.3372391},
  doi		= {10.1145/3368691.3372391},
  abstract	= {This article discusses a model and a method for assessing
		  students' competencies in e-learning system, verification
		  of fulfillment of the educational program goals and
		  formation of a graduate with competencies set as a goal at
		  the entrance. It also offers assessment at the level of a
		  discipline, a module and verification of the achievement of
		  the educational program goals.},
  booktitle	= {Proceedings of the Second International Conference on Data
		  Science, E-Learning and Information Systems},
  articleno	= {58},
  numpages	= {5},
  keywords	= {artificial intelligence, competencies, e-learning,
		  evaluation, knowledge, knowledge base, knowledge models,
		  logic, ontology, sets},
  location	= {Dubai, United Arab Emirates},
  series	= {DATA '19}
}

@InProceedings{	  10.1145/3487553.3524701,
  author	= {Cuffy, Clint and French, Evan and Fehrmann, Sophia and
		  McInnes, Bridget T.},
  title		= {Exploring Representations for Singular and Multi-Concept
		  Relations for Biomedical Named Entity Normalization},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524701},
  doi		= {10.1145/3487553.3524701},
  abstract	= {Since the rise of the COVID-19 pandemic, peer-reviewed
		  biomedical repositories have experienced a surge in
		  chemical and disease related queries. These queries have a
		  wide variety of naming conventions and nomenclatures from
		  trademark and generic, to chemical composition mentions.
		  Normalizing or disambiguating these mentions within texts
		  provides researchers and data-curators with more relevant
		  articles returned by their search query. Named entity
		  normalization aims to automate this disambiguation process
		  by linking entity mentions onto their appropriate candidate
		  concepts within a biomedical knowledge base or ontology. We
		  explore several term embedding aggregation techniques in
		  addition to how the term’s context affects evaluation
		  performance. We also evaluate our embedding approaches for
		  normalizing term instances containing one or many relations
		  within unstructured texts.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {823–832},
  numpages	= {10},
  keywords	= {MeSH identifier, concept linking, concept mapping, concept
		  normalization, concept unique identifier, datasets, entity
		  linking, entity normalization, named entity disambiguation,
		  named entity linking, named entity normalization, neural
		  networks, transformer, word embeddings},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Article{	  10.1145/3476106,
  author	= {Zhao, Jiashu and Huang, Jimmy Xiangji and Deng, Hongbo and
		  Chang, Yi and Xia, Long},
  title		= {Are Topics Interesting or Not? An LDA-based Topic-graph
		  Probabilistic Model for Web Search Personalization},
  year		= {2022},
  issue_date	= {July 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {3},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3476106},
  doi		= {10.1145/3476106},
  abstract	= {In this article, we propose a Latent Dirichlet
		  Allocation– (LDA) based topic-graph probabilistic
		  personalization model for Web search. This model represents
		  a user graph in a latent topic graph and simultaneously
		  estimates the probabilities that the user is interested in
		  the topics, as well as the probabilities that the user is
		  not interested in the topics. For a given query issued by
		  the user, the webpages that have higher relevancy to the
		  interested topics are promoted, and the webpages more
		  relevant to the non-interesting topics are penalized. In
		  particular, we simulate a user’s search intent by
		  building two profiles: A positive user profile for the
		  probabilities of the user is interested in the topics and a
		  corresponding negative user profile for the probabilities
		  of being not interested in the the topics. The profiles are
		  estimated based on the user’s search logs. A clicked
		  webpage is assumed to include interesting topics. A skipped
		  (viewed but not clicked) webpage is assumed to cover some
		  non-interesting topics to the user. Such estimations are
		  performed in the latent topic space generated by LDA.
		  Moreover, a new approach is proposed to estimate the
		  correlation between a given query and the user’s search
		  history so as to determine how much personalization should
		  be considered for the query. We compare our proposed models
		  with several strong baselines including state-of-the-art
		  personalization approaches. Experiments conducted on a
		  large-scale real user search log collection illustrate the
		  effectiveness of the proposed models.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= dec,
  articleno	= {51},
  numpages	= {24},
  keywords	= {Personalization, probabilistic model, Web search, Latent
		  Dirichlet Allocation (LDA), topic-graph}
}

@InProceedings{	  10.1145/3534678.3539385,
  author	= {Yang, Puhai and Huang, Heyan and Wei, Wei and Mao,
		  Xian-Ling},
  title		= {Toward Real-life Dialogue State Tracking Involving
		  Negative Feedback Utterances},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539385},
  doi		= {10.1145/3534678.3539385},
  abstract	= {Recently, the research of dialogue systems has been widely
		  concerned, especially task-oriented dialogue systems, which
		  have received increased attention due to their wide
		  application prospect. As a core component, dialogue state
		  tracking (DST) plays a key role in task-oriented dialogue
		  systems, and its function is to parse natural language
		  dialogues into dialogue state formed by slot-value pairs.
		  It is well known that dialogue state tracking has been well
		  studied and explored on current benchmark datasets such as
		  the MultiWOZ. However, almost all current research
		  completely ignores the user negative feedback utterances
		  that exist in real-life conversations when a system error
		  occurs, which often contains user-provided corrective
		  information for the system error. Obviously, user negative
		  feedback utterances can be used to correct the inevitable
		  errors in automatic speech recognition and model
		  generalization. Thus, in this paper, we will explore the
		  role of negative feedback utterances in dialogue state
		  tracking in detail through simulated negative feedback
		  utterances. Specifically, due to the lack of dataset
		  involving negative feedback utterances, first, we have to
		  define the schema of user negative feedback utterances and
		  propose a joint modeling method for feedback utterance
		  generation and filtering. Then, we explore three aspects of
		  interaction mechanism that should be considered in
		  real-life conversations involving negative feedback
		  utterances and propose evaluation metrics related to
		  negative feedback utterances. Finally, on WOZ2.0 and
		  MultiWOZ2.1 datasets, by constructing simulated negative
		  feedback utterances in training and testing, we not only
		  verify the important role of negative feedback utterances
		  in dialogue state tracking, but also analyze the advantages
		  and disadvantages of different interaction mechanisms
		  involving negative feedback utterances, lighting future
		  research on negative feedback utterances.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2222–2232},
  numpages	= {11},
  keywords	= {dialogue state tracking, negative feedback, real-life
		  dialogue},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3535756.3535770,
  author	= {Xiao, Yao and Zhan, Zehui and Yuan, Man},
  title		= {Event Graph Construction Based on Disciplinary Procedural
		  Knowledge: Concept Model and Application},
  year		= {2022},
  isbn		= {9781450396974},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3535756.3535770},
  doi		= {10.1145/3535756.3535770},
  abstract	= {Abstract: At present, procedural knowledge is mainly
		  represented by production rules. Although this
		  representation is concise, problems exist on lack of
		  semantic elements and single semantic relationship, which
		  is not conducive to the representation of procedural
		  knowledge and hinders the teaching and application of
		  procedural knowledge. Therefore, this paper firstly
		  analyzes the semantic elements that procedural knowledge
		  should have, and the lack of time and space elements in
		  traditional representation methods. Combining with the
		  cognitive background of procedural knowledge and
		  introducing event cognition, the representation method of
		  procedural knowledge graph and the construction process
		  model are obtained. The verification proves that the event
		  graph is a procedural knowledge representation method with
		  complete semantic roles and rich semantic relations.},
  booktitle	= {Proceedings of the 8th International Conference on
		  Education and Training Technologies},
  pages		= {85–91},
  numpages	= {7},
  keywords	= {Event Graph, Knowledge Model, Knowledge Representation,
		  Procedural knowledge},
  location	= {Macau, China},
  series	= {ICETT '22}
}

@InBook{	  10.1145/3677389.3702611,
  author	= {Yang, Can},
  title		= {Knowledge Graph-Enhanced Artwork Image Captioning},
  year		= {2025},
  isbn		= {9798400710933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677389.3702611},
  abstract	= {This paper explores the unique challenges of artwork image
		  captioning, a task that demands deep understanding of
		  historical, cultural, and stylistic elements often absent
		  in traditional image captioning. We conducted preliminary
		  experiments using a Meshed Memory Transformer on the
		  Iconclass AI Test Set, which revealed significant
		  improvements in standard metrics but highlighted critical
		  limitations in current datasets and evaluation methods. To
		  address these issues, we propose a novel approach
		  integrating knowledge graphs with large language models.
		  This approach involves creating a specialized art ontology
		  and knowledge graph, and developing new evaluation metrics
		  specifically designed for artwork captioning. While not yet
		  implemented, this proposed method aims to generate more
		  comprehensive, contextually rich, and accurate captions for
		  artwork images. Our research lays the groundwork for future
		  advancements in artwork image captioning, potentially
		  enhancing the accessibility and educational value of
		  digital art collections.},
  booktitle	= {Proceedings of the 24th ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {113},
  numpages	= {3}
}

@InProceedings{	  10.1145/3500931.3501011,
  author	= {Guo, Chaohui and Lin, Shaofu and Huang, Zhisheng and Yao,
		  Yahong},
  title		= {Mental Health Question and Answering System Based on Bert
		  Model and Knowledge Graph Technology},
  year		= {2021},
  isbn		= {9781450395588},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3500931.3501011},
  doi		= {10.1145/3500931.3501011},
  abstract	= {With the development and progress of society, people are
		  facing increasing pressure. The emergence of this
		  phenomenon has led to a rapid increase in the incidence of
		  mental illness. In order to deal with this phenomenon, this
		  paper proposes a system of question and answering on the
		  basic knowledge of mental health (MHQ&amp;A) by using deep
		  learning retrieval technology and knowledge graph
		  technology. The system MHQ&amp;A is designed mainly for the
		  general public, to answer the basic knowledge of mental
		  health, especially the field of depression. First of all,
		  the basic and the professional question and answer data
		  about mental health were respectively obtained by the
		  reptilian bot from the "IASK" website knowledge and the
		  "Dr. Dingxiang" website. Then, the questions and answers
		  obtained through the crawler are made into a Question and
		  Answering Knowledge Graph of Basic Health Knowledge in the
		  mental health field, which is combined with semantic data
		  of antidepressants and the semantic data of depression
		  papers. Finally, a set of template matching rules is
		  designed to determine the type of problem of users. If the
		  questions are about the professional knowledge of medicine
		  or thesis, the reasoning template will be used to reason
		  and search the answer in the "Question and Answering
		  Knowledge Graph of Basic Health Knowledge in the Mental
		  Health Field". If the questions are about other basic
		  knowledge in the field of mental health, the BERT model is
		  used to vectorize the questions of users, and the matching
		  questions and corresponding answers in the MHQ&amp;A are
		  found through cosine similarity calculation. Through the
		  test of system accuracy, it is proved that the system can
		  effectively combine deep learning technology and
		  knowledge.},
  booktitle	= {Proceedings of the 2nd International Symposium on
		  Artificial Intelligence for Medicine Sciences},
  pages		= {472–476},
  numpages	= {5},
  keywords	= {Deep learning, Knowledge Graph, Mental illness, Question
		  and answering system},
  location	= {Beijing, China},
  series	= {ISAIMS '21}
}

@InProceedings{	  10.1109/models-c.2019.00101,
  author	= {H\"{o}ser, Moritz},
  title		= {Modeling adaptive learning agents for domain knowledge
		  transfer},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00101},
  doi		= {10.1109/MODELS-C.2019.00101},
  abstract	= {The implementation of intelligent agents in industrial
		  applications is often prevented by the high cost of
		  adopting such a system to a particular problem domain. This
		  paper states the thesis that when learning agents are
		  applied to work environments that require domain-specific
		  experience, the agent benefits if it can be further adapted
		  by a supervising domain expert. Closely interacting with
		  the agent, a domain expert should be able to understand its
		  decisions and update the underlying knowledge base as
		  needed.The result would be an agent with individualized
		  knowledge that comes in part from the domain experts. The
		  model of such an adaptive learning agent must take into
		  account the problem domain, the design of the learning
		  agent and the perception of the domain user. Therefore,
		  already in the modeling phase, more attention must be paid
		  to make the learning element of the agent adaptable by an
		  operator. Domain modeling and meta-modeling methods could
		  help to make inner processes of the agent more accessible.
		  In addition, the knowledge gained should be made reusable
		  for future agents in similar environments.To begin with,
		  the existing methods for modeling agent systems and the
		  underlying concepts will be evaluated, based on the
		  requirements for different industrial scenarios. The
		  methods are then compiled into a framework that allows for
		  the description and modeling of such systems in terms of
		  adaptability to a problem domain. Where necessary, new
		  methods or tools will be introduced to close the gap
		  between inconsistent modeling artifacts.The framework shall
		  then be used to build learning agents for real-life
		  scenarios and observe their application in a case study.
		  The results will be used to assess the quality of the
		  adapted knowledge base and compare it to a manual knowledge
		  modeling process.},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {660–665},
  numpages	= {6},
  keywords	= {adaptive learning agents, domain modeling, knowledge
		  engineering, multi-modeling},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.1145/3340531.3412151,
  author	= {Bahrani, Mohammad and Roelleke, Thomas},
  title		= {FDCM: Towards Balanced and Generalizable Concept-based
		  Models for Effective Medical Ranking},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412151},
  doi		= {10.1145/3340531.3412151},
  abstract	= {Concept-based IR is expected to improve the quality of
		  medical ranking since it captures more semantics than BOW
		  representations. However, bringing concepts and BOW
		  together into a transparent IR framework is challenging. We
		  propose a new aggregation parameter to combine conceptual
		  and term-based Dirichlet Compound Model scores effectively.
		  The determination of this linear parameter is the result of
		  exploring to what degree the difference of the conceptual
		  and term-based sum of IDFs is influential to the
		  integration. Instead of employing heuristics to find
		  combined models, this paper aims to build the grounds for
		  establishing reasonable aggregation standards based on
		  semantic query performance predictors.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {1957–1960},
  numpages	= {4},
  keywords	= {concept-based ir, dirichlet compound language modelling,
		  query formulation, semantic ir},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@Proceedings{	  10.1145/3450569,
  title		= {SACMAT '21: Proceedings of the 26th ACM Symposium on
		  Access Control Models and Technologies},
  year		= {2021},
  isbn		= {9781450383653},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the ACM
		  Symposium on Access Control Models and Technologies (SACMAT
		  2021). This year's symposium continues its tradition of
		  being the premier forum for the presentation of research
		  results and experience reports on leading-edge issues of
		  access control, including models, systems, applications,
		  and theory, while also embracing a renovated focus on the
		  general area of security.The aim of the symposium is to
		  share novel access control and security solutions that
		  fulfill the needs of heterogeneous applications and
		  environments, and to identify new directions for future
		  research and development.SACMAT provides researchers and
		  practitioners with a unique opportunity to share their
		  perspectives with others interested in the various aspects
		  of access control and security.},
  location	= {Virtual Event, Spain}
}

@InProceedings{	  10.1145/3277593.3277619,
  author	= {Belkaroui, Rami and Bertaux, Aur\'{e}lie and Labbani,
		  Ouassila and Hugol-Gential, Cl\'{e}mentine and Nicolle,
		  Christophe},
  title		= {Towards events ontology based on data sensors network for
		  viticulture domain},
  year		= {2018},
  isbn		= {9781450365642},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3277593.3277619},
  doi		= {10.1145/3277593.3277619},
  abstract	= {Wine Cloud project is the first "Big Data" platform on the
		  french viticulture value chain. The aim of this platform is
		  to provide a complete traceability of the life cycle of the
		  wine, from the wine-grower to the consumer. In particular,
		  Wine Cloud may qualify as an agricultural decision platform
		  that will be used for vine life cycle management in order
		  to predict the occurrence of major risks (vine diseases,
		  grape vine pests, physiological risks, fermentation
		  stoppage, oxidation of vine, etc...). Also to make wine
		  production more rational by offering winegrower a set of
		  recommendation regarding their strategy's of production
		  development.The proposed platform "Wine Cloud" is based on
		  heterogeneous sensors network (agricultural machines, plant
		  sensors and measuring stations) deployed throughout a
		  vineyard. These sensors allow for capturing data from the
		  agricultural process and remote monitoring vineyards in the
		  Internet of Things (IoT) era. However, the sensors data
		  from different source is hard to work together for lack of
		  semantic. Therefore, the task of coherently combining
		  heterogeneous sensors data becomes very challenging. The
		  integration of heterogeneous data from sensors can be
		  achieved by data mining algorithms able to build
		  correlations. Nevertheless, the meaning and the value of
		  these correlations is difficult to perceive without
		  highlighting the meaning of the data and the semantic
		  description of the measured environment.In order to bridge
		  this gap and build causality relationships form
		  heterogeneous sensor data, we propose an ontology-based
		  approach, that consists in exploring heterogeneous sensor
		  data (light, temperature, atmospheric pressure, etc) in
		  terms of ontologies enriched with semantic meta-data
		  describing the life cycle of the monitored environment.},
  booktitle	= {Proceedings of the 8th International Conference on the
		  Internet of Things},
  articleno	= {44},
  numpages	= {7},
  keywords	= {IoT, big data, event ontology, ontologies, semantic sensor
		  data, smart viticulture},
  location	= {Santa Barbara, California, USA},
  series	= {IOT '18}
}

@InProceedings{	  10.1145/3230905.3230929,
  author	= {El Hajjamy, Oussama and Alaoui, Larbi and Bahaj, Mohamed},
  title		= {Semantic integration of heterogeneous classical data
		  sources in ontological data warehouse},
  year		= {2018},
  isbn		= {9781450353045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3230905.3230929},
  doi		= {10.1145/3230905.3230929},
  abstract	= {The development of semantic web technologies and the
		  expansion of the amount of data managed within companies
		  databases lead to an enormous quantity of heterogeneous,
		  distributed and autonomous data sources. However, this
		  growth of information will give rise to real obstacles if
		  we cannot maintain the pace with these changes and meet the
		  needs of users. To succeed, it is necessary to find a
		  solution for integrating data from traditional information
		  systems into richer systems. In this perspective,
		  Ontologies are a key component in solving the problem of
		  semantic heterogeneity, thus enabling semantic
		  interoperability between different web applications and
		  services. In this paper, we provide and develop a
		  semi-automatic approach for designing an ontological data
		  warehouse from various sources. Our approach is to convert
		  the different classical data sources (UML, XML, RDB) to
		  local ontologies (OWL2), then merge these ontologies into a
		  global ontological model based on syntactic, structural and
		  semantic similarity measurement techniques to identify
		  similar concepts and avoid their redundancy in the merge
		  result. Our study is proven by a developed prototype that
		  demonstrates the efficiency and power of our strategy and
		  validates the theoretical concept.},
  booktitle	= {Proceedings of the International Conference on Learning
		  and Optimization Algorithms: Theory and Applications},
  articleno	= {36},
  numpages	= {8},
  keywords	= {Integrating data, OWL2, RDB, Semantic web, UML, XML, data
		  warehouse, ontologies},
  location	= {Rabat, Morocco},
  series	= {LOPAL '18}
}

@InProceedings{	  10.1145/3652620.3688566,
  author	= {Dagenais, Kyanna and David, Istvan},
  title		= {Driving Requirements Evolution by Engineers' Opinions},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688566},
  doi		= {10.1145/3652620.3688566},
  abstract	= {Requirements are often incomplete or imprecise. Especially
		  when innovation is a pronounced aspect of product
		  development, sufficiently refined requirements can only be
		  obtained by leveraging engineering knowledge gained through
		  the exploration of innovative designs. However, such
		  innovative designs often contradict prevalent requirements
		  and might be deemed incorrect unless requirements evolve.
		  In this paper, we present a method to drive requirements
		  evolution by engineering opinions---early indicators of
		  emergent engineering knowledge. Opinions about the
		  suitability of a new design emerge earlier than hard
		  evidence can be produced, potentially accelerating the
		  evolution of requirements and saving time and costs. In
		  this work, we develop a sound formal framework to inform
		  requirements engineers about the potential need for
		  requirements evolution based on engineering opinions. We
		  formalize engineering opinions in terms of subjective logic
		  and unify them with key concepts of model-driven
		  engineering.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {920–929},
  numpages	= {10},
  keywords	= {collaboration, consistency, design space exploration,
		  model-driven engineering, ontologies, parallel engineering,
		  uncertainty},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3293614.3293627,
  author	= {Barbalho, Ingridy M. P. and Silva, Patricio de A. and
		  Fernandes, Felipe R. dos S. and Neto, Francisco M. M. and
		  Leite, Cicilia R. M.},
  title		= {An Investigation on the use of Ontologies for pattern
		  classification - Study applied to the monitoring of food
		  intake},
  year		= {2018},
  isbn		= {9781450365727},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3293614.3293627},
  doi		= {10.1145/3293614.3293627},
  abstract	= {Several tools are developed with the purpose of solving
		  problems and exposing results similar to human reasoning.
		  For this, various artificial intelligence techniques are
		  being implemented to improve these applications. For the
		  poorly structured and high volume data, the ontology
		  presents itself as a technique capable of structuring this
		  data and exposing representative results. In this way, this
		  work describes the use of an ontology as the data
		  classification technique and pattern recognition. The
		  objective is to develop an ontological structure capable of
		  analyzing and classifying the movements and sound signals
		  of the chewing and swallowing process in solids or liquids.
		  To validate the ontology, the tests were performed in real
		  environments. The results obtained, based on the realized
		  experiments, point to the viability of the use of
		  ontologies for the problem in question.},
  booktitle	= {Proceedings of the Euro American Conference on Telematics
		  and Information Systems},
  articleno	= {4},
  numpages	= {8},
  keywords	= {Artificial Intelligence, Data classification, Food Intake,
		  Ontologies},
  location	= {Fortaleza, Brazil},
  series	= {EATIS '18}
}

@InProceedings{	  10.1145/3467707.3467762,
  author	= {DIB, Ahmed Taki Eddine and MAAMRI, Ramdane},
  title		= {Bigraphical Modelling and Design of Multi-Agent Systems},
  year		= {2021},
  isbn		= {9781450389501},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3467707.3467762},
  doi		= {10.1145/3467707.3467762},
  abstract	= {Multi-agent systems are recognized as a major area of
		  distributed artificial intelligence. In fact, MAS have
		  found multiple applications, including the design and
		  development of complex, hierarchical and critical systems.
		  However, ensuring the accuracy of complex interactions and
		  the correct execution of activities of a MAS is becoming a
		  tedious task. In this work, we focus on the formal
		  specification of interaction, holonic and sociotechnical
		  concepts to the BRS-MAS model. The proposed approach, is
		  based on Bigraphical reactive systems. Bigraphs, provide
		  means to specify at same time locality and connectivity of
		  different type of system ranging from soft systems to cyber
		  physical systems. In addition, to its intuitive graphical
		  representation, it provides algebraic definition. This,
		  makes the resulted specifications more precise. Further, it
		  enables the verification of the specified system at the
		  design time (before the implementation) using verification
		  tools.},
  booktitle	= {Proceedings of the 2021 7th International Conference on
		  Computing and Artificial Intelligence},
  pages		= {365–371},
  numpages	= {7},
  keywords	= {Algebraic language theory, Computing methodologies, Formal
		  specification, Holonic, Multi-agent system, Theory of
		  computation},
  location	= {Tianjin, China},
  series	= {ICCAI '21}
}

@InProceedings{	  10.1145/3497623.3497652,
  author	= {Zhang, Congcong and Zhao, Haifeng and Cao, Mingwei},
  title		= {Research on General Text Classification Model Integrating
		  Character-Level Attention and Multi-Scale Features},
  year		= {2022},
  isbn		= {9781450390439},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3497623.3497652},
  doi		= {10.1145/3497623.3497652},
  abstract	= {To solve the problem of poor interpretability of the model
		  caused by the random initialization of convolution kernel
		  in convolution neural network,and the problem of local and
		  single feature extraction scale, a general character-level
		  classification model is designed by referring to the method
		  in CV. Firstly,a multi-scale feature extraction module is
		  added to the network embedding layer to extract rich
		  context information, and the problem of matrix sparsity is
		  solved to some extent by setting different void rates.
		  Then, the network depth is controlled by the number of
		  blocks. Different depths have different grasps of global
		  information and can adapt to tasks with different
		  complexity. Next, add a modified attention mechanism module
		  after the block to enhance the attention of the model to
		  key parts. Finally, the full connection layer of the
		  network is replaced by the full convolution layer to
		  simplify the model. The block is a compressed version of
		  deep separable convolution, and the overall model
		  parameters are reduced to about one-tenth of the original,
		  but the accuracy and performance are improved. The results
		  show that the model is very effective.},
  booktitle	= {Proceedings of the 2021 10th International Conference on
		  Computing and Pattern Recognition},
  pages		= {183–187},
  numpages	= {5},
  keywords	= {attention mechanism, full convolution,compression,
		  multi-scale features},
  location	= {Shanghai, China},
  series	= {ICCPR '21}
}

@InProceedings{	  10.1145/3167132.3167344,
  author	= {Halilaj, Lavdim and Grangel-Gonz\'{a}lez, Irl\'{a}n and
		  Lohmann, Steffen and Vidal, Maria-Esther and Auer, S\"{o}ren},
  title		= {EffTE: a dependency-aware approach for test-driven
		  ontology development},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167344},
  doi		= {10.1145/3167132.3167344},
  abstract	= {The development of domain-specific ontologies requires
		  joint efforts among different groups of stakeholders, such
		  as ontology engineers and domain experts. By following a
		  test-driven development technique, a set of test cases
		  ensures that ontology changes do not violate predefined
		  requirements. However, since the number of test cases can
		  be large and their evaluation time may be high, the
		  ontology development process can be negatively impacted. We
		  propose EffTE, an approach for efficient test-driven
		  ontology development relying on a graph-based model of
		  dependencies between test cases. It enables prioritization
		  and selection of test cases to be evaluated. Traversing the
		  dependency graph is realized using breadth-first search
		  along with a mechanism that tracks tabu test cases, i.e.,
		  test cases to be ignored for further evaluation due to
		  faulty parent test cases. As a result, the number of
		  evaluated test cases is minimized, thus reducing the time
		  required for validating the ontology after each
		  modification. We conducted an empirical evaluation to
		  determine the efficiency of our approach. The evaluation
		  results suggest that our approach is more efficient than an
		  exhaustive evaluation of the test cases; in particular with
		  an increasing ontology size and number of test cases.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {1976–1983},
  numpages	= {8},
  keywords	= {dependency graph, ontology engineering, test cases,
		  test-driven ontology development},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InProceedings{	  10.1145/3585088.3589356,
  author	= {Tseng, Tiffany and King Chen, Jennifer and Abdelrahman,
		  Mona and Kery, Mary Beth and Hohman, Fred and Hilliard,
		  Adriana and Shapiro, R. Benjamin},
  title		= {Collaborative Machine Learning Model Building with
		  Families Using Co-ML},
  year		= {2023},
  isbn		= {9798400701313},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3585088.3589356},
  doi		= {10.1145/3585088.3589356},
  abstract	= {Existing novice-friendly machine learning (ML) modeling
		  tools center around a solo user experience, where a single
		  user collects only their own data to build a model.
		  However, solo modeling experiences limit valuable
		  opportunities for encountering alternative ideas and
		  approaches that can arise when learners work together;
		  consequently, it often precludes encountering critical
		  issues in ML around data representation and diversity that
		  can surface when different perspectives are manifested in a
		  group-constructed data set. To address this issue, we
		  created Co-ML – a tablet-based app for learners to
		  collaboratively build ML image classifiers through an
		  end-to-end, iterative model-building process. In this
		  paper, we illustrate the feasibility and potential richness
		  of collaborative modeling by presenting an in-depth case
		  study of a family (two children 11 and 14-years-old working
		  with their parents) using Co-ML in a facilitated
		  introductory ML activity at home. We share the Co-ML system
		  design and contribute a discussion of how using Co-ML in a
		  collaborative activity enabled beginners to collectively
		  engage with dataset design considerations underrepresented
		  in prior work such as data diversity, class imbalance, and
		  data quality. We discuss how a distributed collaborative
		  process, in which individuals can take on different
		  model-building responsibilities, provides a rich context
		  for children and adults to learn ML dataset design.},
  booktitle	= {Proceedings of the 22nd Annual ACM Interaction Design and
		  Children Conference},
  pages		= {40–51},
  numpages	= {12},
  keywords	= {children, collaboration, families, learning, machine
		  learning},
  location	= {Chicago, IL, USA},
  series	= {IDC '23}
}

@InProceedings{	  10.1145/3485730.3493445,
  author	= {Maresca, Fabio and Solmaz, G\"{u}rkan and Cirillo, Flavio},
  title		= {OntoAugment: Ontology Matching through Weakly-Supervised
		  Label Augmentation},
  year		= {2021},
  isbn		= {9781450390972},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485730.3493445},
  doi		= {10.1145/3485730.3493445},
  abstract	= {Ontology matching enables harmonizing heterogeneous data
		  models. Existing ontology matching approaches include
		  machine learning. In particular, recent works leverage weak
		  supervision (WS) through programmatic labeling to avoid the
		  intensive hand-labeling for large ontologies. Programmatic
		  labeling relies on heuristics and rules, called Labeling
		  Functions (LFs), that generate noisy and incomplete labels.
		  However, to cover a reasonable portion of the dataset,
		  programmatic labeling might require a significant number of
		  LFs that might be time expensive and not always
		  straightforward to program.This paper proposes a novel
		  system, namely OntoAugment, that augments LF labels for the
		  ontology matching problem, starting from outcomes of the
		  LFs. Our solution leverages the "similarity of
		  similarities" between ontology concept bipairs that are two
		  pairs of concepts. OntoAugment projects a label yielded by
		  an LF for a concept pair to a similar pair that the same LF
		  does not label. Thus, a wider portion of the dataset is
		  covered even with a limited set of LFs. Experimentation
		  results show that OntoAugment provides significant
		  improvements (up to 11 F1 points) compared to the
		  state-of-the-art WS approach when fewer LFs are used,
		  whereas it maintains the performance without creating
		  additional noise when a higher number of LFs already
		  achieves high performance.},
  booktitle	= {Proceedings of the 19th ACM Conference on Embedded
		  Networked Sensor Systems},
  pages		= {420–425},
  numpages	= {6},
  keywords	= {data programming, ontology matching, semantic, weak
		  supervision},
  location	= {Coimbra, Portugal},
  series	= {SenSys '21}
}

@InProceedings{	  10.1145/3567445.3571109,
  author	= {Schleipen, Miriam and Schubert, Viktor and Dzidic, Samir
		  and Penner, Dimitri and Spieckermann, Sven},
  title		= {A modeling approach for integration and contextualization
		  of simulation-based digital services in IIoT},
  year		= {2023},
  isbn		= {9781450396653},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3567445.3571109},
  doi		= {10.1145/3567445.3571109},
  abstract	= {In the context of the Industrial Internet of Things (IIoT)
		  production plants and components are increasingly growing
		  together with information technologies. This is often
		  realized by means of digital twins. They collect data in
		  real time and learn from this data; they control processes
		  automatically or support human decisions; and they
		  communicate and interact through the internet. This is more
		  and more evolving to intercompany interactions based on
		  digital services. In addition to data of isolated assets
		  (e.g. production resources), new capabilities for
		  standard-based data integration and orchestration are
		  necessary to contextualize the interaction of multiple
		  digital twins and services. This paper suggests an approach
		  to use common standards in the industrial context such as
		  AutomationML, FMI, and OPC UA as basis for integration and
		  contextualization of simulation-based digital services on
		  IIoT platforms.},
  booktitle	= {Proceedings of the 12th International Conference on the
		  Internet of Things},
  pages		= {205–210},
  numpages	= {6},
  keywords	= {AutomationML, FMI, capability, simulation, skill, smart
		  service},
  location	= {Delft, Netherlands},
  series	= {IoT '22}
}

@Article{	  10.14778/3717755.3717772,
  author	= {Zhao, Fuheng and Deep, Shaleen and Psallidas, Fotis and
		  Floratou, Avrilia and Agrawal, Divyakant and Abbadi, Amr
		  El},
  title		= {Sphinteract: Resolving Ambiguities in NL2SQL through User
		  Interaction},
  year		= {2025},
  issue_date	= {December 2024},
  publisher	= {VLDB Endowment},
  volume	= {18},
  number	= {4},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3717755.3717772},
  doi		= {10.14778/3717755.3717772},
  abstract	= {Translating natural language questions into SQL queries
		  (NL2SQL) is a challenging task of great practical
		  importance. Prior work has extensively studied how to
		  address NL2SQL using Large Language Models (LLMs) with
		  solutions ranging from careful prompt engineering, to
		  fine-tuning existing LLMs, or even training custom models.
		  However, a remaining challenging problem in NL2SQL is the
		  inherent ambiguity in the natural language questions asked
		  by users. In this paper, we introduce Sphinteract, a
		  framework designed to assist LLMs in generating
		  high-quality SQL answers that accurately reflect the user
		  intent. Our key insight to resolve ambiguity is to take
		  into account minimal user feedback interactively. We
		  introduce the Summarize, Review, Ask (SRA) paradigm, which
		  guides LLMs in identifying ambiguities in NL2SQL tasks and
		  generates targeted questions for the user to answer. We
		  propose three different methods of how to process user
		  feedback and generate SQL queries based on user input. Our
		  experiments on the challenging KaggleDBQA and BIRD
		  benchmarks demonstrate that by means of asking
		  clarification questions to the user, LLMs can efficiently
		  incorporate the feedback, resulting in accuracy
		  improvements of up to 42\%.},
  journal	= {Proc. VLDB Endow.},
  month		= may,
  pages		= {1145–1158},
  numpages	= {14}
}

@InProceedings{	  10.1145/3712716.3712726,
  author	= {Korol, Allan and Sikos, Leslie F.},
  title		= {FEAR: A Novel Framework for Representing Digital Forensic
		  Artifacts in Knowledge Graphs},
  year		= {2025},
  isbn		= {9798400710766},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3712716.3712726},
  doi		= {10.1145/3712716.3712726},
  abstract	= {In digital forensics, knowledge graphs have demonstrated
		  significant potential via software agent automation and
		  knowledge discovery using encoded expert knowledge in, for
		  example, the form of Semantic Web rules. These advancements
		  have been limited in terms of efficiently encoding
		  extracted digital artifacts into graph representation, the
		  associated overhead of implementing frameworks to handle
		  digital forensic evidence, and the lack of sharing of such
		  code that is often a preamble to other research. This paper
		  introduces a digital forensic framework, Forensic
		  Extraction and Representation (FEAR), that enables a
		  simplified process of accessing and encoding extracted
		  digital forensic artifacts in semantic knowledge graphs.
		  The adoption of such a framework can facilitate the sharing
		  of expert knowledge and reduce the burden of development
		  for researchers exploring the application of knowledge
		  graphs, software agents, and automated reasoning in the
		  field of digital forensics, while accelerating the adoption
		  of emerging research by practitioners.},
  booktitle	= {Proceedings of the Digital Forensics Doctoral Symposium},
  articleno	= {9},
  numpages	= {8},
  keywords	= {digital forensic artifacts, semantic knowledge graph,
		  digital forensic software agent, digital forensic
		  framework, declarative language},
  location	= { },
  series	= {DFDS '25}
}

@InProceedings{	  10.1145/3243907.3243915,
  author	= {Atarashi, Ray and Sone, Takuro and Komohara, Yu and
		  Tsukada, Manabu and Kasuya, Takashi and Okumura, Hiraku and
		  Ikeda, Masahiro and Esaki, Hiroshi},
  title		= {The Software Defined Media Ontology for Music Events},
  year		= {2018},
  isbn		= {9781450364959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3243907.3243915},
  doi		= {10.1145/3243907.3243915},
  abstract	= {With the advent of viewing services based on the Internet,
		  the importance of object-based viewing services for
		  interpreting objects existing in space and utilizing them
		  as content is increasing. Since 2014, the Software Defined
		  Media Consortium has been researching object-based media
		  and Internet-based viewing spaces. This paper defineds a
		  framework in event participants and professional recorders
		  each freely share recorded data, and a third party can
		  creates an application based on the data. This study aims
		  to provide an SDM ontology-based contents management
		  mechanism with a detailed description of the object-based
		  audio and video data and the recording environment. The
		  data can be shared via the Internet and is highly reusable.
		  We implemented this management mechanism and have developed
		  and validated applications that are capable of
		  interactively playing 3D content from any viewpoints
		  freely.},
  booktitle	= {Proceedings of the 1st International Workshop on Semantic
		  Applications for Audio and Music},
  pages		= {15–23},
  numpages	= {9},
  keywords	= {Content management, Ontology, RDF},
  location	= {Monterey, CA, USA},
  series	= {SAAM '18}
}

@Article{	  10.1145/3453185,
  author	= {Tan, Minghuan and Jiang, Jing and Dai, Bing Tian},
  title		= {A BERT-Based Two-Stage Model for Chinese Chengyu
		  Recommendation},
  year		= {2021},
  issue_date	= {November 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3453185},
  doi		= {10.1145/3453185},
  abstract	= {In Chinese, Chengyu are fixed phrases consisting of four
		  characters. As a type of idioms, their meanings usually
		  cannot be derived from their component characters. In this
		  article, we study the task of recommending a Chengyu given
		  a textual context. Observing some of the limitations with
		  existing work, we propose a two-stage model, where during
		  the first stage we re-train a Chinese BERT model by masking
		  out Chengyu from a large Chinese corpus with a wide
		  coverage of Chengyu. During the second stage, we fine-tune
		  the re-trained, Chengyu-oriented BERT on a specific Chengyu
		  recommendation dataset. We evaluate this method on ChID and
		  CCT datasets and find that it can achieve the state of the
		  art on both datasets. Ablation studies show that both
		  stages of training are critical for the performance gain.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  articleno	= {92},
  numpages	= {18},
  keywords	= {Question answering, Chengyu recommendation, idiom
		  understanding}
}

@InProceedings{	  10.1145/3696410.3714782,
  author	= {Zhao, Xuejiao and Liu, Siyan and Yang, Su-Yin and Miao,
		  Chunyan},
  title		= {MedRAG: Enhancing Retrieval-augmented Generation with
		  Knowledge Graph-Elicited Reasoning for Healthcare Copilot},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696410.3714782},
  doi		= {10.1145/3696410.3714782},
  abstract	= {Retrieval-augmented generation (RAG) is a well-suited
		  technique for retrieving privacy-sensitive Electronic
		  Health Records (EHR). It can serve as a key module of the
		  healthcare copilot, helping reduce misdiagnosis for
		  healthcare practitioners and patients. However, the
		  diagnostic accuracy and specificity of existing
		  heuristic-based RAG models used in the medical domain are
		  inadequate, particularly for diseases with similar
		  manifestations. This paper proposes MedRAG, a RAG model
		  enhanced by knowledge graph (KG)-elicited reasoning for the
		  medical domain that retrieves diagnosis and treatment
		  recommendations based on manifestations. MedRAG
		  systematically constructs a comprehensive four-tier
		  hierarchical diagnostic KG encompassing critical diagnostic
		  differences of various diseases. These differences are
		  dynamically integrated with similar EHRs retrieved from an
		  EHR database, and reasoned within a large language model.
		  This process enables more accurate and specific decision
		  support, while also proactively providing follow-up
		  questions to enhance personalized medical decision-making.
		  MedRAG is evaluated on both a public dataset DDXPlus and a
		  private chronic pain diagnostic dataset (CPDD) collected
		  from Tan Tock Seng Hospital, and its performance is
		  compared against various existing RAG methods. Experimental
		  results show that, leveraging the information integration
		  and relational abilities of the KG, our MedRAG provides
		  more specific diagnostic insights and outperforms
		  state-of-the-art models in reducing misdiagnosis rates. Our
		  code will be available at
		  https://github.com/SNOWTEAM2023/MedRAG},
  booktitle	= {Proceedings of the ACM on Web Conference 2025},
  pages		= {4442–4457},
  numpages	= {16},
  keywords	= {decision support, healthcare copilot, knowledge graph,
		  large language models, retrieval-augmented generation},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.5555/3463952.3463955,
  author	= {Istrate, Gabriel},
  title		= {Models We Can Trust: Toward a Systematic Discipline of
		  (Agent-Based) Model Interpretation and Validation},
  year		= {2021},
  isbn		= {9781450383073},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {We advocate the development of a discipline of interacting
		  with and extracting information from models, both
		  mathematical (e.g. game-theoretic ones) and computational
		  (e.g. agent-based models). We outline some directions for
		  the development of a such a discipline: - the development
		  of logical frameworks for the systematic formal
		  specification of stylized facts and social mechanisms in
		  (mathematical and computational) social science. Such
		  frameworks would bring to attention new issues, such as
		  phase transitions, i.e. dramatical changes in the validity
		  of the stylized facts beyond some critical values in
		  parameter space. We argue that such statements are useful
		  for those logical frameworks describing properties of ABM.
		  - the adaptation of tools from the theory of reactive
		  systems (such as bisimulation) to obtain practically
		  relevant notions of two systems "having the same behavior".
		  - the systematic development of an adversarial theory of
		  model perturbations, that investigates the robustness of
		  conclusions derived from models of social behavior to
		  variations in several features of the social dynamics.
		  These may include: activation order, the underlying social
		  network, individual agent behavior.},
  booktitle	= {Proceedings of the 20th International Conference on
		  Autonomous Agents and MultiAgent Systems},
  pages		= {6–11},
  numpages	= {6},
  keywords	= {adversarial perturbations, agent-based simulation,
		  bisimulation, game-theoretic models, logical frameworks,
		  robustness},
  location	= {Virtual Event, United Kingdom},
  series	= {AAMAS '21}
}

@InProceedings{	  10.1145/3415958.3433076,
  author	= {Tissaoui, Anis and Sassi, Salma and Chbeir, Richard},
  title		= {LEOnto: New Approach for Ontology Enrichment using LDA},
  year		= {2020},
  isbn		= {9781450381154},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3415958.3433076},
  doi		= {10.1145/3415958.3433076},
  abstract	= {The Latent Dirichlet Allocation (LDA) model [18] was
		  originally developed and utilised for document modeling and
		  topic extraction in Information Retrieval. To design high
		  quality domain ontologies, effective and usable
		  methodologies are needed to facilitate their building
		  process. In this paper, we propose a new approach for
		  semi-automatic ontology enriching from textual corpus based
		  on LDA model. In our approach, LDA is adopted to provide
		  efficient dimension reduction, able to capture semantic
		  relationships between word-topic and topic-document in
		  terms of probability distributions with minimum human
		  intervention. We conducted several experiments with
		  different model parameters and the corresponding behavior
		  of the enriching technique was evaluated by domain experts.
		  We also compared the results of our method with two
		  existing learning methods using the same dataset. The study
		  showed that our method outperforms the other methods in
		  terms of recall and precision measures.},
  booktitle	= {Proceedings of the 12th International Conference on
		  Management of Digital EcoSystems},
  pages		= {132–139},
  numpages	= {8},
  keywords	= {Knowledge acquisition, LDA, Ontology enrichment, Ontology
		  learning, Probabilistic topic models},
  location	= {Virtual Event, United Arab Emirates},
  series	= {MEDES '20}
}

@InProceedings{	  10.1145/3291533.3291571,
  author	= {Voutos, Yorghos and Mylonas, Phivos},
  title		= {A semantic data model for sensory spatio-temporal
		  environmental concepts},
  year		= {2018},
  isbn		= {9781450366106},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3291533.3291571},
  doi		= {10.1145/3291533.3291571},
  abstract	= {Nowadays, the well-known Resource Description Framework
		  (RDF) forms a rather general method for web resources'
		  conceptual description or even for generic information
		  modeling. However, RDF's capabilities are challenged once
		  used to effectively represent non-thematic metadata, e.g,
		  in the form of spatial and temporal objects deriving
		  primarily from sensor information. In addition, Wireless
		  Sensor Network (WSN) is considered today to be a widely
		  adopted platform, related to environmental monitoring and
		  decision making applications. Specifically, exclusive
		  subjects, such as environmental degradation and optimized
		  agriculture, provide a scope of applied research on the
		  basis of multilevel semantic data analysis. Observations
		  and sensors are the core of empirical science and their
		  implementation (i.e., the increasing volume of data,
		  heterogeneity of devices, data formats and measurement
		  procedures) produce a large volume of unsupervised data.
		  Thus, the prevailing growth of sensing systems has
		  currently led to the development of defined
		  interoperability among standards on web semantics. In
		  particular, Semantic Sensor Network (SSN) ontologies
		  prospect on modeling the capabilities and properties of
		  sensors, monitoring procedures and observations.
		  Furthermore, the dynamically evolving natural phenomena
		  require proper conceptualization of environmental change
		  and monitoring agents. Consequently, this paper describes
		  an inaugural attempt to create a conceptual framework of
		  spatially and temporally-enabled environmental variables
		  for sensing systems.},
  booktitle	= {Proceedings of the 22nd Pan-Hellenic Conference on
		  Informatics},
  pages		= {180–185},
  numpages	= {6},
  keywords	= {environmental monitoring, ontology model, semantics,
		  sensors},
  location	= {Athens, Greece},
  series	= {PCI '18}
}

@InProceedings{	  10.1145/3369166.3369174,
  author	= {Cartealy, Imam and Liao, Li},
  title		= {Metabolic Pathway Membership Inference using an
		  Ontology-based Similarity Approach},
  year		= {2020},
  isbn		= {9781450372510},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3369166.3369174},
  doi		= {10.1145/3369166.3369174},
  abstract	= {Determining whether a protein belongs to a metabolic
		  pathway is an important annotation task, can provide
		  context to the basic functional annotation, and aid
		  reconstruction of incomplete pathways. In this work, we
		  develop a method for pathway membership inference based
		  gene ontology (GO) similarity between a query protein and
		  proteins that are known to be members of a given pathway.
		  By comparing with various existing measures of GO term
		  semantic similarity, we develop an effective and efficient
		  way to take into account of both information content of
		  individual GO terms and the whole GO hierarchy. We test the
		  classifier using 10-fold cross validation for all metabolic
		  pathways reported in KEGG database and demonstrate that our
		  method outperforms with statistical significance in
		  comparison to a suite of existing semantic similarity
		  measures, as evaluated using ROC score. And our method
		  outperforms other methods in running time by multiple
		  orders of magnitude for long pathways.},
  booktitle	= {Proceedings of the 2019 8th International Conference on
		  Bioinformatics and Biomedical Science},
  pages		= {97–102},
  numpages	= {6},
  keywords	= {Gene Ontology, Membership Prediction, Metabolic Pathway,
		  Semantic Similarity},
  location	= {Beijing, China},
  series	= {ICBBS '19}
}

@Article{	  10.1145/3677074,
  author	= {Hadan, Hilda and Sgandurra, Sabrina Alicia and
		  Zhang-Kennedy, Leah and Nacke, Lennart E.},
  title		= {From Motivating to Manipulative: The Use of Deceptive
		  Design in a Game's Free-to-Play Transition},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CHI PLAY},
  url		= {https://doi.org/10.1145/3677074},
  doi		= {10.1145/3677074},
  abstract	= {Over the last decade, the free-to-play (F2P) game business
		  model has gained popularity in the games industry. We
		  examine the role of deceptive design during a game's
		  transition to F2P and its impacts on players. Our analysis
		  focuses on game mechanics and a Reddit analysis of the
		  Overwatch (OW) series after it transitioned to an F2P
		  model. Our study identifies nine game mechanics that use
		  deceptive design patterns. We also identify factors
		  contributing to a negative gameplay experience. Business
		  model transitions in games present possibilities for
		  problematic practices. Our findings identify the need for
		  game developers and publishers to balance player
		  investments and fairness of rewards. A game's successful
		  transition depends on maintaining fundamental components of
		  player motivation and ensuring transparent communication.
		  Compared to existing taxonomies in other media, games need
		  a comprehensive classification of deceptive design. We
		  emphasize the importance of understanding player
		  perceptions and the impact of deceptive practices in future
		  research.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= oct,
  articleno	= {309},
  numpages	= {31},
  keywords	= {Overwatch, deceptive design, free-to-play, game model
		  transition, game player perception}
}

@InProceedings{	  10.1109/models-c.2019.00103,
  author	= {Bogdanova, Daria and Snoeck, Monique},
  title		= {Use of personalized feedback reports in a blended
		  conceptual modelling course},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00103},
  doi		= {10.1109/MODELS-C.2019.00103},
  abstract	= {Despite the substantial number of existing publications on
		  conceptual modelling education and feedback, in particular,
		  the perfect balance between the effectiveness of feedback
		  and the costs of the feedback design tailored to the
		  field-specific needs of conceptual modelling remains an
		  unanswered scientific and pedagogical question. The
		  existing educational literature and online courses on
		  conceptual modelling tend to overlook the essential aspects
		  of metacognition and self-regulation in the learning
		  process. The problem of providing feedback is exacerbated
		  by the time-consuming nature of manual feedback provision
		  and the difficulties of automating the provision of
		  personalized and elaborated feedback. This paper presents
		  an experience report on designing and implementing a
		  learning ontology-based personalized feedback report aimed
		  at raising student self-awareness and self-regulated
		  learning in a university level conceptual modelling course,
		  while the design aims at automation of the feedback
		  provisioning in the near future. It describes the stages of
		  learning report development and provides directions for
		  adapting this type of feedback for various learning
		  settings in conceptual modelling education, in view of
		  potential future automation of report provision.},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {672–679},
  numpages	= {8},
  keywords	= {blended learning, conceptual modelling, data modelling,
		  education, formative assessment, formative feedback,
		  learning report},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.5555/3466184.3466455,
  author	= {Casale, Giuliano},
  title		= {Integrated performance evaluation of extended queueing
		  network models with line},
  year		= {2021},
  isbn		= {9781728194998},
  publisher	= {IEEE Press},
  abstract	= {Despite the large literature on queueing theory and its
		  applications, tool support to analyze these models is
		  mostly focused on discrete-event simulation and mean-value
		  analysis (MVA). This circumstance diminishes the
		  applicability of other types of advanced queueing analysis
		  methods to practical engineering problems, for example
		  analytical methods to extract probability measures useful
		  in learning and inference. In this tool paper, we present
		  LINE 2.0, an integrated software package to specify and
		  analyze extended queueing network models. This new version
		  of the tool is underpinned by an object-oriented language
		  to declare a fairly broad class of extended queueing
		  networks. These abstractions have been used to integrate in
		  a coherent setting over 40 different simulation-based and
		  analytical solution methods, facilitating their use in
		  applications.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2377–2388},
  numpages	= {12},
  location	= {Orlando, Florida},
  series	= {WSC '20}
}

@InProceedings{	  10.1145/3340531.3412009,
  author	= {Nikolov, Andriy and d'Aquin, Mathieu},
  title		= {Uncovering Semantic Bias in Neural Network Models Using a
		  Knowledge Graph},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412009},
  doi		= {10.1145/3340531.3412009},
  abstract	= {While neural networks models have shown impressive
		  performance in many NLP tasks, lack of interpretability is
		  often seen as a disadvantage. Individual relevance scores
		  assigned by post-hoc explanation methods are not sufficient
		  to show deeper systematic preferences and potential biases
		  of the model that apply consistently across examples. In
		  this paper we apply rule mining using knowledge graphs in
		  combination with neural network explanation methods to
		  uncover such systematic preferences of trained neural
		  models and capture them in the form of conjunctive rules.
		  We test our approach in the context of text classification
		  tasks and show that such rules are able to explain a
		  substantial part of the model behaviour as well as indicate
		  potential causes of misclassifications when the model is
		  applied outside of the initial training context.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {1175–1184},
  numpages	= {10},
  keywords	= {explainable AI, knowledge graphs, neural networks, rule
		  mining},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3377024.3377027,
  author	= {Caesar, Birte},
  title		= {Engineering support for variability modeling for
		  context-sensitive reconfiguration of collaborative
		  manufacturing systems},
  year		= {2020},
  isbn		= {9781450375016},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3377024.3377027},
  doi		= {10.1145/3377024.3377027},
  abstract	= {The manufacturing domain faces new challenges due to
		  market changes. One of these changes affects consumer
		  behavior, i.e. customers demand individualized products in
		  small batches. Varying production requests require
		  different configurations of manufacturing systems. But
		  until today most of these systems are designed for single
		  purpose usage, therefore new manufacturing systems are
		  required. One solution are modular manufacturing systems,
		  which can be composed of diverse modules from different
		  vendors providing varying capabilities [1]. Modular
		  manufacturing systems are re-configurable at two different
		  levels. First, on the system group level, where modules
		  with the required capabilities are selected and
		  orchestrated. Second, on the module level, where a
		  configuration has to be selected that provides the
		  capability with the right manufacturing parameters. To be
		  able to select and orchestrate the modules on the system
		  group level, each module has to provide a self-description,
		  which covers the current configuration [2].},
  booktitle	= {Proceedings of the 14th International Working Conference
		  on Variability Modelling of Software-Intensive Systems},
  articleno	= {14},
  numpages	= {2},
  keywords	= {context-sensitive reconfiguration, reverse engineering,
		  variability mining, variability modeling},
  location	= {Magdeburg, Germany},
  series	= {VaMoS '20}
}

@InProceedings{	  10.1145/3411763.3451783,
  author	= {Seaborn, Katie and Pennefather, Peter and Miyake, Norihisa
		  and Otake-Matsuura, Mihoko},
  title		= {Crossing the Tepper Line: An Emerging Ontology for
		  Describing the Dynamic Sociality of Embodied AI},
  year		= {2021},
  isbn		= {9781450380959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411763.3451783},
  doi		= {10.1145/3411763.3451783},
  abstract	= {Artificial intelligences (AI) are increasingly being
		  embodied and embedded in the world to carry out tasks and
		  support decision-making with and for people. Robots,
		  recommender systems, voice assistants, virtual humans—do
		  these disparate types of embodied AI have something in
		  common? Here we show how they can manifest as “socially
		  embodied AI.” We define this as the state that embodied
		  AI “circumstantially” take on within interactive
		  contexts when perceived as both social and agentic by
		  people. We offer a working ontology that describes how
		  embodied AI can dynamically transition into socially
		  embodied AI. We propose an ontological heuristic for
		  describing the threshold: the Tepper line. We reinforce our
		  theoretical work with expert insights from a card sort
		  workshop. We end with two case studies to illustrate the
		  dynamic and contextual nature of this heuristic.},
  booktitle	= {Extended Abstracts of the 2021 CHI Conference on Human
		  Factors in Computing Systems},
  articleno	= {281},
  numpages	= {6},
  keywords	= {AI, Artificial agents, Ontology, Social embodiment, Social
		  perceptions},
  location	= {Yokohama, Japan},
  series	= {CHI EA '21}
}

@InProceedings{	  10.1145/3483845.3483880,
  author	= {Li, Baizhen and Zhan, Yibin and Wei, Zhihua and Huang, Shi
		  and Sun, Lijun},
  title		= {Improved non-autoregressive dialog state tracking model},
  year		= {2021},
  isbn		= {9781450390453},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3483845.3483880},
  doi		= {10.1145/3483845.3483880},
  abstract	= {Dialogue systems, a powerful tool of human-machine
		  interaction, are widely applied in e-commerce, online
		  education, and cellphone assistant, etc. Dialogue state
		  tracking (DST), updating the state of user goals during
		  dialogue, is a core part of task-oriented dialogue systems.
		  Recent research has made progress in low-latency and
		  good-performance DST neural network models, i.e.,
		  non-autoregressive dialogue state tracking model (NADST).
		  However, there are still some rooms for improvement in
		  dialogue state tracking. In this paper, we propose
		  following ways to improve the efficiency of NADST: (1)
		  adding shrinkage residual network into fertility
		  prediction; (2) constructing residual connection between
		  different hierarchical attentions; (3) inserting a relative
		  position encoding into state decoder for improving the
		  performance of state prediction. The results of analysis
		  and experiments indicate that the proposed model is the
		  SOTA non-autoregressive method of dialog state tracking.},
  booktitle	= {Proceedings of the 2021 2nd International Conference on
		  Control, Robotics and Intelligent System},
  pages		= {199–203},
  numpages	= {5},
  location	= {Qingdao, China},
  series	= {CCRIS '21}
}

@InProceedings{	  10.1145/3341161.3343854,
  author	= {Grisstte, Hanane and Nfaoui, ELhabib},
  title		= {Daily life patients sentiment analysis model based on
		  well-encoded embedding vocabulary for related-medication
		  text},
  year		= {2020},
  isbn		= {9781450368681},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341161.3343854},
  doi		= {10.1145/3341161.3343854},
  abstract	= {Millions of health-related messages and fresh
		  communications can reveal important public health issues.
		  New Drugs, Diseases, Adverse Drug Reactions (ADRs) keep
		  appearing on social media in new Unicode versions. In
		  particular, generative Model for both Sentiment analysis
		  (SA) and Naturel Language Understanding (NLU) requires
		  medical human labeled data or making use of resources for
		  weak supervision that operates with the ignorance and the
		  inability to define related-medication targets, and results
		  in inaccurate sentiment prediction performance. The
		  frequent use of informal medical language, nonstandard
		  format and abbreviation forms, as well as typos in social
		  media messages has to be taken into account. We probe the
		  transition-based approach between patients language used in
		  social media messages and formal medical language used in
		  the descriptions of medical concepts in a standard
		  ontology[21] to be formal input of our neural network
		  model. At this end, we propose daily life patients
		  Sentiment Analysis model based on hybrid embedding
		  vocabulary for related-medication text under distributed
		  dependency, and concepts translation methodology by
		  incorporating medical knowledge from social media and real
		  life medical science systems. The proposed neural network
		  layers is shared between medical concept Normalization
		  model and sentiment prediction model in order to understand
		  and leverage related-sentiment information behind
		  conceptualized features in Multiple context. The
		  experiments were performed on various real world scenarios
		  where limited resources in this case.},
  booktitle	= {Proceedings of the 2019 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {921–928},
  numpages	= {8},
  keywords	= {BiLSTM model, medical concepts normalization, patient
		  self-reports, sentiment analysis, social media},
  location	= {Vancouver, British Columbia, Canada},
  series	= {ASONAM '19}
}

@InProceedings{	  10.5555/3382225.3382427,
  author	= {Hu, Hengyi and Kerschberg, Larry},
  title		= {Evolving medical ontologies based on causal inference},
  year		= {2020},
  isbn		= {9781538660515},
  publisher	= {IEEE Press},
  abstract	= {Causal inference and analytics plays a critical role in
		  public health and disease prevention. Through mining of
		  large patient datasets, it is possible to identify
		  opportunities for intervention and to determine the
		  effectiveness of treatment. There are currently many
		  methods to analyze and learn causal relationships in large
		  patient datasets, as well as specific causal studies in
		  epidemiology that define specific relationships among
		  symptoms and treatments. This paper introduces a novel
		  methodology to utilize causal knowledge to extend and
		  improve a standard hierarchical medical ontology. First, we
		  will obtain the hierarchical structure of the patient
		  symptom variables based on the Medical Dictionary for
		  Regulatory Activities Terminology (MedDRA). Then, we will
		  learn a Causal Bayesian Network (CBN) using Max-Min
		  Hill-Climbing (MMHC), a hybrid constraint and score-based
		  learning algorithm, on the pre-existing National Institutes
		  of Mental Health (NIMH) study on Sequenced Treatment
		  Alternatives to Relieve Depression (STAR*D) patient
		  dataset. Finally, we will use the causal links discovered
		  in the CBN to evolve the ontology and its hierarchy.},
  booktitle	= {Proceedings of the 2018 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {954–957},
  numpages	= {4},
  keywords	= {bayesian networks, causal inference, causal networks,
		  causality, data management, data mining, healthcare data,
		  healthcare information technology, ontology, ontology
		  evolution, patient data},
  location	= {Barcelona, Spain},
  series	= {ASONAM '18}
}

@Article{	  10.1109/taslp.2022.3225537,
  author	= {Li, Yu and Hu, Bojie and Liu, Jian and Chen, Yufeng and
		  Xu, Jinan},
  title		= {A Neighborhood Re-Ranking Model With Relation Constraint
		  for Knowledge Graph Completion},
  year		= {2022},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3225537},
  doi		= {10.1109/TASLP.2022.3225537},
  abstract	= {Knowledge graph completion (KGC) aims to predict missing
		  links based on observed triples. However, current KGC
		  models are still limited by the following two aspects. (1)
		  the entity semantics is implicitly learned by neural
		  network and merely depends on existing facts, which mostly
		  suffers from less additional specific knowledge. Although
		  previous studies have noticed that entity type information
		  can effectively improve KGC task, most of them rely on
		  labeled type-specific data. (2) the recent graph-based
		  models mainly concentrate on Graph Neural Network (GNN) to
		  update source entity representation, regardless of the
		  separate role that neighborhood information plays and may
		  mix noisy neighbor features for target prediction. To
		  address the above two issues, we propose a neighborhood
		  re-ranking model with relation constraint for KGC task. We
		  suggest that both relation constraint and structured
		  information located in triples can boost the model
		  performance. More importantly, we automatically generate
		  explicit constraints as additional type feature to enrich
		  entity representation instead of depending on human
		  annotated labels. Meanwhile, we construct a neighborhood
		  completion module to re-rank candidate entities for full
		  use of the neighbor structure rather than traditional GNN
		  updating manner. Extensive experiments on seven benchmarks
		  demonstrate that our model achieves the competitive results
		  in comparison to the recent advanced baselines.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {411–425},
  numpages	= {15}
}

@Article{	  10.1145/3211871,
  author	= {Ochieng, Peter and Kyanda, Swaib},
  title		= {Large-Scale Ontology Matching: State-of-the-Art Analysis},
  year		= {2018},
  issue_date	= {July 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {51},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3211871},
  doi		= {10.1145/3211871},
  abstract	= {Ontologies have become a popular means of knowledge
		  sharing and reuse. This has motivated the development of
		  large-sized independent ontologies within the same or
		  different domains with some overlapping information among
		  them. To integrate such large ontologies, automatic
		  matchers become an inevitable solution. However, the
		  process of matching large ontologies has high space and
		  time complexities. Therefore, for a tool to efficiently and
		  accurately match these large ontologies within the limited
		  computing resources, it must have techniques that can
		  significantly reduce the high space and time complexities
		  associated with the ontology matching process. This article
		  provides a review of the state-of-the-art techniques being
		  applied by ontology matching tools to achieve scalability
		  and produce high-quality mappings when matching large
		  ontologies. In addition, we provide a direct comparison of
		  the techniques to gauge their effectiveness in achieving
		  scalability. A review of the state-of-the-art ontology
		  matching tools that employ each strategy is also provided.
		  We also evaluate the state-of-the-art tools to gauge the
		  progress they have made over the years in improving
		  alignment’s quality and reduction of execution time when
		  matching large ontologies.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {75},
  numpages	= {35},
  keywords	= {Survey, mapping repair, ontology mapping, repair,
		  scalability}
}

@InProceedings{	  10.1109/icssp.2019.00017,
  author	= {Islam, Chadni and Babar, Muhammad Ali and Nepal, Surya},
  title		= {An ontology-driven approach to automating the process of
		  integrating security software systems},
  year		= {2019},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ICSSP.2019.00017},
  doi		= {10.1109/ICSSP.2019.00017},
  abstract	= {A wide variety of security software systems need to be
		  integrated into a Security Orchestration Platform (SecOrP)
		  to streamline the processes of defending against and
		  responding to cybersecurity attacks. Lack of
		  interpretability and interoperability among security
		  systems are considered the key challenges to fully leverage
		  the potential of the collective capabilities of different
		  security systems. The processes of integrating security
		  systems are repetitive, time-consuming and error-prone;
		  these processes are carried out manually by human experts
		  or using ad-hoc methods. To help automate security systems
		  integration processes, we propose an
		  &lt;u&gt;On&lt;/u&gt;tology-driven approach for
		  &lt;u&gt;S&lt;/u&gt;ecurity OrchestrAtion Platform
		  (OnSOAP). The developed solution enables interpretability,
		  and interoperability among security systems, which may
		  exist in operational silos. We demonstrate OnSOAP's support
		  for automated integration of security systems to execute
		  the incident response process with three security systems
		  (Splunk, Limacharlie, and Snort) for a Distributed Denial
		  of Service (DDoS) attack. The evaluation results show that
		  OnSOAP enables SecOrP to interpret the input and output of
		  different security systems, produce error-free integration
		  details, and make security systems interoperable with each
		  other to automate and accelerate an incident response
		  process.},
  booktitle	= {Proceedings of the International Conference on Software
		  and System Processes},
  pages		= {54–63},
  numpages	= {10},
  keywords	= {automated integration process, incident response process,
		  ontology, security orchestration, security system},
  location	= {Montreal, Quebec, Canada},
  series	= {ICSSP '19}
}

@InProceedings{	  10.1145/3348445.3348461,
  author	= {Arif, Khawaja Sarmad and Qamar, Usman and Wahab, Kanwal
		  and Riaz, Muhammad Qasim},
  title		= {Building a Biomedical Ontology for Respiratory Tract
		  Infection},
  year		= {2019},
  isbn		= {9781450371957},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3348445.3348461},
  doi		= {10.1145/3348445.3348461},
  abstract	= {Respiratory tract infections are most common disease which
		  can affect any human during any part of their age.
		  According to sources almost 60\% of all antibiotic
		  prescription is due to Respiratory tract infection. The
		  concepts and their relations related to Respiratory tract
		  infection are need to be explained with the help of
		  biomedical literature as well as historical records. But
		  these literature or records cannot be efficiently managed
		  by users due to their unstructured representation.
		  Biomedical Ontologies are best way to identify the concepts
		  and their respective relations from huge amount of
		  unstructured data. Our research aimed to create a
		  biomedical Ontology for the domain of Respiratory tract
		  infection using UMLS as a data source, which contains
		  concepts, subtypes, their relationships, and semantic
		  types. As a result ontology contains 26 main and sub types
		  of Respiratory tract infections, also 234 broad relations
		  with 107325 relation counts and 1151 narrow relation with
		  34580 relation counts. The ontology created is evaluated by
		  domain experts and results are formulated.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Computer and Communications Management},
  pages		= {8–12},
  numpages	= {5},
  keywords	= {Biomedical Ontology, RTIs, Respiratory Tract Infection,
		  Semantic web, UMLS},
  location	= {Bangkok, Thailand},
  series	= {ICCCM '19}
}

@InProceedings{	  10.1145/3192714.3196316,
  author	= {Pelzetter, Jens},
  title		= {Using Ontologies as a Foundation for Web Accessibility
		  Tools},
  year		= {2018},
  isbn		= {9781450356510},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3192714.3196316},
  doi		= {10.1145/3192714.3196316},
  abstract	= {Creating web sites has become quite a complex task. One of
		  most important aspects of a modern web site is
		  accessibility. However, despite extensive standards many
		  web sites have accessibility issues. This paper presents a
		  new approach for creating tools to improve the
		  accessibility of web sites using ontologies.},
  booktitle	= {Proceedings of the 15th International Web for All
		  Conference},
  articleno	= {26},
  numpages	= {2},
  keywords	= {Accessibility Web Ontologies},
  location	= {Lyon, France},
  series	= {W4A '18}
}

@Article{	  10.1145/3338843,
  author	= {Jackson, Daniel},
  title		= {Alloy: a language and tool for exploring software
		  designs},
  year		= {2019},
  issue_date	= {September 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {62},
  number	= {9},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3338843},
  doi		= {10.1145/3338843},
  abstract	= {Exploiting a simple, expressive logic based on relations
		  to describe designs and automate their analysis.},
  journal	= {Commun. ACM},
  month		= aug,
  pages		= {66–76},
  numpages	= {11}
}

@InProceedings{	  10.1145/3544109.3544138,
  author	= {Yuan, Min},
  title		= {Modeling Analysis of the Influence of Seoul City Image on
		  Tourists' Willingness to Revisit Based on Parallel
		  Computing},
  year		= {2022},
  isbn		= {9781450395786},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544109.3544138},
  doi		= {10.1145/3544109.3544138},
  abstract	= {Tourism is closely related to people's lives today, and
		  the development level of tourism informatization is an
		  important indicator to measure the modern tourism industry.
		  At present, more and more data on the Internet is released
		  in the form of linked data, which reduces the complexity of
		  the integration of distributed, heterogeneous or autonomous
		  data, and at the same time promotes the application of
		  linked data. The purpose of this article is to model the
		  influence of Seoul's city image on tourists' willingness to
		  revisit based on parallel computing. This paper studies the
		  similarity calculation efficiency in the data set of
		  passenger-related passenger revisiting intention resources.
		  According to the established tourist tourism ontology, this
		  paper adopts the MapReduce parallel computing framework to
		  design a parallel computing method of related data
		  similarity to improve the discovery efficiency of the
		  related data model of the willingness of large-scale
		  tourists to revisit. Experimental research shows that this
		  article analyzes the difference in perceptions of various
		  image factors by tourists of different ages, and finds that
		  the Р value in the single-factor analysis of variance
		  table is greater than 0.05, indicating that tourists of
		  different ages do not have significant perceptions of each
		  image factor.},
  booktitle	= {Proceedings of the 3rd Asia-Pacific Conference on Image
		  Processing, Electronics and Computers},
  pages		= {164–168},
  numpages	= {5},
  location	= {Dalian, China},
  series	= {IPEC '22}
}

@InProceedings{	  10.1145/3434074.3446353,
  author	= {Wen, Ruchen},
  title		= {Toward Hybrid Relational-Normative Models of Robot
		  Cognition},
  year		= {2021},
  isbn		= {9781450382908},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3434074.3446353},
  doi		= {10.1145/3434074.3446353},
  abstract	= {Most previous work on enabling robots' moral competence
		  has used norm-based systems of moral reasoning. However, a
		  number of limitations to norm-based ethical theories have
		  been widely acknowledged. These limitations may be
		  addressed by role-based ethical theories, which have been
		  extensively discussed in the philosophy of technology
		  literature but have received little attention within
		  robotics. My work proposes a hybrid role/norm-based model
		  of robot cognitive processes including moral cognition.},
  booktitle	= {Companion of the 2021 ACM/IEEE International Conference on
		  Human-Robot Interaction},
  pages		= {568–570},
  numpages	= {3},
  keywords	= {human-robot interaction, robot ethics, robotic moral
		  competence, role ethics},
  location	= {Boulder, CO, USA},
  series	= {HRI '21 Companion}
}

@InProceedings{	  10.5555/3522802.3522932,
  author	= {Khemiri, Abdelhak and Yugma, Claude and
		  Dauz\`{e}re-P\'{e}r\`{e}s, St\'{e}phane},
  title		= {Towards a generic semiconductor manufacturing simulation
		  model},
  year		= {2022},
  publisher	= {IEEE Press},
  abstract	= {Simulation is one of the most used approaches to analyze
		  semiconductor manufacturing systems. However, most
		  simulation models are designed for single-use applications
		  and study a limited set of problems that are not reusable
		  afterwards. Recently, in order to overcome this problem,
		  the idea to develop a generic wafer fab simulation model
		  has emerged. Nonetheless, few papers address the
		  development of a generic wafer fab simulation. This paper
		  proposes a generic, data-driven simulation model to
		  evaluate and analyze a wide range of problems arising in
		  modern semiconductor manufacturing systems. We discuss the
		  issues related to the genericity of such a simulation model
		  and the data and semantic integration issues faced by
		  users.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  articleno	= {159},
  numpages	= {10},
  location	= {Phoenix, Arizona},
  series	= {WSC '21}
}

@InProceedings{	  10.1145/3407982.3407997,
  author	= {Popov, Miroslav and Ivanova, Tatyana},
  title		= {Knowledge Model for Developing, Searching and Using
		  Personalized Learning Content for Learners, Having Dyslexia
		  Disability},
  year		= {2020},
  isbn		= {9781450377683},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3407982.3407997},
  doi		= {10.1145/3407982.3407997},
  abstract	= {Much research in the area of e-learning systems today is
		  focused on personalization and adaptivity, but only a few
		  of the proposed systems are intended for students, having
		  learning disabilities. These learners are significant
		  amount of all learners (children, having some dyslexia
		  symptoms are between 10\% and 20\% of all children).
		  Dyslexia symptoms can be specific for different learners
		  and can require specific organization of learning. All the
		  experts in learning say that suitable learning is a key for
		  achieving excellent results for these learners.We made
		  extensive study on the relevant theories intended for
		  better understanding of the requirements of an e-learning
		  for people, having learning disabilities and existing
		  frameworks or tools for people with dyslexia. As a result
		  we propose a knowledge model for developing, describing,
		  searching and recommending personalized learning resources
		  in the web for learners, having dyslexia learning
		  disability. We also propose agent-based software
		  architecture, that manage and use implemented knowledge for
		  supporting leaning resource development, searching, ranking
		  according to specified criteria and recommendation of these
		  resources to dyslexic learners.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Computer Systems and Technologies},
  pages		= {258–265},
  numpages	= {8},
  keywords	= {Dyslexia, E-Learning, Ontology, Web-based learning,
		  knowledge},
  location	= {Ruse, Bulgaria},
  series	= {CompSysTech '20}
}

@InProceedings{	  10.1145/3586183.3606741,
  author	= {Chen, Weihao and Yu, Chun and Wang, Huadong and Wang,
		  Zheng and Yang, Lichen and Wang, Yukun and Shi, Weinan and
		  Shi, Yuanchun},
  title		= {From Gap to Synergy: Enhancing Contextual Understanding
		  through Human-Machine Collaboration in Personalized
		  Systems},
  year		= {2023},
  isbn		= {9798400701320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3586183.3606741},
  doi		= {10.1145/3586183.3606741},
  abstract	= {This paper presents LangAware, a collaborative approach
		  for constructing personalized context for context-aware
		  applications. The need for personalization arises due to
		  significant variations in context between individuals based
		  on scenarios, devices, and preferences. However, there is
		  often a notable gap between humans and machines in the
		  understanding of how contexts are constructed, as observed
		  in trigger-action programming studies such as IFTTT.
		  LangAware enables end-users to participate in establishing
		  contextual rules in-situ using natural language. The system
		  leverages large language models (LLMs) to semantically
		  connect low-level sensor detectors to high-level contexts
		  and provide understandable natural language feedback for
		  effective user involvement. We conducted a user study with
		  16 participants in real-life settings, which revealed an
		  average success rate of 87.50\% for defining contextual
		  rules in a variety of 12 campus scenarios, typically
		  accomplished within just two modifications. Furthermore,
		  users reported a better understanding of the machine’s
		  capabilities by interacting with LangAware.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {110},
  numpages	= {15},
  keywords	= {Context-Aware Systems, End User Context Construction,
		  Large Language Models, Personalization},
  location	= {San Francisco, CA, USA},
  series	= {UIST '23}
}

@Article{	  10.1145/3530221,
  author	= {Malmi, Lauri and Sheard, Judy and Kinnunen, P\"{a}ivi and
		  Simon and Sinclair, Jane},
  title		= {Development and Use of Domain-specific Learning Theories,
		  Models, and Instruments in Computing Education},
  year		= {2022},
  issue_date	= {March 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {1},
  url		= {https://doi.org/10.1145/3530221},
  doi		= {10.1145/3530221},
  abstract	= {Use of theory within a field of research provides the
		  foundation for designing effective research programs and
		  establishing a deeper understanding of the results
		  obtained. This, together with the emergence of
		  domain-specific theory, is often taken as an indicator of
		  the maturity of any research area. This article explores
		  the development and subsequent usage of domain-specific
		  theories and theoretical constructs (TCs) in computing
		  education research (CER). All TCs found in 878 papers
		  published in three major CER publication venues over the
		  period 2005–2020 were identified and assessed to
		  determine the nature and purpose of the constructs found.
		  We focused more closely on areas related to learning,
		  studying, and progression, where our analysis found 80 new
		  TCs that had been developed, based on multiple
		  epistemological perspectives. Several existing frameworks
		  were used to categorize the areas of CER focus in which TCs
		  were found, the methodology by which they were developed,
		  and the nature and purpose of the TCs. A citation analysis
		  was undertaken, with 1,727 citing papers accessed to
		  determine to what extent and in what ways TCs had been used
		  and developed to inform subsequent work, also considering
		  whether these aspects vary according to different focus
		  areas within computing education. We noted which TCs were
		  used most often and least often, and we present several
		  brief case studies that demonstrate progressive development
		  of domain-specific theory. The exploration provides
		  insights into trends in theory development and suggests
		  areas in which further work might be called for. Our
		  findings indicate a general interest in the development of
		  TCs during the period studied, and we show examples of how
		  different approaches to theory development have been used.
		  We present a framework suggesting how strategies for
		  developing new TCs in CER might be structured and discuss
		  the nature of theory development in relation to the field
		  of CER.},
  journal	= {ACM Trans. Comput. Educ.},
  month		= dec,
  articleno	= {6},
  numpages	= {48},
  keywords	= {Computing education, theory, theoretical construct,
		  literature, research, instrument}
}

@InProceedings{	  10.1145/3266237.3266266,
  author	= {da Silva Quirino, Glaice Kelly and Barcellos, Monalessa
		  Perini and de Almeida Falbo, Ricardo},
  title		= {Visual notations for software pattern languages: a mapping
		  study},
  year		= {2018},
  isbn		= {9781450365031},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3266237.3266266},
  doi		= {10.1145/3266237.3266266},
  abstract	= {Reuse has been recognized as an important practice in
		  software engineering. The use of patterns makes it easier
		  to reuse successful solutions, speeds up the development
		  process, and promotes the application of good practices.
		  Related patterns can be organized in a Pattern Language
		  (PL), which represents the patterns and their relations,
		  and provides guidance on how to select, reuse and integrate
		  them. Visual notations are often used to provide a
		  graphical representation to PLs. Aiming to investigate how
		  PLs related to software have been visually represented, we
		  carried out a systematic mapping. We identified and
		  analyzed 64 PLs. As a result, we noticed a lack of
		  consensus on the elements that should be represented in a
		  PL and the symbols used to represent them. Moreover, most
		  PLs have ambiguous or inexpressive visual
		  representations.},
  booktitle	= {Proceedings of the XXXII Brazilian Symposium on Software
		  Engineering},
  pages		= {72–81},
  numpages	= {10},
  keywords	= {mapping study, pattern language, visual notation},
  location	= {Sao Carlos, Brazil},
  series	= {SBES '18}
}

@InProceedings{	  10.5555/3586210.3586318,
  author	= {Bocciarelli, Paolo and D'Ambrogio, Andrea and Wagner,
		  Gerd},
  title		= {Resource Modeling in Business Process Simulation},
  year		= {2023},
  publisher	= {IEEE Press},
  abstract	= {Business Process (BP) models address the specification of
		  the flow of events and activities, along with the
		  dependencies of activities on resources. BP models are
		  often analyzed by using simulation-based approaches. This
		  paper focuses on resource modeling for BP modeling and
		  simulation, by first introducing the most important
		  concepts and discussing how resources are modeled in the
		  standard BP modeling notation (i.e., BPMN) and in the area
		  of Discrete Event Simulation. Then, the paper presents two
		  newer BP modeling and simulation approaches, namely the
		  Object Event Modeling and Simulation with the Discrete
		  Event Process Modeling Notation (DPMN) based on the
		  JavaScript-based simulation framework OESjs, and
		  Performability-enabled BPMN (PyBPMN), with the Java-based
		  simulation framework eBPMN. A simple but effective case
		  study dealing with a pizza service process is used to
		  illustrate the main features of the presented approaches.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1296–1310},
  numpages	= {15},
  location	= {Singapore, Singapore},
  series	= {WSC '22}
}

@InProceedings{	  10.1145/3677052.3698597,
  author	= {Cho, Nicole and Srishankar, Nishan and Cecchi, Lucas and
		  Watson, William},
  title		= {FISHNET: Financial Intelligence from Sub-querying,
		  Harmonizing, Neural-Conditioning, Expert Swarms, and Task
		  Planning},
  year		= {2024},
  isbn		= {9798400710810},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677052.3698597},
  doi		= {10.1145/3677052.3698597},
  abstract	= {Financial intelligence generation from vast data sources
		  has typically relied on traditional methods of
		  knowledge-graph construction or database engineering.
		  Recently, fine-tuned financial domain-specific Large
		  Language Models (LLMs), have emerged. While these
		  advancements are promising, limitations such as high
		  inference costs, hallucinations, and the complexity of
		  concurrently analyzing high-dimensional financial data,
		  emerge. This motivates our invention FISHNET (Financial
		  Intelligence from Sub-querying, Harmonizing,
		  Neural-Conditioning, Expert swarming, and Task planning),
		  an agentic architecture that accomplishes highly complex
		  analytical tasks for more than 98,000 regulatory filings
		  that vary immensely in terms of semantics, data hierarchy,
		  or format. FISHNET shows remarkable performance for
		  financial insight generation (61.8\% success rate over
		  5.0\% Routing, 45.6\% RAG R-Precision). We conduct rigorous
		  ablations to empirically prove the success of FISHNET, each
		  agent’s importance, and the optimized performance of
		  assembling all agents. Our modular architecture can be
		  leveraged for a myriad of use-cases, enabling scalability,
		  flexibility, and data integrity that are critical for
		  financial tasks.},
  booktitle	= {Proceedings of the 5th ACM International Conference on AI
		  in Finance},
  pages		= {591–599},
  numpages	= {9},
  keywords	= {Harmonizing, LLM Agents, Planning, Sub-querying,
		  Swarming},
  location	= {Brooklyn, NY, USA},
  series	= {ICAIF '24}
}

@InProceedings{	  10.1145/3274895.3274944,
  author	= {Bernard, Camille and Plumejeaud-Perreau, Christine and
		  Villanova-Oliver, Marl\`{e}ne and Gensel, J\'{e}r\^{o}me
		  and Dao, Hy},
  title		= {An ontology-based algorithm for managing the evolution of
		  multi-level territorial partitions},
  year		= {2018},
  isbn		= {9781450358897},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3274895.3274944},
  doi		= {10.1145/3274895.3274944},
  abstract	= {Through times, regions all over the world are very often
		  subject to change (their names, their belonging, their
		  composition, and their geometries). In this paper, we
		  present a Semantic Matching Algorithm for automatically
		  detecting, describing and publishing in the Linked Open
		  Data Web, rich descriptions of changes occurring in
		  multi-level territorial partitions (e.g., partitions made
		  of major regions, regions and districts levels). We adopt a
		  Linked Data (LD) approach for the semantic descriptions of
		  the changes they undergo, relying on two existing generic
		  ontologies, TSN-Ontology and TSN-Change Ontology. The
		  created RDF graphs draw the lineage of each region over
		  time (horizontal reading of the graphs), as well as the
		  propagation of a change event through the partition levels
		  (vertical reading).},
  booktitle	= {Proceedings of the 26th ACM SIGSPATIAL International
		  Conference on Advances in Geographic Information Systems},
  pages		= {456–459},
  numpages	= {4},
  keywords	= {change detection, evolutive multi-level territorial
		  partition, geospatial data matching, linked open data,
		  semantic matching algorithm, semantic web, spatio-temporal
		  ontology, versioning},
  location	= {Seattle, Washington},
  series	= {SIGSPATIAL '18}
}

@InProceedings{	  10.1145/3648188.3675152,
  author	= {Li, Ge and Vachtsevanou, Danai and Lem\'{e}e,
		  J\'{e}r\'{e}my and Mayer, Simon and Strecker, Jannis},
  title		= {Reader-aware Writing Assistance through Reader Profiles},
  year		= {2024},
  isbn		= {9798400705953},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3648188.3675152},
  doi		= {10.1145/3648188.3675152},
  abstract	= {Establishing rapport between authors and readers of
		  scientific texts is essential for supporting readers in
		  understanding texts as intended, facilitating
		  socio-discursive practices within disciplinary communities,
		  and helping in identifying interdisciplinary links among
		  scientific writings. We propose a Reader-aware Congruence
		  Assistant (RaCA), which supports writers to create texts
		  that are adapted to target readers. Similar to
		  user-centered design which is based on user profiles, RaCA
		  features reader-centered writing through reader profiles
		  that are dynamically computed from information discovered
		  through academic search engines. Our assistant then
		  leverages large language models to measure the congruence
		  of a written text with a given reader profile, and provides
		  feedback to the writer. We demonstrate our approach with an
		  implemented prototype that illustrates how RaCA exploits
		  information available on the Web to construct reader
		  profiles, assesses writer-reader congruence and offers
		  writers color-coded visual feedback accordingly. We argue
		  that our approach to reader-oriented scientific writing
		  paves the way towards the more personalized interaction of
		  readers and writers with scientific content, and discuss
		  how integration with Semantic Web technologies and Adaptive
		  User Interface design can help materialize this vision
		  within an ever-growing Web of scientific ideas, proof, and
		  discourse.},
  booktitle	= {Proceedings of the 35th ACM Conference on Hypertext and
		  Social Media},
  pages		= {344–350},
  numpages	= {7},
  keywords	= {Natural Language Processing, Personalized Text Adaptation,
		  Reader Profile, Text Congruence},
  location	= {Poznan, Poland},
  series	= {HT '24}
}

@InProceedings{	  10.1145/3589335.3651914,
  author	= {Hanikov\'{a}, Kate\v{r}ina and Chud\'{a}n, David and
		  Sv\'{a}tek, Vojt\v{e}ch and Vajde\v{c}ka, Peter and Troncy,
		  Rapha\"{e}l and Vencovsk\'{y}, Filip and Syrov\'{a}tkov\'{a}, Jana},
  title		= {Towards Fact-check Summarization Leveraging on
		  Argumentation Elements Tied to Entity Graphs},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651914},
  doi		= {10.1145/3589335.3651914},
  abstract	= {Fact-check consumers can have different preferences
		  regarding the amount of text being used for explaining the
		  claim veracity verdict. Dynamically adapting the size of a
		  fact-check report is thus an important functionality for
		  systems designed to convey claim verification
		  explainability. Recent works have experimented with
		  applying transformers-based or LLM-based text summarization
		  methods in a zero-shot or few-shot manner, making use of
		  some existing texts available in the summary parts of
		  fact-check reports (e.g., called "justification'' in
		  PolitiFact). However, for complex fact-checks, the purely
		  sub-symbolic summarizers tend to either omit some elements
		  of the fact-checker's argumentation chains or include
		  contextual statements that may not be essential at the
		  given level of granularity. In this paper, we propose a new
		  method for enhancing fact-check summarization with the aim
		  of injecting elements of structured fact-checker
		  argumentation. This argumentation is, in turn, not only
		  captured at the discourse level but tied to an entity graph
		  representing the fact-check, for which we employ the PURO
		  diagrammatic language. We have empirically performed a
		  manual analysis of fact-check reports from two fact-checker
		  websites, yielding (1) textual snippets containing the
		  argumentation essence of the fact-check report and (2)
		  categorized argumentation elements tied to entity graphs.
		  These snippets are then fed to a state-of-the-art hybrid
		  summarizer which has previously produced accurate
		  fact-check summaries, as an additional input. We observe
		  mild improvements on various ROUGE metrics, even if the
		  validity of the results is limited given the small size of
		  the dataset. We also compare the human-provided
		  argumentation element categories with those returned, for
		  the given fact-check ground truth summary, using a
		  pre-trained language model upon both basic and augmented
		  prompting. This yields a moderate accuracy as the model
		  often fails to comply with the explicit given
		  instructions.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1473–1481},
  numpages	= {9},
  keywords	= {argumentation, entity graph, fact-checking, text
		  summarization},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1109/sc41406.2024.00013,
  author	= {Dharuman, Gautham and Hippe, Kyle and Brace, Alexander and
		  Foreman, Sam and Hatanp\"{a}\"{a}, V\"{a}in\"{o} and
		  Sastry, Varuni K. and Zheng, Huihuo and Ward, Logan and
		  Muralidharan, Servesh and Vasan, Archit and Kale, Bharat
		  and Mann, Carla M. and Ma, Heng and Cheng, Yun-Hsuan and
		  Zamora, Yuliana and Liu, Shengchao and Xiao, Chaowei and
		  Emani, Murali and Gibbs, Tom and Tatineni, Mahidhar and
		  Canchi, Deepak and Mitchell, Jerome and Yamada, Koichi and
		  Garzaran, Maria and Papka, Michael E. and Foster, Ian and
		  Stevens, Rick and Anandkumar, Anima and Vishwanath,
		  Venkatram and Ramanathan, Arvind},
  title		= {MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal
		  Protein Design Workflows with Direct Preference
		  Optimization},
  year		= {2024},
  isbn		= {9798350352917},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/SC41406.2024.00013},
  doi		= {10.1109/SC41406.2024.00013},
  abstract	= {We present a scalable, end-to-end workflow for protein
		  design. By augmenting protein sequences with natural
		  language descriptions of their biochemical properties, we
		  train generative models that can be preferentially aligned
		  with protein fitness landscapes. Through complex
		  experimental- and simulation-based observations, we
		  integrate these measures as preferred parameters for
		  generating new protein variants and demonstrate our
		  workflow on five diverse supercomputers. We achieve &gt;1
		  ExaFLOPS sustained performance in mixed precision on each
		  supercomputer and a maximum sustained performance of 4.11
		  ExaFLOPS and peak performance of 5.57 ExaFLOPS. We
		  establish the scientific performance of our model on two
		  tasks: (1) across a predetermined benchmark dataset of deep
		  mutational scanning experiments to optimize the
		  fitness-determining mutations in the yeast protein HIS7,
		  and (2) in optimizing the design of the enzyme malate
		  dehydrogenase to achieve lower activation barriers (and
		  therefore increased catalytic rates) using simulation data.
		  Our implementation thus sets high watermarks for multimodal
		  protein design workflows.},
  booktitle	= {Proceedings of the International Conference for High
		  Performance Computing, Networking, Storage, and Analysis},
  articleno	= {7},
  numpages	= {13},
  keywords	= {AI, HPC, Large language models, protein design},
  location	= {Atlanta, GA, USA},
  series	= {SC '24}
}

@InProceedings{	  10.5555/3522802.3522977,
  author	= {G\"{u}tlein, Moritz and German, Reinhard and Djanatliev,
		  Anatoli},
  title		= {Hide your model! layer abstractions for data-driven
		  co-simulations},
  year		= {2022},
  publisher	= {IEEE Press},
  abstract	= {Modeling and simulating of problems that span across
		  multiple domains can be tricky. Often, the need for a
		  co-simulation arises, for example because the modeling
		  cannot be done with a single tool. Domain experts may face
		  a barrier when it comes to the implementation of such a
		  co-simulation. In addition, the demand for integrating data
		  from various sources into simulation models seems to be
		  growing. Therefore, we propose an abstraction concept that
		  hides simulators and models behind generalized interfaces
		  that are derived from prototypical classes. The data-driven
		  abstraction concept facilitates having an assembly kit with
		  predefined simulator building blocks that can be easily
		  plugged together. Furthermore, data streams can be
		  seamlessly ingested into such a composed model. Likewise,
		  the co-simulation can be accessed via the resulting
		  interfaces for further processing and interactions.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  articleno	= {206},
  numpages	= {12},
  location	= {Phoenix, Arizona},
  series	= {WSC '21}
}

@InProceedings{	  10.1145/3469213.3470224,
  author	= {Li, Haixia},
  title		= {A New query method for the temporal RDF Model RDFMT Based
		  on SPARQL},
  year		= {2021},
  isbn		= {9781450390200},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3469213.3470224},
  doi		= {10.1145/3469213.3470224},
  abstract	= {With the explosion of real-time data, the representation
		  and query of temporal data has become a hot research topic.
		  Many researchers have proposed various temporal
		  representation models and query methods. On the basis of
		  the proposed temporal model RDFMT, we put forward the query
		  language SPARQLMT for RDFMT. SPARQLMT is expanded based on
		  SPARQL language, adding a syntax and semantics that is
		  convenient for querying temporal information. As we all
		  know, SPARQL is the official query language of the standard
		  RDF model. SPARQLMT is based on SPARQL, which is also
		  conducive to using the SPARQL query engine. In this paper
		  we mainly illustrate a query method SPARQLMT for the RDFMT
		  by extending SPARQL and give the semantics and syntax of
		  SPARQLMT.},
  booktitle	= {2021 2nd International Conference on Artificial
		  Intelligence and Information Systems},
  articleno	= {24},
  numpages	= {6},
  location	= {Chongqing, China},
  series	= {ICAIIS 2021}
}

@Article{	  10.5555/3648699.3648933,
  author	= {Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Agarwal,
		  Aravind and Cheng, Yun and Morency, Louis-Philippe and
		  Salakhutdinov, Ruslan},
  title		= {MULTIZOO \&amp; MULTIBENCH: a standardized toolkit for
		  multimodal deep learning},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {JMLR.org},
  volume	= {24},
  number	= {1},
  issn		= {1532-4435},
  abstract	= {Learning multimodal representations involves integrating
		  information from multiple heterogeneous sources of data. In
		  order to accelerate progress towards understudied
		  modalities and tasks while ensuring real-world robustness,
		  we release MULTIZOO, a public toolkit consisting of
		  standardized implementations of &gt; 20 core multimodal
		  algorithms and MULTIBENCH, a large-scale benchmark spanning
		  15 datasets, 10 modalities, 20 prediction tasks, and 6
		  research areas. Together, these provide an automated
		  end-to-end machine learning pipeline that simplifies and
		  standardizes data loading, experimental setup, and model
		  evaluation. To enable holistic evaluation, we offer a
		  comprehensive methodology to assess (1) generalization, (2)
		  time and space complexity, and (3) modality robustness.
		  MULTIBENCH paves the way towards a better understanding of
		  the capabilities and limitations of multimodal models,
		  while ensuring ease of use, accessibility, and
		  reproducibility. Our toolkits are publicly available, will
		  be regularly updated, and welcome inputs from the
		  community. Code: https://github.com/pliang279/MultiBench
		  Documentation:
		  https://multibench.readthedocs.io/en/latest/},
  journal	= {J. Mach. Learn. Res.},
  month		= jan,
  articleno	= {234},
  numpages	= {7},
  keywords	= {multimodal learning, representation learning, benchmarks,
		  open source software}
}

@InProceedings{	  10.1145/3184558.3186943,
  author	= {Shi, Baoxu and Weninger, Tim},
  title		= {Visualizing the Flow of Discourse with a Concept
		  Ontology},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3186943},
  doi		= {10.1145/3184558.3186943},
  abstract	= {Understanding and visualizing human discourse has long
		  being a challenging task. Although recent work on argument
		  mining have shown success in classifying the role of
		  various sentences, the task of recognizing concepts and
		  understanding the ways in which they are discussed remains
		  challenging. Given an email thread or a transcript of a
		  group discussion, our task is to extract the relevant
		  concepts and understand how they are referenced and
		  re-referenced throughout the discussion. In the present
		  work, we present a preliminary approach for extracting and
		  visualizing group discourse by adapting Wikipedia's
		  category hierarchy to be an external concept ontology. From
		  a user study, we found that our method achieved better
		  results than 4 strong alternative approaches, and we
		  illustrate our visualization method based on the extracted
		  discourse flows.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {89–90},
  numpages	= {2},
  keywords	= {concept ontology, discourse visualization},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3502223.3502744,
  author	= {Takhom, Akkharawoot and Utasri, Tharathon and Leenoi,
		  Dhanon and Soomjinda, Pitchaya and Boonkwan, Prachya and
		  Supnithi, Thepchai},
  title		= {Knowledge Graph Enhanced Community Consensus: a
		  Scenario-based Knowledge Construction on Buddha Images},
  year		= {2022},
  isbn		= {9781450395656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3502223.3502744},
  doi		= {10.1145/3502223.3502744},
  booktitle	= {Proceedings of the 10th International Joint Conference on
		  Knowledge Graphs},
  pages		= {191–194},
  numpages	= {4},
  keywords	= {Community consensus, Community-driven approach, Knowledge
		  construction, Knowledge engineering, Knowledge graph,
		  Ontology development, Semantic Web},
  location	= {Virtual Event, Thailand},
  series	= {IJCKG '21}
}

@InProceedings{	  10.1145/3432291.3432305,
  author	= {Cao, Suqun and Lingao, Wang and Ji, Rendong and Wang, Chao
		  and Yao, Liu and Kai, Lin and Abdalla, Ahmed N. and k.,
		  Sujatha},
  title		= {Clinical Decision Support System Based on KNN/Ontology
		  Extraction Method},
  year		= {2020},
  isbn		= {9781450375733},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3432291.3432305},
  doi		= {10.1145/3432291.3432305},
  abstract	= {The complexity of the knowledge structure in the clinical
		  cases, involving a wide range of attributes, results in
		  making its case similarity calculation more complex. The
		  existing medical ontologies, due to different expressions
		  of the same concepts in computer information retrieval,
		  causes difficulties in terms of sharing useful information
		  in different database systems. This paper constructs a new
		  decision support system based on KNN/ontology method was
		  proposed. The detail of the methods and processes of common
		  clinical case knowledge acquisition in combination with the
		  method of obtaining structured information has been
		  presented. The clinical case data similarity calculation
		  method based on various types such as symptom information,
		  medical history information, complications, surgical
		  information, diagnostic results and other information, for
		  record of a clinical diagnosis and treatment process. The
		  validity of the similarity calculation method and the
		  weight calculation method is verified by the clinical case
		  data. The proposed methods can be effective for improving
		  the quality and level of clinical services for medical
		  service organizations.},
  booktitle	= {Proceedings of the 2020 3rd International Conference on
		  Signal Processing and Machine Learning},
  pages		= {56–62},
  numpages	= {7},
  keywords	= {Neural Network, Similarity, clinical medicine, extraction
		  method},
  location	= {Beijing, China},
  series	= {SPML '20}
}

@InProceedings{	  10.1145/3731120.3744624,
  author	= {Sign\'{e}, Quentin and Boughanem, Mohand and Moreno, Jose
		  G. and Belkacem, Thiziri},
  title		= {A Substring Extraction-Based RAG Method for Minimising
		  Hallucinations in Aircraft Maintenance Question Answering},
  year		= {2025},
  isbn		= {9798400718618},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3731120.3744624},
  doi		= {10.1145/3731120.3744624},
  abstract	= {Hallucination occurs when a language model generates
		  plausible yet nonfactual information. In particular,
		  faithfulness hallucinations (inconsistency with a given
		  context) cannot be tolerated in critical domains such as
		  aircraft maintenance due to the potentially severe
		  consequences. To mitigate this issue, Retrieval Augmented
		  Generation (RAG) methods have been introduced. These
		  approaches are relevant for reducing the risks of
		  hallucination but do not eliminate them, as the generator
		  may still produce content unfaithful to the retrieved
		  context. This paper proposes a novel RAG approach that
		  leverages a substring extraction tool from retrieved
		  documents to minimise hallucinations. Experiments performed
		  on real aircraft maintenance documentation revealed that,
		  despite the lower accuracy of the answers compared to
		  traditional RAG methods, the proposed approach demonstrates
		  an improved control over hallucination risks. This
		  highlights the potential of our method in highly technical
		  use cases where accuracy and reliability are key.},
  booktitle	= {Proceedings of the 2025 International ACM SIGIR Conference
		  on Innovative Concepts and Theories in Information
		  Retrieval (ICTIR)},
  pages		= {513–521},
  numpages	= {9},
  keywords	= {question answering, retrieval augmented generation,
		  technical maintenance},
  location	= {Padua, Italy},
  series	= {ICTIR '25}
}

@InProceedings{	  10.1145/3397482.3450619,
  author	= {Kaindl, Hermann},
  title		= {ModelGenGUIs – High-level Interaction Design with
		  Discourse Models for Automated GUI Generation},
  year		= {2021},
  isbn		= {9781450380188},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397482.3450619},
  doi		= {10.1145/3397482.3450619},
  abstract	= {Since manual creation of user interfaces is hard and
		  expensive, automated generation may become more and more
		  important in the future. Instead of generating UIs from
		  simple abstractions, transforming them from high-level
		  models should be more attractive. In particular, we let an
		  interaction designer model discourses in the sense of
		  dialogues (supported by a tool), inspired by human-human
		  communication. This tutorial informs about our approach,
		  both about its advantages and its challenges (e.g., in
		  terms of usability of generated UIs). In particular, our
		  unique approach to optimization for a given device (e.g., a
		  Smartphone) that applies Artificial Intelligence (AI)
		  techniques will be high-lighted, as well as the techniques
		  based on ontologies for automated GUI generation and
		  customization. We also address low-vision accessibility of
		  Web-pages, by combining automated design-time generation of
		  Web-pages with responsive design for improving
		  accessibility.},
  booktitle	= {Companion Proceedings of the 26th International Conference
		  on Intelligent User Interfaces},
  pages		= {3–4},
  numpages	= {2},
  keywords	= {Interaction design, automated GUI generation,
		  customization, low-vision accessibility of Web-pages,
		  discourse models, task models},
  location	= {College Station, TX, USA},
  series	= {IUI '21 Companion}
}

@InProceedings{	  10.1145/3511808.3557617,
  author	= {Anelli, Vito Walter and Biancofiore, Giovanni Maria and De
		  Bellis, Alessandro and Di Noia, Tommaso and Di Sciascio,
		  Eugenio},
  title		= {Interpretability of BERT Latent Space through Knowledge
		  Graphs},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557617},
  doi		= {10.1145/3511808.3557617},
  abstract	= {The advent of pretrained language have renovated the ways
		  of handling natural languages, improving the quality of
		  systems that rely on them. BERT played a crucial role in
		  revolutionizing the Natural Language Processing (NLP) area.
		  However, the deep learning framework it implements lacks
		  interpretability. Thus, recent research efforts aimed to
		  explain what BERT learns from the text sources exploited to
		  pre-train its linguistic model. In this paper, we analyze
		  the latent vector space resulting from the BERT
		  context-aware word embeddings. We focus on assessing
		  whether regions of the BERT vector space hold an explicit
		  meaning attributable to a Knowledge Graph (KG). First, we
		  prove the existence of explicitly meaningful areas through
		  the Link Prediction (LP) task. Then, we demonstrate these
		  regions being linked to explicit ontology concepts of a KG
		  by learning classification patterns. To the best of our
		  knowledge, this is the first attempt at interpreting the
		  BERT learned linguistic knowledge through a KG relying on
		  its pretrained context-aware word embeddings.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {3806–3810},
  numpages	= {5},
  keywords	= {deep learning, knowledge graphs, natural language
		  processing},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.5555/3507788.3507811,
  author	= {Kici, Derya and Bozanta, Aysun and Cevik, Mucahit and
		  Parikh, Devang and Ba\c{s}ar, Ay\c{s}e},
  title		= {Text classification on software requirements
		  specifications using transformer models},
  year		= {2021},
  publisher	= {IBM Corp.},
  address	= {USA},
  abstract	= {Text classification in Software Requirements
		  Specifications (SRS) documents is an essential task for
		  various purposes including automatically extracting
		  requirements and their types as well as identification of
		  duplicate or conflicting information, which all contribute
		  to avoiding potential issues in the later stages of the
		  software development life cycle. While a variety of machine
		  learning approaches have been considered for text
		  classification over SRS documents, many of these fail to
		  provide adequate performance as they often ignore the
		  meaning of software artifacts or integrate domain knowledge
		  for the classification task. Recent advances in deep
		  learning methodology have significantly contributed to
		  Natural Language Processing (NLP) and text classification.
		  One of the main challenges in using deep learning models
		  for various NLP tasks in the software engineering domain is
		  the scarcity of labeled textual data. In addition, even
		  with sufficient data, training from the scratch still
		  requires significant training time and computational
		  resources. Transfer learning is a novel approach that
		  proposes a solution to such reservations by providing
		  pre-trained models that enable fine-tuning with the
		  customized data. In this research, we conduct an empirical
		  analysis on multi-class text classification over SRS
		  documents using different pre-trained transformer models
		  including BERT, DistilBERT, Roberta, AlBERT, and XLNet, and
		  compare their performance. We test the performance of these
		  models using three SRS datasets: DOORS, NFR-PROMISE, and
		  PURE. Our numerical study shows that the transformer models
		  are able to generate highly accurate results to classify
		  all categories except Priority of the requirements. While
		  all models provide a 80\% or higher accuracy for other
		  classification tasks, the accuracy of the models to
		  classify the Priority does not exceed 60\%.},
  booktitle	= {Proceedings of the 31st Annual International Conference on
		  Computer Science and Software Engineering},
  pages		= {163–172},
  numpages	= {10},
  keywords	= {BERT, NLP, software requirement specifications, text
		  classification, transfer learning},
  location	= {Toronto, Canada},
  series	= {CASCON '21}
}

@InProceedings{	  10.1145/3177148.3180086,
  author	= {Refoufi, Allaoua and Benarab, Achref},
  title		= {A Robust Approach to the Ontology Matching Problem},
  year		= {2018},
  isbn		= {9781450352901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3177148.3180086},
  doi		= {10.1145/3177148.3180086},
  abstract	= {Ontology matching is the process that identifies
		  correspondences between similar concepts in two different
		  ontologies of the same domain of discourse to solve
		  knowledge heterogeneous problems. We propose an automatic
		  similarity based matching algorithm that exploits almost
		  all types of entities descriptions as well as their
		  relations to effectively compute the correspondences
		  between the two to be matched ontologies. The iterative
		  algorithm computes each measure of similarity separately
		  and then aggregates them in a linear combination to compose
		  the final similarity score. The measures used deal with
		  linguistic, semantic, and structural as well as many other
		  measures to gain efficiency. We also include a new
		  similarity measure based on dynamic programming in
		  conjunction with known measures to refine the similarity
		  process. Finally, we provide comparative experimental
		  results in support of our method on several well-known
		  ontology benchmarks recommended by the OAEI1. The results
		  obtained are shown to be quite superior compared to the
		  state-of-the-art ontology matching systems.},
  booktitle	= {Proceedings of the 2nd Mediterranean Conference on Pattern
		  Recognition and Artificial Intelligence},
  pages		= {76–83},
  numpages	= {8},
  keywords	= {OWL/XML, Ontology matching, WordNet, ontology alignment,
		  similarity measure},
  location	= {Rabat, Morocco},
  series	= {MedPRAI '18}
}

@InProceedings{	  10.1145/3637494.3637508,
  author	= {Li, Xuxin and Wang, Ge and Wang, Yue and Zhou, Qirui},
  title		= {Mixed Knowledge-enhance Empathetic Dialogue Generation},
  year		= {2024},
  isbn		= {9798400716300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637494.3637508},
  doi		= {10.1145/3637494.3637508},
  abstract	= {Empathy plays a pivotal role in human communication, and
		  thus, it is an essential capability that any human-centered
		  dialogue system should possess. Early research on
		  empathetic response generation often focused on directly
		  capturing the emotional state of the context using fixed
		  emotion labels. However, the logical aspects exhibited in
		  human conversations heavily rely on experiential and
		  knowledge-based resources within the brain. This implies
		  that whether the aim is to acquire more nuanced emotional
		  states or to generate responses enriched with comprehensive
		  information, the incorporation of external knowledge as
		  supplementary information in empathetic dialogue systems is
		  imperative. In response to this challenge, we propose a
		  novel approach for extracting external knowledge. This is
		  achieved by designing two components: a fine-grained
		  knowledge graph constructed using the context and an
		  external knowledge base, and coarse-grained knowledge
		  acquisition based on COMET. These two scales of knowledge
		  are then integrated with the context using methods like
		  context refinement. This not only make the model to gain a
		  deeper understanding of the user's context but also
		  enhances the expression of empathy in the dialogue system.
		  We conducted extensive experiments on the
		  EMPATHETICDIALOGUES dataset and demonstrated the
		  superiority of our approach over the baseline model.CCS
		  CONCEPTS • Computing methodologies∼Artificial
		  intelligence∼Natural language processing∼Discourse,
		  dialogue and pragmatics},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Electronics, Computers and Communication Technology},
  pages		= {77–81},
  numpages	= {5},
  keywords	= {Natural Language Processing, deep learning, dialog system,
		  empathetic dialogue generation, external knowledge},
  location	= {Guilin, China},
  series	= {CECCT '23}
}

@InProceedings{	  10.1145/3230905.3230941,
  author	= {Guermah, Hatim and Fissaa, Tarik and Guermah, Bassma and
		  Hafiddi, Hatim and Nassar, Mahmoud},
  title		= {Using Context Ontology and Linear SVM for Chronic Kidney
		  Disease Prediction},
  year		= {2018},
  isbn		= {9781450353045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3230905.3230941},
  doi		= {10.1145/3230905.3230941},
  abstract	= {In the e-Health learning area, the use of chronic patient
		  context has become very important given the increase in the
		  number of individuals who suffer from these diseases and
		  the unavailability of medications. Specifically, chronic
		  kidney failure is one of the diseases that goes undetected
		  and undiagnosed until it is well advanced. The need for
		  preventive prediction remains an essential task for the
		  well-being of patients at risk. In this paper, we aim to
		  explore the added value of using ontology-based prediction,
		  focusing on Linear SVM, to deal with Chronic Kidney Disease
		  Problems.},
  booktitle	= {Proceedings of the International Conference on Learning
		  and Optimization Algorithms: Theory and Applications},
  articleno	= {48},
  numpages	= {6},
  keywords	= {Chronic Kidney Disease, Context-Awareness, Linear SVM,
		  Ontologies},
  location	= {Rabat, Morocco},
  series	= {LOPAL '18}
}

@InProceedings{	  10.1109/models-c.2019.00020,
  author	= {Rossi, Maria Teresa and De Sanctis, Martina and Iovino,
		  Ludovico and Rutle, Adrian},
  title		= {A multilevel modelling approach for tourism flows
		  detection},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00020},
  doi		= {10.1109/MODELS-C.2019.00020},
  abstract	= {Application development in the Internet of Things (IoT)
		  faces various issues such as lack of separation of concerns
		  and lack of high-level abstraction to address its large
		  scale and heterogeneity. MDE supports the management of
		  this heterogeneity raising the level of abstraction and
		  thanks to its core operations. Multilevel modelling makes
		  it possible to extend MDE techniques to more than two
		  meta-levels permitting model elements to have a dual
		  type-instance dimension, making it particularly suitable
		  for this application domain. People flow monitoring and
		  detection is one of the hot topics in smart cities
		  projects. In this paper, we exploit MDE techniques, through
		  multilevel modelling approaches, to design the
		  infrastructure supporting a solution part of a
		  comprehensive project related to urban informatics.
		  Moreover, even if we target the people flow monitoring and
		  detection scenario, the provided multilevel approach is
		  open and extensible to further IoT scenarios, to
		  specifically manage the evolutionary nature of the IoT.},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {103–112},
  numpages	= {10},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.1145/3456172.3456211,
  author	= {Hananto, Valentinus R. and Serd\"{u}lt, Uwe and Kryssanov,
		  Victor},
  title		= {A Tourism Knowledge Model through Topic Modeling from
		  Online Reviews},
  year		= {2021},
  isbn		= {9781450388450},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3456172.3456211},
  doi		= {10.1145/3456172.3456211},
  abstract	= {Ontologies and knowledge models have gained more
		  recognition because of their extensive use in recommender
		  systems. The lack of automatic approaches in ontology
		  engineering, however, becomes a challenge to fulfill
		  increasing needs for such knowledge models in the field of
		  tourism. In this study, a system for building tourism
		  knowledge models from online reviews is proposed. The main
		  contribution of the study is the application of topic
		  modeling to build a knowledge model that, in turn, allows
		  for an automated labeling process to train classifiers.
		  Given a collection of unlabeled tourism online reviews,
		  Latent Dirichlet Allocation (LDA) is applied to
		  automatically label each document. Each topic discovered by
		  LDA is labeled with one specific category, representing its
		  semantic meaning based on an existing general ontology as a
		  reference. These automatically labeled documents are used
		  for classification, and the result is compared with manual
		  annotation. Experiments on Indonesian tourism datasets
		  showed that the automatic labeling approach using LDA
		  provides for a precision score of 70\%. In classification
		  tasks, this approach can achieve comparable or even better
		  classification performance than the manual labeling. The
		  results obtained suggest that the developed system is
		  capable of building a tourism knowledge model and providing
		  acceptable-quality training data for the development of
		  tourism recommender systems.},
  booktitle	= {Proceedings of the 2021 7th International Conference on
		  Computing and Data Engineering},
  pages		= {87–93},
  numpages	= {7},
  keywords	= {recommender systems, topic modeling, tourism knowledge
		  model},
  location	= {Phuket, Thailand},
  series	= {ICCDE '21}
}

@Article{	  10.14778/3681954.3682023,
  author	= {Ke, Jin and Zacouris, Zenon and Acosta, Maribel},
  title		= {Efficient Validation of SHACL Shapes with Reasoning},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {11},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3681954.3682023},
  doi		= {10.14778/3681954.3682023},
  abstract	= {As the usage of knowledge graphs (KGs) becomes more
		  pervasive in practical applications, there is a burgeoning
		  need for high-quality data. The SHApes Constraint Language
		  (SHACL) allows for expressing certain types of quality
		  constraints that define sub-structures and correct values
		  in KGs modelled with RDF. Nevertheless, performing SHACL
		  validation without entailment often yields onesided
		  outcomes, as it falls short of validating crucial implicit
		  data encoded in the KG ontology. Current solutions that
		  incorporate entailment into SHACL validation are
		  inefficient, due to the time-intensive process of applying
		  inference rules to the entire dataset. Moreover, applying
		  entailment for SHACL validation can generate large amounts
		  of redundant triples, exacerbating the validation workload
		  and resulting in erroneous or redundant validation results.
		  In light of these challenges, we propose Re-SHACL, an
		  approach that combines targeted reasoning and entity
		  merging techniques to generate a concise, consolidated RDF
		  graph devoid of redundancy. Re-SHACL significantly reduces
		  execution time and improves the accuracy of the validation
		  reports. Our experiments demonstrate that Re-SHACL can be
		  combined with state-of-the-art validators to deliver
		  accurate validation reports efficiently.},
  journal	= {Proc. VLDB Endow.},
  month		= jul,
  pages		= {3589–3601},
  numpages	= {13}
}

@InProceedings{	  10.5555/3586210.3586257,
  author	= {Schroeder, Shane A. and Vendome, Christopher and
		  Giabbanelli, Philippe J. and Montfort, Alan M.},
  title		= {Towards Reusable Building Blocks to Develop Covid-19
		  Simulation Models},
  year		= {2023},
  publisher	= {IEEE Press},
  abstract	= {Modeling \&amp; Simulation has played an essential role in
		  supporting the decision-making activities of policymakers
		  for COVID-19. However, a proliferation of models has been
		  noted in the literature, and new models are only more
		  likely to emerge given the shift to long-term management of
		  the disease and the call for highly tailored tools. Having
		  a multiplicity of models can have benefits, for example
		  when contributing to ensembles of models. However, if each
		  model is created from scratch, there is significant
		  redundancy in efforts hence time inefficiency and a
		  heightened risk of bugs. Our study examines the naturally
		  occurring practices of modelers who wrote COVID-19 models
		  in NetLogo to identify redundancy in code and thus suggest
		  reusable 'building blocks' that would speed-up the process
		  of model development as well as improving code quality.
		  Based on 28 models, we identified five themes and discussed
		  their transformation into potential building blocks for
		  simulation.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {569–580},
  numpages	= {12},
  location	= {Singapore, Singapore},
  series	= {WSC '22}
}

@InProceedings{	  10.1145/3616855.3635724,
  author	= {Dave, Vachik S. and Pang, Linsey and Cui, Xiquan and Luo,
		  Chen and Zamani, Hamed and Wu, Lingfei and Karypis,
		  George},
  title		= {The 3rd International Workshop on Interactive and Scalable
		  Information Retrieval Methods for eCommerce (ISIR-eCom
		  2024)},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3635724},
  doi		= {10.1145/3616855.3635724},
  abstract	= {Over the past few years, consumer behavior has shifted
		  from traditional in-store shopping to online shopping. For
		  example, eCommerce sales have grown from around 5\% of
		  total US sales in 2012 to around 15.4\% in year 2023. This
		  rapid growth of eCommerce has created new challenges and
		  vital new requirements for intelligent information
		  retrieval systems. Which lead to the primary motivations of
		  this workshop:(1) Since the pandemic hit, eCommerce became
		  an important part of people's routine and they started
		  using online shop- ping for smallest grocery items to big
		  electronics as well as cars. With such a large assortment
		  of products and millions of users, achieving higher
		  scalability without losing accuracy is a leading concern
		  for information retrieval systems for eCommerce.(2) The
		  diverse buyers make the relevance of the results highly
		  subjective, because relevance varies for different buyers.
		  The most suitable and intuitive solution to this problem is
		  to make the system interactive and provide correct
		  relevance for different users. Hence, interactive
		  information retrieval systems are becoming necessity in
		  eCommerce.(3) To handle sudden change in buyers' behavior,
		  industries adopted existing sub-optimal information
		  retrieval techniques for various eCommerce tasks.
		  Parallelly, they also started exploring/researching for
		  better solutions and in dire need of help from research
		  community.This workshop will provide a forum to discuss and
		  learn the latest trends for interactive and scalable
		  information retrieval approaches for eCommerce. It will
		  provide academic and industrial researchers a platform to
		  present their latest works, share research ideas, present
		  and discuss various challenges, and identify the areas
		  where further research is needed. It will foster the
		  development of a strong research community focused on
		  solving eCommerce-related information retrieval problems
		  that provide superior eCommerce experience to all users.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {1208–1209},
  numpages	= {2},
  keywords	= {ecommerce search, information retrieval, interactive
		  systems, large language models (llms) in ecommerce, natural
		  language processing (nlp) for ecommerce, ranking models,
		  recommender systems},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@Article{	  10.1145/3745789,
  author	= {Abraham, Joel and Austin, Mark and Gilbert, Mark R. and
		  Celiku, Orieta},
  title		= {Semantic Foundations for Precision Medicine},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3745789},
  doi		= {10.1145/3745789},
  abstract	= {Precision medicine, which aims to optimize medical care at
		  the individual level, remains a significant challenge and
		  aspiration in oncology. The pathway to a successful
		  implementation requires methods that can work with a vast
		  heterogeneity of cancer, and consider the interplay of
		  environmental, societal, biological, and clinical factors.
		  To support decision-making in this context, computational
		  frameworks must integrate large-scale, diverse, and noisy
		  data, discover fine-grained patient subgroups with shared
		  underlying characteristics, and characterize the imperfect
		  preclinical spaces where novel therapies are tested. We
		  propose an integrated digital-twin framework in which
		  machine learning and semantic models collaboratively
		  represent and reason with diverse patient data and medical
		  domain knowledge to generate treatment recommendations.
		  Clinical and molecular characteristics are used to discover
		  subtypes of brain cancers, which are represented as
		  ontologies with associated rules to determine a patient’s
		  membership in a given subtype. Similarly, preclinical
		  models used for therapeutic testing are characterized and
		  assessed for their similarity to patient cancer models. By
		  semantically discovering links between these preclinical
		  models and patient cancer subtypes, novel therapeutics
		  tested on preclinical models can be prioritized and
		  hypothesized for individual patients. This approach, which
		  requires empirical testing, demonstrates how cross-domain
		  reasoning can be used to propose individualized treatment
		  plans.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= jun,
  keywords	= {Digital Twins, Semantic Models, Ontologies, Machine
		  Learning}
}

@InProceedings{	  10.1145/3281375.3281378,
  author	= {Saleme, Est\^{e}v\~{a}o B. and Santos, Celso A. S. and
		  Falbo, Ricardo A. and Ghinea, Gheorghita and Andres,
		  Frederic},
  title		= {Towards a reference ontology on mulsemedia systems},
  year		= {2018},
  isbn		= {9781450356220},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3281375.3281378},
  doi		= {10.1145/3281375.3281378},
  abstract	= {The use of multiple senses in interactive applications has
		  become increasingly feasible due to the upsurge of
		  commercial, off-the-shelf devices to produce sensory
		  effects. Creating Multiple Sensorial Media (MulSeMedia)
		  immersive systems requires understanding their digital
		  ecosystem. Mulsemedia systems encompass a set of
		  applications, and devices of different types assembled to
		  communicate or express feelings from the virtual world to
		  the real world. Despite existing standards, tools, and
		  recent research devoted to them, there is still a lack of
		  formal and explicit representation of what mulse-media is.
		  Misconceptions could eventually lead to the construction of
		  solutions that might not take into account reuse,
		  integration, standardization, among other design features.
		  In this paper, we propose to establish a common
		  conceptualization about mulsemedia systems through a
		  reference ontology, named MulseOnto, covering their main
		  notions. To evaluate it, we applied ontology verification
		  and validation techniques, including assessment by humans
		  and a data-driven approach. The results showed that
		  MulseOnto can be used as a consensual conceptual model for
		  exploring the knowledge about the whole chain of mulsemedia
		  systems.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Management of Digital EcoSystems},
  pages		= {23–30},
  numpages	= {8},
  keywords	= {mulsemedia systems, multimedia, reference ontology},
  location	= {Tokyo, Japan},
  series	= {MEDES '18}
}

@InProceedings{	  10.1145/3687997.3695635,
  author	= {Esterhuyse, Christopher A. and van Binsbergen, L. Thomas},
  title		= {Cooperative Specification via Composition Control},
  year		= {2024},
  isbn		= {9798400711800},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3687997.3695635},
  doi		= {10.1145/3687997.3695635},
  abstract	= {High-level, declarative specification languages are
		  typically highly modular: specifications are comprised of
		  fragments that are themselves meaningful. As such, complex
		  specifications are built from incrementally composed
		  fragments. In a cooperative specification, different
		  fragments are contributed by different agents, usually
		  capturing requirements on different facets of the system.
		  For example, legal regulators and system administrators
		  cooperate to specify the behaviour of a data exchange
		  system. In practice, cooperative specification is
		  difficult, as different contributors' requirements are
		  difficult to elicit, express, and compose. In this work, we
		  characterise cooperative specification and adopt an
		  approach that leverages language features specifically
		  introduced for controlling specification composition. In
		  our approach, specifications model the domain as usual, but
		  also specify how specifications may change. For example, a
		  legal regulator defines 'consent to process data' and
		  specifies which agents may consent, and which relaxations
		  of the requirement are permitted. We propose and
		  demonstrate generic language extensions that improve
		  composition control in three case study languages: Datalog,
		  Alloy, and eFLINT. We reflect on how these extensions
		  improve composition control, and afford new data exchange
		  scenarios. Finally, we relate our contributions to existing
		  works, and to the greater vision of multi-agent data
		  exchange to the satisfaction of their shared, complex,
		  dynamic requirements.},
  booktitle	= {Proceedings of the 17th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  pages		= {2–15},
  numpages	= {14},
  keywords	= {Data Exchange, Meta-Programming, Program Composition,
		  Program Refinement, Specification Languages},
  location	= {Pasadena, CA, USA},
  series	= {SLE '24}
}

@Article{	  10.1145/3401027,
  author	= {Rizvi, Syed Zain Raza and Fong, Philip W. L.},
  title		= {Efficient Authorization of Graph-database Queries in an
		  Attribute-supporting ReBAC Model},
  year		= {2020},
  issue_date	= {November 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {4},
  issn		= {2471-2566},
  url		= {https://doi.org/10.1145/3401027},
  doi		= {10.1145/3401027},
  abstract	= {Neo4j is a popular graph database that offers two
		  versions: an enterprise edition and a community edition.
		  The enterprise edition offers customizable Role-based
		  Access Control features through custom developed
		  procedures, while the community edition does not offer any
		  access control support. Being a graph database, Neo4j
		  appears to be a natural application for Relationship-Based
		  Access Control (ReBAC), an access control paradigm where
		  authorization decisions are based on relationships between
		  subjects and resources in the system (i.e., an
		  authorization graph). In this article, we present AReBAC,
		  an attribute-supporting ReBAC model for Neo4j that provides
		  finer-grained access control by operating over resources
		  instead of procedures. AReBAC&nbsp;employs Nano-Cypher, a
		  declarative policy language based on Neo4j’s Cypher query
		  language, the result of which allows us to weave database
		  queries with access control policies and evaluate both
		  simultaneously. Evaluating the combined query and policy
		  produces a result that (i) matches the search criteria, and
		  (ii) the requesting subject is authorized to access.
		  AReBAC&nbsp;is accompanied by the algorithms and their
		  implementation required for the realization of the
		  presented ideas, including GP-Eval, a query evaluation
		  algorithm. We also introduce Live-End Backjumping (LBJ), a
		  backtracking scheme that provides a significant performance
		  boost over conflict-directed backjumping for evaluating
		  queries. As demonstrated in our previous work, the original
		  version of GP-Eval already performs significantly faster
		  than the Neo4j’s Cypher evaluation engine. The optimized
		  version of GP-Eval, which employs LBJ, further improves the
		  performance significantly, thereby demonstrating the
		  capabilities of the technique.},
  journal	= {ACM Trans. Priv. Secur.},
  month		= jul,
  articleno	= {18},
  numpages	= {33},
  keywords	= {Neo4j, Relationship-based access control, attributes,
		  graph database, graph patterns, live-end backjumping,
		  nano-cypher}
}

@InProceedings{	  10.1145/3502223.3502746,
  author	= {Rusmawati, Yanti},
  title		= {Automated Reasoning on Machine Learning Model of
		  Legislative Election Prediction},
  year		= {2022},
  isbn		= {9781450395656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3502223.3502746},
  doi		= {10.1145/3502223.3502746},
  abstract	= {Prediction models using machine learning have been
		  utilized in various fields, including the general election
		  prediction model. However, we still need more insight into
		  the model result through explainable AI. To reason the
		  result, in this in-use paper, here we compare two
		  approaches: using ontology reasoner Prot\'{e}g\'{e} and
		  Silas (a machine learning tool empowered with automated
		  reasoning). Using the data set of the Indonesia legislative
		  election in 2019, we build the prediction model, followed
		  by extracting the formula from the decision tree then
		  reasoning the model predicates. The result shows that to
		  some extent we can have a better understanding of the
		  reasonable result from the machine learning prediction
		  model.},
  booktitle	= {Proceedings of the 10th International Joint Conference on
		  Knowledge Graphs},
  pages		= {200–204},
  numpages	= {5},
  keywords	= {automated reasoning, explainable AI, machine learning},
  location	= {Virtual Event, Thailand},
  series	= {IJCKG '21}
}

@InProceedings{	  10.1145/3366423.3380229,
  author	= {Andresel, Medina and Corman, Julien and Ortiz, Magdalena
		  and Reutter, Juan L. and Savkovic, Ognjen and Simkus,
		  Mantas},
  title		= {Stable Model Semantics for Recursive SHACL},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380229},
  doi		= {10.1145/3366423.3380229},
  abstract	= {SHACL (SHape Constraint Language) is a W3C recommendation
		  for validating graph-based data against a set of
		  constraints (called shapes). Importantly, SHACL allows to
		  define recursive shapes, i.e. a shape may refer to itself,
		  directly of indirectly. The recommendation left open the
		  semantics of recursive shapes, but proposals have emerged
		  recently to extend the official semantics to support
		  recursion. These proposals are based on the principle of
		  possibility (or non-contradiction): a graph is considered
		  valid against a schema if one can assign shapes to nodes in
		  such a way that all constraints are satisfied. This
		  semantics is not constructive, as it does not provide
		  guidelines about how to obtain such an assignment, and it
		  may lead to unfounded assignments, where the only reason to
		  assign a shape to a node is that it allows validating the
		  graph. In contrast, we propose in this paper a stricter,
		  more constructive semantics for SHACL, based on stable
		  models, which are well-known in Answer Set Programming
		  (ASP). This semantics additionally requires a shape
		  assignment to be properly justified by the input
		  constraints. We further exploit the connection to logic
		  programming, and show that SHACL constraints can be
		  naturally represented as logic programs, and that the
		  validation problem for a graph and a SHACL schema can be
		  encoded as an ASP reasoning task. The proposed semantics
		  also enjoys computationally tractable validation in the
		  presence of constraints with stratified negation (as
		  opposed to the previous semantics). We also extend our
		  semantics to 3-valued stable models, which yields a more
		  relaxed notion of validation, tolerant to certain faults in
		  the schema or data. By exploiting a connection between
		  3-valued stable model semantics and the well-founded
		  semantics for logic programs, we can use our translation
		  into ASP to show another tractability result. Finally, we
		  provide a preliminary evaluation of the approach, which
		  leverages an ASP solver to perform graph validation.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {1570–1580},
  numpages	= {11},
  keywords	= {SHACL, answer set programming, graph-structured data},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@Article{	  10.1145/3342356,
  author	= {Harikrishna, D. M. and Rao, K. Sreenivasa},
  title		= {Children’s Story Classification in Indian Languages
		  Using Linguistic and Keyword-based Features},
  year		= {2019},
  issue_date	= {March 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3342356},
  doi		= {10.1145/3342356},
  abstract	= {The primary objective of this work is to classify Hindi
		  and Telugu stories into three genres: fable, folk-tale, and
		  legend. In this work, we are proposing a framework for
		  story classification (SC) using keyword and part-of-speech
		  (POS) features. For improving the performance of SC system,
		  feature reduction techniques and combinations of various
		  POS tags are explored. Further, we investigated the
		  performance of SC by dividing the story into parts
		  depending on its semantic structure. In this work, stories
		  are (i) manually divided into parts based on their
		  semantics as introduction, main, and climax; and (ii)
		  automatically divided into equal parts based on number of
		  sentences in a story as initial, middle, and end. We have
		  also examined sentence increment model, which aims at
		  determining an optimum number of sentences required to
		  identify story genre by incremental selection of sentences
		  in a story. Experiments are conducted on Hindi and Telugu
		  story corpora consisting of 300 and 150 short stories,
		  respectively. The performance of SC system is evaluated
		  using different combinations of keyword and POS-based
		  features, with three well-established machine learning
		  classifiers: (i) Naive Bayes (NB), (ii) k-Nearest Neighbour
		  (KNN), and (iii) Support Vector Machine (SVM). Performance
		  of the classifier is evaluated using 10-fold
		  cross-validation and effectiveness of classifier is
		  measured using precision, recall, and F-measure. From the
		  classification results, it is observed that adding
		  linguistic information boosts the performance of story
		  classification. In view of the structure of the story,
		  main, and initial parts of the story have shown
		  comparatively better performance. The results from the
		  sentence incremental model have indicated that the first
		  nine and seven sentences in Hindi and Telugu stories,
		  respectively, are sufficient for better classification of
		  stories. In most of the studies, SVM models outperformed
		  the other models in classification accuracy.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {30},
  numpages	= {22},
  keywords	= {K-nearest neighbour, keyword features, latent semantic
		  analysis, linear discriminant analysis, linguistic
		  features, naive Bayes, sparse representation, story
		  classification, support vector machines, text-to-speech,
		  vector space model}
}

@Article{	  10.5555/3580619.3580624,
  author	= {Feng, Xingzhan and Periyasamy, Kasi},
  title		= {A Cost Estimation Model for Scrum Projects},
  year		= {2022},
  issue_date	= {November 2022},
  publisher	= {Consortium for Computing Sciences in Colleges},
  address	= {Evansville, IN, USA},
  volume	= {38},
  number	= {4},
  issn		= {1937-4771},
  abstract	= {One of the daunting tasks of software developers is to
		  estimate the development cost of a new software product.
		  Most cost estimation models use the set of requirements for
		  the product as the starting point. Function-point, COCOMO
		  and use case-based cost estimation models belong to this
		  category. These models assume that the requirements of the
		  product are fairly rigid. However, with the advent of
		  agile-based software development methods, the requirements
		  keep changing during the development process. Therefore,
		  traditional cost estimation models need to be refined to
		  accommodate changes in requirements. The refinements should
		  reflect the changes in cost when the requirements change.
		  In this paper, we describe a cost estimation model for
		  projects that use scrum, a popular agile method. The model
		  uses the requirements of the new software product, written
		  in the form of user stories, as the primary source. The
		  cost is adjusted every time the requirements are changed or
		  new requirements are introduced. We have also developed a
		  project tracking tool for scrum projects in which this
		  model has been implemented. The model was applied to
		  academic projects developed by graduate students; the
		  results indicate that estimations are fairly reasonable.},
  journal	= {J. Comput. Sci. Coll.},
  month		= nov,
  pages		= {30–37},
  numpages	= {8}
}

@Article{	  10.1145/3705313,
  author	= {Dang, Xiaochao and Ding, Guozhen and Dong, Xiaohui and Li,
		  Fenfang and Gao, Shiwei and Wang, Yue},
  title		= {UIE-Based Relational Extraction Task for Mine Hoist Fault
		  Data},
  year		= {2025},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3705313},
  doi		= {10.1145/3705313},
  abstract	= {Information extraction is pivotal in natural language
		  processing, where the goal is to convert unstructured text
		  into structured information. A significant challenge in
		  this domain is the diversity and specific needs of various
		  processing tasks. Traditional approaches typically utilize
		  separate frameworks for different information extraction
		  tasks, such as named entity recognition and relationship
		  extraction, which hampers their uniformity and scalability.
		  In this study, this study introduce a Universal Information
		  Extraction (UIE) framework combined with a cue learning
		  strategy, significantly improving the efficiency and
		  accuracy of extracting mine hoist fault data. Initially,
		  domain-specific data is manually labeled to fine-tune the
		  model, and the accuracy is further enhanced by constructing
		  negative examples during this fine-tuning process. The
		  model then focuses on faults using the Structured
		  Extraction Language (SEL) and a schema-based prompt syntax,
		  the Structural Schema Instructor (SSI), which targets and
		  extracts key information from the fault data to meet
		  specific domain requirements. Experimental results show
		  that UIE substantially improves the processing efficiency
		  and the F1 accuracy of the extracted mine hoist fault data,
		  with the fine-tuned F1 score increasing from 23.59\% to
		  92.51\%.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  articleno	= {4},
  numpages	= {23},
  keywords	= {Joint extraction, mechanical problem, mining sector,
		  prompt learning}
}

@InProceedings{	  10.1145/3424978.3425095,
  author	= {Shi, Yu and Qin, Qiuli},
  title		= {Construction of Neurosurgery Knowledge Graph Based on
		  Bi-LSTM-CRF Model},
  year		= {2020},
  isbn		= {9781450377720},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3424978.3425095},
  doi		= {10.1145/3424978.3425095},
  abstract	= {Medical knowledge is complex. The knowledge graph provides
		  an efficient solution for integrating medical knowledge and
		  analyzing medical data. In this paper, the neurosurgery
		  knowledge graph is constructed. First, the ontology pattern
		  library is constructed, and then named entity recognition
		  is performed based on the Bi-LSTM-CRF model. The entities
		  are extracted from the text and the entity relationships
		  are defined. The knowledge graph of the
		  symptom-disease-department is integrated, and finally
		  stored in Neo4j Graph database. As the basis of information
		  retrieval, intelligent question answering, diagnosis and
		  treatment system, knowledge graph can effectively transfer
		  medical knowledge and provide intelligent medical
		  assistance for doctors and patients.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Computer Science and Application Engineering},
  articleno	= {113},
  numpages	= {5},
  keywords	= {Bi-LSTM-CRF model, Knowledge graph, Neurosurgery},
  location	= {Sanya, China},
  series	= {CSAE '20}
}

@InProceedings{	  10.5555/3586210.3586387,
  author	= {Wilsdorf, Pia and Uhrmacher, Adelinde M.},
  title		= {Creating PROV-DM Graphs from Model Databases},
  year		= {2023},
  publisher	= {IEEE Press},
  abstract	= {Documenting the provenance of the main products of a
		  simulation study plays a crucial role in improving the
		  understanding of mechanistic, biological models as well as
		  their reproducibility and credibility. With model databases
		  already an ample collection of simulation models, including
		  metainformation and source files, exists. In this paper, we
		  bridge the gap between the information contained in model
		  databases and the PROV-DM provenance standard, which allows
		  making the diverse products and their relationships
		  formally explicit. We present a procedure for creating
		  PROV-DM graphs from model database entries, and illustrate
		  the approach based on ten different models from the
		  BioModels database. These case studies demonstrate the
		  advantages of having a standardized provenance view in
		  addition to the regular database entries, i.e., enhanced
		  means for visualizing the structure of the simulation study
		  and the curation process.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2118–2129},
  numpages	= {12},
  location	= {Singapore, Singapore},
  series	= {WSC '22}
}

@InProceedings{	  10.1145/3726302.3730344,
  author	= {Koleva, Aneta and Ringsquandl, Martin and Hatem, Ahmed and
		  Runkler, Thomas and Tresp, Volker},
  title		= {Wiki-TabNER: Integrating Named Entity Recognition into
		  Wikipedia Tables},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3730344},
  doi		= {10.1145/3726302.3730344},
  abstract	= {Interest in solving table interpretation tasks has grown
		  over the years, yet it still relies on existing datasets
		  that may be overly simplified. This is potentially reducing
		  the effectiveness of the dataset for thorough evaluation
		  and failing to accurately represent tables as they appear
		  in the real-world. To enrich the existing benchmark
		  datasets, we extract and annotate a new, more challenging
		  dataset. The proposed Wiki-TabNER dataset features complex
		  tables containing several entities per cell, with named
		  entities labeled using DBpedia classes. This dataset is
		  specifically designed to address named entity recognition
		  (NER) task within tables, but it can also be used as a more
		  challenging dataset for evaluating the entity linking task.
		  In this paper we describe the distinguishing features of
		  the Wiki-TabNER dataset and the labeling process. In
		  addition, we propose a prompting framework for evaluating
		  the new large language models on the within tables NER
		  task. Finally, we perform qualitative analysis to gain
		  insights into the challenges encountered by the models and
		  to understand the limitations of the proposed~dataset.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3812–3820},
  numpages	= {9},
  keywords	= {named entity recognition, table interpretation},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@Article{	  10.1145/3131610,
  author	= {Chantas, Giannis and Karavarsamis, Sotiris and
		  Nikolopoulos, Spiros and Kompatsiaris, Ioannis},
  title		= {A Probabilistic, Ontological Framework for Safeguarding
		  the Intangible Cultural Heritage},
  year		= {2018},
  issue_date	= {September 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {11},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3131610},
  doi		= {10.1145/3131610},
  abstract	= {In this article, we propose Multi-Entity Bayesian Networks
		  (MEBNs) as the probabilistic ontological framework for the
		  analysis of the Tsamiko and Salsa dances. More
		  specifically, our analysis has the objective of the dancer
		  assessment with respect to both choreography execution
		  accuracy and the synchronization of the dance movements
		  with the musical rhythm. For this task, we make use of the
		  explicit, expert-provided knowledge on dance movements and
		  their relations to the musical beat. Due to the complexity
		  of this knowledge, the MEBNs were used as the probabilistic
		  ontological framework in which the knowledge is formalized.
		  The reason we opt for MEBNs for this task is that they
		  combine Bayesian and formal (first-order) logic into a
		  single model. In this way, the Bayesian probabilistic part
		  of MEBNs was used to capture, using example data and
		  training, the implicit part of the expert knowledge about
		  dances, i.e., this part of the knowledge that cannot be
		  formalized and explicitly defined accurately enough, while
		  the logical maintains the explicit knowledge representation
		  in the same way ontologies do. Moreover, we present in
		  detail the MEBN models we built for Tsamiko and Salsa,
		  using expert-provided explicit knowledge. Last, we conduct
		  experiments that demonstrate the effectiveness of the
		  proposed MEBN-based methodology we employ to achieve our
		  analysis objectives. The results of the experiments
		  demonstrate the superiority of MEBNs to conventional
		  models, such as BNs, in terms of the dancer assessment
		  accuracy.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {12},
  numpages	= {29},
  keywords	= {Intangible cultural heritage, multi-entity bayesian
		  networks, multimodal semantic analysis}
}

@InProceedings{	  10.1145/3360901.3364443,
  author	= {Kaffee, Lucie-Aim\'{e}e and Endris, Kemele M. and Simperl,
		  Elena and Vidal, Maria-Esther},
  title		= {Ranking Knowledge Graphs By Capturing Knowledge about
		  Languages and Labels},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364443},
  doi		= {10.1145/3360901.3364443},
  abstract	= {Capturing knowledge about the mulitilinguality of a
		  knowledge graph is of supreme importance to understand its
		  applicability across multiple languages. Several metrics
		  have been proposed for describing mulitilinguality at the
		  level of a whole knowledge graph. Albeit enabling the
		  understanding of the ecosystem of knowledge graphs in terms
		  of the utilized languages, they are unable to capture a
		  fine-grained description of the languages in which the
		  different entities and properties of the knowledge graph
		  are represented. This lack of representation prevents the
		  comparison of existing knowledge graphs in order to decide
		  which are the most appropriate for a multilingual
		  application.In this work, we approach the problem of
		  ranking knowledge graphs based on their language features
		  and propose LINGVO, a framework able to capture
		  mulitilinguality at different levels of granularity.
		  Grounded in knowledge graph descriptions, LINGVO is,
		  additionally, able to solve the problem of ranking
		  knowledge graphs according to a degree of mulitilinguality
		  of the represented entities. We have empirically studied
		  the effectiveness of LINGVO in a benchmark of queries to be
		  executed against existing knowledge graphs. The observed
		  results provide evidence that LINGVO captures the
		  mulitilinguality of the studied knowledge graphs similarly
		  than a crowd-sourced gold standard.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {21–28},
  numpages	= {8},
  keywords	= {knowledge graph, multilinguality, question answering,
		  ranking},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.1145/3208806.3208830,
  author	= {Floty\'{n}ski, Jakub and Soboci\'{n}ski, Pawe\l{}},
  title		= {Semantic 4-dimensional modeling of VR content in a
		  heterogeneous collaborative environment},
  year		= {2018},
  isbn		= {9781450358002},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3208806.3208830},
  doi		= {10.1145/3208806.3208830},
  abstract	= {Interactive 3D content gains increasing use in VR/AR
		  applications in different domains, such as education,
		  training, engineering, spatial and urban planning as well
		  as architectural and interior design. While modeling and
		  presenting interactive 3D scenes in collaborative VR/AR
		  environments, different 3D objects are added, modified and
		  removed by different users, which leads to the evolution of
		  the scenes over time. Representation of VR content covering
		  temporal knowledge is essential to enable exploration of
		  such time-dependent VR/AR content. However, the available
		  approaches do not enable exploration of VR content with
		  regards to its time-dependent components and properties,
		  which limits their usage in web-based systems. The main
		  contribution of this paper is 4--dimensional representation
		  of VR content, which encompasses time being the fourth
		  dimension. The representation is based on the semantic web
		  standards and ontologies, which enable the use of domain
		  knowledge for collaborative creation and exploration of
		  content. This could improve the availability of VR/AR
		  applications to domain specialists without expertise in 3D
		  graphics and animation, thus improving the overall
		  dissemination of VR/AR on the web. The representation has
		  been implemented in a heterogeneous collaborative VR
		  environment for urban design.},
  booktitle	= {Proceedings of the 23rd International ACM Conference on 3D
		  Web Technology},
  articleno	= {11},
  numpages	= {10},
  keywords	= {3D content, collaborative design, ontologies, semantic
		  web, web 3D/VR/AR},
  location	= {Pozna\'{n}, Poland},
  series	= {Web3D '18}
}

@InProceedings{	  10.1145/3498851.3498968,
  author	= {Wang, Mengzhen and Chen, Jianhui and Lin, Shaofu},
  title		= {Medication Recommendation Based on a Knowledge-enhanced
		  Pre-training Model},
  year		= {2022},
  isbn		= {9781450391870},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3498851.3498968},
  doi		= {10.1145/3498851.3498968},
  abstract	= {More and more attention has been paid to electronic
		  medical record (EMR)-based auxiliary diagnosis and
		  treatment, in which medication recommendation is an
		  important research direction. The existing medication
		  recommendation models mainly depend on the data of
		  patients, diagnosis and medications. However, the
		  insufficient amount of clinical data with temporal
		  dependencies becomes a major obstacle. This paper proposes
		  a new knowledge-enhanced pre-training model for medication
		  recommendation. On the one hand, the classification
		  knowledge in diagnostic codes and drug codes is encoded by
		  Graph Attention Network and fused into the clinical data
		  for expanding the data content. On the other hand, a large
		  number of single visit data of EMR are used to create the
		  pre-trained visit model by a modified BERT for expanding
		  the data scale. The experimental results on EMR data from
		  more than 2,000 medical and health institutions in Hainan,
		  China show that the fusion of classification knowledge and
		  pre-training model can effectively improve the accuracy of
		  medication recommendation.},
  booktitle	= {IEEE/WIC/ACM International Conference on Web Intelligence
		  and Intelligent Agent Technology},
  pages		= {290–294},
  numpages	= {5},
  keywords	= {Electronic medical record, Graph Attention Network,
		  Medication recommendation, Pre-training model},
  location	= {Melbourne, VIC, Australia},
  series	= {WI-IAT '21}
}

@InProceedings{	  10.1145/3703323.3703698,
  author	= {Gowhar, Saliq and Kempaiah, Praveen and Kamath S, Sowmya
		  and Sugumaran, Vijayan},
  title		= {Imbalanced Multi-Class Research Article Classification
		  using Sentence Transformers and Machine Learning
		  Algorithms},
  year		= {2025},
  isbn		= {9798400711244},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703323.3703698},
  doi		= {10.1145/3703323.3703698},
  abstract	= {Categorizing scientific articles into specific research
		  fields is a challenging problem, considering the volume and
		  variety of published literature. However, existing
		  classification systems often suffer from limitations
		  regarding taxonomy or the models used for classification.
		  This article explores approaches built on Sentence
		  Transformer embeddings combined with Machine Learning
		  algorithms to classify articles into 123 predefined
		  classes, with the dataset being heavily imbalanced in
		  nature. The effectiveness of Large Language Models (LLMs)
		  for generating synthetic data is also experimented with,
		  along with synonym augmentation and SMOTE. The
		  best-performing model, the One vs Rest classifier trained
		  on MP-Net sentence embeddings with SMOTE, achieved an
		  accuracy of 77\%, and outperformed all the other models.},
  booktitle	= {Proceedings of the 8th International Conference on Data
		  Science and Management of Data (12th ACM IKDD CODS and 30th
		  COMAD)},
  pages		= {309–310},
  numpages	= {2},
  keywords	= {Document classification, Machine Learning, Sentence
		  Transformers, Natural Language Processing},
  location	= { },
  series	= {CODS-COMAD '24}
}

@InProceedings{	  10.1145/3227609.3227661,
  author	= {Shi, Ling and Roman, Dumitru},
  title		= {Ontologies for the Real Property Domain},
  year		= {2018},
  isbn		= {9781450354899},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3227609.3227661},
  doi		= {10.1145/3227609.3227661},
  abstract	= {Real property, also known as real estate, realty or
		  immovable property, is one of the most important assets for
		  the world economy. Real property data is valuable input for
		  decision makers in various domains. Real property data has
		  temporal and spatial characteristics and is distributed
		  across multiple systems. Integration of real property data
		  from legal and business systems (possibly from different
		  countries) with contextual data in related domains is a
		  challenging task that requires cross-domain knowledge. Real
		  property ontologies, capturing relevant domain knowledge in
		  a structured way, are essential in the process of
		  integrating real property data. This paper identifies key
		  aspects of the real property domain from a data integration
		  perspective, and surveys ontologies for real property and
		  its related domains. It analyzes geospatial standards for
		  representing geospatial concepts and attributes relevant to
		  real properties, the Land Administration Domain Model and
		  its implementations, and ontologies for real property
		  transactions and for real property data integration. This
		  survey aims to collect and compare existing real property
		  ontologies and conceptual models, serving as a reference
		  point for ontologies and conceptual models in the real
		  property domain.},
  booktitle	= {Proceedings of the 8th International Conference on Web
		  Intelligence, Mining and Semantics},
  articleno	= {14},
  numpages	= {8},
  keywords	= {LADM, Real property ontology, cadaster, geospatial data},
  location	= {Novi Sad, Serbia},
  series	= {WIMS '18}
}

@Article{	  10.1145/3511215,
  author	= {Alshammari, Nasser O. and Alharbi, Fawaz D.},
  title		= {Combining a Novel Scoring Approach with Arabic Stemming
		  Techniques for Arabic Chatbots Conversation Engine},
  year		= {2022},
  issue_date	= {July 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3511215},
  doi		= {10.1145/3511215},
  abstract	= {Arabic is recognized as one of the main languages around
		  the world. Many attempts and efforts have been done to
		  provide computing solutions to support the language.
		  Developing Arabic chatbots is still an evolving research
		  field and requires extra efforts due to the nature of the
		  language. One of the common tasks of any natural language
		  processing application is the stemming step. It is
		  important for developing chatbots, since it helps with
		  pre-processing the input data and it can be involved with
		  different phases of the chatbot development process. The
		  aim of this article is to combine a scoring approach with
		  Arabic stemming techniques for developing an Arabic chatbot
		  conversation engine. Two experiments are conducted to
		  evaluate the proposed solution. The first experiment is to
		  select which stemmer is more accurate when applying our
		  solution, since our algorithm can support various stemmers.
		  The second experiment was conducted to evaluate our
		  proposed approach against various machine learning models.
		  The results show that the ISRIS stemming algorithm is the
		  best fit for our solution with accuracy 78.06\%. The
		  results also indicate that our novel solution achieved an
		  F1 score of 65.5\%, while the other machine learning models
		  achieved slightly lower scores. Our study presents a novel
		  technique by combining scoring mechanisms with stemming
		  processes to produce the best answer for every query sent
		  by chatbots users compared to other approaches. This can be
		  helpful for developing Arabic chatbot and can support many
		  domains such as education, business, and health. This
		  technique is among the first techniques that developed
		  purposefully to serve the development of Arabic chatbots
		  conversation engine.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  articleno	= {84},
  numpages	= {21},
  keywords	= {Arabic language, stemming, machine learning, chatbot,
		  Natural language processing}
}

@InProceedings{	  10.1145/3178461.3178483,
  author	= {Laadidi, Yassine and Bahaj, Mohamed},
  title		= {Simplification of OWL Ontology Sources for Data
		  Warehousing},
  year		= {2018},
  isbn		= {9781450354387},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3178461.3178483},
  doi		= {10.1145/3178461.3178483},
  abstract	= {Nowadays, with the emergence of new web technologies, no
		  one could deny the necessity of including such external
		  data sources in the analysis process in order to provide
		  the necessary knowledge for companies to improve their
		  services and increase their profits. However, processing
		  data in an open environment such as the web has become too
		  difficult due to the diversity of distributed data sources
		  and incapability of machines to 'understand' the real
		  semantic of web resources. The Semantic Web (SW) provides
		  the semantic annotations to describe and link scattered
		  information over the web and facilitate inference
		  mechanisms using ontologies. Web Ontology Language (OWL) is
		  the W3C recommendation. A Data warehouse (DW) is used in
		  decision making processes to store multidimensional (MD)
		  information from heterogeneous data sources using ETL
		  (Extract, Transform and Load) techniques. In this paper, we
		  introduce firstly a simplification method of OWL inputs and
		  then we define the related MD schema. Transformation rules
		  are applied for defining multidimensional concepts over the
		  OWL graph.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Software Engineering and Information Management},
  pages		= {77–81},
  numpages	= {5},
  keywords	= {OWL ontology, data warehousing, semantic web},
  location	= {Casablanca, Morocco},
  series	= {ICSIM '18}
}

@InProceedings{	  10.1145/3428502.3428619,
  author	= {Kalogirou, Victoria and van Dooren, Sander and Dimopoulos,
		  Ilias and Charalabidis, Yannis and De-Baets, Jean-Paul and
		  Lobo, Georges},
  title		= {Linked government data hub, an ontology agnostic data
		  harvester and API},
  year		= {2020},
  isbn		= {9781450376747},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3428502.3428619},
  doi		= {10.1145/3428502.3428619},
  abstract	= {Openness and Transparency are important principles for
		  citizens and therefore for eGovernment. Most government
		  portals are designed in a way that finding relevant and
		  up-to-date information requires time and effort. Data and
		  information are scattered in different platforms in a
		  non-collaborative way, leading to a labyrinth of
		  redirections. Sharing of data and information is hindered
		  by the lack of interoperability between the multiple
		  vocabularies and formats (mostly proprietary). This paper
		  attempts to bridge those systems by proposing at a national
		  level a data hub model that aggregates and categorizes data
		  from various services. Linked data and standardized
		  vocabularies are used to gather information from multiple
		  public sectors and services. The case study reuses some
		  basic components of Joinup collaborative platform to
		  provide aggregated up-to-date information on the portal of
		  the Greek Public Administration Di@vgeia. The model has the
		  potential of a wider application in different business
		  domains and multiple Government levels.},
  booktitle	= {Proceedings of the 13th International Conference on Theory
		  and Practice of Electronic Governance},
  pages		= {779–782},
  numpages	= {4},
  keywords	= {public services, eGovernment (eGov), Linked Data (LD),
		  Interoperability (IoP), Data Hub, Application Program
		  Interface (API)},
  location	= {Athens, Greece},
  series	= {ICEGOV '20}
}

@InProceedings{	  10.1145/3373017.3373038,
  author	= {Wibowo, Adi and Davis, Joseph},
  title		= {Requirements Traceability Ontology to Support Requirements
		  Management},
  year		= {2020},
  isbn		= {9781450376976},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3373017.3373038},
  doi		= {10.1145/3373017.3373038},
  booktitle	= {Proceedings of the Australasian Computer Science Week
		  Multiconference},
  articleno	= {21},
  numpages	= {9},
  keywords	= {Requirement traceability, requirements management,
		  traceability ontology},
  location	= {Melbourne, VIC, Australia},
  series	= {ACSW '20}
}

@InProceedings{	  10.1145/3478905.3478908,
  author	= {Li, Haixia and Yan, Li},
  title		= {A Temporal RDF Model for Multi-grained Time Information
		  Modeling},
  year		= {2021},
  isbn		= {9781450390248},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3478905.3478908},
  doi		= {10.1145/3478905.3478908},
  abstract	= {With the rapid increase of temporal data, how to represent
		  and manage temporal data has become a research issue worth
		  digging in. To better represent temporal data, there have
		  been many works on adding the dimension to RDF or other
		  data representations such as relational databases. However,
		  few works pay attention to the problem of updating time
		  information in the form of triple elements in RDF. Note
		  that this not only makes it easy to express that the
		  relationship between entities is effective over a period of
		  time, but also makes it easy to express that the entities
		  themselves are effective in the time. A model of temporal
		  data representation based on RDF is proposed in this paper
		  which not only considering the validity of triples, but
		  also considering the temporal validity of the entities
		  themselves within the triples.},
  booktitle	= {2021 4th International Conference on Data Science and
		  Information Technology},
  pages		= {9–14},
  numpages	= {6},
  keywords	= {RDF, Temporal data, modeling},
  location	= {Shanghai, China},
  series	= {DSIT 2021}
}

@Article{	  10.1145/3603499,
  author	= {Sangsavate, Suntarin and Sinthupinyo, Sukree and
		  Chandrachai, Achara},
  title		= {Experiments of Supervised Learning and Semi-Supervised
		  Learning in Thai Financial News Sentiment: A Comparative
		  Study},
  year		= {2023},
  issue_date	= {July 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {7},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3603499},
  doi		= {10.1145/3603499},
  abstract	= {Sentiment classification is an instrument of natural
		  language processing tasks in text analysis to measure
		  customer feedback from given documents such as product
		  reviews, news, and texts. This research aims to experiment
		  with Thai financial news sentiment classification and
		  evaluate sentiment classification performance. In this
		  research, we show financial news sentiment classification
		  experimental results when comparing supervised and
		  semi-supervised methods. In the research methodology, we
		  use PyThaiNLP to tokenize and remove stopwords and split
		  datasets into 85\% of the training set and 15\% of the
		  testing set. Next, we classify sentiment using machine
		  learning and deep learning approaches with feature
		  extraction such as bag-of-words, term frequency–inverse
		  document frequency, and word embedding (Word2Vec and
		  Bidirectional Encoder Representations from Transformers
		  (BERT)) in given texts. The results show that support
		  vector machine with the BERT model yields the best
		  performance at 83.38\%; in contrast, the random forest
		  classifier with bag-of-words yields the worst performance
		  at 54.10\% in the machine learning approach. Another
		  experiment reveals that long short-term memory with the
		  BERT model yields the best performance at 84.07\% in
		  contrast to the convolutional neural network with
		  bag-of-words, which yields the worst performance at 69.80\%
		  in the deep learning approach. The results imply that
		  support vector machine, convolutional neural network, and
		  long short-term memory are suitable for classifying
		  sentiment in complex structure language. From this study,
		  we observe the importance of sentiment classification tools
		  between supervised and semi-supervised learning, and we
		  look forward to furthering this work.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  articleno	= {197},
  numpages	= {36},
  keywords	= {Natural language processing, semi-supervised learning,
		  sentiment classification, supervised learning, Thai
		  language}
}

@InProceedings{	  10.1145/3469213.3470316,
  author	= {Liu, Bin and Wu, Baojun and Huang, Xinxin},
  title		= {Information system development method for domain ontology
		  reuse},
  year		= {2021},
  isbn		= {9781450390200},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3469213.3470316},
  doi		= {10.1145/3469213.3470316},
  booktitle	= {2021 2nd International Conference on Artificial
		  Intelligence and Information Systems},
  articleno	= {113},
  numpages	= {5},
  location	= {Chongqing, China},
  series	= {ICAIIS 2021}
}

@Article{	  10.1145/3313801,
  author	= {Gon\c{c}ales, Lucian Jos\'{e} and Farias, Kleinner and
		  Oliveira, Toacy Cavalcante De and Scholl, Murilo},
  title		= {Comparison of Software Design Models: An Extended
		  Systematic Mapping Study},
  year		= {2019},
  issue_date	= {May 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3313801},
  doi		= {10.1145/3313801},
  abstract	= {Model comparison has been widely used to support many
		  tasks in model-driven software development. For this
		  reason, many techniques of comparing them have been
		  proposed in the last few decades. However, academia and
		  industry have overlooked a classification of currently
		  available approaches to the comparison of design models.
		  Hence, a thorough understanding of state-of-the-art
		  techniques remains limited and inconclusive. This article,
		  therefore, focuses on providing a classification and a
		  thematic analysis of studies on the comparison of software
		  design models. We carried out a systematic mapping study
		  following well-established guidelines to answer nine
		  research questions. In total, 56 primary studies (out of
		  4,132) were selected from 10 widely recognized electronic
		  databases after a careful filtering process. The main
		  results are that a majority of the primary studies (1)
		  provide coarse-grained techniques of the comparison of
		  general-purpose diagrams, (2) adopt graphs as principal
		  data structure and compare software design models
		  considering structural properties only, (3) pinpoint
		  commonalities and differences between software design
		  models rather than assess their similarity, and (4) propose
		  new techniques while neglecting the production of empirical
		  knowledge from experimental studies. Finally, this article
		  highlights some challenges and directions that can be
		  explored in upcoming studies.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {48},
  numpages	= {41},
  keywords	= {UML, model comparison, model similarity, software design
		  models}
}

@InProceedings{	  10.1145/3395035.3425206,
  author	= {Putze, Felix and Burri, Merlin and Vortmann, Lisa-Marie
		  and Schultz, Tanja},
  title		= {Model-based Prediction of Exogeneous and Endogeneous
		  Attention Shifts During an Everyday Activity},
  year		= {2021},
  isbn		= {9781450380027},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3395035.3425206},
  doi		= {10.1145/3395035.3425206},
  abstract	= {Human attention determines to a large degree how users
		  interact with technical devices and how technical artifacts
		  can support them optimally during their tasks. Attention
		  shifts between different targets, triggered through
		  changing requirements of an ongoing task or through salient
		  distractions in the environment. Such shifts mark important
		  transition points which an intelligent system needs to
		  predict and attribute to an endogenous or exogenous cause
		  for an appropriate reaction. In this paper, we describe a
		  model which performs this task through a combination of
		  bottom-up and topdown modeling components. We evaluate the
		  model in a scenario with a dynamic task in a rich
		  environment and show that the model is able to predict
		  attention future switches with a robust classification
		  performance.},
  booktitle	= {Companion Publication of the 2020 International Conference
		  on Multimodal Interaction},
  pages		= {417–425},
  numpages	= {9},
  keywords	= {attention shifts, exogenous and endogenous attention,
		  top-down and bottom-up modeling},
  location	= {Virtual Event, Netherlands},
  series	= {ICMI '20 Companion}
}

@InProceedings{	  10.1145/3640115.3640224,
  author	= {Wu, Bing and Song, Yuanbin and Cao, Jinhao},
  title		= {Automatic Generation of GIM Data Audit Rules Based on
		  Sentence Embedding Vectors},
  year		= {2024},
  isbn		= {9798400708299},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640115.3640224},
  doi		= {10.1145/3640115.3640224},
  abstract	= {The digital design results of power substations establish
		  the foundation for their running and maintaining.
		  Currently, the digital design of substations is delivered
		  in the format of Grid Information Model (GIM), an
		  alternative Building Information Modeling (BIM) format for
		  describing power grid infrastructure in China. Since the
		  correctness, compliance, and consistency of GIM data are
		  necessary conditions for information sharing and business
		  decision support, the GIM data must be audited before
		  sharing among the stakeholders. The traditional manual
		  review of GIM data is too inefficient and costly to
		  execute, and thus the power grid industry seeks automatic
		  review approaches. However, one challenge for automated
		  auditing of GIM data is the lack of auditing rules. In
		  order to establish such a rule base for GIM data auditing,
		  this study first categorizes the audit rules, and proposes
		  an XML encoding method for the audit rules. Meanwhile, the
		  methodology of converting the rules described in natural
		  language into XML is also rules proposed using the SBERT
		  model. The application of the developed tool is
		  demonstrated and verified through case studies.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Information Technologies and Electrical Engineering},
  pages		= {668–673},
  numpages	= {6},
  keywords	= {Controlled natural language, Grid information model, Model
		  audit, Natural language processing, Rule representation,
		  Sentence BERT},
  location	= {Changde, Hunan, China},
  series	= {ICITEE '23}
}

@InProceedings{	  10.1145/3472163.3472267,
  author	= {Holubova, Irena and Contos, Pavel and Svoboda, Martin},
  title		= {Multi-Model Data Modeling and Representation: State of the
		  Art and Research Challenges},
  year		= {2021},
  isbn		= {9781450389914},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3472163.3472267},
  doi		= {10.1145/3472163.3472267},
  abstract	= {Following the current trend, most of the well-known
		  database systems, being relational, NoSQL, or NewSQL,
		  denote themselves as multi-model. This industry-driven
		  approach, however, lacks plenty of important features of
		  the traditional DBMSs. The primary problem is a design of
		  an optimal multi-model schema and its sufficiently general
		  and efficient representation. In this paper, we provide an
		  overview and discussion of the promising approaches that
		  could potentially be capable of solving these issues, along
		  with a summary of the remaining open problems.},
  booktitle	= {Proceedings of the 25th International Database Engineering
		  \&amp; Applications Symposium},
  pages		= {242–251},
  numpages	= {10},
  keywords	= {Category theory, Conceptual modeling, Inter-model
		  relationships, Logical models, Multi-model data},
  location	= {Montreal, QC, Canada},
  series	= {IDEAS '21}
}

@InProceedings{	  10.1145/3184558.3191598,
  author	= {Ben Ellefi, Mohamed and Papini, Odile and Merad, Djamal
		  and Boi, Jean-Marc and Royer, Jean-Philip and Pasquet,
		  J\'{e}r\^{o}me and Sourisseau, Jean-Christophe and Castro,
		  Filipe and Nawaf, Mohammad Motasem and Drap, Pierre},
  title		= {Cultural Heritage Resources Profiling: Ontology-based
		  Approach},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3191598},
  doi		= {10.1145/3184558.3191598},
  abstract	= {Cultural heritage (CH) resources are very heterogeneous
		  since the information was collected from vast diversity of
		  cultural sites and digitally recorded in different formats.
		  With the progress of 3D technologies, photogrammetry
		  techniques become the adopted solution for representing CH
		  artifacts by turning photos from small finds, to entire
		  landscapes, into accurate 3D models. To meet knowledge
		  representation with cultural heritage photogrammetry, this
		  paper proposes an ontology-profiling method for modeling a
		  real case of archaeological amphorae. The ontological
		  profile consists of all needed information to represent a
		  CH resource including typology attributes, geo-spatial
		  information and photogrammetry process. An example
		  illustrating the applicability of this profiling method to
		  the problem of CH resources conceptualization is presented.
		  We also outline our perspectives for using ontologies in
		  data-driven science, in particular on modeling a complete
		  pipeline that manages both the photogrammetric process and
		  the archaeological knowledge.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {1489–1496},
  numpages	= {8},
  keywords	= {archaeology, cultural heritage, ontology, photogrammetry,
		  profiles},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3563359.3596661,
  author	= {Tsitseklis, Konstantinos and Stavropoulou, Georgia and
		  Zafeiropoulos, Anastasios and Thanou, Athina and
		  Papavassiliou, Symeon},
  title		= {RECBOT: Virtual Museum navigation through a Chatbot
		  assistant and personalized Recommendations},
  year		= {2023},
  isbn		= {9781450398916},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3563359.3596661},
  doi		= {10.1145/3563359.3596661},
  abstract	= {The trend for digitalization of museums has been on the
		  rise in recent years, as museums seek to make their
		  collections and exhibitions more accessible to a wider
		  audience. This has involved the use of technologies such as
		  virtual and augmented reality, online exhibits, and digital
		  archives. These digital initiatives have allowed museums to
		  reach new audiences and provide immersive experiences that
		  enhance visitors’ engagement with the exhibits. Following
		  this trend, in the current work, we propose a
		  conversational agent that assists remote visitors in
		  accessing a museum’s collection. The proposed
		  architecture includes a chatbot for user interaction that
		  employs Natural Language Processing techniques for
		  understanding the user’s input. To increase visitor
		  engagement, a hybrid recommender system is developed that
		  combines content-based and collaborative-filtering
		  components. The available data is modeled in the form of a
		  Knowledge Graph, which allows for useful insights to be
		  extracted from it.},
  booktitle	= {Adjunct Proceedings of the 31st ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {388–396},
  numpages	= {9},
  keywords	= {Natural Language Processing, chatbot, conversational
		  agent, online museum, recommender system, virtual tour},
  location	= {Limassol, Cyprus},
  series	= {UMAP '23 Adjunct}
}

@InProceedings{	  10.1145/3474124.3474169,
  author	= {Agarwal, Neha and Sikka, Geeta and Awasthi, Lalit Kumar},
  title		= {Comparative Study of Topic Modeling and Word Embedding
		  Approaches for Web Service Clustering},
  year		= {2021},
  isbn		= {9781450389204},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3474124.3474169},
  doi		= {10.1145/3474124.3474169},
  abstract	= {Vector space representation of web services plays a
		  prominent role in enhancing the performance of different
		  web service-based processes like clustering,
		  recommendation, ranking, discovery, etc. Generally, Term
		  Frequency - Inverse Document Frequency (TF-IDF) and topic
		  modeling methods are widely used for service
		  representation. In recent years, word embedding techniques
		  have attracted researchers a lot because they can map
		  services or documents based on semantic similarity. This
		  paper provides a comparative analysis of two topic modeling
		  techniques, i.e., Latent Dirichlet Allocation (LDA) and
		  Gibbs Sampling algorithm for Dirichlet Multinomial Mixture
		  (GSDMM) \&amp; two word embedding techniques, i.e.,
		  word2vec and fastText. These topic modeling and word
		  embedding techniques are applied to a dataset of web
		  service documents for vector space representation. K-Means
		  clustering is used to analyze the performance, and results
		  are evaluated based on standard evaluation criteria.
		  Results demonstrate that word2vec model outperforms other
		  techniques and provides a satisfactory improvement on
		  clustering.},
  booktitle	= {Proceedings of the 2021 Thirteenth International
		  Conference on Contemporary Computing},
  pages		= {309–313},
  numpages	= {5},
  keywords	= {K-Means Clustering, Topic Models, Web Services, Word
		  Embedding},
  location	= {Noida, India},
  series	= {IC3-2021}
}

@InProceedings{	  10.1145/3290420.3290459,
  author	= {Li, Fei and Liao, Lejian and Li, Chunyi and He, Sixing},
  title		= {Efficient and density adaptive edge weight model for
		  measuring semantic similarity},
  year		= {2018},
  isbn		= {9781450365345},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3290420.3290459},
  doi		= {10.1145/3290420.3290459},
  abstract	= {The measurement of semantic similarity between concepts is
		  an important research topic in natural language processing.
		  However, previous efforts suffered from the mismatch of the
		  accuracy and efficiency. In this paper, we propose an edge
		  weight model for improving the accuracy of edge-based
		  measures that have an inherent high efficiency. It combines
		  the edge counting model with the information theory and
		  deduces a function of edge weight based on the number of
		  direct hyponyms of the subsumer in the edge. This model
		  doesn't require any additional parameter and can adapt the
		  effect of different densities to edges. Extensive
		  experiments on four test datasets for WordNet and SNOMED-CT
		  demonstrate that the proposed edge weight model can
		  significantly improve the accuracy of various edge-based
		  similarity measures and has a wide coverage over different
		  ontologies. Compared with IC-based measures, our model has
		  a remarkable advantage in efficiency and is comparable to
		  it in accuracy.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Communication and Information Processing},
  pages		= {127–134},
  numpages	= {8},
  keywords	= {SNOMED-CT, edge-weight, information theory, semantic
		  similarity, wordnet},
  location	= {Qingdao, China},
  series	= {ICCIP '18}
}

@InProceedings{	  10.1145/3625007.3627505,
  author	= {Ranade, Priyanka and Joshi, Anupam},
  title		= {FABULA: Intelligence Report Generation Using
		  Retrieval-Augmented Narrative Construction},
  year		= {2024},
  isbn		= {9798400704093},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625007.3627505},
  doi		= {10.1145/3625007.3627505},
  abstract	= {Narrative construction is the process of representing
		  disparate event information into a logical plot structure
		  that models an end to end story. Intelligence analysis is
		  an example of a domain that can benefit tremendously from
		  narrative construction techniques, particularly in aiding
		  analysts during the largely manual and costly process of
		  synthesizing event information into comprehensive
		  intelligence reports. Manual intelligence report generation
		  is often prone to challenges such as integrating dynamic
		  event information, writing fine-grained queries, and
		  closing information gaps. This motivates the development of
		  a system that retrieves and represents critical aspects of
		  events in a form that aids in automatic generation of
		  intelligence reports.We introduce a Retrieval Augmented
		  Generation (RAG) approach to augment prompting of an
		  autoregressive decoder by retrieving structured information
		  asserted in a knowledge graph to generate targeted
		  information based on a narrative plot model. We apply our
		  approach to the problem of neural intelligence report
		  generation and introduce FABULA, framework to augment
		  intelligence analysis workflows using RAG. An analyst can
		  use FABULA to query an Event Plot Graph (EPG) to retrieve
		  relevant event plot points, which can be used to augment
		  prompting of a Large Language Model (LLM) during
		  intelligence report generation. Our evaluation studies show
		  that the plot points included in the generated intelligence
		  reports have high semantic relevance, high coherency, and
		  low data redundancy.},
  booktitle	= {Proceedings of the 2023 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {603–610},
  numpages	= {8},
  keywords	= {retrieval augmented generation, large language models,
		  knowledge graphs, narratives},
  location	= {Kusadasi, Turkiye},
  series	= {ASONAM '23}
}

@InProceedings{	  10.1145/3461702.3462557,
  author	= {Aka, Osman and Burke, Ken and Bauerle, Alex and Greer,
		  Christina and Mitchell, Margaret},
  title		= {Measuring Model Biases in the Absence of Ground Truth},
  year		= {2021},
  isbn		= {9781450384735},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3461702.3462557},
  doi		= {10.1145/3461702.3462557},
  abstract	= {The measurement of bias in machine learning often focuses
		  on model performance across identity subgroups (such as man
		  and woman) with respect to groundtruth labels. However,
		  these methods do not directly measure the associations that
		  a model may have learned, for example between labels and
		  identity subgroups. Further, measuring a model's bias
		  requires a fully annotated evaluation dataset which may not
		  be easily available in practice.We present an elegant
		  mathematical solution that tackles both issues
		  simultaneously, using image classification as a working
		  example. By treating a classification model's predictions
		  for a given image as a set of labels analogous to a "bag of
		  words", we rank the biases that a model has learned with
		  respect to different identity labels. We use man, woman as
		  a concrete example of an identity label set (although this
		  set need not be binary), and present rankings for the
		  labels that are most biased towards one identity or the
		  other. We demonstrate how the statistical properties of
		  different association metrics can lead to different
		  rankings of the most "gender biased" labels, and conclude
		  that normalized pointwise mutual information (nPMI) is most
		  useful in practice. Finally, we announce an open-sourced
		  nPMI visualization tool using TensorBoard.},
  booktitle	= {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {327–335},
  numpages	= {9},
  keywords	= {bias, datasets, fairness, image tagging, information
		  extraction, model analysis, stereotypes},
  location	= {Virtual Event, USA},
  series	= {AIES '21}
}

@InProceedings{	  10.1145/3229345.3229405,
  author	= {Barboza, Tatiana and Santoro, Fl\'{a}via Maria and
		  Bai\~{a}o, Fernanda},
  title		= {Automatic Validation of Knowledge-intensive Process Models
		  through Alloy},
  year		= {2018},
  isbn		= {9781450365598},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3229345.3229405},
  doi		= {10.1145/3229345.3229405},
  abstract	= {Knowledge-intensive Processes (KiP) are poorly structured,
		  dynamic and highly complex. The Knowledge Intensive Process
		  Ontology (KiPO) constitutes a semantically rich
		  conceptualization (encompassing a set of logical rules)
		  about the domain of KiP that may serve as a basis to
		  understand, identify and manage KiP effectively. However,
		  applying KiPO in real scenarios requires its instantiation,
		  validation and simulation in an application level, which
		  are complex tasks for users that typically are not experts
		  in non-trivial issues on conceptual modeling. This work
		  proposes a rule-based strategy to validate or simulate KiP
		  models. The proposed strategy transforms the KiPO rules
		  into the existing specifications in the Alloy logic-based
		  language, using the Alloy Analyzer model analyzer. The main
		  contribution of this research is to show the applicability
		  of the Alloy tool to this context in a case study with four
		  different scenarios. A process modeler can directly benefit
		  from these results.},
  booktitle	= {Proceedings of the XIV Brazilian Symposium on Information
		  Systems},
  articleno	= {57},
  numpages	= {8},
  keywords	= {BPM, Conceptual Modeling, Knowledge-intensive Process,
		  Model validation},
  location	= {Caxias do Sul, Brazil},
  series	= {SBSI '18}
}

@Article{	  10.1145/3439800,
  author	= {Bi, Mingwen and Zhang, Qingchuan and Zuo, Min and Xu,
		  Zelong and Jin, Qingyu},
  title		= {Bi-directional Long Short-Term Memory Model with Semantic
		  Positional Attention for the Question Answering System},
  year		= {2021},
  issue_date	= {September 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3439800},
  doi		= {10.1145/3439800},
  abstract	= {The intelligent question answering system aims to provide
		  quick and concise feedback on the questions of users.
		  Although the performance of phrase-level and numerous
		  attention models have been improved, the sentence
		  components and position information are not emphasized
		  enough. This article combines Ci-Lin and word2vec to divide
		  all of the words in the question-answer pairs into groups
		  according to the semantics and select one kernel word in
		  each group. The remaining words are common words and
		  realize the semantic mapping mechanism between kernel words
		  and common words. With this Chinese semantic mapping
		  mechanism, the common words in all questions and answers
		  are replaced by the semantic kernel words to realize the
		  normalization of the semantic representation. Meanwhile,
		  based on the bi-directional LSTM model, this article
		  introduces a method of the combination of semantic role
		  labeling and positional context, dividing the sentence into
		  multiple semantic segments according to semantic logic. The
		  weight is given to the neighboring words in the same
		  semantic segment and propose semantic role labeling
		  position attention based on the bi-directional LSTM model
		  (BLSTM-SRLP). The good performance of the BLSTM-SRLP model
		  has been demonstrated in comparative experiments on the
		  food safety field dataset (FS-QA).},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {77},
  numpages	= {13},
  keywords	= {Question answering, BLSTM model, semantic positional-based
		  attention, Chinese semantic mapping mechanism}
}

@Article{	  10.1145/3365000,
  author	= {Zhu, Meng and Wang, Alf Inge},
  title		= {Model-driven Game Development: A Literature Review},
  year		= {2019},
  issue_date	= {November 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3365000},
  doi		= {10.1145/3365000},
  abstract	= {Model-driven game development (MDGD) introduces
		  model-driven methodology to the computer game domain,
		  shifting the focus of game development from coding to
		  modeling to make game development faster and easier. The
		  research on MDGD is concerned with both the general
		  model-driven software development methodology and the
		  particular characteristics of game development. People in
		  the MDGD community have proposed several approaches in the
		  past decades, addressing both the technology and the
		  development process in the context of MDGD. This article
		  presents the state-of-art of MDGD research based on a
		  literature review of 26 approaches in the field. The review
		  is structured around five perspectives: target game
		  domains, domain frameworks, modelling languages, tooling,
		  and evaluation methods. The article also includes
		  reflections and a discussion of the challenges within
		  MDGD.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {123},
  numpages	= {32},
  keywords	= {Model-driven software development, game development}
}

@Article{	  10.1145/3514254,
  author	= {Williams, Rua M. and Alikhademi, Kiana and Munyaka, Imani
		  N. S. and Gilbert, Juan E.},
  title		= {MetaCogs: Mitigating Executive Dysfunction via Agent-based
		  Modeling for Metacognitive Strategy Development},
  year		= {2022},
  issue_date	= {September 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {3},
  issn		= {1936-7228},
  url		= {https://doi.org/10.1145/3514254},
  doi		= {10.1145/3514254},
  abstract	= {Executive functions (EF) are a collection of cognitive
		  domains governing task initiation, motor planning,
		  attention, and goal-oriented action. Difficulties with EF
		  have marked impacts on adaptive living skills, learning
		  outcomes, and quality of life for people with cognitive and
		  psychosocial disabilities, as well as the broader
		  population. While there is considerable research interest
		  in EF training intervention for disabled populations, very
		  few studies explore metacognitive intervention for people
		  with cognitive disabilities. Metacognition comprises
		  conscious beliefs and strategies around task management and
		  goal setting. Metacognitive awareness has been shown to
		  mediate the effects of executive function on self-regulated
		  learning. Metacognitive interventions have also shown
		  promise in general education, military training, and
		  medical practice. We present a virtual reality experience
		  deploying agent-based modeling to support explicit
		  metacognitive strategy instruction for undergraduate
		  students of all neurotypes. Our results support that
		  explicit instructional material explaining executive
		  function and metacognition in relation to problem-solving
		  experiences influenced participant self-concept and
		  awareness of personal traits and cognitive processes.},
  journal	= {ACM Trans. Access. Comput.},
  month		= jul,
  articleno	= {24},
  numpages	= {32},
  keywords	= {Metacognition, executive function, autism, ADHD, virtual
		  reality}
}

@InProceedings{	  10.1109/models-c.2019.00110,
  author	= {Ortiz, Victor-Alejandro and Esta\~{n}ol, Montserrat and
		  Marinescu, Maria-Cristina and Sancho, Maria-Ribera and
		  Teniente, Ernest and Rueda, Carmen},
  title		= {A semantic model to fight social exclusion},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00110},
  doi		= {10.1109/MODELS-C.2019.00110},
  abstract	= {This work presents a semantic model meant to help with the
		  identification and prediction of individuals at risk of
		  social exclusion. The model is based on the
		  self-sufficiency matrix, a tool that evaluates a person's
		  self-sufficiency in different areas, and that is used by
		  Barcelona's City Council. Existing data sources can then be
		  mapped to this model, in order to analyze, query, and
		  visualize the data.},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {730–731},
  numpages	= {2},
  keywords	= {modeling, self-suffiency matrix, semantic technologies,
		  social exclusion},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.1145/3282373.3282386,
  author	= {Bo\v{z}i\'{c}, Bojan and R\'{\i}os, Andr\'{e} and Delany,
		  Sarah Jane},
  title		= {Validation of Tagging Suggestion Models for a Hotel
		  Ticketing Corpus},
  year		= {2018},
  isbn		= {9781450364799},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3282373.3282386},
  doi		= {10.1145/3282373.3282386},
  abstract	= {This paper investigates methods for the prediction of tags
		  on a textual corpus that describes hotel staff inputs in a
		  ticketing system. The aim is to improve the tagging process
		  and find the most suitable method for suggesting tags for a
		  new text entry. The paper consists of two parts: (i)
		  exploration of existing sample data, which includes
		  statistical analysis and visualisation of the data to
		  provide an overview, and (ii) evaluation of tag prediction
		  approaches. We have included different approaches from
		  different research fields in order to cover a broad
		  spectrum of possible solutions. As a result, we have tested
		  a machine learning model for multi-label classification
		  (using gradient boosting), a statistical approach (using
		  frequency heuristics), and two simple similarity-based
		  classification approaches (Nearest Centroid and k-Nearest
		  Neighbours). The experiment which compares the approaches
		  uses recall to measure the quality of results. Finally, we
		  provide a recommendation of the modelling approach which
		  produces the best accuracy in terms of tag prediction on
		  the sample data.},
  booktitle	= {Proceedings of the 20th International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {15–23},
  numpages	= {9},
  keywords	= {Multi-label Classification, Natural Language Processing,
		  Tag Prediction, k-Nearest Neighbour},
  location	= {Yogyakarta, Indonesia},
  series	= {iiWAS2018}
}

@InProceedings{	  10.1145/3652620.3687795,
  author	= {Sch\"{o}berl, Stefan and Banse, Christian and Geist,
		  Verena and Kunz, Immanuel and Pinzger, Martin},
  title		= {CertGraph: Towards a Comprehensive Knowledge Graph for
		  Cloud Security Certifications},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687795},
  doi		= {10.1145/3652620.3687795},
  abstract	= {This paper introduces CertGraph, a knowledge graph-based
		  approach designed to streamline security certification
		  which integrates evidence from multiple sources. Unlike
		  existing approaches, we consider the complete stack from
		  software to policies, and enable the fusion of evidence
		  from different views and sources. Its extensible ontology
		  is designed to accommodate multiple domains, including
		  cloud security, AI models, and source code. By providing an
		  automated and systematic approach to build an ontology,
		  CertGraph aims to facilitate more effective security
		  certification and compliance verification.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {76–77},
  numpages	= {2},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3571662.3571664,
  author	= {Zhang, Zhuoran and Xu, Shibiao and Guo, Li and Lian,
		  Wenke},
  title		= {Multi-modal Variational Auto-Encoder Model for Micro-video
		  Popularity Prediction},
  year		= {2023},
  isbn		= {9781450397100},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571662.3571664},
  doi		= {10.1145/3571662.3571664},
  abstract	= {Popularity prediction of micro videos on multimedia is a
		  hotly studied topic due to the widespread use of video
		  upload sharing services. It’s also a challenging task
		  because popular pattern is affected by multiple factors and
		  is hard to be modeled. The goal of this paper is to use
		  feature extraction techniques and variation auto-encoder
		  (VAE) framework to predict the popularity of online
		  micro-videos. First, we identify four declarable modalities
		  that are important for adaptability and expansibility.
		  Then, we design a multi-modal based VAE regression model
		  (MASSL) to exploit the domestic and foreign information
		  extracted from heterogeneous features. The model can be
		  applied to large-scale multimedia platforms, even the
		  modality absence scenarios. With extensive experiments
		  conducted on the dataset, which was originally generated
		  from the most popular video-sharing website in China, the
		  result demonstrates the effectiveness of our proposed model
		  by comparing with baseline approaches.},
  booktitle	= {Proceedings of the 8th International Conference on
		  Communication and Information Processing},
  pages		= {9–16},
  numpages	= {8},
  keywords	= {deep learning, popularity prediction, social media},
  location	= {Beijing, China},
  series	= {ICCIP '22}
}

@InProceedings{	  10.1145/3708557.3716350,
  author	= {Yordanova, Kristina Y. and Stoev, Teodor and Rebl, Henrike
		  and Hahn, Olga and Peters, Kirsten},
  title		= {Text2RLab: No-Code Methodology for Robotic Programming and
		  Interaction in Laboratory Tasks},
  year		= {2025},
  isbn		= {9798400714092},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708557.3716350},
  doi		= {10.1145/3708557.3716350},
  abstract	= {Using robotic systems in laboratory settings increases the
		  quality and reproducibility of laboratory experiments. One
		  challenge laboratory personal faces is the need of
		  programming knowledge to set up the robotic system. To
		  address this problem, in this work we propose a no-code
		  methodology for robotic programming in laboratory tasks.
		  The methodology takes as an input instructions in natural
		  language and generates machine readable models of execution
		  steps. The final result is a model with a probabilistic
		  structure that allows connecting sensor observations to the
		  model’s states, thus providing a mechanism for robotic
		  action execution and state estimation. The proposed
		  methodology has the potential to increase the applicability
		  of robotic systems and thus to improve the quality and
		  reproducibility of laboratory experiments.},
  booktitle	= {Companion Proceedings of the 30th International Conference
		  on Intelligent User Interfaces},
  pages		= {96–100},
  numpages	= {5},
  keywords	= {robotic programming, model learning, plan generation,
		  probabilistic modelling},
  location	= { },
  series	= {IUI '25 Companion}
}

@Article{	  10.1109/tcbb.2022.3197320,
  author	= {Ye, Cheng and Swiers, Rowan and Bonner, Stephen and
		  Barrett, Ian},
  title		= {A Knowledge Graph-Enhanced Tensor Factorisation Model for
		  Discovering Drug Targets},
  year		= {2022},
  issue_date	= {Nov.-Dec. 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3197320},
  doi		= {10.1109/TCBB.2022.3197320},
  abstract	= {The drug discovery and development process is a long and
		  expensive one, costing over 1 billion USD on average per
		  drug and taking 10-15 years. To reduce the high levels of
		  attrition throughout the process, there has been a growing
		  interest in applying machine learning methodologies to
		  various stages of drug discovery and development in the
		  recent decade, especially at the earliest stage –
		  identification of druggable disease genes. In this paper,
		  we have developed a new tensor factorisation model to
		  predict potential drug targets (genes or proteins) for
		  treating diseases. We created a three-dimensional data
		  tensor consisting of 1,048 gene targets, 860 diseases and
		  230,011 evidence attributes and clinical outcomes
		  connecting them, using data extracted from the Open Targets
		  and PharmaProjects databases. We enriched the data with
		  gene target representations learned from a drug
		  discovery-oriented knowledge graph and applied our proposed
		  method to predict the clinical outcomes for unseen gene
		  target and disease pairs. We designed three evaluation
		  strategies to measure the prediction performance and
		  benchmarked several commonly used machine learning
		  classifiers together with Bayesian matrix and tensor
		  factorisation methods. The result shows that incorporating
		  knowledge graph embeddings significantly improves the
		  prediction accuracy and that training tensor factorisation
		  alongside a dense neural network outperforms all other
		  baselines. In summary, our framework combines two actively
		  studied machine learning approaches to disease target
		  identification, namely tensor factorisation and knowledge
		  graph representation learning, which could be a promising
		  avenue for further exploration in data-driven drug
		  discovery.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= aug,
  pages		= {3070–3080},
  numpages	= {11}
}

@InProceedings{	  10.1145/3167132.3167163,
  author	= {de Kok, Sophie and Punt, Linda and van den Puttelaar,
		  Rosita and Ranta, Karoliina and Schouten, Kim and
		  Frasincar, Flavius},
  title		= {Review-level aspect-based sentiment analysis using an
		  ontology},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167163},
  doi		= {10.1145/3167132.3167163},
  abstract	= {The rapid growth of the World Wide Web has led to an
		  explosion of information that is available on this
		  platform. This has resulted in an increased interest in
		  sentiment analysis, where the goal is to determine the
		  opinion regarding a topic. Aspect-based sentiment analysis
		  aims to capture the sentiment within a segment of text for
		  mentioned aspects, rather than for the text as a whole. The
		  task we consider is aspect-based sentiment analysis at the
		  review-level for restaurant reviews. We focus on
		  ontology-enhanced methods that complement a standard
		  machine learning algorithm. For this task we use two
		  different algorithms, a review-based and a sentence
		  aggregation algorithm. By using an ontology as a knowledge
		  base, the classification performance of our models improves
		  significantly. Furthermore, the review-based algorithm
		  gives more accurate predictions than the sentence
		  aggregation algorithm.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {315–322},
  numpages	= {8},
  keywords	= {SVM, aspect-based sentiment analysis, domain ontology,
		  reviews},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InBook{	  10.1145/3382097.3382100,
  title		= {Semantic modeling},
  year		= {2020},
  isbn		= {9781450376174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3382097.3382100},
  abstract	= {Enterprises have made amazing advances by taking advantage
		  of data about their business to provide predictions and
		  understanding of their customers, markets, and products.
		  But as the world of business becomes more interconnected
		  and global, enterprise data is no long a monolith; it is
		  just a part of a vast web of data. Managing data on a
		  world-wide scale is a key capability for any business
		  today.The Semantic Web treats data as a distributed
		  resource on the scale of the World Wide Web, and
		  incorporates features to address the challenges of massive
		  data distribution as part of its basic design. The aim of
		  the first two editions was to motivate the Semantic Web
		  technology stack from end-to-end; to describe not only what
		  the Semantic Web standards are and how they work, but also
		  what their goals are and why they were designed as they
		  are. It tells a coherent story from beginning to end of how
		  the standards work to manage a world-wide distributed web
		  of knowledge in a meaningful way.The third edition builds
		  on this foundation to bring Semantic Web practice to
		  enterprise. Fabien Gandon joins Dean Allemang and Jim
		  Hendler, bringing with him years of experience in global
		  linked data, to open up the story to a modern view of
		  global linked data. While the overall story is the same,
		  the examples have been brought up to date and applied in a
		  modern setting, where enterprise and global data come
		  together as a living, linked network of data. Also included
		  with the third edition, all of the data sets and queries
		  are available online for study and experimentation at
		  data.world/swwo.},
  booktitle	= {Semantic Web for the Working Ontologist: Effective
		  Modeling for Linked Data, RDFS, and OWL}
}

@InProceedings{	  10.1109/icse-seet52601.2021.00013,
  author	= {Daun, Marian and Brings, Jennifer and Goger, Marcel and
		  Koch, Walter and Weyer, Thorsten},
  title		= {Teaching model-based requirements engineering to industry
		  professionals: an experience report},
  year		= {2021},
  isbn		= {9780738133201},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ICSE-SEET52601.2021.00013},
  doi		= {10.1109/ICSE-SEET52601.2021.00013},
  abstract	= {The use of conceptual models to foster requirements
		  engineering has been proposed and evaluated as beneficial
		  for several decades. For instance, goal-oriented
		  requirements engineering or the specification of scenarios
		  are commonly done using conceptual models. Bringing such
		  model-based requirements engineering approaches into
		  industrial practice typically requires industrial training.
		  In this paper, we report lessons learned from a training
		  program for teaching industry professionals modelbased
		  requirements engineering. Particularly, we as educators and
		  learners report experiences from designing the training
		  program, conducting the actual training, and applying the
		  instructed material in our day-to-day work. From these
		  findings we provide guidelines for educators designing
		  requirements engineering courses for industry
		  professionals.},
  booktitle	= {Proceedings of the 43rd International Conference on
		  Software Engineering: Joint Track on Software Engineering
		  Education and Training},
  pages		= {40–49},
  numpages	= {10},
  keywords	= {conceptual modeling, industrial training, requirements
		  engineering},
  location	= {Virtual Event, Spain},
  series	= {ICSE-JSEET '21}
}

@Book{		  10.1145/3462257,
  author	= {Jalali, Laleh and Jain, Ramesh},
  title		= {Event Mining for Explanatory Modeling},
  year		= {2021},
  isbn		= {9781450384827},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  volume	= {35},
  abstract	= {This book introduces the concept of Event Mining for
		  building explanatory models from analyses of correlated
		  data. Such a model may be used as the basis for predictions
		  and corrective actions. The idea is to create, via an
		  iterative process, a model that explains causal
		  relationships in the form of structural and temporal
		  patterns in the data. The first phase is the data-driven
		  process of hypothesis formation, requiring the analysis of
		  large amounts of data to find strong candidate hypotheses.
		  The second phase is hypothesis testing, wherein a domain
		  expert’s knowledge and judgment is used to test and
		  modify the candidate hypotheses.The book is intended as a
		  primer on Event Mining for data-enthusiasts and information
		  professionals interested in employing these event-based
		  data analysis techniques in diverse applications. The
		  reader is introduced to frameworks for temporal knowledge
		  representation and reasoning, as well as temporal data
		  mining and pattern discovery. Also discussed are the design
		  principles of event mining systems. The approach is reified
		  by the presentation of an event mining system called
		  EventMiner, a computational framework for building
		  explanatory models. The book contains case studies of using
		  EventMiner in asthma risk management and an architecture
		  for the objective self. The text can be used by researchers
		  interested in harnessing the value of heterogeneous big
		  data for designing explanatory event-based models in
		  diverse application areas such as healthcare, biological
		  data analytics, predictive maintenance of systems, computer
		  networks, and business intelligence.}
}

@InProceedings{	  10.1145/3371158.3371176,
  author	= {Jayasimha, Aditya and Gangavarapu, Tushaar and Kamath, S.
		  Sowmya and Krishnan, Gokul S.},
  title		= {Deep Neural Learning for Automated Diagnostic Code Group
		  Prediction Using Unstructured Nursing Notes},
  year		= {2020},
  isbn		= {9781450377386},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3371158.3371176},
  doi		= {10.1145/3371158.3371176},
  abstract	= {Disease prediction, a central problem in clinical care and
		  management, has gained much significance over the last
		  decade. Nursing notes documented by caregivers contain
		  valuable information concerning a patient's state, which
		  can aid in the development of intelligent clinical
		  prediction systems. Moreover, due to the limited adaptation
		  of structured electronic health records in developing
		  countries, the need for disease prediction from such
		  clinical text has garnered substantial interest from the
		  research community. The availability of large, publicly
		  available databases such as MIMIC-III, and advancements in
		  machine and deep learning models with high predictive
		  capabilities have further facilitated research in this
		  direction. In this work, we model the latent knowledge
		  embedded in the unstructured clinical nursing notes, to
		  address the clinical task of disease prediction as a
		  multi-label classification of ICD-9 code groups. We present
		  EnTAGS, which facilitates aggregation of the data in the
		  clinical nursing notes of a patient, by modeling them
		  independent of one another. To handle the sparsity and high
		  dimensionality of clinical nursing notes effectively, our
		  proposed EnTAGS is built on the topics extracted using
		  Non-negative matrix factorization. Furthermore, we explore
		  the applicability of deep learning models for the clinical
		  task of disease prediction, and assess the reliability of
		  the proposed models using standard evaluation metrics. Our
		  experimental evaluation revealed that the proposed approach
		  consistently exceeded the state-of-the-art prediction model
		  by 1.87\% in accuracy, 12.68\% in AUPRC, and 11.64\% in MCC
		  score.},
  booktitle	= {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
  pages		= {152–160},
  numpages	= {9},
  keywords	= {Clinical Decision Support Systems, Deep Learning, Disease
		  Prediction, Healthcare Analytics, Multi-label
		  Classification, Natural Language Processing},
  location	= {Hyderabad, India},
  series	= {CoDS COMAD 2020}
}

@Article{	  10.1145/3674847,
  author	= {Howison, Mark and Ensor, William O. and Maharjan, Suraj
		  and Parikh, Rahil and Sengamedu, Srinivasan H. and Daniels,
		  Paul and Gaither, Amber and Yeats, Carrie and Reddy,
		  Chandan K. and Hastings, Justine S.},
  title		= {Extracting Structured Labor Market Information from Job
		  Postings with Generative AI},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {1},
  url		= {https://doi.org/10.1145/3674847},
  doi		= {10.1145/3674847},
  abstract	= {Labor market information is an important input to labor,
		  workforce, education, and macroeconomic policy. However,
		  granular and real-time data on labor market trends are
		  lacking; publicly available data from survey samples are
		  released with significant lags and miss critical
		  information such as skills and benefits. We use generative
		  Artificial Intelligence to automatically extract structured
		  labor market information from unstructured online job
		  postings for the entire U.S. labor market. To demonstrate
		  our methodology, we construct a sample of 6,800 job
		  postings stratified by 68 major occupational groups,
		  extract structured information on educational requirements,
		  remote-work flexibility, full-time availability, and
		  benefits, and show how these job characteristics vary
		  across occupations. As a validation, we compare frequencies
		  of educational requirements by occupation from our sample
		  to survey data and find no statistically significant
		  difference. Finally, we discuss the scalability to
		  collections of millions of job postings. Our results
		  establish the feasibility of measuring labor market trends
		  at scale from online job postings thanks to advances in
		  generative AI techniques. Improved access to such insights
		  at scale and in real-time could transform the ability of
		  policy leaders, including federal and state agencies and
		  education providers, to make data-informed decisions that
		  better support the American workforce.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= feb,
  articleno	= {9},
  numpages	= {12},
  keywords	= {Workforce, education, policy, large language models,
		  Amazon Bedrock}
}

@InProceedings{	  10.1145/3412841.3441957,
  author	= {Meijer, Lisa and Frasincar, Flavius and Tru\c{s}c\u{a},
		  Maria Mihaela},
  title		= {Explaining a neural attention model for aspect-based
		  sentiment classification using diagnostic classification},
  year		= {2021},
  isbn		= {9781450381048},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3412841.3441957},
  doi		= {10.1145/3412841.3441957},
  abstract	= {Many high performance machine learning models for
		  Aspect-Based Sentiment Classification (ABSC) produce black
		  box models, and therefore barely explain how they classify
		  a certain sentiment value towards an aspect. In this paper,
		  we propose explanation models, that inspect the internal
		  dynamics of a state-of-the-art neural attention model, the
		  LCR-Rot-hop, by using a technique called Diagnostic
		  Classification. Our diagnostic classifier is a simple
		  neural network, which evaluates whether the internal layers
		  of the LCR-Rot-hop model encode useful word information for
		  classification, i.e., the part of speech, the sentiment
		  value, the presence of aspect relation, and the
		  aspect-related sentiment value of words. We conclude that
		  the lower layers in the LCR-Rot-hop model encode the part
		  of speech and the sentiment value, whereas the higher
		  layers represent the presence of a relation with the aspect
		  and the aspect-related sentiment value of words.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on Applied
		  Computing},
  pages		= {821–827},
  numpages	= {7},
  keywords	= {aspect-based sentiment classification, diagnostic
		  classification, neural rotatory attention model},
  location	= {Virtual Event, Republic of Korea},
  series	= {SAC '21}
}

@InProceedings{	  10.1145/3543873.3587618,
  author	= {Zhang, Yinglun and Broyaka, Antonina and Kastens, Jude and
		  Featherstone, Allen M. and Shimizu, Cogan and Hitzler,
		  Pascal and Mcginty, Hande K\"{u}\c{c}\"{u}k},
  title		= {Sustainable Grain Transportation in Ukraine Amidst War
		  Utilizing KNARM and KnowWhereGraph},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587618},
  doi		= {10.1145/3543873.3587618},
  abstract	= {In this work, we propose a sustainable path-finding
		  application for grain transportation during the ongoing
		  Russian military invasion in Ukraine. This application is
		  to build a suite of algorithms to find possible optimal
		  paths for transporting grain that remains in Ukraine. The
		  application uses the KNowledge Acquisition and
		  Representation Methodology(KNARM) and the KnowWhereGraph to
		  achieve this goal. Currently, we are working towards
		  creating an ontology that will allow for a more effective
		  heuristic approach by incorporating the lessons learned
		  from the KnowWhereGraph. The aim is to enhance the
		  path-finding process and provide more accurate and
		  efficient results. In the future, we will continue
		  exploring and implementing new techniques that can further
		  improve the sustainability of the path-finding applications
		  with a knowledge graph backend for grain transportation
		  through hazardous and adversarial environments. The code is
		  available upon reviewer’s request. It can not be made
		  public due to the sensitive nature of the data.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {742–745},
  numpages	= {4},
  keywords	= {global food systems, knowledge graphs, ontology
		  engineering, path-finding},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@Article{	  10.1145/3616017,
  author	= {Smith, Ronnie and Dragone, Mauro},
  title		= {Generalisable Dialogue-based Approach for Active Learning
		  of Activities of Daily Living},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {3},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3616017},
  doi		= {10.1145/3616017},
  abstract	= {While Human Activity Recognition systems may benefit from
		  Active Learning by allowing users to self-annotate their
		  Activities of Daily Living (ADLs), many proposed methods
		  for collecting such annotations are for short-term data
		  collection campaigns for specific datasets. We present a
		  reusable dialogue-based approach to user interaction for
		  active learning in activity recognition systems, which
		  utilises semantic similarity measures and a dataset of
		  natural language descriptions of common activities (which
		  we make publicly available). Our approach involves
		  system-initiated dialogue, including follow-up questions to
		  reduce ambiguity in user responses where appropriate. We
		  apply this approach to two active learning scenarios: (i)
		  using an existing CASAS dataset, demonstrating long-term
		  usage; and (ii) using an online activity recognition
		  system, which tackles the issue of online segmentation and
		  labelling. We demonstrate our work in context, in which a
		  natural language interface provides knowledge that can help
		  interpret other multi-modal sensor data. We provide results
		  highlighting the potential of our dialogue- and semantic
		  similarity-based approach. We evaluate our work: (i)
		  quantitatively, as an efficient way to seek users’ input
		  for active learning of ADLs; and (ii) qualitatively,
		  through a user study in which users were asked to compare
		  our approach and an established method. Results show the
		  potential of our approach as a hands-free interface for
		  annotation of sensor data as part of an active learning
		  system. We provide insights into the challenges of active
		  learning for activity recognition under real-world
		  conditions and identify potential ways to address them.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= sep,
  articleno	= {18},
  numpages	= {37},
  keywords	= {Human-in-the-Loop (HITL) annotation, Active Learning (AL),
		  natural language, semantic similarity, Human Activity
		  Recognition (HAR) labelling}
}

@Article{	  10.14778/3705829.3705832,
  author	= {Tang, Xiu and Liu, Wenhao and Wu, Sai and Yao, Chang and
		  Yuan, Gongsheng and Ying, Shanshan and Chen, Gang},
  title		= {QueryArtisan: Generating Data Manipulation Codes for
		  Ad-hoc Analysis in Data Lakes},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {VLDB Endowment},
  volume	= {18},
  number	= {2},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3705829.3705832},
  doi		= {10.14778/3705829.3705832},
  abstract	= {Query processing over data lakes is a challenging task,
		  often requiring extensive data pre-processing activities
		  such as data cleaning, transformation, and loading.
		  However, the advent of Large Language Models (LLMs) has
		  illuminated a new pathway to address these complexities by
		  offering a unified approach to understanding the diverse
		  datasets submerged in data lakes. In this paper, we
		  introduce QueryArtisan, a novel LLM-powered analytic tool
		  specifically designed for data lakes. QueryArtisan
		  transcends traditional ETL (Extract, Transform, Load)
		  processes by generating just-intime code for
		  dataset-specific queries. It eliminates the need for an
		  intermediary schema, enabling users to query the data lake
		  directly using natural language. To achieve this, we have
		  developed a suite of heterogeneous operators capable of
		  processing data across various modalities. Additionally,
		  QueryArtisan incorporates a cost model-based query
		  optimization technique, significantly enhancing its code
		  generation capabilities for efficient query resolution. Our
		  extensive experimental evaluations, conducted with
		  real-life datasets, demonstrate that QueryArtisan markedly
		  outperforms existing solutions in terms of effectiveness,
		  efficiency and usability.},
  journal	= {Proc. VLDB Endow.},
  month		= oct,
  pages		= {108–116},
  numpages	= {9}
}

@InBook{	  10.1145/3501714.3501757,
  author	= {Shpitser, Ilya and Richardson, Thomas S. and Robins, James
		  M.},
  title		= {Multivariate Counterfactual Systems and Causal Graphical
		  Models},
  year		= {2022},
  isbn		= {9781450395861},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  url		= {https://doi.org/10.1145/3501714.3501757},
  booktitle	= {Probabilistic and Causal Inference: The Works of Judea
		  Pearl},
  pages		= {813–852},
  numpages	= {40}
}

@InProceedings{	  10.1145/3388440.3414702,
  author	= {Gbenro, Sola and Hippe, Kyle and Cao, Renzhi},
  title		= {HMMeta: Protein Function Prediction using Hidden Markov
		  Models},
  year		= {2020},
  isbn		= {9781450379649},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3388440.3414702},
  doi		= {10.1145/3388440.3414702},
  abstract	= {As the body of genomic product data increases at a much
		  faster rate than can be annotated, computational analysis
		  of protein function has never been more important. In this
		  research, we introduce a novel protein function prediction
		  method HMMeta, which is based on the prominent natural
		  language prediction technique Hidden Markov Models (HMM).
		  With a new representation of protein sequence as a
		  language, we trained a unique HMM for each Gene Ontology
		  (GO) term taken from the UniProt database, which in total
		  has 27,451 unique GO IDs leading to the creation of 27,451
		  Hidden Markov Models. We employed data augmentation to
		  artificially inflate the number of protein sequences
		  associated with GO terms that have a limited amount in the
		  database, and this helped to balance the number of protein
		  sequences associated with each GO term. Predictions are
		  made by running the sequence against each model created.
		  The models within eighty percent of the top scoring model,
		  or 75 models with the highest scores, whichever is less,
		  represent the functions that are most associated with the
		  given sequence. We benchmarked our method in the latest
		  Critical Assessment of protein Function Annotation (CAFA 4)
		  experiment as CaoLab2, and we also evaluated HMMeta against
		  several other protein function prediction methods against a
		  subset of the UniProt database. HMMeta achieved favorable
		  results as a sequence-based method, and outperforms a few
		  notable methods in some categories through our evaluation,
		  which shows great potential for automated protein function
		  prediction. The tool is available at
		  https://github.com/KPHippe/HMM-For-Protein-Prediction.},
  booktitle	= {Proceedings of the 11th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  articleno	= {104},
  numpages	= {6},
  keywords	= {Hidden Markov Model, Machine Learning, Protein Function
		  Prediction},
  location	= {Virtual Event, USA},
  series	= {BCB '20}
}

@InProceedings{	  10.1145/3473141.3473236,
  author	= {Acuna, Gabriel Edrick and Alvarez, Luis Antonio and
		  Miraflores, Jeffrey and Samonte, Mary Jane},
  title		= {Towards the Development of an Adaptive E-learning System
		  with Chatbot Using Personalized E-learning Model},
  year		= {2021},
  isbn		= {9781450389723},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3473141.3473236},
  doi		= {10.1145/3473141.3473236},
  abstract	= {Although there is no distinctive header, this is the
		  abstract. This submission template allows authors to submit
		  their papers E-learning has become one of the most used
		  electronic systems in the field of education. Although it
		  is beneficial, there are still some lacking capabilities
		  and considerations that can negatively affect the
		  performance of the students. This leads to the innovation
		  that makes e-learning systems adaptive to the users’
		  personality, knowledge, behavior, interest, or preferences,
		  the system is called personalized e-learning system. This
		  survey paper aims to provide the general parameters in
		  creating a personalized e-learning system based on the 150
		  research papers collected, and a timespan of 2016 to 2020
		  as a condition. Through a series of literature reviews of
		  research papers published in the last five years, also
		  related to personalized e-learning systems, this paper
		  presents the common components, tools and algorithms, and
		  learning model that are generally used in developing a
		  personalized e-learning system to help as reference in
		  developing more effective personalized e-learning systems.
		  Moreover, considering the findings of this study, this
		  paper has proposed developing a hybrid e-learning system
		  with a chatbot.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Frontiers of Educational Technologies},
  pages		= {120–125},
  numpages	= {6},
  keywords	= {Adaptive, Chatbot, Myer-Briggs Type Indicator Theory,
		  Personalized e-Learning Model},
  location	= {Bangkok, Thailand},
  series	= {ICFET '21}
}

@InProceedings{	  10.1145/3357254.3357277,
  author	= {Huang, Jih-Jeng},
  title		= {Computing the semantic similarity between documents by the
		  copula-based econometric models},
  year		= {2019},
  isbn		= {9781450372299},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357254.3357277},
  doi		= {10.1145/3357254.3357277},
  abstract	= {Semantic similarity is important information with which
		  decision-makers can cluster, classify, or compare documents
		  in text mining. Statistical and topological methods are two
		  major ways to determine semantic similarity. However,
		  conventional methods ignore the time factor when
		  calculating the similarity between documents. It should be
		  highlighted that narrative emotions play a critical role in
		  comparing documents. In this paper, copula-based
		  econometric models, including ARMA and GARCH families, are
		  used to calculate the narrative semantic similarity between
		  documents.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Artificial Intelligence and Pattern Recognition},
  pages		= {134–139},
  numpages	= {6},
  keywords	= {copula, econometric models, semantic similarity, text
		  mining},
  location	= {Beijing, China},
  series	= {AIPR '19}
}

@Article{	  10.1145/3638555,
  author	= {Lin, Tzu-Mi and Hung, Man-Chen and Lee, Lung-Hao},
  title		= {Leveraging Dual Gloss Encoders in Chinese Biomedical
		  Entity Linking},
  year		= {2024},
  issue_date	= {February 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3638555},
  doi		= {10.1145/3638555},
  abstract	= {Entity linking is the task of assigning a unique identity
		  to named entities mentioned in a text, a sort of word sense
		  disambiguation that focuses on automatically determining a
		  pre-defined sense for a target entity to be disambiguated.
		  This study proposes the DGE (Dual Gloss Encoders) model for
		  Chinese entity linking in the biomedical domain. We
		  separately model a dual encoder architecture, comprising a
		  context-aware gloss encoder and a lexical gloss encoder,
		  for contextualized embedding representations. DGE are then
		  jointly optimized to assign the nearest gloss with the
		  highest score for target entity disambiguation. The
		  experimental datasets consist of a total of 10,218
		  sentences that were manually annotated with glosses defined
		  in the BabelNet 5.0 across 40 distinct biomedical entities.
		  Experimental results show that the DGE model achieved an
		  F1-score of 97.81, outperforming other existing methods. A
		  series of model analyses indicate that the proposed
		  approach is effective for Chinese biomedical entity
		  linking.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= feb,
  articleno	= {28},
  numpages	= {15},
  keywords	= {Word sense disambiguation, lexical semantics, language
		  transformers, natural language understanding, biomedical
		  informatics}
}

@InProceedings{	  10.1145/3277593.3277597,
  author	= {Sahlmann, Kristina and Schwotzer, Thomas},
  title		= {Ontology-based virtual IoT devices for edge computing},
  year		= {2018},
  isbn		= {9781450365642},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3277593.3277597},
  doi		= {10.1145/3277593.3277597},
  abstract	= {An IoT network may consist of hundreds heterogeneous
		  devices. Some of them may be constrained in terms of
		  memory, power, processing and network capacity. Manual
		  network and service management of IoT devices are
		  challenging. We propose a usage of an ontology for the IoT
		  device descriptions enabling automatic network management
		  as well as service discovery and aggregation. Our IoT
		  architecture approach ensures interoperability using
		  existing standards, i.e. MQTT protocol and Semantic Web
		  technologies. We herein introduce virtual IoT devices and
		  their semantic framework deployed at the edge of network.
		  As a result, virtual devices are enabled to aggregate
		  capabilities of IoT devices, derive new services by
		  inference, delegate requests/responses and generate events.
		  Furthermore, they can collect and pre-process sensor data.
		  These tasks on the edge computing overcome the shortcomings
		  of the cloud usage regarding siloization, network
		  bandwidth, latency and speed. We validate our proposition
		  by implementing a virtual device on a Raspberry Pi.},
  booktitle	= {Proceedings of the 8th International Conference on the
		  Internet of Things},
  articleno	= {15},
  numpages	= {7},
  keywords	= {M2M, MQTT, edge computing, internet of things, oneM2M
		  ontology, semantic interoperability},
  location	= {Santa Barbara, California, USA},
  series	= {IOT '18}
}

@Article{	  10.1145/3689040,
  author	= {Bomba, Federico and Men\'{e}ndez-Blanco, Mar\'{\i}a and
		  Grigis, Paolo and Cremaschi, Michele and De Angeli,
		  Antonella},
  title		= {The Choreographer-Performer Continuum: A Diffraction Tool
		  to Illuminate Authorship in More Than Human
		  Co-Performances},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {31},
  number	= {6},
  issn		= {1073-0516},
  url		= {https://doi.org/10.1145/3689040},
  doi		= {10.1145/3689040},
  abstract	= {The design of robust and trustworthy Generative AI (GenAI)
		  requires a deep understanding of the agencies emerging from
		  human interactions with them. To contribute to this goal,
		  we retrospectively studied an art project involving a
		  visual artist, a computer scientist, an artistic director,
		  and a generative model (GPT-2). The model was fine-tuned
		  with trip reports describing the experience of eating
		  psychedelic mushrooms. Building on agential realism, we
		  analysed the co-performance between the artist and the
		  model as their agency moved along the
		  choreographer-performer continuum. Results reveal
		  ontological surprises, leading to the proposal of entangled
		  authorship to de-individualise the production of knowledge
		  from a More Than Human perspective. The paper illustrates
		  how art can expose different forms of relationships,
		  challenging the idea of GenAI as just a tool that
		  simplifies or replaces human labour. We conclude by
		  emphasising the transformational potential of GenAI for
		  novel modes of engagement between humans and machines.},
  journal	= {ACM Trans. Comput.-Hum. Interact.},
  month		= dec,
  articleno	= {75},
  numpages	= {23},
  keywords	= {Agency, Agential Realism, Large Language Models, AI and
		  Art, Creative AI, Hallucination}
}

@InProceedings{	  10.1145/3703187.3703192,
  author	= {Li, Chengxue and Chen, Xuyang and Ding, Min and Jin, Wei
		  and Gao, Feng},
  title		= {Research on Chinese Knowledge Base and Knowledge Q&amp;A
		  Technology for Power Grid Dispatching},
  year		= {2024},
  isbn		= {9798400707254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703187.3703192},
  doi		= {10.1145/3703187.3703192},
  abstract	= {To support online professional knowledge query for power
		  grid dispatchers, this article proposes a method for
		  constructing a knowledge base based on knowledge graph,
		  which implements systematic organization and management of
		  knowledge resources in the field of power-grid dispatching.
		  Moreover, a questions and answers (Q&amp;A) service is
		  design based on large language model and proposed knowledge
		  base. Based on the constructed knowledge base and Q&amp;A
		  service, auxiliary learning functions can be provided in
		  the domain of power grid operation. This enables accurate
		  acquisition of professional knowledge through Chinese
		  natural language interaction, enhancing the effectiveness
		  and flexibility of online training for power-grid
		  dispatchers.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Computer Information Science and Artificial Intelligence},
  pages		= {18–23},
  numpages	= {6},
  keywords	= {Graph database, Knowledge graph, Large language model,
		  Question answering},
  location	= { },
  series	= {CISAI '24}
}

@InProceedings{	  10.1145/3220228.3220246,
  author	= {Al-Fedaghi, Sabah and Alduwaisan, Yousef},
  title		= {Modeling of an enterprise and information system: process
		  specification based on the flow of things},
  year		= {2018},
  isbn		= {9781450364454},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3220228.3220246},
  doi		= {10.1145/3220228.3220246},
  abstract	= {Current modeling of enterprise and information systems is
		  based on diverse methods such as function-orientation,
		  data-orientation, process-orientation object-orientation,
		  and object/process-orientation. Other emerging modeling
		  methods include the ontology capture method and Business
		  Process Modeling Notation (BPMN). These approaches have
		  been criticized for lack of execution environment into
		  which events that cause change could be incorporated. Such
		  a topic is important in the study of dynamic behavior of
		  systems for both analysis and control. This paper further
		  develops a new approach oriented toward things that flow.
		  Specifically, the paper concentrates on the study of
		  process specification in analysis and design of
		  enterprise/system architectures in order to most
		  effectively facilitate their control. Here we produce a
		  single, integrated diagrammatic representation that
		  uniformly incorporates structural and behavioral aspects
		  into an underlying conceptual description. The viability of
		  the model is demonstrated by applying it to a case study of
		  services provided by an existing organizational unit.},
  booktitle	= {Proceedings of the International Conference on
		  Geoinformatics and Data Analysis},
  pages		= {142–150},
  numpages	= {9},
  keywords	= {BPMN, UML, conceptual modeling, data flow models, process
		  specification},
  location	= {Prague, Czech Republic},
  series	= {ICGDA '18}
}

@InProceedings{	  10.1145/3308558.3313511,
  author	= {Vedula, Nikhita and Maneriker, Pranav and Parthasarathy,
		  Srinivasan},
  title		= {BOLT-K: Bootstrapping Ontology Learning via Transfer of
		  Knowledge},
  year		= {2019},
  isbn		= {9781450366748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308558.3313511},
  doi		= {10.1145/3308558.3313511},
  abstract	= {Dynamically extracting and representing continually
		  evolving knowledge entities is an essential scaffold for
		  grounded intelligence and decision making. Creating
		  knowledge schemas for newly emerging, unfamiliar,
		  domain-specific ideas or events poses the following
		  challenges: (i) detecting relevant, often previously
		  unknown concepts associated with the new domain; and (ii)
		  learning ontological, semantically accurate relationships
		  among the new concepts, despite having severely limited
		  annotated data. To this end, we propose a novel LSTM-based
		  framework with attentive pooling, BOLT-K, to learn an
		  ontology for a target subject or domain. We bootstrap our
		  ontology learning approach by adapting and transferring
		  knowledge from an existing, functionally related source
		  domain. We also augment the inadequate labeled data
		  available for the target domain with various strategies to
		  minimize human expertise during model development and
		  training. BOLT-K first employs semantic and graphical
		  features to recognize the entity or concept pairs likely to
		  be related to each other, and filters out spurious concept
		  combinations. It is then jointly trained on knowledge from
		  the target and source domains to learn relationships among
		  the target concepts. The target concepts and their
		  corresponding relationships are subsequently used to
		  construct an ontology. We extensively evaluate our
		  framework on several, real-world bio-medical and commercial
		  product domain ontologies. We obtain significant
		  improvements of 5-25\% F1-score points over
		  state-of-the-art baselines. We also examine the potential
		  of BOLT-K in detecting the presence of novel kinds of
		  relationships that were unseen during training.},
  booktitle	= {The World Wide Web Conference},
  pages		= {1897–1908},
  numpages	= {12},
  location	= {San Francisco, CA, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3617023.3617059,
  author	= {Hoppe, Pedro Henrique Brunoro and Souza, V\'{\i}tor
		  Est\^{e}v\~{a}o Silva},
  title		= {Support for Single Page Application Frameworks on
		  FrameWeb},
  year		= {2023},
  isbn		= {9798400709081},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3617023.3617059},
  doi		= {10.1145/3617023.3617059},
  abstract	= {In the field of Web Engineering, many methods have been
		  proposed to guide developers in designing and coding Web
		  applications. The FrameWeb method is a model-driven
		  approach that targets the development of systems that use
		  certain kinds of frameworks in their architecture,
		  proposing the use of models that incorporate concepts from
		  these frameworks during design. Currently, the FrameWeb
		  method does not consider SPA (Single Page Application)
		  frameworks and, in recent years, they have gained a lot of
		  popularity among developers. In this work, we propose to
		  add support for SPA frameworks to FrameWeb. With our
		  research, we have managed to update the FrameWeb meta-model
		  so that its modeling language now supports SPA frameworks
		  and their constructs. FrameWeb tools (graphical editor and
		  code generator) also evolved to support the new elements.
		  Experiments of modeling existing SPAs with this new version
		  of FrameWeb, generating code from the models and comparing
		  with the original, showed that, in average, around 69\% of
		  the HTML tags could be generated from the models. The
		  support for SPA frameworks in FrameWeb allows developers to
		  design and model their applications using constructs that
		  relate to the frameworks used in practice, facilitating
		  developer communication using the models and generating
		  code to improve developer productivity.},
  booktitle	= {Proceedings of the 29th Brazilian Symposium on Multimedia
		  and the Web},
  pages		= {260–268},
  numpages	= {9},
  keywords	= {DSL, FrameWeb, MDD, Reuse, SPA Frameworks, Software
		  Engineering, WIS Frameworks, Web Engineering, language,
		  method, tools},
  location	= {Ribeir\~{a}o Preto, Brazil},
  series	= {WebMedia '23}
}

@InProceedings{	  10.1145/3610978.3640622,
  author	= {Jokinen, Kristiina and Wilcock, Graham},
  title		= {Exploring a Japanese Cooking Database},
  year		= {2024},
  isbn		= {9798400703232},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3610978.3640622},
  doi		= {10.1145/3610978.3640622},
  abstract	= {The paper describes ongoing work applying Generative AI to
		  a real world application. We use Retrieval Augmented
		  Generation and other GenAI tools that combine large
		  language models with Neo4j knowledge graphs. These tools
		  help a robot to chat in English about Japanese cooking
		  using a knowledge base that is in Japanese.},
  booktitle	= {Companion of the 2024 ACM/IEEE International Conference on
		  Human-Robot Interaction},
  pages		= {578–582},
  numpages	= {5},
  keywords	= {Japanese cooking, cypher query language, generative AI,
		  graph databases, knowledge graphs, large language models,
		  retrieval augmented generation, semantic search, social
		  robots},
  location	= {Boulder, CO, USA},
  series	= {HRI '24}
}

@InProceedings{	  10.1145/3567512.3567516,
  author	= {Steimann, Friedrich and Freitag, Marius},
  title		= {The Semantics of Plurals},
  year		= {2022},
  isbn		= {9781450399197},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3567512.3567516},
  doi		= {10.1145/3567512.3567516},
  abstract	= {Inside many software languages lives an expression
		  language that caters for the computation of single values
		  from single values. These languages' fixation on
		  single-valuedness is often at odds with their application
		  domains, in which many values, or plurals, regularly occur
		  in the places of single. While the classical mathematical
		  means of dealing with plurals is the set, in computing,
		  other representations have evolved, notably strings and the
		  much lesser known bunches. We review bunch theory in the
		  context of expression languages including non-recursive
		  functions, and show how giving bunches set semantics
		  suggests that evaluating bunch functions amounts to
		  computing with relations. We maintain that the ensuing
		  seamless integration of relations in expression languages
		  that otherwise know only functions makes a worthwhile
		  contribution in a field in which the difference between
		  modeling, with its preference for relations, and
		  programming, with its preference for functions, is
		  increasingly considered accidental.},
  booktitle	= {Proceedings of the 15th ACM SIGPLAN International
		  Conference on Software Language Engineering},
  pages		= {36–54},
  numpages	= {19},
  keywords	= {bunches, collections, denotational semantics, modeling,
		  relational languages},
  location	= {Auckland, New Zealand},
  series	= {SLE 2022}
}

@Article{	  10.1145/3702323,
  author	= {Aloraini, Abdulrahman and Yu, Juntao and Aliady, Wateen
		  and Poesio, Massimo},
  title		= {A Survey of Coreference and Zeros Resolution for Arabic},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3702323},
  doi		= {10.1145/3702323},
  abstract	= {Coreference resolution is the task of resolving mentions
		  that refer to the same entity into clusters. The area and
		  its tasks are crucial in natural language processing (NLP)
		  applications. Extensive surveys of this task have been
		  conducted for English and Chinese; not too much for Arabic.
		  The few Arabic surveys do not cover recent progress and the
		  challenges for Arabic anaphora; and do not cover zero
		  resolution and comprehensive resolution of zeros and full
		  mentions, or anaphora resolution beyond coreference (e.g.,
		  bridging). In this paper, we examine the state-of-the-art
		  for Arabic anaphora resolution, highlighting the challenges
		  and advances in this field. We provide a comprehensive
		  survey of the methods employed for Arabic coreference
		  resolution, as well as an overview of the existing datasets
		  and challenges. The goal is to equip researchers with a
		  thorough understanding of Arabic anaphora resolution and to
		  suggest potential future directions in the field.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  keywords	= {Anaphora Resolution, Coreference Resolution, Arabic
		  Natural Language Processing, Machine Learning.}
}

@InProceedings{	  10.1145/3456887.3457465,
  author	= {Lu, Bai},
  title		= {Trend estimation model of students' thought and behavior
		  based on big data},
  year		= {2021},
  isbn		= {9781450389969},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3456887.3457465},
  doi		= {10.1145/3456887.3457465},
  abstract	= {With the rapid development of global informatization, the
		  ways for contemporary college students to acquire knowledge
		  are enriched. College students have active thinking, strong
		  ability to accept new things, easy to be influenced by
		  various cultures, open-minded, and novel values. College
		  students' ideas have gradually matured, but they are highly
		  malleable and likely to be influenced by the external
		  environment. In order to quantitatively analyze the trend
		  of students' thinking and behavior, an estimation model of
		  students' thinking and behavior trend based on big data is
		  proposed. Constructing semantic ontology big data
		  distribution set of students' thought and behavior trends,
		  establishing semantic ontology fusion feature distribution
		  set of students' thought and behavior trend estimation by
		  adopting multi-source parameter distributed reconstruction
		  and phase space fusion analysis methods, analyzing the
		  parameter feature quantity of students' thought and
		  behavior trend distribution fusion by combining ambiguity
		  detection and information feature matching methods, and
		  constructing association rule distribution set of students'
		  thought and behavior trend estimation by adopting global
		  ambiguity reconstruction and feature reconstruction
		  methods. Through fuzzy detection and information fusion,
		  the semantic structure characteristics of students'
		  ideological and behavioral trends are analyzed. By matching
		  ideological and behavioral characteristics and mining
		  statistical information, students' ideological and
		  behavioral trends are estimated and self-adaptive
		  convergence control is realized, and the optimal solution
		  of students' ideological and behavioral trends estimation
		  is obtained. The simulation results show that this method
		  is highly adaptive and stable in estimating the trend of
		  students' thinking and behavior, and improves the accurate
		  probability of estimating the trend of students' thinking
		  and behavior.},
  booktitle	= {2021 2nd International Conference on Computers,
		  Information Processing and Advanced Education},
  pages		= {1084–1089},
  numpages	= {6},
  keywords	= {Big data, Information fusion, Students' thought and
		  behavior, Trend estimation},
  location	= {Ottawa, ON, Canada},
  series	= {CIPAE 2021}
}

@Article{	  10.1145/3464938,
  author	= {Russo, Daniel},
  title		= {The Agile Success Model: A Mixed-methods Study of a
		  Large-scale Agile Transformation},
  year		= {2021},
  issue_date	= {October 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {30},
  number	= {4},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3464938},
  doi		= {10.1145/3464938},
  abstract	= {Organizations are increasingly adopting Agile frameworks
		  for their internal software development. Cost reduction,
		  rapid deployment, requirements and mental model alignment
		  are typical reasons for an Agile transformation. This
		  article presents an in-depth field study of a large-scale
		  Agile transformation in a mission-critical environment,
		  where stakeholders’ commitment was a critical success
		  factor. The goal of such a transformation was to implement
		  mission-oriented features, reducing costs and time to
		  operate in critical scenarios. The project lasted several
		  years and involved over 40 professionals. We report how a
		  hierarchical and plan-driven organization exploited Agile
		  methods to develop a Command \&amp; Control (C2) system.
		  Accordingly, we first abstract our experience, inducing a
		  success model of general use for other comparable
		  organizations by performing a post-mortem study. The goal
		  of the inductive research process was to identify critical
		  success factors and their relations. Finally, we validated
		  and generalized our model through Partial Least Squares -
		  Structural Equation Modelling, surveying 200 software
		  engineers involved in similar projects. We conclude the
		  article with data-driven recommendations concerning the
		  management of Agile projects.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jul,
  articleno	= {52},
  numpages	= {46},
  keywords	= {Agile, ethnography, grounded theory, mixed methods
		  research, multivariate analysis, partial least squares,
		  structural equation modeling}
}

@InProceedings{	  10.1145/3550355.3552407,
  author	= {Rasti, Aidin and Amyot, Daniel and Parvizimosaed, Alireza
		  and Roveri, Marco and Logrippo, Luigi and Anda, Amal Ahmed
		  and Mylopoulos, John},
  title		= {Symboleo2SC: from legal contract specifications to smart
		  contracts},
  year		= {2022},
  isbn		= {9781450394666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550355.3552407},
  doi		= {10.1145/3550355.3552407},
  abstract	= {Smart contracts (SCs) are software systems that monitor
		  and control the execution of legal contracts to ensure
		  compliance with the contracts' terms and conditions. They
		  often exploit Internet-of-Things technologies to support
		  their monitoring functions, and blockchain technology to
		  ensure the integrity of their data. Ethereum and business
		  blockchain platforms, such as Hyperledger Fabric, are
		  popular choices for SC development. However, there is a gap
		  in the knowledge of SCs between developers and legal
		  experts. Symboleo is a formal specification language for
		  legal contracts that was introduced to address this issue.
		  Symboleo specifications directly encode legal concepts such
		  as parties, obligations, and powers. In this paper, we
		  propose a tool-supported method for translating Symboleo
		  specifications into smart contracts. We have extended the
		  current Symboleo IDE, implemented the ontology and
		  semantics of Symboleo into a reusable library, and
		  developed the Symboleo2SC tool to generate Hyperledger
		  Fabric code exploiting this library. Symboleo2SC was
		  evaluated with three sample contracts. The results shows
		  that legal contract specifications in Symboleo can be fully
		  converted to SCs for monitoring purposes. Moreover,
		  Symboleo2SC helps simplify the SC development process,
		  saves development effort, and helps reduce risks of coding
		  errors.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems},
  pages		= {300–310},
  numpages	= {11},
  keywords	= {blockchain, code generation, domain-specific languages,
		  legal ontology, smart contracts},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@Article{	  10.1109/tcbb.2020.2968882,
  author	= {Zhang, Fuhao and Song, Hong and Zeng, Min and Wu,
		  Fang-Xiang and Li, Yaohang and Pan, Yi and Li, Min},
  title		= {A Deep Learning Framework for Gene Ontology Annotations
		  With Sequence- and Network-Based Information},
  year		= {2020},
  issue_date	= {Nov.-Dec. 2021},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {18},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2020.2968882},
  doi		= {10.1109/TCBB.2020.2968882},
  abstract	= {Knowledge of protein functions plays an important role in
		  biology and medicine. With the rapid development of
		  high-throughput technologies, a huge number of proteins
		  have been discovered. However, there are a great number of
		  proteins without functional annotations. A protein usually
		  has multiple functions and some functions or biological
		  processes require interactions of a plurality of proteins.
		  Additionally, Gene Ontology provides a useful
		  classification for protein functions and contains more than
		  40,000 terms. We propose a deep learning framework called
		  DeepGOA to predict protein functions with protein sequences
		  and protein-protein interaction (PPI) networks. For protein
		  sequences, we extract two types of information: sequence
		  semantic information and subsequence-based features. We use
		  the word2vec technique to numerically represent protein
		  sequences, and utilize a Bi-directional Long and Short Time
		  Memory (Bi-LSTM) and multi-scale convolutional neural
		  network (multi-scale CNN) to obtain the global and local
		  semantic features of protein sequences, respectively.
		  Additionally, we use the InterPro tool to scan protein
		  sequences for extracting subsequence-based information,
		  such as domains and motifs. Then, the information is
		  plugged into a neural network to generate high-quality
		  features. For the PPI network, the Deepwalk algorithm is
		  applied to generate its embedding information of PPI. Then
		  the two types of features are concatenated together to
		  predict protein functions. To evaluate the performance of
		  DeepGOA, several different evaluation methods and metrics
		  are utilized. The experimental results show that DeepGOA
		  outperforms DeepGO and BLAST.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jan,
  pages		= {2208–2217},
  numpages	= {10}
}

@InProceedings{	  10.1145/3297280.3297595,
  author	= {Dewitte, Pierre and Wuyts, Kim and Sion, Laurens and Van
		  Landuyt, Dimitri and Emanuilov, Ivo and Valcke, Peggy and
		  Joosen, Wouter},
  title		= {A comparison of system description models for data
		  protection by design},
  year		= {2019},
  isbn		= {9781450359337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297280.3297595},
  doi		= {10.1145/3297280.3297595},
  abstract	= {Since the General Data Protection Regulation (GDPR)
		  entered into force, every actor involved in the processing
		  of personal data must comply with Data Protection by Design
		  (DPbD). Doing so requires assessing the risks to data
		  subjects' rights and freedoms and implementing appropriate
		  countermeasures. While legal experts traditionally apply
		  Data Protection Impact Assessments (DPIA), software
		  engineers rely on threat modeling for their
		  assessment.Despite significant differences, both approaches
		  nonetheless revolve around (i) a description of the system
		  and (ii) the identification, assessment and mitigation of
		  specific risks. In practice, however, DPIAs and threat
		  modeling are usually performed in complete isolation,
		  following their own, unharmonized lexicon and abstractions.
		  Such as disconnect lowers the quality of the assessment and
		  of the conceptual and architectural trade-offsIn this
		  paper, we present (i) an overview of the legal and
		  architectural modeling requirements and (ii) incentives and
		  recommendations for aligning both modeling paradigms in
		  order to support data protection by design from both a
		  legal and a technical perspective.},
  booktitle	= {Proceedings of the 34th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1512–1515},
  numpages	= {4},
  keywords	= {data protection by design, privacy, system model, threat
		  modeling},
  location	= {Limassol, Cyprus},
  series	= {SAC '19}
}

@Article{	  10.1145/3524111,
  author	= {Niraula, Nobal B. and Dulal, Saurab and Koirala, Diwa},
  title		= {Linguistic Taboos and Euphemisms in Nepali},
  year		= {2022},
  issue_date	= {November 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3524111},
  doi		= {10.1145/3524111},
  abstract	= {Languages across the world have words, phrases, and
		  behaviors—the taboos—that are avoided in public
		  communication considering them as obscene or disturbing to
		  the social, religious, and ethical values of society.
		  However, people deliberately use these linguistic taboos
		  and other language constructs to make hurtful, derogatory,
		  and obscene comments. It is nearly impossible to construct
		  a universal set of offensive or taboo terms because
		  offensiveness is determined entirely by different factors
		  such as socio-physical setting, speaker-listener
		  relationship, and word choices. In this article, we present
		  a detailed corpus-based study of offensive language in
		  Nepali. We identify and describe more than 18 different
		  categories of linguistic offenses including politics,
		  religion, race, and sex. We discuss 12 common euphemisms,
		  such as synonym, metaphor, and circumlocution. In addition,
		  we introduce a manually constructed dataset of more than
		  1,000 offensive and taboo terms popular among contemporary
		  speakers. We describe the first experiments that provide
		  baseline results in detecting offensive language in Nepali.
		  This in-depth study of offensive language and resource will
		  provide a foundation for several downstream tasks, such as
		  offensive language detection and language learning.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {129},
  numpages	= {26},
  keywords	= {Offensive language, linguistic taboo, dataset, offensive
		  language detection, low-resource language, Nepali
		  language}
}

@InProceedings{	  10.1145/3297662.3365813,
  author	= {Spiliotopoulos, Dimitris and Margaris, Dionisis and
		  Vassilakis, Costas},
  title		= {Citizen Engagement for Transparent and Accountable Policy
		  Modelling},
  year		= {2020},
  isbn		= {9781450362382},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297662.3365813},
  doi		= {10.1145/3297662.3365813},
  abstract	= {This work presents a platform for linked legislative data
		  to engage citizens in transparent and effective
		  democracies. With a focus on scaling up participatory
		  approaches from local to national level, the approach
		  extends well established and open source tools and
		  technologies, to build mobile monitoring and analysis tools
		  that increase transparency of law-making and implementation
		  to citizens. This is achieved by combining open data and
		  open services with user and citizen generated content, in
		  order to address citizen's needs in the context of open
		  government. Data and feeds from trusted sources are
		  interconnected with new and re-purposed data feeds
		  generated by users via the social web to form a meaningful,
		  searchable, customizable, reusable and open data-focused
		  personalised mobile public service approach. The framework
		  exploits the social aspects of open data, as well as the
		  training of users, citizens and public servants to be able
		  to understand and demand useful public open data, as well
		  as facilitate the opening of more data.},
  booktitle	= {Proceedings of the 11th International Conference on
		  Management of Digital EcoSystems},
  pages		= {158–165},
  numpages	= {8},
  keywords	= {Accountability, Citizen Engagement, Legislation, Mobile
		  Public Services, Natural Language Processing, Policy
		  Modelling, Transparency, e-Government},
  location	= {Limassol, Cyprus},
  series	= {MEDES '19}
}

@InProceedings{	  10.1145/3665601.3669846,
  author	= {Feng, Yanlin and Rahman, Sajjadur and Feng, Aaron and
		  Chen, Vincent and Kandogan, Eser},
  title		= {CMDBench: A Benchmark for Coarse-to-fine Multimodal Data
		  Discovery in Compound AI Systems},
  year		= {2024},
  isbn		= {9798400706943},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665601.3669846},
  doi		= {10.1145/3665601.3669846},
  abstract	= {Compound AI systems (CASs) that employ LLMs as agents to
		  accomplish knowledge-intensive tasks via interactions with
		  tools and data retrievers have garnered significant
		  interest within database and AI communities. While these
		  systems have the potential to supplement typical analysis
		  workflows of data analysts in enterprise data platforms,
		  unfortunately, CASs are subject to the same data discovery
		  challenges that analysts have encountered over the years
		  — silos of multimodal data sources, created across teams
		  and departments within an organization, make it difficult
		  to identify appropriate data sources for accomplishing the
		  task at hand. Existing data discovery benchmarks do not
		  model such multimodality and multiplicity of data sources.
		  Moreover, benchmarks of CASs prioritize only evaluating
		  end-to-end task performance. To catalyze research on
		  evaluating the data discovery performance of multimodal
		  data retrievers in CASs within a real-world setting, we
		  propose CMDBench, a benchmark modeling the complexity of
		  enterprise data platforms. We adapt existing datasets and
		  benchmarks in open-domain — from question answering and
		  complex reasoning tasks to natural language querying over
		  structured data — to evaluate coarse- and fine-grained
		  data discovery and task execution performance. Our
		  experiments reveal the impact of data retriever design on
		  downstream task performance — 46\% drop in task accuracy
		  on average — across various modalities, data sources, and
		  task difficulty. The results indicate the need to develop
		  optimization strategies to identify appropriate LLM agents
		  and retrievers for efficient execution of CASs over
		  enterprise data.},
  booktitle	= {Proceedings of the Conference on Governance, Understanding
		  and Integration of Data for Effective and Responsible AI},
  pages		= {16–25},
  numpages	= {10},
  keywords	= {Benchmark, Compound AI Systems., Data Discovery, LLMs},
  location	= {Santiago, AA, Chile},
  series	= {GUIDE-AI '24}
}

@Article{	  10.1109/tcbb.2022.3173789,
  author	= {Ranjan, Ashish and Fahad, Md Shah and Fern\'{a}ndez-Baca,
		  David and Tripathi, Sudhakar and Deepak, Akshay},
  title		= {MCWS-Transformers: Towards an Efficient Modeling of
		  Protein Sequences via Multi Context-Window Based Scaled
		  Self-Attention},
  year		= {2022},
  issue_date	= {March-April 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {2},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3173789},
  doi		= {10.1109/TCBB.2022.3173789},
  abstract	= {This paper advances the self-attention mechanism in the
		  standard transformer network specific to the modeling of
		  the protein sequences. We introduce a novel
		  &lt;italic&gt;context-window based scaled
		  self-attention&lt;/italic&gt; mechanism for processing
		  protein sequences that is based on the notion of (i)
		  &lt;italic&gt;local context&lt;/italic&gt; and (ii)
		  &lt;italic&gt;large contextual pattern&lt;/italic&gt;. Both
		  notions are essential to building a good representation for
		  protein sequences. The proposed
		  &lt;italic&gt;context-window based scaled
		  self-attention&lt;/italic&gt; mechanism is further used to
		  build the &lt;italic&gt;multi context-window based scaled
		  (MCWS) transformer&lt;/italic&gt; network for the protein
		  function prediction task at the protein sub-sequence level.
		  Overall, the proposed &lt;italic&gt;MCWS
		  transformer&lt;/italic&gt; network produced improved
		  predictive performances, outperforming existing
		  state-of-the-art approaches by substantial margins. With
		  respect to the standard transformer network, the proposed
		  network produced improvements in F1-score of +2.30% and
		  +2.08% on the biological process (BP) and molecular
		  function (MF) datasets, respectively. The corresponding
		  improvements over the state-of-the-art
		  ProtVecGen-Plus+ProtVecGen-Ensemble approach are +3.38%
		  (BP) and +2.86% (MF). Equally important, robust
		  performances were obtained across protein sequences of
		  different lengths.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= may,
  pages		= {1188–1199},
  numpages	= {12}
}

@Article{	  10.1145/3461736,
  author	= {Bowen, Judy and Dittmar, Anke and Weyers, Benjamin},
  title		= {Task Modelling for Interactive System Design: A Survey of
		  Historical Trends, Gaps and Future Needs},
  year		= {2021},
  issue_date	= {June 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {EICS},
  url		= {https://doi.org/10.1145/3461736},
  doi		= {10.1145/3461736},
  abstract	= {Task models have been used for decades in interactive
		  system design and there are several mature modelling
		  approaches with corresponding tool support. However, in our
		  own work, we have also experienced their limitations,
		  especially in situations where task models are partial
		  ancillary models and not primary artifacts. This was one of
		  the motivations for this paper, which presents a systematic
		  examination of literature to better understand the current
		  place of task models in the continual evolution of
		  user-centred software development practices. While overview
		  work in this domain typically focuses on the analysis of
		  representative task modelling notations and/or tools and
		  relies on foundation papers, we apply a mixed top-down and
		  bottom-up approach to identify relevant themes and trends
		  in the use of task models over the last twenty-years. The
		  paper identifies and discusses dominant patterns of use as
		  well as gaps. It provides a comprehensive framing of both
		  past and present trends in task modelling and supports
		  those who want to incorporate task modelling in their own
		  work. From this we identify areas of research that should
		  receive greater attention in order to address future
		  considerations.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= may,
  articleno	= {214},
  numpages	= {22},
  keywords	= {interactive system design, task analysis, task models,
		  task-based design}
}

@InProceedings{	  10.1145/3469213.3470326,
  author	= {Chen, Qiuping},
  title		= {Formal Semantic Model for Mobile Cloud Service System},
  year		= {2021},
  isbn		= {9781450390200},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3469213.3470326},
  doi		= {10.1145/3469213.3470326},
  abstract	= {The paper proposes a semantic retrieval model for mobile
		  learning resources based on ontology. The model includes
		  three modules: word segmentation of retrieval information,
		  semantic expansion, and semantic retrieval. We describe the
		  agent as an object node in the category theory, the
		  interaction and dependency between the agents as a
		  morphism, and the entire cloud service system as a type
		  category graph. The case study shows that the framework
		  cannot only support the modeling and analysis of the
		  service system at the abstract level, but also support the
		  service composition by mapping the abstract model to the
		  realization technology, which can analyze the correctness
		  of the requirement decomposition and service composition
		  well. It can be used to guide the description and
		  construction of the service system.},
  booktitle	= {2021 2nd International Conference on Artificial
		  Intelligence and Information Systems},
  articleno	= {123},
  numpages	= {4},
  location	= {Chongqing, China},
  series	= {ICAIIS 2021}
}

@Article{	  10.1145/3604605,
  author	= {Ibrohim, Muhammad Okky and Bosco, Cristina and Basile,
		  Valerio},
  title		= {Sentiment Analysis for the Natural Environment: A
		  Systematic Review},
  year		= {2023},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3604605},
  doi		= {10.1145/3604605},
  abstract	= {In this systematic review, Kitchenham’s framework is
		  used to explore what tasks, techniques, and benchmarks for
		  Sentiment Analysis have been developed for addressing
		  topics about the natural environment. We comprehensively
		  analyze seven dimensions including contribution, topical
		  focus, data source and query, annotation, language, detail
		  of the task, and technology/algorithm used. By showing how
		  this research area has grown during the last few years, our
		  investigation provides important findings about the results
		  achieved and the challenges that need to be still addressed
		  for making this technology actually helpful for
		  stakeholders such as policymakers and governments.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {88},
  numpages	= {37},
  keywords	= {Natural environment, data-driven policy, sentiment
		  analysis, natural language processing (NLP), systematic
		  review}
}

@Article{	  10.1145/3584861,
  author	= {Das, Ringki and Singh, Thoudam Doren},
  title		= {Image–Text Multimodal Sentiment Analysis Framework of
		  Assamese News Articles Using Late Fusion},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3584861},
  doi		= {10.1145/3584861},
  abstract	= {Before the arrival of the web as a corpus, people detected
		  positive and negative news based on the understanding of
		  the textual content from physical newspaper rather than an
		  automatic identification approach from readily available
		  e-newspapers. Thus, the earlier sentiment analysis approach
		  is based on unimodal data, and less effort is paid to the
		  multimodal data. However, the presence of multimodal
		  information helps us to get a clearer understanding of the
		  sentiment. To the best of our knowledge, less work has been
		  introduced on the image–text multimodal sentiment
		  analysis framework of Assamese, a low-resource Indian
		  language mostly spoken in the northeast part of India. We
		  built an Assamese news articles dataset consisting of news
		  text and associated images and one image caption to conduct
		  an experimental study. Focusing on important words and
		  discriminative regions of the images mostly related to
		  sentiment, two individual unimodal such as textual and
		  visual models are proposed. The visual model is developed
		  using an encoder-decoder–based image caption generation
		  system. An image–text multimodal approach is proposed to
		  explore the internal correlation between textual and visual
		  features for joint sentiment classification. Finally, we
		  propose the multimodal sentiment analysis framework, i.e.,
		  Textual Visual Multimodal Fusion, by employing a late
		  fusion scheme to merge the three different modalities for
		  the final sentiment prediction. Experimental results
		  conducted on the Assamese dataset built in-house
		  demonstrate that the contextual integration of multimodal
		  features delivers better performance than unimodal
		  features.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {161},
  numpages	= {30},
  keywords	= {Multimodal sentiment analysis, low resource language,
		  caption generation, machine learning classifier, late
		  fusion}
}

@Article{	  10.1145/3359253,
  author	= {Kursuncu, Ugur and Gaur, Manas and Castillo, Carlos and
		  Alambo, Amanuel and Thirunarayan, Krishnaprasad and Shalin,
		  Valerie and Achilov, Dilshod and Arpinar, I. Budak and
		  Sheth, Amit},
  title		= {Modeling Islamist Extremist Communications on Social Media
		  using Contextual Dimensions: Religion, Ideology, and Hate},
  year		= {2019},
  issue_date	= {November 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {CSCW},
  url		= {https://doi.org/10.1145/3359253},
  doi		= {10.1145/3359253},
  abstract	= {Terror attacks have been linked in part to online
		  extremist content. Online conversations are cloaked in
		  religious ambiguity, with deceptive intentions, often
		  twisted from mainstream meaning to serve a malevolent
		  ideology. Although tens of thousands of Islamist extremism
		  supporters consume such content, they are a small fraction
		  relative to peaceful Muslims. The efforts to contain the
		  ever-evolving extremism on social media platforms have
		  remained inadequate and mostly ineffective. Divergent
		  extremist and mainstream contexts challenge machine
		  interpretation, with a particular threat to the precision
		  of classification algorithms. Radicalization is a subtle
		  long-running persuasive process that occurs over time. Our
		  context-aware computational approach to the analysis of
		  extremist content on Twitter breaks down this persuasion
		  process into building blocks that acknowledge inherent
		  ambiguity and sparsity that likely challenge both manual
		  and automated classification. Based on prior empirical and
		  qualitative research in social sciences, particularly
		  political science, we model this process using a
		  combination of three contextual dimensions -- religion,
		  ideology, and hate -- each elucidating a degree of
		  radicalization and highlighting independent features to
		  render them computationally accessible. We utilize
		  domain-specific knowledge resources for each of these
		  contextual dimensions such as Qur'an for religion, the
		  books of extremist ideologues and preachers for political
		  ideology and a social media hate speech corpus for hate.
		  The significant sensitivity of the Islamist extremist
		  ideology and its local and global security implications
		  require reliable algorithms for modelling such
		  communications on Twitter. Our study makes three
		  contributions to reliable analysis: (i) Development of a
		  computational approach rooted in the contextual dimensions
		  of religion, ideology, and hate, which reflects strategies
		  employed by online Islamist extremist groups, (ii) An
		  in-depth analysis of relevant tweet datasets with respect
		  to these dimensions to exclude likely mislabeled users, and
		  (iii) A framework for understanding online radicalization
		  as a process to assist counter-programming. Given the
		  potentially significant social impact, we evaluate the
		  performance of our algorithms to minimize mislabeling,
		  where our context-aware approach outperforms a competitive
		  baseline by 10.2\% in precision, thereby enhancing the
		  potential of such tools for use in human review.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= nov,
  articleno	= {151},
  numpages	= {22},
  keywords	= {contextual dimensions, islamist extremism,
		  multi-dimensional modeling, radicalization, user modeling}
}

@InProceedings{	  10.1145/3167132.3167370,
  author	= {Tiso, Alessandro and Reggio, Gianna and Leotta, Maurizio
		  and Ricca, Filippo},
  title		= {A method for developing model to text transformations},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167370},
  doi		= {10.1145/3167132.3167370},
  abstract	= {In the field of business process development, model
		  transformations play a key role, for example for moving
		  from business process models to either code or inputs for
		  simulation systems, as well as to convert models expressed
		  with notation A into equivalent models expressed with
		  notation B. In the literature, many cases of useful
		  transformations of business process models can be found.
		  However, in general each transformation has been developed
		  in an ad-hoc fashion, at a quite low-level, and its quality
		  is often neglected. To ensure the quality of the
		  transformations is important to apply to them all the
		  well-known software engineering principles and practices,
		  from the requirements definition to the testing activities.
		  For this reason, we propose a method, MeDMoT, for
		  developing non-trivial Model to Text Transformations, which
		  prescribes how to: (1) capture and specify the
		  transformation requirements; (2) design the transformation,
		  (3) implement the transformation and (4) test the
		  transformation. The method has been applied in several case
		  studies, including a transformation of UML business
		  processes into inputs for an agent-based simulator.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {138–141},
  numpages	= {4},
  keywords	= {UML, model to text transformations, model-driven},
  location	= {Pau, France},
  series	= {SAC '18}
}

@Article{	  10.1109/taslp.2023.3293030,
  author	= {Yoshino, Koichiro and Chen, Yun-Nung and Crook, Paul and
		  Kottur, Satwik and Li, Jinchao and Hedayatnia, Behnam and
		  Moon, Seungwhan and Fei, Zhengcong and Li, Zekang and
		  Zhang, Jinchao and Feng, Yang and Zhou, Jie and Kim,
		  Seokhwan and Liu, Yang and Jin, Di and Papangelis,
		  Alexandros and Gopalakrishnan, Karthik and Hakkani-Tur,
		  Dilek and Damavandi, Babak and Geramifard, Alborz and Hori,
		  Chiori and Shah, Ankit and Zhang, Chen and Li, Haizhou and
		  Sedoc, Jo\~{a}o and D'Haro, Luis F. and Banchs, Rafael and
		  Rudnicky, Alexander},
  title		= {Overview of the Tenth Dialog System Technology Challenge:
		  DSTC10},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3293030},
  doi		= {10.1109/TASLP.2023.3293030},
  abstract	= {This article introduces the Tenth Dialog System Technology
		  Challenge (DSTC-10). This edition of the DSTC focuses on
		  applying end-to-end dialog technologies for five distinct
		  tasks in dialog systems, namely 1. Incorporation of Meme
		  images into open domain dialogs, 2. Knowledge-grounded
		  Task-oriented Dialogue Modeling on Spoken Conversations, 3.
		  Situated Interactive Multimodal dialogs, 4. Reasoning for
		  Audio Visual Scene-Aware Dialog, and 5. Automatic
		  Evaluation and Moderation of Open-domainDialogue Systems.
		  This article describes the task definition, provided
		  datasets, baselines, and evaluation setup for each track.
		  We also summarize the results of the submitted systems to
		  highlight the general trends of the state-of-the-art
		  technologies for the tasks.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jul,
  pages		= {765–778},
  numpages	= {14}
}

@InProceedings{	  10.1145/3583780.3614753,
  author	= {Hoseini, Sayed and Ali, Ahmed and Shaker, Haron and Quix,
		  Christoph},
  title		= {SEDAR: A Semantic Data Reservoir for Heterogeneous
		  Datasets},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614753},
  doi		= {10.1145/3583780.3614753},
  abstract	= {Data lakes have emerged as a solution for managing vast
		  and diverse datasets for modern data analytics. To prevent
		  them from becoming ungoverned, semantic data management
		  techniques are crucial, which involve connecting metadata
		  with knowledge graphs, following the principles of Linked
		  Data. This semantic layer enables more expressive data
		  management, integration from various sources and enhances
		  data access utilizing the concepts and relations to
		  semantically enrich the data. Some frameworks have been
		  proposed, but requirements like data versioning, linking of
		  datasets, managing machine learning projects, automated
		  semantic modeling and ontology-based data access are not
		  supported in one uniform system. We demonstrate SEDAR, a
		  comprehensive semantic data lake that includes support for
		  data ingestion, storage, processing, and governance with a
		  special focus on semantic data management. The demo will
		  showcase how the system allows for various ingestion
		  scenarios, metadata enrichment, data source linking,
		  profiling, semantic modeling, data integration and
		  processing inside a machine learning life cycle.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5056–5060},
  numpages	= {5},
  keywords	= {data lake, ontology-based data access, semantic data lake,
		  semantic data management, semantic modeling},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3411764.3445674,
  author	= {Zhou, Zhilan and Wen, Ximing and Wang, Yue and Gotz,
		  David},
  title		= {Modeling and Leveraging Analytic Focus During Exploratory
		  Visual Analysis},
  year		= {2021},
  isbn		= {9781450380966},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411764.3445674},
  doi		= {10.1145/3411764.3445674},
  abstract	= {Visual analytics systems enable highly interactive
		  exploratory data analysis. Across a range of fields, these
		  technologies have been successfully employed to help users
		  learn from complex data. However, these same exploratory
		  visualization techniques make it easy for users to discover
		  spurious findings. This paper proposes new methods to
		  monitor a user’s analytic focus during visual analysis of
		  structured datasets and use it to surface relevant articles
		  that contextualize the visualized findings. Motivated by
		  interactive analyses of electronic health data, this paper
		  introduces a formal model of analytic focus, a
		  computational approach to dynamically update the focus
		  model at the time of user interaction, and a prototype
		  application that leverages this model to surface relevant
		  medical publications to users during visual analysis of a
		  large corpus of medical records. Evaluation results with 24
		  users show that the modeling approach has high levels of
		  accuracy and is able to surface highly relevant medical
		  abstracts.},
  booktitle	= {Proceedings of the 2021 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {21},
  numpages	= {15},
  keywords	= {Analytic Focus, Insight Provenance, User Modeling, Visual
		  Analytics},
  location	= {Yokohama, Japan},
  series	= {CHI '21}
}

@InProceedings{	  10.5555/3306127.3331869,
  author	= {Pico-Valencia, Pablo and Holgado-Terriza, Juan A. and
		  Senso, Jos\'{e}},
  title		= {An Agent Model Based on Open Linked Data for Building
		  Internet of Agents Ecosystems},
  year		= {2019},
  isbn		= {9781450363099},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {This paper presents an smart, collaborative and
		  self-adaptive reactive agent model aimed at managing the
		  resources of objects connected to Internet of Things (IoT).
		  This agent model, called Linked Open Agent (LOA), is
		  described using both a semantic agent contract built from
		  descriptors published as linked data, and a workflow for
		  agent control that is completed at runtime by the agent
		  itself to address its behavior. The accuracy for semantic
		  discovering agents partners was evaluated and compared with
		  generic models of discovery such as the Yellow Pages of
		  Java Agent DEvelopment Framework (JADE) and the Java
		  implementation of the Universal Description, Discovery, and
		  Integration (jUDDI). The results demonstrated that our
		  method had a better accuracy for recovering agents than the
		  accuracy of JADE and JUDDI.},
  booktitle	= {Proceedings of the 18th International Conference on
		  Autonomous Agents and MultiAgent Systems},
  pages		= {1536–1538},
  numpages	= {3},
  keywords	= {contract, internet of agents, internet of things, linked
		  open data},
  location	= {Montreal QC, Canada},
  series	= {AAMAS '19}
}

@InProceedings{	  10.1145/3597512.3597529,
  author	= {Hatherall, Louise and Kek\"{u}ll\"{u}o\u{g}lu, Dilara and
		  Kokciyan, Nadin and Rovatsos, Michael and Sethi, Nayha and
		  Vierkant, Tillmann and Vallor, Shannon},
  title		= {Responsible Agency Through Answerability: Cultivating the
		  Moral Ecology of Trustworthy Autonomous Systems},
  year		= {2023},
  isbn		= {9798400707346},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3597512.3597529},
  doi		= {10.1145/3597512.3597529},
  abstract	= {The decades-old debate over so-called ‘responsibility
		  gaps’ in intelligent systems has recently been
		  reinvigorated by rapid advances in machine learning
		  techniques that are delivering many of the capabilities of
		  machine autonomy that Matthias [1] originally anticipated.
		  The emerging capabilities of intelligent learning systems
		  highlight and exacerbate existing challenges with
		  meaningful human control of, and accountability for, the
		  actions and effects of such systems. The related challenge
		  of human ‘answerability’ for system actions and harms
		  has come into focus in recent literature on responsibility
		  gaps [2, 3]. We describe a proposed interdisciplinary
		  approach to designing for answerability in autonomous
		  systems, grounded in an instrumentalist framework of
		  ‘responsible agency cultivation’ drawn from moral
		  philosophy and cognitive sciences as well as empirical
		  results from structured interviews and focus groups in the
		  application domains of health, finance and government. We
		  outline a prototype dialogue agent informed by these
		  emerging results and designed to help bridge the structural
		  gaps in organisations that typically impede the human
		  agents responsible for an autonomous sociotechnical system
		  from answering to vulnerable patients of responsibility.},
  booktitle	= {Proceedings of the First International Symposium on
		  Trustworthy Autonomous Systems},
  articleno	= {50},
  numpages	= {5},
  keywords	= {AI ethics, Agency, Answerability, Dialogue agents,
		  Responsibility gaps, Sociotechnical Systems Design},
  location	= {Edinburgh, United Kingdom},
  series	= {TAS '23}
}

@InProceedings{	  10.1109/models-c.2019.00067,
  author	= {Amrani, Moussa and Blouin, Dominique and Heinrich, Robert
		  and Rensink, Arend and Vangheluwe, Hans and Wortmann,
		  Andreas},
  title		= {Towards a formal specification of multi-paradigm
		  modelling},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00067},
  doi		= {10.1109/MODELS-C.2019.00067},
  abstract	= {The notion of a programming paradigm is used to classify
		  programming languages and their accompanying workflows
		  based on their salient features. Similarly, the notion of a
		  modelling paradigm can be used to characterise the plethora
		  of modelling approaches used to engineer complex
		  Cyber-Physical Systems (CPS). Modelling paradigms encompass
		  formalisms, abstractions, workflows and supporting
		  tool(chain)s. A precise definition of this modelling
		  paradigm notion is lacking however. Such a definition will
		  increase insight, will allow for formal reasoning about the
		  consistency of modelling frameworks and may serve as the
		  basis for the construction of new modelling, simulation,
		  verification, synthesis, ... environments to support design
		  of CPS. We present a formal framework aimed at capturing
		  the notion of modelling paradigm, as a first step towards a
		  comprehensive formalisation of multi-paradigm modelling.
		  Our formalisation is illustrated by CookieCAD, a simple
		  Computer-Aided Design paradigm used in the development of
		  cookie stencils.},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {419–424},
  numpages	= {6},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.1145/3543873.3587662,
  author	= {Paulus, Alexander and Pomp, Andr\'{e} and Meisen, Tobias},
  title		= {The PLASMA Framework: Laying the Path to Domain-Specific
		  Semantics in Dataspaces},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587662},
  doi		= {10.1145/3543873.3587662},
  abstract	= {Modern data management is evolving from centralized
		  integration-based solutions to a non-integration-based
		  process of finding, accessing and processing data, as
		  observed within dataspaces. Common reference dataspace
		  architectures assume that sources publish their own
		  domain-specific schema. These schemas, also known as
		  semantic models, can only be partially created
		  automatically and require oversight and refinement by human
		  modellers. Non-expert users, such as mechanical engineers
		  or municipal workers, often have difficulty building models
		  because they are faced with multiple ontologies, classes,
		  and relations, and existing tools are not designed for
		  non-expert users. The PLASMA framework consists of a
		  platform and auxiliary services that focus on providing
		  non-expert users with an accessible way to create and edit
		  semantic models, combining automation approaches and
		  support systems such as a recommendation engine. It also
		  provides data conversion from raw data to RDF. In this
		  paper we highlight the main features, like the modeling
		  interface and the data conversion engine. We discuss how
		  PLASMA as a tool is suitable for building semantic models
		  by non-expert users in the context of dataspaces and show
		  some applications where PLASMA has already been used in
		  data management projects.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {1474–1479},
  numpages	= {6},
  keywords	= {dataspace, graphical user interface, resource description
		  framework, semantic mapping, semantic model},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@Article{	  10.1109/tcbb.2021.3069040,
  author	= {Zhao, Qichang and Yang, Mengyun and Cheng, Zhongjian and
		  Li, Yaohang and Wang, Jianxin},
  title		= {Biomedical Data and Deep Learning Computational Models for
		  Predicting Compound-Protein Relations},
  year		= {2021},
  issue_date	= {July-Aug. 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {4},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2021.3069040},
  doi		= {10.1109/TCBB.2021.3069040},
  abstract	= {The identification of compound-protein relations (CPRs),
		  which includes compound-protein interactions (CPIs) and
		  compound-protein affinities (CPAs), is critical to drug
		  development. A common method for compound-protein relation
		  identification is the use of &lt;italic&gt;in
		  vitro&lt;/italic&gt; screening experiments. However, the
		  number of compounds and proteins is massive, and
		  &lt;italic&gt;in vitro&lt;/italic&gt; screening experiments
		  are labor-intensive, expensive, and time-consuming with
		  high failure rates. Researchers have developed a
		  computational field called virtual screening (VS) to aid
		  experimental drug development. These methods utilize
		  experimentally validated biological interaction information
		  to generate datasets and use the physicochemical and
		  structural properties of compounds and target proteins as
		  input information to train computational prediction models.
		  At present, deep learning has been widely used in computer
		  vision and natural language processing and has experienced
		  epoch-making progress. At the same time, deep learning has
		  also been used in the field of biomedicine widely, and the
		  prediction of CPRs based on deep learning has developed
		  rapidly and has achieved good results. The purpose of this
		  study is to investigate and discuss the latest applications
		  of deep learning techniques in CPR prediction. First, we
		  describe the datasets and feature engineering (i.e.,
		  compound and protein representations and descriptors)
		  commonly used in CPR prediction methods. Then, we review
		  and classify recent deep learning approaches in CPR
		  prediction. Next, a comprehensive comparison is performed
		  to demonstrate the prediction performance of representative
		  methods on classical datasets. Finally, we discuss the
		  current state of the field, including the existing
		  challenges and our proposed future directions. We believe
		  that this investigation will provide sufficient references
		  and insight for researchers to understand and develop new
		  deep learning methods to enhance CPR predictions.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= mar,
  pages		= {2092–2110},
  numpages	= {19}
}

@InProceedings{	  10.1145/3336191.3371772,
  author	= {Zhou, Tianshuo and Li, Ziyang and Cheng, Gong and Wang,
		  Jun and Wei, Yu'Ang},
  title		= {GREASE: A Generative Model for Relevance Search over
		  Knowledge Graphs},
  year		= {2020},
  isbn		= {9781450368223},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3336191.3371772},
  doi		= {10.1145/3336191.3371772},
  abstract	= {Relevance search is to find top-ranked entities in a
		  knowledge graph (KG) that are relevant to a query entity.
		  Relevance is ambiguous, particularly over a schema-rich KG
		  like DBpedia which supports a wide range of different
		  semantics of relevance based on numerous types of relations
		  and attributes. As users may lack the expertise to
		  formalize the desired semantics, supervised methods have
		  emerged to learn the hidden user-defined relevance from
		  user-provided examples. Along this line, in this paper we
		  propose a novel generative model over KGs for relevance
		  search, named GREASE. The model applies to meta-path based
		  relevance where a meta-path characterizes a particular type
		  of semantics of relating the query entity to answer
		  entities. It is also extended to support properties that
		  constrain answer entities. Extensive experiments on two
		  large-scale KGs demonstrate that GREASE has advanced the
		  state of the art in effectiveness, expressiveness, and
		  efficiency.},
  booktitle	= {Proceedings of the 13th International Conference on Web
		  Search and Data Mining},
  pages		= {780–788},
  numpages	= {9},
  keywords	= {generative model, knowledge graph, meta-path, relevance
		  search},
  location	= {Houston, TX, USA},
  series	= {WSDM '20}
}

@InProceedings{	  10.1145/3594315.3594362,
  author	= {Yu, Yang and Liu, Xiangzhi},
  title		= {Research on enterprise text classification methods of
		  BiLSTM and CNN based on BERT},
  year		= {2023},
  isbn		= {9781450399029},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3594315.3594362},
  doi		= {10.1145/3594315.3594362},
  abstract	= {The traditional enterprise text data classification method
		  ignores the context of the text. Each word is independent
		  from each other and cannot represent semantic information.
		  The text description and classification effect is poor, and
		  the feature engineering needs human intervention, so the
		  generalization ability is not strong. In view of the low
		  efficiency and accuracy of enterprise text data
		  classification, this paper proposes a bidirectional encoder
		  representation based on Transformer (BERT) The enterprise
		  text classification model BBLC-ATT based on convolutional
		  neural networks (CNN) and bi-directional long short-term
		  memory (BiLSTM) neural networks and attention mechanism
		  (Attention). The model uses BERT training word vector and
		  combines the features of CNN and BiLSTM to capture local
		  potential features and context information. Secondly, the
		  feature vectors extracted from the hybrid network layer are
		  input into the self-attention layer to extract the
		  syntactic and semantic features between words in the
		  enterprise text sentences. Finally, this paper compares
		  BBLC-ATT model with traditional deep learning model in
		  terms of accuracy, accuracy, recall and F1 value. The
		  experimental results show that the BBLC-ATT model is
		  superior to other models in all evaluation indicators, and
		  the accuracy rate is increased by 3.28\% - 15.86\%.},
  booktitle	= {Proceedings of the 2023 9th International Conference on
		  Computing and Artificial Intelligence},
  pages		= {491–495},
  numpages	= {5},
  keywords	= {Attention model, BERT model, BiLSTM model, CNN model,
		  Natural language processing, Text content classification},
  location	= {Tianjin, China},
  series	= {ICCAI '23}
}

@Article{	  10.1109/tcbb.2018.2849362,
  author	= {Acharya, Sudipta and Saha, Sriparna and Pradhan,
		  Prasanna},
  title		= {Multi-Factored Gene-Gene Proximity Measures Exploiting
		  Biological Knowledge Extracted from Gene Ontology:
		  Application in Gene Clustering},
  year		= {2020},
  issue_date	= {Jan.-Feb. 2020},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {17},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2018.2849362},
  doi		= {10.1109/TCBB.2018.2849362},
  abstract	= {To describe the cellular functions of proteins and genes,
		  a potential dynamic vocabulary is Gene Ontology (GO), which
		  comprises of three sub-ontologies namely,
		  Biological-process, Cellular-component, and
		  Molecular-function. It has several applications in the
		  field of bioinformatics like annotating/measuring gene-gene
		  or protein-protein semantic similarity, identifying
		  genes/proteins by their GO annotations for disease gene and
		  target discovery, etc. To determine semantic similarity
		  between genes, several semantic measures have been proposed
		  in literature, which involve information content of
		  &lt;italic&gt;GO-terms&lt;/italic&gt;, GO tree structure,
		  or the combination of both. But, most of the existing
		  semantic similarity measures do not consider different
		  topological and information theoretic aspects of
		  &lt;italic&gt;GO-terms&lt;/italic&gt; collectively.
		  Inspired by this fact, in this article, we have first
		  proposed three novel semantic similarity/distance measures
		  for genes covering different aspects of GO-tree. These are
		  further implanted in the frameworks of well-known
		  multi-objective and single-objective based clustering
		  algorithms to determine functionally similar genes. For
		  comparative analysis, 10 popular existing GO based semantic
		  similarity/distance measures and tools are also considered.
		  Experimental results on &lt;italic&gt;Mouse
		  genome&lt;/italic&gt;, &lt;italic&gt;Yeast&lt;/italic&gt;,
		  and &lt;italic&gt;Human genome&lt;/italic&gt; datasets
		  evidently demonstrate the supremacy of multi-objective
		  clustering algorithms in association with proposed
		  multi-factored similarity/distance measures. Clustering
		  outcomes are further validated by conducting some
		  biological/statistical significance tests. Supplementary
		  information is available at
		  &lt;uri&gt;https://www.iitp.ac.in/sriparna/journals.html&lt;/uri&gt;.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= feb,
  pages		= {207–219},
  numpages	= {13}
}

@InProceedings{	  10.1145/3194696.3194706,
  author	= {Azarm, Mana and Peyton, Liam},
  title		= {An ontology for a patient-centric healthcare
		  interoperability framework},
  year		= {2018},
  isbn		= {9781450357340},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3194696.3194706},
  doi		= {10.1145/3194696.3194706},
  abstract	= {Healthcare clients are increasingly interested to be
		  involved and informed of their healthcare delivery and
		  status [1] [2]. They need to be able to access, view, and
		  analyze their health data easily and securely. Clients need
		  one single gateway to their medical records. Some
		  healthcare providers are creating portals for their clients
		  to flow some data for them to view [3]. In addition,
		  clients can request a portion of their health data in paper
		  format from their healthcare providers by filling in forms
		  and manually submitting their requests. But, this is not
		  sufficient for the average healthcare client. There is a
		  need for a platform independent tool that can automatically
		  gather and combine a client's health information from the
		  various providers in their circle of care and provide the
		  information securely and electronically without
		  inconveniencing the client with multiple requests and
		  sharing agreements [4]. Healthcare providers can also
		  benefit from such a tool in the sense of gaining insights
		  from their colleagues' efforts automatically and without
		  starting a separate quest for each piece of information. In
		  this paper we propose framework and toolset that can
		  provide a secure single point of access to a client's full
		  picture of their personal health information. In
		  particular, we delve into one of the key components of our
		  framework which is our proposed ontology.},
  booktitle	= {Proceedings of the International Workshop on Software
		  Engineering in Healthcare Systems},
  pages		= {34–41},
  numpages	= {8},
  keywords	= {healthcare API, healthcare applications, healthcare
		  integration, healthcare ontology, patient centric},
  location	= {Gothenburg, Sweden},
  series	= {SEHS '18}
}

@InProceedings{	  10.1145/3372454.3372465,
  author	= {Fujino, Iwao and Claramunt, Christophe and Boudraa,
		  Abdel-Ouahab},
  title		= {Extracting 4-Attributes Vessel Courses from AIS Data with
		  PQK-Means and Topic Model},
  year		= {2020},
  isbn		= {9781450372015},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3372454.3372465},
  doi		= {10.1145/3372454.3372465},
  abstract	= {AIS (Automatic Identification System) data received from
		  moving vessels over an area of interest can be of very much
		  interest for deriving maritime trajectory patterns. In this
		  paper, a novel approach to extract course patterns from AIS
		  data of vessels is presented. From machine learning and
		  natural language processing principles, a topic model might
		  be used for extracting implicit patterns underlying massive
		  and unstructured collection of incoming data. To apply
		  topic model to AIS data, PQk-means vector quantization to
		  convert AIS data record to code documents is introduced.
		  Then, a topic model is applied to extract course patterns
		  from AIS data. In fact, courses, not only encompasses
		  trajectory locations, but also headings and speeds, are
		  recognized by the proposed algorithm. The performance of
		  PQk-means is evaluated using the relative root mean square
		  error and elapsed time. The potential of the approach is
		  illustrated by a series of experimental results derived
		  from practical AIS data set in a region of North West
		  France.},
  booktitle	= {Proceedings of the 3rd International Conference on Big
		  Data Research},
  pages		= {129–135},
  numpages	= {7},
  keywords	= {Automatic Identification System (AIS), Course Pattern
		  Extraction, Maritime Big Data, PQk-means, Topic Model,
		  Vector Quantization},
  location	= {Cergy-Pontoise, France},
  series	= {ICBDR '19}
}

@InProceedings{	  10.1145/3394486.3403223,
  author	= {Li, Diya and Zaki, Mohammed J.},
  title		= {RECIPTOR: An Effective Pretrained Model for Recipe
		  Representation Learning},
  year		= {2020},
  isbn		= {9781450379984},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394486.3403223},
  doi		= {10.1145/3394486.3403223},
  abstract	= {Recipe representation plays an important role in food
		  computing for perception, recognition, recommendation and
		  other applications. Learning pretrained recipe embeddings
		  is a challenging task, as there is a lack of high quality
		  annotated food datasets. In this paper, we provide a joint
		  approach for learning effective pretrained recipe
		  embeddings using both the ingredients and cooking
		  instructions. We present RECIPTOR, a novel set
		  transformer-based joint model to learn recipe
		  representations, that preserves permutation-invariance for
		  the ingredient set and uses a novel knowledge graph (KG)
		  derived triplet sampling approach to optimize the learned
		  embeddings so that related recipes are closer in the latent
		  semantic space. The embeddings are further jointly
		  optimized by combining similarity among cooking
		  instructions with a KG based triplet loss. We
		  experimentally show that RECIPTOR's recipe embeddings
		  outperform state-of-the-art baselines on two newly designed
		  downstream classification tasks by a wide margin.},
  booktitle	= {Proceedings of the 26th ACM SIGKDD International
		  Conference on Knowledge Discovery \&amp; Data Mining},
  pages		= {1719–1727},
  numpages	= {9},
  keywords	= {food computing, food knowledge graph, recipe embedding,
		  representation learning, set transformer},
  location	= {Virtual Event, CA, USA},
  series	= {KDD '20}
}

@InProceedings{	  10.1145/3419804.3420267,
  author	= {Hassane, Omar and Mustafiz, Sadaf and Khendek, Ferhat and
		  Toeroe, Maria},
  title		= {A Model Traceability Framework for Network Service
		  Management},
  year		= {2020},
  isbn		= {9781450381406},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3419804.3420267},
  doi		= {10.1145/3419804.3420267},
  abstract	= {Automating enactment along with traceability management of
		  processes using model-driven engineering methods could be
		  of significant benefit to the Network Functions
		  Virtualization (NFV) paradigm in view of its move towards
		  zero-touch automation of the orchestration and management
		  of network services (NS). Earlier, we proposed an
		  integrated process modelling and enactment environment with
		  traceability support, MAPLE-T, for NS management. In this
		  paper, we extend MAPLE-T with the notion of intents. We
		  propose the usage of intents at both the process model (PM)
		  and model-transformation levels as part of our traceability
		  information. We define intents as information representing
		  the objective of the PM actions/activities and their
		  implementations. We extend MAPLE-T with traceability
		  visualization support to visualize trace links relating
		  models at different levels through the captured intents.
		  The intent-enriched traceability information and the
		  enhanced visualization enable semantically richer
		  traceability analysis. We apply our traceability generation
		  and analysis approach to the NS design process in order to
		  show the benefits of intents not only for the process, but
		  also for the whole NS lifecycle management operations.},
  booktitle	= {Proceedings of the 12th System Analysis and Modelling
		  Conference},
  pages		= {64–73},
  numpages	= {10},
  keywords	= {megamodelling, model transformation, process modelling,
		  traceability},
  location	= {Virtual Event, Canada},
  series	= {SAM '20}
}

@InProceedings{	  10.1145/3194085.3194086,
  author	= {Damm, Werner and Galbas, Roland},
  title		= {Exploiting learning and scenario-based specification
		  languages for the verification and validation of highly
		  automated driving},
  year		= {2018},
  isbn		= {9781450357395},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3194085.3194086},
  doi		= {10.1145/3194085.3194086},
  abstract	= {We propose a series of methods based on learning key
		  structural properties from traffic data-basis and on
		  statistical model checking, ultimately leading to the
		  construction of a scenario catalogue capturing requirements
		  for controlling criticality for highly autonomous vehicles.
		  We sketch underlying mathematical foundations which allow
		  to derive formal confidence levels that vehicles tested by
		  such a scenario catalogue will maintain the required
		  control of criticality in real traffic matching the
		  probability distributions of key parameters of data
		  recorded in the reference data base employed for this
		  process.},
  booktitle	= {Proceedings of the 1st International Workshop on Software
		  Engineering for AI in Autonomous Systems},
  pages		= {39–46},
  numpages	= {8},
  keywords	= {formal specification, highly automated driving, learning,
		  requirement analysis, statistical model-checking,
		  verification and validation},
  location	= {Gothenburg, Sweden},
  series	= {SEFAIS '18}
}

@Article{	  10.1145/3627168,
  author	= {Liebeskind, Chaya and Liebeskind, Shmuel and Bouhnik,
		  Dan},
  title		= {Machine Translation for Historical Research: A Case Study
		  of Aramaic-Ancient Hebrew Translations},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3627168},
  doi		= {10.1145/3627168},
  abstract	= {In this article, by the ability to translate Aramaic to
		  another spoken languages, we investigated machine
		  translation in a cultural heritage domain for two primary
		  purposes: evaluating the quality of ancient translations
		  and preserving Aramaic (an endangered language). First, we
		  detailed the construction of a publicly available Biblical
		  parallel Aramaic-Hebrew corpus based on two ancient (early
		  2nd to late 4th century) Hebrew-Aramaic translations:
		  Targum Onkelus and Targum Jonathan. Then using the
		  statistical machine translation approach, which in our use
		  case significantly outperforms neural machine translation,
		  we validated the excepted high quality of the translations.
		  The trained model failed to translate Aramaic texts of
		  other dialects. However, when we trained the same
		  statistical machine translation model on another
		  Aramaic-Hebrew corpus of a different dialect (Zohar, 13th
		  century), a very high translation score was achieved. We
		  examined an additional important cultural heritage source
		  of Aramaic texts, the Babylonian Talmud (early 3rd to late
		  5th century). Since we do not have a parallel
		  Aramaic-Hebrew corpus of the Talmud, we used the model
		  trained on the Bible corpus for translation. We performed
		  an analysis of the results and suggest some potential
		  promising future research.},
  journal	= {J. Comput. Cult. Herit.},
  month		= feb,
  articleno	= {20},
  numpages	= {23},
  keywords	= {Bible translation, neural machine translation, statistical
		  machine translation, low-resource languages,
		  Aramaic-Hebrew}
}

@InProceedings{	  10.5555/3374138.3374161,
  author	= {Wagner, Gerd},
  title		= {Process design modeling with extended event graphs},
  year		= {2019},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {Schruben's Event Graphs (EGs), defining the event types of
		  a simulation model and event scheduling arrows between
		  them, representing causal regularities, provide an elegant
		  visual modeling language and formalism for event-based
		  simulation, which can be viewed as the most fundamental
		  Discrete Event Simulation (DES) approach. We show how to
		  extend and visually improve the language of EGs by adding
		  elements of the Business Process Modeling Notation (BPMN):
		  (1) mini diamonds for designating conditional control flow
		  arrows, (2) Gateways for conditional and parallel
		  branching, (3) typed Data Objects for accommodating
		  object-oriented (OO) state structure modeling, and (4)
		  Activities. The resulting extension of EGs, called Discrete
		  Event Process Modeling Notation (DPMN), is more expressive
		  and visually more clear than traditional EGs, and its
		  visual syntax is harmonized with BPMN process diagrams,
		  thus building a bridge between the DES and the Business
		  Process Management research communities.},
  booktitle	= {Proceedings of the 2019 Summer Simulation Conference},
  articleno	= {23},
  numpages	= {12},
  keywords	= {BPMN, DPMN, discrete event simulation, event graphs,
		  object event simulation},
  location	= {Berlin, Germany},
  series	= {SummerSim '19}
}

@Article{	  10.1145/3651313,
  author	= {Denisenko, Natalia and Zhang, Youzhi and Pulice, Chiara
		  and Bhattasali, Shohini and Jajodia, Sushil and Resnik,
		  Philip and Subrahmanian, V.S.},
  title		= {A Psycholinguistics-inspired Method to Counter IP Theft
		  Using Fake Documents},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3651313},
  doi		= {10.1145/3651313},
  abstract	= {Intellectual property (IP) theft is a growing problem. We
		  build on prior work to deter IP theft by generating n fake
		  versions of a technical document so a thief has to expend
		  time and effort in identifying the correct document. Our
		  new SbFAKE framework proposes, for the first time, a novel
		  combination of language processing, optimization, and the
		  psycholinguistic concept of surprisal to generate a set of
		  such fakes. We start by combining psycholinguistic-based
		  surprisal scores and optimization to generate two bilevel
		  surprisal optimization problems (an Explicit one and a
		  simpler Implicit one) whose solutions correspond directly
		  to the desired set of fakes. As bilevel problems are
		  usually hard to solve, we then show that these two bilevel
		  surprisal optimization problems can each be reduced to
		  equivalent surprisal-based linear programs. We performed
		  detailed parameter tuning experiments and identified the
		  best parameters for each of these algorithms. We then
		  tested these two variants of SbFAKE (with their best
		  parameter settings) against the best performing prior work
		  in the field. Our experiments show that SbFAKE is able to
		  more effectively generate convincing fakes than past work.
		  In addition, we show that replacing words in an original
		  document with words having similar surprisal scores
		  generates greater levels of deception.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= jun,
  articleno	= {7},
  numpages	= {25},
  keywords	= {AI for security, fake document generation}
}

@InProceedings{	  10.1145/3701716.3717540,
  author	= {Todorov, Konstantin and Fafalios, Pavlos and Dietze,
		  Stefan and Dimitrov, Dimitar},
  title		= {BeyondFacts 2025: 5th International Workshop on
		  Computational Methods for Online Discourse Analysis},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3717540},
  doi		= {10.1145/3701716.3717540},
  abstract	= {This workshop explores the intersection of computational
		  and interdisciplinary approaches to analyzing online
		  discourse, including claims, arguments, and opinions on
		  controversial topics. With the rise of mis- and
		  disinformation, bias, and echo chambers, NLP-based methods
		  such as argument mining, stance detection, and fact
		  verification have become essential. However, these tasks
		  require robust conceptual foundations across fields like
		  communication studies, computational linguistics, and
		  computer science. BeyondFacts fosters collaboration among
		  diverse research communities-including social sciences,
		  political science, computational journalism, and computer
		  science-to enhance machine-interpretation and analysis of
		  societal debates using techniques from Web mining, AI, and
		  NLP.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {2612–2615},
  numpages	= {4},
  keywords	= {computational fact-checking, computational journalism,
		  intent detection, knowledge graphs, llms, mis- and
		  dis-information spread and detection, online discourse
		  analysis, social media mining, stance viewpoint discovery,
		  web mining},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@Article{	  10.1145/3582261,
  author	= {Wei, Kaiwen and Jin, Li and Zhang, Zequn and Guo, Zhi and
		  Li, Xiaoyu and Liu, Qing and Feng, Weimiao},
  title		= {More Than Syntaxes: Investigating Semantics to Zero-shot
		  Cross-lingual Relation Extraction and Event Argument Role
		  Labelling},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3582261},
  doi		= {10.1145/3582261},
  abstract	= {Syntactic dependency structures are commonly utilized as
		  language-agnostic features to solve the word order
		  difference issues in zero-shot cross-lingual relation and
		  event extraction tasks. However, while sentences in
		  multiple forms can be employed to express the same meaning,
		  the syntactic structure may vary considerably in specific
		  scenarios. To fix this problem, we find semantics are
		  rarely considered, which could provide a more consistent
		  semantic analysis of sentences and be served as another
		  bridge between different languages. Therefore, in this
		  article, we introduce Syntax and Semantic Driven Network
		  (SSDN) to equip syntax and semantic knowledge across
		  languages simultaneously. Specifically,
		  predicate–argument structures from semantic role
		  labelling are explicitly incorporated into word
		  representations. Then, a semantic-aware relational graph
		  convolutional network and a transformer-based encoder are
		  utilized to model both semantic dependency and syntactic
		  dependency structures, respectively. Finally, a fusion
		  module is introduced to integrate output representations
		  adaptively. We conduct experiments on the widely used
		  Automatic Content Extraction 2005 English, Chinese, and
		  Arabic datasets. The evaluation results demonstrate that
		  the proposed method achieves the state-of-the-art
		  performance. Further study also indicates SSDN could
		  produce robust representations that facilitate the transfer
		  operations across languages.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {61},
  numpages	= {21},
  keywords	= {Cross-lingual relation and event extraction, zero-resource
		  transfer, semantic parsing, relational graph convolutional
		  network}
}

@InProceedings{	  10.1145/3539618.3591997,
  author	= {Lu, Jiaying and Shen, Jiaming and Xiong, Bo and Ma,
		  Wenjing and Staab, Steffen and Yang, Carl},
  title		= {HiPrompt: Few-Shot Biomedical Knowledge Fusion via
		  Hierarchy-Oriented Prompting},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591997},
  doi		= {10.1145/3539618.3591997},
  abstract	= {Medical decision-making processes can be enhanced by
		  comprehensive biomedical knowledge bases, which require
		  fusing knowledge graphs constructed from different sources
		  via a uniform index system. The index system often
		  organizes biomedical terms in a hierarchy to provide the
		  aligned entities with fine-grained granularity. To address
		  the challenge of scarce supervision in the biomedical
		  knowledge fusion (BKF) task, researchers have proposed
		  various unsupervised methods. However, these methods
		  heavily rely on ad-hoc lexical and structural matching
		  algorithms, which fail to capture the rich semantics
		  conveyed by biomedical entities and terms. Recently, neural
		  embedding models have proved effective in semantic-rich
		  tasks, but they rely on sufficient labeled data to be
		  adequately trained. To bridge the gap between the
		  scarce-labeled BKF and neural embedding models, we propose
		  HiPrompt, a supervision-efficient knowledge fusion
		  framework that elicits the few-shot reasoning ability of
		  large language models through hierarchy-oriented prompts.
		  Empirical results on the collected KG-Hi-BKF benchmark
		  datasets demonstrate the effectiveness of HiPrompt.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2052–2056},
  numpages	= {5},
  keywords	= {biomedical knowledge fusion, few-shot prompting, large
		  language models for resource-constrained field, retrieve
		  \&amp; re-rank},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3365109.3368779,
  author	= {Liu, Jie and Tang, Yan and Xu, Xinyi},
  title		= {HISDOM: A Hybrid Ontology Mapping System based on
		  Convolutional Neural Network and Dynamic Weight},
  year		= {2019},
  isbn		= {9781450370165},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3365109.3368779},
  doi		= {10.1145/3365109.3368779},
  abstract	= {In the information explosion era, mapping multiple
		  ontologies in different knowledge bases could provide a
		  common layer from which several ontologies could be
		  accessed and exchanged in semantically sound manners.
		  However, most ontology mapping approaches rely heavily on
		  human annotation and are restricted to limited domains.
		  Moreover, most prior work is limited to combining a few
		  mapping factors with moderate mapping accuracy, and the
		  ontology mapping weigh calculation process is not adaptive
		  and dynamic. Based on the current generalization of
		  ontology mapping methods, we propose a novel ontology
		  mapping system called HISDOM to improve the generalization
		  performance of ontology mapping. HISDOM uses comprehensive
		  factors like concept names, attributes, instances, and
		  structural similarities to determine the similarity of
		  ontology. A key novelty of HISDOM is leveraging CNN to
		  calculate the comment similarity to assist the mapping
		  process of concepts in the ontology. Then, HISDOM
		  dynamically derives the weight of different factors in the
		  overall ontology similarity proportional to the amount of
		  information of each factor in the ontology. Finally, based
		  on the overall similarity, HISDOM determines whether the
		  two concepts in the ontology can be mapped. We study the
		  performance of HISDOM through extensive experiments. The
		  results show that HISDOM outperforms several baselines in
		  ontology mapping tasks, the dynamic and adaptive weighting
		  mechanism is effective, and all the mapping factors of
		  HISDOM are positive towards improving the ontology mapping
		  accuracy.},
  booktitle	= {Proceedings of the 6th IEEE/ACM International Conference
		  on Big Data Computing, Applications and Technologies},
  pages		= {67–70},
  numpages	= {4},
  keywords	= {convolutional neural network, dynamic weight, hybrid
		  similarity, ontology mapping},
  location	= {Auckland, New Zealand},
  series	= {BDCAT '19}
}

@Book{		  10.1145/3233795,
  editor	= {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip
		  R. and Sonntag, Daniel and Potamianos, Gerasimos and
		  Kr\"{u}ger, Antonio},
  title		= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan \&amp;
		  Claypool},
  abstract	= {The Handbook of Multimodal-Multisensor Interfaces provides
		  the first authoritative resource on what has become the
		  dominant paradigm for new computer interfaces---user input
		  involving new media (speech, multi-touch, hand and body
		  gestures, facial expressions, writing) embedded in
		  multimodal-multisensor interfaces.This three-volume
		  handbook is written by international experts and pioneers
		  in the field. It provides a textbook, reference, and
		  technology roadmap for professionals working in this and
		  related areas.This third volume focuses on state-of-the-art
		  multimodal language and dialogue processing, including
		  semantic integration of modalities. The development of
		  increasingly expressive embodied agents and robots has
		  become an active test-bed for coordinating multimodal
		  dialogue input and output, including processing of language
		  and nonverbal communication. In addition, major application
		  areas are featured for commercializing
		  multimodal-multisensor systems, including automotive,
		  robotic, manufacturing, machine translation, banking,
		  communications, and others. These systems rely heavily on
		  software tools, data resources, and international standards
		  to facilitate their development. For insights into the
		  future, emerging multimodal-multisensor technology trends
		  are highlighted for medicine, robotics, interaction with
		  smart spaces, and similar topics. Finally, this volume
		  discusses the societal impact of more widespread adoption
		  of these systems, such as privacy risks and how to mitigate
		  them. The handbook chapters provide a number of
		  walk-through examples of system design and processing,
		  information on practical resources for developing and
		  evaluating new systems, and terminology and tutorial
		  support for mastering this emerging field. In the final
		  section of this volume, experts exchange views on a timely
		  and controversial challenge topic, and how they believe
		  multimodal-multisensor interfaces need to be equipped to
		  most effectively advance human performance during the next
		  decade.}
}

@InProceedings{	  10.1145/3625549.3658830,
  author	= {Huang, Shaohan and Luan, Zhongzhi},
  title		= {Semantic-Aware Log Understanding and Analysis},
  year		= {2024},
  isbn		= {9798400704130},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625549.3658830},
  doi		= {10.1145/3625549.3658830},
  abstract	= {The exponential growth in system complexity and the
		  corresponding surge in log data volume necessitate advanced
		  log analysis techniques for efficient system management and
		  anomaly detection. Traditional log understanding and
		  analysis methods often fail to capture the rich semantic
		  context inherent in log messages, leading to suboptimal
		  monitoring and diagnostic capabilities. This paper aims to
		  bridge the semantic gap by integrating cutting-edge
		  semantic technologies into the log analysis pipeline. We
		  leverage natural language processing, information
		  retrieval, and large language models to enrich log data
		  with semantic information, facilitating a deeper
		  understanding of log messages. Our methodology enhances
		  anomaly detection accuracy by utilizing hierarchical
		  contextual information and pre-training technology, and
		  refining log-based QA processes by log retrieval and log
		  reader. Preliminary results demonstrate a significant
		  improvement in identifying and diagnosing system anomalies,
		  as well as in the automated answering log questions. This
		  research not only presents a breakthrough in log data
		  analysis but also sets the stage for future advancements in
		  intelligent system monitoring and proactive fault
		  resolution. Through this semantic-aware approach, we
		  envision a new paradigm in log analysis that transcends
		  traditional machine learning methods, offering a more
		  robust and intuitive understanding of system behaviors and
		  states.},
  booktitle	= {Proceedings of the 33rd International Symposium on
		  High-Performance Parallel and Distributed Computing},
  pages		= {413–416},
  numpages	= {4},
  keywords	= {semantic-aware analysis, log understanding, natural
		  language processing, anomaly detection, log parsing},
  location	= {Pisa, Italy},
  series	= {HPDC '24}
}

@InProceedings{	  10.1145/3345252.3345288,
  author	= {Ivanova, Tatyana},
  title		= {Resources and Semantic-based knowledge models for
		  personalized and self-regulated learning in the Web: survey
		  and trends},
  year		= {2019},
  isbn		= {9781450371490},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3345252.3345288},
  doi		= {10.1145/3345252.3345288},
  abstract	= {Learning is a complex and multifaceted process. Research
		  findings in recent years show that student's control over
		  the learning process is important for achieving higher
		  results. As every student or lifelong learner have his
		  specific interests and needs, in many cases no one learning
		  course can meet all these needs. It is important to ensure
		  possibilities for learners to find additional knowledge
		  sources or tools during his learning process. Cloud
		  Learning consider the entire Web (including Social Web
		  tools, Open Educational Resources and many other sources
		  for learning) as a space for learning content. Finding
		  exactly the needed resource for every learning need in this
		  enormous space is a challenge. There is a need to explore
		  all the resources useful for learning, classify them in a
		  way that will support searching, and to express relations
		  between them using semantic annotations.In this paper we
		  analyze the current state of the resources, appropriate for
		  learning in the internet from technological point of view.
		  We classify resources according to several dimensions,
		  important for searching and using by learners. We take
		  special attention to the ways of semantic description of
		  resources and users (used metadata and models) as metadata
		  are the most important for finding the most appropriate
		  resources for specific learning task. Tasks as searching
		  and retrieval for learning are not new and our main aim in
		  this work is to outline resent changes in web-based
		  learning, resulting from technological changes and discuss
		  how they will affect resource searching. We will discuss
		  how cloud-based technologies and embedded semantic
		  descriptions will simplify searching and finding
		  appropriate learning sources.},
  booktitle	= {Proceedings of the 20th International Conference on
		  Computer Systems and Technologies},
  pages		= {316–323},
  numpages	= {8},
  keywords	= {Cloud-based learning, E-Learning, Ontology, Web-based
		  learning},
  location	= {Ruse, Bulgaria},
  series	= {CompSysTech '19}
}

@Article{	  10.1145/3664615,
  author	= {Ji, Shaoxiong and Li, Xiaobo and Sun, Wei and Dong, Hang
		  and Taalas, Ara and Zhang, Yijia and Wu, Honghan and
		  Pitk\"{a}nen, Esa and Marttinen, Pekka},
  title		= {A Unified Review of Deep Learning for Automated Medical
		  Coding},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {12},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3664615},
  doi		= {10.1145/3664615},
  abstract	= {Automated medical coding, an essential task for healthcare
		  operation and delivery, makes unstructured data manageable
		  by predicting medical codes from clinical documents. Recent
		  advances in deep learning and natural language processing
		  have been widely applied to this task. However, deep
		  learning–based medical coding lacks a unified view of the
		  design of neural network architectures. This review
		  proposes a unified framework to provide a general
		  understanding of the building blocks of medical coding
		  models and summarizes recent advanced models under the
		  proposed framework. Our unified framework decomposes
		  medical coding into four main components, i.e., encoder
		  modules for text feature extraction, mechanisms for
		  building deep encoder architectures, decoder modules for
		  transforming hidden representations into medical codes, and
		  the usage of auxiliary information. Finally, we introduce
		  the benchmarks and real-world usage and discuss key
		  research challenges and future directions.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {306},
  numpages	= {41},
  keywords	= {Medical coding, deep learning, unified framework}
}

@InProceedings{	  10.5555/3466184.3466189,
  author	= {Wagner, Gerd},
  title		= {Business process modeling and simulation with DPMN:
		  resource-constrained activities},
  year		= {2021},
  isbn		= {9781728194998},
  publisher	= {IEEE Press},
  abstract	= {This tutorial article, which is extracted from (Wagner
		  2019), shows how to use UML Class Diagrams and Discrete
		  Event Process Modeling Notation (DPMN) Process Diagrams for
		  making simulation models of business processes with
		  resource-constrained activities based on the DES paradigm
		  of Object Event Modeling and Simulation. In this approach,
		  the state structure of a business system is captured by a
		  UML Class Diagram, which defines the types of objects,
		  events and activities underlying a DPMN Process Diagram,
		  which captures the causal regularities of the system in the
		  form of a set of event rules. DPMN Process Diagrams extend
		  the Event Graphs proposed by Schruben (1983) by adding
		  elements from the Business Process Modeling Notation
		  (BPMN), viz. data objects and activities, and, as its main
		  innovation over BPMN, resource-dependent activity start
		  arrows.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {45–59},
  numpages	= {15},
  location	= {Orlando, Florida},
  series	= {WSC '20}
}

@InProceedings{	  10.1145/3448823.3448854,
  author	= {Raich, Krispin and Kathrein, Robert and Erharter, Michael
		  and D\"{o}ller, Mario},
  title		= {Spatial Extension Model for Multimodal Traffic
		  Management},
  year		= {2021},
  isbn		= {9781450389532},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3448823.3448854},
  doi		= {10.1145/3448823.3448854},
  abstract	= {The management of traffic information is an essential
		  component of Intelligent Transport Systems (ITS). This
		  traffic is no longer confined to roads but is increasingly
		  emerging into the airspace. Recent developments in the
		  field of unmanned aerial systems (UAS) enable new ways of
		  transportation. Furthermore, the sensor systems of these
		  traffic participants, such as cars, drones, and also
		  intelligent infrastructure, are becoming increasingly
		  prominent and powerful. Thus, future ITS will be confronted
		  with the challenge of ever-increasing amounts of data from
		  a wide variety of different traffic participants. Based on
		  this, a novel concept is presented, how data from
		  multi-modal traffic users can be accumulated. To accomplish
		  this, a parametrized geographic data-centric model is
		  presented that can map traffic routes of arbitrary shape in
		  2- and 3D environments. By this, traffic management of
		  multimodal vehicles (cars, UAS, etc.) can be represented by
		  one unique multimodal model. Furthermore, the parametrized
		  data model allows situational data processing (e.g.
		  congestion, weather condition, etc.) on a local and global
		  basis. The geographic model extends Geo JSON as its
		  foundation in order to rely on well-established
		  standards.},
  booktitle	= {Proceedings of the 2020 4th International Conference on
		  Vision, Image and Signal Processing},
  articleno	= {51},
  numpages	= {6},
  keywords	= {3D corridor, UAS paths, air traffic management, drone
		  corridor, intelligent transportation systems},
  location	= {Bangkok, Thailand},
  series	= {ICVISP 2020}
}

@InProceedings{	  10.5555/3466184.3466471,
  author	= {Li, Yitong and Ji, Wenying and AbouRizk, Simaan M.},
  title		= {Automated abstraction of operation processes from
		  unstructured text for simulation modeling},
  year		= {2021},
  isbn		= {9781728194998},
  publisher	= {IEEE Press},
  abstract	= {Abstraction of operation processes is a fundamental step
		  for simulation modeling. To reliably abstract an operation
		  process, modelers rely on text information to study and
		  understand details of operations. Aiming at reducing
		  modelers' interpretation load and ensuring the reliability
		  of the abstracted information, this research proposes a
		  systematic methodology to automate the abstraction of
		  operation processes. The methodology applies rule-based
		  information extraction to automatically extract operation
		  process-related information from unstructured text and
		  creates graphical representations of operation processes
		  using the extracted information. To demonstrate the
		  applicability and feasibility of the proposed methodology,
		  a text description of an earthmoving operation is used to
		  create its corresponding graphical representation. Overall,
		  this research enhances the state-of-the-art simulation
		  modeling through achieving automated abstraction of
		  operation processes, which largely reduces modelers'
		  interpretation load and ensures the reliability of the
		  abstracted operation processes.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2517–2525},
  numpages	= {9},
  location	= {Orlando, Florida},
  series	= {WSC '20}
}

@InProceedings{	  10.1145/3323503.3360644,
  author	= {Bertalan, Vithor Gomes and Ruiz, Evandro Eduardo Seron},
  title		= {Using topic modeling to find main discussion topics in
		  brazilian political websites},
  year		= {2019},
  isbn		= {9781450367639},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3323503.3360644},
  doi		= {10.1145/3323503.3360644},
  abstract	= {Knowing the main discussion topics debated by the general
		  public is a valuable asset to politicians and professionals
		  involved with politics. Lately, alternative media websites
		  became popular venues in which political ideas are debated
		  without the influence of mainstream media. In this article,
		  we propose the construction of a topic modeling framework,
		  using LSI, LDA, and HDP, to identify main discussion issues
		  in political websites. Experiments show that these models
		  presented results similar to state of the art, offering a
		  viable solution to track political discourse in left-wing
		  and right-wing websites.},
  booktitle	= {Proceedings of the 25th Brazillian Symposium on Multimedia
		  and the Web},
  pages		= {245–248},
  numpages	= {4},
  keywords	= {hierarchial dirichlet process, latent dirichlet
		  allocation, latent semantic indexing, political texts,
		  topic coherence, topic modeling},
  location	= {Rio de Janeiro, Brazil},
  series	= {WebMedia '19}
}

@InProceedings{	  10.5555/3466184.3466452,
  author	= {Wilsdorf, Pia and Haack, Fiete and Uhrmacher, Adelinde
		  M.},
  title		= {Conceptual models in simulation studies: making it
		  explicit},
  year		= {2021},
  isbn		= {9781728194998},
  publisher	= {IEEE Press},
  abstract	= {Conceptual models play an important role in conducting
		  simulation studies. A formal or at least explicit
		  specification of conceptual models is key for effectively
		  exploiting them during simulation studies and thereafter,
		  for interpreting and reusing the simulation results.
		  However, the perception of conceptual models varies
		  strongly and with it possible means for specification. A
		  broad definition of the conceptual model, i.e., as a loose
		  collection of early-stage products of the simulation study,
		  holds the potential to unify existing definitions, but also
		  poses specific challenges for specification. To approach
		  these challenges, without claiming to be exhaustive, we
		  identify a set of products, which includes research
		  question, data, and requirements, and define relations and
		  properties of these products. Based on a cell biological
		  case study and a prototypical implementation, we show how
		  the formal structuring of the conceptual model assists in
		  building a simulation model.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2353–2364},
  numpages	= {12},
  location	= {Orlando, Florida},
  series	= {WSC '20}
}

@Article{	  10.1145/3715114,
  author	= {Chen, Xiaohong and Chen, Shi and Jin, Zhi and Bian, Han
		  and Chen, Zihan and Li, Haotian},
  title		= {Expressing the Needs in Smart Home: What Is the End
		  Users’ Favorite Way},
  year		= {2025},
  issue_date	= {April 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {32},
  number	= {2},
  issn		= {1073-0516},
  url		= {https://doi.org/10.1145/3715114},
  doi		= {10.1145/3715114},
  abstract	= {The Internet of Things (IoT) has witnessed remarkable
		  advancements, enabling smart homes with user-centric
		  features. To effectively articulate their personalized
		  needs, it becomes crucial to equip end users with
		  programming capabilities. Currently, the executable
		  Trigger-Action Programming (TAP) rules have become the
		  mainstream paradigm for IoT end-user programming. To
		  simplify the creation of TAP rules, many studies have
		  proposed various levels of requirements abstraction, yet
		  the connections between them remain unclear. In this
		  article, we employ a mixed-methods study to identify the
		  preferred way of expressing end users’ requirements in
		  practical scenarios. Subsequently, from the perspective of
		  requirements engineering, we categorize the needs of smart
		  home into three hierarchical levels of abstraction.
		  Accordingly, we propose an innovative multi-level
		  requirements description language called SH-RDL. We also
		  address potential challenges and conduct an evaluation to
		  validate SH-RDL’s usability, understandability and
		  error-prevention. This will aid in the broader adoption of
		  IoT end-user programming.},
  journal	= {ACM Trans. Comput.-Hum. Interact.},
  month		= apr,
  articleno	= {16},
  numpages	= {38},
  keywords	= {IoT End-User Programming, Requirements Engineering, Smart
		  Homes, User Intentions, Requirements Description Language}
}

@InProceedings{	  10.1145/3184558.3191538,
  author	= {Baader, Franz and Borgwardt, Stefan and Forkel, Walter},
  title		= {Patient Selection for Clinical Trials Using Temporalized
		  Ontology-Mediated Query Answering},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3191538},
  doi		= {10.1145/3184558.3191538},
  abstract	= {Finding suitable candidates for clinical trials is a
		  labor-intensive task that requires expert medical
		  knowledge. Our goal is to design (semi-)automated
		  techniques that can support clinical researchers in this
		  task. We investigate the issues involved in designing
		  formal query languages for selecting patients that are
		  eligible for a given clinical trial, leveraging existing
		  ontology-based query answering techniques. In particular,
		  we propose to use a temporal extension of existing
		  approaches for accessing data through ontologies written in
		  Description Logics. We sketch how such a query answering
		  system could work and show that eligibility criteria and
		  patient data can be adequately modeled in our formalism.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {1069–1074},
  numpages	= {6},
  keywords	= {clinical trials, patient cohort recruitment, temporal
		  description logic},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3388176.3388209,
  author	= {Barth, Linard and Ehrat, Matthias and Fuchs, Rainer and
		  Haarmann, Jens},
  title		= {Systematization of Digital Twins: Ontology and Conceptual
		  Framework},
  year		= {2020},
  isbn		= {9781450377256},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3388176.3388209},
  doi		= {10.1145/3388176.3388209},
  abstract	= {The development and progress in information and
		  communication technologies will transform traditional
		  products into smart products and allow to offer novel smart
		  services [1]. Herein, the digital twin (DT) concept is
		  regarded as a key technology to create value with smart
		  services [2]. Although the research and applications of DTs
		  emerge continuously many concerns are to be scrutinized
		  [3]. The lack of a shared conceptual framework for DTs with
		  an unambiguous terminology [4] complicates cross-functional
		  discussions. Therefore, a systematization of the main
		  dimensions of DTs is proposed in the form of an ontology
		  and a conceptual framework thereof derived. The research
		  questions addressed in this paper are a) «Which dimensions
		  are used to classify and structure DTs in academic
		  literature?», b) «What are the fundamental differences or
		  specifications within these dimensions?» and c) «How do
		  these different specifications relate to each other?» The
		  focus of the research is on the objective to find
		  classification systematics that are a) representing the
		  entire spectrum of DTs, b) universally valid in all DT
		  related domains and c) applicable in research and practice.
		  A systematic literature review on the relevant aspects of
		  DTs was conducted and the findings iteratively advanced
		  within workshop sessions with academic experts. DTs are
		  considered as integrators of physical and digital worlds as
		  well as internal and external value creation. Further, the
		  creation of DTs requires per definition the use of digital
		  data. Hence, the proposed ontology and conceptual framework
		  for DTs include the following main dimensions to consider
		  for every DT: Data resources, external value creation and
		  internal value creation. The main subdimensions of the data
		  resources are the data sources to obtain the data, the data
		  categories and the data formats. The main subdimension of
		  the external value creation are the attributes of the
		  services as the basis of the value propositions, the level
		  of smartness of the connected products and the actors on
		  the different levels of the ecosystem. The main
		  subdimensions of the internal value creation are the
		  lifecycle phases of products, the product management levels
		  and the different generations of both. The proposed
		  ontology and conceptual framework support researchers and
		  practitioners in positioning and structuring their intended
		  DT activities and communicating them to internal and
		  external stakeholders. The holistic view on the data
		  resource dimension further allows to easily deduct the
		  needed data for certain applications or deduct possible
		  applications from already available data.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Information Science and Systems},
  pages		= {13–23},
  numpages	= {11},
  keywords	= {Digital Twin, conceptual framework, ontology,
		  systematization},
  location	= {Cambridge, United Kingdom},
  series	= {ICISS '20}
}

@InProceedings{	  10.1145/3209219.3209259,
  author	= {Martin, St\'{e}phane and Faltings, Boi and Schickel,
		  Vincent},
  title		= {Enhancing Session-Based Recommendations through Sequential
		  Modeling},
  year		= {2018},
  isbn		= {9781450355896},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209219.3209259},
  doi		= {10.1145/3209219.3209259},
  abstract	= {Recommender systems typically determine the items they
		  should recommend by learning models of user-preferences.
		  Most often, those preferences are modeled as static and
		  independent of context. In real life however, users
		  consider items in sequence: TV series are watched episode
		  by episode and accessories are chosen after the main
		  appliance. Unfortunately, since sequences are more complex
		  to model, they are often not taken into account. We
		  developed an efficient sequence-modeling approach based on
		  Bayesian Variable-order Markov Models and combined it with
		  an existing content-based system, the Ontology Filtering.
		  We tested this approach through live evaluations on two
		  e-commerce sites. It dramatically increased performance,
		  more than doubling the CTR and strongly increasing
		  recommendation-mediated sales. These tests also confirm
		  that the technique works efficiently and reliably in a
		  production setting.},
  booktitle	= {Proceedings of the 26th Conference on User Modeling,
		  Adaptation and Personalization},
  pages		= {359–360},
  numpages	= {2},
  keywords	= {context-tree, e-commerce, recommender systems,
		  sequence-modeling, variable-order Markov model},
  location	= {Singapore, Singapore},
  series	= {UMAP '18}
}

@InBook{	  10.1145/3728725.3728757,
  author	= {Xia, Weiyi and Zhang, Yinggang and Zhao, Ben and Liu, Wei
		  and Han, Linjie and Ye, Qifu},
  title		= {Intelligent PLC Code Generation in HCPS 2.0: A
		  Multi-dimensional Taxonomy and Evolutionary Framework},
  year		= {2025},
  isbn		= {9798400713453},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3728725.3728757},
  abstract	= {The manufacturing industry is accelerating towards the
		  New-Generation Intelligent Manufacturing, where the
		  automatic generation of Programmable Logic Controllers
		  (PLCs) code serves as a key technology, crucial for
		  enhancing the intelligence and efficiency of manufacturing
		  systems. Traditional manual programming has become
		  increasingly inadequate to meet the demands for flexibility
		  and real-time performance in intelligent manufacturing.
		  This paper reviews the integration of intelligent
		  manufacturing and PLC technologies, highlighting the core
		  role of Human-Cyber-Physical Systems (HCPS) and
		  demonstrating how it effectively guides theoretical
		  research and engineering practices in new-generation
		  intelligent manufacturing. We innovatively classify
		  automatic PLC code generation methods through multiple
		  dimensions and propose a new framework for Structured Text
		  (ST) code generation. Finally, this paper discusses future
		  trends in controller code generation, identifying
		  multimodal-driven intelligent generation, adaptive learning
		  and real-time optimization, and cloud-edge collaborative
		  intelligent generation as significant directions, forming a
		  technical roadmap of "parallel promotion and integrated
		  development" to support comprehensive intelligent
		  transformation and upgrading of manufacturing.},
  booktitle	= {Proceedings of the 2025 2nd International Conference on
		  Generative Artificial Intelligence and Information
		  Security},
  pages		= {202–212},
  numpages	= {11}
}

@Article{	  10.1145/3450969,
  author	= {Mousavi, Zahra and Faili, Heshaam},
  title		= {Developing the Persian Wordnet of Verbs Using Supervised
		  Learning},
  year		= {2021},
  issue_date	= {July 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3450969},
  doi		= {10.1145/3450969},
  abstract	= {Nowadays, wordnets are extensively used as a major
		  resource in natural language processing and information
		  retrieval tasks. Therefore, the accuracy of wordnets has a
		  direct influence on the performance of the involved
		  applications. This paper presents a fully-automated method
		  for extending a previously developed Persian wordnet to
		  cover more comprehensive and accurate verbal entries. At
		  first, by using a bilingual dictionary, some Persian verbs
		  are linked to Princeton WordNet synsets. A feature set
		  related to the semantic behavior of compound verbs as the
		  majority of Persian verbs is proposed. This feature set is
		  employed in a supervised classification system to select
		  the proper links for inclusion in the wordnet. We also
		  benefit from a pre-existing Persian wordnet, FarsNet, and a
		  similarity-based method to produce a training set. This is
		  the largest automatically developed Persian wordnet with
		  more than 27,000 words, 28,000 PWN synsets and 67,000
		  word-sense pairs that substantially outperforms the
		  previous Persian wordnet with about 16,000 words, 22,000
		  PWN synsets and 38,000 word-sense pairs.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {68},
  numpages	= {18},
  keywords	= {verbs, Persian language, wordnet, Ontology}
}

@InProceedings{	  10.1145/3646547.3689015,
  author	= {Huang, Ziyuan and Tang, Jiaming and Karir, Manish and Liu,
		  Mingyan and Sarabi, Armin},
  title		= {Analyzing Corporate Privacy Policies using AI Chatbots},
  year		= {2024},
  isbn		= {9798400705922},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3646547.3689015},
  doi		= {10.1145/3646547.3689015},
  abstract	= {In this paper, we present and evaluate an automated
		  pipeline for the large-scale analysis of corporate privacy
		  policies. Organizations usually develop their privacy
		  policies in isolation to best balance their business needs,
		  user rights, as well as regulatory requirements. A
		  wide-ranging and structured analysis of corporate privacy
		  policies is essential to facilitate a deeper understanding
		  of how organizations have balanced competing requirements.
		  Our approach consists of a web crawler that can navigate to
		  and scrape content from web pages that contain privacy
		  policies, and a set of AI chatbot task prompts to process
		  and extract structured/labeled annotations from the raw
		  data. The analysis includes the types of collected user
		  data, the purposes for which data is collected and
		  processed, data retention and protection practices, and
		  user rights and choices. Our validation shows that our
		  annotations are highly accurate and consistent. We use this
		  architecture to gather data on the privacy policies of
		  companies in the Russell 3000 index, resulting in hundreds
		  of thousands of annotations across all categories. Analysis
		  of the resulting data allows us to obtain unique insights
		  into the state of the privacy policy ecosystem as a
		  whole.},
  booktitle	= {Proceedings of the 2024 ACM on Internet Measurement
		  Conference},
  pages		= {505–515},
  numpages	= {11},
  keywords	= {ai chatbots, large language models, privacy policies, text
		  annotation, web crawling},
  location	= {Madrid, Spain},
  series	= {IMC '24}
}

@InProceedings{	  10.1145/3372020.3391559,
  author	= {Foster, Simon and Nemouchi, Yakoub and O'Halloran, Colin
		  and Stephenson, Karen and Tudor, Nick},
  title		= {Formal Model-Based Assurance Cases in Isabelle/SACM: An
		  Autonomous Underwater Vehicle Case Study},
  year		= {2020},
  isbn		= {9781450370714},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3372020.3391559},
  doi		= {10.1145/3372020.3391559},
  abstract	= {Isabelle/SACM is a tool for automated construction of
		  model-based assurance cases with integrated formal methods,
		  based on the Isabelle proof assistant. Assurance cases show
		  how a system is safe to operate, through a human
		  comprehensible argument demonstrating that the requirements
		  are satisfied, using evidence of various provenances. They
		  are usually required for certification of critical systems,
		  often with evidence that originates from formal methods.
		  Automating assurance cases increases rigour, and helps with
		  maintenance and evolution. In this paper we apply
		  Isabelle/SACM to a fragment of the assurance case for an
		  autonomous underwater vehicle demonstrator. We encode the
		  metric unit system (SI) in Isabelle, to allow modelling
		  requirements and state spaces using physical units. We
		  develop a behavioural model in the graphical RoboChart
		  state machine language, embed the artifacts into
		  Isabelle/SACM, and use it to demonstrate satisfaction of
		  the requirements.},
  booktitle	= {Proceedings of the 8th International Conference on Formal
		  Methods in Software Engineering},
  pages		= {11–21},
  numpages	= {11},
  keywords	= {Assurance Cases, Autonomous Systems, Isabelle/HOL,
		  Verification},
  location	= {Seoul, Republic of Korea},
  series	= {FormaliSE '20}
}

@InProceedings{	  10.1145/3576842.3589161,
  author	= {Zhang, Ruipeng and Xie, Mengjun},
  title		= {A Knowledge Graph Question Answering Approach to IoT
		  Forensics},
  year		= {2023},
  isbn		= {9798400700378},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3576842.3589161},
  doi		= {10.1145/3576842.3589161},
  abstract	= {Internet of Things (IoT) forensics has been a particularly
		  challenging task for forensic practitioners due to the
		  heterogeneity of IoT environments as well as the complexity
		  and volume of IoT data. With the advent of artificial
		  intelligence, question-answering (QA) systems have emerged
		  as a potential solution for users to access sophisticated
		  forensic knowledge and data. In this light, we present a
		  novel IoT forensics framework that employs knowledge graph
		  question answering (KGQA). Our framework enables
		  investigators to access forensic artifacts and
		  cybersecurity knowledge using natural language questions
		  facilitated by a deep-learning-powered KGQA model. The
		  proposed framework demonstrates high efficacy in answering
		  natural language questions over the experimental IoT
		  forensic knowledge graph.},
  booktitle	= {Proceedings of the 8th ACM/IEEE Conference on Internet of
		  Things Design and Implementation},
  pages		= {446–447},
  numpages	= {2},
  keywords	= {Digital Forensics, Internet of Things, Knowledge Graph,
		  Ontology Design, Question Answering},
  location	= {San Antonio, TX, USA},
  series	= {IoTDI '23}
}

@InBook{	  10.1145/3382097.3382112,
  title		= {Ontologies on the Web—putting it all together},
  year		= {2020},
  isbn		= {9781450376174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3382097.3382112},
  abstract	= {Enterprises have made amazing advances by taking advantage
		  of data about their business to provide predictions and
		  understanding of their customers, markets, and products.
		  But as the world of business becomes more interconnected
		  and global, enterprise data is no long a monolith; it is
		  just a part of a vast web of data. Managing data on a
		  world-wide scale is a key capability for any business
		  today.The Semantic Web treats data as a distributed
		  resource on the scale of the World Wide Web, and
		  incorporates features to address the challenges of massive
		  data distribution as part of its basic design. The aim of
		  the first two editions was to motivate the Semantic Web
		  technology stack from end-to-end; to describe not only what
		  the Semantic Web standards are and how they work, but also
		  what their goals are and why they were designed as they
		  are. It tells a coherent story from beginning to end of how
		  the standards work to manage a world-wide distributed web
		  of knowledge in a meaningful way.The third edition builds
		  on this foundation to bring Semantic Web practice to
		  enterprise. Fabien Gandon joins Dean Allemang and Jim
		  Hendler, bringing with him years of experience in global
		  linked data, to open up the story to a modern view of
		  global linked data. While the overall story is the same,
		  the examples have been brought up to date and applied in a
		  modern setting, where enterprise and global data come
		  together as a living, linked network of data. Also included
		  with the third edition, all of the data sets and queries
		  are available online for study and experimentation at
		  data.world/swwo.},
  booktitle	= {Semantic Web for the Working Ontologist: Effective
		  Modeling for Linked Data, RDFS, and OWL}
}

@InProceedings{	  10.1145/3444370.3444600,
  author	= {Cai, Yinyin and Gu, Zhaoquan and Wang, Le and Li, Shudong
		  and Han, Weihong},
  title		= {An APT Group Knowledge Model based on MDATA},
  year		= {2021},
  isbn		= {9781450387828},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3444370.3444600},
  doi		= {10.1145/3444370.3444600},
  abstract	= {Situational awareness is significant for cyber security,
		  which can help researchers and security analysts obtain the
		  network security situation comprehensively and accurately.
		  Advanced Persistent Threat (APT) attack could cause severe
		  consequences to cyberspace and detecting such attacks have
		  become a very important part of cyber security situational
		  awareness. Some APT attacks may belong to a same group,
		  many countries and organizations have established databases
		  for APT groups, such as adopting knowledge graph (KG) to
		  represent the knowledge. However, cyberspace security
		  knowledge varies by temporal and spatial characteristics,
		  such as the attack technologies are updated very
		  frequently, traditional KG cannot represent such knowledge
		  timely. To address this problem, the MDATA
		  (Multi-dimensional Data Association and inTelligent
		  Analysis) model is proposed in [1], which is a supplement
		  and improvement to traditional KG. In this paper, we
		  introduce an APT group knowledge model based on MDATA,
		  which adds spatial-temporal characteristics of the APT
		  groups. We also analyze how this knowledge model could help
		  address the challenges of APT attack awareness.},
  booktitle	= {Proceedings of the 2020 International Conference on
		  Cyberspace Innovation of Advanced Technologies},
  pages		= {374–378},
  numpages	= {5},
  keywords	= {spatial-temporal characteristics, knowledge model, cyber
		  situational awareness, MDATA, APT attack},
  location	= {Guangzhou, China},
  series	= {CIAT 2020}
}

@Article{	  10.1145/3704723,
  author	= {Buneman, Peter and Dosso, Dennis and Lissandrini, Matteo
		  and Silvello, Gianmaria and Sun, He},
  title		= {Can We Measure the Impact of a Database?},
  year		= {2025},
  issue_date	= {May 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {68},
  number	= {5},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3704723},
  doi		= {10.1145/3704723},
  abstract	= {If we want to measure the impact of a database, can we use
		  its organization to treat it the same way we treat any
		  other publishing agent, such as a journal or an author?},
  journal	= {Commun. ACM},
  month		= apr,
  pages		= {69–76},
  numpages	= {8},
  keywords	= {Data Citation, h-index, Scientific and Curated Databases}
}

@InProceedings{	  10.1145/3320435.3320472,
  author	= {Guchev, Vladimir and Cena, Federica and Vernero, Fabiana
		  and Gena, Cristina},
  title		= {Visual Annotations for Hybrid Graph-based User Model},
  year		= {2019},
  isbn		= {9781450360210},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3320435.3320472},
  doi		= {10.1145/3320435.3320472},
  abstract	= {Structured user model data not only allow system
		  personalization, but also may be of interest as a source
		  for analysis: in particular, for the study of general
		  trends and for the detection of anomalies in preferences
		  and mutually-referenced features among different user
		  models. Such sources are multidimensional and interrelated,
		  and recently started to be represented as graph-based
		  datasets. Among the most effective ways of studying such
		  data is visual exploration based on data-driven graph
		  drawing approaches: in particular, node-link and
		  node-link-group diagrams. The paper provides an overview of
		  advanced approaches to the graphical representation of
		  multidimensional data derived from user modeling and
		  presents a proposal for developing flexible and scalable
		  user interfaces for the hypergraph-based visual exploration
		  of relations within a user model (UM). Then, we propose
		  these principles in the visualization of an existing
		  adaptive system.},
  booktitle	= {Proceedings of the 27th ACM Conference on User Modeling,
		  Adaptation and Personalization},
  pages		= {31–35},
  numpages	= {5},
  keywords	= {graph and hypergraph drawing, user model visualization},
  location	= {Larnaca, Cyprus},
  series	= {UMAP '19}
}

@InProceedings{	  10.1145/3703790.3703811,
  author	= {Gui, Zhou and Freund, Michael and Harth, Andreas},
  title		= {A Context-aware Conversational Interface for Controlling
		  Internet-of-Things Devices},
  year		= {2025},
  isbn		= {9798400712852},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703790.3703811},
  doi		= {10.1145/3703790.3703811},
  abstract	= {This paper presents a demonstration of a context-aware
		  conversational interface for interacting with multiple IoT
		  devices using multi-turn natural language commands. Our
		  system facilitates device discovery and identification
		  through a Knowledge Graph (KG) and addresses the
		  interoperability issue by using the Web of Things (WoT)
		  specifications as an intermediate abstraction layer. The
		  proposed system comprises four major components: a
		  context-aware multi-turn dialogue interface, a device
		  discovery and identification module using a domain-specific
		  KG, a text-to-code module to parse natural language
		  commands into an executable code format, and a customized
		  code execution engine. We demonstrate our system using two
		  Philips Hue smart lamps and one Elgato Stream Deck
		  controller. The Philips Hue smart lamp device can be
		  controlled standalone via natural language, allowing users
		  to adjust the power state, the color, and the brightness.
		  Also, the lamp can be integrated with the Elgato Stream
		  Deck Controller to enable user-defined automation rules
		  following the Trigger Action programming (TAP) paradigm.},
  booktitle	= {Proceedings of the 14th International Conference on the
		  Internet of Things},
  pages		= {160–163},
  numpages	= {4},
  keywords	= {Conversational Interface, Internet of Things, Trigger
		  Action Programming, Natural Language Understanding,
		  Knowledge Graph},
  location	= { },
  series	= {IoT '24}
}

@Article{	  10.1109/tcbb.2020.3044230,
  author	= {Hakala, Kai and Kaewphan, Suwisa and Bj\"{o}rne, Jari and
		  Mehryary, Farrokh and Moen, Hans and Tolvanen, Martti and
		  Salakoski, Tapio and Ginter, Filip},
  title		= {Neural Network and Random Forest Models in Protein
		  Function Prediction},
  year		= {2020},
  issue_date	= {May-June 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {3},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2020.3044230},
  doi		= {10.1109/TCBB.2020.3044230},
  abstract	= {Over the past decade, the demand for automated protein
		  function prediction has increased due to the volume of
		  newly sequenced proteins. In this paper, we address the
		  function prediction task by developing an ensemble system
		  automatically assigning Gene Ontology (GO) terms to the
		  given input protein sequence. We develop an ensemble system
		  which combines the GO predictions made by random forest
		  (RF) and neural network (NN) classifiers. Both RF and NN
		  models rely on features derived from BLAST sequence
		  alignments, taxonomy and protein signature analysis tools.
		  In addition, we report on experiments with a NN model that
		  directly analyzes the amino acid sequence as its sole
		  input, using a convolutional layer. The Swiss-Prot database
		  is used as the training and evaluation data. In the CAFA3
		  evaluation, which relies on experimental verification of
		  the functional predictions, our submitted ensemble model
		  demonstrates competitive performance ranking among top-10
		  best-performing systems out of over 100 submitted systems.
		  In this paper, we evaluate and further improve the
		  CAFA3-submitted system. Our machine learning models
		  together with the data pre-processing and feature
		  generation tools are publicly available as an open source
		  software at
		  &lt;uri&gt;https://github.com/TurkuNLP/CAFA3&lt;/uri&gt;.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= dec,
  pages		= {1772–1781},
  numpages	= {10}
}

@Article{	  10.1145/3643132,
  author	= {Jain, Minni and Jindal, Rajni and Jain, Amita},
  title		= {Automatic Construction of Interval-Valued Fuzzy Hindi
		  WordNet using Lexico-Syntactic Patterns and Word
		  Embeddings},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3643132},
  doi		= {10.1145/3643132},
  abstract	= {A computational lexicon is the backbone of any language
		  processing system. It helps computers to understand the
		  language complexity as a human does by inculcating words
		  and their semantic associations. Manually constructed
		  famous Hindi WordNet (HWN) consists of various classical
		  semantic relations (crisp relations). To handle uncertainty
		  and represent Hindi WordNet more semantically, Type- 1
		  fuzzy graphs are applied to relations of Hindi WordNet. But
		  uncertainty in the crisp membership degree is not
		  considered in Type 1 fuzzy set (T1FS). Also collecting
		  billions (5,55,69,51,753 relations in HWN) of membership
		  values from experts (humans) is not feasible. This paper
		  applied the concept of Interval-Valued Fuzzy graphs and
		  proposed Interval- Valued Fuzzy Hindi WordNet (IVFHWN).
		  IVFHWN automatically identifies Interval- Valued Fuzzy
		  relations between words and their degree of membership
		  using word embeddings and lexico-syntactic patterns. The
		  experimental results for the word sense disambiguation
		  problem show better outcomes when IVFHWN is being used in
		  place of Type 1 Fuzzy Hindi WordNet and classical Hindi
		  WordNet.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= feb,
  keywords	= {Hindi WordNet, Interval- Valued Fuzzy Graphs,
		  Computational Lexicon, Natural Language Processing,
		  Low-Resource Language}
}

@Article{	  10.1145/3229094,
  author	= {Magnaudet, Mathieu and Chatty, St\'{e}phane and Conversy,
		  St\'{e}phane and Leriche, S\'{e}bastien and Picard, Celia
		  and Prun, Daniel},
  title		= {Djnn/Smala: A Conceptual Framework and a Language for
		  Interaction-Oriented Programming},
  year		= {2018},
  issue_date	= {June 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {EICS},
  url		= {https://doi.org/10.1145/3229094},
  doi		= {10.1145/3229094},
  abstract	= {The persistent difficulty to develop and maintain
		  interactive software has unveiled the inadequacy of
		  traditional imperative programming languages. In the recent
		  years, several solutions have been proposed to enrich the
		  existing languages with constructs dedicated to
		  interaction. In this paper, we propose a different approach
		  that takes interaction as the primary concern to build a
		  new programming language. We present Djnn, a conceptual
		  framework based on the concepts of process and process
		  activation, then we introduce Smala a programming language
		  derived from this framework. We propose a solution for the
		  unification of the concepts of event and data-flow, and for
		  the derivation of complex control structures from a small
		  set of basic ones. We detail the syntax and the semantics
		  of Smala. Finally, we illustrate through a real-size
		  application how it enables building all parts of an
		  interactive software. Djnn and Smala may offer designers
		  and programmers usable means to think of interactions and
		  translate them into running code.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jun,
  articleno	= {12},
  numpages	= {27},
  keywords	= {smala, reactive programming, interactive software, gui
		  programming, djnn}
}

@InProceedings{	  10.1145/3701716.3715454,
  author	= {Yang, Yitian and Chen, Huaming},
  title		= {Construction of Domain-Specific Knowledge Graph for
		  Advanced Persistent Threat Behaviour Analysis and
		  Detection},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715454},
  doi		= {10.1145/3701716.3715454},
  abstract	= {Advanced Persistent Threat (APT) represents a
		  sophisticated and targeted attack campaign, often
		  orchestrated by well-resourced organizations. Understanding
		  APT is crucial for adapting to their evolving tactics and
		  effectively mitigating their infiltration methods. A key
		  approach to accurately analysing and detecting APTs
		  involves studying the behaviour, identifying their attack
		  stage and uncovering the employed components. Existing
		  works for APT detection heavily rely on network traffic
		  analysis, limiting their practical applicability in
		  real-world scenarios. This paper introduces a novel method
		  to analyse and detect APT behaviours by constructing a
		  domain-specific knowledge graph (APT-KG), utilising the
		  MITRE ATT&amp;CK framework as its foundation. A
		  hierarchical clustering-based model is proposed to uncover
		  the correlations among various network attack techniques.
		  It first vectorizes the attack techniques according to
		  ATT&amp;CK standards, filtering out low-frequency
		  techniques to optimise dimensionality reduction.
		  Parent-child relationships within the ATT&amp;CK
		  classification further facilitate this process. We observe
		  that this strategy can reveal fully connected relationships
		  among high-frequency techniques, while connections are
		  preserved within clusters between high and low-frequency
		  techniques. This structured representation enables the
		  inference of potential attack patterns, achieving a
		  prediction accuracy of 88.11\%. We then integrate the
		  identified associations with APT-KG. Experimental
		  validation demonstrates that APT-KG significantly enhances
		  understanding of attack interrelations and improves the
		  efficiency of APT detection and response mechanisms.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {1480–1484},
  numpages	= {5},
  keywords	= {advanced persistent threat (apt), data mining, knowledge
		  graph, network security, ontology construction},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3545862.3545890,
  author	= {Camelo, Diogo and Ascens\~{a}o, Jo\~{a}o and Alves, Rui
		  and Matos, Paulo},
  title		= {Mech Desk: An ontology based system to help drivers
		  diagnosis vehicle problems},
  year		= {2022},
  isbn		= {9781450396400},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3545862.3545890},
  doi		= {10.1145/3545862.3545890},
  abstract	= {Semantic Web is a vision about an extension of the
		  existing World Wide Web, which provides tools and
		  technologies to support the transparent exchange of
		  information and knowledge among organizations. As one of
		  the building blocks of Semantic Technology, ontologies are
		  part of the W3C standards stack for the Semantic Web.
		  Nowadays, multiple areas can be aborded by ontologies and
		  the semantic web world, as the subject of this project,
		  mechanics. Mechanics have been accentuated in a visible
		  way, where the reality of living without means of
		  transportation is not feasible in people's lives. The
		  development of new methods to increase the knowledge of
		  drivers and everyday people about automated vehicles is
		  essential. Regarding cars, revisions, maintenance,
		  inspections, change of parts, among others, are necessary
		  and "mandatory" subjects and due to this, it is possible to
		  prevent future damage by prolonging the life of the car. In
		  certain cases, this doesn't happen, either due to wear of
		  parts or unforeseen events, and despite being a busy
		  market, drivers are not always informed about the best
		  cares to take or the problems that may arise. As such, the
		  theme of this project is to make a relationship between
		  mechanical details, issues, and solutions, throughout an
		  ontology, to help an everyday driver to a better perception
		  of what he encounters at hand. For that purpose, the
		  defined ontology was exposed via a mobile application, with
		  it providing to the user, several details that he can or
		  not relate, and trough them, provide a connection with a
		  certain problem and solution. The semantic web ontology was
		  developed in Prot\'{e}g\'{e}, exposed into Apache Jena
		  Fuseki server, and was running in an Azure Virtual Machine,
		  allowing it to be available into the OutSystems
		  application.},
  booktitle	= {Proceedings of the 8th International Conference on
		  Frontiers of Educational Technologies},
  pages		= {169–175},
  numpages	= {7},
  keywords	= {‘Mobile Development', ‘Ontology’, 'Semantic Web’,
		  'Vehicles Maintenance'},
  location	= {Yokohama, Japan},
  series	= {ICFET '22}
}

@InProceedings{	  10.1145/3239372.3239397,
  author	= {Ballar\'{\i}n, Manuel and Marc\'{e}n, Ana C. and
		  Pelechano, Vicente and Cetina, Carlos},
  title		= {Measures to report the Location Problem of Model Fragment
		  Location},
  year		= {2018},
  isbn		= {9781450349499},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3239372.3239397},
  doi		= {10.1145/3239372.3239397},
  abstract	= {Model Fragment Location (MFL) aims at identifying model
		  elements that are relevant to a requirement, feature, or
		  bug. Many MFL approaches have been introduced in the last
		  few years to address the identification of the model
		  elements that correspond to a specific functionality.
		  However, there is a lack of detail when the measurements
		  about the search space (models) and the measurements about
		  the solution to be found (model fragment) are reported.
		  Generally, the only reported measure is the model size. In
		  this paper, we propose using five measurements (size,
		  volume, density, multiplicity, and dispersion) to report
		  the location problems. These measurements are the result of
		  analyzing 1,308 MFLs in a family of industrial models over
		  the last four years. Using two MFL approaches, we emphasize
		  the importance of these measurements in order to compare
		  results. Our work not only proposes improving the reporting
		  of the location problem, but it also provides real
		  measurements of location problems that are useful to other
		  researchers in the design of synthetic location problems.},
  booktitle	= {Proceedings of the 21th ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {189–199},
  numpages	= {11},
  keywords	= {Bug Location, Feature Location, Model Fragment Location,
		  Traceability Link Recovery},
  location	= {Copenhagen, Denmark},
  series	= {MODELS '18}
}

@InProceedings{	  10.1145/3270112.3270121,
  author	= {Ciccozzi, Federico and Famelis, Michalis and Kappel, Gerti
		  and Lambers, Leen and Mosser, Sebastien and Paige, Richard
		  F. and Pierantonio, Alfonso and Rensink, Arend and Salay,
		  Rick and Taentzer, Gabi and Vallecillo, Antonio and Wimmer,
		  Manuel},
  title		= {Towards a body of knowledge for model-based software
		  engineering},
  year		= {2018},
  isbn		= {9781450359658},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3270112.3270121},
  doi		= {10.1145/3270112.3270121},
  abstract	= {Model-based Software Engineering (MBSE) is now accepted as
		  a Software Engineering (SE) discipline and is being taught
		  as part of more general SE curricula. However, an agreed
		  core of concepts, mechanisms and practices --- which
		  constitutes the Body of Knowledge of a discipline --- has
		  not been captured anywhere, and is only partially covered
		  by the SE Body of Knowledge (SWEBOK). With the goals of
		  characterizing the contents of the MBSE discipline,
		  promoting a consistent view of it worldwide, clarifying its
		  scope with regard to other SE disciplines, and defining a
		  foundation for a curriculum development on MBSE, this paper
		  provides a proposal for an extension of the contents of
		  SWEBOK with the set of fundamental concepts, terms and
		  mechanisms that should constitute the MBSE Body of
		  Knowledge.},
  booktitle	= {Proceedings of the 21st ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  pages		= {82–89},
  numpages	= {8},
  keywords	= {body of knowledge, model-based software engineering},
  location	= {Copenhagen, Denmark},
  series	= {MODELS '18}
}

@Article{	  10.1145/3626568,
  author	= {Haffar, Nafaa and Zrigui, Mounir},
  title		= {A Synergistic Bidirectional LSTM and N-gram Multi-channel
		  CNN Approach Based on BERT and FastText for Arabic Event
		  Identification},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {11},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3626568},
  doi		= {10.1145/3626568},
  abstract	= {Event extraction from texts continues to pose a challenge
		  for many NLP systems. This article presents a novel neural
		  network architecture that can extract and classify events
		  from Arabic sentences. The model combines word
		  representations and Part-Of-Speech (POS) tags and uses a
		  bidirectional LSTM layer and a dual combined convolutional
		  neural network. The first layer of the network focuses on
		  sentence representations, while the second layer focuses on
		  POS representations. The model takes advantage of both
		  N-gram character features from FastText and contextual
		  representations from bidirectional encoder representations
		  from transformers. This combination proves to be
		  successful, as evidenced by the good results obtained from
		  evaluating the model on the Arabic TimeML corpus. Our
		  results show that combining both contextual and N-gram
		  representations outperforms the traditional skip-gram
		  model.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {246},
  numpages	= {27},
  keywords	= {BERT representation, FastText representation, events
		  identification, Arabic language, deep learning, TimeML
		  standard}
}

@InProceedings{	  10.1145/3382025.3414973,
  author	= {Schlie, Alexander and Kn\"{u}ppel, Alexander and Seidl,
		  Christoph and Schaefer, Ina},
  title		= {Incremental feature model synthesis for clone-and-own
		  software systems in MATLAB/Simulink},
  year		= {2020},
  isbn		= {9781450375696},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3382025.3414973},
  doi		= {10.1145/3382025.3414973},
  abstract	= {Families of related MATLAB/Simulink systems commonly
		  emerge ad hoc using clone-and-own practices. Extractively
		  migrating systems towards a software product line (SPL) can
		  be a remedy. A feature model (FM) represents all potential
		  configurations of an SPL, ideally, in non-technical domain
		  terms. However, yielding a sensible FM from automated
		  synthesis remains a major challenge due to domain knowledge
		  being a prerequisite for features to be adequate
		  abstractions. In incremental reverse engineering,
		  subsequent generation of FMs may further overwrite changes
		  and design decisions made during previous manual FM
		  refinement.In this paper, we propose an approach to largely
		  automate the synthesis of a suitable FM from a set of
		  cloned MATLAB/Simulink models as part of reverse
		  engineering an SPL. We fully automate the extraction of an
		  initial, i.e., a technical, FM that closely aligns with
		  realization artifacts and their variability, and further
		  provide operations to manually refine it to incorporate
		  domain knowledge. Most importantly, we provide concepts to
		  capture such operations and to replay them on a
		  structurally different technical FM stemming from a
		  subsequent reverse engineering increment that included
		  further systems of the portfolio. We further provide an
		  implementation and demonstrate the feasibility of our
		  approach using two MATLAB/Simulink data sets from the
		  automotive domain.},
  booktitle	= {Proceedings of the 24th ACM Conference on Systems and
		  Software Product Line: Volume A - Volume A},
  articleno	= {7},
  numpages	= {12},
  keywords	= {variability, synthesis, refinement, mapping, individual,
		  incremental, feature model, clone-and-own, MATLAB/Simulink,
		  150\% model},
  location	= {Montreal, Quebec, Canada},
  series	= {SPLC '20}
}

@InProceedings{	  10.1145/3382026.3431246,
  author	= {Kenner, Andy},
  title		= {Model-Based Evaluation of Vulnerabilities in Software
		  Systems},
  year		= {2020},
  isbn		= {9781450375702},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3382026.3431246},
  doi		= {10.1145/3382026.3431246},
  abstract	= {Vulnerabilities in software systems result from faults,
		  which occur at different stages in a software's life cycle,
		  for example, in the design (i.e., undesired
		  feature-interactions), the development (i.e., buffer
		  overflows), or the operation (i.e., configuration errors).
		  Various databases provide detailed information about
		  vulnerabilities in software systems or the way to exploit
		  it, but face severe limitations. The information is
		  scattered across these databases, fluctuates in quality and
		  granularity, and provides only an insight into a single
		  vulnerability per entry. Even for a single software system
		  it is challenging for any security-related stakeholder to
		  determine the threat level, which consists of all
		  vulnerabilities of the software system and its environment
		  (i.e., operating system). Manual vulnerability management
		  is feasible only to a limited extend if we want to identify
		  all configurations that are affected by vulnerabilities, or
		  determine a system's threat level and the resulting risk we
		  have to deal with. For variant-rich systems, we also have
		  to deal with variability, allowing different stakeholders
		  to understand the threats to their particular setup. To
		  deal with this variability, we propose vulnerability
		  feature models, which offer a homogeneous view on all
		  vulnerabilities of a software system. These models and the
		  resulting analyses offer advantages in many disciplines of
		  the vulnerability management process. In this paper, we
		  report the research plan for our project, in which we focus
		  on the model-based evaluation of vulnerabilities. This
		  includes research objectives that take into account the
		  design of vulnerability feature models, their application
		  in the process of vulnerability management, and the impact
		  of evolution, discovery, and verification of
		  vulnerabilities.},
  booktitle	= {Proceedings of the 24th ACM International Systems and
		  Software Product Line Conference - Volume B},
  pages		= {112–119},
  numpages	= {8},
  keywords	= {Exploit, Feature Model, Variability Model, Vulnerability,
		  Vulnerability Analysis and Management},
  location	= {Montreal, QC, Canada},
  series	= {SPLC '20}
}

@InProceedings{	  10.1145/3733567.3735567,
  author	= {H\"{u}s\"{u}nbeyi, Z. Melce and Seddah, Djam\'{e} and
		  Scheffler, Tatjana},
  title		= {Integrating Semantic Representations in a Cross-Modal
		  Approach to Fact-Checking},
  year		= {2025},
  isbn		= {9798400718915},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3733567.3735567},
  doi		= {10.1145/3733567.3735567},
  abstract	= {We propose a cross-modal approach with deep fusion of a
		  language model and graph structures based on Abstract
		  Meaning Representations (AMRs) enriched with Wikidata to
		  address the fact-checking problem. We collect and make
		  available a large dataset of fact-checked claim sentences,
		  and systematically compare a transformer-based model with
		  Graph Neural Networks (GNNs) based on AMR graphs and
		  extended with external information. Furthermore, we
		  evaluate the integration of language models and GNNs for
		  the fact verification task. While GNN models on AMR-based
		  graphs alone yield lower scores than transformer based
		  language models on their own, the combined cross-modal
		  approach—leveraging a multilayer and deep interaction
		  between textual and structural information—demonstrates
		  the best performance. Finally, we evaluate the
		  generalization capability of this cross-modal approach
		  integrating AMR-based graph structures on out-of-domain
		  English and German claims.},
  booktitle	= {Proceedings of the 4th ACM International Workshop on
		  Multimedia AI against Disinformation},
  pages		= {17–27},
  numpages	= {11},
  keywords	= {fact-checking, AMR graphs, external knowledge,
		  cross-modal, deep and interactive fusion of LMs and GNNs},
  location	= { },
  series	= {MAD' 25}
}

@InProceedings{	  10.1145/3411564.3411629,
  author	= {Villa\c{c}a, Lu\'{\i}s Henrique Neves and Azevedo,
		  Leonardo Guerreiro and Siqueira, Sean Wolfgand Matsui},
  title		= {Microservice Architecture for Multistore Database Using
		  Canonical Data Model},
  year		= {2020},
  isbn		= {9781450388733},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411564.3411629},
  doi		= {10.1145/3411564.3411629},
  abstract	= {In a microservice architecture, solutions are created by
		  teams focused on specific domains and needs. They
		  independently develop and deploy distributed services in
		  the network. One characteristic of microservices is
		  decentralized data management. Each microservice may use
		  different data management technology which best fit its
		  needs. Hence, it is an issue to integrate data of
		  heterogenous microservices to come up with consolidate data
		  views. Flexible and efficient solutions in this scenario
		  are needed. This work is based on the use of a canonical
		  data model as the mechanism for data integration in
		  microservices. The canonical data model is the reference
		  for query specifications and data integration. This work
		  proposes and implements a microservice architecture based
		  on this strategy and it is composed by nodes that
		  intercommunicate through several mechanisms (e.g., SPARQL,
		  GraphQL and JDBC queries, calls to REST services and
		  proprietary APIs). The solution was analyzed in a proof of
		  concept in a fictitious scenario but using real services
		  available at DBPedia and Twitter. The evaluation goal was
		  to qualitatively analyze the use of the architecture in the
		  design, development and execution of microservices in order
		  to identify the characteristics one should consider when
		  using the canonical data model strategy. The evaluation
		  employed the criteria of ISO/IEC 25010 model that most
		  relate to the SOA challenges, which were: usability;
		  performance; compatibility; and, maintainability. The
		  identified advantages and disadvantages of using the
		  architecture (i.e., the strategy) can be used by architects
		  and developers to make their development decisions.},
  booktitle	= {Proceedings of the XVI Brazilian Symposium on Information
		  Systems},
  articleno	= {20},
  numpages	= {8},
  keywords	= {canonical data model, microservice architecture,
		  multistore, polyglot persistence, service-oriented
		  architecture},
  location	= {S\~{a}o Bernardo do Campo, Brazil},
  series	= {SBSI '20}
}

@InProceedings{	  10.1145/3308558.3313439,
  author	= {Wang, Chengyu and Fan, Yan and He, Xiaofeng and Zhou,
		  Aoying},
  title		= {A Family of Fuzzy Orthogonal Projection Models for
		  Monolingual and Cross-lingual Hypernymy Prediction},
  year		= {2019},
  isbn		= {9781450366748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308558.3313439},
  doi		= {10.1145/3308558.3313439},
  abstract	= {Hypernymy is a semantic relation, expressing the
		  “is-a” relation between a concept and its instances.
		  Such relations are building blocks for large-scale
		  taxonomies, ontologies and knowledge graphs. Recently, much
		  progress has been made for hypernymy prediction in English
		  using textual patterns and/or distributional
		  representations. However, applying such techniques to other
		  languages is challenging due to the high language
		  dependency of these methods and the lack of large training
		  datasets of lower-resourced languages. In this work, we
		  present a family of fuzzy orthogonal projection models for
		  both monolingual and cross-lingual hypernymy prediction.
		  For the monolingual task, we propose a Multi-Wahba
		  Projection (MWP) model to distinguish hypernymy vs.
		  non-hypernymy relations based on word embeddings. This
		  model establishes distributional fuzzy mappings from
		  embeddings of a term to those of its hypernyms and
		  non-hypernyms, which consider the complicated linguistic
		  regularities of these relations. For cross-lingual
		  hypernymy prediction, a Transfer MWP (TMWP) model is
		  proposed to transfer the semantic knowledge from the source
		  language to target languages based on neural word
		  translation. Additionally, an Iterative Transfer MWP
		  (ITMWP) model is built upon TMWP, which augments the
		  training sets of target languages when target languages are
		  lower-resourced with limited training data. Experiments
		  show i) MWP outperforms previous methods over two hypernymy
		  prediction tasks for English; and ii) TMWP and ITMWP are
		  effective to predict hypernymy over seven non-English
		  languages.},
  booktitle	= {The World Wide Web Conference},
  pages		= {1965–1976},
  numpages	= {12},
  keywords	= {Multi-Wahba Projection, cross-lingual transfer learning,
		  hypernymy prediction},
  location	= {San Francisco, CA, USA},
  series	= {WWW '19}
}

@Article{	  10.1145/3419972,
  author	= {Liu, Bulou and Li, Chenliang and Zhou, Wei and Ji, Feng
		  and Duan, Yu and Chen, Haiqing},
  title		= {An Attention-based Deep Relevance Model for Few-shot
		  Document Filtering},
  year		= {2020},
  issue_date	= {January 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {39},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3419972},
  doi		= {10.1145/3419972},
  abstract	= {With the large quantity of textual information produced on
		  the Internet, a critical necessity is to filter out the
		  irrelevant information and organize the rest into
		  categories of interest (e.g., an emerging event). However,
		  supervised-learning document filtering methods heavily rely
		  on a large number of labeled documents for model training.
		  Manually identifying plenty of positive examples for each
		  category is expensive and time-consuming. Also, it is
		  unrealistic to cover all the categories from an evolving
		  text source that covers diverse kinds of events, user
		  opinions, and daily life activities. In this article, we
		  propose a novel attention-based deep relevance model for
		  few-shot document filtering (named ADRM), inspired by the
		  relevance feedback methodology proposed for ad hoc
		  retrieval. ADRM calculates the relevance score between a
		  document and a category by taking a set of seed words and a
		  few seed documents relevant to the category. It constructs
		  the category-specific conceptual representation of the
		  document based on the corresponding seed words and seed
		  documents. Specifically, to filter irrelevant yet noisy
		  information in the seed documents, ADRM employs two types
		  of attention mechanisms (namely whole-match attention and
		  max-match attention) and generates category-specific
		  representations for them. Then ADRM is devised to extract
		  the relevance signals by modeling the hidden feature
		  interactions in the word embedding space. The relevance
		  signals are extracted through a gated convolutional
		  process, a self-attention layer, and a relevance
		  aggregation layer. Extensive experiments on three
		  real-world datasets show that ADRM consistently outperforms
		  the existing technical alternatives, including the
		  conventional classification and retrieval baselines, and
		  the state-of-the-art deep relevance ranking models for
		  few-shot document filtering. We also perform an ablation
		  study to demonstrate that each component in ADRM is
		  effective for enhancing filtering performance. Further
		  analysis shows that ADRM is robust under varying parameter
		  settings.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= oct,
  articleno	= {6},
  numpages	= {35},
  keywords	= {Few-shot learning, deep learning, document filtering}
}

@Article{	  10.1145/3447879.3447882,
  author	= {Vedula, Nikhita},
  title		= {Modeling knowledge and functional intent for context-aware
		  pragmatic analysis},
  year		= {2021},
  issue_date	= {Winter 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2021},
  number	= {Winter},
  issn		= {1931-1745},
  url		= {https://doi.org/10.1145/3447879.3447882},
  doi		= {10.1145/3447879.3447882},
  abstract	= {Nikhita Vedula is an Applied Scientist at Amazon Alexa
		  Science. She obtained her PhD in Computer Science and
		  Engineering from the Ohio State University in August 2020,
		  advised by Professor Srinivasan Parthasarathy. She received
		  her bachelor's degree from the National Institute of
		  Technology, Nagpur, India in 2015. Her research interests
		  are at the intersection of data mining, natural language
		  processing and social computing. Over the course of her
		  PhD, her research involved designing efficient and novel
		  machine learning and computational linguistic techniques
		  that extract, interpret and transform the vast,
		  unstructured digital content into structured knowledge
		  representations in diverse contexts. She has worked with
		  researchers from interdisciplinary fields such as emergency
		  response, marketing, sociology and psychology. She
		  performed research internships at Nokia Bell Laboratories,
		  Adobe Research and Amazon Alexa AI. Her work has been
		  published at several top data mining conferences such as
		  the Web Conference, SIGIR, WSDM and ICDM. Her work on
		  detecting user intentions from their natural language
		  interactions won the Best paper award at the Web Conference
		  2020. She was a recipient of a Graduate Research Award
		  (2020), a Presidential Fellowship (2019) and a University
		  Graduate Fellowship (2015) at the Ohio State University.
		  She was also selected as a Rising Star in EECS (2019).},
  journal	= {SIGWEB Newsl.},
  month		= feb,
  articleno	= {3},
  numpages	= {4}
}

@InProceedings{	  10.1109/models-c.2019.00017,
  author	= {Partridge, Chris and Mitchell, Andrew and Loneragan,
		  Michael and Atkinson, Hayden and de Cesare, Sergio and
		  Khan, Mesbah},
  title		= {Coordinate systems: level ascending ontological options},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00017},
  doi		= {10.1109/MODELS-C.2019.00017},
  abstract	= {A major challenge faced in the deployment of collaborating
		  unmanned vehicles is enabling the semantic interoperability
		  of sensor data. One aspect of this, where there is
		  significant opportunity for improvement, is characterizing
		  the coordinate systems for sensed position data. We are
		  involved in a proof of concept project that addresses this
		  challenge through a foundational conceptual model using a
		  constructional approach based upon the BORO Foundational
		  Ontology. The model reveals the characteristics as sets of
		  options for configuring the coordinate systems. This paper
		  examines how these options involve, ontologically,
		  ascending levels. It identifies two types of levels, the
		  well-known type levels and the less well-known
		  tuple/relation levels.},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {78–87},
  numpages	= {10},
  keywords	= {BORO foundational ontology, constructional ontology,
		  geometric coordinate system ontology, multi-level options,
		  multi-platform-domain sensor system, power-tuple-builder,
		  power-type-builder},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.1145/3482632.3482749,
  author	= {Cui, Gaili},
  title		= {Design of Intelligent Recognition English Translation
		  Model Based on Feature Extraction Algorithm},
  year		= {2021},
  isbn		= {9781450390255},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3482632.3482749},
  doi		= {10.1145/3482632.3482749},
  abstract	= {In recent years, with the deepening of globalization,
		  international cooperation is becoming more and more
		  extensive, and the importance of English is increasing.
		  Aiming at the problems that the semantic context of English
		  is not obvious in the process of English translation in
		  traditional English translation system, the optimal
		  translation solution is not reached in the process of
		  selecting the optimal feature semantics, and the
		  translation accuracy is low, an intelligent recognition
		  English translation model based on feature extraction
		  algorithm is designed. Search module is used to complete
		  the search of basic meaning and subject content of
		  vocabulary to be proofread, grasp the user's behavior data
		  through behavior log and optimize the system; In the method
		  based on the maximum entropy principle, the whole task of
		  clause recognition is divided into three parts: sentence
		  head recognition, sentence tail recognition and complete
		  clause recognition. Experimental results show that the
		  proposed algorithm has higher recognition rate.},
  booktitle	= {2021 4th International Conference on Information Systems
		  and Computer Aided Education},
  pages		= {553–557},
  numpages	= {5},
  location	= {Dalian, China},
  series	= {ICISCAE 2021}
}

@InProceedings{	  10.1145/3459637.3482440,
  author	= {Sheng, Qiang and Zhang, Xueyao and Cao, Juan and Zhong,
		  Lei},
  title		= {Integrating Pattern- and Fact-based Fake News Detection
		  via Model Preference Learning},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482440},
  doi		= {10.1145/3459637.3482440},
  abstract	= {To defend against fake news, researchers have developed
		  various methods based on texts. These methods can be
		  grouped as 1) pattern-based methods, which focus on shared
		  patterns among fake news posts rather than the claim
		  itself; and 2) fact-based methods, which retrieve from
		  external sources to verify the claim's veracity without
		  considering patterns. The two groups of methods, which have
		  different preferences of textual clues, actually play
		  complementary roles in detecting fake news. However, few
		  works consider their integration. In this paper, we study
		  the problem of integrating pattern- and fact-based models
		  into one framework via modeling their preference
		  differences, i.e., making the pattern- and fact-based
		  models focus on respective preferred parts in a post and
		  mitigate interference from non-preferred parts as possible.
		  To this end, we build a Preference-aware Fake News
		  Detection Framework (Pref-FEND), which learns the
		  respective preferences of pattern- and fact-based models
		  for joint detection. We first design a heterogeneous
		  dynamic graph convolutional network to generate the
		  respective preference maps, and then use these maps to
		  guide the joint learning of pattern- and fact-based models
		  for final prediction. Experiments on two real-world
		  datasets show that Pref-FEND effectively captures model
		  preferences and improves the performance of models based on
		  patterns, facts, or both.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {1640–1650},
  numpages	= {11},
  keywords	= {fact-checking, fake news detection, graph neural networks,
		  pattern mining, preference learning},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.1145/3744740,
  author	= {Trinh, Tam and Dao, Anh and Hy, Thi Hong Nhung and Hy,
		  Truong Son},
  title		= {VietMedKG: Knowledge Graph and Benchmark for Traditional
		  Vietnamese Medicine},
  year		= {2025},
  issue_date	= {July 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {7},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3744740},
  doi		= {10.1145/3744740},
  abstract	= {Traditional Vietnamese Medicine (TVM) and Traditional
		  Chinese Medicine (TCM) have shared significant similarities
		  due to their geographical location, cultural exchanges, and
		  hot and humid climatic conditions. However, unlike TCM,
		  which has substantial works published to construct a
		  knowledge graph, there is a notable absence of a
		  comprehensive knowledge graph for TVM. This article
		  presents the first endeavor to build a knowledge graph for
		  TVM based on extensive existing resources from TCM. We name
		  our knowledge graph as VietMedKG. We propose a translation
		  and filtration process to adapt TCM knowledge graphs to
		  TVM, identifying the overlapping and unique elements of
		  TVM. In addition, the constructed knowledge graph is then
		  exploited further for developing a curated benchmark for
		  the knowledge graph-based question-answering problem with
		  the potential to support doctors and patients in assisting
		  doctors and patients in identifying various diseases. Our
		  work will not only bridge the gap between TCM and TVM but
		  also set the foundation for future research into TVM
		  community. Our source code is publicly available at .},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  articleno	= {69},
  numpages	= {17},
  keywords	= {Knowledge graph, traditional vietnamese mecidine,
		  graph-based question answering, retrieval augmented
		  generation}
}

@InProceedings{	  10.1145/3698587.3701527,
  author	= {Agapito, Giuseppe and Cannataro, Mario and Lloyd, Wes J.
		  and Zucco, Chiara},
  title		= {13th Workshop on Parallel and AI-based Bioinformatics and
		  Biomedicine (ParBio): Editorial},
  year		= {2024},
  isbn		= {9798400713026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698587.3701527},
  doi		= {10.1145/3698587.3701527},
  abstract	= {The goal of ParBio is to bring together scientists in
		  high-performance computing, computational biology, and
		  medicine to discuss parallel implementation of
		  bioinformatics and biomedical applications and the
		  challenges and opportunities of moving these applications
		  to the cloud or edge. The workshop will also address
		  Artificial Intelligence (AI), Large Language Models (LLMs),
		  machine learning, and big data analytics in healthcare and
		  bioinformatics, focusing on the integrated analysis of
		  molecular and clinical data. This is motivated by the
		  increasing production of experimental and clinical data and
		  the shift towards data storage, integration, and
		  analysis.},
  booktitle	= {Proceedings of the 15th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  articleno	= {95},
  numpages	= {1},
  keywords	= {Bioinformatics, Machine Learning, Parallel algorithms},
  location	= {Shenzhen, China},
  series	= {BCB '24}
}

@InProceedings{	  10.1145/3470481.3472704,
  author	= {Aryan, Peb Ruswono and Ekaputra, Fajar Juang and Sabou,
		  Marta and Hauer, Daniel and Mosshammer, Ralf and Einfalt,
		  Alfred and Miksa, Tomasz and Rauber, Andreas},
  title		= {Explainable cyber-physical energy systems based on
		  knowledge graph},
  year		= {2021},
  isbn		= {9781450386081},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3470481.3472704},
  doi		= {10.1145/3470481.3472704},
  abstract	= {Explainability can help cyber-physical systems alleviating
		  risk in automating decisions that are affecting our life.
		  Building an explainable cyber-physical system requires
		  deriving explanations from system events and causality
		  between the system elements. Cyber-physical energy systems
		  such as smart grids involve cyber and physical aspects of
		  energy systems and other elements, namely social and
		  economic. Moreover, a smart-grid scale can range from a
		  small village to a large region across countries.
		  Therefore, integrating these varieties of data and
		  knowledge is a fundamental challenge to build an
		  explainable cyber-physical energy system. This paper aims
		  to use knowledge graph based framework to solve this
		  challenge. The framework consists of an ontology to model
		  and link data from various sources and graph-based
		  algorithm to derive explanations from the events. A
		  simulated demand response scenario covering the above
		  aspects further demonstrates the applicability of this
		  framework.},
  booktitle	= {Proceedings of the 9th Workshop on Modeling and Simulation
		  of Cyber-Physical Energy Systems},
  articleno	= {4},
  numpages	= {6},
  keywords	= {explainability, knowledge graphs, ontologies, smart grid
		  simulation, smart grids},
  location	= {Virtual Event},
  series	= {MSCPES '21}
}

@InProceedings{	  10.1145/3500931.3500960,
  author	= {Shengxin, Hu and Qing, Wang and Lu, Chen and Xingxin,
		  Zhang and Leiqing, Huang and Tianyu, He and Songhe, Li and
		  Xiangmin, Dong and Bingxiang, Yang},
  title		= {The establishment and evaluation of the automatic crisis
		  balance analysis model for social network users based on
		  artificial intelligence technology},
  year		= {2021},
  isbn		= {9781450395588},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3500931.3500960},
  doi		= {10.1145/3500931.3500960},
  abstract	= {Online social media provides people with a platform to
		  express their emotions anonymously. Social media has been
		  identified as an important data source for suicide
		  prevention related to emotional problems in China. Almost
		  Three million messages were published by 450,000 users in a
		  particular Chinese social media data base. This study aims
		  to develop a Crisis Balance Analysis Model based on
		  concepts of "balancing factors" as described by Aguilera.
		  Through interactions with psychological experts, deep
		  learning architecture that was built and refined. Three
		  annotation levels free annotations (zero cost), easy
		  annotations (by psychology students), and hard annotations
		  (by psychology experts) were used. Our Model was evaluated
		  accordingly and showed that its performance at each level
		  was promising. Finally, suicide risks, cognitive
		  distortions and interpersonal problems could be identified
		  for messages from social media users using this model,
		  which providing basis for proactive crisis intervention.},
  booktitle	= {Proceedings of the 2nd International Symposium on
		  Artificial Intelligence for Medicine Sciences},
  pages		= {157–161},
  numpages	= {5},
  keywords	= {Artificial intelligent, Crisis, Depression, Suicide
		  prevention},
  location	= {Beijing, China},
  series	= {ISAIMS '21}
}

@InProceedings{	  10.1145/3323878.3325807,
  author	= {Holubov\'{a}, Irena and Scherzinger, Stefanie},
  title		= {Unlocking the potential of nextGen multi-model databases
		  for semantic big data projects},
  year		= {2019},
  isbn		= {9781450367660},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3323878.3325807},
  doi		= {10.1145/3323878.3325807},
  abstract	= {A new vision in semantic big data processing is to create
		  enterprise data hubs, with a 360° view on all data that
		  matters to a corporation. As we discuss in this paper, a
		  new generation of multi-model database systems seems a
		  promising architectural choice for building such scalable,
		  non-native triple stores. In this paper, we first
		  characterize this new generation of multi-model databases.
		  Then, discussing an example scenario, we show how they
		  allow for agile and flexible schema management, spanning a
		  large design space for creative and incremental data
		  modelling. We identify the challenge of generating sound
		  triple-views from data stored in several, interlinked
		  models, for SPARQL querying. We regard this as one of
		  several appealing research challenges where the semantic
		  big data and the database architecture community may join
		  forces.},
  booktitle	= {Proceedings of the International Workshop on Semantic Big
		  Data},
  articleno	= {6},
  numpages	= {6},
  keywords	= {multi-model DBMS, schema evolution, semantic data
		  management},
  location	= {Amsterdam, Netherlands},
  series	= {SBD '19}
}

@Article{	  10.1145/3612921,
  author	= {Bensalem, Raja and Haddar, Kais and Blache, Philippe},
  title		= {An Arabic Probabilistic Parser Based on a Property
		  Grammar},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {10},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3612921},
  doi		= {10.1145/3612921},
  abstract	= {The specificities of Arabic parsing, such as
		  agglutination, vocalization, and the relatively order-free
		  words in Arabic sentences, remain major issues to consider.
		  To promote its robustness, such parseing should define
		  different types of constraints. Property Grammar (PG)
		  formalism verifies the satisfiability of the constraints
		  directly on the units of the structure, thanks to its
		  properties (or relations). In this context, we propose to
		  build a probabilistic parser with syntactic properties,
		  using a PG, and we measure the production rules in terms of
		  different implicit information and in particular the
		  syntactic properties. We experimented with our parser on
		  the treebank ATB, using the parsing algorithm CYK, and we
		  obtained encouraging results. Our method is also automatic
		  for implementation of most property types. Its
		  generalization for other languages or corpus domains (using
		  treebanks) could be a good perspective. Its combination
		  with pre-trained models of BERT may also make our parser
		  faster.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  articleno	= {237},
  numpages	= {25},
  keywords	= {Probabilistic parser, property grammar formalism, Arabic
		  language, lexicalized grammar}
}

@InProceedings{	  10.1145/3383652.3423907,
  author	= {Ishii, Ryo and Ren, Xutong and Muszynski, Michal and
		  Morency, Louis-Philippe},
  title		= {Can Prediction of Turn-management Willingness Improve
		  Turn-changing Modeling?},
  year		= {2020},
  isbn		= {9781450375863},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383652.3423907},
  doi		= {10.1145/3383652.3423907},
  abstract	= {For smooth conversation, participants must carefully
		  monitor the turn-management (a.k.a. speaking and listening)
		  willingness of other conversational partners and adjust
		  turn-changing behaviors accordingly. Many studies have
		  focused on predicting the actual moments of speaker changes
		  (a.k.a. turn-changing), but to the best of our knowledge,
		  none of them explicitly modeled the turn-management
		  willingness from both speakers and listeners in dyad
		  interactions. We address the problem of building models for
		  predicting this willingness of both. Our models are based
		  on trimodal inputs, including acoustic, linguistic, and
		  visual cues from conversations. We also study the impact of
		  modeling willingness to help improve the task of
		  turn-changing prediction. We introduce a dyadic
		  conversation corpus with annotated scores of
		  speaker/listener turn-management willingness. Our results
		  show that using all of three modalities of speaker and
		  listener is important for predicting turn-management
		  willingness. Furthermore, explicitly adding willingness as
		  a prediction task improves the performance of turn-changing
		  prediction. Also, turn-management willingness prediction
		  becomes more accurate with this multi-task learning
		  approach.},
  booktitle	= {Proceedings of the 20th ACM International Conference on
		  Intelligent Virtual Agents},
  articleno	= {28},
  numpages	= {8},
  keywords	= {multimodal signal processing, multitask learning,
		  turn-changing prediction, turn-management willingness},
  location	= {Virtual Event, Scotland, UK},
  series	= {IVA '20}
}

@InProceedings{	  10.1145/3429889.3429904,
  author	= {Liang, Changwei and Pan, Xiaosheng and Kong, Jiangping},
  title		= {A Speech-Driven 3-D Lip Synthesis with Realistic Dynamics
		  in Mandarin Chinese},
  year		= {2020},
  isbn		= {9781450388603},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3429889.3429904},
  doi		= {10.1145/3429889.3429904},
  abstract	= {In this paper, a new speech-driven lip synchronization
		  method is developed, predicting the 3-D geometric shape of
		  the lip without using speech recognition model in the
		  visualization procedure, and can be trained and evaluated
		  with realistic dynamics. Videos of Mandarin Chinese words
		  are used. Speech signals are calculated into MFCC as audio
		  features. 68-points facial landmarks are annotated from the
		  corresponding videos through the prediction algorithm from
		  the Dlib Library. Eos, a 3-D Morphable Face Model, is
		  applied, using the facial landmarks, to predict the 3-D
		  shape, where we can acquire 3-D landmarks. A
		  machine-learning sequence-tagging model, averaged
		  Structured Perceptron using Viterbi algorithm, is applied
		  for modelling the direct prediction of labial parameters
		  from the acoustic MFCC parameters. The 3-D labial area
		  shape from the 'eos' prediction of a frame is morphed
		  according to the predicted 3-D labial landmarks, forming
		  the 3-D lip sequence, which can be plotted synchronically
		  with the acoustic signal. In this 3-D lip synthesis,
		  acoustic features and realistic lip shapes are directly
		  mapped, where lip units and speech recognition are not
		  applied, preserving more realistic articulatory or
		  personality details; and the predicted geometric shapes are
		  comparable with realistic dynamics, with the comparison
		  indicating that this synthesis is of good effect.},
  booktitle	= {Proceedings of the 1st International Symposium on
		  Artificial Intelligence in Medical Sciences},
  pages		= {79–84},
  numpages	= {6},
  keywords	= {3-D lip synthesis, Mandarin Chinese, realistic dynamics,
		  speech-driven},
  location	= {Beijing, China},
  series	= {ISAIMS '20}
}

@Article{	  10.1145/3369780,
  author	= {Razis, Gerasimos and Anagnostopoulos, Ioannis and
		  Zeadally, Sherali},
  title		= {Modeling Influence with Semantics in Social Networks: A
		  Survey},
  year		= {2020},
  issue_date	= {January 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3369780},
  doi		= {10.1145/3369780},
  abstract	= {The discovery of influential entities in all kinds of
		  networks (e.g., social, digital, or computer) has always
		  been an important field of study. In recent years, Online
		  Social Networks (OSNs) have been established as a basic
		  means of communication and often influencers and opinion
		  makers promote politics, events, brands, or products
		  through viral content. In this work, we present a
		  systematic review across (i) online social influence
		  metrics, properties, and applications and (ii) the role of
		  semantic in modeling OSNs information. We found that both
		  areas can jointly provide useful insights towards the
		  qualitative assessment of viral user-generated content, as
		  well as for modeling the dynamic properties of influential
		  content and its flow dynamics.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {7},
  numpages	= {38},
  keywords	= {Information quality, online social influence, social
		  networks, social semantics}
}

@InProceedings{	  10.1145/3440094.3440389,
  author	= {Kabanda, Gabriel},
  title		= {A bayesian network model for machine learning and cyber
		  security},
  year		= {2021},
  isbn		= {9781450387675},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3440094.3440389},
  doi		= {10.1145/3440094.3440389},
  abstract	= {The phenomenal growth in the use of internet-based
		  technologies has resulted in complexities in cyber security
		  subjecting organizations to cyber-attacks. This research is
		  purposed to develop a cyber-security system that uses the
		  Bayesian Network structure and Machine Learning. The
		  research determined the cyber-security framework
		  appropriate for a developing nation; evaluated network
		  detection and prevention systems that use Artificial
		  Intelligence paradigms such as finite automata, neural
		  networks, genetic algorithms, fuzzy logic, support vector
		  machines, or diverse data-mining-based approaches; analyzed
		  Bayesian Networks that can be represented as graphical
		  models and are directional to represent cause-effect
		  relationships; and developed a Bayesian Network model that
		  can handle complexity in cybersecurity. The Pragmatism
		  paradigm used in this research, as a philosophy is
		  intricately related to the mixed-method approach, which is
		  largely quantitative with the research design being a
		  survey and an experiment, but supported by qualitative
		  approaches where Focus Group discussions were held. The
		  Artificial Intelligence paradigms evaluated include machine
		  learning methods, autonomous robotic vehicles, artificial
		  neural networks, and fuzzy logic. Alternative improved
		  solutions discussed include the use of machine learning
		  algorithms specifically Artificial Neural Networks (ANN),
		  Decision Tree C4.5, Random Forests, and Support Vector
		  Machines (SVM).},
  booktitle	= {Proceedings of the 2nd Africa-Asia Dialogue Network (AADN)
		  International Conference on Advances in Business Management
		  and Electronic Commerce Research},
  articleno	= {9},
  numpages	= {7},
  keywords	= {Bayesian network model, artificial intelligence (AI),
		  artificial neural networks (ANN) and decision tree,
		  cybersecurity, machine learning (ML)},
  location	= {Ganzhou, China},
  series	= {AADNIC-ABMECR '20}
}

@InProceedings{	  10.1145/3508397.3564825,
  author	= {Laamech, Nouha and Munier, Manuel and Pham, Congduc},
  title		= {IdSM-O: An IoT Data Sharing Management Ontology for Data
		  Governance},
  year		= {2022},
  isbn		= {9781450392198},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3508397.3564825},
  doi		= {10.1145/3508397.3564825},
  abstract	= {The main purpose of IoT is to deliver reliable, high
		  quality services and innovative solutions by transforming
		  the captured data into meaningful information, and thus
		  improving user's daily life. In this regard, it is in the
		  interest of the community to encourage entities within IoT
		  environments to share their data, and therefore serve
		  public interest and contribute to the innovation and
		  technological progress. Meanwhile, the distributed nature
		  of IoT networks and the diversity of its actors lead to the
		  recognition of security and data sharing management as one
		  of the major challenges of the IoT domain. For instance,
		  due to insufficient governance of the shared data within
		  IoT environments, data provider retains little to no
		  control over his assets once he has agreed to share them.
		  Furthermore, data consumers are not able to trace the
		  source of the available resource nor its history processing
		  to assess its quality. All this creates a digital
		  environment that is certainly functional but lacks mutual
		  trust between its actors, which can prevent the domain's
		  full potential to be exploited, and therefore disrupt the
		  implemented services. In our work, we propose an approach
		  to improve data sharing management using three main
		  elements: semantic modeling, usage control policies, and
		  data provenance.},
  booktitle	= {Proceedings of the 14th International Conference on
		  Management of Digital EcoSystems},
  pages		= {88–95},
  numpages	= {8},
  keywords	= {data governance, data sharing management, semantic
		  modeling, usage control},
  location	= {Venice, Italy},
  series	= {MEDES '22}
}

@InProceedings{	  10.1145/3449726.3459579,
  author	= {Kostovska, Ana and Vermetten, Diederick and Doerr, Carola
		  and D\v{z}eroski, Sa\v{s}o and Panov, Pan\v{c}e and
		  Eftimov, Tome},
  title		= {OPTION: optimization algorithm benchmarking ontology},
  year		= {2021},
  isbn		= {9781450383516},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3449726.3459579},
  doi		= {10.1145/3449726.3459579},
  abstract	= {Many platforms for benchmarking optimization algorithms
		  offer users the possibility of sharing their experimental
		  data with the purpose of promoting reproducible and
		  reusable research. However, different platforms use
		  different data models and formats, which drastically
		  inhibits identification of relevant data sets, their
		  interpretation, and their interoperability. Consequently, a
		  semantically rich, ontology-based, machine-readable data
		  model is highly desired.We report in this paper on the
		  development of such an ontology, which we name OPTION
		  (OPTImization algorithm benchmarking ONtology). Our
		  ontology provides the vocabulary needed for semantic
		  annotation of the core entities involved in the
		  benchmarking process, such as algorithms, problems, and
		  evaluation measures. It also provides means for automated
		  data integration, improved interoperability, powerful
		  querying capabilities and reasoning, thereby enriching the
		  value of the benchmark data. We demonstrate the utility of
		  OPTION by annotating and querying a corpus of benchmark
		  performance data from the BBOB workshop data - a use case
		  which can be easily extended to cover other benchmarking
		  data collections.},
  booktitle	= {Proceedings of the Genetic and Evolutionary Computation
		  Conference Companion},
  pages		= {239–240},
  numpages	= {2},
  location	= {Lille, France},
  series	= {GECCO '21}
}

@Article{	  10.1145/3711939,
  author	= {Pennanen, Niki and Linkola, Simo and Kantosalo, Anna and
		  Hiillos, Nicolas and M\"{a}nnist\"{o}, Tomi and
		  Guckelsberger, Christian},
  title		= {From Product to Producer: The Impact of Perceptual
		  Evidence and Robot Embodiment on the Human Assessment of AI
		  Creativity},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {3},
  url		= {https://doi.org/10.1145/3711939},
  doi		= {10.1145/3711939},
  abstract	= {While creative artificial intelligence (AI) is becoming
		  integral to our lives, we know little about what makes us
		  call AI “creative”. Informed by prior theoretical and
		  empirical work, we investigate how perceiving evidence of a
		  creative act beyond the final product affects our
		  assessment of robot creativity. We study embodiment
		  morphology as a potential moderator of this relationship,
		  informing a 3 \texttimes{} 2 factorial design. In two
		  lab experiments on visual art, participants (N =
		  30 + 60) assessed drawings produced by two physical
		  robots with different morphologies, under exposure to
		  product, process and producer as three levels of perceptual
		  evidence. The data supports that the human assessment of
		  robot creativity is significantly higher the more is
		  revealed beyond the product about the creation process, and
		  eventually the producer. We find no significant effects of
		  embodiment morphology, contrasting existing hypotheses and
		  offering a more detailed understanding for future work. The
		  latter is also informed by additional exploratory analyses
		  revealing factors potentially influencing creativity
		  assessments, including perceived robot likeability and
		  participants’ experience with robotics and AI. Our
		  insights empirically ground existing design patterns,
		  foster fairness and validity in system comparisons, and
		  contribute to a deeper understanding of our relationship
		  with creative AI and thus its adoption in society.},
  journal	= {J. Hum.-Robot Interact.},
  month		= apr,
  articleno	= {41},
  numpages	= {41},
  keywords	= {Human-Robot Interaction, Computational Creativity,
		  Creative AI, Human Perception, Embodiment}
}

@InProceedings{	  10.1145/3584684.3597263,
  author	= {Ilani, Arnon and Dolev, Shlomi},
  title		= {Invited Paper: Common Public Knowledge for Enhancing
		  Machine Learning Data Sets},
  year		= {2023},
  isbn		= {9798400701283},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584684.3597263},
  doi		= {10.1145/3584684.3597263},
  abstract	= {In this study, we show the advantages of incorporating
		  multi-source knowledge from publicly available sources,
		  such as ChatGPT and Wikipedia, into existing datasets to
		  enhance the performance of machine learning models for
		  routine tasks, such as classification. specifically, we
		  propose the utilization of supplementary data from external
		  sources and demonstrate the utility of widely accessible
		  knowledge in the context of the Forest Cover Type
		  Prediction task launched by the Roosevelt National Forest
		  of Northern Colorado. Additionally, we exhibit an
		  improvement in classification accuracy for the Isolated
		  Letter Speech Recognition dataset when incorporating
		  information on regional accents in the prediction of spoken
		  English letter names.},
  booktitle	= {Proceedings of the 5th Workshop on Advanced Tools,
		  Programming Languages, and PLatforms for Implementing and
		  Evaluating Algorithms for Distributed Systems},
  articleno	= {2},
  numpages	= {10},
  keywords	= {ontology, machine learning, random forests, feature
		  engineering, world knowledge, speech recognition, isolated
		  letter, forest management, tree cover type, ChatGPT},
  location	= {Orlando, FL, USA},
  series	= {ApPLIED 2023}
}

@InProceedings{	  10.1145/3330204.3330233,
  author	= {Barroso, Jos\'{e} S. and Pimentel, Mariano and Nunes,
		  Vanessa and Cappelli, Claudia},
  title		= {Design Science Research to design a conceptual model about
		  prosopographic information related to politicians},
  year		= {2019},
  isbn		= {9781450372374},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3330204.3330233},
  doi		= {10.1145/3330204.3330233},
  abstract	= {The growing demand for information about politicians and
		  the speed with which news is propagated by media and social
		  networks reveals in contemporary times a more participatory
		  view of the citizen in politics and social control in
		  government actions. Speculation about information about
		  politicians and their respective parties have become more
		  constant, such as in electoral periods, investigations on
		  processes, journalistic interest, etc. This can be observed
		  during the electoral period of 2018 in which the search for
		  information about the candidates and their respective
		  parties was notorious. Given that information transparency
		  is paramount for a democratic regime, the organization and
		  consolidation of data from official sources is a reliable
		  tool for consultation and dissemination of knowledge.
		  According to this period, this work sought to understand
		  some cognitive models that explain electoral behavior,
		  which led to the investigation of factors that influence
		  the decision to vote. The observed cognitive models
		  indicate some attitudes, opinions, satisfactions, events
		  and government assessments that interfere to some degree in
		  the choice of vote, approval or disapproval for some
		  purpose. Among the factors of influence observed, there are
		  prosopographic information, which deals with the biography
		  of politicians, and which are available in the various
		  transparency portals. However, the data contained in these
		  platforms are not organized in such a way as to make the
		  information more intelligible and more reliable to the
		  citizen. In this sense, this research is interested in the
		  prosopographic information content, which would be better
		  evaluated if the data of the politicians were organized
		  according to a conceptual model of knowledge. The
		  methodological approach adopted in this research is the
		  Design Science Reserch (DSR), which directs the
		  construction of an artifact in a given context, whose
		  theoretical conjectures are based on the search and
		  production of knowledge. As a result, research contributes
		  to the knowledge base, as it discusses the evolution of
		  cognitive models and through the design of the artifact,
		  delivers reliable and relevant information about the life
		  of a politician.},
  booktitle	= {Proceedings of the XV Brazilian Symposium on Information
		  Systems},
  articleno	= {24},
  numpages	= {8},
  keywords	= {Conceptual Modeling, Data Transparency, Organized
		  Information, Prosopography Politicial},
  location	= {Aracaju, Brazil},
  series	= {SBSI '19}
}

@InProceedings{	  10.1145/3701716.3715310,
  author	= {Zhou, Yan and Zhou, Baifan and Li, Huajian and Lyu,
		  Qianhang and Qu, Yuanwei and Waaler, Arild and Yu, Ingrid
		  C.},
  title		= {Dataset for Industrial Question Answering with Explanation
		  and Scalable Ensemble Generation},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715310},
  doi		= {10.1145/3701716.3715310},
  abstract	= {The digital and green transition under Industry 4.0 has
		  accelerated the adoption of AI in industries such as
		  manufacturing, energy, and mining. Question Answering with
		  Explanation (QAE), as a way of human interaction with AI,
		  is crucial for enhancing transparency and trust in
		  high-stakes industrial applications. However, industrial
		  QAE remains underexplored due to the lack of publicly
		  available, high-quality datasets, hindered by the need for
		  expert effort and corporate restrictions. To this end, we
		  introduce PANDAX ( https://doi.org/10.5281/zenodo.14510798
		  ), the first open-source industrial QAE dataset, and SEG, a
		  scalable method for generating high-quality QAE datasets
		  using LLMs. PANDAX focuses on three key topics of
		  industrial system information: partonomy, functionality,
		  and parameters, across critical domains such as green
		  technology and cooling systems. SEG ensures scalability and
		  quality through ensemble generation, majority voting,
		  expert ranking, etc. The human evaluation validates
		  PANDAX's high quality, positioning it as a valuable
		  resource for advancing QAE techniques, benchmarking
		  language technologies, and supporting research in
		  explainable AI for industrial systems.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {825–828},
  numpages	= {4},
  keywords	= {dataset generation, industrial dataset resource, question
		  answering with explanation, system information},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3383583.3398539,
  author	= {Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta,
		  Amit and Jah, Moriba},
  title		= {Modeling Data Curation to Scientific Inquiry: A Case Study
		  for Multimodal Data Integration},
  year		= {2020},
  isbn		= {9781450375856},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383583.3398539},
  doi		= {10.1145/3383583.3398539},
  abstract	= {Scientific data publications may include interactive data
		  applications designed by scientists to explore a scientific
		  problem. Defined as knowledge systems, their development is
		  complex when data are aggregated from multiple sources over
		  time. Multimodal data are created, encoded, and maintained
		  differently, and even when reporting about identical
		  phenomena, fields and their values may be inconsistent
		  across datasets. To assure the validity and accuracy of the
		  application, the data has to abide by curation requirements
		  similar to those ruling digital libraries. We present a
		  novel, inquiry-driven curation approach aimed to optimize
		  multimodal datasets curation and maximize data reuse by
		  domain researchers. We demonstrate the method through the
		  ASTRIAGraph project, in which multiple data sources about
		  near earth space objects are aggregated into a central
		  knowledge system. The process involves multidisciplinary
		  collaboration, resulting in the design of a data model as
		  the backbone for both data curation and scientific inquiry.
		  We demonstrate a) how data provenance information is needed
		  to assess the uncertainty of the results of scientific
		  inquiries involving multiple data sources, and b) that
		  continuous curation of integrated datasets is facilitated
		  when undertaken as integral to the research project. The
		  approach provides flexibility to support expansion of
		  scientific inquiries and data in the knowledge system, and
		  allows for transparent and explainable results.},
  booktitle	= {Proceedings of the ACM/IEEE Joint Conference on Digital
		  Libraries in 2020},
  pages		= {235–242},
  numpages	= {8},
  keywords	= {big data, data curation, data integration, data model,
		  graph database, knowledge system, space traffic
		  management},
  location	= {Virtual Event, China},
  series	= {JCDL '20}
}

@Article{	  10.1145/3394979,
  author	= {Rocha Silva, Thiago and Winckler, Marco and
		  Tr\ae{}tteberg, Hallvard},
  title		= {Ensuring the Consistency between User Requirements and
		  Task Models: A Behavior-Based Automated Approach},
  year		= {2020},
  issue_date	= {June 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {EICS},
  url		= {https://doi.org/10.1145/3394979},
  doi		= {10.1145/3394979},
  abstract	= {Evaluating and ensuring the consistency between user
		  requirements and modeling artifacts is a long-time issue
		  for model-based software design. Conflicts in requirements
		  specifications can lead to many design errors and have a
		  decisive impact on the quality of systems under
		  development. This article presents an approach based on
		  Behavior-Driven Development (BDD) to provide automated
		  assessment for task models, which are intended to model the
		  flow of user and system tasks in an interactive system. The
		  approach has been evaluated by exploiting user requirements
		  described by a group of experts in the domain of business
		  trips. Such requirements gave rise to a set of BDD stories
		  that have been used to automatically assess scenarios
		  extracted from task models that were reengineered from an
		  existing web system for booking business trips. The results
		  have shown our approach, by performing a static analysis of
		  the source files, was able to identify different types of
		  inconsistencies between the user requirements and the set
		  of task models analyzed.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jun,
  articleno	= {77},
  numpages	= {32},
  keywords	= {automated requirements assessment, behavior-driven
		  development (BDD), task models, user stories}
}

@InProceedings{	  10.1145/3706598.3713280,
  author	= {Singh, Aneesha and Dechant, Martin Johannes and Patel,
		  Dilisha and Soubutts, Ewan and Barbareschi, Giulia and
		  Ayobi, Amid and Newhouse, Nikki},
  title		= {Exploring Positionality in HCI: Perspectives, Trends, and
		  Challenges},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713280},
  doi		= {10.1145/3706598.3713280},
  abstract	= {Positionality acknowledges that researchers’
		  subjectivities, values and experiences influence approaches
		  to and outcomes of research. It underlines and promotes
		  self-awareness and explicit demonstration of reflexivity.
		  To understand how positionality is conceptualised and used
		  in HCI, we conducted two studies: (i) a scoping review of
		  positionality and reflexivity statements in CHI papers from
		  the last 11 years and (ii) a survey of HCI researchers
		  (n=75). Our findings show that positionality statements are
		  often viewed as a box-ticking exercise and their influence
		  on the research is seldom discussed. They are also often
		  restricted to more sensitive areas of research and may
		  impact marginalised identities. We argue that positionality
		  statements may be valuable but not as markers of
		  methodological rigour; their content should be at the
		  discretion of authors and methodologically consistent. Our
		  contributions include a current snapshot of positionality
		  in HCI and reflections on its current role and future
		  directions in HCI.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {451},
  numpages	= {18},
  keywords	= {positionality, reflexivity, identity, methodology,
		  methods},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3365438.3410987,
  author	= {Kesper, Arno and Wenz, Viola and Taentzer, Gabriele},
  title		= {Detecting quality problems in research data: a
		  model-driven approach},
  year		= {2020},
  isbn		= {9781450370196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3365438.3410987},
  doi		= {10.1145/3365438.3410987},
  abstract	= {As scientific progress highly depends on the quality of
		  research data, there are strict requirements for data
		  quality coming from the scientific community. A major
		  challenge in data quality assurance is to localise quality
		  problems that are inherent to data. Due to the dynamic
		  digitalisation in specific scientific fields, especially
		  the humanities, different database technologies and data
		  formats may be used in rather short terms to gain
		  experiences. We present a model-driven approach to analyse
		  the quality of research data. It allows abstracting from
		  the underlying database technology. Based on the
		  observation that many quality problems show anti-patterns,
		  a data engineer formulates analysis patterns that are
		  generic concerning the database format and technology. A
		  domain expert chooses a pattern that has been adapted to a
		  specific database technology and concretises it for a
		  domain-specific database format. The resulting concrete
		  patterns are used by data analysts to locate quality
		  problems in their databases. As proof of concept, we
		  implemented tool support that realises this approach for
		  XML databases. We evaluated our approach concerning
		  expressiveness and performance in the domain of cultural
		  heritage based on a qualitative study on quality problems
		  occurring in cultural heritage data.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {354–364},
  numpages	= {11},
  keywords	= {data quality, model-driven development, pattern matching},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@Proceedings{	  10.1145/3701716,
  title		= {WWW '25: Companion Proceedings of the ACM on Web
		  Conference 2025},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to The ACM Web Conference 2025 (WWW'25), held from
		  April 28 to May 2, 2025, at the Sydney Convention \&amp;
		  Exhibition Centre in Australia. Recognizing the breadth of
		  this year's program, we are publishing two sets of
		  proceedings: one dedicated to research track papers, the
		  Web4Good track, and keynotes; and a companion proceedings
		  featuring the Demo Paper Track, Short Paper Track, PhD
		  Symposium, Special Day, Resource Track, and History of Web
		  sessions. Through this rich assortment of activities, we
		  aim to foster a vibrant community where delegates can share
		  ideas, forge collaborations, and cultivate a sustainable,
		  inclusive environment.Marking its 34th edition, WWW'25
		  welcomes more than 1,000 industry and academic experts
		  eager to shape the future of Web technologies and
		  applications. The conference logo, depicting the Sydney
		  Harbour Bridge, symbolizes the Web's key function of
		  "connecting" people and information. Originally founded at
		  CERN in 1994 as the International World Wide Web Conference
		  (WWW), this event has long been the leading venue for
		  research, development, standards, and applications related
		  to the Web.Over the years, WWW has introduced important
		  breakthroughs, from The Anatomy of a Large-Scale Web Search
		  Engine in 1998-which heralded Google-to the EigenTrust
		  algorithm in 2003 and the YAGO knowledge base in 2007. In
		  the period between 2024 and 2025, large language models
		  (LLMs) have significantly influenced countless industries
		  and everyday life, prompting fundamental shifts in the Web
		  ecosystem. We anticipate that the research and discussions
		  at this year's conference will spark further breakthroughs
		  in this rapidly evolving field.Continuing the tradition of
		  depth and diversity, WWW'25 accepted 26 workshops and 20
		  tutorials as pre-conference events. In addition, consistent
		  with previous editions, we feature a Demo Paper Track,
		  Short Paper Track, PhD Symposium, Special Day, Resource
		  Track, History of Web sessions, and Artifact Badging. Some
		  of these are incorporated into the main conference schedule
		  to encourage communication and collaboration among
		  attendees.A commitment to diversity is pivotal to fostering
		  robust, sustainable Web technologies. Our Web4Good track
		  highlights how Web-based tools can address societal
		  challenges, while the Industry Track showcases novel and
		  impactful results from the industrial sector. This year, we
		  have also launched an Emerging World Track to broaden
		  participation from developing countries, as well as a
		  competition track to enhance industry
		  engagement..Organizing WWW'25 has been a true team effort.
		  We sincerely thank the authors who contributed their work,
		  and we extend our gratitude to the program and senior
		  program committees for their dedication to reviewing
		  submissions and offering feedback. We also appreciate the
		  many additional chairs whose efforts ensured that each
		  element of the program ran smoothly. Together, they have
		  helped this conference continue to grow into a premier
		  event for the Web research community.Finally, we would like
		  to thank ACM SIGWEB and our industry sponsors-Meta, Huawei,
		  Google, Baidu, Taobao, JD, and Infinigence-for their
		  generous support, as well as our local government sponsors,
		  Business Event Sydney and the NSW Government. We are
		  equally grateful to our academic partner, the University of
		  Technology Sydney, and the PCO company, ICMSA, for their
		  indispensable help with registration, venue logistics, and
		  social events. Their collective contributions have made the
		  2025 edition of this conference a resounding success.},
  location	= {Sydney NSW, Australia}
}

@InProceedings{	  10.1145/3603163.3609035,
  author	= {Renda, Giulia and Daquino, Marilena and Presutti,
		  Valentina},
  title		= {Melody: A Platform for Linked Open Data Visualisation and
		  Curated Storytelling},
  year		= {2023},
  isbn		= {9798400702327},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603163.3609035},
  doi		= {10.1145/3603163.3609035},
  abstract	= {Data visualisation and storytelling techniques help
		  experts highlight relations between data and share complex
		  information with a broad audience. However, existing
		  solutions targeted to Linked Open Data visualisation have
		  several restrictions and lack the narrative element. In
		  this article we present MELODY, a web interface for
		  authoring data stories based on Linked Open Data. MELODY
		  has been designed using a novel methodology that harmonises
		  existing Ontology Design and User Experience methodologies
		  (eXtreme Design and Design Thinking), and provides reusable
		  User Interface components to create and publish web-ready
		  article-alike documents based on data retrievable from any
		  SPARQL endpoint. We evaluate the software by comparing it
		  with existing solutions, and we show its potential impact
		  in projects where data dissemination is crucial.},
  booktitle	= {Proceedings of the 34th ACM Conference on Hypertext and
		  Social Media},
  articleno	= {27},
  numpages	= {8},
  keywords	= {Linked Open Data, data visualization, design thinking,
		  ontology design, storytelling},
  location	= {Rome, Italy},
  series	= {HT '23}
}

@InProceedings{	  10.1145/3516875.3516922,
  author	= {Widianto, Idam Ragil Widianto and Ardiansyah, Roy and
		  Saputri, Dwi Yuniasih},
  title		= {The Empowerment of Critical Thinking Skills through
		  Problem-Based Learning Model Viewed From Epigenetic},
  year		= {2022},
  isbn		= {9781450386920},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3516875.3516922},
  doi		= {10.1145/3516875.3516922},
  abstract	= {Critical thinking skills can face 21st-century challenges,
		  which increasingly require technology and science in a
		  global society in this world. Thus, education must be
		  oriented towards mathematics and natural sciences
		  accompanied by social and human sciences. Therefore, this
		  study aims to describe the empowerment of critical thinking
		  skills through a problem-based learning model viewed from
		  the epigenetic aspect. This research employed library
		  research. In this study, data collection was obtained from
		  news, articles in journals, and relevant books. The
		  analysis was carried out in four stages: 1) data
		  collection, 2) data reduction, 3) data display, and 4)
		  conclusion. This study's results revealed that learning
		  needs to pay attention to nature and nurture because it
		  dramatically affects the mastery of thinking skills, and
		  bridging the two things in learning will be a challenge for
		  science teachers in the future. The problem-based learning
		  model can be utilized as a natural science learning model
		  that can empower critical thinking skills from an
		  epigenetic perspective. It can be concluded that the
		  cellular and molecular mechanisms of learning and memory
		  have long been a major focus of neurology and molecular
		  biology, a concern regarding the epigenetic mechanisms
		  behind dynamic changes in the transcription of genes
		  responsible for memory formation and maintenance.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Learning Innovation and Quality Education},
  articleno	= {38},
  numpages	= {3},
  location	= {Surakarta, Indonesia},
  series	= {ICLIQE '21}
}

@InProceedings{	  10.1145/3715275.3732003,
  author	= {Sabuncuoglu, Alpay and Burr, Christopher and Maple,
		  Carsten},
  title		= {Justified Evidence Collection for Argument-based AI
		  Fairness Assurance},
  year		= {2025},
  isbn		= {9798400714825},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715275.3732003},
  doi		= {10.1145/3715275.3732003},
  abstract	= {It is well recognised that ensuring fair AI systems is a
		  complex sociotechnical challenge, which requires careful
		  deliberation and continuous oversight across all stages of
		  a system’s lifecycle, from defining requirements to model
		  deployment and deprovisioning. Dynamic argument-based
		  assurance cases, which present structured arguments
		  supported by evidence, have emerged as a systematic
		  approach to evaluating and mitigating safety risks and
		  hazards in AI-enabled system development and have also been
		  extended to deal with broader normative goals such as
		  fairness and explainability. This paper introduces a
		  systems-engineering-driven framework, supported by software
		  tooling, to operationalise a dynamic approach to
		  argument-based assurance in two stages. In the first stage,
		  during the requirements planning phase, a
		  multi-disciplinary and multi-stakeholder team define goals
		  and claims to be established (and evidenced) by conducting
		  a comprehensive fairness governance process. In the second
		  stage, a continuous monitoring interface gathers evidence
		  from existing artefacts (e.g. metrics from automated
		  tests), such as model, data, and use case documentation, to
		  support these arguments dynamically. The framework’s
		  effectiveness is demonstrated through an illustrative case
		  study in finance, with a focus on supporting
		  fairness-related arguments.},
  booktitle	= {Proceedings of the 2025 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {18–28},
  numpages	= {11},
  keywords	= {trustworthy and ethical assurance, continuous fairness
		  monitoring, system transparency artefacts, large language
		  models in finance},
  location	= { },
  series	= {FAccT '25}
}

@InProceedings{	  10.5555/3716662.3716664,
  author	= {Akbulut, Canfer and Weidinger, Laura and Manzini, Arianna
		  and Gabriel, Iason and Rieser, Verena},
  title		= {All Too Human? Mapping and Mitigating the Risks from
		  Anthropomorphic AI},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {The development of highly-capable conversational agents,
		  underwritten by large language models, has the potential to
		  shape user interaction with this technology in profound
		  ways, particularly when the technology is anthropomorphic,
		  or appears human-like. Although the effects of
		  anthropomorphic AI are often benign, anthropomorphic design
		  features also create new kinds of risk. For example, users
		  may form emotional connections to human-like AI, creating
		  the risk of infringing on user privacy and autonomy through
		  over-reliance. To better understand the possible pitfalls
		  of anthropomorphic AI systems, we make two contributions:
		  first, we explore anthropomorphic features that have been
		  embedded in interactive systems in the past, and leverage
		  this precedent to highlight the current implications of
		  anthropomorphic design. Second, we propose research
		  directions for informing the ethical design of
		  anthropomorphic AI. In advancing the responsible
		  development of AI, we promote approaches to the ethical
		  foresight, evaluation, and mitigation of harms arising from
		  user interactions with anthropomorphic AI.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {13–26},
  numpages	= {14},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@Article{	  10.1145/3291043,
  author	= {Abdulahhad, Karam and Berrut, Catherine and Chevallet,
		  Jean-Pierre and Pasi, Gabriella},
  title		= {Modeling Information Retrieval by Formal Logic: A Survey},
  year		= {2019},
  issue_date	= {January 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3291043},
  doi		= {10.1145/3291043},
  abstract	= {Several mathematical frameworks have been used to model
		  the information retrieval (IR) process, among them, formal
		  logics. Logic-based IR models upgrade the IR process from
		  document-query comparison to an inference process, in which
		  both documents and queries are expressed as sentences of
		  the selected formal logic. The underlying formal logic also
		  permits one to represent and integrate knowledge in the IR
		  process. One of the main obstacles that has prevented the
		  adoption and large-scale diffusion of logic-based IR
		  systems is their complexity. However, several logic-based
		  IR models have been recently proposed that are applicable
		  to large-scale data collections. In this survey, we present
		  an overview of the most prominent logical IR models that
		  have been proposed in the literature. The considered
		  logical models are categorized under different axes, which
		  include the considered logics and the way in which
		  uncertainty has been modeled, for example, degrees of
		  belief or degrees of truth. Accordingly, the main
		  contribution of the article is to categorize the
		  state-of-the-art logical models on a fine-grained basis,
		  and for the considered models the related implementation
		  aspects are described. Consequently, the proposed survey is
		  finalized to better understand and compare the different
		  logical IR models. Last, but not least, this article aims
		  at reconsidering the potentials of logical approaches to IR
		  by outlining the advances of logic-based approaches in
		  close research areas.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {15},
  numpages	= {37},
  keywords	= {uncertainty, survey, logical models, information retrieval
		  models, Formal logics}
}

@Article{	  10.1145/3656587,
  author	= {Lorv\~{a}o Antunes, Ant\'{o}nio and Barateiro, Jos\'{e}
		  and Cardoso, Elsa},
  title		= {Strategic Analysis in the Public Sector Using Semantic Web
		  Technologies},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {3},
  url		= {https://doi.org/10.1145/3656587},
  doi		= {10.1145/3656587},
  abstract	= {This article addresses the complex challenges that public
		  organizations face in designing, implementing, and
		  evaluating their strategies, where public interest and
		  regulatory compliance often intertwine with strategic
		  objectives. This research investigates the application of
		  ontologies in the field of public sector strategy
		  management to enhance the capacity of organizations to make
		  informed data-driven decisions, efficiently allocate
		  resources, and effectively navigate the intricate landscape
		  of the public sector. The LNEC - National Laboratory for
		  Civil Engineering’s strategy is used as an exploratory
		  case study. Semantic web technologies are used to perform
		  strategy analysis, including validating the strategy
		  formulation and supporting the strategy execution by
		  assessing performance indicators, verifying the design of
		  cause-and-effect relationships between strategic
		  objectives, and monitoring and empirically validating these
		  relationships. The increased interoperability of these
		  technologies enables information sharing across systems and
		  organizations. Following the strategy analysis,
		  recommendations are provided, leading to a more robust and
		  data-driven strategic management approach, enabling
		  accurate, traceable, and continuous monitoring of an
		  organization’s strategy. Theoretical and practical
		  implications are discussed, along with limitations and
		  future work. This research offers a blueprint for public
		  sector organizations seeking to optimize their strategies,
		  foster transparency, and deliver more effective services to
		  the public they serve.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= sep,
  articleno	= {20},
  numpages	= {20},
  keywords	= {Strategy Analysis, Public Sector, Semantic Web, Balanced
		  Scorecard, Ontology}
}

@Article{	  10.1145/3609483,
  author	= {Moscato, Vincenzo and Postiglione, Marco and Sperl\'{\i},
		  Giancarlo},
  title		= {Few-shot Named Entity Recognition: Definition, Taxonomy
		  and Research Directions},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {5},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3609483},
  doi		= {10.1145/3609483},
  abstract	= {Recent years have seen an exponential growth (+98\% in
		  2022 w.r.t. the previous year) of the number of research
		  articles in the few-shot learning field, which aims at
		  training machine learning models with extremely limited
		  available data. The research interest toward few-shot
		  learning systems for Named Entity Recognition (NER) is thus
		  at the same time increasing. NER consists in identifying
		  mentions of pre-defined entities from unstructured text,
		  and serves as a fundamental step in many downstream tasks,
		  such as the construction of Knowledge Graphs, or Question
		  Answering. The need for a NER system able to be trained
		  with few-annotated examples comes in all its urgency in
		  domains where the annotation process requires time,
		  knowledge and expertise (e.g., healthcare, finance, legal),
		  and in low-resource languages. In this survey, starting
		  from a clear definition and description of the few-shot NER
		  (FS-NER) problem, we take stock of the current
		  state-of-the-art and propose a taxonomy which divides
		  algorithms in two macro-categories according to the
		  underlying mechanisms: model-centric and data-centric. For
		  each category, we line-up works as a story to show how the
		  field is moving toward new research directions. Eventually,
		  techniques, limitations, and key aspects are deeply
		  analyzed to facilitate future studies.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  articleno	= {94},
  numpages	= {46},
  keywords	= {Few-shot learning, Named Entity Recognition}
}

@InProceedings{	  10.1145/3638584.3638670,
  author	= {Sai P, Daiveek and Rajesh, Anouksha},
  title		= {Semantic Topic Extraction from Research Artifacts},
  year		= {2024},
  isbn		= {9798400708688},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638584.3638670},
  doi		= {10.1145/3638584.3638670},
  abstract	= {The GENESIS project introduces a novel framework for
		  Research Artifact Data Semantics to enhance semantic web
		  technologies within academic institutions. This paper
		  introduces a system developed as part of the GENESIS
		  challenge, focusing on extracting and labelling topics from
		  diverse research artifacts. The approach employs knowledge
		  graphs, topic models, and named entity recognition to infer
		  meaningful topics. Evaluation encompasses measures such as
		  topic coherence and semantic similarity. Future work
		  includes refining entity linking techniques, optimizing
		  system performance, and implementing advanced graph
		  centrality algorithms. This contribution advances semantic
		  understanding in various research contexts.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {539–545},
  numpages	= {7},
  keywords	= {knowledge graph, linked data, natural language processing,
		  text summarization, topic modelling},
  location	= {Beijing, China},
  series	= {CSAI '23}
}

@InProceedings{	  10.1145/3657604.3662030,
  author	= {Moore, Steven and Schmucker, Robin and Mitchell, Tom and
		  Stamper, John},
  title		= {Automated Generation and Tagging of Knowledge Components
		  from Multiple-Choice Questions},
  year		= {2024},
  isbn		= {9798400706332},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3657604.3662030},
  doi		= {10.1145/3657604.3662030},
  abstract	= {Knowledge Components (KCs) linked to assessments enhance
		  the measurement of student learning, enrich analytics, and
		  facilitate adaptivity. However, generating and linking KCs
		  to assessment items requires significant effort and
		  domain-specific knowledge. To streamline this process for
		  higher-education courses, we employed GPT-4 to generate KCs
		  for multiple-choice questions (MCQs) in Chemistry and
		  E-Learning. We analyzed discrepancies between the KCs
		  generated by the Large Language Model (LLM) and those made
		  by humans through evaluation from three domain experts in
		  each subject area. This evaluation aimed to determine
		  whether, in instances of non-matching KCs, evaluators
		  showed a preference for the LLM-generated KCs over their
		  human-created counterparts. We also developed an ontology
		  induction algorithm to cluster questions that assess
		  similar KCs based on their content. Our most effective LLM
		  strategy accurately matched KCs for 56\% of Chemistry and
		  35\% of E-Learning MCQs, with even higher success when
		  considering the top five KC suggestions. Human evaluators
		  favored LLM-generated KCs, choosing them over
		  human-assigned ones approximately two-thirds of the time, a
		  preference that was statistically significant across both
		  domains. Our clustering algorithm successfully grouped
		  questions by their underlying KCs without needing explicit
		  labels or contextual information. This research advances
		  the automation of KC generation and classification for
		  assessment items, alleviating the need for student data or
		  predefined KC labels.},
  booktitle	= {Proceedings of the Eleventh ACM Conference on Learning @
		  Scale},
  pages		= {122–133},
  numpages	= {12},
  keywords	= {concept labeling, knowledge component, knowledge labeling,
		  learning engineering, multiple-choice question},
  location	= {Atlanta, GA, USA},
  series	= {L@S '24}
}

@InProceedings{	  10.1145/3627673.3679528,
  author	= {Yang, Zhihao and Zhao, Yizheng},
  title		= {What a Surprise! Computing Rewritten Modules Can Be as
		  Efficient as Computing Subset Modules},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679528},
  doi		= {10.1145/3627673.3679528},
  abstract	= {Uniform Interpolation (UI) is an advanced non-standard
		  reasoning service that seeks to refine ontologies by
		  creating rewritten modules. These modules, known as uniform
		  interpolants, retain only "relevant names" while preserving
		  their meanings in the absence of other names. UI holds
		  significant potential across various domains where tailored
		  ontology modules are required. However, realizing its full
		  potential demands highly optimized techniques for
		  generating such modules. Previous studies have identified
		  notable challenges in generating uniform interpolants for
		  EL-ontologies, where their computation is substantially
		  more complex and computationally demanding than standard
		  subset modules.Despite these obstacles, this paper
		  introduces an advanced "forgetting" method tailored for
		  computing uniform interpolants of ELIO-ontologies with
		  ABoxes. We show that with effective normalization and
		  inference strategies, these uniform interpolants can be
		  computed efficiently, matching the speed of standard module
		  computation. A comprehensive evaluation using a prototype
		  implementation of this method achieved a 100\% success rate
		  on two major benchmark datasets, Oxford-ISG and BioPortal,
		  with results delivered within seconds. The efficiency of
		  our approach is attributed to our novel linear strategy for
		  introducing definers, in sharp contrast to existing
		  strategies that lead to an exponential increase in definers
		  and computational inefficiency. Our method is unique in its
		  ability to create signature-restricted modules for
		  large-scale ontologies, making it a vital addition to the
		  community's toolkit.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2940–2949},
  numpages	= {10},
  keywords	= {description logics, forgetting, ontologies, uniform
		  interpolation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3477314.3507253,
  author	= {Amaral, Larissa Mangolim and Siqueira, F\'{a}bio Levy and
		  Brand\~{a}o, Anarosa Alves Franco},
  title		= {A survey on requirements notations in software engineering
		  research},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3507253},
  doi		= {10.1145/3477314.3507253},
  abstract	= {Requirements can be documented using different notations
		  that vary, for example, on expressivity, formality, and
		  visualization. Each of them have advantages and
		  disadvantages on different contexts. In fact, there is not
		  much information on which and how requirements notations
		  are being used, most of all when considering not only the
		  Software Engineering industry, but also its research
		  community. Therefore, this study investigates researchers'
		  applications for requirements notations. Furthermore, we
		  explore their reasoning behind the choice for a notation
		  and how formally these notations are defined. First we
		  reviewed the literature to identify general requirements
		  notations and their definition methods. Then, we conducted
		  a survey study with Software Engineering researchers,
		  asking them about the notations they have used. We received
		  251 usable responses from 24 countries. Our main findings
		  are that participants' preferred notations are Use Case,
		  Class Diagram, and User Stories; and they choose notations
		  based mainly on their suitability to the problem, their
		  adequacy to the stakeholders, and common usage.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1291–1298},
  numpages	= {8},
  keywords	= {metamodel, ontology, requirements language, requirements
		  notation, survey},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@InProceedings{	  10.5555/3427510.3427542,
  author	= {Neubauer, Kevin and Bucher, Harald and Haas, Benedikt and
		  Becker, J\"{u}rgen},
  title		= {Model-based development and simulative verification of
		  logical vehicle functions using executable UN/ECE
		  regulations},
  year		= {2020},
  isbn		= {9781713814290},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {On the way towards autonomous driving, a steadily
		  increasing number of Advanced Driver Assistance Systems
		  leads to a tremendous test effort to approve their safe
		  operation. However, route-based real-world tests cannot
		  cover this effort sufficiently which is why virtual testing
		  has become part of the vehicle development and approval
		  process as well. Since regulations such as those of the
		  United Nations Economic Commission for Europe, are
		  mandatory for vehicle approval there is a need to integrate
		  their prescribed test scenarios into virtual test
		  environments. In this paper, we present a novel approach to
		  transform these textually available scenarios into
		  executable state machines. It is complemented by a holistic
		  simulation-based verification where model-based vehicle
		  functions are stimulated by sensor data of the virtual
		  vehicle under test to achieve meaningful and more realistic
		  results. We prototyped a tool-chain to execute
		  approval-related test scenarios on the example of an
		  Advanced Emergency Braking System.},
  booktitle	= {Proceedings of the 2020 Summer Simulation Conference},
  articleno	= {31},
  numpages	= {12},
  keywords	= {E/E architecture, homologation, model-based, simulation,
		  verification},
  location	= {Virtual Event, Spain},
  series	= {SummerSim '20}
}

@InProceedings{	  10.1145/3418094.3418121,
  author	= {Li, Guoxuan},
  title		= {DeepFCA: Matching Biomedical Ontologies Using Formal
		  Concept Analysis Embedding Techniques},
  year		= {2020},
  isbn		= {9781450377768},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3418094.3418121},
  doi		= {10.1145/3418094.3418121},
  abstract	= {Biomedical ontologies contain target domain knowledge. In
		  many cases, multiple ontologies are created independently
		  for different purposes in the same biomedical domain. To
		  fuse and extend existing knowledge, we need to find the
		  corresponding entities (i.e. classes and properties) from
		  different ontologies. Formal Concept Analysis (FCA) is a
		  mature mathematical tool for biomedical ontology matching
		  tasks and has achieved competitive performance. The
		  FCA-based method mainly matches the ontologies through
		  lexical tokens and structural information. This method
		  ignores the inherent semantics of entities. On the other
		  hand, representation learning techniques are widely used in
		  different NLP tasks to capture the semantic similarity of
		  words. In this paper, we propose a novel biomedical
		  ontology matching method which we dub DeepFCA. We use
		  pre-trained word vectors to initialize the vector
		  representations onto which semantic information is
		  inscribed. FCA embedding techniques are used to refine
		  these vectors. DeepFCA combines FCA and word2vec methods to
		  enhance the performance of biomedical ontology matching. To
		  the best of our knowledge, this is the first attempt to
		  apply FCA embedding techniques to biomedical ontology
		  matching. Experiments on real-world biomedical ontologies
		  show that DeepFCA improves the recall and F1-measure
		  compared with the traditional FCA-based algorithm. It also
		  achieves competitive performance compared with several
		  state-of-the-art systems.},
  booktitle	= {Proceedings of the 4th International Conference on Medical
		  and Health Informatics},
  pages		= {259–265},
  numpages	= {7},
  keywords	= {Artificial intelligence, Biomedical ontology matching,
		  Formal concept analysis, Word embedding},
  location	= {Kamakura City, Japan},
  series	= {ICMHI '20}
}

@Article{	  10.1145/3502854,
  author	= {Shoaib, Umar and Fiaz, Laiba and Chakraborty, Chinmay and
		  Rauf, Hafiz Tayyab},
  title		= {Context-aware Urdu Information Retrieval System},
  year		= {2023},
  issue_date	= {March 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3502854},
  doi		= {10.1145/3502854},
  abstract	= {World Wide Web (WWW) is playing a vital role for sharing
		  dynamic knowledge in every field of life. The information
		  on web comprises a huge amount of data in different forms
		  such as structured, semi structured, or few is totally in
		  unstructured format. Due to huge size of information,
		  searching from larger textual data about the specific topic
		  or getting precise information is a challenging task. All
		  this leads to the problem of word sense ambiguity (WSA).
		  Urdu language-based information retrieval system using
		  different techniques related to Web Semantic Search Engine
		  architecture is proposed to efficiently retrieve the
		  relevant information and solve the problem of WSA. The
		  proposed system has average precision ratio 96\% as
		  compared to average precision ratio of 74\% and 75\%
		  average precision Google for single word query. For the
		  long text queries, our system outperforms the existing
		  famous search engines with 92\% accuracy such as Bing and
		  Google having 16.50\% and 16\% accuracy, respectively.
		  Similarly, the proposed system for single word query, the
		  recall ratio is 32.25\% as compared to 25\% and 25\% of
		  Bing and Google. The results of recall ratio for long text
		  query are improved as well, showing 6.38\% as compared to
		  6.20\% and 4.8\% of Bing and Google, respectively. The
		  results showed that the proposed system gives better and
		  efficient results as compared to the existing systems for
		  Urdu language.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {70},
  numpages	= {19},
  keywords	= {Urdu language, information retrieval, semantic web,
		  ontology, triplets, quad extraction, context-based, Web
		  Semantic Search Engine, WSA, searching and indexing,
		  keywords, corpus, Uniform Resource Identifier}
}

@InProceedings{	  10.1145/3674225.3674330,
  author	= {Xu, Jianing and Lou, Fei and Jiang, Ying and Chen, Bo and
		  Zhong, Zhenyuan},
  title		= {A Method for Constructing a Knowledge Graph of Electric
		  Power Digital Marketing Based on Artificial Intelligence
		  Deep Learning},
  year		= {2024},
  isbn		= {9798400716638},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3674225.3674330},
  doi		= {10.1145/3674225.3674330},
  abstract	= {With the rapid development of digitalization and
		  informatization in the power industry, power companies have
		  accumulated a large amount of data in various business
		  fields. This article focuses on the intelligent application
		  requirements in the field of electric power marketing, and
		  designs and constructs a knowledge graph of electric power
		  marketing business that includes domain background
		  knowledge. Firstly, utilize the relationships between the
		  basic business data tables organized by domain experts to
		  construct a conceptual ontology. Next, through operations
		  such as data cleaning, data filtering, and feature
		  selection, traverse the data tables of the business
		  database, use knowledge graph tools to obtain a knowledge
		  graph, and finally use the power marketing knowledge graph
		  to build an intelligent question answering application that
		  supports natural language question answering services in
		  the field of power marketing, better serving power users.
		  Experimental results have shown that the power marketing
		  knowledge graph constructed in this article, along with
		  intelligent question answering applications, can accurately
		  answer user questions and significantly improve user
		  satisfaction.},
  booktitle	= {Proceedings of the 2024 International Conference on Power
		  Electronics and Artificial Intelligence},
  pages		= {582–587},
  numpages	= {6},
  keywords	= {Electricity Marketing, Knowledge graph, Natural language
		  processing},
  location	= {Xiamen, China},
  series	= {PEAI '24}
}

@Article{	  10.1145/3579821,
  author	= {Andr\'{e}, \'{E}tienne and Liu, Shuang and Liu, Yang and
		  Choppy, Christine and Sun, Jun and Dong, Jin Song},
  title		= {Formalizing UML State Machines for Automated Verification
		  – A Survey},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {13s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3579821},
  doi		= {10.1145/3579821},
  abstract	= {The Unified Modeling Language (UML) is a standard for
		  modeling dynamic systems. UML behavioral state machines are
		  used for modeling the dynamic behavior of object-oriented
		  designs. The UML specification, maintained by the Object
		  Management Group (OMG), is documented in natural language
		  (in contrast to formal language). The inherent ambiguity of
		  natural languages may introduce inconsistencies in the
		  resulting state machine model. Formalizing UML state
		  machine specification aims at solving the ambiguity problem
		  and at providing a uniform view to software designers and
		  developers. Such a formalization also aims at providing a
		  foundation for automatic verification of UML state machine
		  models, which can help to find software design
		  vulnerabilities at an early stage and reduce the
		  development cost. We provide here a comprehensive survey of
		  existing work from&nbsp;1997 to&nbsp;2021 related to
		  formalizing UML state machine semantics for the purpose of
		  conducting model checking at the design stage.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {277},
  numpages	= {47},
  keywords	= {UML, semantics, formal specification, formal
		  verification}
}

@InProceedings{	  10.1145/3701551.3704125,
  author	= {Sakor, Ahmad and Brunet, Mauricio and Iglesias, Enrique
		  and Rivas, Ariam and Rohde, Philipp D. and Kraft, Angelina
		  and Vidal, Maria-Esther},
  title		= {Integrating Knowledge Graphs and Neuro-Symbolic AI: LDM
		  Enables FAIR and Federated Research Data Management},
  year		= {2025},
  isbn		= {9798400713293},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701551.3704125},
  doi		= {10.1145/3701551.3704125},
  abstract	= {Managing research digital objects (RDOs) in compliance
		  with FAIR principles is crucial for ensuring accessibility,
		  interoperability, and reusability across scientific
		  domains. The Leibniz Data Manager (LDM) is a
		  state-of-the-art framework that integrates Knowledge Graphs
		  (KGs) and Neuro-Symbolic AI, combining the reasoning power
		  of Large Language Models (LLMs) with structured metadata.
		  LDM supports the management and enhancement of RDOs through
		  entity linking, connecting datasets to external KGs like
		  Wikidata and the Open Research Knowledge Graph (ORKG).
		  Additionally, LDM offers federated query processing across
		  KGs, enabling users to explore related papers, datasets,
		  and resources through natural language questions. This demo
		  showcases LDM's capabilities to explore RDOs, compare
		  existing datasets, and extend metadata. By blending
		  Neuro-Symbolic AI with FAIR and federated research data
		  management, LDM offers a powerful tool for accelerating
		  data-driven discovery in science. LDM is publicly
		  accessible at https://service.tib.eu/ldmservice/.},
  booktitle	= {Proceedings of the Eighteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1044–1047},
  numpages	= {4},
  keywords	= {data science, digital repositories, federated search},
  location	= {Hannover, Germany},
  series	= {WSDM '25}
}

@Article{	  10.1145/3530257,
  author	= {Wu, Chuhan and Wu, Fangzhao and Huang, Yongfeng and Xie,
		  Xing},
  title		= {Personalized News Recommendation: Methods and Challenges},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3530257},
  doi		= {10.1145/3530257},
  abstract	= {Personalized news recommendation is important for users to
		  find interesting news information and alleviate information
		  overload. Although it has been extensively studied over
		  decades and has achieved notable success in improving user
		  experience, there are still many problems and challenges
		  that need to be further studied. To help researchers master
		  the advances in personalized news recommendation, in this
		  article, we present a comprehensive overview of
		  personalized news recommendation. Instead of following the
		  conventional taxonomy of news recommendation methods, in
		  this article, we propose a novel perspective to understand
		  personalized news recommendation based on its core problems
		  and the associated techniques and challenges. We first
		  review the techniques for tackling each core problem in a
		  personalized news recommender system and the challenges
		  they face. Next, we introduce the public datasets and
		  evaluation methods for personalized news recommendation. We
		  then discuss the key points on improving the responsibility
		  of personalized news recommender systems. Finally, we raise
		  several research directions that are worth investigating in
		  the future. This article can provide up-to-date and
		  comprehensive views on personalized news recommendation. We
		  hope this article can facilitate research on personalized
		  news recommendation as well as related fields in natural
		  language processing and data mining.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {24},
  numpages	= {50},
  keywords	= {News recommendation, personalization, survey, user
		  modeling, natural language processing}
}

@InProceedings{	  10.1145/3495018.3501225,
  author	= {Li, Shaoyi},
  title		= {Research on Financial Risk Control Model and Algorithm
		  Based on Machine Learning under the Background of Rural
		  Revitalization Strategy},
  year		= {2022},
  isbn		= {9781450385046},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3495018.3501225},
  doi		= {10.1145/3495018.3501225},
  abstract	= {The strategy of rural revitalization is of great
		  significance to the reconstruction of rural economic
		  growth, in which rural industries, represented by the
		  integration and development of rural industries, have
		  sprung up. The purpose of this paper is to use machine
		  learning (ML) technology to build an effective risk control
		  model, so as to help Internet finance enterprises better
		  control the loan risk. Sample data of Internet financial
		  platform borrowers are extracted from multiple dimensions,
		  and then the data are further processed, and the data used
		  to build the model is extracted by feature engineering.
		  Combined with the Gradient Boosting Decision Tree (GBDT)
		  algorithm in ML algorithm, the comprehensive evaluation is
		  carried out by using the basic information of bank
		  customers, flow records, user detection information and
		  user detection scale. The performance of the wind control
		  model is further improved by ML, which provides guidance
		  and reference for the performance improvement of the
		  model.},
  booktitle	= {2021 3rd International Conference on Artificial
		  Intelligence and Advanced Manufacture},
  pages		= {3010–3014},
  numpages	= {5},
  location	= {Manchester, United Kingdom},
  series	= {AIAM2021}
}

@InProceedings{	  10.1145/3663548.3675598,
  author	= {Nevsky, Alexandre and Bircanin, Filip and Cruice, Madeline
		  N and Wilson, Stephanie and Simperl, Elena and Neate,
		  Timothy},
  title		= {"I Wish You Could Make the Camera Stand Still":
		  Envisioning Media Accessibility Interventions with People
		  with Aphasia},
  year		= {2024},
  isbn		= {9798400706776},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3663548.3675598},
  doi		= {10.1145/3663548.3675598},
  abstract	= {Audiovisual media is integral to modern living, yet is not
		  always accessible to all. Modern accessibility
		  interventions, such as subtitles, support many, however,
		  communities with complex communication needs are largely
		  unconsidered. In this work, we envision future
		  accessibility interventions from the ground up with one
		  such community – people with aphasia. Over two workshops
		  and a probe activity, we problematise the space of
		  audiovisual consumption by people with aphasia, and
		  co-envision directions for development in accessible
		  audiovisual media. From low-fi diegetic prototypes to
		  mid-fidelity solutions, we explore new visions of
		  accessibility interventions for complex communication needs
		  – notably enabling high levels of content manipulation
		  and personalisation. Our findings raise open questions and
		  set directions for the research community in developing
		  accessibility interventions for audiovisual media to
		  support users with diverse needs in accessing audiovisual
		  content.},
  booktitle	= {Proceedings of the 26th International ACM SIGACCESS
		  Conference on Computers and Accessibility},
  articleno	= {46},
  numpages	= {17},
  keywords	= {Accessibility, aphasia, audiovisual, complex communication
		  needs, envisioning, media, probes, prototype},
  location	= {St. John's, NL, Canada},
  series	= {ASSETS '24}
}

@InProceedings{	  10.1145/3463274.3463344,
  author	= {Junaid, Waqas},
  title		= {Evaluating the Effectiveness of Problem Frames for
		  Contextual Modeling of Cyber-Physical Systems: a Tool Suite
		  with Adaptive User Interfaces},
  year		= {2021},
  isbn		= {9781450390538},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3463274.3463344},
  doi		= {10.1145/3463274.3463344},
  abstract	= {Bridging the gap between academic research and industrial
		  application is an important issue to promote Jackson's
		  Problem Frames approach (PF) to the software engineering
		  community. Various attempts have been made to tackle this
		  problem, such as defining formal semantics of PF for
		  software development, and providing a semi-formal approach
		  to model transformations of problem diagrams, with
		  automated tool support. In this paper, we propose to
		  exclusively focus on exploring and evaluating the
		  effectiveness of Jackson's problem diagrams for modeling
		  the context of cyber-physical systems, by developing a
		  suite of support tools enhanced with adaptive user
		  interfaces, and empirically and comprehensively assess its
		  usability. This paper introduces the state of the art,
		  corresponding research questions, research methodologies
		  and current progress of our research.},
  booktitle	= {Proceedings of the 25th International Conference on
		  Evaluation and Assessment in Software Engineering},
  pages		= {284–287},
  numpages	= {4},
  keywords	= {cyber-physical systems, context-based knowledge, adaptive
		  user interfaces, Problem Frames},
  location	= {Trondheim, Norway},
  series	= {EASE '21}
}

@InProceedings{	  10.1145/3185089.3185092,
  author	= {Gazzawe, Foziah and Lock, Russell and Dawson, Christian},
  title		= {Use of Ontology in Identifying Missing Artefact Links},
  year		= {2018},
  isbn		= {9781450354141},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3185089.3185092},
  doi		= {10.1145/3185089.3185092},
  abstract	= {The techniques of requirement traceability have evolved
		  over recent years. However, as much as they have
		  contributed to the software engineering field, significant
		  ambiguity remains in many software engineering processes.
		  This paper reports on an investigation of requirement
		  traceability artefacts, stakeholders, and SDLC development
		  models. Data were collected to gather evidence of artefacts
		  and their properties from previous studies. The aim was to
		  find the missing link between artefacts and their
		  relationship to one another, the stakeholders, and SDLC
		  models. This paper undertakes the first phase of the main
		  research project, which aims to develop a framework for
		  guiding software developers to actively manage
		  traceability. After inquiring into and examining previous
		  research on this topic, the links between artefacts and
		  their functions were identified. The analysis resulted in
		  the development of a new model for requirement
		  traceability, defined in the form of an ontology portraying
		  the contributively relations between software artefacts
		  using common properties with the aid of Prot\'{e}g\'{e}
		  Software. This study thus provides an important insight
		  into the future of the requirement artefacts relation, and
		  thereby lays an important foundation towards increasing our
		  understanding of their potential and limitations.},
  booktitle	= {Proceedings of the 2018 7th International Conference on
		  Software and Computer Applications},
  pages		= {6–9},
  numpages	= {4},
  keywords	= {Requirements Traceability, Requirement Artefacts, Mapping
		  the Requirement Artefacts, Artefacts Link},
  location	= {Kuantan, Malaysia},
  series	= {ICSCA '18}
}

@InProceedings{	  10.1145/3314493.3314504,
  author	= {Shangguan, Duansen and Chen, Liping and Ding, Jianwan},
  title		= {A Hierarchical Digital Twin Model Framework for Dynamic
		  Cyber-Physical System Design},
  year		= {2019},
  isbn		= {9781450360951},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3314493.3314504},
  doi		= {10.1145/3314493.3314504},
  abstract	= {Cyber-physical system (CPS) is a new trend in the complex
		  system related research works, where network connectivity
		  enhances computing power and systemic behavior emerges
		  through the competition, interaction, collaboration and
		  integration among individual interweaving, which consists
		  of real-time monitoring, data management, physical feedback
		  control. From this perspective, CPS is a dynamic entity
		  with rich functions. However, designers may encounter a
		  difficult situation, in which subsequent dynamic changes of
		  the system are discussed and appropriate functionalities
		  are added in the early design phase. Since the digital twin
		  is the digital duplicate of the physical entity, it can
		  dynamically evolve following the product life cycle. In
		  this paper, we propose a hierarchical digital twin model
		  framework for CPS design. In the light of digital twin
		  concept, the hierarchical high-level models facilitate
		  storage of information from the entire product life cycle.
		  Finally, an industrial robot application is presented to
		  demonstrate the efficacy of the model framework.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Mechatronics and Robotics Engineering},
  pages		= {123–129},
  numpages	= {7},
  keywords	= {Modeling&amp;Simulation, Industrial Robot, Digital Twin,
		  Complex System, CPS},
  location	= {Rome, Italy},
  series	= {ICMRE'19}
}

@InProceedings{	  10.5555/3291291.3291325,
  author	= {Bekele, Amente and Samuel, Joe and Nizami, Shermeen and
		  Basharat, Amna and Giffen, Randy and Green, James R.},
  title		= {Ontology driven temporal event annotator mHealth
		  application framework},
  year		= {2018},
  publisher	= {IBM Corp.},
  address	= {USA},
  abstract	= {We present an application (app) framework to facilitate
		  the collection of gold standard temporal event annotations.
		  These data will enable training and evaluation of machine
		  learning algorithms for predicting events of clinical
		  significance. Recording of such data using pen and paper
		  can prove to be tedious and error-prone due to the
		  variation in the types of events and the frequency of
		  occurrence. To address this problem, we developed an
		  mHealth application framework that presents an intuitive
		  and configurable user interface for annotating a timeline
		  with events.The presented Temporal Event Annotator (TEA)
		  app framework supports dynamically building a customized
		  application inclusive of events, event categories, and
		  study attributes based on the design input of a specific
		  study. This is accomplished by presenting a terminology
		  schema for the hierarchical definition of event types and
		  an additional user interface (UI) schema to support
		  UI-specific attributes.We describe the framework
		  architecture independent of specific technology
		  implementations. We also describe specific instantiations
		  of the framework that we used to develop and evaluate apps
		  for three different use cases: 1) patient monitoring in the
		  Neonatal Intensive Care Unit (NICU), 2) estimating patient
		  stress levels during immersive rehabilitation therapy, and
		  3) quantifying the patient experience during emergency
		  neonatal transport. The TEA framework provides a reliable
		  and intuitive solution for temporal event annotation that
		  accounts for the unique experimental requirements of each
		  study.},
  booktitle	= {Proceedings of the 28th Annual International Conference on
		  Computer Science and Software Engineering},
  pages		= {309–314},
  numpages	= {6},
  keywords	= {mobile applications, medical event annotations, life and
		  medical sciences, healthcare, data model, data entry and
		  integration},
  location	= {Markham, Ontario, Canada},
  series	= {CASCON '18}
}

@InProceedings{	  10.1145/3628454.3629551,
  author	= {Nimpattanavong, Chollakorn and Taveekitworachai, Pittawat
		  and Khan, Ibrahim and Nguyen, Thai Van and Thawonmas, Ruck
		  and Choensawat, Worawat and Sookhanaphibarn, Kingkarn},
  title		= {Am I Fighting Well? Fighting Game Commentary Generation
		  With ChatGPT},
  year		= {2023},
  isbn		= {9798400708497},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3628454.3629551},
  doi		= {10.1145/3628454.3629551},
  abstract	= {This paper presents a new approach for leveraging ChatGPT
		  in fighting game commentary generation task. Commentary
		  generation often relies on deep learning techniques, which
		  typically demand extensive data to achieve effectiveness.
		  Large language models (LLMs) have become essential due to
		  their remarkable ability to process data efficiently,
		  thanks to their extensive training on vast datasets. Our
		  proposed approach integrates the use of LLMs, specifically
		  the GPT-3.5 model, for generating commentaries through the
		  utilization of various prompts with data from the
		  open-source fighting game, DareFightingICE. Four prompt
		  variants are employed to assess the effectiveness of each
		  prompt components. Objective evaluation using natural
		  language metrics reveals that different prompt components
		  significantly affect the generated commentaries.
		  Additionally, subjective evaluation through a questionnaire
		  reveals that prompts without parameter definitions received
		  the highest preference from human evaluators. These results
		  suggest that LLMs exhibit versatility in generating
		  fighting game commentaries and hold promise for broader
		  applications.},
  booktitle	= {Proceedings of the 13th International Conference on
		  Advances in Information Technology},
  articleno	= {14},
  numpages	= {7},
  keywords	= {ChatGPT, Commentary Generation, DareFightingICE, Fighting
		  Game, Prompt Engineering},
  location	= {Bangkok, Thailand},
  series	= {IAIT '23}
}

@InProceedings{	  10.1145/3366030.3366110,
  author	= {Saad, Farag and Hackl-Sommer, Rene},
  title		= {Building a Semantic Model for Linking and Visualizing
		  Patent Citations (SeMViPaC)},
  year		= {2020},
  isbn		= {9781450371797},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366030.3366110},
  doi		= {10.1145/3366030.3366110},
  abstract	= {Patents are a high-quality resource of information that is
		  currently insufficiently leveraged. In times when
		  continuously rising prices for basic knowledge access
		  threaten to throttle academic research everywhere, this is
		  a resource that can not be longer neglected. Thus, in the
		  project we plan to extract and semantically describe
		  citations from patents. Furthermore, we will establish
		  linking and data integration between disparate data
		  sources, i.e. linking patents with resources in the LOD
		  (Linked Open Data) cloud. Based on the developed semantic
		  citation model a visualization tool to explore and gain
		  better insight and understanding of the extracted citation
		  data will be developed and made available to the public.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {645–648},
  numpages	= {4},
  keywords	= {Visualization, Semantic, Patents, Exploration, Citations},
  location	= {Munich, Germany},
  series	= {iiWAS2019}
}

@Article{	  10.1145/3183628.3183632,
  author	= {Kalra, Sumit and Prabhakar, T. V.},
  title		= {Ontology-based framework for internal-external quality
		  trade-offs and tenant management in multi-tenant
		  applications},
  year		= {2018},
  issue_date	= {December 2017},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {4},
  issn		= {1559-6915},
  url		= {https://doi.org/10.1145/3183628.3183632},
  doi		= {10.1145/3183628.3183632},
  abstract	= {Software Quality Attributes (QAs) can be categorized as
		  either internal to the system as experienced by the
		  developers or external to the system perceived by the end
		  users. These QA categories have trade-off among them - an
		  emphasis on internal QA may result in a compromise of an
		  external QA. For example, there is a trade-off between
		  maintainability and performance. Model-driven development
		  approaches manage this trade-off and increase the degree of
		  internal QA maintainability. In this work, we propose an
		  ontology-based communication mechanism among software
		  components to handle the trade-off. The approach increases
		  the degree of internal QAs such as modifiability,
		  maintainability, testability during the design and
		  development phases without compromising the external QAs
		  for the end users during the operation phase. We also
		  evaluate a prototype system to validate the proposed
		  approach using Software Architecture Analysis Method
		  (SAAM). It is also easier to integrate into the software
		  development lifecycle as compared to existing model-driven
		  approaches. The internal quality attributes become more
		  significant in a multi-tenant scenario than conventional
		  software. It requires managing dynamic requirements of
		  tenants continuously. The proposed approach also useful in
		  such scenario to reduce the maintenance overhead without
		  compromising the degree of multi-tenancy.},
  journal	= {SIGAPP Appl. Comput. Rev.},
  month		= jan,
  pages		= {46–58},
  numpages	= {13},
  keywords	= {software product quality attributes, quality attributes
		  trade-off, multi-tenant, internal quality attributes,
		  external quality attributes}
}

@InProceedings{	  10.1145/3385209.3385235,
  author	= {Kumar, Akshi and Sharma, Aditi and Nayyar, Anand},
  title		= {Fuzzy Logic based Hybrid Model for Automatic Extractive
		  Text Summarization},
  year		= {2020},
  isbn		= {9781450376594},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3385209.3385235},
  doi		= {10.1145/3385209.3385235},
  abstract	= {In the contemporary age of information, accessing data
		  becomes easy, but finding knowledge is very difficult. The
		  participation \&amp; publishing of information has
		  consequently escalated the suffering of 'Information Glut.'
		  Assisting users' informational searches with reduced
		  reading or surfing time by extracting and evaluating
		  accurate, authentic \&amp; relevant information are the
		  primary concerns in the present milieu. Automatic text
		  summarization condenses an original document into a shorter
		  form to create a smaller, compact version from the abundant
		  information that is available, preserving the content
		  \&amp; meaning such that it meets the needs of the user.
		  Though many summarization techniques have been proposed,
		  there are no 'silver bullets' to achieve the superlative
		  results as of human-generated summaries. Fuzzy Logic has
		  appeared as a robust theoretical framework for studying
		  human reasoning. A new hybrid model based on fuzzy logic
		  has been proposed using two graph-based techniques named
		  TextRank and LexRank and one semantic-based technique named
		  Latent semantic analysis (LSA). The techniques are
		  evaluated on the Opinosis dataset using 'ROUGE-1'
		  (Recall-Oriented Understudy for Gisting Evaluation-1) and
		  'time to extract the keywords.' The proposed technique has
		  outperformed the existing techniques when compared with the
		  results given by the original studies.},
  booktitle	= {Proceedings of the 2020 5th International Conference on
		  Intelligent Information Technology},
  pages		= {7–15},
  numpages	= {9},
  keywords	= {Automatic text summarization, Extractive Text
		  summarization, Fuzzy logic, Hybrid Model, LSA, LexRank,
		  TextRank},
  location	= {Hanoi, Viet Nam},
  series	= {ICIIT '20}
}

@InProceedings{	  10.1145/3460210.3493583,
  author	= {Mero\~{n}o-Pe\~{n}uela, Albert and Pernisch, Romana and
		  Gu\'{e}ret, Christophe and Schlobach, Stefan},
  title		= {Multi-domain and Explainable Prediction of Changes in Web
		  Vocabularies},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493583},
  doi		= {10.1145/3460210.3493583},
  abstract	= {Web vocabularies (WV) have become a fundamental tool for
		  structuring Web data: over 10 million sites use structured
		  data formats and ontologies to markup content. Maintaining
		  these vocabularies and keeping up with their changes are
		  manual tasks with very limited automated support, impacting
		  both publishers and users. Existing work shows that machine
		  learning can be used to reliably predict vocabulary
		  changes, but on specific domains (e.g. biomedicine) and
		  with limited explanations on the impact of changes (e.g.
		  their type, frequency, etc.). In this paper, we describe a
		  framework that uses various supervised learning models to
		  learn and predict changes in versioned vocabularies,
		  independent of their domain. Using well-established results
		  in ontology evolution we extract domain-agnostic and
		  human-interpretable features and explain their influence on
		  change predictability. Applying our method on 139 WV from 9
		  different domains, we find that ontology structural and
		  instance data, the number of versions, and the release
		  frequency highly correlate with predictability of change.
		  These results can pave the way towards integrating
		  predictive models into knowledge engineering practices and
		  methods.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {193–200},
  numpages	= {8},
  keywords	= {vocabulary change, ontology evolution, change modelling},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@InProceedings{	  10.1145/3314183.3323463,
  author	= {Barria-Pineda, Jordan and Akhuseyinoglu, Kamil and
		  Brusilovsky, Peter},
  title		= {Explaining Need-based Educational Recommendations Using
		  Interactive Open Learner Models},
  year		= {2019},
  isbn		= {9781450367110},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3314183.3323463},
  doi		= {10.1145/3314183.3323463},
  abstract	= {Students might pursue different goals throughout their
		  learning process. For example, they might be seeking new
		  material to expand their current level of knowledge,
		  repeating content of prior classes to prepare for an exam,
		  or working on addressing their most recent misconceptions.
		  Multiple potential goals require an adaptive e-learning
		  system to recommend learning content appropriate for
		  students' intent and to explain this recommendation in the
		  context of this goal. In our prior work, we explored
		  explainable recommendations for the most typical 'knowledge
		  expansion goal". In this paper, we focus on students'
		  immediate needs to remedy misunderstandings when they solve
		  programming problems. We generate learning content
		  recommendations to target the concepts with which students
		  have struggled more recently. At the same time, we produce
		  explanations for this recommendation goal in order to
		  support students' understanding of why certain learning
		  activities are recommended. The paper provides an overview
		  of the design of this explainable educational recommender
		  system and describes its ongoing evaluation},
  booktitle	= {Adjunct Publication of the 27th Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {273–277},
  numpages	= {5},
  keywords	= {open learner models, explanations, educational recommender
		  systems},
  location	= {Larnaca, Cyprus},
  series	= {UMAP'19 Adjunct}
}

@InProceedings{	  10.1145/3720554.3736184,
  author	= {Zhao, Rui and Wright, Jesse},
  title		= {Introduce an Auditing Layer to Web Science},
  year		= {2025},
  isbn		= {9798400715358},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3720554.3736184},
  doi		= {10.1145/3720554.3736184},
  abstract	= {Scientific discoveries increasingly depend on data and
		  data processing, and Web Science is no exception. As an
		  established practice, data-intensive research typically
		  uses scientific workflows and provenance to facilitate data
		  and method sharing while automatically preserving
		  processing history. Prior research has reported the
		  possibility of ex-post policy-based compliance checking
		  from provenance data. Based on these works, in this paper,
		  we present the conceptual design of a framework of
		  data-harvesting Web Science practices, especially by
		  introducing a common auditing layer. We discuss the
		  framework’s practical, scientific, and ethical
		  advantages, including its applicability in the period of
		  large language model (LLM), autonomous agent, and
		  artificial intelligence (AI) explosion. We hope this
		  framework design can incubate a new norm for research
		  practice to be transparent, ethical, and lightweight.},
  booktitle	= {Companion Publication of the 17th ACM Web Science
		  Conference 2025},
  pages		= {49–53},
  numpages	= {5},
  keywords	= {Data governance, transparency, audit, usage control},
  location	= { },
  series	= {Websci Companion '25}
}

@Article{	  10.1145/3359316,
  author	= {Pradhan, Alisha and Findlater, Leah and Lazar, Amanda},
  title		= {"Phantom Friend" or "Just a Box with Information":
		  Personification and Ontological Categorization of Smart
		  Speaker-based Voice Assistants by Older Adults},
  year		= {2019},
  issue_date	= {November 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {CSCW},
  url		= {https://doi.org/10.1145/3359316},
  doi		= {10.1145/3359316},
  abstract	= {As voice-based conversational agents such as Amazon Alexa
		  and Google Assistant move into our homes, researchers have
		  studied the corresponding privacy implications,
		  embeddedness in these complex social environments, and use
		  by specific user groups. Yet it is unknown how users
		  categorize these devices: are they thought of as just
		  another object, like a toaster? As a social companion?
		  Though past work hints to human-like attributes that are
		  ported onto these devices, the anthropomorphization of
		  voice assistants has not been studied in depth. Through a
		  study deploying Amazon Echo Dot Devices in the homes of
		  older adults, we provide a preliminary assessment of how
		  individuals 1) perceive having social interactions with the
		  voice agent, and 2) ontologically categorize the voice
		  assistants. Our discussion contributes to an understanding
		  of how well-developed theories of anthropomorphism apply to
		  voice assistants, such as how the socioemotional context of
		  the user (e.g., loneliness) drives increased
		  anthropomorphism. We conclude with recommendations for
		  designing voice assistants with the ontological category in
		  mind, as well as implications for the design of
		  technologies for social companionship for older adults.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= nov,
  articleno	= {214},
  numpages	= {21},
  keywords	= {voice assistants, smart speakers, personification,
		  ontology, older adults, anthropomorphism}
}

@InProceedings{	  10.1145/3401335.3401344,
  author	= {Pouri, Maria J. and Hilty, Lorenz M.},
  title		= {The Relevance of Digital Sharing Business Models for
		  Sustainability},
  year		= {2020},
  isbn		= {9781450375955},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3401335.3401344},
  doi		= {10.1145/3401335.3401344},
  abstract	= {There is a growing discussion about the "Digital Sharing
		  Economy" (DSE). The pervasiveness of digital platforms and
		  the growing interest in a sharing (rather than ownership)
		  style of consumption have allowed for sharing practices to
		  scale up and become a widespread phenomenon. Digital
		  sharing platforms offer a wide variety of services which
		  appear to be more affordable, efficient, and accessible
		  than their conventional counterparts, making them more
		  attractive in the eyes of consumers. The DSE has manifested
		  itself most remarkably in consumer-to-consumer (C2C) and
		  business-to-consumer (B2C) sharing models. New business
		  models have been created to capture and offer the values
		  driving the emerging sharing trend.The innovative,
		  digitally enabled mode of providing access to resources as
		  a service in the DSE has changed consumption patterns both
		  at micro level, as a change in individual lifestyles, and
		  at macro level, manifested in a transformation of
		  socio-economic structures. These ongoing changes may have
		  both positive and negative implications for society from a
		  sustainability perspective. Recognising that the (potential
		  and actual) impacts of sharing platforms on sustainability
		  have not been studied in a systematic way yet, the present
		  paper aims to develop a systematic insight into this
		  interaction by focusing on the business models emerging
		  around sharing platforms as a central starting point. To
		  achieve this, we use a typology of business models that
		  recognizes the affordances and key attributes of sharing in
		  the DSE. The typology covers both C2C and B2C models of
		  sharing. Based on this typology, we discuss the
		  implications of each type of sharing model for
		  sustainability by asking two central questions: How may the
		  given type of sharing affect resource consumption? And what
		  will be the potential impacts on social practices and
		  structures? We hope that the present study can serve as a
		  guideline for assessing the sustainability impacts of
		  sharing platforms -- either already operating in the market
		  or envisaged. By highlighting the aspects most relevant
		  from a sustainability point of view, we expect to
		  contribute to an evolution of the DSE business models
		  towards sustainable development.},
  booktitle	= {Proceedings of the 7th International Conference on ICT for
		  Sustainability},
  pages		= {77–87},
  numpages	= {11},
  keywords	= {Sustainability impacts, Socio-economic structures, Sharing
		  platforms, Sharing business models, Resource consumption,
		  Information and communication technology (ICT), Digital
		  sharing economy},
  location	= {Bristol, United Kingdom},
  series	= {ICT4S2020}
}

@Article{	  10.1145/3449213,
  author	= {Mishra, Swati and Rzeszotarski, Jeffrey M.},
  title		= {Crowdsourcing and Evaluating Concept-driven Explanations
		  of Machine Learning Models},
  year		= {2021},
  issue_date	= {April 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {CSCW1},
  url		= {https://doi.org/10.1145/3449213},
  doi		= {10.1145/3449213},
  abstract	= {An important challenge in building explainable
		  artificially intelligent (AI) systems is designing
		  interpretable explanations. AI models often use low-level
		  data features which may be hard for humans to interpret.
		  Recent research suggests that situating machine decisions
		  in abstract, human understandable concepts can help.
		  However, it is challenging to determine the right level of
		  conceptual mapping. In this research, we explore
		  granularity (of data features) and context (of data
		  instances) as dimensions underpinning conceptual mappings.
		  Based on these measures, we explore strategies for
		  designing explanations in classification models. We
		  introduce an end-to-end concept elicitation pipeline that
		  supports gathering high-level concepts for a given data
		  set. Through crowd-sourced experiments, we examine how
		  providing conceptual information shapes the effectiveness
		  of explanations, finding that a balance between coarse and
		  fine-grained explanations help users better estimate model
		  predictions. We organize our findings into systematic
		  themes that can inform design considerations for future
		  systems.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= apr,
  articleno	= {139},
  numpages	= {26},
  keywords	= {classification, concepts, explanations, machine learning}
}

@Article{	  10.1145/3626307.3626310,
  author	= {Guittoum, Amal and A\"{\i}ssaoui, Fran\c{c}ois and Bolle,
		  S\'{e}bastien and Boyer, Fabienne and De Palma, Noel},
  title		= {Leveraging Semantic Technologies for Collaborative
		  Inference of Threatening IoT Dependencies},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {3},
  issn		= {1559-6915},
  url		= {https://doi.org/10.1145/3626307.3626310},
  doi		= {10.1145/3626307.3626310},
  abstract	= {IoT Device Management (DM) refers to the remote
		  administration of customer devices. In practice, DM is
		  ensured by multiple actors such as operators or device
		  manufacturers, each operating independently via their DM
		  solution. These siloed DM solutions are limited in
		  addressing IoT threats related to device dependencies, such
		  as cascading failures, as these threats spread across
		  devices managed by different DM actors, and their
		  mitigation can no longer be performed without collaborative
		  DM efforts. The first step toward collaborative mitigation
		  of these threats is the identification of threatening
		  dependency topology. However, this task is challenging,
		  requiring the inference of dependencies from the data held
		  by different actors. In this work, we propose a
		  collaborative framework that infers the threatening
		  topology of dependencies by accessing and aggregating data
		  from legacy DM solutions. It combines the assets of
		  Semantic Web standards and Digital Twin technology to
		  capture on-demand the topology of dependencies, and it is
		  designed to be used in business applications such as
		  customer care to enhance customer Quality of Experience. We
		  integrate our solution within the in-use Orange's Digital
		  Twin platform Thing in the future and demonstrate its
		  effectiveness by automatically inferring threatening
		  dependencies in the two settings: a simulated smart home
		  scenario managed by ground-truth DM solutions, such as
		  Orange's implementation of the USP Controller and Samsung's
		  SmartThings Platform, and a realistic smart home called
		  DOMUS testbed.},
  journal	= {SIGAPP Appl. Comput. Rev.},
  month		= sep,
  pages		= {32–48},
  numpages	= {17},
  keywords	= {thing description, semantic web, ontology, inference,
		  entity resolution, digital twin, dependencies management,
		  collaboration, SHACL, IoT device management}
}

@InProceedings{	  10.1145/3274192.3274213,
  author	= {Queiroz, Randerson and Marques, Anna Beatriz and Lopes,
		  Adriana and Oliveira, Edson and Conte, Tayana},
  title		= {Evaluating Usability of IFML Models: How Usability is
		  Perceived and Propagated},
  year		= {2018},
  isbn		= {9781450366014},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3274192.3274213},
  doi		= {10.1145/3274192.3274213},
  abstract	= {System acceptance is strongly related to its usability. It
		  is important that the usability is carefully designed
		  during the system design steps and later propagated to the
		  interface. This care avoids reworking the interface design
		  of an application in the future because of its usability.
		  However, we did not find works that specifically addresses
		  the modeling of interfaces in conjunction with usability.
		  The Interaction Flow Modeling Language (IFML) is a proposal
		  that supports the modeling of the interface. This work
		  investigates the following research question: "usability in
		  IFML models is perceived and propagated to the final
		  interface?" In order to answer this research question, we
		  performed an empirical study to evaluate how usability is
		  perceived by participants through IFML models and if
		  usability is propagated to an interface prototype. The
		  study showed that not all aspects of usability are easily
		  perceived and propagated in the interface through IFML
		  models.},
  booktitle	= {Proceedings of the 17th Brazilian Symposium on Human
		  Factors in Computing Systems},
  articleno	= {21},
  numpages	= {10},
  keywords	= {User Interface, Usability, Interface Model, IFML, FUFs,
		  Empirical studies},
  location	= {Bel\'{e}m, Brazil},
  series	= {IHC '18}
}

@Article{	  10.1145/3280985,
  author	= {Lara, Juan De and Guerra, Esther},
  title		= {Refactoring Multi-Level Models},
  year		= {2018},
  issue_date	= {October 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {27},
  number	= {4},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3280985},
  doi		= {10.1145/3280985},
  abstract	= {Multi-level modelling promotes flexibility in modelling by
		  enabling the use of several meta-levels instead of just
		  two, as is the case in mainstream two-level modelling
		  approaches. While this approach leads to simpler models for
		  some scenarios, it introduces an additional degree of
		  freedom as designers can decide the meta-level where an
		  element should reside, having to ascertain the suitability
		  of such decisions.In this respect, model refactorings have
		  been successfully applied in the context of two-level
		  modelling to rearrange the elements of a model while
		  preserving its meaning. Following this idea, we propose a
		  catalogue of 17 novel refactorings specific to multi-level
		  models. Their objective is to help designers in rearranging
		  elements across and within meta-levels and exploring the
		  consequences. In this article, we detail each refactoring
		  in the catalogue, show a classification across different
		  dimensions, and describe the support we provide in our
		  MetaDepth tool. We present two experiments to assess two
		  aspects of our refactorings. The first one validates the
		  predicted semantic side effects of the refactorings on the
		  basis of more than 210.000 refactoring applications. The
		  second one measures the impact of refactorings on three
		  quality attributes of multi-level models.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= nov,
  articleno	= {17},
  numpages	= {56},
  keywords	= {multi-level modelling, model refactoring, MetaDepth,
		  Meta-modelling}
}

@InProceedings{	  10.1145/3469968.3469998,
  author	= {Liu, Jiahao and Shen, Yifan and Zhang, Yijie and
		  krishnamoorthy, Sujatha},
  title		= {Resume Parsing based on Multi-label Classification using
		  Neural Network models},
  year		= {2021},
  isbn		= {9781450389808},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3469968.3469998},
  doi		= {10.1145/3469968.3469998},
  abstract	= {Application for jobs usually brings much work for both
		  appliers and HR. Appliers want to apply for the jobs which
		  they are most suitable. The number of applications for a
		  particular position can be significant, making the
		  candidates’ selection cumbersome for HR. Nowadays, hiring
		  processes are often conducted through the Virtual mode with
		  emails. This creates chances for analyzing the data in the
		  resume. Therefore, to enhance selection problems’
		  efficiency, resume parsing algorithms have been developed
		  in recent years to predict resume-based skills or good jobs
		  quickly. The artificial neural network is a hot spot in the
		  field of artificial intelligence since the 1980s. It
		  abstracts the human brain's neural network from the angle
		  of information processing, establishes some simple models,
		  and forms different networks according to different
		  connection modes. In recent years, neural networks-based
		  algorithms perform high efficiency in processing text
		  classification. This paper put forward some of the
		  efficient algorithms used in text classification, Like
		  BPNN, CNN, BiLSTM, and CRNN, for resume parsing. The
		  original resumes are parsed by splitting them into words,
		  and word base is trained to get the most appropriate word,
		  which has a high score in the resume is resulting suitable
		  job for each resume. The CRNN performs best in resume
		  parsing, which the accuracy can reach 96\%. CNN places the
		  lowest accuracy. The BPNN achieves good accuracy but brings
		  inflexible.},
  booktitle	= {Proceedings of the 6th International Conference on Big
		  Data and Computing},
  pages		= {177–185},
  numpages	= {9},
  keywords	= {Resume Parsing, Neural Network, Bi-LSTM, BPNN},
  location	= {Shenzhen, China},
  series	= {ICBDC '21}
}

@InProceedings{	  10.1145/3391274.3393640,
  author	= {Edgar, Vatricia and La Place, Cecilia and Schmidt, Julia
		  and Bansal, Ajay and Bansal, Srividya},
  title		= {SustainOnt: an ontology for defining an index of
		  neighborhood sustainability across domains},
  year		= {2020},
  isbn		= {9781450379748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3391274.3393640},
  doi		= {10.1145/3391274.3393640},
  abstract	= {Massive amounts of data, both structured and unstructured,
		  are available to be harvested for competitive business
		  advantage, sound government policies, and new insights in a
		  broad array of applications. This paper specifically
		  focuses on extraction, integration, and querying of open
		  data available about environmental sustainability. The
		  global trend toward urbanization has created a need for
		  residents of urban neighborhoods to better understand the
		  factors impacting the social, environmental, and economic
		  sustainability of an area. To date, there is no concise
		  representation of all aspects of sustainability. This paper
		  aims to fill this gap. A version of sustainability resting
		  on economic, societal, and environmental development as the
		  three main indicators was chosen to inform an ontology
		  called SustainOnt used to organize and analyze relevant
		  data from various sources. The newly-linked data is made
		  available through a dual-platform application aimed at
		  reaching a wide array of audiences. An initial prototype
		  has been designed, using data for a small region, to
		  provide a sustainability index of each city and/or
		  neighborhood area that can be more accessible to people
		  without the means to directly analyze the available data.},
  booktitle	= {Proceedings of The International Workshop on Semantic Big
		  Data},
  articleno	= {9},
  numpages	= {6},
  keywords	= {sustainability, ontology, linked open data, data
		  integration},
  location	= {Portland, Oregon},
  series	= {SBD '20}
}

@InProceedings{	  10.1145/3243907.3243913,
  author	= {Rashid, Sabbir M. and De Roure, David and McGuinness,
		  Deborah L.},
  title		= {A Music Theory Ontology},
  year		= {2018},
  isbn		= {9781450364959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3243907.3243913},
  doi		= {10.1145/3243907.3243913},
  abstract	= {Many existing music ontologies have focused on expressing
		  metadata related to performances or recordings, aiding with
		  recommendations of songs or artists, and studying the
		  psychological affects of music. These music ontologies
		  provide a foundation for describing many practical aspects
		  related to music. We believe further primitives are needed
		  in order to represent written music and provide a
		  foundation for performing analysis of music. We are
		  motivated by questions related to analyzing music that
		  might inform composers or musicians. Informational elements
		  may include possible underlying chords from a set of notes,
		  as well as summaries of key signatures or scales used in a
		  given song. In order to leverage Semantic Web technologies
		  to answer such questions, we present our Music Theory
		  Ontology that expands on existing work by including
		  theoretical concepts that were absent from previous music
		  ontologies. We further describe a methodology for using the
		  ontology to infer new knowledge. We demonstrate this
		  capability by inferring the notes in various scales and
		  chords, and evaluate the ontology in terms of competency
		  question answering.},
  booktitle	= {Proceedings of the 1st International Workshop on Semantic
		  Applications for Audio and Music},
  pages		= {6–14},
  numpages	= {9},
  location	= {Monterey, CA, USA},
  series	= {SAAM '18}
}

@InProceedings{	  10.1145/3624062.3624094,
  author	= {Kousha, Pouya and Sathu, Vivekananda and Lieber, Matthew
		  and Subramoni, Hari and Panda, Dhabaleswar K.},
  title		= {Democratizing HPC Access and Use with Knowledge Graphs},
  year		= {2023},
  isbn		= {9798400707858},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3624062.3624094},
  doi		= {10.1145/3624062.3624094},
  abstract	= {The field of High-Performance Computing (HPC) is
		  undergoing rapid evolution, with an expanding and diverse
		  user base harnessing its unparalleled computational
		  capabilities. As the range of HPC applications grows,
		  newcomers to the field are faced with the daunting task of
		  optimizing their applications for efficient execution on
		  HPC systems. Traditional documentation, often spanning
		  dozens of pages, is cumbersome for finding answers and
		  ill-suited for integration with emerging conversational
		  AI-powered user interfaces like chatbots. Addressing this
		  challenge, we propose a novel HPC ontology crafted to
		  encapsulate HPC runtime relations in a scalable fashion.
		  Our proposed ontology not only facilitates the transfer and
		  querying of this knowledge but also serves as a
		  foundational pillar for our AI-powered Speech Assistant
		  Interface (SAI)[13]. This ensures reproducibility,
		  reliability, and optimal performance when executing tasks.
		  In this paper, we elucidate the relationships and
		  properties underpinning our ontology and showcase how users
		  can interact with knowledge graphs based on our proposed
		  ontology to derive insights.},
  booktitle	= {Proceedings of the SC '23 Workshops of the International
		  Conference on High Performance Computing, Network, Storage,
		  and Analysis},
  pages		= {243–251},
  numpages	= {9},
  keywords	= {Documentation, HPC, Knowledge Graph, Ontology},
  location	= {Denver, CO, USA},
  series	= {SC-W '23}
}

@InProceedings{	  10.1145/3529372.3530915,
  author	= {Tudhope, Douglas and Gnoli, Claudio and Golub, Koraljka
		  and Mayr, Philipp},
  title		= {20th European NKOS workshop: networked knowledge
		  organization systems and services},
  year		= {2022},
  isbn		= {9781450393454},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3529372.3530915},
  doi		= {10.1145/3529372.3530915},
  abstract	= {The workshop will explore the potential of Knowledge
		  Organization Systems (KOS), such as classification systems,
		  taxonomies, thesauri, ontologies, and lexical databases, in
		  the context of current developments and possibilities.
		  These tools help model the underlying semantic structure of
		  a domain for purposes of information retrieval, knowledge
		  discovery, language engineering, etc. The workshop provides
		  an opportunity to discuss projects, research and
		  development activities, evaluation approaches, lessons
		  learned, and research findings. The main theme of the
		  workshop is Designing for Cultural Hospitality and
		  Indigenous Knowledge in KOS.},
  booktitle	= {Proceedings of the 22nd ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {55},
  numpages	= {2},
  keywords	= {vocabulary mapping, thesauri, terminology services,
		  taxonomies, ontologies, knowledge organization systems,
		  classification systems},
  location	= {Cologne, Germany},
  series	= {JCDL '22}
}

@InBook{	  10.5555/3716662.3716790,
  author	= {Varshney, Kush R.},
  title		= {Decolonial AI Alignment: Openness,
		  Vi\'{s}eundefineda-Dharma, and Including Excluded
		  Knowledges},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {Prior work has explicated the coloniality of artificial
		  intelligence (AI) development and deployment through
		  mechanisms such as extractivism, automation, sociological
		  essentialism, surveillance, and containment. However, that
		  work has not engaged much with alignment: teaching
		  behaviors to a large language model (LLM) in line with
		  desired values, and has not considered a mechanism that
		  arises within that process: moral absolutism---a part of
		  the coloniality of knowledge. Colonialism has a history of
		  altering the beliefs and values of colonized peoples; in
		  this paper, I argue that this history is recapitulated in
		  current LLM alignment practices and technologies.
		  Furthermore, I suggest that AI alignment be decolonialized
		  using three forms of openness: openness of models, openness
		  to society, and openness to excluded knowledges. This
		  suggested approach to decolonial AI alignment uses ideas
		  from the argumentative moral philosophical tradition of
		  Hinduism, which has been described as an open-source
		  religion. One concept used is vi\'{s}eundefineda-dharma, or
		  particular context-specific notions of right and wrong. At
		  the end of the paper, I provide a suggested reference
		  architecture to work toward the proposed framework.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {1467–1481},
  numpages	= {15}
}

@Proceedings{	  10.1145/3696410,
  title		= {WWW '25: Proceedings of the ACM on Web Conference 2025},
  year		= {2025},
  isbn		= {9798400712746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The 2025 ACM Web Conference (WWW '25) took place from
		  April 28 to May 2, 2025, in the Sydney Convention \&amp;
		  Exhibition Centre, Australia. Its logo, featuring the
		  Sydney Harbour Bridge, symbolizes the core "connecting"
		  function of the Web. Formerly known as the International
		  World Wide Web Conference (WWW), this event originated at
		  CERN in 1994 and has long served as the premier venue for
		  presenting and discussing research, development, standards,
		  and applications related to the Web.The 2025 ACM Web
		  Conference (WWW'25) took place from April 28 to May 2,
		  2025, in the Sydney Convention \&amp; Exhibition Centre,
		  Australia. Its logo, featuring the Sydney Harbour Bridge,
		  symbolizes the core "connecting" function of the Web.
		  Formerly known as the International World Wide Web
		  Conference (WWW), this event originated at CERN in 1994 and
		  has long served as the premier venue for presenting and
		  discussing research, development, standards, and
		  applications related to the Web.Between 2024 and 2025,
		  large language models (LLMs) significantly impacted nearly
		  every industry and many aspects of daily life, prompting
		  transformations in the Web's ecosystem. Acknowledging the
		  importance of LLMs in advancing Web technologies, the call
		  for papers (CFP) across ten research tracks was slightly
		  modified. Three program chairs - Liane Lewin-Eytan, Helen
		  Huang, and Elad Yom-Tov - led the program committee, which
		  used OpenReview to evaluate and accept the research track
		  papers.},
  location	= {Sydney NSW, Australia}
}

@InProceedings{	  10.1145/3454127.3456595,
  author	= {Benabdellah, Abla Chaouni and Benghabrit, Asmaa and
		  Bouhaddou, Imane and Zekhnini, Kamar},
  title		= {An agent organizational method for modeling the complexity
		  of the design process},
  year		= {2021},
  isbn		= {9781450388719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3454127.3456595},
  doi		= {10.1145/3454127.3456595},
  abstract	= {The management of the design process is a challenging
		  mission; and most researchers would argue that design is
		  linked to intentional action and it cannot emerge out of
		  complexity. In fact, the interactions between processes,
		  operators, and activities define an unexpected emergent
		  behavior, which is based on complex assumptions such as
		  non-linearity, dynamic and adaptive firm behavior.
		  Therefore, we need a complex thinking. This article
		  proposes to explore how we may deepen our understanding of
		  design process as a complex adaptive system. In fact, this
		  new understanding creates a quite challenge for researches
		  to develop appropriate tools to support design reasoning
		  and decision-making. In this respect, the aim of this paper
		  is first to define the complexity of design process as a
		  complexity of system, by matching its characteristics with
		  those of complex adaptive systems (CAS). Second, the paper
		  provides an agent organizational modelization of the design
		  process in order to support its complexity by following the
		  ASPECS methodology which is an agent-oriented software
		  process for engineering complex systems as well as the
		  knowledge identification of the design process using the
		  RIOCK meta-model.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Networking, Information Systems \&amp; Security},
  articleno	= {20},
  numpages	= {7},
  location	= {KENITRA, AA, Morocco},
  series	= {NISS '21}
}

@Article{	  10.1145/3622933,
  author	= {Jia, Qi and Liu, Yizhu and Ren, Siyu and Zhu, Kenny Q.},
  title		= {Taxonomy of Abstractive Dialogue Summarization: Scenarios,
		  Approaches, and Future Directions},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3622933},
  doi		= {10.1145/3622933},
  abstract	= {Abstractive dialogue summarization generates a concise and
		  fluent summary covering the salient information in a
		  dialogue among two or more interlocutors. It has attracted
		  significant attention in recent years based on the massive
		  emergence of social communication platforms and an urgent
		  requirement for efficient dialogue information
		  understanding and digestion. Different from news or
		  articles in traditional document summarization, dialogues
		  bring unique characteristics and additional challenges,
		  including different language styles and formats, scattered
		  information, flexible discourse structures, and unclear
		  topic boundaries. This survey provides a comprehensive
		  investigation of existing work for abstractive dialogue
		  summarization from scenarios, approaches to evaluations. It
		  categorizes the task into two broad categories according to
		  the type of input dialogues, i.e., open-domain and
		  task-oriented, and presents a taxonomy of existing
		  techniques in three directions, namely, injecting dialogue
		  features, designing auxiliary training tasks, and using
		  additional data. A list of datasets under different
		  scenarios and widely accepted evaluation metrics are
		  summarized for completeness. After that, the trends of
		  scenarios and techniques are summarized, together with deep
		  insights into correlations between extensively exploited
		  features and different scenarios. Based on these analyses,
		  we recommend future directions, including more controlled
		  and complicated scenarios, technical innovations and
		  comparisons, publicly available datasets in special
		  domains, and so on.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {67},
  numpages	= {38},
  keywords	= {abstractive summarization, dialogue context modeling,
		  Dialogue summarization}
}

@InProceedings{	  10.1145/3677779.3677821,
  author	= {Zhang, Shuai and Guan, Yanzhi and Gu, Zhongyu},
  title		= {Research on named entity recognition in the field of CNC
		  machine tool design based on deep learningKnowledge map of
		  mechanical field},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677779.3677821},
  doi		= {10.1145/3677779.3677821},
  abstract	= {Our goal is to extract entities from the text data of
		  unstructured CNC machine tool design for the construction
		  of knowledge graph. The key entity extraction problem in
		  the construction of CNC machine tool design knowledge graph
		  is studied. In order to realize the recognition of named
		  entities, we have formulated the standard and labeling
		  method of knowledge classification for the field of CNC
		  machine tools, and constructed the corresponding domain
		  data set. In addition, we also propose an entity
		  recognition technology based on RoBertTa-BiLSTM-LCRF for
		  CNC machine tool design text. Firstly, we fine-tune the
		  RoBertTa-BiLSTM-LCRF model using data sets in the field of
		  CNC machine tools, and then use RoBERTa to encode the text
		  to generate a vector representation ; next, we use
		  bidirectional long short-term memory ( BiLSTM ) to extract
		  the features of vectors. Finally, we introduce LCRF as the
		  overall optimization layer of the label, so as to derive
		  the best answer and label the entity.The experimental
		  results show that the F1 value of the model in the data set
		  reaches 71.16 \% ; for most of the key entities, the
		  value of F1 exceeds 65 \% ; this method shows significant
		  advantages in the entity recognition of CNC machine tool
		  design knowledge. It can accurately identify the core
		  entities in the machine tool design knowledge document, and
		  provides a solid data support for the construction of CNC
		  machine tool design knowledge graph.},
  booktitle	= {Proceedings of the International Conference on Modeling,
		  Natural Language Processing and Machine Learning},
  pages		= {257–262},
  numpages	= {6},
  location	= {Xi'an, China},
  series	= {CMNM '24}
}

@InProceedings{	  10.1145/3313831.3376793,
  author	= {Das Swain, Vedant and Saha, Koustuv and Reddy, Manikanta
		  D. and Rajvanshy, Hemang and Abowd, Gregory D. and De
		  Choudhury, Munmun},
  title		= {Modeling Organizational Culture with Workplace Experiences
		  Shared on Glassdoor},
  year		= {2020},
  isbn		= {9781450367080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3313831.3376793},
  doi		= {10.1145/3313831.3376793},
  abstract	= {Organizational culture (OC) encompasses the underlying
		  beliefs, values, and practices that are unique to an
		  organization. However, OC is inherently subjective and a
		  coarse construct, and therefore challenging to quantify.
		  Alternatively, self-initiated workplace reviews on online
		  platforms like Glassdoor provide the opportunity to
		  leverage the richness of language to understand OC. In as
		  much, first, we use multiple job descriptors to
		  operationalize OC as a word vector representation. We
		  validate this construct with language used in 650k
		  different Glassdoor reviews. Next, we propose a methodology
		  to apply our construct on Glassdoor reviews to quantify the
		  OC of employees by sector. We validate our measure of OC on
		  a dataset of 341 employees by providing empirical evidence
		  that it helps explain job performance. We discuss the
		  implications of our work in guiding tailored interventions
		  and designing tools for improving employee functioning.},
  booktitle	= {Proceedings of the 2020 CHI Conference on Human Factors in
		  Computing Systems},
  pages		= {1–15},
  numpages	= {15},
  keywords	= {glassdoor, organizational culture, social media,
		  wordvector},
  location	= {Honolulu, HI, USA},
  series	= {CHI '20}
}

@InProceedings{	  10.1145/3652620.3686250,
  author	= {Cederbladh, Johan and Eisenberg, Martin and Berardinelli,
		  Luca and Bilic, Damir},
  title		= {Automation Support for System Simulation and Architecture
		  Layout Design in Cyber-Physical Systems Engineering},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3686250},
  doi		= {10.1145/3652620.3686250},
  abstract	= {Simulations have long been part of hardware-centric system
		  domains. Similarly, architecture design is a common
		  practice for complex industrial systems, which comprise
		  many components that can be arranged in different layouts
		  according to given requirements. Configuring simulation
		  models and choosing the architecture design can be
		  time-consuming activities. This paper presents a
		  model-driven approach to automate the simulation
		  configuration and architecture layouting engineering
		  activities by leveraging model-driven optimisation
		  techniques. The approach leverages a research solution,
		  MOMoT (Marrying Optimisation and Model Transformations), an
		  academic tool that combines search-based algorithms and
		  model transformations. MOMoT is extended with two software
		  modules, leveraging the Functional Mock-up Interface
		  standard for simulation configuration and an architectural
		  description language to design architecture layouts. Our
		  solution is presented in the context of Volvo Construction
		  Equipment's industrial use case, which is part of the
		  European-funded project AIDOaRt. Our approach contributes
		  to automated decision support to simulation and
		  architecture design through model-driven optimisation while
		  preserving the organisation's engineering practices.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {299–310},
  numpages	= {12},
  keywords	= {simulation, models, optimisation, architecture, layout},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Proceedings{	  10.1145/3664647,
  title		= {MM '24: Proceedings of the 32nd ACM International
		  Conference on Multimedia},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {We are delighted to welcome you to Melbourne, Australia
		  for ACM Multimedia 2024, the 32nd ACM International
		  Conference on Multimedia. ACM Multimedia is the premier
		  international conference series in the area of multimedia
		  within the field of computer science. Since 1993, ACM
		  Multimedia has been bringing together worldwide researchers
		  and practitioners from academia and industry to present
		  their innovative research and to discuss recent
		  advancements in multimedia.For the first time since the end
		  of the COVID-19 pandemic, this year's conference returns to
		  the Asia-Pacific region and resumes as a full-fledged,
		  inperson event. With no travel restrictions or significant
		  visa challenges, we are excited to once again experience
		  the warmth of face-to-face gatherings, where we can
		  reconnect with colleagues and friends.The enthusiasm and
		  support from the community have been incredible. ACM
		  Multimedia 2024 received over 4,300 main conference
		  submissions, accepting more than 1,100 papers (please refer
		  to the TPC Chairs' message for details). In addition, 10
		  Grand Challenges were selected from 22 submissions, 18
		  workshops from 30 submissions, and 8 tutorials from 13
		  proposals. We've prepared an exciting five-day program:
		  workshops, grand challenges, and tutorials will be held on
		  the 1st and 5th days, with the main conference occupying
		  the middle three days. All accepted papers will be
		  accessible online prior to the conference, and we are
		  working to ensure proceedings are available through the ACM
		  Digital Library around the conference period.This year's
		  conference features three distinguished academic keynote
		  speeches, several prestigious SIGMM award talks, a panel
		  discussion on Generative AI in Multimedia, a refreshed
		  Brave New Idea (BNI) session, and our inaugural industry
		  program.The opening keynote will be delivered by Prof.
		  Pascale Fung from HKUST, a Fellow of AAAI, ACL, and IEEE.
		  Her talk will explore the pressing topic of Agents in the
		  Large Language Model (LLM) Era. Prof. Judy Kay from the
		  University of Sydney, a renowned expert in HCI, user
		  modeling, and ubiquitous computing, will give the second
		  keynote on how to empower individuals to harness and
		  control their multimodal data. The final academic keynote
		  will be presented by Prof. Jiebo Luo from the University of
		  Rochester, a Fellow of ACM, AAAI, IEEE, SPIE, and IAPR, as
		  well as a member of Academia Europaea and the US National
		  Academy of Inventors. He will discuss leveraging LLMs as
		  social multimedia analysis engines.This year, we continue
		  using OpenReview to ensure an open and transparent review
		  process. Thanks to the exceptional efforts of the technical
		  program committee, every paper received at least three
		  reviews before the review announcement. The BNI track has
		  also revamped its review process to align with the main
		  conference, promoting visionary papers. Additionally, we
		  are excited to introduce the industry program to ACM
		  Multimedia for the first time, featuring industry keynote
		  speeches, expert talks, and demonstrations (please refer to
		  the industry chairs' message for further details).We are
		  also committed to making the conference inclusive and
		  accessible. To support students with financial constraints,
		  we have awarded travel grants to at least 25 students from
		  the ACM Multimedia 2024 budget, with an additional 20+
		  students receiving SIGMM travel grants. Over 20 local
		  students have also been recruited as volunteers, benefiting
		  from complimentary registration. Furthermore, we have
		  arranged childcare facilities to accommodate attendees with
		  young children. A welcome reception will take place on the
		  2nd day of the conference, followed by a gala dinner on the
		  3rd day, featuring exciting cultural performances.We hope
		  you find this year's program engaging and thought-provoking
		  and that it offers valuable opportunities to exchange ideas
		  with fellow researchers and practitioners from around the
		  globe. We also encourage you to take time to explore the
		  beautiful city of Melbourne and its surrounding regions.},
  location	= {Melbourne VIC, Australia}
}

@Article{	  10.1145/3735974,
  author	= {Ansar, Wazib and Goswami, Saptarsi and Chakrabarti, Amlan
		  and Chakraborty, Basabi},
  title		= {TexIm FAST: Text-to-Image Encoding for Semantic Similarity
		  Evaluation of Disproportionate Sequences},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {6},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3735974},
  doi		= {10.1145/3735974},
  abstract	= {One of the principal objectives of Natural Language
		  Processing (NLP) is to generate meaningful representations
		  from text. Improving the informativeness of the
		  representations has led to a tremendous rise in the
		  dimensionality and the memory footprint. It leads to a
		  cascading effect amplifying the complexity of the
		  downstream model by increasing its parameters. The
		  available techniques cannot be applied to cross-modal
		  applications such as text-to-image. To ameliorate these
		  issues, a novel Text-to-Image Fixed-dimensional encoding
		  technique through a self-supervised Variational
		  Auto-Encoder (VAE) for semantic evaluation applying
		  transformers (TexIm FAST) has been proposed in this
		  article. The pictorial representations allow oblivious
		  inference while retaining the linguistic intricacies and
		  are potent in cross-modal applications. TexIm FAST deals
		  with variable-length sequences and generates
		  uniform-dimensional images with over 75\% reduced memory
		  footprint. It enhances the efficiency of the models for
		  downstream tasks by reducing its parameters. The efficacy
		  of TexIm FAST has been extensively analyzed for the task of
		  Semantic Textual Similarity (STS) on a benchmark dataset
		  and two new datasets put forth containing disproportionate
		  sequences. The results demonstrate its exceptional ability
		  to compare disparate-length sequences such as a text with
		  its summary with 3\% improvement in accuracy compared to
		  the SOTA despite having 68\% less parameters.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= jul,
  articleno	= {168},
  numpages	= {23},
  keywords	= {Oblivious Inference, Semantic Similarity, Text-to-Image,
		  Text Embedding, Transformers NLP, Variational
		  Auto-Encoder}
}

@InProceedings{	  10.1145/3243082.3267457,
  author	= {Oliveira, Yuri and Silveira, Leonardo and Souza, Cidcley},
  title		= {A Model-Driven Approach to Evolve Recommender Systems},
  year		= {2018},
  isbn		= {9781450358675},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3243082.3267457},
  doi		= {10.1145/3243082.3267457},
  abstract	= {Recommender systems have become an important issue on Web
		  applications, but its research is usually focused on
		  algorithms and data optimization. However, as the
		  recommendation techniques improve and these systems become
		  more commonly used in software applications, there is the
		  need of easily adapt and evolve them. To address this need,
		  we propose a model-driven approach to evolve recommender
		  systems and present an architecture solution from our
		  research, using an events management system as the domain
		  for an use-case scenario. Future work might demonstrate the
		  architecture feasibility.},
  booktitle	= {Proceedings of the 24th Brazilian Symposium on Multimedia
		  and the Web},
  pages		= {169–172},
  numpages	= {4},
  keywords	= {software architecture, recommender systems, model-driven
		  engineering},
  location	= {Salvador, BA, Brazil},
  series	= {WebMedia '18}
}

@Article{	  10.1145/3588911,
  author	= {Omar, Reham and Dhall, Ishika and Kalnis, Panos and
		  Mansour, Essam},
  title		= {A Universal Question-Answering Platform for Knowledge
		  Graphs},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {1},
  url		= {https://doi.org/10.1145/3588911},
  doi		= {10.1145/3588911},
  abstract	= {Knowledge from diverse application domains is organized as
		  knowledge graphs (KGs) that are stored in RDF engines
		  accessible in the web via SPARQL endpoints. Expressing a
		  well-formed SPARQL query requires information about the
		  graph structure and the exact URIs of its components, which
		  is impractical for the average user. Question answering
		  (QA) systems assist by translating natural language
		  questions to SPARQL. Existing QA systems are typically
		  based on application-specific human-curated rules, or
		  require prior information, expensive pre-processing and
		  model adaptation for each targeted KG. Therefore, they are
		  hard to generalize to a broad set of applications and KGs.
		  In this paper, we propose KGQAn, a universal QA system that
		  does not need to be tailored to each target KG. Instead of
		  curated rules, KGQAn introduces a novel formalization of
		  question understanding as a text generation problem to
		  convert a question into an intermediate abstract
		  representation via a neural sequence-to-sequence model. We
		  also develop a just-in-time linker that maps at query time
		  the abstract representation to a SPARQL query for a
		  specific KG, using only the publicly accessible APIs and
		  the existing indices of the RDF store, without requiring
		  any pre-processing. Our experiments with several real KGs
		  demonstrate that KGQAn is easily deployed and outperforms
		  by a large margin the state-of-the-art in terms of quality
		  of answers and processing time, especially for arbitrary
		  KGs, unseen during the training.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {57},
  numpages	= {25},
  keywords	= {RDF, just-in-time entity and relation linking, knowledge
		  graphs, natural language question answering, seq2seq
		  models}
}

@InProceedings{	  10.1145/3723010.3723036,
  author	= {B\"{o}hm, Karsten},
  title		= {Towards a Semantic Representation of Framework
		  Recommendations for Curricular Specifications in Higher
		  Education},
  year		= {2025},
  isbn		= {9798400712821},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3723010.3723036},
  doi		= {10.1145/3723010.3723036},
  abstract	= {Curricular specifications play an important role in the
		  Higher Education sector and the domain of Computer Science
		  and Software Engineering is characterized by a wide range
		  of education programs with a broad range of topic.
		  Therefore, recommendation frameworks play an important role
		  and their usage is beneficial for a unification of
		  education profiles in a systematic way. This research is
		  contributing to this development by exploring how a
		  recommendation for the domain of Business Informatics in
		  German speaking countries can be improved by formalizing
		  the recommendations in a semantic model that relies on
		  sophisticated European ontologies in the domain like the
		  European Learning Model (ELM) and related data models. It
		  employs Generative Artificial Intelligence Systems to
		  create semantic models in an experimental way and evaluates
		  the resulting model quality. The results show that a
		  formalization using GenAI has a high potential, but
		  currently also shows deficits in the correctness of the
		  resulting models, requiring human oversight during the
		  model creation.},
  booktitle	= {Proceedings of the 6th European Conference on Software
		  Engineering Education},
  pages		= {154–160},
  numpages	= {7},
  keywords	= {Business Informatics, Competence Specification, European
		  Learning Model, Higher Education, Learning Framework,
		  Semantic Web},
  location	= { },
  series	= {ECSEE '25}
}

@Article{	  10.1145/3577204,
  author	= {Boudi, Zakaryae and Wakrime, Abderrahim Ait and Toub,
		  Mohamed and Haloua, Mohamed},
  title		= {A Deep Reinforcement Learning Framework with Formal
		  Verification},
  year		= {2023},
  issue_date	= {March 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {35},
  number	= {1},
  issn		= {0934-5043},
  url		= {https://doi.org/10.1145/3577204},
  doi		= {10.1145/3577204},
  abstract	= {Artificial Intelligence (AI) and data are reshaping
		  organizations and businesses. Human Resources (HR)
		  management and talent development make no exception, as
		  they tend to involve more automation and growing quantities
		  of data. Because this brings implications on workforce,
		  career transparency, and equal opportunities, overseeing
		  what fuels AI and analytical models, their quality
		  standards, integrity, and correctness becomes an imperative
		  for those aspiring to such systems. Based on an ontology
		  transformation to B-machines, this article presents an
		  approach to constructing a valid and error-free career
		  agent with Deep Reinforcement Learning (DRL). In short, the
		  agent's policy is built on a framework we called Multi
		  State-Actor (MuStAc) using a decentralized training
		  approach. Its purpose is to predict both relevant and valid
		  career steps to employees, based on their profiles and
		  company pathways (observations). Observations can comprise
		  various data elements such as the current occupation, past
		  experiences, performance, skills, qualifications, and so
		  on. The policy takes in all these observations and outputs
		  the next recommended career step, in an environment set as
		  the combination of an HR ontology and an Event-B model,
		  which generates action spaces with respect to formal
		  properties. The Event-B model and formal properties are
		  derived using OWL to B transformation.},
  journal	= {Form. Asp. Comput.},
  month		= mar,
  articleno	= {5},
  numpages	= {17},
  keywords	= {Atelier B, Model Transformation, Safe RL, Safe AI, AI
		  Control, Formal Verification, Event-B}
}

@InProceedings{	  10.1145/3331184.3331427,
  author	= {Firsov, Anton and Bugay, Vladimir and Karpenko, Anton},
  title		= {USEing Transfer Learning in Retrieval of Statistical
		  Data},
  year		= {2019},
  isbn		= {9781450361729},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331184.3331427},
  doi		= {10.1145/3331184.3331427},
  abstract	= {DSSM-like models showed good results in retrieval of short
		  documents that semantically match the query. However, these
		  models require large collections of click-through data that
		  are not available in some domains. On the other hand, the
		  recent advances in NLP demonstrated the possibility to
		  fine-tune language models and models trained on one set of
		  tasks to achieve a state of the art results on a multitude
		  of other tasks or to get competitive results using much
		  smaller training sets. Following this trend, we combined
		  DSSM-like architecture with USE (Universal Sentence
		  Encoder) and BERT (Bidirectional Encoder Representations
		  from Transformers) models in order to be able to fine-tune
		  them on a small amount of click-through data and use them
		  for information retrieval. This approach allowed us to
		  significantly improve our search engine for statistical
		  data.},
  booktitle	= {Proceedings of the 42nd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1391–1392},
  numpages	= {2},
  keywords	= {transfer learning, language model, information retrieval},
  location	= {Paris, France},
  series	= {SIGIR'19}
}

@Article{	  10.1145/3577925,
  author	= {Schiappa, Madeline C. and Rawat, Yogesh S. and Shah,
		  Mubarak},
  title		= {Self-Supervised Learning for Videos: A Survey},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {13s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3577925},
  doi		= {10.1145/3577925},
  abstract	= {The remarkable success of deep learning in various domains
		  relies on the availability of large-scale annotated
		  datasets. However, obtaining annotations is expensive and
		  requires great effort, which is especially challenging for
		  videos. Moreover, the use of human-generated annotations
		  leads to models with biased learning and poor domain
		  generalization and robustness. As an alternative,
		  self-supervised learning provides a way for representation
		  learning that does not require annotations and has shown
		  promise in both image and video domains. In contrast to the
		  image domain, learning video representations are more
		  challenging due to the temporal dimension, bringing in
		  motion and other environmental dynamics. This also provides
		  opportunities for video-exclusive ideas that advance
		  self-supervised learning in the video and multimodal
		  domains. In this survey, we provide a review of existing
		  approaches on self-supervised learning focusing on the
		  video domain. We summarize these methods into four
		  different categories based on their learning objectives:
		  (1) pretext tasks, (2) generative learning, (3) contrastive
		  learning, and (4) cross-modal agreement. We further
		  introduce the commonly used datasets, downstream evaluation
		  tasks, insights into the limitations of existing works, and
		  the potential future directions in this area.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {288},
  numpages	= {37},
  keywords	= {visual-language models, multimodal learning,
		  representation learning, zero-shot learning, video
		  understanding, deep learning, Self-supervised learning}
}

@Article{	  10.1145/3746658,
  author	= {Ozdemir, Anil and Odaci, Berke and Tanatar Baruh, Lorans
		  and Varol, Onur and Balcisoy, Selim},
  title		= {Enhancing Cultural Heritage Archive Analysis via Automated
		  Entity Extraction and Graph-Based Representation Learning},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3746658},
  doi		= {10.1145/3746658},
  abstract	= {Recent efforts to digitize textual, visual, and physical
		  forms of cultural heritage require advanced tools for
		  preservation and analysis. The availability of extensive
		  online data creates a need for intelligent systems to help
		  users and archivists understand latent relationships in
		  these collections. A major challenge in cultural heritage
		  studies is the labor-intensive process of analyzing these
		  materials. Inconsistent linguistic terms and ambiguous
		  concepts in digital documents make it difficult to uncover
		  relationships without expert supervision. Moreover, while
		  advanced models based on large-scale pretraining
		  demonstrate strong performance in extracting semantic
		  relationships, they depend on extensive pretraining on
		  large external datasets, limiting their applicability for
		  smaller or specialized collections. We propose a system
		  that combines natural language processing for entity
		  extraction with graph representation learning to model
		  relationships among documents, categories, and n-grams,
		  resulting in a fully-connected network representation.
		  Unlike methods requiring large-scale pretraining, our
		  approach operates effectively using only the information
		  available in the dataset itself, making it particularly
		  suited for smaller cultural heritage document collections.
		  The system extracts significant terms from document
		  metadata, produces embeddings for each document, and uses
		  these embeddings to build a recommendation system for
		  entity discovery. We tested the system on a collection of
		  early 20th-century documents from Crete, evaluating its
		  performance against alternative methods in collaboration
		  with experts from the archival research organization SALT.
		  This approach not only facilitates deeper insights into
		  smaller, specialized collections but also reduces
		  dependency on vast external training resources, enhancing
		  its practical utility in cultural heritage studies.},
  note		= {Just Accepted},
  journal	= {J. Comput. Cult. Herit.},
  month		= jul,
  keywords	= {Natural Language Processing, Machine Learning, Graph
		  Representation Learning, Recommendation Systems}
}

@Article{	  10.1613/jair.1.13167,
  author	= {Koto, Fajri and Baldwin, Timothy and Lau, Jey Han},
  title		= {FFCI: A Framework for Interpretable Automatic Evaluation
		  of Summarization},
  year		= {2022},
  issue_date	= {May 2022},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {73},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.13167},
  doi		= {10.1613/jair.1.13167},
  abstract	= {In this paper, we propose FFCI, a framework for
		  fine-grained summarization evaluation that comprises four
		  elements: faithfulness (degree of factual consistency with
		  the source), focus (precision of summary content relative
		  to the reference), coverage (recall of summary content
		  relative to the reference), and inter-sentential coherence
		  (document fluency between adjacent sentences). We construct
		  a novel dataset for focus, coverage, and inter-sentential
		  coherence, and develop automatic methods for evaluating
		  each of the four dimensions of FFCI based on
		  cross-comparison of evaluation metrics and model-based
		  evaluation methods, including question answering (QA)
		  approaches, semantic textual similarity (STS),
		  next-sentence prediction (NSP), and scores derived from 19
		  pre-trained language models. We then apply the developed
		  metrics in evaluating a broad range of summarization models
		  across two datasets, with some surprising findings.},
  journal	= {J. Artif. Int. Res.},
  month		= may,
  numpages	= {55},
  keywords	= {neural networks, machine learning, natural language}
}

@InProceedings{	  10.1145/3277139.3277154,
  author	= {Lin, Menglong and Yao, Yiping},
  title		= {Modeling framework of general simulation model based on
		  model template},
  year		= {2018},
  isbn		= {9781450364867},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3277139.3277154},
  doi		= {10.1145/3277139.3277154},
  abstract	= {The model is the core of the simulation system, aiming at
		  the problems of low reuse efficiency of the current
		  simulation model and long model development cycle, this
		  paper proposes a generic model generation technology
		  framework. First, the simulation model is split into
		  independent components through military concept analysis,
		  and then the common concept of the same model is extracted
		  to form a model template with a unified description
		  specification. The specific simulation model is
		  instantiated quickly through parameterized configuration.
		  The modeling framework can effectively support the
		  generation and application of the platform models in
		  military simulation systems, and has great scalability and
		  reusability.},
  booktitle	= {Proceedings of the 1st International Conference on
		  Information Management and Management Science},
  pages		= {223–227},
  numpages	= {5},
  keywords	= {simulation model, modeling framework, model template,
		  component-based},
  location	= {Chengdu, China},
  series	= {IMMS '18}
}

@InProceedings{	  10.1145/3658271.3658339,
  author	= {Molina De Armas, Elvismary and Hamazaki Da Silva, Geiza
		  Maria and Torres Izquierdo, Yenier and Lemos, Melissa and
		  De Lima Britto, Paulo Vin\'{\i}cius and Corseuil, Eduardo
		  Thadeu and Souza Garcia, Robinson Luiz},
  title		= {A Proposal of a Knowledge Graph for Digital Engineering
		  Systems Integration for Operation and Maintenance
		  Activities in Industrial Plants},
  year		= {2024},
  isbn		= {9798400709968},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3658271.3658339},
  doi		= {10.1145/3658271.3658339},
  abstract	= {Context: Over the last years, we have observed Knowledge
		  Graphs (KGs) being used more and more as a tool for
		  representing knowledge, data integration and querying data.
		  Problem: There are many distinguished yet
		  partially-integrated information management systems used to
		  support the life-cycle of Oil and Gas industrial plants.
		  Our approach considers a 3D plants viewer system, a visual
		  navigation system on platforms, and the integrated
		  intelligent search system. However, these systems lack a
		  semantic integration that can guide the user actions over
		  each functionality for a unique asset. Solution: This paper
		  presents the use of KGs to represent and help monitoring
		  and controlling operational and maintenance activities
		  within an Oil and Gas industrial environment. Our approach
		  highlights the challenges and initial work required to
		  establish a fully-integrated management domain, where the
		  execution of the aforementioned activities can easily be
		  managed. SI Theory: This study draws inspiration from
		  Representation Theory, which posits that an information
		  system faithfully mirrors specific phenomena occurring in
		  the physical world. Method: To develop this work, it was
		  necessary to review the literature related to the
		  development of KGs and ontologies. The generated KG was
		  developed using well-established standards like the
		  Industrial Data Ontology (IDO), and the Capital Facilities
		  Information Handover Specification (CFIHOS), complemented
		  with the use of other ontologies. Summary of Results: A
		  prototype of the conceptual KG was implemented, verifying
		  the viability of our approach for data integration.
		  Contributions and Impact in IS area: The resulted graph
		  contains the main terms in compliance with international
		  semantic standards for representing operational and
		  maintenance activities data associated with facilities
		  involved in Oil and Gas production. Finally, the KG
		  resulting from this effort can be further extended through
		  the incorporation of new tools and subdomains in the
		  industrial plants life-cycle.},
  booktitle	= {Proceedings of the 20th Brazilian Symposium on Information
		  Systems},
  articleno	= {67},
  numpages	= {10},
  keywords	= {Data Integration, Digital Engineering, Industrial Plants,
		  Knowledge Graphs, Ontology, Operation and Maintenance
		  activities},
  location	= {Juiz de Fora, Brazil},
  series	= {SBSI '24}
}

@InProceedings{	  10.1145/3331453.3361301,
  author	= {Ma, Zhiyi and Wang, Xiaoxi and Chen, Hongjie and Qiu, Ye},
  title		= {A Modeling Tool for Sensor-based Mobile Applications},
  year		= {2019},
  isbn		= {9781450362948},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331453.3361301},
  doi		= {10.1145/3331453.3361301},
  abstract	= {With the variety of the sensors on mobile devices,
		  sensor-based mobile applications are constantly emerging.
		  Usually, it is necessary to model sensor-based mobile
		  applications, and the modeling requires the domain
		  metamodels and the modeling tools. This paper presents a
		  set of the modeling concepts from the aspects of sensor
		  data perception, sensor data understanding, and sensor data
		  processing, and then builds a metamodel with UML profile
		  mechanism. Moreover, the paper proposes a modeling tool
		  architecture and discusses the design and implementation of
		  key modules in the architecture based on this metamodel.
		  According to the metamodel, architecture, and key modules,
		  the developers can build the tools to model sensor-based
		  mobile applications and generate key code to greatly
		  improve the development productivity and quality of the
		  applications.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Computer Science and Application Engineering},
  articleno	= {130},
  numpages	= {6},
  keywords	= {Sensors, Modeling tool, Mobile applications, Metamodel},
  location	= {Sanya, China},
  series	= {CSAE '19}
}

@InProceedings{	  10.1145/3538637.3538838,
  author	= {He, Fang and Zhang, Xiaoyang and Wang, Dan},
  title		= {Cement-α: an ontology-based data access system for
		  building analytics with multiple data sources},
  year		= {2022},
  isbn		= {9781450393973},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3538637.3538838},
  doi		= {10.1145/3538637.3538838},
  abstract	= {To enhance the portability of data-driven building
		  analytics across buildings, data models have been developed
		  to provide unified representations of building data, such
		  as Brick, which defines how to construct an ontology to
		  capture the semantics of building entities and the
		  relationships among them. Unfortunately, existing data
		  models are all developed for the data of building systems,
		  e.g., the HVAC system of a building. Yet, building
		  analytics can require data that are external to a building
		  system, e.g., weather data for cooling load forecasting.
		  Such external data can be accessed from external sources,
		  e.g., observatory, yet these data have their own data
		  models and storage methods.To enable portable data access
		  from multiple data sources for building analytics, in this
		  paper, we firstly define building periphery data, and then
		  we study the approach to develop a data model to support
		  building analytics which require both building data and
		  building periphery data. We further develop Cement-α, an
		  ontology-based data access system to extract both building
		  and building periphery data. We evaluate the Cement-α
		  qualitatively and quantitatively by using four real-world
		  building analytics, and find that the development effort of
		  building analytics with multiple data sources can be
		  reduced by an average of 57.2\%.},
  booktitle	= {Proceedings of the Thirteenth ACM International Conference
		  on Future Energy Systems},
  pages		= {436–437},
  numpages	= {2},
  keywords	= {smart building, machine learning, data analytics, data
		  access},
  location	= {Virtual Event},
  series	= {e-Energy '22}
}

@InProceedings{	  10.1145/3418688.3418696,
  author	= {Alaa El Din Talha, Shorouk},
  title		= {A Semantic Based Annotation Technique for the Internet of
		  Things},
  year		= {2020},
  isbn		= {9781450387866},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3418688.3418696},
  doi		= {10.1145/3418688.3418696},
  abstract	= {Due to the recent deployments of Internet of Things (IoT)
		  technologies in many real-life applications, enormous
		  amount of diverse and real-time streams of data are being
		  generated. To facilitate dealing with the heterogeneity of
		  IoT data streams, semantic technologies became the main
		  element to guarantee data interoperability, with its nature
		  to unify concepts, extend knowledge, and share a
		  machine-readable representation of data. In this paper, we
		  propose an adaptable approach for IoT data semantic
		  annotation, to achieve an efficient way to enrich data
		  semantically considering its heterogeneity, volume, and
		  frequency. A use case is implemented using Apache Kafka,
		  Spark to deal with data streams in real-time, and a
		  Semantic ontology model extending the SOSA standard
		  ontology is developed to annotate data into enriched
		  Resource Description Framework (RDF) triples.},
  booktitle	= {Proceedings of the 2020 3rd International Conference on
		  Computing and Big Data},
  pages		= {42–47},
  numpages	= {6},
  keywords	= {Internet of Things, Ontology, Semantic Annotation, stream
		  processing},
  location	= {Taichung, Taiwan},
  series	= {ICCBD '20}
}

@InProceedings{	  10.1145/3437963.3441784,
  author	= {Balashankar, Ananth and Beutel, Alex and Subramanian,
		  Lakshminarayanan},
  title		= {Enhancing Neural Recommender Models through
		  Domain-Specific Concordance},
  year		= {2021},
  isbn		= {9781450382977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3437963.3441784},
  doi		= {10.1145/3437963.3441784},
  abstract	= {Recommender models trained on historical observational
		  data alone can be brittle when domain experts subject them
		  to counterfactual evaluation. In many domains, experts can
		  articulate common, high-level mappings or rules between
		  categories of inputs (user's history) and categories of
		  outputs (preferred recommendations). One challenge is to
		  determine how to train recommender models to adhere to
		  these rules. In this work, we introduce the goal of
		  domain-specific concordance: the expectation that a
		  recommender model follow a set of expert-defined
		  categorical rules. We propose a regularization-based
		  approach that optimizes for robustness on rule-based input
		  perturbations. To test the effectiveness of this method, we
		  apply it in a medication recommender model over
		  diagnosis-medicine categories, and in movie and music
		  recommender models, on rules over categories based on movie
		  tags and song genres. We demonstrate that we can increase
		  the category-based robustness distance by up to 126\%
		  without degrading accuracy, but rather increasing it by up
		  to 12\% compared to baseline models in the popular
		  MIMIC-III, MovieLens-20M and Last.fm Million Song datasets.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {1002–1010},
  numpages	= {9},
  keywords	= {recommender systems, information systems},
  location	= {Virtual Event, Israel},
  series	= {WSDM '21}
}

@Article{	  10.1109/tcbb.2020.3010975,
  author	= {Nguyen, Trinh-Trung-Duong and Ho, Quang-Thai and Le,
		  Nguyen-Quoc-Khanh and Phan, Van-Dinh and Ou, Yu-Yen},
  title		= {Use Chou's 5-Steps Rule With Different Word Embedding
		  Types to Boost Performance of Electron Transport Protein
		  Prediction Model},
  year		= {2020},
  issue_date	= {March-April 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {2},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2020.3010975},
  doi		= {10.1109/TCBB.2020.3010975},
  abstract	= {Living organisms receive necessary energy substances
		  directly from cellular respiration. The completion of
		  electron storage and transportation requires the process of
		  cellular respiration with the aid of electron transport
		  chains. Therefore, the work of deciphering electron
		  transport proteins is inevitably needed. The identification
		  of these proteins with high performance has a prompt
		  dependence on the choice of methods for feature extraction
		  and machine learning algorithm. In this study, protein
		  sequences served as natural language sentences comprising
		  words. The nominated word embedding-based feature sets,
		  hinged on the word embedding modulation and protein motif
		  frequencies, were useful for feature choosing. Five word
		  embedding types and a variety of conjoint features were
		  examined for such feature selection. The support vector
		  machine algorithm consequentially was employed to perform
		  classification. The performance statistics within the
		  5-fold cross-validation including average accuracy,
		  specificity, sensitivity, as well as MCC rates surpass
		  0.95. Such metrics in the independent test are 96.82,
		  97.16, 95.76 percent, and 0.9, respectively. Compared to
		  state-of-the-art predictors, the proposed method can
		  generate more preferable performance above all metrics
		  indicating the effectiveness of the proposed method in
		  determining electron transport proteins. Furthermore, this
		  study reveals insights about the applicability of various
		  word embeddings for understanding surveyed sequences.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jul,
  pages		= {1235–1244},
  numpages	= {10}
}

@InBook{	  10.1145/3382097.3382113,
  title		= {Good and bad modeling practices},
  year		= {2020},
  isbn		= {9781450376174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3382097.3382113},
  abstract	= {Enterprises have made amazing advances by taking advantage
		  of data about their business to provide predictions and
		  understanding of their customers, markets, and products.
		  But as the world of business becomes more interconnected
		  and global, enterprise data is no long a monolith; it is
		  just a part of a vast web of data. Managing data on a
		  world-wide scale is a key capability for any business
		  today.The Semantic Web treats data as a distributed
		  resource on the scale of the World Wide Web, and
		  incorporates features to address the challenges of massive
		  data distribution as part of its basic design. The aim of
		  the first two editions was to motivate the Semantic Web
		  technology stack from end-to-end; to describe not only what
		  the Semantic Web standards are and how they work, but also
		  what their goals are and why they were designed as they
		  are. It tells a coherent story from beginning to end of how
		  the standards work to manage a world-wide distributed web
		  of knowledge in a meaningful way.The third edition builds
		  on this foundation to bring Semantic Web practice to
		  enterprise. Fabien Gandon joins Dean Allemang and Jim
		  Hendler, bringing with him years of experience in global
		  linked data, to open up the story to a modern view of
		  global linked data. While the overall story is the same,
		  the examples have been brought up to date and applied in a
		  modern setting, where enterprise and global data come
		  together as a living, linked network of data. Also included
		  with the third edition, all of the data sets and queries
		  are available online for study and experimentation at
		  data.world/swwo.},
  booktitle	= {Semantic Web for the Working Ontologist: Effective
		  Modeling for Linked Data, RDFS, and OWL}
}

@Article{	  10.1162/coli_a_00378,
  author	= {Ranta, Aarne and Angelov, Krasimir and Gruzitis, Normunds
		  and Kolachina, Prasanth},
  title		= {Abstract Syntax as Interlingua: Scaling Up the Grammatical
		  Framework from Controlled Languages to Robust Pipelines},
  year		= {2020},
  issue_date	= {June 2020},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA},
  volume	= {46},
  number	= {2},
  issn		= {0891-2017},
  url		= {https://doi.org/10.1162/coli_a_00378},
  doi		= {10.1162/coli_a_00378},
  abstract	= {Abstract syntax is an interlingual representation used in
		  compilers. Grammatical Framework (GF) applies the abstract
		  syntax idea to natural languages. The development of GF
		  started in 1998, first as a tool for controlled language
		  implementations, where it has gained an established
		  position in both academic and commercial projects. GF
		  provides grammar resources for over 40 languages, enabling
		  accurate generation and translation, as well as grammar
		  engineering tools and components for mobile and Web
		  applications. On the research side, the focus in the last
		  ten years has been on scaling up GF to wide-coverage
		  language processing. The concept of abstract syntax offers
		  a unified view on many other approaches: Universal
		  Dependencies, WordNets, FrameNets, Construction Grammars,
		  and Abstract Meaning Representations. This makes it
		  possible for GF to utilize data from the other approaches
		  and to build robust pipelines. In return, GF can contribute
		  to data-driven approaches by methods to transfer resources
		  from one language to others, to augment data by rule-based
		  generation, to check the consistency of hand-annotated
		  corpora, and to pipe analyses into high-precision semantic
		  back ends. This article gives an overview of the use of
		  abstract syntax as interlingua through both established and
		  emerging NLP applications involving GF.},
  journal	= {Comput. Linguist.},
  month		= jun,
  pages		= {425–486},
  numpages	= {62}
}

@InProceedings{	  10.1145/3384544.3384549,
  author	= {Khan, Rimsha and Azam, Farooque and Maqbool, Bilal and
		  Anwar, Muhammad Waseem},
  title		= {A Framework for Automated Reengineering of BPMN Models by
		  Excluding Inefficient Activities},
  year		= {2020},
  isbn		= {9781450376655},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3384544.3384549},
  doi		= {10.1145/3384544.3384549},
  abstract	= {Business Process Reengineering (BPR), originally floated
		  in the early 1990s, is gaining importance in industry and
		  academia. BPR helps the organization rethink their work
		  rationally by redesigning their current processes and
		  resource consumption. Due to the high rate of software
		  evolution, there is a need to run legacy systems on a new
		  computing platform. BPMN models are subject to erroneous or
		  unnecessary activities that are taking too many resources.
		  Such process models are leading to additional cost and
		  effort. Re-engineering help in improving the legacy system
		  or in this context a set of legacy processes to perform
		  better than before. This work presents a framework for
		  automatic reengineering of a BPMN by identifying activities
		  that are taking too much time and resources but are
		  insignificant to the business process. An extensive
		  literature review has led to the extraction of three
		  important parameters based on which the business process
		  activities can be evaluated as necessary or unnecessary
		  i.e. time, resources and priority of an activity. The
		  proposed model has been validated using a case study on the
		  Claim Management System. This work shall be beneficial for
		  the research community and developers targeting
		  construction of a BPR tool},
  booktitle	= {Proceedings of the 2020 9th International Conference on
		  Software and Computer Applications},
  pages		= {147–151},
  numpages	= {5},
  keywords	= {BPMN, Business Process Re-engineering (BPR), Resource
		  Optimization, Software Automation},
  location	= {Langkawi, Malaysia},
  series	= {ICSCA '20}
}

@InProceedings{	  10.1145/3428502.3428611,
  author	= {Loutsaris, Michalis Avgerinos and Charalabidis, Yannis},
  title		= {Legal informatics from the aspect of interoperability: a
		  review of systems, tools and ontologies},
  year		= {2020},
  isbn		= {9781450376747},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3428502.3428611},
  doi		= {10.1145/3428502.3428611},
  abstract	= {In the reality of globalization, the legislation of every
		  country needs to be followed in order to achieve a
		  well-organized globalization process because the rule of
		  Law is a cornerstone, a fundamental foundation of every
		  democratic state and it should be observed and respected by
		  all in the society. The economic and industrial
		  globalization has increased international competition and
		  given rise to the need for an increasingly integrated and
		  evolving legal system but the fundamental debates over
		  globalization of the 1990s more or less petered out,
		  without leading to a clear consensus. So, society is still
		  overwhelmed with an over-load of legal information while in
		  the era of Digital Transformation, technologies such as Big
		  data, artificial intelligence, machine learning,
		  blockchain, 3D promise to have a profoundly disruptive
		  effect on the industry, business models, governance models
		  and on the way we interact with each other in society.
		  However, there is not a legal information system capable of
		  supporting the legislation of all countries in order to
		  facilitate the above operation but there are many
		  initiatives in order to develop legal ontologies. These
		  legal ontologies in combination with the disruptive
		  technologies can be help the problem of fragmented legal
		  information across-border in order to create the Big Linked
		  Open Legal Data.},
  booktitle	= {Proceedings of the 13th International Conference on Theory
		  and Practice of Electronic Governance},
  pages		= {731–737},
  numpages	= {7},
  keywords	= {Legal text mining, Legal ontologies, Legal information
		  systems, Legal Interoperability, Legal Editing Tools},
  location	= {Athens, Greece},
  series	= {ICEGOV '20}
}

@Article{	  10.1145/3393692,
  author	= {Kaur, Manpreet and Salim, Flora D. and Ren, Yongli and
		  Chan, Jeffrey and Tomko, Martin and Sanderson, Mark},
  title		= {Joint Modelling of Cyber Activities and Physical Context
		  to Improve Prediction of Visitor Behaviors},
  year		= {2020},
  issue_date	= {August 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1550-4859},
  url		= {https://doi.org/10.1145/3393692},
  doi		= {10.1145/3393692},
  abstract	= {This article investigates the cyber-physical behavior of
		  users in a large indoor shopping mall by leveraging
		  anonymized (opt in) Wi-Fi association and browsing logs
		  recorded by the mall operators. Our analysis shows that
		  many users exhibit a high correlation between their cyber
		  activities and their physical context. To find this
		  correlation,propose a mechanism to semantically label a
		  physical space with rich categorical information from
		  DBPedia concepts and compute a contextual similarity that
		  represents a user’s activities with the mall context. We
		  demonstrate the application of cyber-physical contextual
		  similarity in two situations: user visit intent
		  classification and future location prediction. The
		  experimental results demonstrate that exploitation of
		  contextual similarity significantly improves the accuracy
		  of such applications.},
  journal	= {ACM Trans. Sen. Netw.},
  month		= aug,
  articleno	= {28},
  numpages	= {25},
  keywords	= {user profiling, user modelling, shopping behaviour,
		  semantic enrichment, retail behaviour, recommender systems,
		  movement analysis, logs analysis, location prediction,
		  knowledge graph, intent recognition, indoor trajectory,
		  cyber-physical, context-aware computing, check-ins, Wi-Fi}
}

@Article{	  10.1145/3229087,
  author	= {Tomlein, Mat\'{u}\v{s} and Gr\o{}nb\ae{}k, Kaj},
  title		= {Augmented Reality Supported Modeling of Industrial Systems
		  to Infer Software Configuration},
  year		= {2018},
  issue_date	= {June 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {EICS},
  url		= {https://doi.org/10.1145/3229087},
  doi		= {10.1145/3229087},
  abstract	= {This paper proposes and evaluates an approach for building
		  models of installed industrial Cyber-Physical Systems using
		  augmented reality on smartphones. It proposes a visual
		  language for annotating devices, containers, flows of
		  liquids and networking connections in augmented reality.
		  Compared to related work, it provides a more lightweight
		  and flexible approach for building 3D models of industrial
		  systems. The models are further used to automatically infer
		  software configuration of controllable industrial products.
		  This addresses a common problem of error-prone and
		  time-consuming configuration of industrial systems in the
		  current practice. The proposed approach is evaluated in a
		  study with 16 domain experts. The study participants are
		  involved in creating a model of an industrial system for
		  water treatment. Their comments show that the approach can
		  enable a less error-prone configuration for more complex
		  systems. Opportunities for improvement in usability and
		  reflections on the potential of the approach are
		  discussed.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jun,
  articleno	= {5},
  numpages	= {17},
  keywords	= {modeling, iot, configuration, augmented reality}
}

@InProceedings{	  10.1145/3429889.3429909,
  author	= {Xiao, Rui and Hu, Fengju and Pei, Wei and Bie, Minkun},
  title		= {Research on Traditional Chinese Medicine Data Mining Model
		  Based on Traditional Chinese Medicine Basic Theories and
		  Knowledge Graphs},
  year		= {2020},
  isbn		= {9781450388603},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3429889.3429909},
  doi		= {10.1145/3429889.3429909},
  abstract	= {In recent years, great progress has been made in the study
		  of knowledge graph in various fields, and it has become a
		  hot topic in Traditional Chinese Medicine (TCM) related
		  fields. This paper utilizes a Chinese Herbal Medicine
		  collection, which includes 537 medicines, retrieved from a
		  hospital affiliated with a TCM university, as data source;
		  referenced the Chinese Pharmacopoeia for building the
		  knowledge graph founded on the basic TCM theory. Via
		  associating the prescription with drug properties, taste
		  and meridian tropism of Chinese medicine and visualizing
		  the complex network of Chinese medicine prescription from a
		  novel perspective, the rules in the prescription can be
		  mined in a deeper level, which has a strong practical
		  reference value for developing new clinical medicine and
		  studying the prescription data mining.},
  booktitle	= {Proceedings of the 1st International Symposium on
		  Artificial Intelligence in Medical Sciences},
  pages		= {102–106},
  numpages	= {5},
  keywords	= {Knowledge Graph, Data Mining, Basic Theories of TCM},
  location	= {Beijing, China},
  series	= {ISAIMS '20}
}

@InProceedings{	  10.1145/3227609.3227659,
  author	= {Loukachevitch, Natalia and Ivanov, Kirill and Dobrov,
		  Boris},
  title		= {Thesaurus-Based Topic Models and Their Evaluation},
  year		= {2018},
  isbn		= {9781450354899},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3227609.3227659},
  doi		= {10.1145/3227609.3227659},
  abstract	= {In this paper we study thesaurus-based topic models and
		  evaluate them from the point of view of topic coherence.
		  Thesaurus-based topic model enhances scores of related
		  terms found in the same text, which means that the model
		  encourages these terms to be in the same topics. We
		  evaluate various variants of such models. At the first
		  step, we carry out manual evaluation of the obtained
		  topics. At the second step, we study the possibility to use
		  the collected manual data for evaluating new variants of
		  thesaurus-based models, propose a method and select the
		  best of its parameters in cross-validation. At the third
		  step, we apply the created evaluation method to estimate
		  the influence of word frequencies on adding thesaurus
		  relations during generating topic models.},
  booktitle	= {Proceedings of the 8th International Conference on Web
		  Intelligence, Mining and Semantics},
  articleno	= {11},
  numpages	= {9},
  keywords	= {topic models, thesaurus, content-based analysis},
  location	= {Novi Sad, Serbia},
  series	= {WIMS '18}
}

@InProceedings{	  10.1145/3368756.3369029,
  author	= {Kouissi, Mohamed and Ghouch, Nihad El and En-naimi, El
		  Mokhtar},
  title		= {New approach for modeling and developing multi-agent
		  systems based on case based reasoning},
  year		= {2019},
  isbn		= {9781450362894},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368756.3369029},
  doi		= {10.1145/3368756.3369029},
  abstract	= {In this paper, we present a multi agent architecture based
		  on Incremental Dynamic Case-Based Reasoning (IDCBR). Our
		  approach inherits from Model Driven Architecture (MDA
		  [11]), which aims to design, develop and implement models
		  or meta-models of multi-agent systems that we build from
		  AUML. We have designed a generic and scalable class diagram
		  to develop complex multi-agent systems [3] for Decision
		  Support System based on IDCBR to predict and anticipate a
		  dynamic situation. The source code of the models is
		  generated by an open source tool called AndroMDA [13]. The
		  model and source code will be used to design and develop
		  applications to implement and simulate multi-agent models
		  for Management of Common Renewable Resources [4].},
  booktitle	= {Proceedings of the 4th International Conference on Smart
		  City Applications},
  articleno	= {49},
  numpages	= {8},
  keywords	= {simulation, multi agents systems, model driven
		  architecture (MDA), incremental dynamic case-based
		  reasoning (IDCBR), decision making, common renewable
		  resources, Jade platform, AUML},
  location	= {Casablanca, Morocco},
  series	= {SCA '19}
}

@InProceedings{	  10.1145/3265689.3265708,
  author	= {Huang, Yadong and Chai, Yueting and Liu, Yi and Zhang,
		  Anting and Wu, Hao},
  title		= {Modeling and Analysis of Demand for Personalized Portal},
  year		= {2018},
  isbn		= {9781450365871},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3265689.3265708},
  doi		= {10.1145/3265689.3265708},
  abstract	= {E-commerce1 has experienced great growth during the past
		  two decades, which changes the consumption mode of
		  consumers significantly. All researchers from institutions
		  and enterprises want to identify and satisfy the
		  personalized demand intelligently and conveniently by every
		  possible means. In this paper, we proposed smart demand
		  strategy based on holographic demand and model of
		  transaction subject, which is applicable for the
		  decentralized, disintermediated, intelligent e-commerce
		  platform. User demands are classified from two aspects,
		  which will improve the accuracy of demand obtaining. In
		  addition, from standardized description of demand and full
		  life cycle tracking of demand, the user demand will be
		  identified comprehensively. Meanwhile, models of the user,
		  including physical, preference, knowledge, digital label,
		  and social attributes, are built based on his standard
		  description and fragmented description from his interactive
		  objects, which results in a holographic demander. Then,
		  smart demand strategy, i.e. demand forecast and
		  recommendation are proposed. Based on trigger point and
		  demand attributes, the user demand will be updated in real
		  time, which ensures the accuracy of demand accusation and
		  recommendation. The relationship and help degree, based on
		  the interactions within the cyberspace, are important
		  references in filtering the recommendation.},
  booktitle	= {Proceedings of the 3rd International Conference on Crowd
		  Science and Engineering},
  articleno	= {19},
  numpages	= {8},
  keywords	= {Subject Model, Personalized portal, Holographic demand,
		  E-commerce, Demand Strategy},
  location	= {Singapore, Singapore},
  series	= {ICCSE'18}
}

@Article{	  10.5555/3546258.3546539,
  author	= {Tosh, Christopher and Krishnamurthy, Akshay and Hsu,
		  Daniel},
  title		= {Contrastive estimation reveals topic posterior information
		  to linear models},
  year		= {2021},
  issue_date	= {January 2021},
  publisher	= {JMLR.org},
  volume	= {22},
  number	= {1},
  issn		= {1532-4435},
  abstract	= {Contrastive learning is an approach to representation
		  learning that utilizes naturally occurring similar and
		  dissimilar pairs of data points to find useful embeddings
		  of data. In the context of document classification under
		  topic modeling assumptions, we prove that contrastive
		  learning is capable of recovering a representation of
		  documents that reveals their underlying topic posterior
		  information to linear models. We apply this procedure in a
		  semi-supervised setup and demonstrate empirically that
		  linear classifiers trained on these representations perform
		  well in document classification tasks with very few
		  training examples.},
  journal	= {J. Mach. Learn. Res.},
  month		= jan,
  articleno	= {281},
  numpages	= {31},
  keywords	= {representation learning, latent Dirichlet allocation,
		  contrastive estimation}
}

@InProceedings{	  10.1145/3350768.3351795,
  author	= {Bispo, Cristiana and Fernandes, Sergio and Magalh\~{a}es,
		  Ana Patr\'{\i}cia},
  title		= {Strategies for Use Case Modeling: A Systematic Literature
		  Review},
  year		= {2019},
  isbn		= {9781450376518},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3350768.3351795},
  doi		= {10.1145/3350768.3351795},
  abstract	= {A major challenge in teaching use-case modeling (UCM) is
		  to mitigate the difficulties of students that prevent them
		  from producing use-case models with quality. The strategies
		  for UCM are scattered in the literature in several areas,
		  and may not be known to the students, who therefore fail to
		  receive the benefits that would mitigate their
		  difficulties. This paper aims to present a systematic
		  literature review (SLR) to identify, gather and analyze
		  strategies for UCM. During the SLR, two thousand two
		  hundred sixty-six studies published between 2008 and 2018
		  were returned from 6 bases (ACM, IEEE, Scopus, Science
		  Direct, SpringerLink and Engineering Village), which
		  resulted in the selection of 39 primary studies. These were
		  classified, following the coding procedures of Grounded
		  Theory, into 13 categories of different strategies for UCM.
		  The results can help teachers in the adoption of the most
		  appropriate UCM strategies for their students. Besides,
		  they provide a quick reference for teachers and researchers
		  interested in conducting additional studies on teaching
		  strategies for UCM.},
  booktitle	= {Proceedings of the XXXIII Brazilian Symposium on Software
		  Engineering},
  pages		= {254–263},
  numpages	= {10},
  keywords	= {Use Case, Systematic Review, Strategy for Use Case
		  Modeling, Software Modeling, Requirement},
  location	= {Salvador, Brazil},
  series	= {SBES '19}
}

@Article{	  10.1145/3543826,
  author	= {Demir, Seniz},
  title		= {Turkish Data-to-Text Generation Using Sequence-to-Sequence
		  Neural Networks},
  year		= {2022},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3543826},
  doi		= {10.1145/3543826},
  abstract	= {End-to-end data-driven approaches lead to rapid
		  development of language generation and dialogue systems.
		  Despite the need for large amounts of well-organized data,
		  these approaches jointly learn multiple components of the
		  traditional generation pipeline without requiring costly
		  human intervention. End-to-end approaches also enable the
		  use of loosely aligned parallel datasets in system
		  development by relaxing the degree of semantic
		  correspondences between training data representations and
		  text spans. However, their potential in Turkish language
		  generation has not yet been fully exploited. In this work,
		  we apply sequence-to-sequence (Seq2Seq) neural models to
		  Turkish data-to-text generation where the input data given
		  in the form of a meaning representation is verbalized. We
		  explore encoder-decoder architectures with attention
		  mechanism in unidirectional, bidirectional, and stacked
		  recurrent neural network (RNN) models. Our models generate
		  one-sentence biographies and dining venue descriptions
		  using a crowdsourced dataset where all field value pairs
		  that appear in meaning representations are fully captured
		  in reference sentences. To support this work, we also
		  explore the performances of our models on a more
		  challenging dataset, where the content of a meaning
		  representation is too large to fit into a single sentence,
		  and hence content selection and surface realization need to
		  be learned jointly. This dataset is retrieved by coupling
		  introductory sentences of person-related Turkish Wikipedia
		  articles with their contained infobox tables. Our empirical
		  experiments on both datasets demonstrate that Seq2Seq
		  models are capable of generating coherent and fluent
		  biographies and venue descriptions from field value pairs.
		  We argue that the wealth of knowledge residing in our
		  datasets and the insights obtained from this study hold the
		  potential to give rise to the development of new end-to-end
		  generation approaches for Turkish and other morphologically
		  rich languages.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= dec,
  articleno	= {37},
  numpages	= {27},
  keywords	= {Wikipedia, Turkish, sequence-to-sequence model,
		  Data-to-text generation}
}

@InProceedings{	  10.1145/3269206.3269230,
  author	= {G\"{u}zel Kalayci, Elem and Xiao, Guohui and Ryzhikov,
		  Vladislav and Kalayci, Tahir Emre and Calvanese, Diego},
  title		= {Ontop-temporal: A Tool for Ontology-based Query Answering
		  over Temporal Data},
  year		= {2018},
  isbn		= {9781450360142},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3269206.3269230},
  doi		= {10.1145/3269206.3269230},
  abstract	= {We present Ontop-temporal, an extension of the
		  ontology-based data access system Ontop for query answering
		  with temporal data and ontologies. Ontop is a system to
		  answer SPARQL queries over various data stores, using
		  standard R2RML mappings and an OWL2QL domain ontology to
		  produce high-level conceptual views over the raw data. The
		  Ontop-temporal extension is designed to handle timestamped
		  log data, by additionally using (i) mappings supporting
		  validity time specification, and (ii) rules based on metric
		  temporal logic to define temporalised concepts. In this
		  demo we present how Ontop-temporal can be used to
		  facilitate the access to the MIMIC-III critical care unit
		  dataset containing log data on hospital admissions,
		  procedures, and diagnoses. We use the ICD9CM diagnoses
		  ontology and temporal rules formalising the selection of
		  patients for clinical trials taken from the
		  clinicaltrials.gov database. We demonstrate how high-level
		  queries can be answered by Ontop-temporal to identify
		  patients eligible for the trials.},
  booktitle	= {Proceedings of the 27th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1927–1930},
  numpages	= {4},
  keywords	= {ontology-based data access, mimic-iii, metric temporal
		  logic},
  location	= {Torino, Italy},
  series	= {CIKM '18}
}

@InProceedings{	  10.1145/3341105.3374111,
  author	= {Mebrek, Wafaa and Bouzeghoub, Amel},
  title		= {A stream reasoning framework based on a multi-agents
		  model},
  year		= {2020},
  isbn		= {9781450368667},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341105.3374111},
  doi		= {10.1145/3341105.3374111},
  abstract	= {Processing on-the-fly high volume of data streams is
		  increasingly needed. To cope with the heterogeneity of this
		  data, RDF model is more and more being adopted leading to
		  plethora of RDF Stream Processing (RSP) systems and
		  languages dealing with issues such as continuous querying,
		  incremental reasoning and complex event processing (CEP).
		  However, most of them has implemented centralized
		  approaches and therefore suffer from some limitations as
		  collaboration, sharing, expressiveness and scalability.
		  Multi-agents systems have widely proven their worth and
		  efficiency in particular their intrinsic decentralized
		  property along with their cooperation and communication
		  mechanism. In this paper we propose a new framework
		  MAS4MEAN (Multi-Agent System for streaM rEAsoNing) based on
		  a multi-agents model to embrace their benefits and tackle
		  the challenges of increasing the scalability and ease of
		  deployment in highly dynamic environments. A preliminary
		  experimental evaluation with a real-world dataset show
		  promising results when compared to an existing work.},
  booktitle	= {Proceedings of the 35th Annual ACM Symposium on Applied
		  Computing},
  pages		= {509–512},
  numpages	= {4},
  keywords	= {stream reasoning, stream processing, multi-agents systems,
		  RDF streams},
  location	= {Brno, Czech Republic},
  series	= {SAC '20}
}

@InProceedings{	  10.1145/3631802.3631826,
  author	= {Winkelnkemper, Felix and Schulte, Carsten},
  title		= {Reconstructing the Digital – An Architectural
		  Perspective for Non-Engineers (Discussion Paper)},
  year		= {2024},
  isbn		= {9798400716539},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631802.3631826},
  doi		= {10.1145/3631802.3631826},
  abstract	= {Knowing and understanding the world of digital artefacts
		  we are living in is a requirement for everyone today,
		  regardless of their general interest in technology.
		  Computer science education, however, often treats pupils as
		  if they all wanted to become engineers. Educational models
		  of computer science are rather not targeted at
		  understanding the behaviour of the digital world, but at
		  constructing it. Our paper complements such classical
		  approaches with an Ontology of the Digital as an approach
		  which reconstructs digital artefacts and thereby creates a
		  model which helps to understand and explain the
		  technological potentials of digital artefacts without
		  relying on minute details of the engineering discipline of
		  computing.},
  booktitle	= {Proceedings of the 23rd Koli Calling International
		  Conference on Computing Education Research},
  articleno	= {21},
  numpages	= {7},
  keywords	= {CS for all, digital artefacts, explanation model,
		  ontology, technological knowledge},
  location	= {Koli, Finland},
  series	= {Koli Calling '23}
}

@InProceedings{	  10.1145/3434780.3436541,
  author	= {Hu, Shuyang and Chew, Esyin},
  title		= {The Investigation and Novel Trinity Modeling for Museum
		  Robots},
  year		= {2021},
  isbn		= {9781450388504},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3434780.3436541},
  doi		= {10.1145/3434780.3436541},
  abstract	= {There have been interactive museum tour-guide robots under
		  investigation since the end of twentieth century. However,
		  those researches are limited to localisations and
		  telepresence with less humanoids deployment or
		  human-touched features. This research used a humanoid robot
		  to develop the first Welsh-based museum robots that can
		  speak bilingual, English and Welsh, addressing the design
		  method, constraints and initial experimental results. This
		  article introduces the definition and development of robots
		  and service robots with three aims: 1) to design and pilot
		  service robots in a public educational environment,
		  National Museum of Wales, Cardiff. This is to develop a
		  semi-autonomous robotic museum programme that can guide and
		  educate visitors, explain exhibits and perform surveys
		  based on a higher level of robot technology platform; 2) to
		  perform voice interaction with the visitors and provides an
		  inquiry and corporate branding services by the robotic
		  programme with initial user experiences inquiry; and 3) to
		  provide educational service robot design recommendation and
		  a novel Trinity conceptual model and design principles to
		  the sector based on the findings from objectives 1 and 2,
		  for preliminary study and research on artificial
		  intelligence in education and social cognition. Case study
		  research method is used to lays a reference for museum
		  robotic research, and it is easy to expand functionally,
		  that is, secondary development; developing an autonomous
		  humanoid robot for museum visit and interactive
		  education.},
  booktitle	= {Eighth International Conference on Technological
		  Ecosystems for Enhancing Multiculturality},
  pages		= {21–28},
  numpages	= {8},
  keywords	= {service robot, educational robot, Nao robot, Museum
		  robot},
  location	= {Salamanca, Spain},
  series	= {TEEM'20}
}

@InProceedings{	  10.1145/3714334.3714388,
  author	= {Li, Haili and Wang, Xiaodong and Zhou, Yunyan and Liu,
		  Weijie and Pan, Shilong},
  title		= {An Overview of Event Extraction Methods based on Semantic
		  Disambiguation},
  year		= {2025},
  isbn		= {9798400711237},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3714334.3714388},
  doi		= {10.1145/3714334.3714388},
  abstract	= {Event extraction is a fundamental and complex task in
		  information extraction, aiming at automatically identifying
		  and extracting structured event information from
		  unstructured text. However, the same word may have
		  different meanings in different contexts, necessitating
		  semantic disambiguation during the classification process.
		  The polysemy of triggers presents a significant challenge
		  to accurate EE. Existing surveys primarily focus on
		  different domains, fields, or technical paradigms, but none
		  provide a systematic summary of the semantic disambiguation
		  techniques employed. This study classifies these techniques
		  into three categories based on the underlying technical
		  frameworks and offers a comprehensive overview of the most
		  advanced methods in each category. Finally, we summarize
		  the application scenarios, strengths, and limitations of
		  existing approaches, and discuss potential directions for
		  future research.},
  booktitle	= {Proceedings of the 2024 2nd International Conference on
		  Artificial Intelligence, Systems and Network Security},
  pages		= {319–326},
  numpages	= {8},
  keywords	= {deep neural networks, event extraction, information
		  extraction, natural language processing, semantic
		  disambiguation},
  location	= { },
  series	= {AISNS '24}
}

@InProceedings{	  10.1145/3373744.3373745,
  author	= {Franco, Aldrin Jaramillo and Giraldo, Germ\'{a}n Urrego},
  title		= {On the Use of Business Process Models to Discover System
		  Requirements},
  year		= {2020},
  isbn		= {9781450372343},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3373744.3373745},
  doi		= {10.1145/3373744.3373745},
  abstract	= {A framework of generic categories of process activities is
		  adopted as a framework of generic categories of system
		  goals in order to guide the reasoning of system analysts
		  and stakeholders for the discovery of system goals from
		  business process models. The categories of process
		  activities are organized in four essential aspects of the
		  process concept: input, evolution, evaluation and decision
		  and output. These categories are characterized by verbs
		  which offer a semantic diversity that clarifies the field
		  of reasoning and guides the discovery of goals. This
		  article proposes an approach for obtaining system goals
		  from business process models; the proposal is illustrated
		  with a diverse and rich set of pertinent goals discovered
		  for a system supporting a "booking a flight" process.},
  booktitle	= {Proceedings of the 2019 11th International Conference on
		  Information Management and Engineering},
  pages		= {1–9},
  numpages	= {9},
  keywords	= {system goals elicitation, business process models reuse,
		  Business processes},
  location	= {London, United Kingdom},
  series	= {ICIME 2019}
}

@InProceedings{	  10.1145/3301761.3301775,
  author	= {Yeh, Jian-hua and Huang, Xin-mao},
  title		= {BKOntoVR: A Virtual Reality Exhibition System for
		  Biographic Ontology-Based Semantic Structure},
  year		= {2018},
  isbn		= {9781450361279},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3301761.3301775},
  doi		= {10.1145/3301761.3301775},
  abstract	= {In this article, we illustrate some of the semantic
		  web-related technologies and design a virtual exhibition
		  system for a set of ontology knowledge structures based on
		  biographical history, which we call BKOntoVR. This is an
		  official framework for processing and presenting
		  biographical history-related messages on the semantic web
		  with virtual-reality technology, including biographical
		  events, time and space relationships, related personal
		  messages, and more. We elaborate on this ontology knowledge
		  architecture and explain how to use our ontological
		  structure called BKOnto as a basis for domain-specific
		  knowledge to support virtual presentation. Information
		  management is becoming an important part of cultural
		  collections related technologies, from the management of
		  personal collections to the establishment of large,
		  decentralized "semantic" databases. These semantic
		  databases can be used to a certain extent using semantic
		  web technology to process and construct a
		  machine-understandable data network. Such a data network
		  can be linked to the referenced knowledge structure to give
		  a concept and a relationship form specification associated
		  with a set of descriptive objects in the definition domain
		  (person, thing, place, etc.) - that is, link to its
		  meaning. Biographical history of the specific characters is
		  through the life and other areas of a systematic
		  description of the introduction of a text that form. In
		  this paper, we overview some of the Semantic Web-related
		  technologies and describe a cognitive knowledge structure
		  for biographical knowledge representation based on OWL
		  markup language, we call it BKOnto. This is a formal
		  framework for dealing with information about biographical
		  history on the semantic web, including biographical events,
		  temporal and spatial relationships, related information of
		  persons, and so on. We describe this ontology knowledge
		  structure and explain how BKOnto can act as a basis for
		  more domain-specific knowledge representation. In this
		  study, we further present such cultural collections in a
		  virtual reality form, by transforming the ontology
		  cognitive architecture data into a virtual reality
		  exhibition space, allowing users to present the semantic
		  structure in the form of multimedia in a three-dimensional
		  space more easily. Such a form of presentation can be used
		  in the virtual exhibition of the museum, making it easier
		  for museums to organize cultural collections of semantic
		  structures and exert their influence through the Internet.
		  The empirical study also uses the Mackay Digital Archives
		  Project (http://dlm.csie.au.edu.tw/) as a source of
		  information to demonstrate the ontology knowledge building
		  process of Mackay's biographical stories, as well as
		  related Digital collection of information.},
  booktitle	= {Proceedings of the 2018 2nd International Conference on
		  Software and E-Business},
  pages		= {69–73},
  numpages	= {5},
  keywords	= {virtual reality, temporal event, semantic web, ontology,
		  museum exhibition, Biographical knowledge},
  location	= {Zhuhai, China},
  series	= {ICSEB '18}
}

@InProceedings{	  10.1145/3436829.3436862,
  author	= {Youssef, Clara K. and Ahmed, Farida M. and Hashem, Hashem
		  M. and Talaat, Veronia E. and Shorim, Nada and Ghanim,
		  Taraggy},
  title		= {GQM-based Tree Model for Automatic Recommendation of
		  Design Pattern Category},
  year		= {2021},
  isbn		= {9781450377218},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3436829.3436862},
  doi		= {10.1145/3436829.3436862},
  abstract	= {Software Design Patterns (DP) are formal approaches that
		  propose generic reusable solutions to different design
		  problems. Building DP automatic recommendation system is
		  one of the most challenging topics in the field of software
		  industry to improve the final software quality. Proposing a
		  DP for a design problem requires good base knowledge about
		  each DP and its functionality. In this paper, we propose an
		  approach that automatically recommends the appropriate
		  design pattern category. The proposed approach is a Goal
		  Question Metric (GQM) based tree model of questions. The
		  software engineer answers these questions based on the user
		  requirements, and finally the approach recommends the
		  category of the suitable DP category based on our designed
		  tree model. The GQM is responsible for weight calculation
		  process at each node based on the questions' answers. The
		  software engineer is responsible for delivering the user
		  requirements to our system, via answering the proposed
		  model. The precision and accuracy obtained by our system is
		  80\% while the recall is 100\%.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Software and Information Engineering},
  pages		= {126–130},
  numpages	= {5},
  keywords	= {Recommendation System, Goal-Question-Metric, Design
		  Patterns, Design Pattern selection, Decision Trees},
  location	= {Cairo, Egypt},
  series	= {ICSIE '20}
}

@Article{	  10.1145/3141772,
  author	= {Siabato, Willington and Claramunt, Christophe and Ilarri,
		  Sergio and Manso-Callejo, Miguel Angel},
  title		= {A Survey of Modelling Trends in Temporal GIS},
  year		= {2018},
  issue_date	= {March 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {51},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3141772},
  doi		= {10.1145/3141772},
  abstract	= {The main achievements of spatio-temporal modelling in the
		  field of Geographic Information Science that spans the past
		  three decades are surveyed. This article offers an overview
		  of: (i) the origins and history of Temporal Geographic
		  Information Systems (T-GIS); (ii) relevant spatio-temporal
		  data models proposed; (iii) the evolution of
		  spatio-temporal modelling trends; and (iv) an analysis of
		  the future trends and developments in T-GIS. It also
		  presents some current theories and concepts that have
		  emerged from the research performed, as well as a summary
		  of the current progress and the upcoming challenges and
		  potential research directions for T-GIS. One relevant
		  result of this survey is the proposed taxonomy of
		  spatio-temporal modelling trends, which classifies 186
		  modelling proposals surveyed from more than 1,450
		  articles.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {30},
  numpages	= {41},
  keywords	= {time geography, temporal models, temporal GIS, survey,
		  spatio-temporal databases, literature review,
		  Spatio-temporal models}
}

@InProceedings{	  10.1145/3338906.3338974,
  author	= {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and
		  St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
  title		= {Principles of feature modeling},
  year		= {2019},
  isbn		= {9781450355728},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3338906.3338974},
  doi		= {10.1145/3338906.3338974},
  abstract	= {Feature models are arguably one of the most intuitive and
		  successful notations for modeling the features of a
		  variant-rich software system. Feature models help
		  developers to keep an overall understanding of the system,
		  and also support scoping, planning, development, variant
		  derivation, configuration, and maintenance activities that
		  sustain the system's long-term success. Unfortunately,
		  feature models are difficult to build and evolve. Features
		  need to be identified, grouped, organized in a hierarchy,
		  and mapped to software assets. Also, dependencies between
		  features need to be declared. While feature models have
		  been the subject of three decades of research, resulting in
		  many feature-modeling notations together with automated
		  analysis and configuration techniques, a generic set of
		  principles for engineering feature models is still missing.
		  It is not even clear whether feature models could be
		  engineered using recurrent principles. Our work shows that
		  such principles in fact exist. We analyzed feature-modeling
		  practices elicited from ten interviews conducted with
		  industrial practitioners and from 31 relevant papers. We
		  synthesized a set of 34 principles covering eight different
		  phases of feature modeling, from planning over model
		  construction, to model maintenance and evolution. Grounded
		  in empirical evidence, these principles provide practical,
		  context-specific advice on how to perform feature modeling,
		  describe what information sources to consider, and
		  highlight common characteristics of feature models. We
		  believe that our principles can support researchers and
		  practitioners enhancing feature-modeling tooling,
		  synthesis, and analyses techniques, as well as scope future
		  research.},
  booktitle	= {Proceedings of the 2019 27th ACM Joint Meeting on European
		  Software Engineering Conference and Symposium on the
		  Foundations of Software Engineering},
  pages		= {62–73},
  numpages	= {12},
  keywords	= {software product lines, modeling principles, Feature
		  models},
  location	= {Tallinn, Estonia},
  series	= {ESEC/FSE 2019}
}

@InProceedings{	  10.5555/3213214.3213226,
  author	= {Bocciarelli, Paolo and D'Ambrogio, Andrea and Giglio,
		  Andrea and Paglia, Emiliano},
  title		= {Model transformation services for MSaaS platforms},
  year		= {2018},
  isbn		= {9781510860186},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {The development of complex systems may take advantage by
		  the introduction of Modeling \&amp; Simulation (M&amp;S)
		  based analysis techniques from the early stages of the
		  system lifecycle. However, M&amp;S approaches typically
		  require significant know-how and effort, as well as
		  remarkable resources to setup and maintain proper execution
		  platforms. Such issues can be tackled by use of automated
		  approaches based on model transformation, which reduce the
		  simulation model building effort, and by the M&amp;S as a
		  Service (MSaaS) paradigm, which brings the benefits of
		  service-oriented architectures and cloud computing into the
		  M&amp;S field, so to reduce the costs of M&amp;S efforts.
		  In this paper, we show how MSaaS platforms can be
		  effectively extended by introducing model transformation
		  services, with specific application to the M&amp;S-based
		  analysis of complex systems specified by use of SysML. The
		  paper also describes a catalog of currently available model
		  transformation services, in order to show how the proposed
		  MSaaS platform may ease the introduction of M&amp;S
		  approaches at any stage of the system development cycle.},
  booktitle	= {Proceedings of the Model-Driven Approaches for Simulation
		  Engineering Symposium},
  articleno	= {12},
  numpages	= {12},
  keywords	= {model-driven, model transformation, cloud computing,
		  MSaaS, MDA},
  location	= {Baltimore, Maryland},
  series	= {Mod4Sim '18}
}

@InProceedings{	  10.5555/3213214.3213218,
  author	= {Durak, Umut and M\"{u}ller, David and M\"{o}cke, Florian
		  and Koch, Claus B.},
  title		= {Modeling and simulation based development of an enhanced
		  ground proximity warning system for multicore targets},
  year		= {2018},
  isbn		= {9781510860186},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {The advances in Cyber-Physical Systems (CPS) are also
		  effecting the aeronautics. The growth of the cyber layer in
		  aircraft is demanding higher throughput and eventually
		  multi-core systems are becoming topics of interest. The
		  development of parallel real-time systems for multicore
		  processors requires new approaches in model-based design
		  and simulation-based verification. The Enhanced Ground
		  Proximity Warning System (EGPWS) is a terrain awareness
		  system that creates aural and visual warnings for the pilot
		  to prevent Controlled Flight into Terrain (CFIT). This
		  paper presents a multi-core parallelization workflow and a
		  corresponding x-in-the-loop testing pipeline for
		  model-based development of an EGPWS.},
  booktitle	= {Proceedings of the Model-Driven Approaches for Simulation
		  Engineering Symposium},
  articleno	= {4},
  numpages	= {12},
  keywords	= {x-in-the-loop testing, multi-core parallelization,
		  model-based development, enhanced ground proximity warning
		  systems},
  location	= {Baltimore, Maryland},
  series	= {Mod4Sim '18}
}

@InProceedings{	  10.1145/3652620.3688214,
  author	= {Balaban, Mira and Hamann, Lars and Khais, Gil and Saad,
		  Amiel Amram and Maraee, Azzam and Sturm, Arnon},
  title		= {Mediation-Based MLM in USE},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688214},
  doi		= {10.1145/3652620.3688214},
  abstract	= {Multi Level Modeling (MLM) has been around in the modeling
		  community for over twenty years. It has attracted much
		  attention, both on theoretical and practical grounds.
		  Multiple approaches have emerged, with different support
		  for MLM concepts on the levels of syntax, semantics and
		  pragmatics. MLM tools support a variety of applications,
		  ranging from ontology specification to software
		  modeling.This paper introduces the MedMLM-USE tool, which
		  implements the Mediation-based MLM theory as an extension
		  of the USE tool. The USE tool has been selected due to its
		  open, well-structured architecture. The MedMLM-USE
		  application extends: (1) the well-defined meta-model that
		  is supported by USE, with MedMLM concepts; (2) the USE
		  modeling services to support MLM-models. This paper
		  describes the extended meta-model and services, provides
		  examples, defines an inheritance semantics for instance-of,
		  and discusses engineering difficulties.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {818–827},
  numpages	= {10},
  keywords	= {multi-level modeling, MLM semantics, MLM implementation},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.5555/3447080.3447095,
  author	= {Drumheller, William R. and Conner, David C.},
  title		= {Online system modeling and documentation using ROS
		  snapshot},
  year		= {2020},
  issue_date	= {October 2020},
  publisher	= {Consortium for Computing Sciences in Colleges},
  address	= {Evansville, IN, USA},
  volume	= {36},
  number	= {3},
  issn		= {1937-4771},
  abstract	= {Robotic systems are complex systems running a number of
		  software components to control the hardware. The Robot
		  Operating System (ROS) is often used in such systems. The
		  integration of these software components, even when the
		  interfaces are heavily documented, can become a daunting
		  task. As part of a typical Software Development Life Cycle,
		  requirements and interfaces change for entire software
		  systems as well as the individual software components that
		  make up those systems.Model-Integrated Computing (MIC) can
		  be used to manage such complexity, and provides a means for
		  validating the system integration throughout its lifecycle.
		  Unfortunately, current MIC tools for ROS systems are
		  limited and lack the ability to automatically gather
		  information needed to make models for ROS entities. This
		  paper presents a new tool, ROS Snapshot, that captures a
		  snapshot of an existing ROS-based system during runtime,
		  and fully documents the system using specified metamodels
		  for each ROS entity. The resulting system model can then be
		  used to document the current state of the system, which is
		  especially important when upgrading versions (e.g.
		  conversion from ROS 1.0 to ROS 2.0). We present a set of
		  current applications for the ROS Snapshot tool, as well as
		  future development plans to integrate the acquired system
		  model into a full MIC system.},
  journal	= {J. Comput. Sci. Coll.},
  month		= oct,
  pages		= {128–141},
  numpages	= {14}
}

@InProceedings{	  10.1145/3700486.3700511,
  author	= {Gao, Jingjing and Wang, Chunyan and Wang, Ruixiang},
  title		= {Utilizing Deep Learning for Named Entity Recognition in
		  Ancient Chinese Stroke Medical Cases},
  year		= {2024},
  isbn		= {9798400710063},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3700486.3700511},
  doi		= {10.1145/3700486.3700511},
  abstract	= {Objective: Based on the deep learning method, to construct
		  the named entity recognition model of Ming and Qing dynasty
		  stroke medical cases, to extract as comprehensive as
		  possible the effective information in the Ming and Qing
		  dynasty stroke medical cases, and to realise the
		  transmission and sharing of knowledge.Method: (1) Collation
		  of textual data from ancient stroke cases and comprehensive
		  information labelling of the texts. (2) Train a
		  Bert-BiLSTM-CRF model of ancient stroke medical cases based
		  on the annotated dataset. (3) The constructed model was
		  evaluated using recall and F1 value.Results: The
		  Bert-BiLSTM-CRF model for stroke medical cases was
		  successfully constructed, and automatic identification of
		  key entities was achieved. The evaluation results show that
		  among the 17 named entities designed in this study, the
		  model has a higher accuracy rate than the others for three
		  entities, namely, Chinese medicine, dose, and pulse, and
		  their F1 values are 85\%, 90\%, and 85\%,
		  respectively.Conclusion: The Bert-BiLSTM-CRF model of
		  ancient stroke cases constructed in this study achieves the
		  automatic identification of entities in medical cases,
		  verifies the effectiveness of NER technology in the
		  processing of ancient Chinese medical books, and provides a
		  reference for research in similar fields.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Biomedicine and Intelligent Technology},
  pages		= {152–157},
  numpages	= {6},
  keywords	= {Ancient medical cases, Deep learning, Named entity
		  recognition, Natural language processing, Stroke},
  location	= { },
  series	= {ICBIT '24}
}

@InProceedings{	  10.1145/3356991.3365474,
  author	= {Palumbo, Rachel and Thompson, Laura and Thakur, Gautam},
  title		= {SONET: a semantic ontological network graph for managing
		  points of interest data heterogeneity},
  year		= {2019},
  isbn		= {9781450369602},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3356991.3365474},
  doi		= {10.1145/3356991.3365474},
  abstract	= {Scalability, standardization, and management are important
		  issues when working with very large Volunteered Geographic
		  Information (VGI). VGI is a rich and valuable source of
		  Points of Interest (POI) information, but its inherent
		  heterogeneity in content, structure, and scale across
		  sources present major challenges for interlinking data
		  sources for analysis. To be useful at scale, the raw
		  information needs to be transformed into a standardized
		  schema that can be easily and reliably used by data
		  analysts. In this work, we tackle the problem of unifying
		  POI categories (e.g. restaurants, temple, and hotel) across
		  multiple data sources to aid in improving land use maps and
		  population distribution estimation as well as support data
		  analysts wishing to fuse multiple data sources with the
		  OpenStreetMap (OSM) mapping platform or working with
		  projects that are already configured in the OSM schema and
		  wish to add additional sources of information. Graph theory
		  and its implementation through the SONET graph database,
		  provides a programmatic way to organize, store, and
		  retrieve standardized POI categories at multiple levels of
		  abstraction. Additionally, it addresses category
		  heterogeneity across data sources by standardizing and
		  managing categories in a way that makes cross-domain
		  analysis possible.},
  booktitle	= {Proceedings of the 3rd ACM SIGSPATIAL International
		  Workshop on Geospatial Humanities},
  articleno	= {6},
  numpages	= {6},
  keywords	= {points of interest, openstreetmap, ontology, graph
		  database, big data},
  location	= {Chicago, Illinois},
  series	= {GeoHumanities '19}
}

@InProceedings{	  10.1145/3534678.3539443,
  author	= {Huang, Jiaxin and Meng, Yu and Han, Jiawei},
  title		= {Few-Shot Fine-Grained Entity Typing with Automatic Label
		  Interpretation and Instance Generation},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539443},
  doi		= {10.1145/3534678.3539443},
  abstract	= {We study the problem of few-shot Fine-grained Entity
		  Typing (FET), where only a few annotated entity mentions
		  with contexts are given for each entity type. Recently,
		  prompt-based tuning has demonstrated superior performance
		  to standard fine-tuning in few-shot scenarios by
		  formulating the entity type classification task as a
		  ''fill-in-the-blank'' problem. This allows effective
		  utilization of the strong language modeling capability of
		  Pre-trained Language Models (PLMs). Despite the success of
		  current prompt-based tuning approaches, two major
		  challenges remain: (1) the verbalizer in prompts is either
		  manually designed or constructed from external knowledge
		  bases, without considering the target corpus and label
		  hierarchy information, and (2) current approaches mainly
		  utilize the representation power of PLMs, but have not
		  explored their generation power acquired through extensive
		  general-domain pre-training. In this work, we propose a
		  novel framework for few-shot FET consisting of two modules:
		  (1) an entity type label interpretation module
		  automatically learns to relate type labels to the
		  vocabulary by jointly leveraging few-shot instances and the
		  label hierarchy, and (2) a type-based contextualized
		  instance generator produces new instances based on given
		  instances to enlarge the training set for better
		  generalization. On three benchmark datasets, our model
		  outperforms existing methods by significant margins.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {605–614},
  numpages	= {10},
  keywords	= {entity typing, few-shot learning, prompt-based learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@Article{	  10.1145/3299887.3299892,
  author	= {Hirzel, Martin and Baudart, Guillaume and Bonifati, Angela
		  and Della Valle, Emanuele and Sakr, Sherif and Akrivi
		  Vlachou, Akrivi},
  title		= {Stream Processing Languages in the Big Data Era},
  year		= {2018},
  issue_date	= {June 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {47},
  number	= {2},
  issn		= {0163-5808},
  url		= {https://doi.org/10.1145/3299887.3299892},
  doi		= {10.1145/3299887.3299892},
  abstract	= {This paper is a survey of recent stream processing
		  languages, which are programming languages for writing
		  applications that analyze data streams. Data streams, or
		  continuous data flows, have been around for decades. But
		  with the advent of the big-data era, the size of data
		  streams has increased dramatically. Analyzing big data
		  streams yields immense advantages across all sectors of our
		  society. To analyze streams, one needs to write a stream
		  processing application. This paper showcases several
		  languages designed for this purpose, articulates underlying
		  principles, and outlines open challenges.},
  journal	= {SIGMOD Rec.},
  month		= dec,
  pages		= {29–40},
  numpages	= {12}
}

@InProceedings{	  10.1145/3747912.3747914,
  author	= {Xie, Weiming and Yao, Zhaomin and Bai, Xiaozhou and Mai,
		  Lang and Zhan, Ying and Wu, Xiaodan and Dai, Yingxin and
		  Pei, Yusong and Zhang, Guoxu and Wang, Zhiguo},
  title		= {A Novel Artificial Intelligence Voice Electronic Medical
		  Record Based on Blockchain},
  year		= {2025},
  isbn		= {9798400715136},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3747912.3747914},
  doi		= {10.1145/3747912.3747914},
  abstract	= {Nowadays, the global digital transformation of healthcare
		  is advancing rapidly with the help of technologies such as
		  electronic medical records, telemedicine, and mobile
		  medical applications. However, there are still challenges
		  in EMR interoperability, security, and data exchange. To
		  address these existing limitations, This study proposes a
		  voice electronic medical record system driven by artificial
		  intelligence and blockchain, which is designed to improve
		  clinical records and nursing coordination. This system
		  adopts a dedicated deep learning architecture. It
		  transcribe the conversations between doctors and patients
		  into text, and then uses natural language processing to
		  extract the relevant medical information. At the same time,
		  it also provides diagnostic prompts, which can reduce the
		  risk of misdiagnosis. Doctors can view and edit these
		  summaries generated by artificial intelligence. Then safely
		  record them on the decentralized crypto blockchain ledger.
		  With federated learning, the model can be continuously
		  improved in multiple centers without infringing on data
		  privacy. This solution integrates automatic speech
		  recognition, distributed ledger technology, and
		  collaborative deep learning, aiming to enhance the EMR
		  efficiency, security, data integrity, and care continuity
		  of medical institutions. The combination of blockchain
		  technology and artificial intelligence technology holds
		  great potential. It can transform fragmented health data
		  into portable and interoperable records under patient
		  control, thus bringing strategic advantages to the health
		  system that is undergoing a comprehensive digital
		  transformation.},
  booktitle	= {Proceedings of the 2025 International Conference on
		  Software Engineering and Computer Applications},
  pages		= {12–19},
  numpages	= {8},
  keywords	= {Artificial Intelligence, Blockchain, Digital Healthcare,
		  Electronic Medical Records, Speech Recognition},
  location	= { },
  series	= {SECA '25}
}

@InProceedings{	  10.1145/3382025.3414955,
  author	= {Ananieva, Sofia and Greiner, Sandra and K\"{u}hn, Thomas
		  and Kr\"{u}ger, Jacob and Linsbauer, Lukas and Gr\"{u}ner,
		  Sten and Kehrer, Timo and Klare, Heiko and Koziolek, Anne
		  and L\"{o}nn, Henrik and Krieter, Sebastian and Seidl,
		  Christoph and Ramesh, S. and Reussner, Ralf and
		  Westfechtel, Bernhard},
  title		= {A conceptual model for unifying variability in space and
		  time},
  year		= {2020},
  isbn		= {9781450375696},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3382025.3414955},
  doi		= {10.1145/3382025.3414955},
  abstract	= {Software engineering faces the challenge of developing and
		  maintaining systems that are highly variable in space
		  (concurrent variations of the system at a single point in
		  time) and time (sequential variations of the system due to
		  its evolution). Recent research aims to address this need
		  by managing variability in space and time simultaneously.
		  However, such research often relies on nonuniform
		  terminologies and a varying understanding of concepts, as
		  it originates from different communities: software
		  product-line engineering and software configuration
		  management. These issues complicate the communication and
		  comprehension of the concepts involved, impeding the
		  development of techniques to unify variability in space and
		  time. To tackle this problem, we performed an iterative,
		  expert-driven analysis of existing tools to derive the
		  first conceptual model that integrates and unifies
		  terminologies and concepts of both dimensions of
		  variability. In this paper, we present the unification
		  process of concepts for variability in space and time, and
		  the resulting conceptual model itself. We show that the
		  conceptual model achieves high coverage and that its
		  concepts are of appropriate granularity with respect to the
		  tools for managing variability in space, time, or both that
		  we considered. The conceptual model provides a
		  well-defined, uniform terminology that empowers researchers
		  and developers to compare their work, clarifies
		  communication, and prevents redundant developments.},
  booktitle	= {Proceedings of the 24th ACM Conference on Systems and
		  Software Product Line: Volume A - Volume A},
  articleno	= {15},
  numpages	= {12},
  keywords	= {version control, variability, revision management, product
		  lines},
  location	= {Montreal, Quebec, Canada},
  series	= {SPLC '20}
}

@InProceedings{	  10.1145/3626772.3657989,
  author	= {Cai, Qingpeng and Zhao, Xiangyu and Pan, Ling and Xin, Xin
		  and Huang, Jin and Zhang, Weinan and Zhao, Li and Yin,
		  Dawei and Yang, Grace Hui},
  title		= {AgentIR: 1st Workshop on Agent-based Information
		  Retrieval},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657989},
  doi		= {10.1145/3626772.3657989},
  abstract	= {Information retrieval (IR) systems have become an
		  essential component in modern society to help users find
		  useful information, which consists of a series of processes
		  including query expansion, item recall, item ranking and
		  re-ranking, etc. Based on the ranked information list,
		  users can provide their feedbacks. Such an interaction
		  process between users and IR systems can be naturally
		  formulated as a decision-making problem, which can be
		  either one-step or sequential. In the last ten years, deep
		  reinforcement learning (DRL) has become a promising
		  direction for decision-making, since DRL utilizes the high
		  model capacity of deep learning for complex decision-making
		  tasks. On the one hand, there have been emerging research
		  works focusing on leveraging DRL for IR tasks. However, the
		  fundamental information theory under DRL settings, the
		  challenge of RL methods for Industrial IR tasks, or the
		  simulations of DRL-based IR systems, has not been deeply
		  investigated. On the other hand, the emerging LLM provides
		  new opportunities for optimizing and simulating IR systems.
		  To this end, we propose the first Agent-based IR workshop
		  at SIGIR 2024, as a continuation from one of the most
		  successful IR workshops, DRL4IR. It provides a venue for
		  both academia researchers and industry practitioners to
		  present the recent advances of both DRL-based IR systems
		  and LLM-based IR systems from the agent-based IR's
		  perspective, to foster novel research, interesting
		  findings, and new applications.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3025–3028},
  numpages	= {4},
  keywords	= {agent-based information retrieval, drl, llm},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3348445.3351305,
  author	= {Iqtidar, Khushbakht and Azam, Farooque and Anwar, Muhammad
		  Waseem and Amjad, Anam},
  title		= {Model-Driven approach to Integrate Requirements for
		  Safety-Critical Systems},
  year		= {2019},
  isbn		= {9781450371957},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3348445.3351305},
  doi		= {10.1145/3348445.3351305},
  abstract	= {A sophisticated approach is required to elicit
		  requirements for Safety-Critical Systems (SCS). Incomplete,
		  inconsistent or ambiguous requirements can result in many
		  safety-critical catastrophes. While specifying a SCS, it is
		  one of the greatest challenges to extract a complete set of
		  consistent requirements. To overcome this problem, we have
		  proposed a meta-model in this paper for integration of
		  requirements which were specified using several
		  representations to ensure the completeness of requirements.
		  The idea is to use a database, for the integration of the
		  extracted data that will implement the meta-model for the
		  requirements. Problems like inconsistencies and ambiguities
		  can be identified and solved according to the defined
		  meta-model, which will help us in the development and
		  testing phase and will minimize the project chaos. We have
		  validated the proposed methodology with using insulin pump
		  system case study. For the Requirement Engineering process
		  of Safety-Critical Systems, the proposed approach is highly
		  beneficial as final product will result in fewer defects,
		  reduced development cost by avoiding rework, easy
		  maintenance, increased satisfaction of the stakeholders and
		  possibility of faster delivery of the safety critical
		  system.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Computer and Communications Management},
  pages		= {58–62},
  numpages	= {5},
  keywords	= {model-driven requirement engineering, meta-model,
		  Safety-Critical System (SCS)},
  location	= {Bangkok, Thailand},
  series	= {ICCCM '19}
}

@Article{	  10.1145/3479155,
  author	= {Katyayan, Pragya and Joshi, Nisheeth},
  title		= {Development of Automatic Rule-based Semantic Tagger and
		  Karaka Analyzer for Hindi},
  year		= {2021},
  issue_date	= {March 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3479155},
  doi		= {10.1145/3479155},
  abstract	= {Hindi is the third most-spoken language in the world (615
		  million speakers) and has the fourth highest native
		  speakers (341 million). It is an inflectionally rich and
		  relatively free word-order language with an immense
		  vocabulary set. Despite being such a celebrated language
		  across the globe, very few Natural Language Processing
		  (NLP) applications and tools have been developed to support
		  it computationally. Moreover, most of the existing ones are
		  not efficient enough due to the lack of semantic
		  information (or contextual knowledge). Hindi grammar is
		  based on Paninian grammar and derives most of its rules
		  from it. Paninian grammar very aggressively highlights the
		  role of karaka theory in free-word order languages. In this
		  article, we present an application that extracts all
		  possible karakas from simple Hindi sentences with an
		  accuracy of 84.2\% and an F1 score of 88.5\%. We consider
		  features such as Parts of Speech tags, post-position
		  markers (vibhaktis), semantic tags for nouns and syntactic
		  structure to grab the context in different-sized word
		  windows within a sentence. With the help of these features,
		  we built a rule-based inference engine to extract karakas
		  from a sentence. The application takes in a text file with
		  clean (without punctuation) simple Hindi sentences and
		  gives back karaka tagged sentences in a separate text file
		  as output.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {39},
  numpages	= {25},
  keywords	= {language resource, feature extraction, semantic tagging,
		  Karaka analyzer}
}

@InProceedings{	  10.1145/3412841.3442056,
  author	= {Halilaj, Lavdim and L\"{u}ttin, J\"{u}rgen and Rothermel,
		  Susanne and Arumugam, Santhosh Kumar and Dindorkar, Ishan},
  title		= {Towards a knowledge graph-based approach for context-aware
		  points-of-interest recommendations},
  year		= {2021},
  isbn		= {9781450381048},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3412841.3442056},
  doi		= {10.1145/3412841.3442056},
  abstract	= {Context-aware Recommender Systems (CARS) are becoming an
		  integral part of the everyday life by providing users the
		  ability to retrieve relevant information based on their
		  contextual situation. To increase the predictive power
		  considering many parameters, such as mood, hunger level and
		  user preferences, information from heterogeneous sources
		  should be leveraged. However, these data sources are
		  typically isolated and unexplored and the efforts for
		  integrating them are exacerbated by variety of data
		  structures used for their modelling and costly
		  pre-processing operations. We propose a Knowledge
		  Graph-based approach to allow integration of data according
		  to abstract semantic models for Points-of-Interests (POI)s
		  recommendation scenarios. By enriching data with
		  information about attributes, relationships and their
		  meaning, additional knowledge can be derived from what
		  already exists. We demonstrate the applicability of the
		  proposed approach with a concrete example showing benefits
		  of the retrieving the dispersed data with a unified access
		  mechanism.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on Applied
		  Computing},
  pages		= {1846–1854},
  numpages	= {9},
  keywords	= {context-aware POI recommendations, knowledge graphs,
		  ontology-based data integration},
  location	= {Virtual Event, Republic of Korea},
  series	= {SAC '21}
}

@InProceedings{	  10.1145/3343031.3351090,
  author	= {Yin, Yifang and Chiou, Meng-Jiun and Liu, Zhenguang and
		  Shrivastava, Harsh and Shah, Rajiv Ratn and Zimmermann,
		  Roger},
  title		= {Multi-Level Fusion based Class-aware Attention Model for
		  Weakly Labeled Audio Tagging},
  year		= {2019},
  isbn		= {9781450368896},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3343031.3351090},
  doi		= {10.1145/3343031.3351090},
  abstract	= {Recognizing ongoing events based on acoustic clues has
		  been a critical research problem for a variety of AI
		  applications. Compared to visual inputs, acoustic cues tend
		  to be less descriptive and less consistent in time domain.
		  The duration of a sound event can be quite short, which
		  creates great difficulties for, especially weakly labeled,
		  audio tagging. To solve these challenges, we present a
		  novel end-to-end multi-level attention model that first
		  makes segment-level predictions with temporal modeling,
		  followed by advanced aggregations along both time and
		  feature domains. Our model adopts class-aware attention
		  based temporal fusion to highlight/suppress the
		  relevant/irrelevant segments to each class. Moreover, to
		  improve the representation ability of acoustic inputs, a
		  new multi-level feature fusion method is proposed to obtain
		  more accurate segment-level predictions, as well as to
		  perform more effective multi-layer aggregation of
		  clip-level predictions. We additionally introduce a weight
		  sharing strategy to reduce model complexity and
		  overfitting. Comprehensive experiments have been conducted
		  on the AudioSet and the DCASE17 datasets. Experimental
		  results show that our proposed method works remarkably well
		  and obtains the state-of-the-art audio tagging results on
		  both datasets. Furthermore, we show that our proposed
		  multi-level fusion based model can be easily integrated
		  with existing systems where additional performance gain can
		  be obtained.},
  booktitle	= {Proceedings of the 27th ACM International Conference on
		  Multimedia},
  pages		= {1304–1312},
  numpages	= {9},
  keywords	= {multi-layer feature fusion, convolutional recurrent neural
		  network, audio tagging, attention-based model},
  location	= {Nice, France},
  series	= {MM '19}
}

@Article{	  10.1145/3397178,
  author	= {Fox, Sarah E. and Menking, Amanda and Eschler, Jordan and
		  Backonja, Uba},
  title		= {Multiples Over Models: Interrogating the Past and
		  Collectively Reimagining the Future of Menstrual
		  Sensemaking},
  year		= {2020},
  issue_date	= {August 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {27},
  number	= {4},
  issn		= {1073-0516},
  url		= {https://doi.org/10.1145/3397178},
  doi		= {10.1145/3397178},
  abstract	= {In this article, we describe our efforts to retrace and
		  reimagine period tracking technology—or, mobile
		  applications designed to support the documentation and
		  quantification of menstrual cycle data. In their current
		  form, these systems often encourage those who menstruate to
		  extract intimate information about the body (e.g.,
		  consistency or color of menstrual flow, physical and
		  emotional symptoms), while promising to predict fertility
		  and offer insight into managing one's period. In doing so,
		  these technologies subtly dictate the forms of knowledge
		  and types of relationships menstruators are expected to
		  establish with their bodies (i.e., transactional or
		  instrumentalized). Through historical analysis and a series
		  of participatory experiments, we offer a vision for
		  menstrual sensemaking that expands on these forms of
		  interaction and ways of knowing to emphasize multiplicity
		  and dimensionality rather than models, predictability, or a
		  user's relation to averages or norms.},
  journal	= {ACM Trans. Comput.-Hum. Interact.},
  month		= sep,
  articleno	= {22},
  numpages	= {24},
  keywords	= {women's health, transgender health, self-tracking,
		  Menstruation}
}

@InProceedings{	  10.1145/3209415.3209427,
  author	= {Androutsopoulou, Aggeliki and Charalabidis, Yannis},
  title		= {A framework for evidence based policy making combining big
		  data, dynamic modelling and machine intelligence},
  year		= {2018},
  isbn		= {9781450354219},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209415.3209427},
  doi		= {10.1145/3209415.3209427},
  abstract	= {Governments and policy makers are striving to respond to
		  contemporary socio-economic challenges, however, often
		  neglecting the human factor and the multidimensionality of
		  policy implications. In this chapter, a framework for
		  evidence based policy making is proposed, which integrates
		  the usage of open big data coming from a multiplicity of
		  sources with policy simulations. It encompasses the
		  application of dynamic modelling methodologies and data
		  mining techniques to extract knowledge from two types of
		  data. On the one hand, objective data such as governmental
		  and statistical data, are used to capture the interlinked
		  policy domains and their underlying casual mechanisms. On
		  the other hand, behavioural patterns and citizens' opinions
		  are extracted from Web 2.0 sources, social media posts,
		  polls and statistical surveys. To combine this multimodal
		  information, our approach suggests a modelling methodology
		  that bases on big data acquisition and processing for the
		  identification of significant factors and counterintuitive
		  interrelations between them, which can be applied in any
		  policy domain. Then, to allow the practical application of
		  the framework an ICT architecture is designed, with the aim
		  to overcome challenges related with big data management and
		  processing. Finally, validation of the approach for driving
		  policy design and implementation in the future in diverse
		  policy domains, is suggested.},
  booktitle	= {Proceedings of the 11th International Conference on Theory
		  and Practice of Electronic Governance},
  pages		= {575–583},
  numpages	= {9},
  keywords	= {policy Modelling, impact assessment, evidence based policy
		  making, dynamic simulation, data mining, behavioural
		  patterns, Big data},
  location	= {Galway, Ireland},
  series	= {ICEGOV '18}
}

@InProceedings{	  10.1145/3746709.3746742,
  author	= {Du, Xinmeng and Wang, Yiruo and Ying, Qunbo and Zeng,
		  Qunyao and Zhang, Yuanjie and Zhao, Li},
  title		= {Construction of visual knowledge graph of crop diseases
		  and pests based on deep learning},
  year		= {2025},
  isbn		= {9798400713163},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3746709.3746742},
  doi		= {10.1145/3746709.3746742},
  abstract	= {In response to the complex entity relationships, data
		  aggregation difficulties, and knowledge sharing challenges
		  in the field of crop pests and diseases, this study
		  leverages the advantages of knowledge graph structure to
		  propose a deep learning-based construction method. This
		  method is grounded in domain ontology and employs a new
		  annotation model to transform entity and relationship
		  extraction into sequence labeling problems, with
		  simultaneous labeling to enhance efficiency. It directly
		  models ternary relationships to address the challenge of
		  extracting overlapping relationships. Using the BERT-BiLSTM
		  +CRF end-to-end model for experimentation, its F1 score
		  reaches 91.34\%, outperforming various classic models.
		  Finally, the extracted knowledge is stored in a Neo4j graph
		  database to achieve knowledge visualization and inference,
		  providing a high-quality knowledge base for downstream
		  applications.},
  booktitle	= {Proceedings of the 2025 6th International Conference on
		  Computer Information and Big Data Applications},
  pages		= {178–182},
  numpages	= {5},
  keywords	= {Crop, deep learning, entity relationship joint extraction,
		  knowledge graph, model, pest and disease},
  location	= { },
  series	= {CIBDA '25}
}

@InProceedings{	  10.1145/3425269.3425272,
  author	= {Silva, Jorge Luiz Machado da and de Fran\c{c}a, Breno B.
		  Nicolau and Rubira, Cec\'{\i}lia Mary Fischer},
  title		= {Generating Trustworthiness Adaptation Plans Based on
		  Quality Models for Cloud Platforms},
  year		= {2020},
  isbn		= {9781450387545},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3425269.3425272},
  doi		= {10.1145/3425269.3425272},
  abstract	= {Cloud computing platforms can offer many benefits related
		  to the provision of service processing and storage for
		  hosting client applications. Trustworthiness can be defined
		  as the trust of a customer in a cloud service and its
		  provider; however, the assurance of this property is not
		  trivial. First, trustworthiness in general is not composed
		  by a single quality attribute, but by the combination of
		  multiple attributes, such as data privacy, performance,
		  reliability, etc. Second, during runtime clients can
		  experience a change of the trustworthiness level required
		  by their application due to the degradation of the cloud
		  service. This article presents a solution that monitors
		  during runtime the set of quality attributes of a specific
		  application and generates adaptation plans in order to
		  certify that an adequate resource amount be provided by the
		  cloud in order to keep its trustworthiness level. Our
		  solution is based on quality models to compute the metric
		  associated to each non-functional requirement and their
		  combination them into different types of trustworthiness
		  levels. The main contribution of the solution is to provide
		  an approach which deals with multiple requirements at the
		  same time (or simultaneously) during runtime in order to
		  adapt the cloud resources to keep the trustworthiness level
		  required by the application. The solution was evaluated by
		  an experiment considering a scenario where the application
		  trustworthiness level was composed by three quality
		  attributes: data privacy, performance and reliability.
		  Initial results have shown that the approach is feasible in
		  terms of the execution of the adaptation plans during
		  runtime to certify the trustworthiness level required by
		  the application.},
  booktitle	= {Proceedings of the 14th Brazilian Symposium on Software
		  Components, Architectures, and Reuse},
  pages		= {141–150},
  numpages	= {10},
  keywords	= {Trustworthiness, Self-adaptive Systems, Cloud Computing,
		  Adaptation Planning},
  location	= {Natal, Brazil},
  series	= {SBCARS '20}
}

@InProceedings{	  10.1145/3639233.3639245,
  author	= {Ranganathan, Aarthi and Tamminaina, Sai Gowtham and Raina,
		  Gaurav},
  title		= {A Study Of Dialog Summarization Across Datasets And
		  Domains},
  year		= {2024},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639233.3639245},
  doi		= {10.1145/3639233.3639245},
  abstract	= {This study of dialog summarization covers multi-domain,
		  multimodal and multilingual datasets, and the potential
		  challenges in the different domains. The scope and progress
		  of this rapidly evolving topic rely on the availability of
		  datasets and emerging domains. Such a study can facilitate
		  the cross-application of datasets to different domains to
		  refine models and also aid scenarios where there is a lack
		  of data in privacy-sensitive settings. Further, our work
		  can enable the cross-fertilization of ideas across domains
		  and in different contexts. Our study encompasses current
		  and emerging domains, a comprehensive compilation of
		  datasets, and avenues for further research.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {196–202},
  numpages	= {7},
  keywords	= {Datasets, Dialog summarization, Domains, Machine learning,
		  NLP},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '23}
}

@InProceedings{	  10.1145/3688671.3688736,
  author	= {Kilanioti, Irene and Papadopoulos, George Angelos},
  title		= {AI-based knowledge graph construction and distributed
		  storage for collaboration on the Sustainable Development
		  Goals},
  year		= {2024},
  isbn		= {9798400709821},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3688671.3688736},
  doi		= {10.1145/3688671.3688736},
  abstract	= {The achievement of the Sustainable Development Goals
		  (SDGs) is crucial for future generations. The plethora of
		  SDG data available for analysis facilitates the tasks of
		  practitioners that gather and assess SDG data, including
		  intergovernmental organizations, government agencies and
		  social welfare organizations. In this paper, we propose a
		  framework that aspires to have a substantial impact for SDG
		  practitioners: We propose AI-based construction of SDG
		  knowledge graphs. AI-based methods along with
		  dimensionality reduction undertake the task to semantically
		  cluster new uncategorised SDG data and novel indicators and
		  efficiently place them in the environment of a distributed
		  knowledge graph store.},
  booktitle	= {Proceedings of the 13th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {40},
  numpages	= {6},
  keywords	= {Sustainable Development Goals ontology, distributed
		  knowledge graphs, Hilbert Space Filling Curves, Artificial
		  Intelligence},
  location	= { },
  series	= {SETN '24}
}

@Article{	  10.1145/3331149,
  author	= {Roels, Reinout and Signer, Beat},
  title		= {A Conceptual Framework and Content Model for Next
		  Generation Presentation Solutions},
  year		= {2019},
  issue_date	= {June 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {EICS},
  url		= {https://doi.org/10.1145/3331149},
  doi		= {10.1145/3331149},
  abstract	= {Mainstream presentation tools such as Microsoft PowerPoint
		  were originally built to mimic physical media like
		  photographic slides and still exhibit the same
		  characteristics. However, the state of the art in
		  presentation tools shows that more recent solutions start
		  to go beyond the classic presentation paradigms. For
		  instance, presentations are becoming increasingly
		  non-linear, content is quickly evolving beyond simple text
		  and images and the way we author our presentations is
		  becoming more collaborative. Nevertheless, existing
		  presentation content models are often based on assumptions
		  that do not apply to the current state of presentations any
		  more, making them incompatible for some use cases and
		  limiting the potential of end-user presentation solutions.
		  In order to support state-of-the-art presentation
		  functionality, we rethink the concept of a presentation and
		  introduce a conceptual framework for presentation content.
		  We then present a new content model for presentation
		  solutions based on the Resource-Selector-Link (RSL)
		  hypermedia metamodel. We further discuss an implementation
		  of our model and show some example use cases. We conclude
		  by outlining how design choices in the model address
		  currently unmet needs with regards to extensibility,
		  content reuse, collaboration, semantics, user access
		  management, non-linearity, and context awareness, resulting
		  in better support for the corresponding end-user
		  functionality in presentation tools.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jun,
  articleno	= {7},
  numpages	= {22},
  keywords	= {slideware, presentations, powerpoint, mindxpres, content
		  model, conceptual framework}
}

@InProceedings{	  10.1109/jcdl.2019.00029,
  author	= {Fenlon, Katrina},
  title		= {Modeling digital humanities collections as research
		  objects},
  year		= {2020},
  isbn		= {9781728115474},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL.2019.00029},
  doi		= {10.1109/JCDL.2019.00029},
  abstract	= {Advancing digital libraries to increase the sustainability
		  and usefulness of digital scholarship depends on
		  identifying and developing data models capable of
		  representing increasingly complex scholarly products. This
		  paper considers the potential for an emergent model of
		  scientific communication, the research objects data model,
		  to accommodate the complexities of digital humanities
		  collections. Digital humanities collections aggregate and
		  enrich diverse sources of evidence and context, serving
		  simultaneously as "publications" and dynamic, interactive
		  platforms for research. The research objects model is an
		  alternative to traditional formats of publication,
		  facilitating aggregation and description of all of the
		  inputs and outputs of a research process, ranging from
		  datasets to papers to executable code. This model
		  increasingly underpins research infrastructures in some
		  scientific domains, yet its efficacy for representing
		  humanities scholarship, and for undergirding humanities
		  cyberinfrastructure, remains largely untested. This study
		  offers a qualitative content analysis of digital humanities
		  collections relying on a content/context analytical
		  framework for characterizing collection components and
		  their interrelationships. This study then maps those
		  components and relationships into a research objects model
		  to identify the model's strengths and limitations for
		  representing diverse digital humanities scholarship.},
  booktitle	= {Proceedings of the 18th Joint Conference on Digital
		  Libraries},
  pages		= {138–147},
  numpages	= {10},
  keywords	= {research objects, digital libraries, digital humanities,
		  data models},
  location	= {Champaign, Illinois},
  series	= {JCDL '19}
}

@Article{	  10.1145/3161607,
  author	= {Chen, Qin and Hu, Qinmin and Huang, Jimmy Xiangji and He,
		  Liang},
  title		= {Modeling Queries with Contextual Snippets for Information
		  Retrieval},
  year		= {2018},
  issue_date	= {July 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {4},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3161607},
  doi		= {10.1145/3161607},
  abstract	= {Query expansion under the pseudo-relevance feedback (PRF)
		  framework has been extensively studied in information
		  retrieval. However, most expansion methods are mainly based
		  on the statistics of single terms, which can generate
		  plenty of irrelevant query terms and decrease retrieval
		  performance. To alleviate this problem, we propose an
		  approach that adapts the PRF-based contextual snippets into
		  a context-aware topic model to enhance query
		  representations. Specifically, instead of selecting a
		  series of independent terms, we make full use of the query
		  contextual information and focus on the snippets with the
		  length of n in the PRF documents. Furthermore, we propose a
		  context-aware topic (CAT) model to mine the topic
		  distributions of the query-relevant snippets, namely, fine
		  contextual snippets. In contrast to the traditional topic
		  models that infer the topics from the whole corpus, we
		  establish a bridge between the snippets and the
		  corresponding PRF documents, which can be used for modeling
		  the topics more precisely and efficiently. Finally, the
		  topic distributions of the fine snippets are used for
		  context-aware and topic-sensitive query representations. To
		  evaluate the performance of our approach, we integrate the
		  obtained queries into a topic-based hybrid retrieval model
		  and conduct extensive experiments on various TREC
		  collections. The experimental results show that our
		  query-modeling approach is more effective in boosting
		  retrieval performance compared with the state-of-the-art
		  methods.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= jan,
  articleno	= {47},
  numpages	= {26},
  keywords	= {topic modeling, query representation, Contextual snippet}
}

@InProceedings{	  10.1145/3290607.3312990,
  author	= {Owen, Alex and Martinez, Kirk},
  title		= {A Dynamic Hierarchical Approach to Modelling and
		  Orchestrating the Web of Things Using the DOM, CSS and
		  JavaScript},
  year		= {2019},
  isbn		= {9781450359719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3290607.3312990},
  doi		= {10.1145/3290607.3312990},
  abstract	= {There is a lot of work in progress by the W3C and others
		  surrounding a Web standards compliant Web of Things (WoT)
		  which it is hoped will unify the current Internet of Things
		  infrastructure. Our contribution to this uses the Document
		  Object Model (DOM) to represent complex physical
		  environments, with a CSS-like syntax for storing and
		  controlling the state of 'things' within it. We describe
		  how JavaScript can be used in conjunction with these to
		  create an approach which is familiar to Web developers and
		  may help them to transition more smoothly into WoT
		  development. We share our implementation and explore some
		  of the many potential avenues for future research. These
		  include rich WoT development tools and the possibility of
		  content production for physical environments.},
  booktitle	= {Extended Abstracts of the 2019 CHI Conference on Human
		  Factors in Computing Systems},
  pages		= {1–6},
  numpages	= {6},
  keywords	= {web of things, web development, linked data, internet of
		  things, document object model, WoT, IoT},
  location	= {Glasgow, Scotland Uk},
  series	= {CHI EA '19}
}

@InProceedings{	  10.1145/3191697.3191714,
  author	= {Johnson, Michael and Stevens, Perdita},
  title		= {Confidentiality in the process of (model-driven) software
		  development},
  year		= {2018},
  isbn		= {9781450355131},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3191697.3191714},
  doi		= {10.1145/3191697.3191714},
  abstract	= {Much is now understood about how to develop software that
		  will have good security properties in use. We claim that a
		  topic which needs more attention, in particular from the Bx
		  community, is security, especially confidentiality, in the
		  software development process itself. What is then at issue
		  is not what particular users of the software may be allowed
		  to know, but rather, what particular developers of the
		  software may be allowed to know. How can software
		  development processes guarantee to respect confidentiality
		  without compromising effective development? The question is
		  of general interest across software engineering, but
		  model-driven development (MDD) seems a particularly
		  promising arena in which to address it, because of MDD's
		  focus on separation of concerns. In MDD, different people
		  work with separate models, where (ideally) each model
		  records all and only the information necessary to those who
		  work with it. When necessary, the models are reconciled by
		  bidirectional transformations, which automate a process
		  which would otherwise have to be undertaken manually by the
		  groups of experts meeting and studying both their models in
		  order to bring them back into consistency. In model-driven
		  development confidentiality issues become particularly
		  clear and tractable, and bidirectional transformations have
		  a key technical role. We hope to encourage the community to
		  take up this challenge, and in this paper we begin our own
		  analysis of a selection of the issues, focusing
		  particularly on developing a threat model and some examples
		  of secure restoration of consistency.},
  booktitle	= {Companion Proceedings of the 2nd International Conference
		  on the Art, Science, and Engineering of Programming},
  pages		= {1–8},
  numpages	= {8},
  keywords	= {Security, Model-driven software development, Cospan,
		  Confidentiality},
  location	= {Nice, France},
  series	= {Programming '18}
}

@InProceedings{	  10.1145/3167918.3167944,
  author	= {Kingsun, Melinda and Myers, Trina and Hardy, Dianna},
  title		= {C-DOM: a structured co-design framework methodology for
		  ontology design and development},
  year		= {2018},
  isbn		= {9781450354363},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167918.3167944},
  doi		= {10.1145/3167918.3167944},
  abstract	= {The development of ontologies is traditionally a process
		  of building vocabularies and understanding complexities of
		  a specific domain to link data and infer knowledge.
		  However, this method does not always include the domain
		  experts throughout the entire development process. This
		  paper presents a Co-design framework as a structured
		  methodology for ontology design and development. The
		  framework, known as the Co-Designing Ontologies Methodology
		  (C-DOM), presents a strategy to draw knowledge from the
		  Subject Matter Expert (SME) without the SME needing to know
		  any-thing about how to create an ontology. The C-DOM
		  framework outlines a series of workshops that align to the
		  layers of the Semantic Technologies architecture and has
		  been developed via an iterative design process and tested
		  in different domains with different levels of complexity
		  and purpose. The first iteration focused on the creation of
		  a light-weight ontology to link generic data in the
		  incident and accident. The next iteration built a
		  heavyweight ontology that included complex inference and
		  reasoning within the urban water management domain. This
		  paper describes the C-DOM in detail for re-use within the
		  Semantic Technologies community and outlines its iterative
		  development process undertake.1},
  booktitle	= {Proceedings of the Australasian Computer Science Week
		  Multiconference},
  articleno	= {22},
  numpages	= {10},
  keywords	= {ontology development methodology, co-development,
		  co-design},
  location	= {Brisband, Queensland, Australia},
  series	= {ACSW '18}
}

@InProceedings{	  10.1145/3462757.3466100,
  author	= {Paley, Andrew and Zhao, Andong L. Li and Pack, Harper and
		  Servantez, Sergio and Adler, Rachel F. and Sterbentz, Marko
		  and Pah, Adam and Schwartz, David and Barrie, Cameron and
		  Einarsson, Alexander and Hammond, Kristian},
  title		= {From data to information: automating data science to
		  explore the U.S. court system},
  year		= {2021},
  isbn		= {9781450385268},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3462757.3466100},
  doi		= {10.1145/3462757.3466100},
  abstract	= {The U.S. court system is the nation's arbiter of justice,
		  tasked with the responsibility of ensuring equal protection
		  under the law. But hurdles to information access obscure
		  the inner workings of the system, preventing stakeholders -
		  from legal scholars to journalists and members of the
		  public - from understanding the state of justice in America
		  at scale. There is an ongoing data access argument here:
		  U.S. court records are public data and should be freely
		  available. But open data arguments represent a
		  half-measure; what we really need is open information. This
		  distinction marks the difference between downloading a zip
		  file containing a quarter-million case dockets and getting
		  the real-time answer to a question like "Are pro se parties
		  more or less likely to receive fee waivers?" To help bridge
		  that gap, we introduce a novel platform and user experience
		  that provides users with the tools necessary to explore
		  data and drive analysis via natural language statements.
		  Our approach leverages an ontology configuration that adds
		  domain-relevant data semantics to database schemas to
		  provide support for user guidance and for search and
		  analysis without user-entered code or SQL. The system is
		  embodied in a "natural-language notebook" user experience,
		  and we apply this approach to the space of case docket data
		  from the U.S. federal court system. Additionally, we
		  provide detail on the collection, ingestion and processing
		  of the dockets themselves, including early experiments in
		  the use of language modeling for docket entry
		  classification with an initial focus on motions.},
  booktitle	= {Proceedings of the Eighteenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {119–128},
  numpages	= {10},
  keywords	= {visualization, notebook interface, natural language
		  processing, information extraction, data analytics},
  location	= {S\~{a}o Paulo, Brazil},
  series	= {ICAIL '21}
}

@InProceedings{	  10.1145/3297280.3297417,
  author	= {Almeida, Ricardo Borges and Covalski, Victor and Machado,
		  Roger and Rosa, Di\'{o}rgenes Yuri Leal da and Yamin,
		  Adenauer Corr\^{e}a and Donato, Lucas Medeiros and Pernas,
		  Ana Marilza},
  title		= {A hierarchical architectural model for network security
		  exploring situational awareness},
  year		= {2019},
  isbn		= {9781450359337},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297280.3297417},
  doi		= {10.1145/3297280.3297417},
  abstract	= {Often network security technologies used by organizations
		  for securing their computational systems are deficient in
		  providing holistic view of the environment. Based on this,
		  our paper presents an architectural model based on a
		  Situational Awareness approach for securing computational
		  systems in distributed environments. The architecture is
		  called EXEHDA-ISSA and is inspired by SIEM systems. It is
		  composed of three modular software components called
		  Collector, SmartLogger, and Manager. These components are
		  interconnected following a multi-level hierarchical model
		  and provide features such as event collection, hybrid event
		  processing and a hybrid approach to contextual data
		  storage. For the purpose of evaluating this proposal, four
		  case studies were developed to validate the holistic view
		  of security events as well as the model's characteristics
		  such as flexibility, autonomy, scalability and the support
		  to heterogeneity. Finally, the strengths and limitations of
		  our approach are discussed, then followed by future
		  works.},
  booktitle	= {Proceedings of the 34th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1365–1372},
  numpages	= {8},
  keywords	= {architectural model, network security, situational
		  awareness},
  location	= {Limassol, Cyprus},
  series	= {SAC '19}
}

@InProceedings{	  10.1145/3508397.3564846,
  author	= {Rinaldi, Antonio M. and Russo, Cristiano and Tommasino,
		  Cristian},
  title		= {A Novel Approach to Populate Multimedia Knowledge Graph
		  via Deep Learning and Semantic Analysis},
  year		= {2022},
  isbn		= {9781450392198},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3508397.3564846},
  doi		= {10.1145/3508397.3564846},
  abstract	= {The growth of data in volume and complexity needs
		  automatic tools to manage and process information. Semantic
		  Web Technologies are a silver bullet in this context due to
		  their capacity to transform human-readable contents into
		  machine-readable ones. Knowledge graphs and the related
		  ontologies represent essential tools for managing very
		  large knowledge bases. The population process of these
		  knowledge structures is composed of expensive and
		  time-consuming tasks, and we propose a novel approach to
		  automate the population step. Our approach is based on
		  novel techniques based on semantic analysis and deep
		  learning using NoSQL technologies. Several results to show
		  the effectiveness of our approach is also reported.},
  booktitle	= {Proceedings of the 14th International Conference on
		  Management of Digital EcoSystems},
  pages		= {40–47},
  numpages	= {8},
  keywords	= {semantics, ontology, knowledge graph, deep learning,
		  NoSQL},
  location	= {Venice, Italy},
  series	= {MEDES '22}
}

@InProceedings{	  10.1145/3539618.3591846,
  author	= {Er-Rahmadi, Btissam and Oncevay, Arturo and Ji, Yuanyi and
		  Pan, Jeff Z.},
  title		= {KATIE: A System for Key Attributes Identification in
		  Product Knowledge Graph Construction},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591846},
  doi		= {10.1145/3539618.3591846},
  abstract	= {We present part of Huawei's efforts in building a Product
		  Knowledge Graph (PKG). We want to identify which product
		  attributes (i.e. properties) are relevant and important in
		  terms of shopping decisions to product categories (i.e.
		  classes). This is particularly challenging when the
		  attributes and their values are mined from online product
		  catalogues, i.e. HTML pages. These web pages contain
		  semi-structured data, which do not follow a concerted
		  format and use diverse vocabulary to designate the same
		  features. We propose a system for key attribute
		  identification (KATIE) based on fine-tuning pre-trained
		  models (e.g., DistilBERT) to predict the applicability and
		  importance of an attribute to a category. We also propose
		  an attribute synonyms identification module that allows us
		  to discover synonymous attributes by considering not only
		  their labels' similarities but also the similarity of their
		  values sets. We have evaluated our approach to Huawei
		  categories taxonomy and a set of internally mined
		  attributes from web pages. KATIE guarantees promising
		  performance results compared to the most recent
		  baselines.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3320–3324},
  numpages	= {5},
  keywords	= {entity resolution, fine-tuning, pre-trained language
		  model, product knowledge graph, relation discovery},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InBook{	  10.1145/3233795.3233806,
  author	= {Tumuluri, Raj and Dahl, Deborah and Patern\`{o}, Fabio and
		  Zancanaro, Massimo},
  title		= {Standardized representations and markup languages for
		  multimodal interaction},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan \&amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233806},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {347–392},
  numpages	= {46}
}

@InProceedings{	  10.5555/3470152.3470156,
  author	= {Jung, Jean Christoph and Papacchini, Fabio and Wolter,
		  Frank and Zakharyaschev, Michael},
  title		= {Model comparison games for horn description logics},
  year		= {2021},
  publisher	= {IEEE Press},
  abstract	= {Horn description logics are syntactically defined
		  fragments of standard description logics that fall within
		  the Horn fragment of first-order logic and for which
		  ontology-mediated query answering is in PTime for data
		  complexity. They were independently introduced in modal
		  logic to capture the intersection of Horn first-order logic
		  with modal logic. In this paper, we introduce model
		  comparison games for the basic Horn description logic
		  hornALC (corresponding to the basic Horn modal logic) and
		  use them to obtain an Ehrenfeucht-Fra\"{\i}ss\'{e} type
		  definability result and a van Benthem style expressive
		  completeness result for hornALC. We also establish a finite
		  model theory version of the latter. The
		  Ehrenfeucht-Fra\"{\i}ss\'{e} type definability result is
		  used to show that checking hornALC indistinguishability of
		  models is EXPTIME-complete, which is in sharp contrast to
		  ALC indistinguishability (i.e., bisimulation equivalence)
		  checkable in PTime. In addition, we explore the behavior of
		  Horn fragments of more expressive description and modal
		  logics by defining a Horn guarded fragment of first-order
		  logic and introducing model comparison games for it.},
  booktitle	= {Proceedings of the 34th Annual ACM/IEEE Symposium on Logic
		  in Computer Science},
  articleno	= {4},
  numpages	= {14},
  location	= {Vancouver, Canada},
  series	= {LICS '19}
}

@InProceedings{	  10.5555/3427510.3427546,
  author	= {Keller, Nicholas and Zeigler, Bernard and Kim, Doohwan and
		  Anderson, Chase and Ceney, James},
  title		= {Supporting the reuse of algorithmic simulation models},
  year		= {2020},
  isbn		= {9781713814290},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {Stateless functions, also referred to as algorithmic
		  models, return an output given inputs that all occur at the
		  same time instant. As relatively simple dynamic models,
		  which define the behavior of variables over a timeline,
		  algorithmic models nevertheless encode knowledge of
		  entities that can be essential for use within models in a
		  particular domain. This paper presents a development
		  methodology for representing algorithmic models within the
		  Discrete Event Systems Specifications (DEVS) formalism and
		  employing the System Entity Structure (SES) to organize
		  these models for reuse in new compositions. A use case
		  example is used for illustration of the development process
		  and the benefits in savings of time and effort are
		  illustrated. Finally, some future possibilities to enhance
		  the support of DEVS environments for this methodology are
		  discussed.},
  booktitle	= {Proceedings of the 2020 Summer Simulation Conference},
  articleno	= {35},
  numpages	= {11},
  keywords	= {reuse, algorithmic, SES, DEVS},
  location	= {Virtual Event, Spain},
  series	= {SummerSim '20}
}

@Article{	  10.1145/3714456,
  author	= {Haider Rizvi, Syed Mustafa and Imran, Ramsha and Mahmood,
		  Arif},
  title		= {Text Classification Using Graph Convolutional Networks: A
		  Comprehensive Survey},
  year		= {2025},
  issue_date	= {August 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {8},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3714456},
  doi		= {10.1145/3714456},
  abstract	= {Text classification is a quintessential and practical
		  problem in natural language processing with applications in
		  diverse domains such as sentiment analysis, fake news
		  detection, medical diagnosis, and document classification.
		  A sizable body of recent works exists where researchers
		  have studied and tackled text classification from different
		  angles with varying degrees of success. Graph convolution
		  network (GCN)-based approaches have gained a lot of
		  traction in this domain over the last decade with many
		  implementations achieving state-of-the-art performance in
		  more recent literature and thus, warranting the need for an
		  updated survey. This work aims to summarize and categorize
		  various GCN-based Text Classification approaches with
		  regard to the architecture and mode of supervision. It
		  identifies their strengths and limitations and compares
		  their performance on various benchmark datasets. We also
		  discuss future research directions and the challenges that
		  exist in this domain.},
  journal	= {ACM Comput. Surv.},
  month		= mar,
  articleno	= {201},
  numpages	= {38},
  keywords	= {GCN, text classification, text analysis, text
		  categorization}
}

@InProceedings{	  10.1145/3511808.3557446,
  author	= {Burgdorf, Andreas and Paulus, Alexander and Pomp,
		  Andr\'{e} and Meisen, Tobias},
  title		= {DocSemMap 2.0: Semantic Labeling based on Textual Data
		  Documentations Using Seq2Seq Context Learner},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557446},
  doi		= {10.1145/3511808.3557446},
  abstract	= {Methods for automated semantic labeling of data are an
		  indispensable basis for increasing the usability of data.
		  On the one hand, they contribute to the homogenization of
		  the annotations and thus to the increase in quality; on the
		  other hand, they reduce the modeling effort, provided that
		  the quality of the used methodology is sufficient. In the
		  past, research has focused primarily on data- and
		  label-based methods. Another approach that has received
		  recent attention is the incorporation of textual data
		  documentations to support the automatic mapping of datasets
		  to a knowledge graph. However, upon deeper analysis, our
		  recent approach called DocSemMap gives away potential in a
		  number of places. In this paper, we extend the current
		  state of the art approach by uncovering existing
		  shortcomings and presenting our own improvements. Using a
		  sequence-to-sequence model (Seq2Seq), we exploit the
		  context of datasets. An additional introduced classifier
		  provides the linkage of documentation and labels for
		  prediction. Our extended approach achieves a sustainable
		  improvement in comparison to the reference approach.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {98–107},
  numpages	= {10},
  keywords	= {seq2seq, semantic mapping, natural language processing,
		  classifier},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3555776.3578573,
  author	= {Guittoum, Amal and A\"{\i}ssaoui, Francois and Bolle,
		  S\'{e}bastien and Boyer, Fabienne and De Palma, Noel},
  title		= {Inferring Threatening IoT Dependencies using Semantic
		  Digital Twins Toward Collaborative IoT Device Management},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3578573},
  doi		= {10.1145/3555776.3578573},
  abstract	= {IoT Device Management (DM) refers to registering,
		  configuring, monitoring, and updating IoT devices. DM is
		  facing new challenges as dependencies between IoT devices
		  generate various threats, such as update breaks and
		  cascading failures. Dependencies-related threats are
		  exacerbated by the fragmentation of the DM market, where
		  multiple actors, e.g., operators and device manufacturers,
		  are uncoordinately ensuring DM on interdependent devices,
		  each using its DM solution. Identifying the topology of
		  threatening dependencies is key in developing
		  dependency-aware DM capabilities for legacy DM solutions to
		  tackle dependencies-related threats efficiently. In this
		  work, we apply Semantic Web and Digital Twin technologies
		  to build a decision-support framework that automatically
		  infers the topology of threatening dependencies in IoT
		  systems. We integrate the proposed framework into the
		  in-use Digital Twin platform Thing in the future and
		  demonstrate its effectiveness by inferring threatening
		  dependencies in smart home scenarios managed by
		  ground-truth DM solutions, such as Orange's implementation
		  of the USP Controller and Samsung's SmartThings Platform.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1732–1741},
  numpages	= {10},
  keywords	= {collaboration, dependencies management, entity resolution,
		  thing description, SHACL, inference, ontology, digital
		  twin, semantic web, IoT device management},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@InProceedings{	  10.1145/3639478.3639814,
  author	= {Speth, Sandro},
  title		= {Architecture-Based Cross-Component Issue Management and
		  Propagation Analysis},
  year		= {2024},
  isbn		= {9798400705021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639478.3639814},
  doi		= {10.1145/3639478.3639814},
  abstract	= {This paper addresses the challenge of issue management in
		  complex, component-based software architectures. In these
		  systems, issues in one component often propagate across the
		  architecture along the call chains. Yet, traditional issue
		  management systems (IMSs) are limited to the boundaries of
		  a single component and lack mechanisms for managing issues
		  concerning their architectural dependencies. We present
		  Gropius, a novel method that enhances issue management by
		  integrating issues in an architecture graph. Gropius allows
		  semantically linking issues across different components,
		  synchronizes changes with underlying IMSs like GitHub, and
		  allows modeling the architecture ontologically by defining
		  the components' semantics at runtime. We explore whether
		  combining issue and architecture management improves the
		  development of component-based architectures regarding
		  issue management. We hypothesize that this method will
		  improve the efficiency and effectiveness of identifying and
		  resolving cross-component issues, maintaining a
		  comprehensive view of the application's state.},
  booktitle	= {Proceedings of the 2024 IEEE/ACM 46th International
		  Conference on Software Engineering: Companion Proceedings},
  pages		= {145–149},
  numpages	= {5},
  keywords	= {issue management, issue propagation analysis,
		  component-based software architecture, model-based
		  analysis},
  location	= {Lisbon, Portugal},
  series	= {ICSE-Companion '24}
}

@Article{	  10.1145/3717413.3717435,
  author	= {Duchon, Markus and Matar, Jessy and Shoyari, Mahsa Faraji
		  and Perzylo, Alexander and Kessler, Ingmar and Buchenberg,
		  Patrick and Kuhn, Philipp and Hamacher, Thomas and
		  Schlachter, Thorsten and S\"{u}ss, Wolfgang and Thinh,
		  Nguyen Xuan and Salari, Haniyeh Ebrahimi and Latko, Jasmin
		  and Xu, Minsheng and Shamovich, Maxim and Schl\"{u}tter,
		  Dominik and Frisch, J\'{e}r\^{o}me and Rustagi, Kushagar
		  and Kraft, Markus and Ayasse, Carolin and Steinke, Florian
		  and Metzger, Michael and Kuper, Laura},
  title		= {A Platform Ecosystem Providing New Data For The Energy
		  Transition},
  year		= {2025},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {4},
  url		= {https://doi.org/10.1145/3717413.3717435},
  doi		= {10.1145/3717413.3717435},
  abstract	= {There is a great need for high-quality and comprehensive
		  data in the energy sector. This data is collected and
		  preprocessed at considerable expense and is not only
		  required for research, but also by planning offices and
		  other industries in connection with planning activities,
		  such as the creation of municipal heat planning. The NEED
		  ecosystem will accelerate these processes establishing an
		  efficient, robust, and scalable energy data ecosystem.
		  Heterogeneous energy-related data sources will be brought
		  together and automatically linked consistently across
		  different sectors as well as temporal and spatial levels.
		  In this context, existing data sources will not be replaced
		  but rather integrated into the NEED ecosystem as dedicated
		  sources including a semantic description on how to utilize
		  them. In addition to conventional data sources from the
		  various planning levels, we envision a quality assessment
		  scheme based on the FAIR criteria. In reality, we are often
		  faced with missing data, too. To close this gap we explore
		  data-driven, model-driven, AI-based, and tool-driven
		  generation of synthetic data. These heterogeneous data
		  sources will be interlinked using ontology modules which
		  will be represented in a knowledge graph. Via a semantic
		  API, queries will be generated to identify the required
		  data sources, which will be orchestrated to provide the
		  data needed. This will enable researchers, planners, and
		  others including their tools to interact with the NEED
		  ecosystem, while a tool proxy will be able to translate the
		  resulting data into proprietary formats, required by some
		  tools to operate. The NEED ecosystem is planned to be a
		  robust, easy-to-maintain, and flexible infrastructure to
		  enhance planning energy measures at different spatial
		  levels and with different time horizons. We envision to
		  evaluate our NEED approach for the transparent provision of
		  data by integrating relevant data sources as microservices,
		  definition and analysis of application scenarios in the
		  planning domain, as well as the integration of various
		  tools for different planning purposes. With these elements,
		  we will be able to quantify the efficiency of data
		  procurement and demonstrate the functionality of the
		  approach using practical use cases.},
  journal	= {SIGENERGY Energy Inform. Rev.},
  month		= feb,
  pages		= {226–237},
  numpages	= {12}
}

@InProceedings{	  10.1145/3400903.3400919,
  author	= {Spitz, Andreas and Aumiller, Dennis and Soproni,
		  B\'{a}lint and Gertz, Michael},
  title		= {A Versatile Hypergraph Model for Document Collections},
  year		= {2020},
  isbn		= {9781450388146},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3400903.3400919},
  doi		= {10.1145/3400903.3400919},
  abstract	= {Efficiently and effectively representing large collections
		  of text is of central importance to information retrieval
		  tasks such as summarization and search. Since models for
		  these tasks frequently rely on an implicit graph structure
		  of the documents or their contents, graph-based document
		  representations are naturally appealing. For tasks that
		  consider the joint occurrence of words or entities,
		  however, existing document representations often fall short
		  in capturing cooccurrences of higher order, higher
		  multiplicity, or at varying proximity levels. Furthermore,
		  while numerous applications benefit from structured
		  knowledge sources, external data sources are rarely
		  considered as integral parts of existing document models.
		  To address these shortcomings, we introduce heterogeneous
		  hypergraphs as a versatile model for representing annotated
		  document collections. We integrate external metadata,
		  document content, entity and term annotations, and document
		  segmentation at different granularity levels in a joint
		  model that bridges the gap between structured and
		  unstructured data. We discuss selection and transformation
		  operations on the set of hyperedges, which can be chained
		  to support a wide range of query scenarios. To ensure
		  compatibility with established information retrieval
		  methods, we discuss projection operations that transform
		  hyperedges to traditional dyadic cooccurrence graph
		  representations. Using PostgreSQL and Neo4j, we investigate
		  the suitability of existing database systems for
		  implementing the hypergraph document model, and explore the
		  impact of utilizing implicit and materialized hyperedge
		  representations on storage space requirements and query
		  performance.},
  booktitle	= {Proceedings of the 32nd International Conference on
		  Scientific and Statistical Database Management},
  articleno	= {7},
  numpages	= {12},
  location	= {Vienna, Austria},
  series	= {SSDBM '20}
}

@InProceedings{	  10.1145/3290605.3300231,
  author	= {Colusso, Lucas and Jones, Ridley and Munson, Sean A. and
		  Hsieh, Gary},
  title		= {A Translational Science Model for HCI},
  year		= {2019},
  isbn		= {9781450359702},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3290605.3300231},
  doi		= {10.1145/3290605.3300231},
  abstract	= {Using scientific discoveries to inform design practice is
		  an important, but difficult, objective in HCI. In this
		  paper, we provide an overview of Translational Science in
		  HCI by triangulating literature related to the
		  research-practice gap with interview data from many parties
		  engaged (or not) in translating HCI knowledge. We propose a
		  model for Translational Science in HCI based on the concept
		  of a continuum to describe how knowledge progresses (or
		  stalls) through multiple steps and translations until it
		  can influence design practice. The model offers a
		  conceptual framework that can be used by researchers and
		  practitioners to visualize and describe the progression of
		  HCI knowledge through a sequence of translations.
		  Additionally, the model may facilitate a precise
		  identification of translational barriers, which allows
		  devising more effective strategies to increase the use of
		  scientific findings in design practice.},
  booktitle	= {Proceedings of the 2019 CHI Conference on Human Factors in
		  Computing Systems},
  pages		= {1–13},
  numpages	= {13},
  keywords	= {translational science, translational research,
		  research-practice gap},
  location	= {Glasgow, Scotland Uk},
  series	= {CHI '19}
}

@InProceedings{	  10.1145/3629527.3652898,
  author	= {Khan, Junaid Ahmed and Molan, Martin and Angelinelli,
		  Matteo and Bartolini, Andrea},
  title		= {ExaQuery: Proving Data Structure to Unstructured Telemetry
		  Data in Large-Scale HPC},
  year		= {2024},
  isbn		= {9798400704451},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3629527.3652898},
  doi		= {10.1145/3629527.3652898},
  abstract	= {High-performance computing (HPC) is the cornerstone of
		  technological advancements in our digital age, but its
		  management is becoming increasingly challenging,
		  particularly as systems approach exascale. Operational data
		  analytics (ODA) and holistic monitoring frameworks aim to
		  alleviate this burden by collecting live telemetry from HPC
		  systems. ODA frameworks rely on NoSQL databases for
		  scalability, with implicit data structures embedded in
		  metric names, necessitating domain knowledge for navigating
		  telemetry data relations. To address the imperative need
		  for explicit representation of relations in telemetry data,
		  we propose a novel ontology for ODA, which we apply to a
		  real HPC installation. The proposed ontology captures
		  relationships between topological components and links
		  hardware components(compute nodes, rack, systems) with
		  job's execution and allocations collected telemetry. This
		  ontology forms the basis for constructing a knowledge
		  graph, enabling graph queries for ODA. Moreover, we propose
		  a comparative analysis of the complexity (expressed in
		  lines of code) and domain knowledge requirement
		  (qualitatively assessed by informed end-users) of complex
		  query implementation with the proposed method and NoSQL
		  methods commonly employed in today's ODAs. We focused on
		  six queries informed by facility managers' daily
		  operations, aiming to benefit not only facility managers
		  but also system administrators and user support. Our
		  comparative analysis demonstrates that the proposed
		  ontology facilitates the implementation of complex queries
		  with significantly fewer lines of code and domain knowledge
		  required as compared to NoSQL methods.},
  booktitle	= {Companion of the 15th ACM/SPEC International Conference on
		  Performance Engineering},
  pages		= {127–134},
  numpages	= {8},
  keywords	= {high performance computing (hpc), operational data
		  analytics(oda), resource description framework (rdf)
		  ontology, sparql},
  location	= {London, United Kingdom},
  series	= {ICPE '24 Companion}
}

@InProceedings{	  10.1145/3341981.3344217,
  author	= {Purpura, Alberto and Maggipinto, Marco and Silvello,
		  Gianmaria and Susto, Gian Antonio},
  title		= {Probabilistic Word Embeddings in Neural IR: A Promising
		  Model That Does Not Work as Expected (For Now)},
  year		= {2019},
  isbn		= {9781450368810},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341981.3344217},
  doi		= {10.1145/3341981.3344217},
  abstract	= {In this paper, we discuss how a promising word vector
		  representation based on Probabilistic Word Embeddings (PWE)
		  can be applied to Neural Information Retrieval (NeuIR). We
		  illustrate PWE pros for text retrieval, and identify the
		  core issues which prevent a full exploitation of their
		  potential. In particular, we focus on the application of
		  elliptical probabilistic embeddings, a type of PWE, to a
		  NeuIR system (i.e., MatchPyramid). The main contributions
		  of this paper are: (i) an analysis of the pros and cons of
		  PWE in NeuIR; (ii) an in-depth comparison of PWE against
		  pre-trained Word2Vec, FastText and WordNet word embeddings;
		  (iii) an extension of the MatchPyramid model to take
		  advantage of broader word relations information from
		  WordNet; (iv) a topic-level evaluation of the MatchPyramid
		  ranking models employing the considered word embeddings.
		  Finally, we discuss some lessons learned and outline some
		  open research problems to employ PWE in NeuIR systems more
		  effectively.},
  booktitle	= {Proceedings of the 2019 ACM SIGIR International Conference
		  on Theory of Information Retrieval},
  pages		= {3–10},
  numpages	= {8},
  keywords	= {probabilistic word embedding, neural information
		  retrieval, natural language processing},
  location	= {Santa Clara, CA, USA},
  series	= {ICTIR '19}
}

@InProceedings{	  10.1145/3460210.3493569,
  author	= {Mansfield, Martin and Tamma, Valentina and Goddard, Phil
		  and Coenen, Frans},
  title		= {Capturing Expert Knowledge for Building Enterprise SME
		  Knowledge Graphs},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493569},
  doi		= {10.1145/3460210.3493569},
  abstract	= {Whilst Knowledge Graphs (KGs) are increasingly used in
		  business scenarios, the construction of enterprise
		  ontologies and the population of KGs from existing
		  relational data remains a significant challenge. In this
		  paper we report our experience in supporting CSols (an SME
		  operating in the analytical laboratory domain) in
		  transitioning their data from legacy databases to a bespoke
		  KG. We modelled the KG using a streamlined approach based
		  on state of the art ontology engineering methodologies,
		  that addresses the challenges faced by SMEs when
		  transitioning to new technologies: lack of resources to
		  devote to the transition, paucity of comprehensive data
		  governance policies, and resistance within the organisation
		  to accepting new practices and knowledge. Our approach uses
		  a combination of UML diagrams and a controlled language
		  glossary to support stakeholders in reaching consensus
		  during the knowledge capture phase, thus reducing the
		  intervention of the ontology engineer only to cases where
		  no agreement can be found. We present a case study
		  illustrating the generation of the KG from a UML
		  specification of part of the analytical domain and from
		  legacy relational data, and we discuss the benefits and
		  challenges of the approach.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {129–136},
  numpages	= {8},
  keywords	= {uml, relational data, r2rml, ontology engineering,
		  enterprise knowledge graphs},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@InProceedings{	  10.1145/3341105.3373910,
  author	= {Donadello, Ivan and Dragoni, Mauro and Eccher, Claudio},
  title		= {Explaining reasoning algorithms with persuasiveness: a
		  case study for a behavioural change system},
  year		= {2020},
  isbn		= {9781450368667},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341105.3373910},
  doi		= {10.1145/3341105.3373910},
  abstract	= {Explainable AI aims at building intelligent systems that
		  are able to provide a clear, and human understandable,
		  justification of their decisions. This holds for both
		  rule-based and data-driven methods. In management of
		  chronic diseases, the users of such systems are patients
		  that follow strict dietary rules to manage such diseases.
		  After receiving the input of the intake food, the system
		  performs reasoning to understand whether the users follow
		  an unhealthy behaviour. Successively, the system has to
		  communicate the results in a clear and effective way, that
		  is, the output message has to persuade users to follow the
		  right dietary rules. In this paper, we address the main
		  challenges to build such systems: i) the natural language
		  generation of messages that explain the reasoner
		  inconsistency; ii) the effectiveness of such messages at
		  persuading the users. Results prove that the persuasive
		  explanations are able to reduce the unhealthy users'
		  behaviours.},
  booktitle	= {Proceedings of the 35th Annual ACM Symposium on Applied
		  Computing},
  pages		= {646–653},
  numpages	= {8},
  keywords	= {ontologies, natural language generation, mHealth,
		  explainable reasoning, explainable AI},
  location	= {Brno, Czech Republic},
  series	= {SAC '20}
}

@InProceedings{	  10.1145/3396743.3396781,
  author	= {Chaveesuk, Singha and Chaiyasoonthorn, Wornchanok and
		  Khalid, Bilal},
  title		= {Understanding the Model of User Adoption and Acceptance of
		  Technology by Thai Farmers: A Conceptual Framework},
  year		= {2020},
  isbn		= {9781450377065},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3396743.3396781},
  doi		= {10.1145/3396743.3396781},
  abstract	= {Thailand is constantly making policy changes and
		  implementing Industrial revolution agendas like Thailand
		  4.0. With the advent of Industry revolution 4.0, Thailand
		  government is making policy initiatives and technological
		  advancement for successful transition to industry 5.0. The
		  initiative is focused to offer farmers an opportunities to
		  incorporate technology in their farming and agricultural
		  processes to produce better crops and high-quality food.
		  The Development of Eastern Economic Corridor is a crucial
		  step in this regard as it focused on the evolution and
		  progression of 10 key industries in Thailand. The plan is
		  designed to be driven through agricultural technology and
		  innovativeness. The future benefits of the ICT based 5.0
		  industrial revolution in agricultural field directly
		  depends on the user adoption and acceptance of agricultural
		  technology. The study, therefore, investigates the
		  acceptance and adoption of ICT based products and services
		  by farmers by utilizing the basics of TAM. The study
		  proposes the framework of FTAM, and identifies the internal
		  and external factors affecting the behavior intentions and
		  attitude of farmers. It was found out that factors like
		  "occupation relevance", "self-efficacy" and "social
		  influence" affect the "Perceived usefulness (PU)" and
		  "Perceived Ease of Use (PEOU) which in return impact their
		  behavior intention and attitude towards utilizing and
		  accepting ICT based products and services in farming and
		  agriculture.},
  booktitle	= {Proceedings of the 2020 2nd International Conference on
		  Management Science and Industrial Engineering},
  pages		= {279–285},
  numpages	= {7},
  keywords	= {innovativeness, Technology Acceptance Model, Perceived
		  Usefulness, Perceived Ease of Use, Internet of Things,
		  Information Communication Technology, Industrial
		  Revolution},
  location	= {Osaka, Japan},
  series	= {MSIE '20}
}

@InProceedings{	  10.1145/3417990.3421413,
  author	= {Frank, Ulrich and T\"{o}pel, Daniel},
  title		= {Contingent level classes: motivation, conceptualization,
		  modeling guidelines, and implications for model
		  management},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3421413},
  doi		= {10.1145/3417990.3421413},
  abstract	= {It has been known for some time that the level of a class
		  may vary with the context it is used in. There are a few
		  approaches that enable modelers to deal with corresponding
		  requirements. However, they usually provide workarounds to
		  avoid the problem of one class being on different levels at
		  the same time. In this paper, the need for those classes,
		  which are called contingent level classes, is motivated
		  from a conceptual perspective. A conceptualization of
		  contingent level classes is presented that addresses
		  principal integrity issues and accounts for resulting
		  constraints on class properties and relationships. Based on
		  that conceptualization, the paper provides an analysis of
		  specific challenges related to change operations on models
		  that include contingent level classes. Subsequently, a set
		  of patterns for coping with certain kinds of change
		  operations is presented.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {86},
  numpages	= {10},
  keywords	= {possible world, multi-level modeling, level jump,
		  contingent level class, change operations},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@InProceedings{	  10.1145/3631700.3664913,
  author	= {Bompotas, Agorakis and Triantafyllopoulos, Panagiotis and
		  Raptis, George E. and Katsini, Christina and Makris,
		  Christos},
  title		= {Towards Exploring Personalized Hyperlink Recommendations
		  Through Machine Learning},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3664913},
  doi		= {10.1145/3631700.3664913},
  abstract	= {The Internet offers a wealth of content, making it
		  increasingly difficult for users to navigate website
		  information. The volume of hyperlinks on a website often
		  leaves users struggling with content overload, hindering
		  their ability to find relevant information of high
		  interest. This problem highlights the critical need for
		  tools to improve the user experience by providing
		  personalized hyperlink recommendations on a specific
		  website. This paper introduces HypeRec, a browser extension
		  that attempts to address this problem by leveraging and
		  comparing different machine learning and recommendation
		  algorithms to guide users to content consistent with their
		  interests and preferences. Our approach involves extracting
		  hyperlinks from a webpage and subjecting the corresponding
		  textual content to natural language processing techniques.
		  In this way, it simplifies the users’ navigation within a
		  website and promotes a more intuitive and satisfying web
		  browsing experience.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {528–533},
  numpages	= {6},
  keywords	= {Content Overload, Hyperlink Analysis, Machine Learning,
		  Natural Language Processing, Personalization,
		  Recommendation Systems, User Experience, Web Navigation,
		  Web Usability},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@InProceedings{	  10.1145/3372782.3406279,
  author	= {Malmi, Lauri and Sheard, Judy and Kinnunen, P\"{a}ivi and
		  Simon and Sinclair, Jane},
  title		= {Theories and Models of Emotions, Attitudes, and
		  Self-Efficacy in the Context of Programming Education},
  year		= {2020},
  isbn		= {9781450370929},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3372782.3406279},
  doi		= {10.1145/3372782.3406279},
  abstract	= {Research into the relationship between learning computing
		  and students' attitudes, beliefs, and emotions often builds
		  on theoretical frameworks from the social sciences in order
		  to understand how these factors influence, for example,
		  students' motivation, study practices, and learning
		  results. In this paper we explore the computing education
		  research literature to identify new theoretical constructs
		  that have emerged from this research. We focus on empirical
		  work in programming education that extends or adapts
		  theories or instruments from the social sciences or that
		  independently develops theories specific to programming.
		  From an initial data set of more than 3800 papers published
		  in the years 2010--2019, we identify 50 papers that present
		  a range of domain-specific theoretical constructs
		  addressing emotions, affect, beliefs, attitudes, and
		  self-efficacy. They include 11 validated instruments and a
		  number of statistical models, but also grounded theories
		  and pedagogical models. We summarize the main results of
		  many of these constructs and provide references for all of
		  them. We also investigate how these constructs have
		  informed further research by analysing over 850 papers that
		  cite these 50 papers. We categorize the ways that theories
		  can inform further research, and give examples of papers in
		  each of these categories. Our findings indicate that among
		  these categories, instruments have been most widely used in
		  further research, thus affirming their value in the
		  field.},
  booktitle	= {Proceedings of the 2020 ACM Conference on International
		  Computing Education Research},
  pages		= {36–47},
  numpages	= {12},
  keywords	= {theory, theoretical construct, self-efficacy, research,
		  programming, instrument, emotion, computing education,
		  belief, attitude, affect},
  location	= {Virtual Event, New Zealand},
  series	= {ICER '20}
}

@InProceedings{	  10.1145/3603163.3609069,
  author	= {Gagliardi, Isabella and Artese, Maria Teresa},
  title		= {Intuitive Semantic Graph Tool for Enhanced Archive
		  Exploration},
  year		= {2023},
  isbn		= {9798400702327},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603163.3609069},
  doi		= {10.1145/3603163.3609069},
  abstract	= {The paper introduces a new method for visualizing and
		  navigating information in a cultural heritage archive in a
		  simple and intuitive way. The proposed approach employs
		  pre-trained language models to cluster data and create
		  semantic graphs. The creation of multilayer maps enables
		  deep exploration of archives with large datasets, while the
		  ability to handle multilingual datasets makes it suitable
		  for archives with documents in various languages. These
		  features combine to provide a user-friendly tool that can
		  be adapted to different contexts and provides an overview
		  of archive contents, to allow even non expert users to
		  successfully query the archive.},
  booktitle	= {Proceedings of the 34th ACM Conference on Hypertext and
		  Social Media},
  articleno	= {11},
  numpages	= {3},
  keywords	= {transformers, pre-trained language models, non-expert
		  users, data visualization, clustering, archives, Bert},
  location	= {Rome, Italy},
  series	= {HT '23}
}

@Article{	  10.1145/3680287,
  author	= {Degha, Houssem Eddine and Laallam, Fatima Zohra},
  title		= {ICA-CRMAS: Intelligent Context-Awareness Approach for
		  Citation Recommendation based on Multi-Agent System},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {3},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3680287},
  doi		= {10.1145/3680287},
  abstract	= {Navigating the ever-expanding sea of scientific literature
		  presents a daunting challenge for researchers seeking
		  relevant and up-to-date information. Traditional citation
		  recommendation systems, while well-intentioned, often fall
		  short due to their limited focus on text-based features and
		  lack of contextual awareness. In this article, we introduce
		  the ICA-CRMAS (Intelligent Context-Aware Approach for
		  Citation Recommendation based on Multi-Agent System), an
		  intelligent system that leverages the power of deep
		  learning, semantic analysis, and multimodal learning to
		  overcome these limitations. ICA-CRMAS goes beyond the
		  surface, delving into the rich tapestry of information
		  within academic papers, including figures, which often hold
		  vital contextual clues. By weaving this contextual data
		  directly into its recommendation models, ICA-CRMAS
		  generates highly personalized and relevant suggestions.
		  This comprehensive approach unlocks enhanced accuracy,
		  diversity, and serendipity, enabling researchers to
		  effectively discover papers aligning with their interests
		  and research objectives. ICA-CRMAS illuminates its
		  reasoning. Instead of opaque suggestions, the system
		  provides clear explanations that justify and illustrate
		  recommended citations. This transparency builds user
		  confidence, allowing researchers to critically engage with
		  and trust the system’s recommendations. Evaluation
		  experiments conducted on real-world academic datasets
		  demonstrate that ICA-CRMAS outperforms existing approaches
		  across various metrics. it surpassing its closest
		  competitor by a margin of 7.53 on accuracy, 6.07\% on MRR
		  and by 5.87 on Recall. User feedback further reinforces its
		  effectiveness, with an Overall System Usability Scale (SUS)
		  score of 76.73, exceeding benchmark scores for comparable
		  systems.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= sep,
  articleno	= {13},
  numpages	= {52},
  keywords	= {Multi-agent systems, ontology, scientific papers,
		  recommendation systems, search engines, user profiles,
		  personalized information filtering, world wide web,
		  context-awareness}
}

@InProceedings{	  10.1145/3417990.3421411,
  author	= {Theisz, Zolt\'{a}n and B\'{a}csi, S\'{a}ndor and Mezei,
		  Gergely and Somogyi, Ferenc A. and Palatinszky, D\'{a}niel},
  title		= {Join potency: a way of combining separate multi-level
		  models},
  year		= {2020},
  isbn		= {9781450381352},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3417990.3421411},
  doi		= {10.1145/3417990.3421411},
  abstract	= {Multi-level modeling has become a mature modeling paradigm
		  both theoretically and by technical means. It has proved
		  itself when a single domain has to be created without
		  accidental complexity. However, when several interconnected
		  domains are to be handled, multi-level modeling is still
		  not as capable as legacy metamodeling. Our position paper
		  aims to narrow the gap by the introduction of a novel
		  technique that can combine several multi-level models from
		  different domains statically. Besides its theoretical
		  proposal, the solution is also defined in our multi-layer
		  modeling framework (Dynamic Multi-Layer Algebra) and is
		  demonstrated by an illustrative example.},
  booktitle	= {Proceedings of the 23rd ACM/IEEE International Conference
		  on Model Driven Engineering Languages and Systems:
		  Companion Proceedings},
  articleno	= {84},
  numpages	= {5},
  keywords	= {potency notion, multi-level modeling, megamodeling,
		  clabject},
  location	= {Virtual Event, Canada},
  series	= {MODELS '20}
}

@InBook{	  10.1145/3677389.3702564,
  author	= {Jin, Yan and Ren, Zongxing and Bi, Chongwu and Sun, Zhuo
		  and Yang, Ruixian},
  title		= {Knowledge Graph Construction of Chinese Traditional Yu
		  Opera Based on Joint Entity-Relation Extraction Method},
  year		= {2025},
  isbn		= {9798400710933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677389.3702564},
  abstract	= {Knowledge of Chinese traditional opera is primarily
		  preserved in informal forms, such as ancient texts,
		  cultural relics, and oral traditions. To appreciate the
		  unique charm of traditional opera, we focus on Yu Opera as
		  a case study and develop the Yu Opera Knowledge Resource
		  (YOKR) Ontology. Our approach involves collecting data from
		  specialized literature, semantically segmenting it, and
		  using the Bert4torch model for entity and relationship
		  extraction. This paper explores the connections among
		  historical figures, roles, plays, music, costumes, and
		  other elements, aiming to enrich knowledge in Yu Opera and
		  provide effective knowledge management services.},
  booktitle	= {Proceedings of the 24th ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {81},
  numpages	= {3}
}

@InProceedings{	  10.1145/3322640.3326728,
  author	= {Zhong, Linwu and Zhong, Ziyi and Zhao, Zinian and Wang,
		  Siyuan and Ashley, Kevin D. and Grabmair, Matthias},
  title		= {Automatic Summarization of Legal Decisions using Iterative
		  Masking of Predictive Sentences},
  year		= {2019},
  isbn		= {9781450367547},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3322640.3326728},
  doi		= {10.1145/3322640.3326728},
  abstract	= {We report on a pilot experiment in automatic, extractive
		  summarization of legal cases concerning Post-traumatic
		  Stress Disorder from the US Board of Veterans' Appeals. We
		  hypothesize that length-constrained extractive summaries
		  benefit from choosing among sentences that are predictive
		  for the case outcome. We develop a novel
		  train-attribute-mask pipeline using a CNN classifier to
		  iteratively select predictive sentences from the case,
		  which measurably improves prediction accuracy on partially
		  masked decisions. We then select a subset for the summary
		  through type classification, maximum marginal relevance,
		  and a summarization template. We use ROUGE metrics and a
		  qualitative survey to evaluate generated summaries along
		  with expert-extracted and expert-drafted summaries. We show
		  that sentence predictiveness does not reliably cover all
		  decision-relevant aspects of a case, illustrate that
		  lexical overlap metrics are not well suited for evaluating
		  legal summaries, and suggest that future work should focus
		  on case-aspect coverage.},
  booktitle	= {Proceedings of the Seventeenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {163–172},
  numpages	= {10},
  keywords	= {text classification, legal case summarization},
  location	= {Montreal, QC, Canada},
  series	= {ICAIL '19}
}

@Article{	  10.1145/3295822,
  author	= {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and
		  Wang, Yinglong and Ma, Jun and Kankanhalli, Mohan},
  title		= {Attentive Long Short-Term Preference Modeling for
		  Personalized Product Search},
  year		= {2019},
  issue_date	= {April 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {37},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3295822},
  doi		= {10.1145/3295822},
  abstract	= {E-commerce users may expect different products even for
		  the same query, due to their diverse personal preferences.
		  It is well known that there are two types of preferences:
		  long-term ones and short-term ones. The former refers to
		  users’ inherent purchasing bias and evolves slowly. By
		  contrast, the latter reflects users’ purchasing
		  inclination in a relatively short period. They both affect
		  users’ current purchasing intentions. However, few
		  research efforts have been dedicated to jointly model them
		  for the personalized product search. To this end, we
		  propose a novel Attentive Long Short-Term Preference model,
		  dubbed as ALSTP, for personalized product search. Our model
		  adopts the neural networks approach to learn and integrate
		  the long- and short-term user preferences with the current
		  query for the personalized product search. In particular,
		  two attention networks are designed to distinguish which
		  factors in the short-term as well as long-term user
		  preferences are more relevant to the current query. This
		  unique design enables our model to capture users’ current
		  search intentions more accurately. Our work is the first to
		  apply attention mechanisms to integrate both long- and
		  short-term user preferences with the given query for the
		  personalized search. Extensive experiments over four Amazon
		  product datasets show that our model significantly
		  outperforms several state-of-the-art product search methods
		  in terms of different evaluation metrics.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {19},
  numpages	= {27},
  keywords	= {long short-term preference, attention mechanism,
		  Personalized product search}
}

@InProceedings{	  10.1145/3184558.3191527,
  author	= {Nikolov, Andriy and Haase, Peter and Herzig, Daniel M. and
		  Trame, Johannes and Kozlov, Artem},
  title		= {Combining RDF Graph Data and Embedding Models for an
		  Augmented Knowledge Graph},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3191527},
  doi		= {10.1145/3184558.3191527},
  abstract	= {Vector embedding models have recently become popular for
		  encoding both structured and unstructured data. In the
		  context of knowledge graphs such models often serve as
		  additional evidence supporting various tasks related to the
		  knowledge base population: e.g., information extraction or
		  link prediction to expand the original dataset. However,
		  the embedding models themselves are often not used directly
		  alongside structured data: they merely serve as additional
		  evidence for structured knowledge extraction. In the
		  metaphactory knowledge graph management platform, we use
		  federated hybrid SPARQL queries for combining explicit
		  information stated in the graph, implicit information from
		  the associated embedding models, and information extracted
		  using vector embeddings in a transparent way for the end
		  user. In this paper we show how we integrated RDF data with
		  vector space models to construct an augmented knowledge
		  graph to be used in customer applications.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {977–980},
  numpages	= {4},
  keywords	= {deep learning, graph embeddings, knowledge graph, machine
		  learning, query federation},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@Article{	  10.1109/tcbb.2020.3000518,
  author	= {V, Sunil Kumar P and Thahsin, Adheeba and M, Manju and G,
		  Gopakumar},
  title		= {A Heterogeneous Information Network Model for Long
		  Non-Coding RNA Function Prediction},
  year		= {2020},
  issue_date	= {Jan.-Feb. 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2020.3000518},
  doi		= {10.1109/TCBB.2020.3000518},
  abstract	= {Exciting information on the functional roles played by
		  long non-coding RNA (lncRNA) has drawn substantial research
		  attention these days. With the advent of techniques such as
		  RNA-Seq, thousands of lncRNAs are identified in very short
		  time spans. However, due to the poor annotation rate, only
		  a few of them are functionally characterised. The wet lab
		  experiments to elucidate lncRNA functions are challenging,
		  slow progressing and sometimes prohibitively expensive.
		  This work attempts to solve the crucial problem of
		  developing computational methods to predict lncRNA
		  functions. The model presented here, predicts the functions
		  of lncRNAs by making use of a meta-path based measure,
		  AvgSim on a Heterogeneous Information Network (HIN). The
		  network is constructed from existing protein and function
		  association data of lncRNAs, lncRNA co-expression data and
		  protein protein interaction data. Out of the 2,758 lncRNA
		  considered for the experiment, the proposed method predicts
		  possible functions for 2,695 lncRNAs with an accuracy of
		  73.68 percent and found to perform better than the other
		  state-of-the-art approaches for an independent test set. A
		  case study of two well-known lncRNAs (HOTAIR and H19) is
		  conducted and the associated functions are identified. The
		  results were validated using experimental evidence from the
		  literature. The script and data used for the implementation
		  of the model is freely available at:
		  &lt;uri&gt;http://bdbl.nitc.ac.in/LncFunPred/index.html&lt;/uri&gt;.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jun,
  pages		= {255–266},
  numpages	= {12}
}

@InProceedings{	  10.1109/jcdl.2019.00117,
  author	= {Weber, Nicholas and Fenlon, Katrina and Organisciak, Peter
		  and Thomer, Andrea K.},
  title		= {Conceptual models in digital libraries, archives, and
		  museums},
  year		= {2020},
  isbn		= {9781728115474},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL.2019.00117},
  doi		= {10.1109/JCDL.2019.00117},
  abstract	= {This workshop addresses the development, use, and
		  evolution of conceptual models in the context of digital
		  libraries, archives, and museums. The workshop will convene
		  domain practitioners and researchers in order to formalize
		  a research agenda for conceptual modelling in digital
		  libraries, and foster a cooperative research community to
		  make progress on these topics over the coming decade.
		  Activities at the workshop will include paper
		  presentations, round-table discussions, and a keynote from
		  an expert in the conceptual and logical foundations of
		  information organization systems. Workshop papers as well
		  as a summary of the proceedings will be published in a free
		  and openly accessible repository.},
  booktitle	= {Proceedings of the 18th Joint Conference on Digital
		  Libraries},
  pages		= {457–458},
  numpages	= {2},
  keywords	= {digital libraries, conceptual models},
  location	= {Champaign, Illinois},
  series	= {JCDL '19}
}

@InProceedings{	  10.1145/3371300.3383339,
  author	= {Pelzetter, Jens},
  title		= {A declarative model for accessibility requirements},
  year		= {2020},
  isbn		= {9781450370561},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3371300.3383339},
  doi		= {10.1145/3371300.3383339},
  abstract	= {The web has become the primary source of information for
		  many people. Many services are provided are on the web.
		  Despite extensive guidelines for the accessibility of web
		  pages, many web sites are not accessible making these web
		  sites difficult or impossible to use for people with
		  disabilities. Evaluating the accessibility of web pages can
		  either be done manually, which is a very laborious task or
		  using automated tools. Unfortunately, the results from
		  different tools are often inconsistent because of the
		  ambiguity of the current guidelines. In this paper, a
		  declarative approach for describing the requirements for
		  accessible web pages is presented. This declarative model
		  will help developers of accessibility evaluation tools to
		  create tools that produce more consistent results and are
		  easier to maintain.},
  booktitle	= {Proceedings of the 17th International Web for All
		  Conference},
  articleno	= {4},
  numpages	= {10},
  keywords	= {accessibility, WCAG, ACT rules},
  location	= {Taipei, Taiwan},
  series	= {W4A '20}
}

@Article{	  10.1145/3725729,
  author	= {Xu, Nuo and Wang, Pinghui and Liang, Zi and Zhao, Junzhou
		  and Guan, Xiaohong},
  title		= {How Vital Is the Jurisprudential Relevance: Law
		  Article-Intervened Legal Case Retrieval and Matching},
  year		= {2025},
  issue_date	= {July 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3725729},
  doi		= {10.1145/3725729},
  abstract	= {Legal case retrieval aims to automatically scour
		  comparable legal cases based on a given query, which is
		  crucial for offering relevant precedents to support the
		  judgment in intelligent legal systems. Due to similar
		  goals, it is often associated with a similar case matching
		  task. To address them, a daunting challenge is assessing
		  the uniquely defined legal-rational similarity within the
		  judicial domain, which distinctly deviates from the
		  semantic similarities in general text retrieval. Past works
		  either tagged domain-specific factors or incorporated
		  reference laws to capture legal-rational information.
		  However, their heavy reliance on expert or unrealistic
		  assumptions restricts their practical applicability in
		  real-world scenarios. In this article, we propose an
		  end-to-end model named LCM-LAI to solve the above
		  challenges. Through meticulous theoretical analysis,
		  LCM-LAI employs a dependent multi-task learning framework
		  to capture legal-rational information within legal cases by
		  a law article prediction sub-task, without any additional
		  assumptions in inference. In addition, LCM-LAI proposes an
		  article-aware attention mechanism to evaluate the
		  legal-rational similarity between across-case sentences
		  based on the law distribution, which is more effective than
		  semantic similarity. We perform a series of exhaustive
		  experiments that include two different tasks that involving
		  four real-world datasets. The results demonstrate that
		  LCM-LAI achieves state-of-the-art performance.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= may,
  articleno	= {85},
  numpages	= {32},
  keywords	= {legal case retrieval, legal case matching, dependent
		  multi-task learning}
}

@Article{	  10.1145/3274784.3274789,
  author	= {Ferro, Nicola and Fuhr, Norbert and Grefenstette, Gregory
		  and Konstan, Joseph A. and Castells, Pablo and Daly,
		  Elizabeth M. and Declerck, Thierry and Ekstrand, Michael D.
		  and Geyer, Werner and Gonzalo, Julio and Kuflik, Tsvi and
		  Lindn, Krister and Magnini, Bernardo and Nie, Jian-Yun and
		  Perego, Raffaele and Shapira, Bracha and Soboroff, Ian and
		  Tintarev, Nava and Verspoor, Karin and Willemsen, Martijn
		  C. and Zobel, Justin},
  title		= {The Dagstuhl Perspectives Workshop on Performance Modeling
		  and Prediction},
  year		= {2018},
  issue_date	= {June 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {1},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3274784.3274789},
  doi		= {10.1145/3274784.3274789},
  abstract	= {This paper reports the findings of the Dagstuhl
		  Perspectives Workshop 17442 on performance modeling and
		  prediction in the domains of Information Retrieval, Natural
		  language Processing and Recommender Systems. We present a
		  framework for further research, which identifies five major
		  problem areas: understanding measures, performance
		  analysis, making underlying assumptions explicit,
		  identifying application features determining performance,
		  and the development of prediction models describing the
		  relationship between assumptions, features and resulting
		  performance.},
  journal	= {SIGIR Forum},
  month		= aug,
  pages		= {91–101},
  numpages	= {11}
}

@InProceedings{	  10.1145/3709026.3709114,
  author	= {Chang, Bingtao and Wen, Weiping and Wu, Xiaojie and Cheng,
		  Siyang and Jiang, Jianchun and Mei, Rui},
  title		= {TCLens: Towards Toxicity Tags Aggregation of Massive
		  Labels Generated by Content Moderation for AIGC},
  year		= {2025},
  isbn		= {9798400718182},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3709026.3709114},
  doi		= {10.1145/3709026.3709114},
  abstract	= {The recent boost of artificial intelligence represented by
		  Large Language Models (LLMs) is surging. Due to the
		  outstanding performance of LLMs, AI-Generated Content
		  (AIGC) has also made important progress in multimodal
		  knowledge creation referring to text, image, audio, and
		  video. However, the security, privacy, and ethical risks
		  associated with AIGC (e.g., fake news, social engineering
		  attacks, and toxic content) have deeply weakened the
		  compliance of AIGC. Although existing content moderation
		  solutions can filter out several types of toxic content,
		  the audit performance of different vendors and techniques
		  are of varying quality. Some AIGC service providers improve
		  the moderation effectiveness by introducing multiple
		  sources of audit vendors. Due to the lack of general
		  content moderation standards and taxonomy, the labels of
		  multi-source moderation vendors vary greatly. To this end,
		  We propose a novel massive label aggregation approach for
		  content moderation named TCLens. First, we collect results
		  of multi-vendor content moderation engines for building
		  massive toxic labels for AIGC. Then, we introduce an
		  ontology for better tagging with the capability of
		  automatic updating and vendor-agnostic. Finally, we
		  implement a prototype of TCLens. Our evaluation
		  demonstrates that it outperforms single-source tagging and
		  existing SOTA solutions.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {466–473},
  numpages	= {8},
  keywords	= {information content moderation, toxicity tags, labels
		  aggregation, AIGC},
  location	= { },
  series	= {CSAI '24}
}

@InProceedings{	  10.1145/3704814.3704819,
  author	= {Pang, Haijie and Li, Chengji},
  title		= {A BERT and TextCNN integration-based Method for Public
		  Complaints and Proposals Text Classification},
  year		= {2025},
  isbn		= {9798400718090},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704814.3704819},
  doi		= {10.1145/3704814.3704819},
  abstract	= {With the wide use of Blockchain technology and online
		  public complaints and proposals (hereinafter referred to as
		  PCP) platforms, huge amounts of data are accordingly
		  produced, which poses a challenge on the government's
		  effectively choosing and classifying the key information.
		  In view of this, this paper endeavors to conduct research
		  on the PCP text classification for a smart government.
		  Firstly, PCP texts are collected automatically and
		  preprocessed, and then a hierarchical classification
		  structure involving targeted government departments and PCP
		  contents is proposed. Finally, a BERT and TextCNN model is
		  adopted for PCP text classification. The experimental
		  results show that the classification structure and model
		  proposed in this paper can effectively categorize the PCP
		  text, providing a scientific analysis and effective
		  technological method to improve the work efficiency and
		  service quality of PCP.},
  booktitle	= {Proceedings of the 8th International Conference on
		  Computer Science and Application Engineering},
  pages		= {15–18},
  numpages	= {4},
  keywords	= {BERT, Blockchain, Public complaints and proposals, Text
		  classification, TextCNN},
  location	= { },
  series	= {CSAE '24}
}

@InProceedings{	  10.1145/3310986.3311025,
  author	= {Jin, Ying and Zhao, Shuai and Wu, Yudong},
  title		= {Geographic Entity Relationship Extraction Model Based on
		  Piecewise Convolution of Residual Network},
  year		= {2019},
  isbn		= {9781450366120},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3310986.3311025},
  doi		= {10.1145/3310986.3311025},
  abstract	= {Nowadays, geographic entity relationship extraction
		  systems generally rely on artificial feature extraction.
		  These features either require complex and complete data
		  sets, or cannot describe deep features such as semantics.
		  And data sets that can be used for geographic relationship
		  extraction are scarce. To tackle these problems, this paper
		  uses distant supervision to map existing knowledge bases
		  into rich unstructured data which contributes to a large
		  amount of training data. In training, this paper uses the
		  deep residual network to extract more abstract and deeper
		  features. Then the piecewise max pooling and selective
		  attention mechanisms are used to further improve the
		  accuracy of the model. Finally, the experimental results
		  show that the deeper network and the piecewise max pooling
		  significantly improve the extraction results.},
  booktitle	= {Proceedings of the 3rd International Conference on Machine
		  Learning and Soft Computing},
  pages		= {160–165},
  numpages	= {6},
  keywords	= {relationship extraction, distant supervision, attention,
		  ResNet, PCNN, Gis},
  location	= {Da Lat, Viet Nam},
  series	= {ICMLSC '19}
}

@InProceedings{	  10.1145/3477314.3507177,
  author	= {Groza, Adrian and Nitu, Cristian},
  title		= {Question answering over logic puzzles using theorem
		  proving},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3507177},
  doi		= {10.1145/3477314.3507177},
  abstract	= {We developed a tool to automatically solve logical puzzles
		  in natural language. The solution is composed by a parser
		  and an inference engine. The parser translates the text
		  into First Order Logic (FOL), while the Mace4 model finder
		  computes the interpretation models of the given FOL theory.
		  The solver uses the Prover9 theorem prover to compute
		  Yes/No answers to natural language questions related to
		  each puzzle. Each answer is backup by a graphical
		  representation of its proof, which is in line with
		  Explainable Artificial Intelligence (XAI). The advantage of
		  using reasoning for Natural Language Understanding (NLU)
		  instead of learned models is that the user can obtain an
		  explanation of the reasoning chain. We illustrate how the
		  system performs on 382 knights and knaves puzzles. These
		  features together with the overall performance rate of
		  80.89\% makes the proposed solution an improvement upon
		  similar solvers for natural language understanding in the
		  puzzles domain.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {871–874},
  numpages	= {4},
  keywords	= {explainable ai, logical puzzles, natural language
		  understanding, question answering},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@InProceedings{	  10.1145/3383583.3398530,
  author	= {Vogt, Lars and D'Souza, Jennifer and Stocker, Markus and
		  Auer, S\"{o}ren},
  title		= {Toward Representing Research Contributions in Scholarly
		  Knowledge Graphs Using Knowledge Graph Cells},
  year		= {2020},
  isbn		= {9781450375856},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383583.3398530},
  doi		= {10.1145/3383583.3398530},
  abstract	= {There is currently a gap between the natural language
		  expression of scholarly publications and their structured
		  semantic content modeling to enable intelligent content
		  search. With the volume of research growing exponentially
		  every year, a search feature operating over semantically
		  structured content is compelling. Toward this end, in this
		  work, we propose a novel semantic data model for modeling
		  the contribution of scientific investigations. Our model,
		  i.e. the Research Contribution Model (RCM), includes a
		  schema of pertinent concepts highlighting six core
		  information units, viz. Objective, Method, Activity, Agent,
		  Material, and Result, on which the contribution hinges. It
		  comprises bottom-up design considerations made from three
		  scientific domains, viz. Medicine, Computer Science, and
		  Agriculture, which we highlight as case studies. For its
		  implementation in a knowledge graph application we
		  introduce the idea of building blocks called Knowledge
		  Graph Cells (KGC), which provide the following
		  characteristics: (1) they limit the expressibility of
		  ontologies to what is relevant in a knowledge graph
		  regarding specific concepts on the theme of research
		  contributions; (2) they are expressible via ABox and TBox
		  expressions; (3) they enforce a certain level of data
		  consistency by ensuring that a uniform modeling scheme is
		  followed through rules and input controls; (4) they
		  organize the knowledge graph into named graphs; (5) they
		  provide information for the front end for displaying the
		  knowledge graph in a human-readable form such as HTML
		  pages; and (6) they can be seamlessly integrated into any
		  existing publishing process thatsupports form-based input
		  abstracting its semantic technicalities including RDF
		  semantification from the user. Thus RCM joins the trend of
		  existing work toward enhanced digitalization of scholarly
		  publication enabled by an RDF semantification as a
		  knowledge graph fostering the evolution of the scholarly
		  publications beyond written text.},
  booktitle	= {Proceedings of the ACM/IEEE Joint Conference on Digital
		  Libraries in 2020},
  pages		= {107–116},
  numpages	= {10},
  keywords	= {semantic publishing, scholarly infrastructure, open
		  science, ontology, machine actionability, fair data
		  principles, digital libraries},
  location	= {Virtual Event, China},
  series	= {JCDL '20}
}

@InProceedings{	  10.1145/3555776.3577686,
  author	= {Loseto, Giuseppe and Scioscia, Floriano and Ruta, Michele
		  and Gramegna, Filippo and Bilenchi, Ivano},
  title		= {Semantic-based Adaptation of Quality of Experience in Web
		  Multimedia Streams},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3577686},
  doi		= {10.1145/3555776.3577686},
  abstract	= {Video streaming accounts for the majority of worldwide
		  Internet traffic, and HTTP-based multimedia has the largest
		  share among technologies and protocols. The wide
		  availability of mobile devices and wireless broadband
		  networks currently leads to wider heterogeneity of fruition
		  contexts and frequent condition changes during a streaming
		  session. MPEG-DASH is the reference standard for Dynamic
		  Adaptive Streaming over HTTP: a provider defines several
		  representations for a segmented multimedia source, with
		  different bit rates, allowing a client to dynamically
		  select the best one based on current conditions, and to
		  download the corresponding sequence of segments for smooth
		  playback. MPEG-DASH does not mandate specific bit rate
		  adaptation schemes; conventional approaches are divided in
		  buffer-based, bandwidth-based and hybrid. Nevertheless,
		  Quality of Experience (QoE) can be influenced by many
		  additional factors. This paper proposes a novel QoE
		  adaptation approach based on dynamic ontology-based
		  annotation of streaming context and mobile matchmaking with
		  DASH representation profiles in a Web Ontology Language
		  (OWL) fragment, exploiting a WebAssembly port of an
		  embedded reasoning engine. The proposed framework enables
		  adaptation based not only on network status, but also on
		  client device capabilities, ambient conditions and
		  multimedia content type. A case study validates the
		  proposal, while early experiments support its
		  sustainability.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1821–1830},
  numpages	= {10},
  keywords	= {MPEG-DASH, multimedia streaming, quality of experience,
		  web ontology language (OWL), semantic matchmaking},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@InProceedings{	  10.1145/3448139.3448180,
  author	= {Lin, Yiwen and Dowell, Nia and Godfrey, Andrew},
  title		= {Skills Matter: Modeling the relationship between decision
		  making processes and collaborative problem-solving skills
		  during Hidden Profile Tasks},
  year		= {2021},
  isbn		= {9781450389358},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3448139.3448180},
  doi		= {10.1145/3448139.3448180},
  abstract	= {Collaborative problem-solving (CPS) is one of the most
		  essential 21st century skills for success across
		  educational and professional settings. The hidden-profile
		  paradigm is one of the most prominent avenues of studying
		  group decision making and underlying issues in information
		  sharing. Previous research on the hidden-profile paradigm
		  has primarily focused on static constructs (e.g., group
		  size, group expertise), or on the information itself
		  (whether certain pieces of information is being shared). In
		  the current study, we propose a lens on individual and
		  group’s collaborative problem-solving skills, to explore
		  the relationships between dynamic discourse processes and
		  decision making in a distributed information environment.
		  Specifically, we sought to examine CPS skills in
		  association with decision change and productive
		  decision-making. Our results suggest that while sharing
		  information has significantly positive association with
		  decision change and effective decision-making, other
		  aspects of social processes appear to be negatively
		  correlated with these outcomes. Cognitive CPS skills,
		  however, exhibit a strong positive relationship with making
		  a (productive) change in students final decisions. We also
		  find that these results are more pronounced at the group
		  level, particularly with cognitive CPS skills. Our study
		  shed lights on a more nuanced picture of how social and
		  cognitive CPS interactions are related to effective
		  information sharing and decision making in collaborative
		  problem-solving interactions.},
  booktitle	= {LAK21: 11th International Learning Analytics and Knowledge
		  Conference},
  pages		= {428–437},
  numpages	= {10},
  keywords	= {group processes, decision making, collaborative problem
		  solving, Hidden-profile paradigm},
  location	= {Irvine, CA, USA},
  series	= {LAK21}
}

@InProceedings{	  10.1145/3603765.3603771,
  author	= {Diaz Gonzalez, Armando D. and Hughes, Kevin S. and Yue,
		  Songhui and Hayes, Sean T.},
  title		= {Applying BioBERT to Extract Germline Gene-Disease
		  Associations for Building a Knowledge Graph from the
		  Biomedical Literature},
  year		= {2023},
  isbn		= {9798400700637},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603765.3603771},
  doi		= {10.1145/3603765.3603771},
  abstract	= {Published biomedical information has and continues to
		  rapidly increase. The recent advancements in Natural
		  Language Processing (NLP), have generated considerable
		  interest in automating the extraction, normalization, and
		  representation of biomedical knowledge about entities such
		  as genes and diseases. Our study analyzes germline
		  abstracts in the construction of knowledge graphs of the
		  immense work that has been done in this area for genes and
		  diseases. This paper presents SimpleGermKG, an automatic
		  knowledge graph construction approach that connects
		  germline genes and diseases. For the extraction of genes
		  and diseases, we employ BioBERT, a pre-trained BERT model
		  on biomedical corpora. We propose an ontology-based and
		  rule-based algorithm to standardize and disambiguate
		  medical terms. For semantic relationships between articles,
		  genes, and diseases, we implemented a part-whole relation
		  approach to connect each entity with its data source and
		  visualize them in a graph-based knowledge representation.
		  Lastly, we discuss the knowledge graph applications,
		  limitations, and challenges to inspire the future research
		  of germline corpora. Our knowledge graph contains 297
		  genes, 130 diseases, and 46,747 triples. Graph-based
		  visualizations are used to show the results.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Information System and Data Mining},
  pages		= {37–42},
  numpages	= {6},
  keywords	= {BioBERT, entity recognition, germline mutations, knowledge
		  graph, semantic relation},
  location	= {Atlanta, USA},
  series	= {ICISDM '23}
}

@InProceedings{	  10.1145/3342558.3345414,
  author	= {Piotrowski, Michael},
  title		= {A Vision for User-Defined Semantic Markup},
  year		= {2019},
  isbn		= {9781450368872},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3342558.3345414},
  doi		= {10.1145/3342558.3345414},
  abstract	= {Typesetting systems, such as LATEX, permit users to define
		  custom markup and corresponding formatting to simplify
		  authoring, ensure the consistent presentation of
		  domain-specific recurring elements and, potentially, enable
		  further processing, such as the generation of an index of
		  such elements. In XML-based and similar systems, the
		  separation of content and form is also reflected in the
		  processing pipeline: while document authors can define
		  custom markup, they cannot define its semantics. This could
		  be said to be intentional to ensure structural integrity of
		  documents, but at the same time it limits the expressivity
		  of markup. The latter is particularly true for so-called
		  lightweight markup languages like Mark-down, which only
		  define very limited sets of generic elements. This vision
		  paper sketches an approach for user-defined semantic markup
		  that could permit authors to define the semantics of
		  elements by formally describing the relations between its
		  constituent parts and to other elements, and to define a
		  formatting intent that would ensure that a default
		  presentation is always available.},
  booktitle	= {Proceedings of the ACM Symposium on Document Engineering
		  2019},
  articleno	= {28},
  numpages	= {4},
  keywords	= {scholarly publishing, markup semantics, document models
		  and structures, document authoring},
  location	= {Berlin, Germany},
  series	= {DocEng '19}
}

@Article{	  10.1145/3505639.3505647,
  author	= {Yu, Xi and Zaza, Sam and Schuberth, Florian and Henseler,
		  J\"{o}rg},
  title		= {Counterpoint: Representing Forged Concepts as Emergent
		  Variables Using Composite-Based Structural Equation
		  Modeling},
  year		= {2022},
  issue_date	= {December 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {SI},
  issn		= {0095-0033},
  url		= {https://doi.org/10.1145/3505639.3505647},
  doi		= {10.1145/3505639.3505647},
  abstract	= {Studying and modeling theoretical concepts is a
		  cornerstone activity in information systems (IS) research.
		  Researchers have been familiar with one type of theoretical
		  concept, namely behavioral concepts, which are assumed to
		  exist in nature and measured by a set of observable
		  variables. In this paper, we present a second type of
		  theoretical concept, namely forged concepts, which are
		  designed and assumed to emerge within their environment.
		  While behavioral concepts are classically operationalized
		  as latent variables, forged concepts are better specified
		  as emergent variables. Additionally, we propose
		  composite-based structural equation modeling (SEM) as a
		  subtype of SEM that is eminently suitable to analyze models
		  containing emergent variables. We shed light on the
		  composite-based SEM steps: model specification, model
		  identification, model estimation, and model assessment.
		  Then, we present an illustrative example from the domain of
		  IS research to demonstrate these four steps and show how
		  modeling with emergent variables proceeds.},
  journal	= {SIGMIS Database},
  month		= dec,
  pages		= {114–130},
  numpages	= {17},
  keywords	= {forged concept, emergent variables, composite-based
		  structural equation modeling, composite model, behavioral
		  concept}
}

@InProceedings{	  10.1145/3640457.3688071,
  author	= {Irrera, Ornella and Lissandrini, Matteo and Dell'Aglio,
		  Daniele and Silvello, Gianmaria},
  title		= {Reproducibility and Analysis of Scientific Dataset
		  Recommendation Methods},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688071},
  doi		= {10.1145/3640457.3688071},
  abstract	= {Datasets play a central role in scholarly communications.
		  However, scholarly graphs are often incomplete,
		  particularly due to the lack of connections between
		  publications and datasets. Therefore, the importance of
		  dataset recommendation—identifying relevant datasets for
		  a scientific paper, an author, or a textual query—is
		  increasing. Although various methods have been proposed for
		  this task, their reproducibility remains unexplored, making
		  it difficult to compare them with new approaches. We
		  reviewed current recommendation methods for scientific
		  datasets, focusing on the most recent and competitive
		  approaches, including an SVM-based model, a bi-encoder
		  retriever, a method leveraging co-authors and citation
		  network embeddings, and a heterogeneous variational graph
		  autoencoder. These approaches underwent a comprehensive
		  analysis under consistent experimental conditions. Our
		  reproducibility efforts show that three methods can be
		  reproduced, while the graph variational autoencoder is
		  challenging due to unavailable code and test datasets.
		  Hence, we re-implemented this method and performed a
		  component-based analysis to examine its strengths and
		  limitations. Furthermore, our study indicated that three
		  out of four considered methods produce subpar results when
		  applied to real-world data instead of specialized datasets
		  with ad-hoc features.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {570–579},
  numpages	= {10},
  keywords	= {Dataset Recommendations, Recommender Systems,
		  Reproducibility},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3389189.3397978,
  author	= {Sosnowski, Tomasz and Yordanova, Kristina},
  title		= {A probabilistic conversational agent for intelligent
		  tutoring systems},
  year		= {2020},
  isbn		= {9781450377737},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3389189.3397978},
  doi		= {10.1145/3389189.3397978},
  abstract	= {The paper proposes a novel approach for a Conversational
		  Intelligent Tutoring System, which combines Natural
		  Language Processing techniques with dynamic Bayesian
		  Inference for probabilistic student state estimation. Our
		  aim is to develop and implement the probabilistic model for
		  the discourse between the user and the Conversational
		  Agent. Most of the already proposed tutoring systems
		  interacting with the user via natural language use rather
		  simple pattern matching rules to carry on the conversation.
		  Even when the fuzzy pattern matching is employed, already
		  existing approaches use these fuzzy values only to find a
		  single best-matching rule, thus immediately discretising
		  the result. Our proposed approach differs in this regard,
		  as we would like to use the fuzzy values resulting from
		  pattern matching as inputs for our probabilistic model. To
		  be more precise, we would like to treat these values as
		  observations for the state estimation in the dynamic
		  Bayesian Network. Our approach is, not coincidentally,
		  similar to the methods typical for state and action
		  recognition. This paper focuses on the Natural Language
		  processing aspect, while the probabilistic model is
		  described in a separate work.},
  booktitle	= {Proceedings of the 13th ACM International Conference on
		  PErvasive Technologies Related to Assistive Environments},
  articleno	= {43},
  numpages	= {7},
  keywords	= {text similarity, sentence similarity, semantic similarity,
		  natural language understanding, natural language
		  processing, markov chain, intelligent tutoring system,
		  dynamic bayesian inference, conversational agent,
		  computational state space models},
  location	= {Corfu, Greece},
  series	= {PETRA '20}
}

@InProceedings{	  10.1145/3596947.3596952,
  author	= {Skobelev, Petr and Simonova, Elena and Tabachinskiy,
		  Aleksey and Kudryakov, Evgeniy and Strizhakov, Anatoly and
		  Goryanin, Oleg and Ermakov, Vasiliy and Chan, Yung-Kuan and
		  Lee, Tzong-Ru and Sung, Yu},
  title		= {Concept and Development of a Multi-Agent Digital Twin of
		  Plant Focused on Broccoli},
  year		= {2023},
  isbn		= {9781450399920},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3596947.3596952},
  doi		= {10.1145/3596947.3596952},
  abstract	= {The paper discusses the principles of developing a
		  multi-agent digital twin of plants using broccoli as an
		  example of plants. The developed model of the digital twin
		  of plants must meet the following requirements: real-time
		  environmental data acquisition, user feedback collection,
		  continuous adaptation of the plant development plan for
		  each event, individual instance for field or field part.
		  The digital twin of plant is designed as an intelligent
		  cyber-physical system that has a user-defined knowledge bas
		  and a multi-agent system for planning and modeling of plant
		  growth and development, as well as for forecasting crop
		  parameters. For this purpose, a new method for estimate
		  stage duration and yield is proposed, which defines a
		  "tube" – a corridor to each of the factors corresponding
		  plant development. The key factors have been determined
		  during consultations with practicing agronomists but can be
		  adjusted by users experience. This concept was originally
		  introduced for wheat digital twin, but now is scaled and
		  modified to simulate broccoli growth process.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Intelligent Systems, Metaheuristics \&amp; Swarm
		  Intelligence},
  pages		= {132–138},
  numpages	= {7},
  keywords	= {precision farming, ontology, multi-agent technologies,
		  multi-agent model, knowledge base, intelligent services,
		  digital twin, broccoli cultivation},
  location	= {Virtual Event, Malaysia},
  series	= {ISMSI '23}
}

@InProceedings{	  10.1145/3508397.3564853,
  author	= {Beldi, Amal and Sassi, Salma and Jemai, Abedrazzek},
  title		= {Learn2Sum: A New Approach to Unsupervised Text
		  Summarization Based on Topic Modeling},
  year		= {2022},
  isbn		= {9781450392198},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3508397.3564853},
  doi		= {10.1145/3508397.3564853},
  abstract	= {Due to the enormous volume of data on the web, it is hard
		  for the user to retrieve effective and useful information
		  within the right time. Thus, it has become a need to
		  generate a brief summary from a large amount of textual
		  data according to the user profile. In this context, text
		  summarization is used to identify important information
		  within text documents. It aims to generate shorter versions
		  of the source text, by including only the relevant and
		  salient information. In recent years, the research on
		  summarization techniques based on topic modeling techniques
		  has become a hot topic among researchers thanks to their
		  ability to classify, understand a large text corpora and
		  extract important topics on the text. However, existing
		  studies do not provide the support of personalization when
		  generating summaries because they need to know not only
		  which documents are most helpful to the users, but also
		  which topics and keywords are more or less related to the
		  user' interests. Thus, existing studies lack of the support
		  of adaptive user modeling for user applications in the
		  emerging areas of automatic summarization, topic modeling
		  and visualization. In this context, we propose a new
		  approach of automated text summarization based on topic
		  modeling techniques and taking into account the user's
		  profile which helps to semantically extract relevant topics
		  of textual documents, summarizing information according to
		  the user' topics interests and finally visualize them
		  through a hyper-graph Experiments have been conducted to
		  measure the effectiveness of our solution compared to
		  existing summarizing approaches based on text content. The
		  results show the superiority of our approach.},
  booktitle	= {Proceedings of the 14th International Conference on
		  Management of Digital EcoSystems},
  pages		= {136–143},
  numpages	= {8},
  keywords	= {user profile, topics, topic modeling, text transformation,
		  summarization, graph, classification},
  location	= {Venice, Italy},
  series	= {MEDES '22}
}

@Article{	  10.1145/3590773,
  author	= {Becattini, Federico and Bongini, Pietro and Bulla, Luana
		  and Bimbo, Alberto Del and Marinucci, Ludovica and
		  Mongiov\`{\i}, Misael and Presutti, Valentina},
  title		= {VISCOUNTH: A Large-scale Multilingual Visual Question
		  Answering Dataset for Cultural Heritage},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {6},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3590773},
  doi		= {10.1145/3590773},
  abstract	= {Visual question answering has recently been settled as a
		  fundamental multi-modal reasoning task of artificial
		  intelligence that allows users to get information about
		  visual content by asking questions in natural language. In
		  the cultural heritage domain, this task can contribute to
		  assisting visitors in museums and cultural sites, thus
		  increasing engagement. However, the development of visual
		  question answering models for cultural heritage is
		  prevented by the lack of suitable large-scale datasets. To
		  meet this demand, we built a large-scale heterogeneous and
		  multilingual (Italian and English) dataset for cultural
		  heritage that comprises approximately 500K Italian cultural
		  assets and 6.5M question-answer pairs. We propose a novel
		  formulation of the task that requires reasoning over both
		  the visual content and an associated natural language
		  description, and present baselines for this task. Results
		  show that the current state of the art is reasonably
		  effective but still far from satisfactory; therefore,
		  further research in this area is recommended. Nonetheless,
		  we also present a holistic baseline to address visual and
		  contextual questions and foster future research on the
		  topic.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= jul,
  articleno	= {193},
  numpages	= {20},
  keywords	= {cultural heritage, Visual question answering}
}

@InProceedings{	  10.1145/3631700.3665237,
  author	= {Zavarella, Vanni and Reforgiato, Diego and Consoli, Sergio
		  and Fenu, Gianni},
  title		= {Charting the Landscape of Digital Health: Towards A
		  Knowledge Graph Approach to News Media Analysis},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3665237},
  doi		= {10.1145/3631700.3665237},
  abstract	= {In this paper, we present our currently on-going work on a
		  method for analyzing digital health transformation in our
		  society by constructing a Knowledge Graph from a large
		  corpus of 7.8 million English news articles, dating from
		  1987 through 2023. We firstly sampled around 95k articles
		  relevant to the Digital Health topic by training and
		  deploying a Deep Learning binary classifier via fine-tuning
		  BERT. Successively, by deploying NLP techniques, we
		  extracted triples from the identified articles to form a
		  Digital Health News Knowledge Graph, which consists of 431k
		  distinct triples connecting 186k entities through 1866
		  relations. The constructed Knowledge Graph provides
		  insights into the evolution of Digital Health in news media
		  and serves as a resource for further research in the field.
		  The analysis that we have carried out reveals significant
		  trends in Digital Health as reflected in the news, with
		  notable peaks coinciding with key events like the COVID-19
		  pandemic.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {419–423},
  numpages	= {5},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@Article{	  10.1145/3371906,
  author	= {Kienzle, J\"{o}rg and Mussbacher, Gunter and Combemale,
		  Benoit and Bastin, Lucy and Bencomo, Nelly and Bruel,
		  Jean-Michel and Becker, Christoph and Betz, Stefanie and
		  Chitchyan, Ruzanna and Cheng, Betty H. C. and Klingert,
		  Sonja and Paige, Richard F. and Penzenstadler, Birgit and
		  Seyff, Norbert and Syriani, Eugene and Venters, Colin C.},
  title		= {Toward model-driven sustainability evaluation},
  year		= {2020},
  issue_date	= {March 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {63},
  number	= {3},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3371906},
  doi		= {10.1145/3371906},
  abstract	= {Exploring the vision of a model-based framework that may
		  enable broader engagement with and informed decision making
		  about sustainability issues.},
  journal	= {Commun. ACM},
  month		= feb,
  pages		= {80–91},
  numpages	= {12}
}

@InProceedings{	  10.1145/3587259.3627563,
  author	= {Alghamdi, Ghadah Abdulrahman S and Schmidt, Renate A. and
		  Gao, Yongsheng},
  title		= {Focus Set Semantic Differences},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627563},
  doi		= {10.1145/3587259.3627563},
  abstract	= {Ontologies are being utilized widely as sources for
		  formally organized information in a range of fields. The
		  SNOMED CT ontology is a key resource in national and
		  international health sectors for automatically linking
		  information captured by diverse clinical information
		  systems and research data ensuring consistent patient data
		  capture and effective data analytics and decision support.
		  Offering a comprehensive multilingual vocabulary for
		  encoding clinical knowledge of multiple domains, the
		  ontology is large and new releases are created regularly to
		  reflect domain changes and user requirements. The main
		  contribution of the paper is a novel automated approach for
		  tracking semantic differences of subdomains in different
		  versions of SNOMED CT targeted at terminologists,
		  debuggers, ontology evaluators and developers of software
		  using SNOMED CT. Whereas the semantic difference sets
		  produced with existing methods are rather large and
		  difficult to analyze, our method produces concise semantic
		  difference sets for user-specified input focus concepts.
		  Our method is based on subontology generation and semantic
		  difference computation using uniform interpolation, which
		  aids in finding inferred differences that other semantic
		  difference tools do not reveal. The obtained semantic
		  difference sets are related to the meaning of focus concept
		  definitions for specific ontology subdomains, where some of
		  these differences would not have been generated without
		  this focused method for computing semantic differences
		  between ontologies. A case study using SNOMED CT has shown
		  the proposed approach is useful for domain experts.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {250–258},
  numpages	= {9},
  keywords	= {Forgetting/Uniform Interpolation, Modularisation, Ontology
		  Engineering, Ontology Extraction, SNOMED CT, Semantic
		  Differences, Subontologies, Subontology Extraction},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3265757.3265764,
  author	= {Grgurina, Natasa and Barendsen, Erik and Suhre, Cor and
		  Zwaneveld, Bert and van Veen, Klaas},
  title		= {Assessment of modeling and simulation in secondary
		  computing science education},
  year		= {2018},
  isbn		= {9781450365888},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3265757.3265764},
  doi		= {10.1145/3265757.3265764},
  abstract	= {The introduction of the new computing science curriculum
		  in the Netherlands in 2019 raises the need for new
		  evidence-based teaching materials that include practical
		  assignments and guidelines for their assessment. As a part
		  of our research project on teaching Computational Science
		  (modeling and simulation), we participate in these efforts
		  and developed a curriculum intervention including a
		  practical assignment and an accompanying assessment
		  instrument consisting of grading rubrics based on the SOLO
		  taxonomy. In this research paper we focus on the assessment
		  instrument. We describe its development and report on a
		  pilot study carried out in the secondary computing science
		  course implementing the curriculum intervention. The
		  instrument proved to be reliable and effective in tracing
		  high and low levels of the students' achievements in
		  modeling and simulation projects and exposed the expected
		  differences in performance levels of various groups of
		  students, which renders it useful for both formative and
		  summative assessment. Furthermore, our application of the
		  instrument has provided new insights into the needs of
		  specific groups of students to receive instruction prior to
		  and during the work on the assignments.},
  booktitle	= {Proceedings of the 13th Workshop in Primary and Secondary
		  Computing Education},
  articleno	= {7},
  numpages	= {10},
  keywords	= {secondary computing education, modeling and simulation,
		  assessment, SOLO taxonomy},
  location	= {Potsdam, Germany},
  series	= {WiPSCE '18}
}

@InProceedings{	  10.1145/3652620.3687790,
  author	= {Crespo, Jos\'{e} Francisco and Juanola, Mart\'{\i} and
		  Oriol, Xavier and Recalde, Mart\'{\i} and Teniente, Ernest},
  title		= {IMP-Logics: a metamodel for analysis and transformations
		  of Datalog programs},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687790},
  doi		= {10.1145/3652620.3687790},
  abstract	= {Datalog is a logic-based query-language used for knowledge
		  representation and reasoning. Through the years, the
		  literature has defined highly valuable algorithms,
		  desirable properties, and useful transformations for this
		  language and its extensions (e.g. Datalog±).However, few
		  to none tools facilitate the implementation of such
		  results, making the existence of mature Datalog-based tools
		  scarce.This demonstration presents IMP-Logics, a Java
		  library that offers a metamodel for Datalog and its
		  extensions Datalog± that will allow researchers to easily
		  implement the algorithms and demonstrations of the
		  properties and transformations the community is working
		  on.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {51–55},
  numpages	= {5},
  keywords	= {datalog, datalog±, dependencies, metamodel},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3333165.3333179,
  author	= {Demaidi, Mona Nabil and Gaber, Mohamed Medhat},
  title		= {TONE: A Method for Terminological Ontology Evaluation},
  year		= {2019},
  isbn		= {9781450360890},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3333165.3333179},
  doi		= {10.1145/3333165.3333179},
  abstract	= {Selecting the most appropriate candidate domain ontology
		  is necessary to ensure that the ontology covers the domain
		  of interest at a reasonable level of detail. Existing
		  approaches have the following drawbacks:(1) Focused on the
		  ontology coverage of concepts in the domain of interest,
		  and ignored the semantic richness associated with each
		  concept.(2) The ontology coverage metrics tend to select
		  either large ontologies with broad scope or ontologies with
		  a small number of concepts which may not capture a
		  particular domain of interest.(3) The approaches are not
		  robust in the coverage and semantic richness metric results
		  when different term extraction and recognition algorithms
		  are used.The limitations mentioned above will result in
		  selecting ontologies which are not related to the domain of
		  interest. Therefore, this paper presents a novel
		  Terminological ONtology Evaluator (TONE). TONE uses a
		  textual corpus to evaluate the ontology coverage and
		  semantic richness. TONE was compared with existing ontology
		  evaluation approaches and it proved that it was able to
		  select the domain ontology which was intentionally
		  developed to cover a specific domain of interest. In
		  addition, TONE proved to be more robust in the coverage and
		  semantic richness metric results compared to existing
		  approaches.},
  booktitle	= {Proceedings of the ArabWIC 6th Annual International
		  Conference Research Track},
  articleno	= {14},
  numpages	= {10},
  keywords	= {semantic richness, percentage agreement, ontology
		  evaluation, Ontologies},
  location	= {Rabat, Morocco},
  series	= {ArabWIC 2019}
}

@Article{	  10.1145/3649451,
  author	= {Du, Kelvin and Xing, Frank and Mao, Rui and Cambria,
		  Erik},
  title		= {Financial Sentiment Analysis: Techniques and
		  Applications},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3649451},
  doi		= {10.1145/3649451},
  abstract	= {Financial Sentiment Analysis (FSA) is an important domain
		  application of sentiment analysis that has gained
		  increasing attention in the past decade. FSA research falls
		  into two main streams. The first stream focuses on defining
		  tasks and developing techniques for FSA, and its main
		  objective is to improve the performances of various FSA
		  tasks by advancing methods and using/curating
		  human-annotated datasets. The second stream of research
		  focuses on using financial sentiment, implicitly or
		  explicitly, for downstream applications on financial
		  markets, which has received more research efforts. The main
		  objective is to discover appropriate market applications
		  for existing techniques. More specifically, the application
		  of FSA mainly includes hypothesis testing and predictive
		  modeling in financial markets. This survey conducts a
		  comprehensive review of FSA research in both the technique
		  and application areas and proposes several frameworks to
		  help understand the two areas’ interactive relationship.
		  This article defines a clearer scope for FSA studies and
		  conceptualizes the FSA-investor sentiment-market sentiment
		  relationship. Major findings, challenges, and future
		  research directions for both FSA techniques and
		  applications have also been summarized and discussed.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {220},
  numpages	= {42},
  keywords	= {Financial sentiment analysis, financial forecasting,
		  natural language processing, information system, machine
		  learning, deep learning}
}

@InProceedings{	  10.1145/3613904.3642720,
  author	= {Benabdallah, Gabrielle and Peek, Nadya},
  title		= {Technical Mentality: Principles for HCI Research and
		  Practice},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642720},
  doi		= {10.1145/3613904.3642720},
  abstract	= {This paper presents a reflection on the role of
		  ontological inquiry in HCI research and practice.
		  Specifically, we introduce philosopher Gilbert Simondon’s
		  proposal of technical mentality, an onto-epistemology based
		  on direct knowledge of technical objects and systems. This
		  paper makes the following contributions: an analysis of
		  Simondon’s ontological critique and its connection to
		  technical mentality; a reflection on the ethical and
		  practical implications of Simondon’s proposal for systems
		  research; an example of technical mentality in practice;
		  and a discussion of how technical mentality might be
		  extended into a design program for HCI through four
		  principles: extension, integration, legibility, and
		  expression.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {283},
  numpages	= {14},
  keywords	= {Design, Gilbert Simondon, Ontology, Philosophy, Technical
		  Mentality},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3357384.3357827,
  author	= {Khabiri, Elham and Gifford, Wesley M. and Vinzamuri,
		  Bhanukiran and Patel, Dhaval and Mazzoleni, Pietro},
  title		= {Industry Specific Word Embedding and its Application in
		  Log Classification},
  year		= {2019},
  isbn		= {9781450369763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357384.3357827},
  doi		= {10.1145/3357384.3357827},
  abstract	= {Word, sentence and document embeddings have become the
		  cornerstone of most natural language processing-based
		  solutions. The training of an effective embedding depends
		  on a large corpus of relevant documents. However, such
		  corpus is not always available, especially for specialized
		  heavy industries such as oil, mining, or steel. To address
		  the problem, this paper proposes a semi-supervised learning
		  framework to create document corpus and embedding starting
		  from an industry taxonomy, along with a very limited set of
		  relevant positive and negative documents. Our solution
		  organizes candidate documents into a graph and adopts
		  different explore and exploit strategies to iteratively
		  create the corpus and its embedding. At each iteration, two
		  metrics, called Coverage and Context Similarity, are used
		  as proxy to measure the quality of the results. Our
		  experiments demonstrate how an embedding created by our
		  solution is more effective than the one created by
		  processing thousands of industry-specific document pages.
		  We also explore using our embedding in downstream tasks,
		  such as building an industry specific classification model
		  given labeled training data, as well as classifying
		  unlabeled documents according to industry taxonomy terms.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2713–2721},
  numpages	= {9},
  keywords	= {word embeddings, text classification, natural language
		  processing},
  location	= {Beijing, China},
  series	= {CIKM '19}
}

@InProceedings{	  10.5555/3320516.3321085,
  author	= {Sarli, Juan L.},
  title		= {An interoperability model for collaborative development of
		  distributed supply chain simulations},
  year		= {2018},
  isbn		= {978153866570},
  publisher	= {IEEE Press},
  abstract	= {Development of a collaborative distributed supply chain
		  simulation implies interoperation of heterogeneous systems.
		  Interoperability among several independent systems requires
		  mutual understanding and meaning of shared data represented
		  in a common structure. These two requirements are always a
		  real challenge. In a High Level Architecture (HLA) based
		  supply chain simulation, the federation object model (FOM)
		  performs as a contract where mutual understanding and
		  shared information are described. However, this contract is
		  usually established manually and then the consistency and
		  completeness cannot be guaranteed. Developing FOM and
		  modifying existing systems to comply with the FOM implies a
		  significant amount of time and effort which reduce the
		  benefits of system reuse. This paper presents a
		  heavy-weighted ontology-based method to construct
		  interoperation models of HLA based supply chain simulation
		  in a human-friendly, efficient, consistent and complete
		  way. Besides, this method provides support to collaboration
		  among several organizations of a supply chain.},
  booktitle	= {Proceedings of the 2018 Winter Simulation Conference},
  pages		= {4222–4223},
  numpages	= {2},
  location	= {Gothenburg, Sweden},
  series	= {WSC '18}
}

@InProceedings{	  10.1145/3338468.3356830,
  author	= {Islam, Md Mazharul and Duan, Qi and Al-Shaer, Ehab},
  title		= {Specification-driven Moving Target Defense Synthesis},
  year		= {2019},
  isbn		= {9781450368285},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3338468.3356830},
  doi		= {10.1145/3338468.3356830},
  abstract	= {Cyber agility enables cyber systems to defend proactively
		  against sophisticated attacks by dynamically changing the
		  system configuration parameters (called mutable parameters)
		  in order to deceive adversaries from reaching their goals,
		  disrupt the attack plans by forcing them to change their
		  adversarial behaviors, and/or deterring them through
		  prohibitively increasing the cost for attacks. However,
		  developing cyber agility such as moving target defense
		  techniques that are provable safe is a highly complex task
		  that requires significant time and expertise. Our goal is
		  to address this challenge by providing a framework for
		  automating the creation of configuration-based moving
		  target techniques rapidly and safely.In this paper, we
		  present a cyber agility synthesis framework, called
		  MTDSynth, that contains a formal ontology, MTD policy
		  language, and MTD controller synthesis engine for
		  implementing configuration-based moving target defense
		  techniques. The policy language contains the agility
		  specifications required to model the MTD technique, such as
		  sensors, mutation trigger, mutation parameters, mutation
		  actions, and mutation constraints. Based on the mutation
		  constraints, the MTD controller synthesis engine provides
		  an MTD policy refinement implementation for SDN
		  configuration with provable properties using constraint
		  satisfaction solvers. We show several examples of MTD
		  controller synthesis, including temporal and spatial IP
		  mutation, path mutation, detector mutation.We developed our
		  ActivSDN over OpenDaylight SDN controller as an open
		  programming environment to enable rapid and safe
		  development of MTD sense-making and decision-making
		  actions. Our implementation and evaluation experiments show
		  not only the feasibility of MTD policy refinement but also
		  the insignificant computational overhead of this refinement
		  process.},
  booktitle	= {Proceedings of the 6th ACM Workshop on Moving Target
		  Defense},
  pages		= {13–24},
  numpages	= {12},
  keywords	= {sdn, mtd, formal language, automation},
  location	= {London, United Kingdom},
  series	= {MTD'19}
}

@Article{	  10.1145/3703918,
  author	= {Bartalesi, Valentina and Pratelli, Nicol\`{o}},
  title		= {Representing Geospatial Knowledge in Narratives},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {1},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3703918},
  doi		= {10.1145/3703918},
  abstract	= {This article explores the representation of geospatial
		  knowledge within narratives through a Semantic Web
		  approach. We introduce the NOnt+Space (NOnt+S) model, an
		  extension of the CIDOC CRM-based Narrative Ontology, which
		  allows the representation of narratives and their
		  geospatial aspects. By leveraging standards such as CRMgeo
		  and GeoSPARQL, NOnt+S ensures systematic and interoperable
		  geospatial representation in narratives, enabling
		  geospatial queries on knowledge graphs. We present an
		  assessment of NOnt+S utilizing data from the H2020 MOVING
		  European project (2021–2024), which collected knowledge
		  about European mountain value chains intended as Cultural
		  Heritage. We have represented this knowledge as geospatial
		  narratives using NOnt+S. GeoSPARQL queries and semantic
		  reasoning applied to the created KG reveal the ontology
		  ability to infer new geospatial knowledge. Our work
		  contributes to the ongoing efforts in the Semantic Web
		  community to integrate and represent geospatial information
		  within narratives, promoting collaboration and
		  interoperability across various scientific domains.},
  journal	= {J. Comput. Cult. Herit.},
  month		= feb,
  articleno	= {17},
  numpages	= {15},
  keywords	= {Semantic Web, Knowledge Graph, CRMgeo, GeoSPARQL,
		  Geospatial Narratives, Digital Humanities}
}

@InProceedings{	  10.1145/3371425.3371442,
  author	= {Cheng, Xianyi and Ji, Guohua and Zhang, Xiaohua and Chen,
		  Fengmei},
  title		= {The semantic tagging model of chinese question sentence
		  chunk based on description logics},
  year		= {2019},
  isbn		= {9781450376334},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3371425.3371442},
  doi		= {10.1145/3371425.3371442},
  abstract	= {QA (Question Answering) is one of the hot spots in
		  artificial intelligence field. At present, English QA
		  research has made great progress in large-scale text. There
		  are still many difficulties in Chinese QA and it is
		  impossible to understand the true meaning of questions. To
		  solve the problem of Chinese question sentence can't
		  provide deep semantic information for syntactic analysis,
		  the paper proposes a semantic tagging model of Chinese
		  question sentence chunk based on description logics. With
		  the knowledge of HNC's concept symbol to get semantic
		  information about the word, and by starting with the
		  connotation of the concept categories, it preliminarily
		  analyzes the logical structure of specific question
		  sentences. Finally, with description logic reasoning
		  mechanism, we get semantic view of question sentence and
		  proving it's verify in practical.},
  booktitle	= {Proceedings of the International Conference on Artificial
		  Intelligence, Information Processing and Cloud Computing},
  articleno	= {44},
  numpages	= {6},
  keywords	= {semantic tagging, description logics, chunk, QA, HNC},
  location	= {Sanya, China},
  series	= {AIIPCC '19}
}

@Article{	  10.1145/3732794,
  author	= {Israelsen, Brett and Ahmed, Nisar R. and Aitken, Matthew
		  and Frew, Eric W. and Lawrence, Dale A. and Argrow, Brian
		  M.},
  title		= {“A Good Bot Always Knows Its Limitations”: Assessing
		  Autonomous System Decision-Making Competencies through
		  Factorized Machine Self-Confidence},
  year		= {2025},
  issue_date	= {December 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {4},
  url		= {https://doi.org/10.1145/3732794},
  doi		= {10.1145/3732794},
  abstract	= {How can intelligent machines assess their competency to
		  complete a task? This question has come into focus for
		  autonomous systems that algorithmically make decisions
		  under uncertainty. We argue that machine
		  self-confidence—a form of meta-reasoning based on
		  self-assessments of system knowledge about the state of the
		  world, itself, and ability to reason about and execute
		  tasks—leads to many computable and useful competency
		  indicators for such agents. This article presents our body
		  of work, so far, on this concept in the form of the
		  Factorized Machine Self-Confidence (FaMSeC) framework,
		  which holistically considers several major factors driving
		  competency in algorithmic decision-making: outcome
		  assessment, solver quality, model quality, alignment
		  quality, and past experience. In FaMSeC, self-confidence
		  indicators are derived via “problem-solving statistics”
		  embedded in Markov Decision Process solvers and related
		  approaches. These statistics come from evaluating
		  probabilistic exceedance margins in relation to certain
		  outcomes and associated competency standards specified by
		  an evaluator. Once designed, and evaluated, the statistics
		  can be easily incorporated into autonomous agents and serve
		  as indicators of competency. We include detailed
		  descriptions and examples for Markov Decision Process
		  agents and show how outcome assessment and solver quality
		  factors can be found for a range of tasking contexts
		  through novel use of meta-utility functions, behavior
		  simulations, and surrogate prediction models. Numerical
		  evaluations are performed to demonstrate that FaMSeC
		  indicators perform as desired (references to human subject
		  studies beyond the scope of this article are provided).},
  journal	= {J. Hum.-Robot Interact.},
  month		= jul,
  articleno	= {66},
  numpages	= {63},
  keywords	= {autonomous robots, proficiency assessment, Markov decision
		  processes, probabilistic models, human-autonomy
		  interaction}
}

@InProceedings{	  10.1145/3603163.3609067,
  author	= {Gounakis, Nikos and Mountantonakis, Michalis and
		  Tzitzikas, Yannis},
  title		= {Evaluating a Radius-based Pipeline for Question Answering
		  over Cultural (CIDOC-CRM based) Knowledge Graphs},
  year		= {2023},
  isbn		= {9798400702327},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603163.3609067},
  doi		= {10.1145/3603163.3609067},
  abstract	= {CIDOC-CRM is an event-based international standard for
		  cultural documentation that has been widely used for
		  offering semantic interoperability in the Cultural Heritage
		  (CH) domain. Although there are several Knowledge Graphs
		  (KGs) expressed by using CIDOC-CRM, the task of Question
		  Answering (QA) has not been studied over such graphs. For
		  this reason, in this paper we propose and evaluate a
		  Radius-based QA pipeline over CIDOC-CRM KGs for
		  single-entity factoid questions. In particular, we propose
		  a generic QA pipeline that comprises several models and
		  methods, including a keyword search model for recognizing
		  the entity of the question (and linking it to the KG),
		  methods that are based on path expansion for constructing
		  subgraphs of different radius (i.e., path lengths) starting
		  from the recognized entity, i.e., for being used as a
		  context, and pre-trained neural models (based on BERT) for
		  answering the question using the mentioned context.
		  Moreover, since there are no available benchmarks over
		  CIDOC-CRM KGs, we construct (by using a real KG) an
		  evaluation benchmark having 10,000 questions, i.e., 5,000
		  single-entity factoid, 2,500 comparative and 2,500
		  confirmation questions. For evaluating the QA pipeline, we
		  use the 5,000 single-entity factoid questions. Concerning
		  the results, the QA pipeline achieves satisfactory results
		  both in the entity recognition step (78\% accuracy) and in
		  the QA process (51\% F1 score).},
  booktitle	= {Proceedings of the 34th ACM Conference on Hypertext and
		  Social Media},
  articleno	= {24},
  numpages	= {10},
  keywords	= {Resource Description Framework, Path Expansion, Natural
		  Language Processing, Linked Data, Knowledge Graph,
		  Event-Based Ontology, Entity Recognition, Cultural
		  Heritage, Answer Extraction},
  location	= {Rome, Italy},
  series	= {HT '23}
}

@InProceedings{	  10.1145/3377170.3377231,
  author	= {Ding, Pan and Zhuoqian, Liang and Yuan, Deng},
  title		= {Textual Information Extraction Model of Financial
		  Reports},
  year		= {2020},
  isbn		= {9781450376631},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3377170.3377231},
  doi		= {10.1145/3377170.3377231},
  abstract	= {This paper proposes a model to extract textual information
		  from financial reports automatically. It takes event
		  extraction as the core, maps narrative information of
		  financial reports into the concepts of financial accounting
		  field, and forms the integration of heterogeneous data of
		  distributed financial information. This study shows that
		  the model can identify text events in large-scale financial
		  reporting corpus, automatically extract events and theirs
		  attribute information, and convert them into structured
		  data. Moreover, this paper presents and evaluates the
		  effect of the model in extracting information from annual
		  reports of listed companies. The experimental results turn
		  out that the model provides semantic mapping between text
		  events and domain knowledge concepts, which is reasonable
		  and reliable to be applied in the field of financial
		  statement analysis.},
  booktitle	= {Proceedings of the 2019 7th International Conference on
		  Information Technology: IoT and Smart City},
  pages		= {404–408},
  numpages	= {5},
  keywords	= {Textual information extraction, Financial report, Event
		  extraction, Corpus},
  location	= {Shanghai, China},
  series	= {ICIT '19}
}

@Article{	  10.1145/3624013,
  author	= {Kumari, Namrata and Singh, Pardeep},
  title		= {Hindi Text Summarization Using Sequence to Sequence Neural
		  Network},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {10},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3624013},
  doi		= {10.1145/3624013},
  abstract	= {Text summarizing reduces a large block of text data to a
		  precise, short, and intelligible text that conveys the
		  whole meaning of the actual text in a few words while
		  maintaining the original context. Due to a lack of relevant
		  summaries, it is hard to understand the main idea of the
		  document. Text summarization using the abstractive
		  technique is well-studied in English, although it is still
		  in its infancy in Indian regional languages. In this study,
		  we investigate the effectiveness of using a
		  sequence-to-sequence (Seq2Seq) neural network based on
		  attention and its optimization for text summarization for
		  the Hindi language (HiATS), explicitly comparing the Adam
		  and RMSprop optimizers. Our method allows the model to take
		  the Hindi language dataset and, as output, provides a
		  concise summary that accurately reflects the gist of the
		  original text. The performance of the models will be
		  evaluated using Rouge-1 and Rouge-2 metrics.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  articleno	= {239},
  numpages	= {18},
  keywords	= {neural network, word embedding, optimizers, Abstractive
		  text summarization}
}

@InProceedings{	  10.1145/3744169.3744197,
  author	= {Hu, Botao Amber},
  title		= {Autonomous Realities: A Journey into Protocolizing Digital
		  Object Permanence in a Future of Many Mixed Realities},
  year		= {2025},
  isbn		= {9798400720031},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3744169.3744197},
  doi		= {10.1145/3744169.3744197},
  abstract	= {Major technology companies envision a future where mixed
		  reality (MR) devices become as ubiquitous as smartphones
		  are today. Yet most collaborative MR research assumes users
		  share a single augmented layer—an assumption that may not
		  hold true. Rather, MR technology is inherently
		  permissionless: users control what they see, making each
		  person’s augmented layers private and unique. We are
		  moving toward a future of multiple overlapping and
		  co-existing mixed realities. This paper employs protocol
		  fiction as a speculative design method to explore this
		  near-future scenario. We follow the journey of a fictional
		  digital pet rock as it travels through successive protocol
		  eras of mixed reality, adapting to the changing
		  infrastructures and protocols it encounters. Through a
		  comic-style narrative, the story unfolds across four
		  protocol-defined chapters: Centralized Realities,
		  Distributed Realities, Persistent Realities, and Autonomous
		  Realities. Each chapter examines moments when digital pet
		  rock owners—wearing MR headsets—engage in social
		  encounters, revealing how protocols shape the ontological
		  nature of digital object permanence and highlight the
		  socio-technical challenges of constructing consensus
		  reality.},
  booktitle	= {Proceedings of the Sixth Decennial Aarhus Conference:
		  Computing X Crisis},
  pages		= {290–302},
  numpages	= {13},
  keywords	= {Social Mixed Reality, Protocol Design, Merging Mixed
		  Reality, Protocol Fiction, Design Fiction, Ontology of
		  Digital Object, Object Permanence, Metaverse
		  Interoperability},
  location	= { },
  series	= {AAR '25}
}

@InProceedings{	  10.1145/3701716.3718483,
  author	= {Prabowo, Arian and Lin, Xiachong and Razzak, Imran and
		  Xue, Hao and Amos, Matthew and White, Stephen D. and Salim,
		  Flora D.},
  title		= {Brick-by-Brick: Cyber-Physical Building Data
		  Classification Challenge},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3718483},
  doi		= {10.1145/3701716.3718483},
  abstract	= {optimization essential in combating climate change.
		  Cyber-Physical Buildings, enabled by the integration of
		  Internet-of-Things (IoT) devices and advanced data
		  analytics tools like AI, offer a smart and effective
		  approach to energy management. A key challenge, however,
		  lies in automating the semantic labeling of IoT devices to
		  ensure machine-interpretable data. The ''Brick-by-Brick:
		  Cyber-Physical Building Data Classification Challenge''
		  aims to tackle this challenge by classifying time-series
		  data from IoT devices within buildings. Participants will
		  engage with a dataset consisting of over 10,000 time-series
		  streams collected over three years across three buildings,
		  representing 91 unique semantic classes. Both the dataset
		  and baselines are established in a published paper. With a
		  total prize pool of 20,000 AUD, the competition is ready to
		  launch in December 2024 and run through February 2025,
		  hosted by AIcrowd. This challenge invites researchers,
		  practitioners, and technologists to drive AI-enabled
		  solutions for advancing the next generation of
		  environmentally sustainable cyber-physical buildings.
		  Additional details on the dataset, benchmark, and code can
		  be found in the official repository (
		  https://github.com/cruiseresearchgroup/DIEF_BTS). The
		  challenge was published on AIcrowd
		  www.aicrowd.com/challenges/brick-by-brick-2024.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {3021–3025},
  numpages	= {5},
  keywords	= {building, classification, machine learning, ontology,
		  timeseries},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3230905.3230935,
  author	= {Khallouki, Hajar and Abatal, Ahmed and Bahaj, Mohamed},
  title		= {An Ontology-based Context awareness for Smart Tourism
		  Recommendation System},
  year		= {2018},
  isbn		= {9781450353045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3230905.3230935},
  doi		= {10.1145/3230905.3230935},
  abstract	= {Smart tourism concept appeared with the development of
		  Smart Cities. Bringing Smartness into Tourism needs a
		  dynamic and interconnected system on which information
		  relating to tourism activities could be exchanged in real
		  time. In this paper, we introduce a new approach for
		  designing mobile tourism recommendation system using
		  context awareness. The proposed approach combines Internet
		  of Things (IoT) technologies with semantic web services to
		  predict the tourist real-time context and provide the
		  suitable services.},
  booktitle	= {Proceedings of the International Conference on Learning
		  and Optimization Algorithms: Theory and Applications},
  articleno	= {43},
  numpages	= {5},
  keywords	= {tourism recommendation system, semantic web, real-time
		  context, context awareness, Smart tourism, IoT},
  location	= {Rabat, Morocco},
  series	= {LOPAL '18}
}

@InProceedings{	  10.1145/3637528.3671941,
  author	= {Yeom, Kyuhwan and Yang, Hyeongjun and Park, Gayeon and
		  Jeon, Myeongheon and Ko, Yunjeong and Oh, Byungkook and
		  Lee, Kyong-Ho},
  title		= {Embedding Two-View Knowledge Graphs with Class Inheritance
		  and Structural Similarity},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671941},
  doi		= {10.1145/3637528.3671941},
  abstract	= {Numerous large-scale knowledge graphs (KGs) fundamentally
		  represent two-view KGs: an ontology-view KG with abstract
		  classes in ontology and an instance-view KG with specific
		  collections of entities instantiated from ontology classes.
		  Two-view KG embedding aims to jointly learn continuous
		  vector representations of entities and relations in the
		  aforementioned two-view KGs. In essence, an ontology schema
		  exhibits a tree-like structure guided by class hierarchies,
		  which leads classes to form inheritance hierarchies.
		  However, existing two-view KG embedding models neglect
		  those hierarchies, which provides the necessity to reflect
		  class inheritance. On the other hand, KG is constructed
		  based on a pre-defined ontology schema that includes
		  heterogeneous relations between classes. Furthermore, these
		  relations are defined within the scope of those among
		  classes since instances inherit all the properties of their
		  corresponding classes, which reveals structural similarity
		  between two multi-relational networks. Despite the
		  consideration to bridge the gap among two-view KG
		  representations, existing methods ignore the existence of
		  structural similarity between two-view KGs. To address
		  these issues, we propose a novel two-view KG embedding
		  model, CISS, considering Class Inheritance and Structural
		  Similarity between two-view KGs. To deal with class
		  inheritance, we utilize class sets, each of which is
		  composed of sibling classes, to learn fine-grained class
		  representations. In addition, we configure virtual
		  instance-view KG from clustered instances and compare
		  subgraph representations of two-view KGs to enhance
		  structural similarity between them. Experimental results
		  show our superior performance compared to existing
		  models.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3931–3941},
  numpages	= {11},
  keywords	= {class inheritance, knowledge graph, ontology, structural
		  similarity},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3241403.3241426,
  author	= {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
  title		= {Model-based support for decision-making in architecture
		  evolution of complex software systems},
  year		= {2018},
  isbn		= {9781450364836},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3241403.3241426},
  doi		= {10.1145/3241403.3241426},
  abstract	= {Design decision support for software architects in complex
		  industrial software systems, such as software ecosystems
		  and systems-of-systems, which feature extensive reuse of
		  third-party solutions and a variety of deployment options,
		  is still an open challenge. We describe three industrial
		  use cases involving considerable re-architecting, where
		  on-premises solutions were migrated to a cloud-based IoT
		  platforms. Based on these use cases, we analyse the
		  challenges and derive requirements for an architecture
		  knowledge model supporting this process. The presented
		  methodology builds upon existing approaches and proposes a
		  model for the description of extant software applications
		  and the management of domain knowledge. We demonstrate its
		  use to support the evolution and/or composition of software
		  applications in a migration scenario in a systematic and
		  traceable manner.},
  booktitle	= {Proceedings of the 12th European Conference on Software
		  Architecture: Companion Proceedings},
  articleno	= {21},
  numpages	= {7},
  keywords	= {systems-of-systems composition, software variability
		  management, software migration, software architecture
		  evolution, model-based decision support},
  location	= {Madrid, Spain},
  series	= {ECSA '18}
}

@InProceedings{	  10.1145/3443279.3443306,
  author	= {Fritz, Simon and Jaenicke, Matthias and Ovtcharova, Jivka
		  and Wicaksono, Hendro},
  title		= {Context-sensitive Assistance in Requirements-based
		  Knowledge Management},
  year		= {2021},
  isbn		= {9781450377607},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3443279.3443306},
  doi		= {10.1145/3443279.3443306},
  abstract	= {In this paper, a concept of a digital assistance system is
		  presented which, based on computer linguistic methods,
		  supports the user in the tasks of requirement-based
		  knowledge management. The concept is divided into six
		  modules that offer context-sensitive support in the
		  identification, documentation, linking, modification and
		  reuse of requirements and the associated knowledge. Since
		  this concept was developed as part of the BMBF-funded SME
		  Innovative Project DAM4KMU, which is primarily aimed at
		  German SMEs, the concept developed was specially designed
		  for processing German-language texts.The digital assistance
		  system pursues the goal, on the one hand, of increasing the
		  quality of the documentation by supporting the user in the
		  creation of complete formulations. On the other hand, with
		  the help of the most modern language models, possible
		  relationships between the information should be identified
		  and linked to each other in a partially automated manner.
		  In addition, the integration of web crawling technologies
		  should make the knowledge available on the Internet
		  available in a context-sensitive manner, in order to lift
		  possible innovations on the one hand and not to forget
		  possible non-considered boundary conditions on the
		  other.The automatic linking of all information is intended
		  to ensure a continuous exchange of knowledge, which should
		  reduce misunderstandings and non-communicated changes to
		  requirements or goals to a minimum.},
  booktitle	= {Proceedings of the 4th International Conference on Natural
		  Language Processing and Information Retrieval},
  pages		= {47–54},
  numpages	= {8},
  keywords	= {Web Crawling, Requirements Engineering, Natural Language
		  Processing, Knowledge management, Digital Assistance},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '20}
}

@Article{	  10.1145/3295662,
  author	= {Badaro, Gilbert and Baly, Ramy and Hajj, Hazem and
		  El-Hajj, Wassim and Shaban, Khaled Bashir and Habash, Nizar
		  and Al-Sallab, Ahmad and Hamdi, Ali},
  title		= {A Survey of Opinion Mining in Arabic: A Comprehensive
		  System Perspective Covering Challenges and Advances in
		  Tools, Resources, Models, Applications, and
		  Visualizations},
  year		= {2019},
  issue_date	= {September 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3295662},
  doi		= {10.1145/3295662},
  abstract	= {Opinion-mining or sentiment analysis continues to gain
		  interest in industry and academics. While there has been
		  significant progress in developing models for sentiment
		  analysis, the field remains an active area of research for
		  many languages across the world, and in particular for the
		  Arabic language, which is the fifth most-spoken language
		  and has become the fourth most-used language on the
		  Internet. With the flurry of research activity in Arabic
		  opinion mining, several researchers have provided surveys
		  to capture advances in the field. While these surveys
		  capture a wealth of important progress in the field, the
		  fast pace of advances in machine learning and natural
		  language processing (NLP) necessitates a continuous need
		  for a more up-to-date literature survey. The aim of this
		  article is to provide a comprehensive literature survey for
		  state-of-the-art advances in Arabic opinion mining. The
		  survey goes beyond surveying previous works that were
		  primarily focused on classification models. Instead, this
		  article provides a comprehensive system perspective by
		  covering advances in different aspects of an opinion-mining
		  system, including advances in NLP software tools, lexical
		  sentiment and corpora resources, classification models, and
		  applications of opinion mining. It also presents future
		  directions for opinion mining in Arabic. The survey also
		  covers latest advances in the field, including deep
		  learning advances in Arabic Opinion Mining. The article
		  provides state-of-the-art information to help new or
		  established researchers in the field as well as industry
		  developers who aim to deploy an operational complete
		  opinion-mining system. Key insights are captured at the end
		  of each section for particular aspects of the
		  opinion-mining system giving the reader a choice of
		  focusing on particular aspects of interest.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {27},
  numpages	= {52},
  keywords	= {sentiment lexicons, sentiment analysis applications,
		  opinion mining, deep learning, arabic natural language
		  processing, Sentiment analysis}
}

@InProceedings{	  10.1145/3176258.3176331,
  author	= {Rizvi, Syed Zain R. and Fong, Philip W. L.},
  title		= {Efficient Authorization of Graph Database Queries in an
		  Attribute-Supporting ReBAC Model},
  year		= {2018},
  isbn		= {9781450356329},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3176258.3176331},
  doi		= {10.1145/3176258.3176331},
  abstract	= {Neo4j is a popular graph database that offers two
		  versions; a paid enterprise edition and a free community
		  edition. The enterprise edition offers customizable
		  Role-Based Access Control (RBAC) features through custom
		  developed procedures, while the community edition does not
		  offer any access control support. Being a graph database,
		  Neo4j is a natural application for Relationship-Based
		  Access Control (ReBAC), an access control paradigm where
		  authorization decisions are based on relationships between
		  subjects and resources in the system. In this paper we
		  present AReBAC, an attribute-supporting ReBAC model for
		  Neo4j (applicable to both editions) that provides finer
		  grained access control. AReBAC employs Nano-Cypher, a
		  declarative policy language based on Neo4j»s Cypher query
		  language, the result of which allows us to weave database
		  queries with access control policies and evaluate both
		  simultaneously. Evaluating the combined query and policy
		  produces a result that i) matches the search criteria, and
		  ii) the requesting subject has access to. Our experiments
		  show that our evaluation algorithm performs faster than
		  Neo4j»s query evaluation engine when evaluating queries
		  that are expressible using Nano-Cypher.},
  booktitle	= {Proceedings of the Eighth ACM Conference on Data and
		  Application Security and Privacy},
  pages		= {204–211},
  numpages	= {8},
  keywords	= {relatioship-based access control, neo4j, graph database,
		  attributes},
  location	= {Tempe, AZ, USA},
  series	= {CODASPY '18}
}

@InProceedings{	  10.5555/3400397.3400521,
  author	= {Benaben, Frederick and Lauras, Matthieu and Fertier,
		  Audrey and Salatg\'{e}, Nicolas},
  title		= {Integrating model-driven engineering as the next challenge
		  for artificial intelligence: application to risk and crisis
		  management},
  year		= {2020},
  isbn		= {9781728132839},
  publisher	= {IEEE Press},
  abstract	= {Artificial Intelligence (AI) is currently on top of the
		  hype regarding simultaneously research publications and
		  industrial development. However, the current status of AI
		  makes it quite far and different from the current
		  understanding of Human intelligence. One suggestion that is
		  made in this article is that Model-Driven approaches could
		  be considered as an interesting avenue to complement
		  classical visions of AI and to provide some missing
		  features. Specifically, the use of Model-Driven Engineering
		  tools (such as metamodel and model transformation) could
		  benefit to the domain of AI by introducing a way to extend
		  the apprehension of unknown situations. To support that
		  proposal, an illustrative example is provided regarding the
		  domain of risk and crisis management.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1549–1563},
  numpages	= {15},
  location	= {National Harbor, Maryland},
  series	= {WSC '19}
}

@Article{	  10.1145/3640018,
  author	= {Kapugama Geeganage, Dakshi Tharanga and Wynn, Moe Thandar
		  and ter Hofstede, Arthur H. M.},
  title		= {Text2EL+: Expert Guided Event Log Enrichment Using
		  Unstructured Text},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {1},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3640018},
  doi		= {10.1145/3640018},
  abstract	= {Through the application of process mining, business
		  processes can be improved on the basis of process execution
		  data captured in event logs. Naturally, the quality of this
		  data determines the quality of the improvement
		  recommendations. Improving data quality is non-trivial, and
		  there is great potential to exploit unstructured text,
		  e.g., from notes, reviews, and comments, for this purpose
		  and to enrich event logs. To this end, this article
		  introduces Text2EL+&nbsp;, a three-phase approach to enrich
		  event logs using unstructured text. In its first phase,
		  events and (case and event) attributes are derived from
		  unstructured text linked to organisational processes. In
		  its second phase, these events and attributes undergo a
		  semantic and contextual validation before their
		  incorporation in the event log. In its third and final
		  phase, recognising the importance of human domain
		  expertise, expert guidance is used to further improve data
		  quality by removing redundant and irrelevant events. Expert
		  input is used to train a Named Entity Recognition (NER)
		  model with customised tags to detect event log elements.
		  The approach applies natural language processing
		  techniques, sentence embeddings, training pipelines and
		  models, as well as contextual and expression validation.
		  Various unstructured clinical notes associated with a
		  healthcare case study were analysed, and completeness,
		  concordance, and correctness of the derived event log
		  elements were evaluated through experiments. The results
		  show that the proposed method is feasible and applicable.},
  journal	= {J. Data and Information Quality},
  month		= mar,
  articleno	= {8},
  numpages	= {28},
  keywords	= {Event data quality, process mining, event log,
		  unstructured text, natural language processing, semantic
		  validation}
}

@InProceedings{	  10.1145/3708035.3736045,
  author	= {Saboia, Priscila and Sweet, James and Sweet, Christopher},
  title		= {OpenBridge: Bridging Domain Scientists and APIs through
		  AI-Powered Interfaces},
  year		= {2025},
  isbn		= {9798400713989},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708035.3736045},
  doi		= {10.1145/3708035.3736045},
  abstract	= {We propose OpenBridge, an MCP-compliant server that
		  enables AI assistants to access scientific APIs through
		  natural language. It converts OpenAPI Specification (OAS)
		  endpoints into Model Context Protocol (MCP) tools and
		  enriches responses with JavaScript Object Notation for
		  Linked Data (JSON-LD) for improved semantic clarity. As we
		  demonstrate using data from Paper Analytical Devices
		  (PADs), OpenBridge allows researchers to retrieve and
		  explore structured data without writing code. Thus, it
		  offers a new approach to bridging domain experts and data
		  systems through AI-assisted, semantically aware interfaces.
		  The implementation is available at
		  https://github.com/PaperAnalyticalDeviceND/OpenBridge.},
  booktitle	= {Practice and Experience in Advanced Research Computing
		  2025: The Power of Collaboration},
  articleno	= {97},
  numpages	= {3},
  keywords	= {AI Agents, OpenAPI, Model Context Protocol, Research Data
		  Infrastructure},
  location	= { },
  series	= {PEARC '25}
}

@InProceedings{	  10.1145/3587259.3627549,
  author	= {D'Aquin, Mathieu and Bunoiu, Renata and Cirstea, Horatiu
		  and Lenczner, Michel and Lieber, Jean and Zamkotsian,
		  Fr\'{e}d\'{e}ric},
  title		= {Combining representation formalisms for reasoning upon
		  mathematical knowledge},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627549},
  doi		= {10.1145/3587259.3627549},
  abstract	= {Knowledge in mathematics (definitions, theorems, proofs,
		  etc.) is usually expressed in a way that combines natural
		  language and mathematical expressions (e.g. equations).
		  Using an ontology formalism such as OWL&nbsp;DL is
		  well-suited for formalizing the natural language part, but
		  complex mathematical expressions can be better handled by
		  symbolic computation systems. We examine this
		  representation issue and propose an original extension of
		  OWL&nbsp;DL by call formulas, i.e., formulas from which
		  assertions can be drawn thanks to calls to external
		  functions. Using this formalism makes it possible to
		  classify a mathematical problem defined by its relations to
		  instances and classes and by some mathematical expressions:
		  if a theorem for solving this problem is represented in the
		  knowledge base, it can be retrieved, and thus, the problem
		  can be solved by applying this theorem. We describe an
		  inference algorithm and discuss its properties as well as
		  its limitations. Indeed, the proposed extension, algorithm,
		  and implementation represent a first step towards a
		  combined formalism for representing mathematical knowledge,
		  with some open issues regarding the representation of more
		  complex problems: the resolution of multiscale,
		  multiphysics cases in physics are foreseen.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {180–187},
  numpages	= {8},
  keywords	= {knowledge modeling, knowledge representation, mathematical
		  knowledge},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3233027.3233031,
  author	= {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
  title		= {An inductive learning perspective on automated generation
		  of feature models from given product specifications},
  year		= {2018},
  isbn		= {9781450364645},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3233027.3233031},
  doi		= {10.1145/3233027.3233031},
  abstract	= {For explicit representation of commonality and variability
		  of a product line, a feature model is mostly used. An open
		  question is how a feature model can be inductively learned
		  in an automated way from a limited number of given product
		  specifications in terms of features.We propose to address
		  this problem through machine learning, more precisely
		  inductive generalization from examples. However, no
		  counter-examples are assumed to exist. Basically, a feature
		  model needs to be complete with respect to all the given
		  example specifications. First results indicate the
		  feasibility of this approach, even for generating
		  hierarchies, but many open challenges remain.},
  booktitle	= {Proceedings of the 22nd International Systems and Software
		  Product Line Conference - Volume 1},
  pages		= {25–30},
  numpages	= {6},
  keywords	= {machine learning, inductive generalization from examples,
		  generating feature models},
  location	= {Gothenburg, Sweden},
  series	= {SPLC '18}
}

@InProceedings{	  10.1145/3168365.3168378,
  author	= {Carbonnel, Jessie and Huchard, Marianne and Nebut,
		  Cl\'{e}mentine},
  title		= {Towards the Extraction of Variability Information to
		  Assist Variability Modelling of Complex Product Lines},
  year		= {2018},
  isbn		= {9781450353984},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3168365.3168378},
  doi		= {10.1145/3168365.3168378},
  abstract	= {Software product line engineering gathers a set of methods
		  that rely on systematic reuse and mass customisation to
		  reduce the development time and cost of a set of similar
		  software systems. Boolean feature models are the de facto
		  standard used to represent product line variability in
		  terms of features, a feature being a distinguishable
		  characteristic of one or several softwares. The extractive
		  adoption of a product line from a set of individually
		  developed softwares requires to extract variability
		  information from a collection of software descriptions to
		  model their variability. With the appearance of more and
		  more complex software systems, software product line
		  engineering faces new challenges including variability
		  extraction and modelling. Extensions of boolean feature
		  models, as multi-valued attributes or UML-like
		  cardinalities have since been proposed to support
		  variability modelling in complex product lines. In this
		  paper, we propose research directions to address the issue
		  of extracting more complex variability information, as a
		  part of extended feature models synthesis from software
		  descriptions. We consider the capabilities of Formal
		  Concept Analysis, a mathematical framework for knowledge
		  discovery, along with two of its extensions called Pattern
		  Structures and Relational Concept Analysis, to answer this
		  problematic. These frameworks bring theoretical foundations
		  to complex variability extraction algorithms.},
  booktitle	= {Proceedings of the 12th International Workshop on
		  Variability Modelling of Software-Intensive Systems},
  pages		= {113–120},
  numpages	= {8},
  keywords	= {Variability Extraction, Software Product Line, Reverse
		  Engineering},
  location	= {Madrid, Spain},
  series	= {VAMOS '18}
}

@InProceedings{	  10.1145/3361570.3361597,
  author	= {Gueddes, Abdelweheb and Mahjoub, Mohamed Ali},
  title		= {e-SAAD system: Ontologies based approach for home Care
		  Services platform},
  year		= {2019},
  isbn		= {9781450362924},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3361570.3361597},
  doi		= {10.1145/3361570.3361597},
  abstract	= {In the first generation of interventions and home support,
		  the needs were triggered generally by phone call and they
		  are described orally. In most cases, there is no registered
		  information about the patient's conditions or medical
		  history. The interveners are either from the private or
		  state health sector. Despite the fact that there are people
		  who can intervene quicker than others, yet they have not
		  been recruited, or work as professional liberal. With the
		  development of technology, systems based on data mining or
		  artificial intelligence have been developed to focus on the
		  intervention's time for example. Although intervention and
		  home-based care are the subject of many studies [1], the
		  resolution of the overall decision-making problem is not
		  sufficiently developed. On the one hand, the state of
		  health presupposes the definition of a patient. There is a
		  set of parameters characterizing the habits of daily life
		  of the person analyzed in parallel with the evolution of
		  physiological and environment. On the other hand, it is
		  necessary to take into consideration the medical Core, his
		  location, his profile not only his professional status but
		  also his abilities and skills that are not explicitly
		  described in his curriculum. Different studies and systems
		  exist in the literature [2]. Each of his studies tackles
		  only a part of the parameters. Indeed, these studies
		  consider either the monitoring of daily activities, the
		  monitoring of physiological data or other environmental
		  parts. Either they consider the specificities of the
		  medical Core's profile, or these systems use a
		  probabilistic data mining that involves many interactions
		  with the experts to interpret the data, either wise an
		  expert system based on the inference rules defined by the
		  medical experts. In addition, most systems do not use
		  controlled vocabulary that provides semantics needed. This
		  complicates information sharing and collaborative work. The
		  objective of the e-SAAD project is to propose a
		  methodological process to facilitate the analysis and
		  procedure of intervention systems and home support. The
		  process should identify the generic and specific aspects of
		  each part. The patient's data set, profile, history, its
		  environment and location should be taken into
		  consideration. As well as the service providers, their
		  profiles, their skills and essentially their availability
		  and locations. These models must be open to be adapted to
		  new data sources.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Information Systems and Technologies},
  articleno	= {21},
  numpages	= {6},
  keywords	= {ontology, home-based care, e-SAAD, Telemedicine, Semantic
		  Web},
  location	= {Cairo, Egypt},
  series	= {ICIST '19}
}

@InProceedings{	  10.1109/models-c.2019.00028,
  author	= {Burgue\~{n}o, Loli and Burdusel, Alexandru and G\'{e}rard,
		  S\'{e}bastien and Wimmer, Manuel},
  title		= {MDE intelligence 2019: 1st workshop on artificial
		  intelligence and model-driven engineering},
  year		= {2021},
  isbn		= {9781728151250},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MODELS-C.2019.00028},
  doi		= {10.1109/MODELS-C.2019.00028},
  abstract	= {Model-driven engineering (MDE) and Artificial Intelligence
		  (AI) are two separate fields in computer science, which can
		  clearly benefit from cross-fertilization and collaboration.
		  There are at least two ways in which such
		  integrations---which we call MDE Intelligence---can
		  manifest: (1) MDE can benefit from integrating AI concepts
		  and ideas to increasing the power and flexibility of
		  model-driven techniques by means of the application of AI
		  algorithms. (2) Conversely, AI can benefit from integrating
		  concepts and ideas from MDE---for example, using
		  domain-specific languages and model transformations allows
		  domain experts to directly express and manipulate their
		  problems while providing an auditable computation
		  pipeline.To discuss and further stimulate such
		  integrations, the 1st edition of the Workshop on Artificial
		  Intelligence and Model-driven Engineering (MDE
		  Intelligence) was held on September 16, 2019 in Munich,
		  Germany, as part of the satellite events of the IEEE/ACM
		  22th International Conference on Model-Driven Engineering
		  Languages and Systems (MODELS 2019).},
  booktitle	= {Proceedings of the 22nd International Conference on Model
		  Driven Engineering Languages and Systems Companion},
  pages		= {168–169},
  numpages	= {2},
  keywords	= {MDE, MDE intelligence, artificial intelligence},
  location	= {Munich, Germany},
  series	= {MODELS '19 Companion}
}

@InProceedings{	  10.1145/3352593.3352604,
  author	= {Boruah, Abhijit and Kakoty, Nayan M. and Ali, Tazid},
  title		= {Reasoning on Objects' Geometric Shapes for Prosthetic Hand
		  Grasping},
  year		= {2020},
  isbn		= {9781450366502},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3352593.3352604},
  doi		= {10.1145/3352593.3352604},
  abstract	= {The problem of knowing what to grasp and deciding how to
		  grasp is an open issue for development of intelligent
		  prosthetic hands. To emulate the potentialities of a human
		  hand, knowledge of the grasping domain has to be
		  accumulated and modelled in a machine interpretable format.
		  In this paper, we have tried to comprehend and model a
		  specific part of the knowledge (information) of a
		  prosthetic hand-grasping domain into a reusable
		  Web-Ontology-Language (OWL) format. This ontology build
		  after basic analysis of hand object coordination, can be
		  used for preserving, improving and sharing the captured
		  knowledge. We begin with our description of the required
		  knowledge of a geometrical concept formed during human
		  grasping, to a point where it can be used to plan grasping
		  based on the objects identified. Using tactile and
		  kinesthetic information along with relevant domain
		  concepts, we emphasized on the rationality of designing an
		  ontology for reusability and sustainability of knowledge.
		  We tried to lay down a visual model of the ontology, also
		  called the Ontograph, which illuminates the existence and
		  relationships among the various objects of the grasping
		  domain. We have also checked the decisive capability of the
		  ontology by reasoning it with Description Logic (DL)
		  queries of data property values for individuals of
		  geometric classes. The output of the queries provided us
		  with individuals of the specific geometric pattern, which
		  can be used to decide the type of grasp that could be
		  implemented on objects.},
  booktitle	= {Proceedings of the 2019 4th International Conference on
		  Advances in Robotics},
  articleno	= {10},
  numpages	= {6},
  keywords	= {Tactile, Reasoning, Prosthetics, Ontology, OWL, Knowledge,
		  Kinesthetic},
  location	= {Chennai, India},
  series	= {AIR '19}
}

@InProceedings{	  10.1145/3539618.3594250,
  author	= {Liao, Lizi and Yang, Grace Hui and Shah, Chirag},
  title		= {Proactive Conversational Agents in the Post-ChatGPT
		  World},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3594250},
  doi		= {10.1145/3539618.3594250},
  abstract	= {ChatGPT and similar large language model (LLM) based
		  conversational agents have brought shock waves to the
		  research world. Although astonished by their human-like
		  performance, we find they share a significant weakness with
		  many other existing conversational agents in that they all
		  take a passive approach in responding to user queries. This
		  limits their capacity to understand the users and the task
		  better and to offer recommendations based on a broader
		  context than a given conversation. Proactiveness is still
		  missing in these agents, including their ability to
		  initiate a conversation, shift topics, or offer
		  recommendations that take into account a more extensive
		  context. To address this limitation, this tutorial reviews
		  methods for equipping conversational agents with proactive
		  interaction abilities.The full-day tutorial is divided into
		  four parts, including multiple interactive exercises. We
		  will begin the tutorial with an interactive exercise and
		  cover the design of existing conversational systems
		  architecture and challenges. The content includes coverage
		  of LLM-based recent advancements such as ChatGPT and Bard,
		  along with reinforcement learning with human feedback
		  (RLHF) technique. Then we will introduce the concept of
		  proactive conversation agents and preset recent
		  advancements in proactiveness of conversational agents,
		  including actively driving conversations by asking
		  questions, topic shifting, and methods that support
		  strategic planning of conversation. Next, we will discuss
		  important issues in conversational responses' quality
		  control, including safety, appropriateness, language
		  detoxication, hallucination, and alignment. Lastly, we will
		  launch another interactive exercise and discussion with the
		  audience to arrive at concluding remarks, prospecting open
		  challenges and new directions. By exploring new techniques
		  for enhancing conversational agents' proactive behavior to
		  improve user engagement, this tutorial aims to help
		  researchers and practitioners develop more effective
		  conversational agents that can better understand and
		  respond to user needs proactively and safely.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3452–3455},
  numpages	= {4},
  keywords	= {conversational ai, conversational search, proactive
		  conversation},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3664476.3669922,
  author	= {Suciu, George and Sachian, Mari-Anais and Bratulescu,
		  Razvan and Koci, Kejsi and Parangoni, Grigor},
  title		= {Entity Recognition on Border Security},
  year		= {2024},
  isbn		= {9798400717185},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664476.3669922},
  doi		= {10.1145/3664476.3669922},
  abstract	= {Entity recognition, also known as named entity recognition
		  (NER), is a fundamental task in natural language processing
		  (NLP) that involves identifying and categorizing entities
		  within text. These entities, such as names of people,
		  organizations, locations, dates, and numerical values,
		  provide structured information from unstructured text data.
		  NER models, ranging from rule-based to machine
		  learning-based approaches, decode linguistic patterns and
		  contextual information to extract entities effectively.
		  This article explores the roles of entities, tokens, and
		  NER models in NLP, detailing their significance in various
		  applications like information retrieval and border
		  security. It delves into the practices of implementing NER
		  in legal document analysis, travel history analysis, and
		  document verification, showcasing its transformative impact
		  in streamlining processes and enhancing security measures.
		  Despite challenges such as ambiguity and data scarcity,
		  ongoing research and emerging trends in multilingual NER
		  and ethical considerations promise to drive innovation in
		  the field. By addressing these challenges and embracing new
		  developments, entity recognition is poised to continue
		  advancing NLP capabilities and powering diverse real-world
		  applications.},
  booktitle	= {Proceedings of the 19th International Conference on
		  Availability, Reliability and Security},
  articleno	= {135},
  numpages	= {6},
  keywords	= {Border Security, Entity, Frameworks, Machine Learning,
		  NER, RNNs, Recognition, SVM, Travel},
  location	= {Vienna, Austria},
  series	= {ARES '24}
}

@InProceedings{	  10.1145/3625469.3625470,
  author	= {Wu, Wenjing and Yuan, Qi and Chen, Qiulan and Cao,
		  Yunzhong},
  title		= {Construction Safety Knowledge Graph Integrating Text and
		  Image Information},
  year		= {2023},
  isbn		= {9798400707681},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625469.3625470},
  doi		= {10.1145/3625469.3625470},
  abstract	= {To improve the extraction efficiency and visualization of
		  construction safety knowledge, this paper combines
		  knowledge graph technology with construction safety domain,
		  and proposes a basic framework of construction safety
		  knowledge ontology based on the evolution logic of safety
		  events according to the characteristics of knowledge.
		  Considering two types of safety knowledge carriers and data
		  sources, text and image, a knowledge graph is designed to
		  contain text semantic features and image features, and the
		  knowledge services based on different dimensional knowledge
		  queries are validated in the experiments. The results show
		  that the BERT-BiLSTM-CRF algorithm can be used to extract
		  entities in text, and YOLOv5-FastPose can extract excavator
		  poses from images. This paper verifies the applicability of
		  knowledge graphs for safety knowledge mining, visualization
		  and services.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Information Management and Management Science},
  pages		= {26–32},
  numpages	= {7},
  keywords	= {Construction safety management, Entity recognition,
		  Knowledge graph, Pose estimation},
  location	= {Chengdu, China},
  series	= {IMMS '23}
}

@InProceedings{	  10.1145/3231053.3231128,
  author	= {jabbar, Sohail and Malik, Kaleem Razzaq and Ahmad,
		  Mudassar},
  title		= {Real-time RDF adaptation model for smart human-care
		  querying in IoT based mobile applications},
  year		= {2018},
  isbn		= {9781450364287},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3231053.3231128},
  doi		= {10.1145/3231053.3231128},
  abstract	= {Majorly, nowadays, the collected raw data through mobiles
		  is huge based on sensors embedded in devices and IoT based
		  applications. These applications use Internet of Things
		  (IoT) and Big Data analytics services and daily activities
		  as routines for recording and analyzing real-time data for
		  human-care. Nowadays, mobile is having services built on
		  sensors to reduce human involvement in data collection.
		  Many issues concerning security and privacy can be resolved
		  if we use data analytics in services to represent data as
		  Resource Description Framework (RDF). The automated
		  transformation mechanism in relational database taken from
		  mobile sensors and applications into semantically annotated
		  RDF stores. This study is comprised of a methodology for
		  refining compatibility between different data models by
		  introducing real-time RDF context model for adopting data
		  to smart querying in mobile applications. Smart querying
		  capabilities come from transformation between sensors with
		  activity services data and RDF data store for mobile
		  applications. Whereas, case study built-up out of
		  applications data is used to show data adaptation process
		  for smart querying for human-care in mobile devices.
		  Multiple queries are used to extract mobile video
		  information smartly and efficiently. According to results
		  shows if standard deviation gets greater than mean that
		  tend of values is spreading over a wider range of values.},
  booktitle	= {Proceedings of the 2nd International Conference on Future
		  Networks and Distributed Systems},
  articleno	= {61},
  numpages	= {5},
  keywords	= {real-time smart querying, real-time data transformation,
		  mobile application, linked data, human-care services, data
		  modeling, big data, IoT},
  location	= {Amman, Jordan},
  series	= {ICFNDS '18}
}

@InProceedings{	  10.1145/3535735.3535755,
  author	= {Zeynalova, Nigar},
  title		= {Student Creativity and Talent Development in Higher
		  Education Institutions of Azerbaijan},
  year		= {2022},
  isbn		= {9781450396196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3535735.3535755},
  doi		= {10.1145/3535735.3535755},
  abstract	= {The aim of the study is to find out how to develop student
		  creativity in Azerbaijan higher education institutions.
		  This research is consists of two parts: descriptive part of
		  the research, where the author tried to analyze background
		  of the study and research have been done in the filed so
		  far. The second part based on a survey, which is conducted
		  for students and teachers of various HEIs of the country.
		  219 respondents answered the research questions from
		  various universities of Azerbaijan. The results of research
		  indicated that creativity can be strengthen in higher
		  education with help and support of teachers and by
		  knowledge, creativity can be further enhanced in the
		  teaching process depending on the teacher's pedagogical
		  skills and approach, various teaching methods can develop
		  creativity of students in higher education including modern
		  innovative technologies, creativity justifies itself at
		  every moment in teaching and learning process in higher
		  education, motivation is a key issue for increasing
		  creativity.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Information and Education Innovations},
  pages		= {163–175},
  numpages	= {13},
  keywords	= {talent, knowledge, innovation, higher education,
		  development, creativity},
  location	= {Belgrade, Serbia},
  series	= {ICIEI '22}
}

@InProceedings{	  10.1145/3216122.3216152,
  author	= {Leclercq, \'{E}ric and Savonnet, Marinette},
  title		= {A Tensor Based Data Model for Polystore: An Application to
		  Social Networks Data},
  year		= {2018},
  isbn		= {9781450365277},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3216122.3216152},
  doi		= {10.1145/3216122.3216152},
  abstract	= {In this article, we show how the mathematical object
		  tensor can be used to build a multi-paradigm model for the
		  storage of social data in data warehouses. From an
		  architectural point of view, our approach allows to link
		  different storage systems (polystore) and limits the impact
		  of ETL tools performing model transformations required to
		  feed different analysis algorithms. Therefore, systems can
		  take advantage of multiple data models both in terms of
		  query execution performance and the semantic expressiveness
		  of data representation. The proposed model allows to reach
		  the logical independence between data and programs
		  implementing analysis algorithms. With a concrete case
		  study on message virality on Twitter during the French
		  presidential election of 2017, we highlight some of the
		  contributions of our model.},
  booktitle	= {Proceedings of the 22nd International Database Engineering
		  \&amp; Applications Symposium},
  pages		= {110–118},
  numpages	= {9},
  keywords	= {Tensor, Polystore, OLAP, Multi-relational Networks,
		  Multi-paradigm Storage, Associative Array},
  location	= {Villa San Giovanni, Italy},
  series	= {IDEAS '18}
}

@InProceedings{	  10.1145/3373722.3373778,
  author	= {Surkova, Anna and Skorynin, Sergey and Chernobaev, Igor},
  title		= {Word embedding and cognitive linguistic models in text
		  classification tasks},
  year		= {2020},
  isbn		= {9781450376709},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3373722.3373778},
  doi		= {10.1145/3373722.3373778},
  abstract	= {The paper considers two linguistic models, analyzed the
		  possibility of their use for the text data classification
		  as well as their associations in the integrated texts
		  presentation. A cognitive approach for the text
		  classification issues is presented. An algorithm to
		  identify the words basic level using WordNet is considered.
		  A model for text classification based on the pre-trained
		  word embeddings is presented. The model consists of three
		  layers: embedding layer Long-Short Term Memory (LSTM)
		  layer, and softmax layer. The model was trained and
		  evaluated on the 20 Newsgroups dataset. The classification
		  quality was assessed by F- measure, precision and recall.
		  The obtained results analysis is carried out. Both
		  described models show good results, low scores for some
		  texts are explained. The advantages and limitations of the
		  linguistic models are shown. In future works the authors
		  are going to combine proposed models and modify them. Thus,
		  for model based on word embedding there are pretty vast
		  opportunities for extension: from experimenting with
		  different word embeddings and various distance metrics to
		  more complicated architecture of layers and even promising
		  state of the art artificial neural network models,
		  activation functions and their modifications. In addition,
		  there is research area of proper ensemble strategy
		  selection.},
  booktitle	= {Proceedings of the XI International Scientific Conference
		  Communicative Strategies of the Information Society},
  articleno	= {12},
  numpages	= {6},
  keywords	= {words vector representations, thesaurus, sequential data,
		  data mining, cognitive semantics, classification, WordNet},
  location	= {St. Petersburg, Russian Federation},
  series	= {CSIS'2019}
}

@Article{	  10.1109/taslp.2023.3290428,
  author	= {Petermann, Darius and Wichern, Gordon and Subramanian,
		  Aswin Shanmugam and Wang, Zhong-Qiu and Roux, Jonathan Le},
  title		= {Tackling the Cocktail Fork Problem for Separation and
		  Transcription of Real-World Soundtracks},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3290428},
  doi		= {10.1109/TASLP.2023.3290428},
  abstract	= {Emulating the human ability to solve the cocktail party
		  problem, i.e., focus on a source of interest in a complex
		  acoustic scene, is a long standing goal of audio source
		  separation research. In this paper, we focus on the
		  cocktail fork problem, which takes a three-pronged approach
		  to source separation by separating an audio mixture such as
		  a movie soundtrack or podcast into the three broad
		  categories of speech, music, and sound effects (SFX -
		  understood to include ambient noise and natural sound
		  events). We evaluate several deep learning-based source
		  separation models on this task using simple objective
		  measures such as signal-to-distortion ratio (SDR) as well
		  as objective metrics that better correlate with human
		  perception. Furthermore, we thoroughly evaluate how source
		  separation can influence the downstream transcription asks
		  of speech recognition for speech and audio tagging for
		  music and SFX. We also investigate the task of activity
		  detection on the three sources as a way to further improve
		  source separation and transcription. While we observe that
		  source separation improves transcription performance in
		  comparison to the original soundtrack, performance is still
		  sub-optimal due to artifacts introduced by the separation
		  process. Therefore, we thoroughly investigate how remixing
		  of the three separated source stems at various relative
		  levels can reduce artifacts and consequently improve
		  transcription performance. We find that remixing music and
		  SFX interferences at a target SNR of 17.5 dB reduces speech
		  recognition word error rate, and similar impact from
		  remixing is observed for tagging music and SFX content.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jul,
  pages		= {2592–2605},
  numpages	= {14}
}

@InProceedings{	  10.1145/3632410.3632466,
  author	= {Kumar, Ayush and Shalghar, Abhay M and Chauhan, Harsh and
		  Ganesan, Balaji and Chaudhuri, Ritwik and Kannan, Aswin},
  title		= {Document structure aware Relation Extraction for Semantic
		  Automation},
  year		= {2024},
  isbn		= {9798400716348},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632410.3632466},
  doi		= {10.1145/3632410.3632466},
  abstract	= {Relational Graph Convolutional Network models are a class
		  of Graph Neural Network models used for link prediction in
		  heterogeneous graphs. They’re being used in a variety of
		  industrial applications including semantic automation tasks
		  in a Lakehouse. In this work, we propose a novel way to
		  incorporate document specific features into a RGCN model
		  that helps improve relation extraction accuracy by about 15
		  points. Further, we extend this document awareness to
		  semantic tasks on tabular data and discuss our results.},
  booktitle	= {Proceedings of the 7th Joint International Conference on
		  Data Science \&amp; Management of Data (11th ACM IKDD CODS
		  and 29th COMAD)},
  pages		= {232–236},
  numpages	= {5},
  keywords	= {RGCN, information extraction, semantic automation},
  location	= {Bangalore, India},
  series	= {CODS-COMAD '24}
}

@InProceedings{	  10.1145/3487664.3487784,
  author	= {Stach, Christoph and Br\"{a}cker, Julia and Eichler,
		  Rebecca and Giebler, Corinna and Mitschang, Bernhard},
  title		= {Demand-Driven Data Provisioning in Data Lakes:
		  BARENTS&nbsp;—&nbsp;A Tailorable Data Preparation Zone},
  year		= {2022},
  isbn		= {9781450395564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487664.3487784},
  doi		= {10.1145/3487664.3487784},
  abstract	= {Data has never been as significant as it is today. It can
		  be acquired virtually at will on any subject. Yet, this
		  poses new challenges towards data management, especially in
		  terms of storage (data is not consumed during processing,
		  i.&nbsp;e., the data volume keeps growing), flexibility
		  (new applications emerge), and operability (analysts are no
		  IT experts). The goal has to be a demand-driven data
		  provisioning, i.&nbsp;e., the right data must be available
		  in the right form at the right time. Therefore, we
		  introduce a tailorable data preparation zone for Data Lakes
		  called BARENTS. It enables users to model in an ontology
		  how to derive information from data and assign the
		  information to use cases. The data is automatically
		  processed based on this model and the refined data is made
		  available to the appropriate use cases. Here, we focus on a
		  resource-efficient data management strategy. BARENTS can be
		  embedded seamlessly into established Big Data
		  infrastructures, e.&nbsp;g., Data Lakes.},
  booktitle	= {The 23rd International Conference on Information
		  Integration and Web Intelligence},
  pages		= {187–198},
  numpages	= {12},
  keywords	= {zone model, ontology, knowledge modeling, food analysis,
		  data transformation, data pre-processing, data management,
		  Data Lakes},
  location	= {Linz, Austria},
  series	= {iiWAS2021}
}

@InProceedings{	  10.1145/3217197.3217207,
  author	= {Yang, Xi and Lehman, Tom and Kettimuthu, Raj and Winkler,
		  Linda and Jung, Eun-Sung},
  title		= {A Model Driven Intelligent Orchestration Approach to
		  Service Automation in Large Distributed Infrastructures},
  year		= {2018},
  isbn		= {9781450358620},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3217197.3217207},
  doi		= {10.1145/3217197.3217207},
  abstract	= {Today's scientific computing applications and workflows
		  operate on heterogeneous and vastly distributed
		  infrastructures. Traditional human-in-the-loop service
		  engineering approach met its insurmountable challenge in
		  dealing with these very complex and diverse networked
		  systems, including conventional and software defined
		  networks, compute, storage, clouds and instruments.
		  Orchestration is the key to integrate and coordinate the
		  networked multi-services and automate end-to-end workflows.
		  In this work, we present a model driven intelligent
		  orchestration approach to this end-to-end automation, which
		  is built upon a semantic modeling solution that supports
		  the full stack of service integration, orchestration,
		  abstraction, and intent and policy representation. We also
		  present the design of a real-world orchestrator called
		  StackV that is able to accommodate highly complex
		  application scenarios such as Software Defined ScienceDMZ
		  (SD-SDMZ) and Hybrid Cloud Inter-Networking (HCIN) by
		  implementing this approach.},
  booktitle	= {Proceedings of the 1st International Workshop on
		  Autonomous Infrastructure for Science},
  articleno	= {5},
  numpages	= {8},
  keywords	= {Service Automation, Modeling, Intelligent Orchestration,
		  Distributed Infrastructure},
  location	= {Tempe, AZ, USA},
  series	= {AI-Science'18}
}

@InProceedings{	  10.1145/3209281.3209333,
  author	= {Oliveira, Marcelo Iury S. and Oliveira, Lairson Emanuel R.
		  A. and Batista, Marlos G. Ribeiro and L\'{o}scio,
		  Bernadette Farias},
  title		= {Towards a meta-model for data ecosystems},
  year		= {2018},
  isbn		= {9781450365260},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209281.3209333},
  doi		= {10.1145/3209281.3209333},
  abstract	= {Data Ecosystems are socio-technical networks that enable
		  collaboration between autonomous actors such as
		  enterprises, institutions, and individuals. While Data
		  Ecosystems are thus gaining importance, research into Data
		  Ecosystems is still in its infancy stages. The terminology
		  and definitions for Data Ecosystem vary greatly. This
		  diversity imposes a pressing problem for the development of
		  a clear understanding of the new opportunities and emergent
		  challenges in exploiting Data Ecosystems. Accurate
		  definitions are required to get a mutual understanding of
		  what Data Ecosystems involve. Moreover, to the best of our
		  knowledge, a model for describing a Data Ecosystem and its
		  essential concepts has not been proposed yet. In this work,
		  we aim to fill these gaps by reviewing the Data Ecosystem
		  literature, and based on the field literature, we propose a
		  meta-model for describing Data Ecosystems. In particular,
		  the proposed meta-model describes the Data Ecosystem
		  fundamental concepts and their inter-relationships for
		  enabling analysis and description of ecosystems.
		  Especially, the meta-model declares explicitly how all
		  these concepts are related to each other in such holistic
		  view, hence facilitating knowledge creation and management
		  in the ecosystem.},
  booktitle	= {Proceedings of the 19th Annual International Conference on
		  Digital Government Research: Governance in the Data Age},
  articleno	= {72},
  numpages	= {10},
  keywords	= {standardization, meta-model, government data, data
		  ecosystem},
  location	= {Delft, The Netherlands},
  series	= {dg.o '18}
}

@Article{	  10.1145/3363574,
  author	= {Lee, John Boaz and Rossi, Ryan A. and Kim, Sungchul and
		  Ahmed, Nesreen K. and Koh, Eunyee},
  title		= {Attention Models in Graphs: A Survey},
  year		= {2019},
  issue_date	= {December 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {6},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3363574},
  doi		= {10.1145/3363574},
  abstract	= {Graph-structured data arise naturally in many different
		  application domains. By representing data as graphs, we can
		  capture entities (i.e., nodes) as well as their
		  relationships (i.e., edges) with each other. Many useful
		  insights can be derived from graph-structured data as
		  demonstrated by an ever-growing body of work focused on
		  graph mining. However, in the real-world, graphs can be
		  both large—with many complex patterns—and noisy, which
		  can pose a problem for effective graph mining. An effective
		  way to deal with this issue is to incorporate
		  “attention” into graph mining solutions. An attention
		  mechanism allows a method to focus on task-relevant parts
		  of the graph, helping it to make better decisions. In this
		  work, we conduct a comprehensive and focused survey of the
		  literature on the emerging field of graph attention models.
		  We introduce three intuitive taxonomies to group existing
		  work. These are based on problem setting (type of input and
		  output), the type of attention mechanism used, and the task
		  (e.g., graph classification, link prediction). We motivate
		  our taxonomies through detailed examples and use each to
		  survey competing approaches from a unique standpoint.
		  Finally, we highlight several challenges in the area and
		  discuss promising directions for future work.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= nov,
  articleno	= {62},
  numpages	= {25},
  keywords	= {graph attention survey, graph attention, deep learning,
		  Attention mechanism}
}

@Article{	  10.1145/3748239.3748249,
  author	= {Liu, Lihui and Wang, Zihao and Tong, Hanghang},
  title		= {Neural-Symbolic Reasoning over Knowledge Graphs: A Survey
		  from a Query Perspective},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {27},
  number	= {1},
  issn		= {1931-0145},
  url		= {https://doi.org/10.1145/3748239.3748249},
  doi		= {10.1145/3748239.3748249},
  abstract	= {Knowledge graph reasoning is pivotal in various domains
		  such as data mining, artificial intelligence, the Web, and
		  social sciences. These knowledge graphs function as
		  comprehensive repositories of human knowledge, facilitating
		  the inference of new information. Traditional symbolic
		  reasoning, despite its strengths, struggles with the
		  challenges posed by incomplete and noisy data within these
		  graphs. In contrast, the rise of Neural Symbolic AI marks a
		  significant advancement, merging the robustness of deep
		  learning with the precision of symbolic reasoning. This
		  integration aims to develop AI systems that are not only
		  highly interpretable and explainable but also versatile,
		  effectively bridging the gap between symbolic and neural
		  methodologies. Additionally, the advent of large language
		  models (LLMs) has opened new frontiers in knowledge graph
		  reasoning, enabling the extraction and synthesis of
		  knowledge in unprecedented ways. This survey offers a
		  thorough review of knowledge graph reasoning, focusing on
		  various query types and the classification of neural
		  symbolic reasoning. Furthermore, it explores the innovative
		  integration of knowledge graph reasoning with large
		  language models, highlighting the potential for
		  groundbreaking advancements. This comprehensive overview is
		  designed to support researchers and practitioners across
		  multiple fields, including data mining, AI, the Web, and
		  social sciences, by providing a detailed understanding of
		  the current landscape and future directions in knowledge
		  graph reasoning.},
  journal	= {SIGKDD Explor. Newsl.},
  month		= jul,
  pages		= {124–136},
  numpages	= {13},
  keywords	= {knowledge graph question answering, knowledge graph
		  reasoning, neural symbolic reasoning}
}

@InProceedings{	  10.1145/3677779.3677815,
  author	= {Guan, Jing},
  title		= {Research on Human-Computer Interaction Design Standards in
		  Artificial Intelligence Products},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677779.3677815},
  doi		= {10.1145/3677779.3677815},
  abstract	= {This article first analyzes the concept and development
		  process of artificial intelligence products, then delves
		  into the application and development of human-computer
		  interaction technology in artificial intelligence products,
		  and then analyzes the standardization research of
		  human-computer interaction design in artificial
		  intelligence products; Finally, the two-dimensional
		  architecture of human-computer interaction design for
		  artificial intelligence products was elaborated, with
		  detailed discussions and planning in terms of technical and
		  value dimensions. The rapid development of artificial
		  intelligence has driven the emergence of a large number of
		  artificial intelligence products, resulting in a
		  fundamental change in the human-computer interaction mode
		  of products and higher requirements for human-computer
		  interaction design. Therefore, how to construct
		  standardized human-computer interaction design patterns in
		  artificial intelligence products in the new era is a key
		  research topic in the development of artificial
		  intelligence products.},
  booktitle	= {Proceedings of the International Conference on Modeling,
		  Natural Language Processing and Machine Learning},
  pages		= {220–225},
  numpages	= {6},
  location	= {Xi'an, China},
  series	= {CMNM '24}
}

@Article{	  10.1145/3660826,
  author	= {Yan, Chuan and Meng, Mark Huasong and Xie, Fuman and Bai,
		  Guangdong},
  title		= {Investigating Documented Privacy Changes in Android OS},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {FSE},
  url		= {https://doi.org/10.1145/3660826},
  doi		= {10.1145/3660826},
  abstract	= {Android has empowered third-party apps to access data and
		  services on mobile devices since its genesis.This involves
		  a wide spectrum of user privacy-sensitive data, such as the
		  device ID and location. In recent years, Android has taken
		  proactive measures to adapt its access control policies for
		  such data, in response to the increasingly strict privacy
		  protection regulations around the world. When each new
		  Android version is released, its privacy changes induced by
		  the version evolution are transparently disclosed, and we
		  refer to them as documented privacy changes (DPCs).
		  Implementing DPCs in Android OS is a non-trivial task, due
		  to not only the dispersed nature of those access control
		  points within the OS, but also the challenges posed by
		  backward compatibility. As a result, whether the actual
		  access control enforcement in the OS implementations aligns
		  with the disclosed DPCs becomes a critical concern. In this
		  work, we conduct the first systematic study on the
		  consistency between the operational behaviors of the OS at
		  runtime and the officially disclosed DPCs. We propose
		  DopCheck, an automatic DPC-driven testing framework
		  equipped with a large language model (LLM) pipeline. It
		  features a serial of analysis to extract the ontology from
		  the privacy change documents written in natural language,
		  and then harnesses the few-shot capability of LLMs to
		  construct test cases for the detection of DPC-compliance
		  issues in OS implementations. We apply DopCheck with the
		  latest versions (10 to 13) of Android Open Source Project
		  (AOSP). Our evaluation involving 79 privacy-sensitive APIs
		  demonstrates that DopCheck can effectively recognize DPCs
		  from Android documentation and generate rigorous test
		  cases. Our study reveals that the status quo of the
		  DPC-compliance issues is concerning, evidenced by 19 bugs
		  identified by DopCheck. Notably, 12 of them are discovered
		  in Android 13 and 6 in Android 10 for the first time,
		  posing more than 35\% Android users to the risk of privacy
		  leakage. Our findings should raise an alert to Android
		  users and app developers on the DPC compliance issues when
		  using or developing an app, and would also underscore the
		  necessity for Google to comprehensively validate the actual
		  implementation against its privacy documentation prior to
		  the OS release.},
  journal	= {Proc. ACM Softw. Eng.},
  month		= jul,
  articleno	= {119},
  numpages	= {24},
  keywords	= {Android, documentation, privacy, testing}
}

@InProceedings{	  10.1145/3183428.3183429,
  author	= {Barn, Balbir S. and Barn, Ravinder},
  title		= {Towards a unified conceptual model for surveillance
		  theories: "we shall meet in the place where there is no
		  darkness" - 1984, george orwell},
  year		= {2018},
  isbn		= {9781450356619},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3183428.3183429},
  doi		= {10.1145/3183428.3183429},
  abstract	= {The erosion of values such as privacy can be a critical
		  factor in preventing the acceptance of new innovative
		  technology especially in challenging environments such as
		  the criminal justice system. Erosion of privacy happens
		  through either deliberate or inadvertent surveillance.
		  Since Bentham's original liberal project in the 1900s, a
		  literature and a whole study area around theories of
		  surveillance has developed. Increasingly this general body
		  of work has focussed on the role of information technology
		  as a vehicle for surveillance activity. Despite an
		  abundance of knowledge, a unified view of key surveillance
		  concepts that is useful to designers of information systems
		  in preventing or reducing unintended surveillance remains
		  elusive. This paper contributes a conceptual model that
		  synthesises the gamut of surveillance theories as a first
		  step to a theory building effort for use by Information
		  Systems professionals. The model is evaluated using a
		  design science research paradigm using data from both
		  examples of surveillance and a recently completed research
		  project that developed technology for the UK youth justice
		  system.},
  booktitle	= {Proceedings of the 40th International Conference on
		  Software Engineering: Software Engineering in Society},
  pages		= {71–80},
  numpages	= {10},
  keywords	= {surveillance, reference model, privacy, conceptual model},
  location	= {Gothenburg, Sweden},
  series	= {ICSE-SEIS '18}
}

@Article{	  10.1145/3582263,
  author	= {Bartalesi, Valentina and Pratelli, Nicolo’ and Lenzi,
		  Emanuele and Pontari, Paolo},
  title		= {Using Semantic Web to Create and Explore an Index of
		  Toponyms Cited in Medieval Geographical Works},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3582263},
  doi		= {10.1145/3582263},
  abstract	= {Western thought in European history was mainly affected by
		  the image of the world created during the Middle Ages and
		  Renaissance. The most popular reason to travel during the
		  Middle Ages was taking a pilgrimage. Jerusalem, Rome, and
		  Santiago de Compostela were the most popular destinations.
		  It is not surprising that a lot of works written by
		  travellers as guides for pilgrims exist. By the beginning
		  of the Renaissance, a more precise image of the world was
		  defined, thanks to the discovery of ancient geographical
		  models, especially the work of Ptolemy. The Italian
		  National Research Project (PRIN) IMAGO --- Index Medii Aevi
		  Geographiae Operum --- (2020-2023) aims to provide a
		  systematic overview of the medieval and renaissance Latin
		  geographical literature using the Semantic Web technologies
		  and the LOD paradigm. Indeed, until now, this literature
		  has not been studied using digital methods. In particular,
		  this article presents how we formally represented the
		  knowledge about the toponyms, or place names, in the IMAGO
		  ontology. To maximise the interoperability, we developed
		  the IMAGO ontology as an extension of two reference
		  vocabularies: the CIDOC CRM and its extension FRBRoo,
		  including its in-progress reformulation, LRMoo.
		  Furthermore, we used Wikidata as reference knowledge base.
		  As case study, we chose to represent the knowledge related
		  to the toponyms cited by the Italian poet Dante Alighieri
		  in his Latin works. We carried out a first experiment for
		  visualising the knowledge about these toponyms on a map and
		  in the form of tables and CSV files.},
  journal	= {J. Comput. Cult. Herit.},
  month		= apr,
  articleno	= {26},
  numpages	= {18},
  keywords	= {Dante Alighieri, Wikidata, CIDOC CRM, toponyms, ontology,
		  Linked Open Data, Semantic Web}
}

@InProceedings{	  10.1145/3411564.3411630,
  author	= {Thalheimer, J\'{e}ferson Miguel and Filho, Aluizio
		  Haendchen and Briks, Fabio Julio Pereira and Ribeiro,
		  Rafael Castaneda and Concatto, Fernando and Viecelli,
		  Ang\'{e}lica Karize},
  title		= {A Microservice-driven Collaborative Agent in Virtual
		  Learning Environments: A Role Model for a Tracing Agent},
  year		= {2020},
  isbn		= {9781450388733},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411564.3411630},
  doi		= {10.1145/3411564.3411630},
  abstract	= {Currently, distance learning comprises almost half of
		  students enrolled in undergraduate courses in Brazil.
		  However, the dropout rate of this modality is over 50\%,
		  and only 22\% of students complete the courses [27].
		  Despite technological advances and good acceptance of this
		  modality, research indicates that the lack of involvement
		  in a virtual community can lead to feelings of loneliness,
		  low self-esteem, isolation and desmotivation. There is
		  evidence that these feelings are among the main factors
		  responsible for the low performance and high evasion rate.
		  Virtual Learning Environments (VLE) handles a large volume
		  of student interaction data. In this context, it is
		  important to create mechanisms to maintain and manage a
		  data structure to facilitate the processes of transforming
		  data into information and knowledge. This paper aims to
		  present a tracing agent responsible for maintaining and
		  managing the data structure in VLE. The agent acts in the
		  context of a microservice-oriented multi-agent system,
		  interacting and collaborating with other agents in order to
		  improve interaction and decision-making processes. This
		  work becomes original and at the same time innovative,
		  presenting an unprecedented combination of technologies and
		  techniques in the context of VLEs.},
  booktitle	= {Proceedings of the XVI Brazilian Symposium on Information
		  Systems},
  articleno	= {21},
  numpages	= {8},
  keywords	= {Virtual Learning Environment, Tracing Agent, Multiagent
		  System},
  location	= {S\~{a}o Bernardo do Campo, Brazil},
  series	= {SBSI '20}
}

@InProceedings{	  10.1145/3345252.3345295,
  author	= {Baeva, Desislava},
  title		= {Using Lindenmayer Systems For Generative Modeling Of
		  Graphic Concepts, Set In Elements Of Bulgarian Folklore
		  Embroidery},
  year		= {2019},
  isbn		= {9781450371490},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3345252.3345295},
  doi		= {10.1145/3345252.3345295},
  abstract	= {L-systems as a type of fractal model are one of the most
		  widely used examples of the integration of mathematics and
		  information technology. They can be seen in many elements
		  created with the means of computer graphics, and have
		  become a new tool that is relevant for modeling in biology,
		  geology, and other natural sciences. Along with their
		  applications in advanced technology science, L-systems also
		  refer to archaic models that are surprisingly common in
		  traditional designs of different ethnicities, and some of
		  their basic concepts are also fundamental to systems of
		  knowledge about the Bulgarian embroidery.This article
		  reviews and analyzes the generative characteristics of some
		  graphic motifs specific to Bulgarian folklore. The
		  applicability of the study consists in finding tools for a
		  general description of these motifs, which can easily refer
		  us to other elements of human creativity - such as speech
		  or music, and provoke the discovery of relationships and
		  relationships between them.},
  booktitle	= {Proceedings of the 20th International Conference on
		  Computer Systems and Technologies},
  pages		= {234–239},
  numpages	= {6},
  keywords	= {generative art, embroidery simulation, L-systems},
  location	= {Ruse, Bulgaria},
  series	= {CompSysTech '19}
}

@InBook{	  10.1145/3382097.3382114,
  title		= {Expert modeling in OWL},
  year		= {2020},
  isbn		= {9781450376174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3382097.3382114},
  abstract	= {Enterprises have made amazing advances by taking advantage
		  of data about their business to provide predictions and
		  understanding of their customers, markets, and products.
		  But as the world of business becomes more interconnected
		  and global, enterprise data is no long a monolith; it is
		  just a part of a vast web of data. Managing data on a
		  world-wide scale is a key capability for any business
		  today.The Semantic Web treats data as a distributed
		  resource on the scale of the World Wide Web, and
		  incorporates features to address the challenges of massive
		  data distribution as part of its basic design. The aim of
		  the first two editions was to motivate the Semantic Web
		  technology stack from end-to-end; to describe not only what
		  the Semantic Web standards are and how they work, but also
		  what their goals are and why they were designed as they
		  are. It tells a coherent story from beginning to end of how
		  the standards work to manage a world-wide distributed web
		  of knowledge in a meaningful way.The third edition builds
		  on this foundation to bring Semantic Web practice to
		  enterprise. Fabien Gandon joins Dean Allemang and Jim
		  Hendler, bringing with him years of experience in global
		  linked data, to open up the story to a modern view of
		  global linked data. While the overall story is the same,
		  the examples have been brought up to date and applied in a
		  modern setting, where enterprise and global data come
		  together as a living, linked network of data. Also included
		  with the third edition, all of the data sets and queries
		  are available online for study and experimentation at
		  data.world/swwo.},
  booktitle	= {Semantic Web for the Working Ontologist: Effective
		  Modeling for Linked Data, RDFS, and OWL}
}

@InProceedings{	  10.1145/3709026.3709105,
  author	= {Aili, Elyar and Yilahun, Hankiz and Imam, Seyyare and
		  Hamdulla, Askar},
  title		= {Relational Representation Augmented Graph Attention
		  Network for Knowledge Graph Completion},
  year		= {2025},
  isbn		= {9798400718182},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3709026.3709105},
  doi		= {10.1145/3709026.3709105},
  abstract	= {Knowledge Graph Completion (KGC) is a popular topic in
		  knowledge graph construction and related applications,
		  aiming to complete the structure of knowledge graphs by
		  predicting missing entities or relations and mining unknown
		  facts in the knowledge graph. In the KGC task, Graph Neural
		  Network (GNN)-based methods have achieved remarkable
		  results due to their advantage of effectively capturing
		  complex relations among entities and generating more
		  accurate and rich entity representations by aggregating
		  information from neighbouring nodes. These methods mainly
		  focus on the representation of entities, and the
		  representation of relations is obtained using simple
		  dimensional transformations or initial embeddings. This
		  treatment ignores the diversity and complex semantics of
		  relations and restricts the efficiency of the model in
		  utilizing relational information in the reasoning process.
		  In this work, we propose the Relational Representation
		  Augmented Graph Attention Network (RRA-GAT), which
		  effectively identifies and weights neighbouring relations
		  that actually contribute to the target relation by
		  filtering out irrelevant information through an attention
		  function based on the information and spatial domain.
		  Furthermore, we capture complex patterns and features in
		  the relational embedding by means of a feed-forward network
		  consisting of a series of linear transformations and
		  nonlinear activation functions. Experiments demonstrate the
		  very advanced performance of RRA-GAT on the link prediction
		  task on standard datasets FB15k-237 and WN18RR (e.g.,
		  improved the MRR metric on the WN18RR dataset by 7.8\%
		  relative improvement).},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {449–455},
  numpages	= {7},
  keywords	= {Graph neural networks, Knowledge graph completion,
		  Knowledge graph embedding},
  location	= { },
  series	= {CSAI '24}
}

@InProceedings{	  10.1145/3323503.3360641,
  author	= {Martini, Bruno G. and Helfer, Gilson A. and Barbosa, Jorge
		  L. V. and Silva, Marcio R. da and de Figueiredo, Rodrigo M.
		  and Modolo, Regina C. E. and Yamin, Adenauer C.},
  title		= {A computational model for ubiquitous intelligent services
		  in indoor agriculture},
  year		= {2019},
  isbn		= {9781450367639},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3323503.3360641},
  doi		= {10.1145/3323503.3360641},
  abstract	= {The application of ubiquitous computing has increased in
		  recent years, especially due to the development of
		  technologies such as mobile computing, accurate sensors and
		  specific protocols for an IoT. One of the trends in this
		  research area is the use of context awareness. In
		  agriculture, the context can be related to the environment,
		  for example, the conditions found inside a greenhouse.
		  Recently a series of studies proposed the use of sensors to
		  monitor the production or the use of cameras to obtain crop
		  information, providing data, reminders and alerts to
		  farmers. This paper proposes a computational model for
		  Indoor Agriculture called IndoorPlant that uses the
		  contexts history analysis to provide intelligent services
		  such as predict the productivity, indicate the problems
		  that the crop may suffer, give suggestions for improvements
		  in the parameters in the greenhouse, among others.
		  IndoorPlant was tested on cucumber prediction using
		  simulated data that was approved by three farmers with more
		  than 10 years of experience each. The results obtained in
		  the prediction of cucumber, with a coefficient of
		  determination (R2) of 0.9912 for root mean square error
		  (RMSE) of 8,06 units of cucumber.},
  booktitle	= {Proceedings of the 25th Brazillian Symposium on Multimedia
		  and the Web},
  pages		= {497–500},
  numpages	= {4},
  keywords	= {prediction in agriculture, indoor agriculture, context
		  awareness, computing in agriculture},
  location	= {Rio de Janeiro, Brazil},
  series	= {WebMedia '19}
}

@InProceedings{	  10.1145/3705391.3705402,
  author	= {Sun, Yuyuan and Li, Tongyan and Chen, Xingyu and Tan,
		  Hao},
  title		= {ConMask-GNN: Leveraging Graph Neural Networks for Enhanced
		  Knowledge Graph Completion in Static Contexts},
  year		= {2025},
  isbn		= {9798400709630},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3705391.3705402},
  doi		= {10.1145/3705391.3705402},
  abstract	= {With the rapid development of knowledge graphs (KG),
		  enhancing the performance of knowledge graph completion has
		  become a critical research challenge. Traditional methods
		  for KG completion rely heavily on entity and relation
		  embeddings, often neglecting the synergistic interaction
		  between graph structures and textual information. To
		  address this issue, this paper proposes a novel approach
		  that combines Graph Neural Networks (GNN) with the ConMask
		  model to improve the effectiveness of static knowledge
		  graph completion tasks. Specifically, the GNN is employed
		  to generate entity embeddings based on graph structures,
		  while ConMask extracts key information from text using a
		  relation-dependent content masking mechanism. The model
		  further integrates the embeddings through a multi-head
		  attention mechanism at the embedding layer, fusing the
		  GNN-based entity embeddings with text embeddings.
		  Additionally, the loss function incorporates contrastive
		  learning, which enhances the representational capacity of
		  the embeddings. Experiments are conducted on standard
		  datasets FB15k-237 and WN18RR, evaluated with metrics such
		  as Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@1,
		  Hits@3, and Hits@10. The results demonstrate that the
		  proposed model significantly outperforms existing baseline
		  methods in prediction accuracy. Finally, ablation studies
		  validate the critical role of combining GNN and ConMask in
		  improving model performance.},
  booktitle	= {Proceedings of the 2024 6th International Conference on
		  Telecommunications and Communication Engineering},
  pages		= {65–69},
  numpages	= {5},
  keywords	= {ConMask, Graph Neural Networks (GNN), Knowledge graph
		  completion, embedding fusion, multi-head attention
		  mechanism},
  location	= { },
  series	= {ICTCE '24}
}

@InProceedings{	  10.1145/3167132.3167271,
  author	= {da Silva, Jo\~{a}o Pablo S. and Ecar, Miguel and Pimenta,
		  Marcelo S. and Kepler, Fabio Natanael and Guedes, Gilleanes
		  T. A. and Betemps, Carlos Michel},
  title		= {Improving self-adaptive systems conceptual modeling},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167271},
  doi		= {10.1145/3167132.3167271},
  abstract	= {Self-adaptive Systems (SaSs) operate under uncertainty
		  conditions and have intrinsic properties that have posed
		  some challenges for requirements analysis. Conceptual
		  modeling is useful to requirements analysis because it aids
		  to understand the situation in which a problem occurs. SaSs
		  conceptual modeling is a non-trivial activity because it is
		  necessary to deal with requirements uncertainty, contextual
		  changes, and behavior adaptation. Since conceptual models
		  are built by humans, their quality heavily depends on the
		  humans expertise, which is not a good software engineering
		  practice. Regarding SaSs, the exposure to quality risks
		  increases because of intrinsic characteristic in this class
		  of system. In this paper, we present a SaSs conceptual
		  modeling approach composed of a metamodel and a modeling
		  process. The process defines how to instantiate the
		  metamodel from requirements specifications to create SaSs
		  conceptual models. We performed a controlled experiment
		  with subjects to evaluate our modeling approach
		  effectiveness. As the outcome, we found that our approach
		  had a better performance than an ad hoc approach. The
		  contribution of this paper is a well-defined approach for
		  guiding SaSs conceptual modeling, supported by evidence of
		  its effectiveness by means of an empirical experiment.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {1292–1299},
  numpages	= {8},
  keywords	= {self-adaptive system, requirements analysis, empirical
		  experiment, conceptual modeling},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InProceedings{	  10.1145/3184558.3186571,
  author	= {Kapugama Geeganage, Dakshi Tharanga},
  title		= {Concept Embedded Topic Modeling Technique},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3186571},
  doi		= {10.1145/3184558.3186571},
  abstract	= {Text contents are overloaded with the digitization of the
		  data and new contents are transmitted through many sources
		  by generating a large volume of information, which spreads
		  all over the world through different communication media.
		  Therefore, text data is available everywhere and reading,
		  understanding and analysing the text data has become a main
		  activity in daily routine. With the increment of the volume
		  and the variety of information, organizing and searching,
		  the required information has become vital. Topic modelling
		  is the state of the art for information organization,
		  understanding and extracting the content. Most of the
		  prevailing topic models use the probabilistic approaches
		  and consider the frequency and the co-occurrence to
		  discover the topics from collections of documents. The
		  proposed research aims to address the existing problems of
		  topic modeling by introducing a concept embedded topic
		  model which generates the most relevant and meaningful
		  topics by understanding the content. The research includes
		  approaches to understand the semantic elements from the
		  content, domain identification of concepts and provide most
		  suitable topics without getting the number of topics from
		  the user beforehand. Capturing the semantics of document
		  collections and generating the most related set of topics
		  according to the actual meaning will be the significance of
		  this research.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {831–835},
  numpages	= {5},
  keywords	= {concepts, semantics, topic modeling},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3375959.3375975,
  author	= {Seok, Hyunseung and Nam, Sunghyun and Lee, Yongju},
  title		= {Implementing A Semantic-based loT Mashup Service},
  year		= {2020},
  isbn		= {9781450372633},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3375959.3375975},
  doi		= {10.1145/3375959.3375975},
  abstract	= {The semantic information provided through the
		  semantic-based IoT system will produce new high value-added
		  products that are completely different from what we have
		  known and experienced. From this point of view, a key issue
		  of current IoT technology and applications is the
		  development of an intelligent IoT platform architecture.
		  Our proposed system collects the IoT data of the sensors
		  from the cloud computer, converts them into RDF, and
		  annotates them with semantics. The converted semantic data
		  are shared and utilized through the ontology repository. We
		  use KT's IoTMakers as a cloud computing environment, and
		  the ontology repository uses Jena's Fuseki server to
		  express SPARQL query results on the Web using Daum Map API
		  and HighCharts API. This gives people the opportunity to
		  access the semantic IoT mash-up service easily and has
		  various application possibilities.},
  booktitle	= {Proceedings of the 2019 2nd Artificial Intelligence and
		  Cloud Computing Conference},
  pages		= {162–167},
  numpages	= {6},
  keywords	= {Semantic-based Mashup Service, Responsive Web Design,
		  Ontology Modeling, IoT},
  location	= {Kobe, Japan},
  series	= {AICCC '19}
}

@InProceedings{	  10.1145/3688574.3688591,
  author	= {Zhu, Jiangtao and Wang, Tiankun and Ma, Xinru and Zuo,
		  Chao and Zhao, Junjie and Luo, Quan},
  title		= {Deep Learning-Based Knowledge Graph Construction for
		  Three-Dimensional Design Specification of Power Plant
		  Engineering},
  year		= {2024},
  isbn		= {9798400717857},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3688574.3688591},
  doi		= {10.1145/3688574.3688591},
  abstract	= {With the rapid development of natural language processing
		  technology, the field of knowledge engineering is gradually
		  advancing towards a novel phase of knowledge expression and
		  orderly data storage. The purpose of this paper is to
		  explore the construction method of knowledge graph of 3D
		  design specification for power plant engineering based on
		  deep learning, and to realize the extraction of key
		  information in the design specification and the automatic
		  construction of knowledge graph by integrating the deep
		  learning models such as BERT, Bi-LSTM, CRF, and so on.},
  booktitle	= {Proceedings of the 2024 6th International Conference on
		  Big Data Engineering},
  pages		= {118–125},
  numpages	= {8},
  keywords	= {Building information modeling, Deep learning, Knowledge
		  graph, Power plant engineering, Three-dimensional design
		  specification},
  location	= {Xining, China},
  series	= {BDE '24}
}

@InProceedings{	  10.1145/3404835.3463113,
  author	= {Nguyen, Hoang-Van and Gelli, Francesco and Poria,
		  Soujanya},
  title		= {DOZEN: Cross-Domain Zero Shot Named Entity Recognition
		  with Knowledge Graph},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3463113},
  doi		= {10.1145/3404835.3463113},
  abstract	= {With the new developments of natural language processing,
		  increasing attention has been given to the task of Named
		  Entity Recognition (NER). However, the vast majority of
		  work focus on a small number of large-scale annotated
		  datasets with a limited number of entities such as person,
		  location and organization. While other datasets have been
		  introduced with domain-specific entities, the smaller size
		  of these largely limits the applicability of
		  state-of-the-art deep models. Even if there are promising
		  new approaches for performing zero-shot learning (ZSL),
		  they are not designed for a cross-domain settings. We
		  propose Cross Domain Zero Shot Named Entity Recognition
		  with Knowledge Graph (DOZEN), which learns the relations
		  between entities across different domains from an existing
		  ontology of external knowledge and a set of analogies
		  linking entities and domains. Experiments performed on both
		  large scale and domain-specific datasets indicate that
		  DOZEN is the most suitable option to extracts unseen
		  entities in a target dataset from a different domain.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1642–1646},
  numpages	= {5},
  keywords	= {cross-domain machine learning, knowledge graph, named
		  entity recognition, natural language processing, zero-shot
		  learning},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3535508.3545550,
  author	= {Wei, Anqi and Wang, Liangjiang},
  title		= {Deep sequence representation learning for predicting human
		  proteins with liquid-liquid phase separation propensity and
		  synaptic functions},
  year		= {2022},
  isbn		= {9781450393867},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3535508.3545550},
  doi		= {10.1145/3535508.3545550},
  abstract	= {With advancements in next-generation sequencing
		  techniques, the whole protein sequence repertoire has
		  increased to a great extent. In the meantime, deep learning
		  techniques have promoted the development of computational
		  methods to interpret large-scale proteomic data and
		  facilitate functional studies of proteins. Inferring
		  properties from protein amino acid sequences has been a
		  long-standing problem in Bioinformatics. Extensive studies
		  have successfully applied natural language processing (NLP)
		  techniques for the representation learning of protein
		  sequences. In this paper, we applied the deep sequence
		  model - UDSMProt, to fine-tune and evaluate two protein
		  prediction tasks: (1) predict proteins with liquid-liquid
		  phase separation propensity and (2) predict synaptic
		  proteins. Our results have shown that, without prior domain
		  knowledge and only based on protein sequences, the
		  fine-tuned language models achieved high classification
		  accuracies and outperformed baseline models using
		  compositional k-mer features in both tasks. Hence, it is
		  promising to apply the protein language model to some
		  learning tasks and the fine-tuned models can be used to
		  predict protein candidates for biological studies.},
  booktitle	= {Proceedings of the 13th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  articleno	= {41},
  numpages	= {8},
  keywords	= {synaptic proteins, protein language model, liquid-liquid
		  phase separation},
  location	= {Northbrook, Illinois},
  series	= {BCB '22}
}

@InProceedings{	  10.1145/3220228.3220263,
  author	= {Hamdy, Abeer and Elsayed, Mohamed},
  title		= {Topic modelling for automatic selection of software design
		  patterns},
  year		= {2018},
  isbn		= {9781450364454},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3220228.3220263},
  doi		= {10.1145/3220228.3220263},
  abstract	= {Design pattern is a high-quality and reusable solution to
		  a recurring software design problem. It is considered an
		  important concept in the software engineering field due to
		  its ability to enhance some of the quality attributes of
		  the software systems including maintainability and
		  extensibility. However, novice developers need to be
		  provided by a tool to assist them in selecting the fit
		  design pattern to solve a design problem. The paper
		  proposes a novel approach for the automatic selection of
		  the fit design pattern. This approach is based on using
		  Latent Dirichlet Allocation (LDA) topic model. The topic is
		  a set of words that often appear together. LDA is able to
		  relate words with similar meaning and to differentiate
		  between uses of words with multiple meanings. In this paper
		  LDA is used to analyze the textual descriptions of design
		  patterns and extract the topics then discover the
		  similarity between the target problem scenario and the
		  collection of patterns using Improved Sqrt-Cosine
		  similarity measure (ISCS). The proposed approach was
		  evaluated using Gang of four design patterns. The
		  experimental results showed that the proposed approach
		  outperforms approach based on the traditional vector space
		  model of Unigrams.},
  booktitle	= {Proceedings of the International Conference on
		  Geoinformatics and Data Analysis},
  pages		= {41–46},
  numpages	= {6},
  keywords	= {topic modelling, text mining, information retrieval, gang
		  of four, design pattern selection, LDA and vector space
		  model, DP recommendation},
  location	= {Prague, Czech Republic},
  series	= {ICGDA '18}
}

@InProceedings{	  10.1145/3386164.3387296,
  author	= {Merkle, Lukas},
  title		= {Cloud-Based Battery Digital Twin Middleware Using
		  Model-Based Development},
  year		= {2020},
  isbn		= {9781450376617},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3386164.3387296},
  doi		= {10.1145/3386164.3387296},
  abstract	= {Following the trends of electrification, the energy
		  storage of vehicles is gaining importance as the most
		  expensive part of an electric car. Since lithium-ion
		  batteries are perishable goods and underlie e. g. aging
		  effects, environmental and operating conditions during
		  manufacturing and car usage need close supervision. With
		  regard to the paradigm of digital twins, data from various
		  life cycle phases needs to be collected and processed to
		  improve the general quality of the system. To achieve this
		  complex task, a suitable framework is needed in order to
		  operate the fleet of digital twins during manufacturing
		  processes, the automotive usage and a potential second
		  life. Based on a literature review, we formulate
		  requirements for a digital twin framework in the field of
		  battery systems. We propose a framework to develop and
		  operate a fleet of digital twins during all life cycle
		  phases. Results feature a case study in which we implement
		  the stated framework in a cloud-computing environment using
		  early stages of battery system production as test a bed.
		  With the help of a self-discharge model of li-ion cells,
		  the system can estimate the SOC of battery modules and
		  provide this information to the arrival testing
		  procedures.},
  booktitle	= {Proceedings of the 2019 3rd International Symposium on
		  Computer Science and Intelligent Control},
  articleno	= {59},
  numpages	= {7},
  keywords	= {Self-Discharge, IoT, Digital Twin, Control Middleware,
		  Battery System},
  location	= {Amsterdam, Netherlands},
  series	= {ISCSIC 2019}
}

@Article{	  10.1145/3597455,
  author	= {Ding, Ling and Chen, Xiaojun and Wei, Jian and Xiang,
		  Yang},
  title		= {MABERT: Mask-Attention-Based BERT for Chinese Event
		  Extraction},
  year		= {2023},
  issue_date	= {July 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {7},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3597455},
  doi		= {10.1145/3597455},
  abstract	= {Event extraction is an essential but challenging task in
		  information extraction. This task has considerably
		  benefited from pre-trained language models, such as BERT.
		  However, when it comes to the trigger-word mismatch problem
		  in languages without natural delimiters, existing methods
		  ignore the complement of lexical information to BERT. In
		  addition, the inherent multi-role noise problem could limit
		  the performance of methods when one sentence contains
		  multiple events. In this article, we propose a
		  Mask-Attention-based BERT (MABERT) framework for Chinese
		  event extraction to address the above problems. Firstly, in
		  order to avoid trigger-word mismatch and integrate lexical
		  features into BERT layers directly, a mask-attention-based
		  transformer augmented with two mask matrices is devised to
		  replace the original one in BERT. By the
		  mask-attention-based transformer, the character sequence
		  interacts with external lexical semantics sufficiently and
		  keeps its structure information at the same time. Moreover,
		  against the multi-role noise problem, we make use of event
		  type information from representation and classification,
		  two aspects to enrich entity features, where type markers
		  and event-schema-based mask matrix are proposed.
		  Experimental results on the widely used ACE2005 dataset
		  show the effectiveness of our proposed MABERT on Chinese
		  event extraction task compared with other state-of-the-art
		  methods.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  articleno	= {192},
  numpages	= {21},
  keywords	= {event ontology, event type markers, mask-attention-based
		  transformer, Event extraction}
}

@InProceedings{	  10.1145/3324884.3416668,
  author	= {Nguyen, Hoang Lam and Nassar, Nebras and Kehrer, Timo and
		  Grunske, Lars},
  title		= {MoFuzz: a fuzzer suite for testing model-driven software
		  engineering tools},
  year		= {2021},
  isbn		= {9781450367684},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3324884.3416668},
  doi		= {10.1145/3324884.3416668},
  abstract	= {Fuzzing or fuzz testing is an established technique that
		  aims to discover unexpected program behavior (e.g., bugs,
		  security vulnerabilities, or crashes) by feeding
		  automatically generated data into a program under test.
		  However, the application of fuzzing to test Model-Driven
		  Software Engineering (MDSE) tools is still limited because
		  of the difficulty of existing fuzzers to provide
		  structured, well-typed inputs, namely models that conform
		  to typing and consistency constraints induced by a given
		  meta-model and underlying modeling framework. By drawing
		  from recent advances on both fuzz testing and automated
		  model generation, we present three different approaches for
		  fuzzing MDSE tools: A graph grammar-based fuzzer and two
		  variants of a coverage-guided mutation-based fuzzer working
		  with different sets of model mutation operators. Our
		  evaluation on a set of real-world MDSE tools shows that our
		  approaches can outperform both standard fuzzers and model
		  generators w.r.t. their fuzzing capabilities. Moreover, we
		  found that each of our approaches comes with its own
		  strengths and weaknesses in terms of fault finding
		  capabilities and the ability to cover different aspects of
		  the system under test. Thus the approaches complement each
		  other, forming a fuzzer suite for testing MDSE tools.},
  booktitle	= {Proceedings of the 35th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {1103–1115},
  numpages	= {13},
  keywords	= {automated model generation, eclipse modeling framework,
		  fuzzing, model-driven software engineering, modeling
		  tools},
  location	= {Virtual Event, Australia},
  series	= {ASE '20}
}

@InProceedings{	  10.1145/3583780.3615992,
  author	= {Miller, Michael},
  title		= {Astrolabe: Visual Graph Database Queries with Tabular
		  Output},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615992},
  doi		= {10.1145/3583780.3615992},
  abstract	= {Graph databases are an established solution for large,
		  highly connected datasets. One challenge associated with
		  deploying graph databases in industrial settings is
		  usability. Typically, developers interact with graph
		  databases through queries in languages such as Cypher or
		  GraphQL. Many end-users, analysts, and administrators are
		  not familiar with these specialized languages.
		  Additionally, these queries return hierarchical data in
		  formats such as JSON (JavaScript Object Notation) or XML
		  (Extensible Markup Language). Additional scripts and
		  interfaces are needed to convert hierarchical data into
		  more easily digested tables. To overcome these challenges,
		  each graph database use-case typically involves significant
		  custom software to explore, view, and export data.We
		  introduce Astrolabe, a generalized interface that addresses
		  the challenges of querying graph databases. In Astrolabe,
		  queries are constructed visually, so users do not need to
		  learn new graph query languages. Results are returned as
		  tables, which can be easily digested by end users or
		  down-stream applications. Astrolabe was designed to
		  function with arbitrary graph databases, so schema
		  definition is not required. Astrolabe revolutionizes graph
		  exploration and querying by allowing graph databases to be
		  viewed as tables, without the need for custom software
		  adapters.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5248},
  numpages	= {1},
  keywords	= {query generation, no-code, knowledge representation,
		  knowledge graph, interactive information retrieval,
		  graphical user interface, graph database, data
		  visualization, data exploration},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Article{	  10.1145/3125621,
  author	= {Ferry, Nicolas and Chauvel, Franck and Song, Hui and
		  Rossini, Alessandro and Lushpenko, Maksym and Solberg,
		  Arnor},
  title		= {CloudMF: Model-Driven Management of Multi-Cloud
		  Applications},
  year		= {2018},
  issue_date	= {May 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {2},
  issn		= {1533-5399},
  url		= {https://doi.org/10.1145/3125621},
  doi		= {10.1145/3125621},
  abstract	= {While the number of cloud solutions is continuously
		  increasing, the development and operation of large-scale
		  and distributed cloud applications are still challenging. A
		  major challenge is the lack of interoperability between the
		  existing cloud solutions, which increases the complexity of
		  maintaining and evolving complex applications potentially
		  deployed across multiple cloud infrastructures and
		  platforms. In this article, we show how the Cloud Modelling
		  Framework leverages model-driven engineering and supports
		  the DevOps ideas to tame this complexity by providing: (i)
		  a domain-specific language for specifying the provisioning
		  and deployment of multi-cloud applications, and (ii) a
		  models@run-time environment for their continuous
		  provisioning, deployment, and adaptation.},
  journal	= {ACM Trans. Internet Technol.},
  month		= jan,
  articleno	= {16},
  numpages	= {24},
  keywords	= {multi-cloud, models@run-time, model-driven engineering,
		  DevOps, Cloud computing}
}

@Article{	  10.1145/3624557,
  author	= {Duong, Huong T. and Ho, Van H. and Do, Phuc},
  title		= {Fact-checking Vietnamese Information Using Knowledge
		  Graph, Datalog, and KG-BERT},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {10},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3624557},
  doi		= {10.1145/3624557},
  abstract	= {In the era of digital information, ensuring the accuracy
		  and reliability of information is crucial, making
		  fact-checking a vital process. Currently, English
		  fact-checking has thrived due to various language
		  processing tools and ample datasets. However, the same
		  cannot be said for Vietnamese fact-checking, which faces
		  significant challenges due to the lack of such resources.
		  To address these challenges, we propose a model for
		  checking Vietnamese facts by synthesizing three popular
		  technologies: Knowledge Graph (KG), Datalog, and KG-BERT.
		  The KG serves as the foundation for the fact-checking
		  process, containing a dataset of Vietnamese information.
		  Datalog, a logical programming language, is used with
		  inference rules to complete the knowledge within the
		  Vietnamese KG. KG-BERT, a Deep Learning (DL) model, is then
		  trained on this KG to rapidly and accurately classify
		  information that needs fact-checking. Furthermore, to put
		  Vietnamese complex sentences into the fact-checking model,
		  we present a solution for extracting triples from these
		  sentences. This approach also contributes significantly to
		  the ease of constructing foundational datasets for the
		  Vietnamese KG. To evaluate the model's performance, we
		  create a Vietnamese dataset comprising 130,190 samples to
		  populate the KG. Using Datalog, we enrich this graph with
		  additional knowledge. The KG is then utilized to train the
		  KG-BERT model, achieving an impressive accuracy of 95\%.
		  Our proposed solution shows great promise for fact-checking
		  Vietnamese information and has the potential to contribute
		  to the development of fact-checking tools and techniques
		  for other languages. Overall, this research makes a
		  significant contribution to the field of data science by
		  providing an accurate solution for fact-checking
		  information in Vietnamese language contexts.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  articleno	= {240},
  numpages	= {23},
  keywords	= {datalog, inference rule, KG-BERT, knowledge graph, Fact
		  checking}
}

@InProceedings{	  10.1145/3706598.3713711,
  author	= {Wu, Y. Kelly and Sohrawardi, Saniat Javid and Gerstner,
		  Candice R. and Wright, Matthew},
  title		= {Understanding and Empowering Intelligence Analysts:
		  User-Centered Design for Deepfake Detection Tools},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713711},
  doi		= {10.1145/3706598.3713711},
  abstract	= {Intelligence analysts must quickly and accurately examine
		  and report on information in multiple modalities, including
		  video, audio, and images. With the rise of Generative AI
		  and deepfakes, analysts face unprecedented challenges, and
		  require effective, reliable, and explainable media
		  detection and analysis tools. This work explores
		  analysts’ requirements for deepfake detection tools and
		  explainability features. From a study of 30 practitioners
		  from the United States Intelligence Community, we
		  identified the need for a comprehensive and explainable
		  solution that incorporates a wide variety of methods and
		  supports the production of intelligence reports. In
		  response, we propose a design for an analyst-centered tool,
		  and introduce a digital media forensics ontology to support
		  analysts’ interactions with the tool and understanding of
		  its results. We conducted a study grounded in work-related
		  tasks as an initial evaluation of this approach, and report
		  on its potential to assist analysts and areas for
		  improvement in future work.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {870},
  numpages	= {26},
  keywords	= {Deepfake, Intelligence Community, Qualitative Studies,
		  Ontology, Explainability},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3701716.3715308,
  author	= {Sun, Qiang and Li, Sirui and Huynh, Du and Reynolds, Mark
		  and Liu, Wei},
  title		= {TimelineKGQA: A Comprehensive Question-Answer Pair
		  Generator for Temporal Knowledge Graphs},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715308},
  doi		= {10.1145/3701716.3715308},
  abstract	= {Question answering over temporal knowledge graphs (TKGs)
		  is crucial for understanding evolving facts and
		  relationships, yet its development is hindered by limited
		  datasets and difficulties in generating custom QA pairs. We
		  propose a novel categorization framework based on
		  timeline-context relationships, along with TimelineKGQA, a
		  universal temporal QA generator applicable to any TKGs. The
		  code is available at:
		  https://github.com/PascalSun/TimelineKGQA as an open source
		  Python package.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {797–800},
  numpages	= {4},
  keywords	= {knowledge graph, question answering, temporal knowledge
		  graph},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3589132.3625629,
  author	= {Wang, Zhaonan and Jin, Bowen and Hu, Wei and Jiang, Minhao
		  and Kang, Seungyeon and Li, Zhiyuan and Zhou, Sizhe and
		  Han, Jiawei and Wang, Shaowen},
  title		= {Geospatial Knowledge Hypercube},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589132.3625629},
  doi		= {10.1145/3589132.3625629},
  abstract	= {Today a tremendous amount of geospatial knowledge is
		  hidden in massive volumes of text data. To facilitate
		  flexible and powerful geospatial analysis and applications,
		  we introduce a new architecture: geospatial knowledge
		  hypercube, a multi-scale, multidimensional knowledge
		  structure that integrates information from geospatial
		  dimensions, thematic themes and diverse application
		  semantics, extracted and computed from spatial-related text
		  data. To construct such a knowledge hypercube, weakly
		  supervised language models are leveraged for automatic,
		  dynamic and incremental extraction of heterogeneous
		  geospatial data, thematic themes, latent connections and
		  relationships, and application semantics, through combining
		  a variety of information from unstructured text, structured
		  tables, and maps. The hypercube lays a foundation for many
		  knowledge discovery and in-depth spatial analysis, and
		  other advanced applications. We have deployed a prototype
		  web application of proposed geospatial knowledge hypercube
		  for public access at:
		  https://hcwebapp.cigi.illinois.edu/.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {79},
  numpages	= {4},
  keywords	= {weakly-supervised text classification, geographic
		  information retrieval, knowledge hypercube},
  location	= {Hamburg, Germany},
  series	= {SIGSPATIAL '23}
}

@Article{	  10.1145/3757064,
  author	= {Guzm\'{a}n, Ana Rosa and Karunaratne, Thashmee},
  title		= {A Framework for Efficient Semantic Elicitation of EU-Wide
		  Evidence for Public Services},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3757064},
  doi		= {10.1145/3757064},
  abstract	= {Digital implementation of the Once-Only Principle (OOP)
		  reduces the administrative burden of accessing public
		  services and enhances public administration performance. It
		  also facilitates the mobility of citizens and businesses
		  within the European Union through legal and effective
		  EU-wide evidence. However, despite the existence of several
		  semantic standards, a lack of an ontology for EU-wide
		  evidence persists, primarily due to the low level of EU
		  harmonisation of information that may prove compliance with
		  procedural requirements. Building EU-wide evidence involves
		  several cross-border communities of practice, composed of
		  experts with different technical, legal, organisational,
		  semantic, idiomatic and cultural backgrounds, spanning
		  various public administration levels and sectors. This
		  diversity introduces a range of cross-border,
		  socio-technical challenges that have not been directly
		  addressed in existing literature or by European projects
		  and initiatives. Based on the lessons learned from the
		  Digital Europe for All (DE4A) large-scale pilot project,
		  the DE4A EU-wide OOP Semantic Elicitation Framework
		  (DEOSEF) is proposed to guide business and semantic experts
		  in the elicitation of EU-wide evidence for target public
		  services, through a collaborative, context-aware, and
		  iterative agile process involving diverse communities of
		  practice. This paper presents DEOSEF as a foundational step
		  for future initiatives involving a combined semantic
		  elicitation-creation of EU-wide evidence.},
  note		= {Just Accepted},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= jul,
  keywords	= {Once-Only principle, Digital public services, Cross-border
		  interoperability}
}

@InProceedings{	  10.1145/3487553.3524704,
  author	= {Xue, Xingsi and Guo, Jianhua},
  title		= {Word Embedding based Heterogeneous Entity Matching on Web
		  of Things},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524704},
  doi		= {10.1145/3487553.3524704},
  abstract	= {Web of Things (WoT) is capable of promoting the knowledge
		  discovery and address interoperability problems of diverse
		  Internet of Things (IoT) applications. However, due to the
		  dynamic and diverse features of data entities on WoT, the
		  heterogeneous entity matching has become arguably the
		  greatest “new frontier” for WoT advancements.
		  Currently, the data entities and the corresponding
		  knowledge on WoT are generally modelled with the ontology,
		  and therefore, matching heterogeneous data entities on WoT
		  can be converted to the problem of matching ontologies.
		  Ontology matching is a complex cognitive process, it is
		  usually initially done manually by domain experts. To
		  effectively distinguish the heterogeneous entities and
		  determine high-quality ontology alignment, this work
		  proposes a word embedding based matching technique. Our
		  approach models the word’s semantic in the vector space,
		  and use two vectors’ cosine angle to measure the
		  corresponding words’ similarity. In addition, the word
		  embedding approach does not depend on a specific knowledge
		  base and retain the rich semantic information of words,
		  which makes our proposal more robust. The experiment uses
		  Ontology Alignment Evaluation Initiative (OAEI)’s
		  benchmark for testing, and the experimental results show
		  that our approach outperforms other advanced matching
		  methods.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {941–947},
  numpages	= {7},
  keywords	= {Word Embedding, Web of Things, Ontology Matching},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3330089.3330101,
  author	= {Saba, Djamel and Maouedj, Rachid and Berbaoui, Brahim},
  title		= {Contribution to the development of an energy management
		  solution in a green smart home (EMSGSH)},
  year		= {2018},
  isbn		= {9781450361019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3330089.3330101},
  doi		= {10.1145/3330089.3330101},
  abstract	= {This document offers a smart solution for managing green
		  energy for a home (EMSGSH). It offers services such as
		  optimizing the energy consumed while assuming green energy
		  available at home. However, the random nature of this type
		  of energy in the power generation, we require using a
		  hybrid energy system (HES) accompanied by energy storage in
		  the form of a battery. The home is considered a distributed
		  system, it is composed of a set of elements that are
		  geographically distributed and in permanent interaction.
		  Following all its features, we chose two approaches for
		  this solution, the multi-agent systems (MAS) for the
		  control of the elements and the domain ontology to ensure a
		  good formal representation of the data associated with the
		  system. We propose a master-slave architecture. A slave
		  agent is proposed for each device to control its local
		  consumption and a single master agent is proposed to
		  control all agents. The objective is to optimize the use of
		  green energy and minimize consumption costs by exploiting
		  the offers of electric power suppliers. The last part of
		  this work was reserved to present the agents (tasks and
		  responsibilities, interactions ...) and to model the
		  agent's society using a unified modeling language (UML),
		  also the ontology elements are presented and edited in the
		  Prot\'{e}g\'{e} software.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Software Engineering and New Technologies},
  articleno	= {5},
  numpages	= {7},
  keywords	= {Unified modeling language, Smart home, Prot\'{e}g\'{e}
		  software, Ontology, Multi agent systems, Hybrid energy
		  systems, Green energy, Energy efficient, Distributed
		  systems, Decision-making},
  location	= {Hammamet, Tunisia},
  series	= {ICSENT 2018}
}

@Article{	  10.1145/3603254,
  author	= {Romberg, Julia and Escher, Tobias},
  title		= {Making Sense of Citizens’ Input through Artificial
		  Intelligence: A Review of Methods for Computational Text
		  Analysis to Support the Evaluation of Contributions in
		  Public Participation},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {1},
  url		= {https://doi.org/10.1145/3603254},
  doi		= {10.1145/3603254},
  abstract	= {Public sector institutions that consult citizens to inform
		  decision-making face the challenge of evaluating the
		  contributions made by citizens. This evaluation has
		  important democratic implications but at the same time,
		  consumes substantial human resources. However, until now
		  the use of artificial intelligence such as
		  computer-supported text analysis has remained an
		  under-studied solution to this problem. We identify three
		  generic tasks in the evaluation process that could benefit
		  from natural language processing (NLP). Based on a
		  systematic literature search in two databases on
		  computational linguistics and digital government, we
		  provide a detailed review of existing methods and their
		  performance. While some promising approaches exist, for
		  instance to group data thematically and to detect arguments
		  and opinions, we show that there remain important
		  challenges before these could offer any reliable support in
		  practice. These include the quality of results, the
		  applicability to non-English language corpuses and making
		  algorithmic models available to practitioners through
		  software. We discuss a number of avenues that future
		  research should pursue that can ultimately lead to
		  solutions for practice. The most promising of these bring
		  in the expertise of human evaluators, for example through
		  active learning approaches or interactive topic modeling.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= mar,
  articleno	= {3},
  numpages	= {30},
  keywords	= {Policy analytics, citizen participation, computational
		  linguistics}
}

@InProceedings{	  10.1145/3687311.3687339,
  author	= {Yang, Xin and Zhao, Fengjuan},
  title		= {Integrating AI with Pedagogies: Drama, Multimodal and the
		  Production-oriented Approach- a Study Based on the 6th
		  SFLEP Intercultural Competence Contest},
  year		= {2024},
  isbn		= {9798400709920},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3687311.3687339},
  doi		= {10.1145/3687311.3687339},
  abstract	= {Cultural studies have garnered significant attention in
		  China for many decades, and the cultivation of
		  intercultural competence within the academic sphere has
		  been meticulously developed, particularly within the
		  university context. Although intercultural competence is
		  inherently multidisciplinary, its cultivation is
		  predominantly integrated into the pedagogy of foreign
		  language instruction. The discourse surrounding
		  intercultural communication is mainly led by educators and
		  scholars in the field of foreign language studies. The
		  SFLEP Intercultural Competence Contest comprises three
		  pivotal tasks: the development of intercultural case
		  studies, scenario analysis, and the narration of Chinese
		  stories. The rapid development of AI has opened new avenues
		  in the field of education, particularly in language
		  learning. The integration of AI with traditional pedagogies
		  like drama, multimodal learning, and the
		  production-oriented approach has been observed to enrich
		  the learning experience and improve intercultural
		  competence. A thorough examination of the 6th iteration of
		  the contest provides the foundation for this exploration.
		  The study not only highlights the potential of AI
		  integrated approaches but also underscores their
		  significant relevance in enhancing scaffolding techniques
		  in foreign language education.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Intelligent Education and Computer Technology},
  pages		= {151–157},
  numpages	= {7},
  location	= {Guilin, China},
  series	= {IECT '24}
}

@Article{	  10.1145/3760786,
  author	= {Jian, Yue and Zhang, Miao and Qin, Ziyue and Xie, Chuyuan
		  and Xiao, Kui and Zhang, Yan and Li, Zhifei},
  title		= {Adaptive Modality Interaction Transformer for Multimodal
		  Knowledge Graph Completion},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3760786},
  doi		= {10.1145/3760786},
  abstract	= {Knowledge graphs (KGs) are frequently confronted with the
		  challenge of incompleteness, a problem that extends to
		  multimodal knowledge graphs (MKGs). The primary goal of
		  multimodal knowledge graph completion (MKGC) is to predict
		  missing entities within MKGs. However, current MKGC methods
		  face difficulties in adequately addressing modal
		  preferences and imbalances in modal information. To
		  overcome these issues, we introduce AdaMKGC, an innovative
		  hybrid model incorporating an adaptive modality interaction
		  transformer. This model employs a dynamic attention
		  interaction strategy and a self-enhancing sampling
		  approach. AdaMKGC achieves a more precise utilization of
		  multimodal information by integrating modal preference
		  information into modal interactions. Additionally, it
		  effectively mitigates the issue of modal imbalance through
		  targeted sampling and adjustment for entities with
		  deficient information. Experimental evaluations demonstrate
		  AdaMKGC's superior performance in overcoming these
		  prevalent challenges. Compared to existing state-of-the-art
		  MKGC models, AdaMKGC shows a notable enhancement of 28\% in
		  MR on the WN18-IMG dataset and an improvement of 2.7\% in
		  Hits@1 on the FB15k-237-IMG dataset. Our code is available
		  at .},
  note		= {Just Accepted},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= aug,
  keywords	= {Multimodal Knowledge Graphs, Knowledge Graph Completion,
		  Link Prediction}
}

@InBook{	  10.1145/3677389.3702495,
  author	= {Mu, Wenchuan and Liu, Junhua and Lim, Kwan Hui},
  title		= {Fast Bibliography Pre-Selection via Two-Vector Semantic
		  Representations},
  year		= {2025},
  isbn		= {9798400710933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677389.3702495},
  abstract	= {In academic writing, bibliography compilations is
		  essential but time-consuming, often requiring repeated
		  searches for references. Hence, an efficient tool for
		  faster bibliography compilation is needed. Our work offers
		  a solution to the challenges of managing large-scale
		  bibliographic databases, introducing a new algorithm that
		  improves both efficiency and sensitivity. Using two-vector
		  semantic modelling, bibliographic entries and queries are
		  embedded into the same vector space to select relevant
		  references based on semantic similarity. Experimental
		  results with 3.37 million entries show the method reduces
		  the time needed to generate a manageable subset,
		  streamlining scholarly writing. Our code and dataset are
		  publicly available at
		  https://github.com/cestwc/bibliography-pre-selection.},
  booktitle	= {Proceedings of the 24th ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {23},
  numpages	= {6}
}

@InProceedings{	  10.1145/3638884.3638979,
  author	= {Wu, Yu and Miao, Lin and Li, Han},
  title		= {Attribute Value Extraction in Weapon Domain Based on
		  Bi-LSTM and Attention},
  year		= {2024},
  isbn		= {9798400708909},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638884.3638979},
  doi		= {10.1145/3638884.3638979},
  abstract	= {Aiming at the problem that the traditional extraction
		  method caused by the diversification of weapon attributes
		  has a large amount of work to construct the label of weapon
		  attributes, in this paper, we propose a weapon attribute
		  value extraction method based on bidirectional long-term
		  and short-term memory network (Bi-LSTM) and attention
		  mechanism. The method first uses the Bi-LSTM model to
		  extract the features of the input text and attribute names.
		  Then, the attention mechanism focuses on the relations
		  between words and attributes in the sentence. Afterward,
		  the global BIO tag marks the position of the attribute
		  values in the sentence. In this way, the method can reduce
		  the workload during the corpus preparation period to
		  improve the generalization ability of the model so that it
		  can extract different weapon attribute data. Compared with
		  Bi-LSTM, Bi-LSTM_CRF, and OpenTag from the experimental
		  results, the F1 values of the proposed model on the weapon
		  domain attribute dataset are increased by about 6.9\%,
		  5.7\%, and 2.5\%, respectively.},
  booktitle	= {Proceedings of the 2023 9th International Conference on
		  Communication and Information Processing},
  pages		= {603–610},
  numpages	= {8},
  keywords	= {Attribute Value Extraction, Information Extraction,
		  Knowledge Base, Natural Language Processing},
  location	= {Lingshui, China},
  series	= {ICCIP '23}
}

@Article{	  10.1145/3609336,
  author	= {Hamed, Naeima and Gaglione, Andrea and Gluhak, Alex and
		  Rana, Omer and Perera, Charith},
  title		= {Query Interface for Smart City Internet of Things Data
		  Marketplaces: A Case Study},
  year		= {2023},
  issue_date	= {August 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {3},
  url		= {https://doi.org/10.1145/3609336},
  doi		= {10.1145/3609336},
  abstract	= {Cities are increasingly becoming augmented with sensors
		  through public, private, and academic sector initiatives.
		  Most of the time, these sensors are deployed with a primary
		  purpose (objective) in mind (e.g., deploy sensors to
		  understand noise pollution) by a sensor owner (i.e., the
		  organization that invests in sensing hardware, e.g., a city
		  council). Over the past few years, communities undertaking
		  smart city development projects have understood the
		  importance of making the sensor data available to a wider
		  community—beyond their primary usage. Different business
		  models have been proposed to achieve this, including
		  creating data marketplaces. The vision is to encourage new
		  startups and small and medium-scale businesses to create
		  novel products and services using sensor data to generate
		  additional economic value. Currently, data are sold as
		  pre-defined independent datasets (e.g., noise level and
		  parking status data may be sold separately). This approach
		  creates several challenges, such as (i) difficulties in
		  pricing, which leads to higher prices (per dataset); (ii)
		  higher network communication and bandwidth requirements;
		  and (iii) information overload for data consumers (i.e.,
		  those who purchase data). We investigate the benefit of
		  semantic representation and its reasoning capabilities
		  toward creating a business model that offers data on demand
		  within smart city Internet of Things data marketplaces. The
		  objective is to help data consumers (i.e., small and medium
		  enterprises) acquire the most relevant data they need. We
		  demonstrate the utility of our approach by integrating it
		  into a real-world IoT data marketplace (developed by the
		  synchronicity-iot.eu project). We discuss design decisions
		  and their consequences (i.e., tradeoffs) on the choice and
		  selection of datasets. Subsequently, we present a series of
		  data modeling principles and recommendations for
		  implementing IoT data marketplaces.},
  journal	= {ACM Trans. Internet Things},
  month		= sep,
  articleno	= {19},
  numpages	= {39},
  keywords	= {knowledge management, linked data, multi-dimensional
		  querying, data discovery, semantic interoperability,
		  Internet of Things}
}

@Article{	  10.1145/3469722,
  author	= {Kulkarni, Dhanashree S. and Rodd, Sunil S.},
  title		= {Sentiment Analysis in Hindi—A Survey on the
		  State-of-the-art Techniques},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3469722},
  doi		= {10.1145/3469722},
  abstract	= {Sentiment Analysis (SA) has been a core interest in the
		  field of text mining research, dealing with computational
		  processing of sentiments, views, and subjective nature of
		  the text. Due to the availability of extensive web-based
		  data in Indian languages such as Hindi, Marathi, Kannada,
		  Tamil, and so on. It has become extremely significant to
		  analyze this data and recover valuable and relevant
		  information. Hindi being the first language of the majority
		  of the population in India, SA in Hindi has turned out to
		  be a critical task particularly for companies and
		  government organizations. This research portrays a
		  systematic review specifically in the field of Hindi SA.
		  The major contribution of this article includes the
		  categorization of numerous articles based on techniques
		  that have attracted researchers in performing SA tasks in
		  Hindi language. This survey classifies these
		  state-of-the-art computational intelligence techniques into
		  four major categories namely lexicon-based techniques,
		  machine learning techniques, deep learning techniques, and
		  hybrid techniques. It discusses the importance of these
		  techniques based on different aspects such as their impact
		  on the issues of SA, levels of analysis, and performance
		  evaluation measures. The research puts forward a
		  comprehensive overview of the majority of the work done in
		  Hindi SA. This study will help researchers in finding out
		  resources such as annotated datasets, linguistic resources,
		  and lexical resources. This survey delivers some
		  significant findings and presents overall future research
		  directions in the field of Hindi SA.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {21},
  numpages	= {46},
  keywords	= {systematic review, lexicon technique, opinion mining,
		  hindi language, Sentiment analysis}
}

@InProceedings{	  10.1145/3341105.3373974,
  author	= {Cornejo Lupa, Maria A. and Ticona-Herrera, Regina P. and
		  Cardinale, Yudith and Barrios-Aranibar, Dennis},
  title		= {A categorization of simultaneous localization and mapping
		  knowledge for mobile robots},
  year		= {2020},
  isbn		= {9781450368667},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341105.3373974},
  doi		= {10.1145/3341105.3373974},
  abstract	= {Autonomous robots are playing important roles in academic,
		  technological, and scientific activities. Thus, their
		  behavior is getting more complex. The main tasks of
		  autonomous robots include mapping an environment and
		  localize themselves. These tasks comprise the Simultaneous
		  Localization and Mapping (SLAM) problem. Representation of
		  the SLAM knowledge (e.g., robot characteristics,
		  environment information, mapping and location information),
		  with a standard and well-defined model, provides the base
		  to develop efficient and interoperable solutions. However,
		  as far as we know, there is not a common classification of
		  such knowledge. Many existing works based on Semantic Web,
		  have formulated ontologies to model information related to
		  only some SLAM aspects, without a standard arrangement. In
		  this paper, we propose a categorization of the knowledge
		  managed in SLAM, based on existing ontologies and SLAM
		  principles. We also classify recent and popular ontologies
		  according to our proposed categories and highlight the
		  lessons to learn from existing solutions.},
  booktitle	= {Proceedings of the 35th Annual ACM Symposium on Applied
		  Computing},
  pages		= {956–963},
  numpages	= {8},
  keywords	= {semantic web, semantic robots, ontologies, mobile robots,
		  SLAM},
  location	= {Brno, Czech Republic},
  series	= {SAC '20}
}

@Article{	  10.14778/3704965.3704970,
  author	= {Bellomarini, Luigi and Benedetto, Davide and Brandetti,
		  Matteo and Sallinger, Emanuel and Vlad, Adriano},
  title		= {The Vadalog Parallel System: Distributed Reasoning with
		  Datalog+/-},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {13},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3704965.3704970},
  doi		= {10.14778/3704965.3704970},
  abstract	= {Over the past years, there has been a growing demand for
		  ontological reasoning systems based on languages of the
		  Datalog+/- family, such as Vadalog, for their ability to
		  effectively model a wide range of real-world problems with
		  powerful features such as existential quantification. As
		  the scale and complexity of data analysis tasks continue to
		  grow, the ability to distribute the computational workload
		  across multiple non-communicating processors has become
		  vital for these systems to achieve scalable performance.The
		  joint presence of existential quantification and recursion
		  poses new challenges, currently unsolved by existing
		  distributed systems, which only concentrate on Datalog and
		  are therefore unsuitable for ontological reasoning. When
		  working across multiple processors, generating all the
		  facts to answer a specific reasoning query, avoiding
		  duplication, and guaranteeing termination are non-trivial
		  tasks as infinitely many new symbols and facts can be
		  generated by existential quantification and recursion.In
		  this paper, we address such challenges and introduce the
		  first distributed framework in the Datalog+/- space. We
		  propose the condition of homomorphic decomposability, which
		  identifies sets of Datalog+/- rules with good distribution
		  properties. We put homomorphic decomposability into action
		  with a distributed reasoning algorithm for Warded
		  Datalog+/-, the core of Vadalog. We implement Vadalog
		  Parallel, a distributed reasoner for Vadalog and provide
		  experimental evaluation against state-of-the-art systems.},
  journal	= {Proc. VLDB Endow.},
  month		= sep,
  pages		= {4614–4626},
  numpages	= {13}
}

@InProceedings{	  10.1145/3447568.3448541,
  author	= {Capodieci, Antonio and Mainetti, Luca and Dipietrangelo,
		  Flavio},
  title		= {Model-Driven approach to Cyber Risk Analysis in Industry
		  4.0},
  year		= {2021},
  isbn		= {9781450376556},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447568.3448541},
  doi		= {10.1145/3447568.3448541},
  abstract	= {In the contest of industrial process and automation, and
		  in particular in the so-called Industry 4.0, the now
		  intensive application of control systems in interconnected
		  networks has led to an increase in unexpected threats to
		  information security for supervisory control and data
		  acquisition (SCADA) and control systems distributed
		  (DCS).Risk assessment is essential and the its common
		  methods such as HHM, IIM, and RFRM have been successfully
		  applied to SCADA systems.Another equally important need is
		  the use of metrics and methodologies to analyze the risk
		  (PRA- probability risk analysis), which includes methods
		  such as FTA, ETA and FEMA and HAZOP. The goal of these
		  methods is, in general, to determine the impact of a
		  problem on the process plant and the risk reduction
		  associated with a particular countermeasure.In this paper
		  we present a methodology named CRiSP (Cyber Risk Analysis
		  in Industrial Process System Environment). CRiSP defines an
		  approach to analyze the risk related to the manipulation of
		  a single element of the plant and to analyze the
		  consequence to entire plant and in the same time to a
		  restricted portion.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Information Systems and Technologies},
  articleno	= {33},
  numpages	= {7},
  keywords	= {Risk management, Risk Analysis, Industry 4.0,
		  Cybersecurity},
  location	= {Lecce, Italy},
  series	= {ICIST '20}
}

@InProceedings{	  10.1145/3543873.3587540,
  author	= {Naik, Riya},
  title		= {Multi-turn mediated solutions for Conversational
		  Artificial Intelligent systems leveraging graph-based
		  techniques},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587540},
  doi		= {10.1145/3543873.3587540},
  abstract	= {The current era is dominated by intelligent Question
		  Answering (QA) systems that can instantly answer almost all
		  their questions, saving users search time and increasing
		  the throughput and precision in the applied domain. A vast
		  amount of work is being carried out in QA systems to
		  deliver better content satisfying users’ information
		  needs [2]. Since QA systems are ascending the cycle of
		  emerging technologies, there are potential research gaps
		  that can be explored. QA systems form a significant part of
		  Conversational Artificial Intelligent systems giving rise
		  to a new research pathway, i.e., Conversational Question
		  Answering (CQA) systems [32]. We propose to design and
		  develop a CQA system leveraging Hypergraph-based
		  techniques. The approach focuses on the multi-turn
		  conversation and multi-context to gauge users’ exact
		  information needs and deliver better answers. We further
		  aim to address "supporting evidence-based retrieval" for
		  fact-based responsible answer generation. Since the QA
		  system requires a large amount of data and processing, we
		  also intend to investigate hardware performance for
		  effective system utilization.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {586–590},
  numpages	= {5},
  keywords	= {Contextual Embeddings, Conversational Artificial
		  Intelligence, Evidence-based retrieval, Graph-based models,
		  Question Answering},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1109/seams.2019.00018,
  author	= {Bennaceur, Amel and Ghezzi, Carlo and Tei, Kenji and
		  Kehrer, Timo and Weyns, Danny and Calinescu, Radu and
		  Dustdar, Schahram and Hu, Zhenjiang and Honiden, Shinichi
		  and Ishikawa, Fuyuki and Jin, Zhi and Kramer, Jeffrey and
		  Litoiu, Marin and Loreti, Michele and Moreno, Gabriel A.
		  and M\"{u}ller, Hausi A. and Nenzi, Laura and Nuseibeh,
		  Bashar and Pasquale, Liliana and Reisig, Wolfgang and
		  Schmidt, Heinz and Tsigkanos, Christos and Zhao, Haiyan},
  title		= {Modelling and analysing resilient cyber-physical systems},
  year		= {2019},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/SEAMS.2019.00018},
  doi		= {10.1109/SEAMS.2019.00018},
  abstract	= {From smart buildings to medical devices to smart nations,
		  software systems increasingly integrate computation,
		  networking, and interaction with the physical environment.
		  These systems are known as Cyber-Physical Systems (CPS).
		  While these systems open new opportunities to deliver
		  improved quality of life for people and reinvigorate
		  computing, their engineering is a difficult problem given
		  the level of heterogeneity and dynamism they exhibit. While
		  progress has been made, we argue that complexity is now at
		  a level such that existing approaches need a major re-think
		  to define principles and associated techniques for CPS. In
		  this paper, we identify research challenges when modelling,
		  analysing and engineering CPS. We focus on three key
		  topics: theoretical foundations of CPS, self-adaptation
		  methods for CPS, and exemplars of CPS serving as a research
		  vehicle shared by a larger community. For each topic, we
		  present an overview and suggest future research directions,
		  thereby focusing on selected challenges. This paper is one
		  of the results of the Shonan Seminar 118 on Modelling and
		  Analysing Resilient Cyber-Physical Systems, which took
		  place in December 2018.},
  booktitle	= {Proceedings of the 14th International Symposium on
		  Software Engineering for Adaptive and Self-Managing
		  Systems},
  pages		= {70–76},
  numpages	= {7},
  location	= {Montreal, Quebec, Canada},
  series	= {SEAMS '19}
}

@InProceedings{	  10.1145/3323873.3325026,
  author	= {Karayil, Tushar and Blandfort, Philipp and Hees, J\"{o}rn
		  and Dengel, Andreas},
  title		= {The Focus-Aspect-Value Model for Explainable Prediction of
		  Subjective Visual Interpretation},
  year		= {2019},
  isbn		= {9781450367653},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3323873.3325026},
  doi		= {10.1145/3323873.3325026},
  abstract	= {Subjective visual interpretation is a challenging yet
		  important topic in computer vision. Many approaches reduce
		  this problem to the prediction of adjective- or
		  attribute-labels from images. However,most of these do not
		  take attribute semantics into account, or only process the
		  image in a holistic manner. Furthermore, there is alack of
		  relevant datasets with fine-grained subjective labels. In
		  this paper, we propose the Focus-Aspect-Value (FAV) model
		  to structure the process of capturing subjectivity in image
		  processing,and introduce a novel dataset following this way
		  of modeling. We run experiments on this dataset to compare
		  several deep learning methods and find that incorporating
		  context information based on tensor multiplication
		  outperforms the default way of information fusion
		  (concatenation).},
  booktitle	= {Proceedings of the 2019 on International Conference on
		  Multimedia Retrieval},
  pages		= {16–24},
  numpages	= {9},
  keywords	= {zero-shot, subjectivity, neural network, logistic
		  regression, information fusion, images, fav},
  location	= {Ottawa ON, Canada},
  series	= {ICMR '19}
}

@InProceedings{	  10.1145/3308560.3316518,
  author	= {McKenna, Lucy and Debruyne, Christophe and O'Sullivan,
		  Declan},
  title		= {Modelling the Provenance of Linked Data Interlinks for the
		  Library Domain},
  year		= {2019},
  isbn		= {9781450366755},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308560.3316518},
  doi		= {10.1145/3308560.3316518},
  abstract	= {As the Web of Data grows, so does the need to establish
		  the quality and trustworthiness of its contents. Increasing
		  numbers of libraries are publishing their metadata as
		  Linked Data (LD). As these institutions are considered
		  authoritative sources of information, it is likely that
		  library LD will be treated with increased credibility over
		  data published by other sources. However, in order to
		  establish this trust, the provenance of library LD must be
		  provided.In 2018 we conducted a survey which explored the
		  position of Information Professionals (IPs), such as
		  librarians, archivists and cataloguers, with regards to LD.
		  Results indicated that IPs find the process of LD
		  interlinking to be a particularly challenging. In order to
		  publish authoritative interlinks, provenance data for the
		  description and justification of the links is required. As
		  such, the goal of this research is to provide a provenance
		  model for the LD interlinking process that meets the
		  requirements of library metadata standards. Many current LD
		  technologies are not accessible to non-technical experts or
		  attuned to the needs of the library domain. By designing a
		  model specifically for libraries, with input from IPs, we
		  aim to facilitate this domain in the process of creating
		  interlink provenance data.},
  booktitle	= {Companion Proceedings of The 2019 World Wide Web
		  Conference},
  pages		= {954–958},
  numpages	= {5},
  keywords	= {semantic web, provenance, linked data, library,
		  interlinking},
  location	= {San Francisco, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3450614.3463389,
  author	= {Diaz-Agudo, Belen and Bosca, Alessio and Bolioli, Andrea
		  and Jimenez Jimenez Diaz, Guilermo and Kuflik, Tsvi and J.
		  Wecker, Alan},
  title		= {Towards Personalized Social Recommendations for Cultural
		  Heritage Activities: Methods and technology to enable
		  cohesive and inclusive recommendations},
  year		= {2021},
  isbn		= {9781450383677},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3450614.3463389},
  doi		= {10.1145/3450614.3463389},
  abstract	= {The aim of the SPICE project is to build social cohesion,
		  both between and within citizen communities, by developing
		  tools and methods to support citizen curation. We define
		  citizen curation as a process in which cultural objects are
		  used as a resource by citizens to develop their own
		  personal interpretations. Within communities, citizens can
		  use their interpretations to build a representation of
		  themselves and their shared perspective on culture.
		  Interpretations can also be used to support social cohesion
		  across groups. In this short position paper we outline the
		  methodologies and technologies needed to be built in order
		  to build a recommender system of cultural objects that will
		  implement these goals of social cohesion and inclusion.},
  booktitle	= {Adjunct Proceedings of the 29th ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {199–202},
  numpages	= {4},
  location	= {Utrecht, Netherlands},
  series	= {UMAP '21}
}

@Article{	  10.14778/3712221.3712232,
  author	= {Gilray, Thomas and Sahebolamri, Arash and Sun, Yihao and
		  Kunapaneni, Sowmith and Kumar, Sidharth and Micinski,
		  Kristopher},
  title		= {Datalog with First-Class Facts},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {VLDB Endowment},
  volume	= {18},
  number	= {3},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3712221.3712232},
  doi		= {10.14778/3712221.3712232},
  abstract	= {Datalog is a popular logic programming language for
		  deductive reasoning tasks in a wide array of applications,
		  including business analytics, program analysis, and
		  ontological reasoning. However, Datalog's restriction to
		  flat facts over atomic constants leads to challenges in
		  working with tree-structured data, such as derivation trees
		  or abstract syntax trees. To ameliorate Datalog's
		  restrictions, popular extensions of Datalog support
		  features such as existential quantification in rule heads
		  (Datalog*, Datalog∃) or algebraic data types
		  (Souffl\'{e}). Unfortunately, these are imperfect solutions
		  for reasoning over structured and recursive data types,
		  with general existentials leading to complex
		  implementations requiring unification, and ADTs unable to
		  trigger rule evaluation and failing to support efficient
		  indexing.We present DL∃!, a Datalog with first-class
		  facts, wherein every fact is identified with a Skolem term
		  unique to the fact. We show that this restriction offers an
		  attractive price point for Datalogbased reasoning over
		  tree-shaped data, demonstrating its application to
		  databases, artificial intelligence, and programming
		  languages. We implemented DL∃! as a system Slog, which
		  leverages the uniqueness restriction of DL∃! to enable a
		  communication-avoiding, massively-parallel implementation
		  built on MPI. We show that Slog outperforms leading systems
		  (Nemo, Vlog, RDFox, and Souffl\'{e}) on a variety of
		  benchmarks, with the potential to scale to thousands of
		  threads.},
  journal	= {Proc. VLDB Endow.},
  month		= nov,
  pages		= {651–665},
  numpages	= {15}
}

@InProceedings{	  10.5555/3721488.3721763,
  author	= {Cooper, Sara and Ros, Raquel and Lemaignan, S\'{e}verin
		  and Gebell\'{\i}, Ferran and Ferrini, Lorenzo and
		  Juri?i\'{c}, Luka},
  title		= {Demonstration of an Open-source ROS 2 Framework and
		  Simulator for Situated Interactive Social Robot},
  year		= {2025},
  publisher	= {IEEE Press},
  abstract	= {We introduce an open-source ROS 2 architecture for
		  situated social robots, along with a simulator that allows
		  mixed-reality development and interactions. The
		  architecture is a hybrid symbolic/subsymbolic system that
		  integrates explicit ontology semantics for perception,
		  reasoning, and execution, with LLMs. It features multimodal
		  social perception by leveraging the open source ROS4HRI
		  framework; LLMs (both edge- and cloud-based) to facilitate
		  natural language interaction between the user and system;
		  KnowledgeCore, an open-source knowledge base, to reason
		  about facts in the world; and an intent-based controller to
		  supervise the execution of parallel/sequential tasks and
		  skills. We demonstrate our system architecture with a
		  social robot running the mixed-reality system.},
  booktitle	= {Proceedings of the 2025 ACM/IEEE International Conference
		  on Human-Robot Interaction},
  pages		= {1770–1772},
  numpages	= {3},
  keywords	= {mixed-reality simulator, ros 2 framework, situated social
		  robots},
  location	= {Melbourne, Australia},
  series	= {HRI '25}
}

@InProceedings{	  10.1145/3539618.3592092,
  author	= {Lin, Hsien-Chin and Feng, Shutong and Geishauser,
		  Christian and Lubis, Nurul and van Niekerk, Carel and Heck,
		  Michael and Ruppik, Benjamin and Vukovic, Renato and
		  Gasi\'{c}, Milica},
  title		= {EmoUS: Simulating User Emotions in Task-Oriented
		  Dialogues},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3592092},
  doi		= {10.1145/3539618.3592092},
  abstract	= {Existing user simulators (USs) for task-oriented dialogue
		  systems only model user behaviour on semantic and natural
		  language levels without considering the user persona and
		  emotions. Optimising dialogue systems with generic user
		  policies, which cannot model diverse user behaviour driven
		  by different emotional states, may result in a high
		  drop-off rate when deployed in the real world. Thus, we
		  present EmoUS, a user simulator that learns to simulate
		  user emotions alongside user behaviour. EmoUS generates
		  user emotions, semantic actions, and natural language
		  responses based on the user goal, the dialogue history, and
		  the user persona. By analysing what kind of system
		  behaviour elicits what kind of user emotions, we show that
		  EmoUS can be used as a probe to evaluate a variety of
		  dialogue systems and in particular their effect on the
		  user's emotional state. Developing such methods is
		  important in the age of large language model chat-bots and
		  rising ethical concerns.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2526–2531},
  numpages	= {6},
  keywords	= {dialogue system, emotion simulation, user simulation},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3336499.3338012,
  author	= {Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius
		  and Gomez, Mauricio Soto and Elves\ae{}ter, Brian and
		  Roman, Dumitru},
  title		= {Modelling and Linking Company Data in the euBusinessGraph
		  Platform},
  year		= {2019},
  isbn		= {9781450368230},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3336499.3338012},
  doi		= {10.1145/3336499.3338012},
  abstract	= {In the business environment, knowledge of company data is
		  essential for a variety of tasks. The European funded
		  project euBusinessGraph enables the establishment of a
		  company data platform where data providers and consumers
		  can publish and access company data. The core of the
		  platform is the semantic data model that is the conceptual
		  representation of company data in a common way so that it
		  is easier to share and interlink company data. In this
		  paper we show how the unified model and Grafterizer, a tool
		  for manipulating and transforming raw data into Linked
		  Data, support the linking challenge proposed in FEIII 2019.
		  Results show that geographical enrichment of RDF data
		  supports the interlinking process between company entities
		  in different datasets.},
  booktitle	= {Proceedings of the 5th Workshop on Data Science for
		  Macro-Modeling with Financial and Economic Datasets},
  articleno	= {12},
  numpages	= {6},
  keywords	= {Record Linkage, RDF, Entity Matching, Company data},
  location	= {Amsterdam, Netherlands},
  series	= {DSMM'19}
}

@InProceedings{	  10.1145/3599957.3606249,
  author	= {Ahn, Sung-Yoon and Lee, Sang-Woong},
  title		= {BERT-based classification of fungi protein sequences with
		  multiple GO labels},
  year		= {2023},
  isbn		= {9798400702280},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3599957.3606249},
  doi		= {10.1145/3599957.3606249},
  abstract	= {Due to the increase of reported fungi-related diseases, it
		  has come to many health organizations concern that there
		  may be possible highly contagious fungi that may cause yet
		  another pandemic. Though the likelihood of such is low,
		  research is needed to grasp the understanding of unknown
		  fungi. Identifying and figuring out the traits of unknown
		  fungi through in vitro and in vivo experiments take time
		  and resources. In silico methods yield faster results with
		  a slight drop in accuracy. Modern in silico approaches
		  utilizing deep learning, allow for faster and more accurate
		  classifications. In this study, we perform the
		  classification of one or more gene ontologies of fungi
		  protein sequences. We collected open-source protein
		  sequences from UniProt and applied an algorithm to label
		  the sequences with their gene ontologies. We use ProtBERT
		  with additional layers to give classification results to
		  all the different gene ontologies. Experimental results
		  reveal that when classifying with the top 5 most frequent
		  gene ontologies, the model was able to yield 0.7915 for
		  F1-score, 0.7073 for MCC, and 0.8865 for AuROC. With the
		  top 10 most frequent gene ontologies it yielded 0.6490 for
		  F1-score, 0.6836 for MCC, and 0.7653 for AuROC.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Research in Adaptive and Convergent Systems},
  articleno	= {28},
  numpages	= {4},
  keywords	= {Gene Ontology, Fungi, BERT},
  location	= {Gdansk, Poland},
  series	= {RACS '23}
}

@InProceedings{	  10.1145/3626772.3657666,
  author	= {Prieur, Maxime and Du Mouza, C\'{e}dric and Gadek,
		  Guillaume and Grilheres, Bruno},
  title		= {Shadowfax: Harnessing Textual Knowledge Base Population},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657666},
  doi		= {10.1145/3626772.3657666},
  abstract	= {Knowledge base population (KBP) from texts involves the
		  extraction and organization of information from
		  unstructured textual data to enhance or create a structured
		  knowledge base. This process is crucial for various
		  applications, such as natural language understanding,
		  question-answering systems, and knowledge-driven
		  decision-making. However the difficulty lies in the
		  complexity of natural language, which is nuanced,
		  ambiguous, and context-dependent. Extracting accurate and
		  reliable information requires overcoming challenges such as
		  entity disambiguation and relation extraction which are
		  time-consuming tasks for users.Shadowfax is an interactive
		  platform designed to support users by streamlining the
		  process of knowledge base population (KPB) from text
		  documents. Unlike other existing tools, it relies on a
		  unified machine learning model to extract relevant
		  information from unstructured text, enabling operational
		  agents to gain a quick overview. The proposed system
		  supports a variety of natural language processing (NLP)
		  tasks using a single architecture, while presenting
		  information in the most comprehensive way possible to the
		  end user.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2796–2800},
  numpages	= {5},
  keywords	= {data mining, deep-learning, end-to-end, information
		  extraction, knowledge base population, user in the loop},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3366030.3366128,
  author	= {Garc\'{\i}a, Roberto and Gil, Rosa},
  title		= {Social Media Copyright Management using Semantic Web and
		  Blockchain},
  year		= {2020},
  isbn		= {9781450371797},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366030.3366128},
  doi		= {10.1145/3366030.3366128},
  abstract	= {Solutions based on distributed ledgers require
		  sophisticated tools for data modelling and integration that
		  can be overcome using semantic and Linked Data
		  technologies. One example is copyright management, where we
		  attempt to adapt the Copyright Ontology so it can be used
		  to build applications that benefit from both worlds, rich
		  information modelling and reasoning together with immutable
		  and accountable information storage that provides trust and
		  confidence on the modelled rights statements. This approach
		  has been applied in the context of an application for the
		  management of social media re-use for journalistic
		  purposes.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {339–343},
  numpages	= {5},
  keywords	= {Semantic Web, Rights Expression Language, Ontology, Linked
		  Data, Ethereum, Distributed Ledger, Copyright, Blockchain},
  location	= {Munich, Germany},
  series	= {iiWAS2019}
}

@InProceedings{	  10.1145/3429889.3430079,
  author	= {Wu, Nankai and Cao, Qingsong and Li, Huanzhe and Hou,
		  Xingquan and Lo, Infat and Kong, Jiangping},
  title		= {Correlation between Pathological Voice Onset and Voice
		  Quality Based on Vocal Attack Time(VAT) and
		  Multidimensional Voice Parameters},
  year		= {2020},
  isbn		= {9781450388603},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3429889.3430079},
  doi		= {10.1145/3429889.3430079},
  abstract	= {Correlation between pathological Voice Onset and Voice
		  Quality of Chinese patients based on Vocal Attack Time(VAT)
		  and Multidimensional Voice Parameters is discussed in this
		  paper. The test subjects were divided into three groups,
		  one is normal voice group and the other two are pathologic
		  voice groups, namely, vocal cord polyps and non vocal cord
		  polyps. We recorded the EGG signal for the above subjects
		  and extracted the Jitter, Shimmer, HNR and VAT parameters
		  by the relevant software. The VAT and other voice
		  parameters at /a:/, /i:/ and /u:/ vowels were then compared
		  and analyzed in different groups. The results showed that
		  there was no significant difference in the VAT between the
		  three groups at /a:/, /i:/, and /u:/ vowels. In addition,
		  the analysis of VAT changes in a patient with vocal cord
		  polyps before and after surgery revealed that neither the
		  overall difference in stops nor the difference in manner of
		  aspiration or not was significant, indicating VAT is not a
		  specific indicator of vocal cord polyps.},
  booktitle	= {Proceedings of the 1st International Symposium on
		  Artificial Intelligence in Medical Sciences},
  pages		= {107–111},
  numpages	= {5},
  keywords	= {Voice Quality, Voice Onset, Vocal Attack Time,
		  Pathological Voice, Electroglottography},
  location	= {Beijing, China},
  series	= {ISAIMS '20}
}

@InProceedings{	  10.1145/3178461.3178468,
  author	= {Wakil, Karzan and Jawawi, Dayang N. A.},
  title		= {A New Adaptive Model for Web Engineering Methods to
		  Develop Modern Web Applications},
  year		= {2018},
  isbn		= {9781450354387},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3178461.3178468},
  doi		= {10.1145/3178461.3178468},
  abstract	= {With the evolution of modern web applications, several web
		  engineering methods proposed to develop web applications.
		  The modern web applications are; Rich Internet Application
		  (RIA), Semantic Web Application (SWA), Ubiquitous Web
		  Applications (UWA), and Intelligent Web Applications (IWA),
		  with each of them having new features. The problem is that
		  current web engineering methods cannot support new features
		  of modern web applications. However, some of them extended
		  for new concern of web applications but have limited,
		  meaning these methods have a lack of adaptability to
		  support features from modern web applications. In an
		  attempt to solve this gap, we have defined a new adaptive
		  model for the web engineering methods that can support the
		  new features of modern web applications. This model very
		  efficient in the process development and will be to
		  increase the usability of the methods.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Software Engineering and Information Management},
  pages		= {32–39},
  numpages	= {8},
  keywords	= {Web Engineering, Web Applications, Adaptive Model},
  location	= {Casablanca, Morocco},
  series	= {ICSIM '18}
}

@Article{	  10.1145/3424667,
  author	= {Ungureanu, George and Medeiros, Jos\'{e} Edil
		  Guimar\~{a}es De and sundstr\"{o}m, Timmy and
		  S\"{o}derquist, Ingemar and \r{A}hlander, Anders and
		  Sander, Ingo},
  title		= {ForSyDe-Atom: Taming Complexity in Cyber Physical System
		  Design with Layers},
  year		= {2021},
  issue_date	= {March 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {2},
  issn		= {1539-9087},
  url		= {https://doi.org/10.1145/3424667},
  doi		= {10.1145/3424667},
  abstract	= {We present ForSyDe-Atom, a formal framework intended as an
		  entry point for disciplined design of complex
		  cyber-physical systems. This framework provides a set of
		  rules for combining several domain-specific languages as
		  structured, enclosing layers to orthogonalize the many
		  aspects of system behavior, yet study their interaction in
		  tandem. We define four layers: one for capturing timed
		  interactions in heterogeneous systems, one for structured
		  parallelism, one for modeling uncertainty, and one for
		  describing component properties. This framework enables a
		  systematic exploitation of design properties in a design
		  flow by facilitating the stepwise projection of certain
		  layers of interest, the isolated analysis and refinement on
		  projections, and the seamless reconstruction of a system
		  model by virtue of orthogonalization. We demonstrate the
		  capabilities of this approach by providing a compact yet
		  expressive model of an active electronically scanned array
		  antenna and signal processing chain, simulate it, validate
		  its conformity with the design specifications, refine it,
		  synthesize a sub-system to VHDL and sequential code, and
		  co-simulate the generated artifacts.},
  journal	= {ACM Trans. Embed. Comput. Syst.},
  month		= jan,
  articleno	= {10},
  numpages	= {27},
  keywords	= {validation, system design language, synthesis, simulation,
		  models of computation, modeling, design methodology,
		  Cyber-physical systems}
}

@InProceedings{	  10.1145/3583780.3615514,
  author	= {Colas, Anthony and Ma, Haodi and He, Xuanli and Bai, Yang
		  and Wang, Daisy Zhe},
  title		= {Can Knowledge Graphs Simplify Text?},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615514},
  doi		= {10.1145/3583780.3615514},
  abstract	= {Knowledge Graph (KG)-to-Text Generation has seen recent
		  improvements in generating fluent and informative sentences
		  which describe a given KG. As KGs are widespread across
		  multiple domains and contain important entity-relation
		  information, and as text simplification aims to reduce the
		  complexity of a text while preserving the meaning of the
		  original text, we propose KGSimple, a novel approach to
		  unsupervised text simplification which infuses
		  KG-established techniques in order to construct a
		  simplified KG path and generate a concise text which
		  preserves the original input's meaning. Through an
		  iterative and sampling KG-first approach, our model is
		  capable of simplifying text when starting from a KG by
		  learning to keep important information while harnessing
		  KG-to-text generation to output fluent and descriptive
		  sentences. We evaluate various settings of the KGSimple
		  model on currently-available KG-to-text datasets,
		  demonstrating its effectiveness compared to unsupervised
		  text simplification models which start with a given complex
		  text. Our code is available on GitHub.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {379–389},
  numpages	= {11},
  keywords	= {text simplification, simulated annealing, natural language
		  generation, knowledge graph, data-to-text, KG-to-text},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Book{		  10.1145/3674127,
  editor	= {Alonso, Omar and Baeza-Yates, Ricardo},
  title		= {Information Retrieval: Advanced Topics and Techniques},
  year		= {2024},
  isbn		= {9798400710506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  volume	= {60},
  abstract	= {In the last decade, deep learning and word embeddings have
		  made significant impacts on information retrieval (IR) by
		  adding techniques based in neural networks and language
		  models. At the same time, certain search modalities such as
		  neural IR and conversational search have become more
		  popular. This book, written by international academic and
		  industry experts, brings the field up to date with detailed
		  discussions of these new approaches and techniques. The
		  book is organized in three sections: Foundations,
		  Adaptations and Concerns, and Verticals.Under Foundations,
		  we address topics that form the basic structure of any
		  modern IR system, including recommender systems. These new
		  techniques are developed to augment indexing, retrieval,
		  and ranking. Neural IR, recommender systems, evaluation,
		  query-driven functionality, and knowledge graphs are
		  covered in this section.IR systems need to adapt to
		  specific user characteristics and preferences, and
		  techniques that were considered too niche a few years ago
		  are now a matter of system design consideration. The
		  Adaptations and Concerns section covers the following
		  topics: conversational search, cross-language retrieval,
		  temporal extraction and retrieval, bias in retrieval
		  systems, and privacy in search.While web search engines are
		  the most popular information access point, there are cases
		  where specific verticals provide a better experience in
		  terms of content and relevance. The Verticals section
		  describes eCommerce, professional search, personal
		  collections, music retrieval, and biomedicine as
		  examples.}
}

@InProceedings{	  10.1145/3613904.3642542,
  author	= {Mildner, Thomas and Cooney, Orla and Meck, Anna-Maria and
		  Bartl, Marion and Savino, Gian-Luca and Doyle, Philip R and
		  Garaialde, Diego and Clark, Leigh and Sloan, John and
		  Wenig, Nina and Malaka, Rainer and Niess, Jasmin},
  title		= {Listening to the Voices: Describing Ethical Caveats of
		  Conversational User Interfaces According to Experts and
		  Frequent Users},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642542},
  doi		= {10.1145/3613904.3642542},
  abstract	= {Advances in natural language processing and understanding
		  have led to a rapid growth in the popularity of
		  conversational user interfaces (CUIs). While CUIs introduce
		  novel benefits, they also yield risks that may exploit
		  people’s trust. Although research looking at unethical
		  design deployed through graphical user interfaces (GUIs)
		  established a thorough understanding of so-called dark
		  patterns, there is a need to continue this discourse within
		  the CUI community to understand potentially problematic
		  interactions. Addressing this gap, we interviewed 27
		  participants from three cohorts: researchers,
		  practitioners, and frequent users of CUIs. Applying
		  thematic analysis, we construct five themes reflecting each
		  cohort’s insights about ethical design challenges and
		  introduce the CUI Expectation Cycle, bridging system
		  capabilities and user expectations while considering each
		  theme’s ethical caveats. This research aims to inform
		  future development of CUIs to consider ethical constraints
		  while adopting a human-centred approach.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {307},
  numpages	= {18},
  keywords	= {CUI, chatbots, conversational agents, conversational user
		  interfaces, dark patterns, deceptive design patterns,
		  ethical design, thematic analysis, voice agents},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@Article{	  10.1145/3736656,
  author	= {de Roode, Gerard and Everts, Maarten},
  title		= {SoK: Unifying Definitions of Privacy and Anonymity in
		  Cryptocurrencies \&amp; DLTs},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3736656},
  doi		= {10.1145/3736656},
  abstract	= {As interest in the practical use of cryptocurrencies
		  continues to grow, so does the focus on the (perceived)
		  privacy and anonymity of users within this domain. Despite
		  this attention, there is a notable absence of standardized
		  definitions for these terms. This paper aims to address
		  this gap by exploring the various interpretations of
		  privacy, anonymity, and related concepts in the context of
		  cryptocurrencies. Drawing from a thorough review of
		  existing literature, we propose practical definitions for
		  both privacy and anonymity. Utilizing these definitions, we
		  introduce an ontology designed to streamline future
		  research, identify knowledge gaps, and facilitate clearer
		  communication in the field.},
  note		= {Just Accepted},
  journal	= {Distrib. Ledger Technol.},
  month		= jul,
  keywords	= {cryptocurrency, privacy, anonymity, ontology}
}

@Article{	  10.14778/3746405.3746417,
  author	= {Cong, Tianji and Nargesian, Fatemeh and Xing, Junjie and
		  Jagadish, H. V.},
  title		= {OpenForge: Probabilistic Metadata Integration},
  year		= {2025},
  issue_date	= {May 2025},
  publisher	= {VLDB Endowment},
  volume	= {18},
  number	= {9},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3746405.3746417},
  doi		= {10.14778/3746405.3746417},
  abstract	= {Modern data stores increasingly rely on metadata to enable
		  diverse activities such as data cataloging and search.
		  However, metadata curation remains a labor-intensive task,
		  and the broader challenge of metadata
		  maintenance—ensuring its consistency and usefulness—has
		  been largely overlooked. In this work, we tackle the
		  problem of resolving relationships among metadata concepts
		  from disparate sources. Inferring these relationships are
		  critical for creating clean and consistent metadata
		  repositories, and a central challenge for metadata
		  integration.We propose OpenForge, a two-stage
		  prior-posterior framework for metadata integration. In the
		  first stage, OpenForge exploits multiple methods including
		  fine-tuned large language models to obtain prior beliefs
		  about concept relationships. In the second stage, OpenForge
		  refines these predictions using the Markov Random Field, a
		  probabilistic graphical model. We formalize metadata
		  integration as an optimization problem, where the objective
		  is to identify the relationship assignments that maximize
		  the joint probability of assignments. The MRF formulation
		  allows OpenForge to capture prior beliefs while encoding
		  critical relationship properties, such as transitivity, in
		  probabilistic inference. Experiments on four datasets show
		  the effectiveness and efficiency of OpenForge. In a use
		  case of matching two metadata vocabularies, OpenForge
		  outperforms GPT-4, the second-best method, by 25 F1
		  points.},
  journal	= {Proc. VLDB Endow.},
  month		= sep,
  pages		= {2914–2927},
  numpages	= {14}
}

@Article{	  10.1145/3736787,
  author	= {Meshi, Avital and Wright, Adam},
  title		= {in(A)n(I)mate - AI-Mediated Conversations with Inanimate
		  Objects},
  year		= {2025},
  issue_date	= {August 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {3},
  url		= {https://doi.org/10.1145/3736787},
  doi		= {10.1145/3736787},
  abstract	= {in(A)n(I)mate is an interactive AI-driven system that
		  invites participants to speak with objects. The piece
		  showcases an innovative use of GPT’s multimodal feature,
		  through its ability to recognize objects in an image and
		  generate responses in its style. Participants place an
		  object of their choice in front of a black box and engage
		  in conversation with it by pressing buttons, often unaware
		  that GPT is generating the responses. in(A)n(I)mate
		  provokes a discussion about human relationships with
		  inanimate matter, and considers the role of non-human
		  agents in mediating and animating objects.},
  journal	= {Proc. ACM Comput. Graph. Interact. Tech.},
  month		= jul,
  articleno	= {36},
  numpages	= {6},
  keywords	= {Artificial Intelligence, Creative AI, New Media Art, LLM,
		  GPT, Interactive art}
}

@Article{	  10.1145/3447772,
  author	= {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and
		  D’amato, Claudia and Melo, Gerard De and Gutierrez,
		  Claudio and Kirrane, Sabrina and Gayo, Jos\'{e} Emilio
		  Labra and Navigli, Roberto and Neumaier, Sebastian and
		  Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid,
		  Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and
		  Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  title		= {Knowledge Graphs},
  year		= {2021},
  issue_date	= {May 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3447772},
  doi		= {10.1145/3447772},
  abstract	= {In this article, we provide a comprehensive introduction
		  to knowledge graphs, which have recently garnered
		  significant attention from both industry and academia in
		  scenarios that require exploiting diverse, dynamic,
		  large-scale collections of data. After some opening
		  remarks, we motivate and contrast various graph-based data
		  models, as well as languages used to query and validate
		  knowledge graphs. We explain how knowledge can be
		  represented and extracted using a combination of deductive
		  and inductive techniques. We conclude with high-level
		  future research directions for knowledge graphs.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {71},
  numpages	= {37},
  keywords	= {shapes, rule mining, ontologies, graph query languages,
		  graph neural networks, graph databases, graph algorithms,
		  embeddings, Knowledge graphs}
}

@InProceedings{	  10.1145/3184558.3186575,
  author	= {Wisniewski, Dawid},
  title		= {Automatic Translation of Competency Questions into
		  SPARQL-OWL Queries},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3186575},
  doi		= {10.1145/3184558.3186575},
  abstract	= {The process of ontology authoring is inseparably connected
		  with the quality assurance phase. One can verify the
		  maturity and correctness of a given ontology by evaluating
		  how many competency questions give correct answers.
		  Competency questions are defined as a set of questions
		  expressed in natural language that the finished ontology
		  should be able to answer to correctly. Although this method
		  can easily indicate what is the development status of an
		  ontology, one has to translate competency questions from
		  natural language into an ontology query language. This task
		  is very hard and time consuming. To overcome this problem,
		  my PhD thesis focuses on methods for automatically checking
		  answerability of competency questions for a given ontology
		  and proposing SPARQL-OWL query (OWL-aware SPARQL query) for
		  each question where it is possible to create the query.
		  Because the task of automatic translation from competency
		  questions to SPARQL-OWL queries is a novel one, besides a
		  method, we have proposed a new benchmark to evaluate such
		  translation.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {855–859},
  numpages	= {5},
  keywords	= {SPARQL-OWL, competency question, ontology, word
		  embedding},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3478431.3499295,
  author	= {Lopez, Jake and Ross, Monique and Garcia, Atalie and
		  Uribe-Gosselin, Carolina},
  title		= {What is a Computer Scientist? Unpacking the Ontological
		  Beliefs of Black and Hispanic Female Computing Students},
  year		= {2022},
  isbn		= {9781450390705},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3478431.3499295},
  doi		= {10.1145/3478431.3499295},
  abstract	= {Underrepresentation of Black and Hispanic women in
		  computer science is a long-standing problem that looks
		  bleak at every level - undergraduate and graduate. This is
		  prompting scholars to explore reasons for these low
		  participation rates. One framework used to understand
		  participation and persistence in STEM fields is identity.
		  Prior work in computer science education suggest that
		  identity is a strong indicator of persistence in these
		  fields. However, it is hard to understand students'
		  perception of identity without also understanding
		  ontological beliefs with regards to a computer scientist.
		  In this study, we explore the nature of a computer
		  scientist. Guided by social identity theory, we designed a
		  study that asked students to describe their definition or
		  ontological belief of what constitutes a computer scientist
		  in contrast to their ability to ascribe a computer science
		  identity to self. Leveraging qualitative methods, we
		  interviewedn = 24 women in computer science (Black and
		  Hispanic, undergraduate and graduate students), in order to
		  explore the role their ontological beliefs had on their
		  computer science identity salience. The research questions
		  guiding this work are: (1) How do Black and Hispanic women
		  describe or define computer scientists? (2) What impact
		  does this definition have on Black and Hispanic women's
		  ability to claim a computing identity? Results suggest that
		  the wide variation in definitions has a negative impact on
		  computer science identity salience. The findings from this
		  work suggest that computing should consider the impacts of
		  the current messaging of what constitutes a computer
		  scientist.},
  booktitle	= {Proceedings of the 53rd ACM Technical Symposium on
		  Computer Science Education - Volume 1},
  pages		= {369–375},
  numpages	= {7},
  keywords	= {undergraduate curriculum, computing education, computer
		  science education, broadening participation},
  location	= {Providence, RI, USA},
  series	= {SIGCSE 2022}
}

@Article{	  10.1145/3564156,
  author	= {Alqahtani, Fatimah and Dohler, Mischa},
  title		= {Survey of Authorship Identification Tasks on Arabic
		  Texts},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3564156},
  doi		= {10.1145/3564156},
  abstract	= {Authorship identification is the process of extracting and
		  analysing the writing styles of authors to identify the
		  authorship. From the writing style, the author and his/her
		  different characteristics can be recognised, which is very
		  useful in digital forensics and cyber investigations. In
		  the literature, authorship identification tasks were
		  addressed on both long and short documents and performed on
		  different languages, such as English, Arabic, Chinese, and
		  Greek. This survey has reviewed the authorship
		  identification tasks for the Arabic language to contribute
		  to this area of research by exploring Arabic language
		  performance and challenges. A total of 27 prominent Arabic
		  studies of each authorship identification domain were
		  reviewed considering the used data, selected features,
		  utilised methods, and results. After a review of the
		  various studies, it was concluded that the results of
		  authorship identification tasks vary based on mostly the
		  selected features and used dataset. Furthermore, the
		  effective features differ from one dataset to another based
		  on the various types of the&nbsp;Arabic language. However,
		  all authorship identification tasks involving the Arabic
		  language face considerable challenges with data
		  pre-processing due to the challenging Arabic concatenative
		  morphology.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {93},
  numpages	= {24},
  keywords	= {stylometry, Arabic texts, authorship verification,
		  authorship attribution, Authorship identification}
}

@Article{	  10.14778/3415478.3415557,
  author	= {Quamar, Abdul and \"{O}zcan, Fatma and Miller, Dorian and
		  Moore, Robert J and Niehus, Rebecca and Kreulen, Jeffrey},
  title		= {Conversational BI: an ontology-driven conversation system
		  for business intelligence applications},
  year		= {2020},
  issue_date	= {August 2020},
  publisher	= {VLDB Endowment},
  volume	= {13},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3415478.3415557},
  doi		= {10.14778/3415478.3415557},
  abstract	= {Business intelligence (BI) applications play an important
		  role in the enterprise to make critical business decisions.
		  Conversational interfaces enable non-technical enterprise
		  users to explore their data, democratizing access to data
		  significantly. In this paper, we describe an ontology-based
		  framework for creating a conversation system for BI
		  applications termed as Conversational BI. We create an
		  ontology from a business model underlying the BI
		  application, and use this ontology to automatically
		  generate various artifacts of the conversation system.
		  These include the intents, entities, as well as the
		  training samples for each intent. Our approach builds upon
		  our earlier work, and exploits common BI access patterns to
		  generate intents, their training examples and adapt the
		  dialog structure to support typical BI operations. We have
		  implemented our techniques in Health Insights (HI), an IBM
		  Watson Healthcare offering, providing analysis over
		  insurance data on claims. Our user study demonstrates that
		  our system is quite intuitive for gaining business insights
		  from data. We also show that our approach not only captures
		  the analysis available in the fixed application dashboards,
		  but also enables new queries and explorations.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {3369–3381},
  numpages	= {13}
}

@InProceedings{	  10.1145/3310273.3323436,
  author	= {Palumbo, Francesca and Fanni, Tiziana and Sau, Carlo and
		  Pulina, Luca and Raffo, Luigi and Masin, Michael and
		  Shindin, Evgeny and de Rojas, Pablo Sanchez and Desnos,
		  Karol and Pelcat, Maxime and Rodr\'{\i}guez, Alfonso and
		  Ju\'{a}rez, Eduardo and Regazzoni, Francesco and Meloni,
		  Giuseppe and Zedda, Katiuscia and Myrhaug, Hans and
		  Kaliciak, Leszek and Andriaanse, Joost and de Olivieria
		  Filho, Julio and Mu\~{n}oz, Pablo and Toffetti, Antonella},
  title		= {CERBERO: Cross-layer modEl-based fRamework for
		  multi-oBjective dEsign of reconfigurable systems in
		  unceRtain hybRid envirOnments: Invited paper: CERBERO teams
		  from UniSS, UniCA, IBM Research, TASE, INSA-Rennes, UPM,
		  USI, Abinsula, AmbieSense, TNO, S&amp;T, CRF},
  year		= {2019},
  isbn		= {9781450366854},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3310273.3323436},
  doi		= {10.1145/3310273.3323436},
  abstract	= {Cyber-Physical Systems (CPS) are embedded computational
		  collaborating devices, capable of sensing and controlling
		  physical elements and, often, responding to humans.
		  Designing and managing systems able to respond to
		  different, concurrent requirements during operation is not
		  straightforward, and introduce the need of proper support
		  at design-time and run-time. The Cross-layer modEl-based
		  fRamework for multi-oBjective dEsign of Reconfigurable
		  systems in unceRtain hybRid envirOnments (CERBERO) EU
		  project has developed a design environment for adaptive
		  CPS. CERBERO approach leverages on model-based
		  methodologies including different technologies and tools
		  developed to cover design and operation from user
		  interactions down to low level computing layer
		  implementation.},
  booktitle	= {Proceedings of the 16th ACM International Conference on
		  Computing Frontiers},
  pages		= {320–325},
  numpages	= {6},
  keywords	= {verification, self-adaptation, SW adaptivity, HW
		  reconfiguration, HW adaptivity, CPS},
  location	= {Alghero, Italy},
  series	= {CF '19}
}

@InProceedings{	  10.1145/3708319.3734180,
  author	= {Perez-Martinez, Roberto and Casas-Ortiz, Alberto and
		  Santos, Olga C.},
  title		= {MoRTELaban: a Neurosymbolic Framework for Motion
		  Representation and Analysis based on Labanotation and Laban
		  Movement Analysis},
  year		= {2025},
  isbn		= {9798400713996},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708319.3734180},
  doi		= {10.1145/3708319.3734180},
  abstract	= {Human motion cannot be fully modeled by subsymbolic
		  representations. While these extract precise hidden
		  patterns in motion data, they are often task-specific and
		  lack a semantic understatement of motion. Symbolic systems
		  that mirror human cognition and explicit expressive
		  processes are necessary for richer motion synthesis and
		  analysis, enabling physical reasoning and expert knowledge
		  encoding. In this work, we propose a neurosymbolic
		  framework that combines Labanotation and Laban Movement
		  Analysis (LMA), originally developed for dance, to
		  represent and analyze human motion symbolically. We expand
		  the existing LabanEditor to support full-body annotation
		  and integrate it with AMASS, Mediapipe, and Kinect inputs
		  through a SMPL-based format. Our system supports automatic
		  annotation for the local functional and expressive aspects
		  of motion, and enables bidirectional conversion between
		  symbols and motion. While still a work in progress, this
		  framework lays the groundwork for explainable, expressive
		  motion modeling that can support human-robot interaction,
		  motion preservation, and psychomotor learning systems.},
  booktitle	= {Adjunct Proceedings of the 33rd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {353–359},
  numpages	= {7},
  keywords	= {motion modeling, movement modeling, knowledge
		  representation, Labanotation, LMA, expert systems},
  location	= { },
  series	= {UMAP Adjunct '25}
}

@InProceedings{	  10.5555/3320516.3320631,
  author	= {Wagner, Gerd and Nardin, Luis G.},
  title		= {Adding agent concepts to object event modeling and
		  simulation},
  year		= {2018},
  isbn		= {978153866570},
  publisher	= {IEEE Press},
  abstract	= {Object Event Modeling and Simulation (OEM&amp;S) is a
		  general Discrete Event Simulation paradigm combining
		  object-oriented modeling with the event scheduling
		  paradigm. We show how to extend OEM&amp;S by adding
		  concepts of agent-based modeling and simulation, resulting
		  in a framework that we call Agent/Object Event Modeling and
		  Simulation (A/OEM&amp;S). The main point for such an
		  extension is to define agents as special objects, which are
		  subject to general (physical) laws of causality captured in
		  the form of event rules, and which have their own behavior
		  allowing them to interact with their inanimate environment
		  and with each other. Because agent behavior is decoupled
		  from physical causality, an A/OE simulator consists of an
		  environment simulator, which simulates the physical world
		  (the objective states of material objects), and agent
		  simulators, which simulate the internal (subjective) states
		  of agents and their behaviors.},
  booktitle	= {Proceedings of the 2018 Winter Simulation Conference},
  pages		= {893–904},
  numpages	= {12},
  location	= {Gothenburg, Sweden},
  series	= {WSC '18}
}

@InProceedings{	  10.1145/3550356.3561599,
  author	= {Balaban, Mira and Khitron, Igal and Maraee, Azzam and
		  Kifer, Michael},
  title		= {Mediation-based MLM in FOModeLer},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561599},
  doi		= {10.1145/3550356.3561599},
  abstract	= {MLM has attracted much attention over the last two
		  decades. MLM activities include philosophical discussions
		  about ontologies, requirements and relevant services, and
		  development of theories, languages, and tools. Approaches
		  differ in their support for MLM concepts on the levels of
		  syntax, semantics and pragmatics.The Mediation-based MLM
		  (MedMLM), is a formal theory that defines a multilevel
		  model as an ordered collection of levels that are
		  inter-related by mediators, and can be enriched by
		  inter-level relationships and interactions. The levels of
		  MedMLM are plain class models, and the mediators define
		  inter-level instantiation relations. MedMLM is unique in
		  supporting a modular architecture of levels and
		  mediators.This paper introduces the MedMLM software
		  modeling tool, that is built on top of the FOModeLer class
		  modeling tool. The tool supports MLM construction, querying
		  and reasoning, meta-reasoning, validation, syntax
		  verification, and plain computation. We also compare the
		  MedMLM tool with older MLM approaches using semantic,
		  syntactic, and pragmatic MLM criteria.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {444–452},
  numpages	= {9},
  keywords	= {multi-level modeling, executable logic, MLM semantics},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@Article{	  10.1145/3677057,
  author	= {Hougaard, Bastian Ils\o{} and Knoche, Hendrik},
  title		= {Aiming, Pointing, Steering: A Core Task Analysis Framework
		  for Gameplay},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CHI PLAY},
  url		= {https://doi.org/10.1145/3677057},
  doi		= {10.1145/3677057},
  abstract	= {Underneath their compelling audiovisual surface, games
		  require players to carry out mundane interaction work, such
		  as pointing, typing, or steering. However, many of these
		  underlying building blocks are not defined rigorously,
		  hampering synthesis and analysis. We elaborate on the
		  origin of tasks within human-computer interaction (HCI) and
		  define tasks' relationship to game terminology (game
		  mechanics, goals, and actions). Our proposed framework
		  draws on systemic-structural theory of activity to aid
		  systematic analysis and exploration of game design by
		  mapping gameplay to abstract core tasks. The framework
		  contains four task tools, applicable when 1) uncovering
		  design properties, 2) designing experimental manipulation,
		  3) creating behavioral measurements, and 4) describing
		  gameplay in literature reviews of game genres and design
		  techniques. We evaluated our framework as a lens to design
		  purposeful games in three case studies within a scientific
		  education. We invite researchers and practitioners to
		  employ the framework as a microscope, to describe and
		  design games rigorously.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= oct,
  articleno	= {292},
  numpages	= {48},
  keywords	= {abstraction, action, activity theory, core task, design
		  landscape, feedback, game design, gameplay, imperative
		  goals, mechanics, ontology, task analysis, task
		  definition}
}

@InProceedings{	  10.1145/3511616.3513115,
  author	= {Thapa, Nischay Bikram and Seifollahi, Sattar and Taheri,
		  Sona},
  title		= {Hospital Readmission Prediction Using Clinical Admission
		  Notes},
  year		= {2022},
  isbn		= {9781450396066},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511616.3513115},
  doi		= {10.1145/3511616.3513115},
  abstract	= {Clinical notes contain contextualised information beyond
		  structured data relating to patients’ past and current
		  health conditions. Despite the richness, their
		  unstructured, long, and high dimensional nature presents
		  challenges to traditional text representation techniques.
		  The advancement of deep contextual representation
		  techniques in natural language processing (NLP) has shown
		  remarkable performance in the biomedical and clinical
		  domains for various information extraction and predictive
		  tasks, including hospital readmission. However, most
		  previous works have proposed discharge summary models where
		  on-site medical intervention is impossible, and readmission
		  could still occur. This paper utilises clinical notes
		  recorded during admissions to study the risk of 30-day
		  hospital readmissions. We employ clinical notes from
		  MIMIC-III and consider competing baselines for clinical
		  text representation, where a set of machine learning and
		  deep learning algorithms are used to classify hospital
		  readmission. The study demonstrates that notes captured
		  during admissions play a crucial role to recognise
		  potential readmission risk supporting healthcare
		  practitioners for practical therapeutic intervention and
		  discharge planning.},
  booktitle	= {Proceedings of the 2022 Australasian Computer Science
		  Week},
  pages		= {193–199},
  numpages	= {7},
  keywords	= {Natural language processing, Hospital readmission,
		  Embedding techniques, Electronic health records},
  location	= {Brisbane, Australia},
  series	= {ACSW '22}
}

@InProceedings{	  10.1145/3318464.3380589,
  author	= {Weir, Nathaniel and Utama, Prasetya and Galakatos, Alex
		  and Crotty, Andrew and Ilkhechi, Amir and Ramaswamy, Shekar
		  and Bhushan, Rohin and Geisler, Nadja and H\"{a}ttasch,
		  Benjamin and Eger, Steffen and Cetintemel, Ugur and Binnig,
		  Carsten},
  title		= {DBPal: A Fully Pluggable NL2SQL Training Pipeline},
  year		= {2020},
  isbn		= {9781450367356},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318464.3380589},
  doi		= {10.1145/3318464.3380589},
  abstract	= {Natural language is a promising alternative interface to
		  DBMSs because it enables non-technical users to formulate
		  complex questions in a more concise manner than SQL.
		  Recently, deep learning has gained traction for translating
		  natural language to SQL, since similar ideas have been
		  successful in the related domain of machine translation.
		  However, the core problem with existing deep learning
		  approaches is that they require an enormous amount of
		  training data in order to provide accurate translations.
		  This training data is extremely expensive to curate, since
		  it generally requires humans to manually annotate natural
		  language examples with the corresponding SQL queries (or
		  vice versa). Based on these observations, we propose DBPal,
		  a new approach that augments existing deep learning
		  techniques in order to improve the performance of models
		  for natural language to SQL translation. More specifically,
		  we present a novel training pipeline that automatically
		  generates synthetic training data in order to (1) improve
		  overall translation accuracy, (2) increase robustness to
		  linguistic variation, and (3) specialize the model for the
		  target database. As we show, our DBPal training pipeline is
		  able to improve both the accuracy and linguistic robustness
		  of state-of-the-art natural language to SQL translation
		  models.},
  booktitle	= {Proceedings of the 2020 ACM SIGMOD International
		  Conference on Management of Data},
  pages		= {2347–2361},
  numpages	= {15},
  keywords	= {natural language to SQL, natural language interface to
		  database, NLIDB, NL2SQL},
  location	= {Portland, OR, USA},
  series	= {SIGMOD '20}
}

@Article{	  10.1145/3653317,
  author	= {Masmoudi, Maroua and Ben Abdallah Ben Lamine, Sana and
		  Karray, Mohamed Hedi and Archimede, Bernard and Baazaoui
		  Zghal, Hajer},
  title		= {Semantic Data Integration and Querying: A Survey and
		  Challenges},
  year		= {2024},
  issue_date	= {August 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {8},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3653317},
  doi		= {10.1145/3653317},
  abstract	= {Digital revolution produces massive, heterogeneous and
		  isolated data. These latter remain underutilized,
		  unsuitable for integrated querying and knowledge
		  discovering. Hence the importance of this survey on data
		  integration which identifies challenging issues and trends.
		  First, an overview of the different generations and basics
		  of data integration is given. Then, semantic data
		  integration is focused, since it semantically links data
		  allowing wider insights and decision-making. More than
		  thirty works are reviewed. The goal is to help analysts to
		  identify relevant criteria to compare then choose among
		  semantic data integration approaches, focusing on the
		  category (materialized, virtual or hybrid) and querying
		  techniques.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {209},
  numpages	= {35},
  keywords	= {Data integration, ontology, query processing, ETL, OBDA,
		  semantic mapping}
}

@InProceedings{	  10.1145/3534678.3539046,
  author	= {Ma, Yiming},
  title		= {CS-RAD: Conditional Member Status Refinement and Ability
		  Discovery for Social Network Applications},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539046},
  doi		= {10.1145/3534678.3539046},
  abstract	= {In a social network environment, member status represents
		  a member's social value in the network. A member's
		  abilities represent the potential of a member projecting
		  his/her social values to others, and also represent the
		  level of credibility and authority for a member to hold
		  certain status. Therefore, the concepts of status and
		  ability are deeply related, and should be consistent with
		  each other. In this paper, we establish the consistency
		  models among different member status and their abilities
		  through analyzing member data and integrating domain
		  knowledge. We use these models to help our members refine
		  their inconsistent status, at the same time, identify
		  ability gaps. To reliably refine a member status, we
		  introduce a practical and human-in-the-loop methodology to
		  build status hierarchy. Conditioned on the hierarchical
		  structure, our modeling process exploits the associations
		  between status and abilities. We applied the technique to
		  LinkedIn member titles -- one of the major types of the
		  member status, and member skills -- the main ability
		  representations at LinkedIn. We showed that our models are
		  intuitive and perform well. The skill gaps identified are
		  actionable and concise. In this paper, we also discuss the
		  aspects of building such systems, and how we could deploy
		  the models in production.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3486–3494},
  numpages	= {9},
  keywords	= {knowledge representation, ontology, social networks,
		  statistical modeling, taxonomy, user modeling},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.5555/3320516.3320611,
  author	= {Ruscheinski, Andreas and Budde, Kai and Warnke, Tom and
		  Wilsdorf, Pia and Hiller, Bjarne C. and Dombrowsky, Marcus
		  and Uhrmacher, Adelinde M.},
  title		= {Generating simulation experiments based on model
		  documentations and templates},
  year		= {2018},
  isbn		= {978153866570},
  publisher	= {IEEE Press},
  abstract	= {An increasing number of approaches for specifying and
		  executing simulation experiments emphasizes the desire to
		  make this part of modeling and simulation studies explicit,
		  and thus, also easier to replicate. We take this one step
		  further by automatically generating simulation experiment
		  specifications from documentations. Based on a
		  template-based approach and documentations, we show how
		  simulation experiment specifications can be generated and
		  executed for experiments, such as statistical model
		  checking and sensitivity analysis, and we identify crucial
		  challenges.},
  booktitle	= {Proceedings of the 2018 Winter Simulation Conference},
  pages		= {715–726},
  numpages	= {12},
  location	= {Gothenburg, Sweden},
  series	= {WSC '18}
}

@InProceedings{	  10.1145/3534678.3539187,
  author	= {Srivastava, Aseem and Suresh, Tharun and Lord, Sarah P.
		  and Akhtar, Md Shad and Chakraborty, Tanmoy},
  title		= {Counseling Summarization Using Mental Health Knowledge
		  Guided Utterance Filtering},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539187},
  doi		= {10.1145/3534678.3539187},
  abstract	= {The psychotherapy intervention technique is a multifaceted
		  conversation between a therapist and a patient. Unlike
		  general clinical discussions, psychotherapy's core
		  components (viz. symptoms) are hard to distinguish, thus
		  becoming a complex problem to summarize later. A structured
		  counseling conversation may contain discussions about
		  symptoms, history of mental health issues, or the discovery
		  of the patient's behavior. It may also contain discussion
		  filler words irrelevant to a clinical summary. We refer to
		  these elements of structured psychotherapy as counseling
		  components. In this paper, the aim is mental health
		  counseling summarization to build upon domain knowledge and
		  to help clinicians quickly glean meaning. We create a new
		  dataset after annotating 12.9K utterances of counseling
		  components and reference summaries for each dialogue.
		  Further, we propose ConSum, a novel counseling-component
		  guided summarization model. ConSum undergoes three
		  independent modules. First, to assess the presence of
		  depressive symptoms, it filters utterances utilizing the
		  Patient Health Questionnaire (PHQ-9), while the second and
		  third modules aim to classify counseling components. At
		  last, we propose a problem-specific Mental Health
		  Information Capture (MHIC) evaluation metric for counseling
		  summaries. Our comparative study shows that we improve on
		  performance and generate cohesive, semantic, and coherent
		  summaries. We comprehensively analyze the generated
		  summaries to investigate the capturing of psychotherapy
		  elements. Human and clinical evaluations on the summary
		  show that ConSum generates quality summary. Further, mental
		  health experts validate the clinical acceptability of the
		  ConSum. Lastly, we discuss the uniqueness in mental health
		  counseling summarization in the real world and show
		  evidences of its deployment on an online application with
		  the support of mpathic.ai},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3920–3930},
  numpages	= {11},
  keywords	= {dialogue summarization, natural language processing},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3184558.3186906,
  author	= {Yen, Ting-Yu and Lee, Yang-Yin and Huang, Hen-Hsen and
		  Chen, Hsin-Hsi},
  title		= {That Makes Sense: Joint Sense Retrofitting from Contextual
		  and Ontological Information},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3186906},
  doi		= {10.1145/3184558.3186906},
  abstract	= {While recent word embedding models demonstrate their
		  abilities to capture syntactic and semantic information,
		  the demand for sense level embedding is getting higher. In
		  this study, we propose a novel joint sense embedding
		  learning model that retrofits the word representation into
		  sense representation from contextual and ontological
		  information. The experiment shows the effectiveness and
		  robustness of our model that outperforms previous
		  approaches in four public available benchmark datasets.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {15–16},
  numpages	= {2},
  keywords	= {joint sense retrofitting, semantic relatedness, sense
		  embedding},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@Article{	  10.14778/3415478.3415512,
  author	= {Buron, Maxime and Goasdou\'{e}, Fran\c{c}ois and
		  Manolescu, Ioana and Mugnier, Marie-Laure},
  title		= {Obi-Wan: ontology-based RDF integration of heterogeneous
		  data},
  year		= {2020},
  issue_date	= {August 2020},
  publisher	= {VLDB Endowment},
  volume	= {13},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3415478.3415512},
  doi		= {10.14778/3415478.3415512},
  abstract	= {We consider the problem of integrating heterogeneous data
		  (relational, JSON, key-values, graphs etc.) and querying it
		  efficiently. Traditional data integration systems fall into
		  two classes: data warehousing, where all data source
		  content is materialized in a single repository, and
		  mediation, where data remains in their original stores and
		  all data can be queried through a mediator.We propose to
		  demonstrate Obi-Wan, a novel mediator following the
		  Ontology-Based Data access (OBDA) paradigm. Obi-Wan
		  integrates data sources of many data models under an
		  interface based on RDF graphs and ontologies (classes,
		  properties, and relations between them). The novelty of
		  Obi-Wan is to combine maximum integration power (GLAV
		  mappings, see below) with the highest query answering power
		  supported by an RDF mediator: RDF queries not only over the
		  data but also over the integration ontologies. This makes
		  it more flexible and powerful than comparable systems.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {2933–2936},
  numpages	= {4}
}

@InProceedings{	  10.1145/3543507.3583238,
  author	= {Yang, Yuting and Lei, Wenqiang and Huang, Pei and Cao,
		  Juan and Li, Jintao and Chua, Tat-Seng},
  title		= {A Dual Prompt Learning Framework for Few-Shot Dialogue
		  State Tracking},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583238},
  doi		= {10.1145/3543507.3583238},
  abstract	= {Dialogue State Tracking (DST) module is an essential
		  component of task-oriented dialog systems to understand
		  users’ goals and needs. Collecting dialogue state labels
		  including slots and values can be costly, requiring experts
		  to annotate all (slot, value) information for each turn in
		  dialogues. It is also difficult to define all possible
		  slots and values in advance, especially with the wide
		  application of dialogue systems in more and more new-rising
		  applications. In this paper, we focus on improving DST
		  module to generate dialogue states in circumstances with
		  limited annotations and knowledge about slot ontology. To
		  this end, we design a dual prompt learning framework for
		  few-shot DST. The dual framework aims to explore how to
		  utilize the language understanding and generation
		  capabilities of pre-trained language models for DST
		  efficiently. Specifically, we consider the learning of slot
		  generation and value generation as dual tasks, and two
		  kinds of prompts are designed based on this dual structure
		  to incorporate task-related knowledge of these two tasks
		  respectively. In this way, the DST task can be formulated
		  as a language modeling task efficiently under few-shot
		  settings. To evaluate the proposed framework, we conduct
		  experiments on two task-oriented dialogue datasets. The
		  results demonstrate that the proposed method not only
		  outperforms existing state-of-the-art few-shot methods, but
		  also can generate unseen slots. It indicates that
		  DST-related knowledge can be probed from pre-trained
		  language models and utilized to address low-resource DST
		  efficiently with the help of prompt learning.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1468–1477},
  numpages	= {10},
  keywords	= {dialogue state tracking, few-shot learning, prompt
		  learning},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3535511.3535513,
  author	= {Ferreira, Fl\'{a}vio and Duarte, Julio and Ugulino,
		  Wallace},
  title		= {Automated Statistics Extraction of Public Security Events
		  Reported Through Microtexts on Social Networks},
  year		= {2022},
  isbn		= {9781450396981},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3535511.3535513},
  doi		= {10.1145/3535511.3535513},
  abstract	= {Lately, Rio de Janeiro State has been characterized by the
		  occurrence of successive public security events (shootings,
		  assaults, robberies, etc.), causing great insecurity,
		  affecting the daily lives of the population, and worrying
		  public security agencies in the fight against crime.
		  Although the indicators of public security events recently
		  decreased, there is still a feeling of insecurity, while
		  the population uses social networks to notify illegal acts
		  that occurred in their vicinity. Although this
		  collaboration is limited to the crimes that occurred, many
		  published messages are difficult to interpret. Knowledge
		  Discovery is a process of extracting data in an implicit,
		  previously unknown, and useful way that can be applied for
		  different purposes. In this context, Natural Language
		  Processing is a powerful tool that allows the extraction of
		  information from these unstructured data. This work
		  proposes a methodology for automatic knowledge extraction,
		  in the form of statistics related to public security events
		  posted on social networks, particularly the ones occurred
		  in Rio de Janeiro. The main contribution of this work is
		  the proposal of a methodology for the construction of an
		  Information System that allows the collection of statistics
		  of notified public security events. In addition to this
		  methodology, which can also be used in the construction of
		  other Information Systems, this work contributes with a
		  public security event recognition model that has a
		  performance of 95\%, and an available dataset that can be
		  used to support other researches, such as: the
		  identification of new behavior patterns, the discovery of
		  hidden knowledge, among other fronts.},
  booktitle	= {Proceedings of the XVIII Brazilian Symposium on
		  Information Systems},
  articleno	= {2},
  numpages	= {7},
  keywords	= {Twitter., Text Mining, Text Classification, Public
		  Security, Natural Language Processing, Machine Learning,
		  Data Mining, Artificial Intelligence},
  location	= {Curitiba, Brazil},
  series	= {SBSI '22}
}

@InProceedings{	  10.1145/3654522.3654568,
  author	= {Nguyen, Anh Quynh and Tran, My Tu and Nguyen, Quang Nhat
		  and Huynh, Huy Khai and Le, Lan Thi Thu and Quach,
		  Luyl-Da},
  title		= {Classification of Rice Plant Disease Based on Descriptive
		  Information with DistilBERT's Architecture},
  year		= {2024},
  isbn		= {9798400716713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3654522.3654568},
  doi		= {10.1145/3654522.3654568},
  abstract	= {Rice has a significant role in human life, and currently,
		  the issue of rice plant diseases is receiving attention in
		  image-related data processing. However, the possibility of
		  applying text classification to address this issue has yet
		  to be explored. Nonetheless, it deserves attention due to
		  the complexity of image-related data. The study gathered
		  descriptive passages on four prevalent rice diseases in
		  Vietnam, with 365 descriptions. The collected data
		  underwent data preprocessing through stopword removal, then
		  visualization to identify crucial words and phrases for
		  disease identification in rice plants. The study used
		  feature extraction and fine-tuning based on DistilBERT's
		  architecture to build models. The research findings showed
		  an impressive peak accuracy rate of 87\%, highlighting the
		  potential of using text classification algorithms to
		  classify diseases in crops using descriptions.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Intelligent Information Technology},
  pages		= {155–163},
  numpages	= {9},
  keywords	= {DistilBERT, Large Language model,, Rice Disease, Text
		  Classification},
  location	= {Ho Chi Minh City, Vietnam},
  series	= {ICIIT '24}
}

@InProceedings{	  10.1145/3647444.3647871,
  author	= {Pandita, Karan and Thakur, Purab Kulranjan Singh and
		  Annamalai, Suresh},
  title		= {Contextual transcription and Summarization of audio using
		  AI},
  year		= {2024},
  isbn		= {9798400709418},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3647444.3647871},
  doi		= {10.1145/3647444.3647871},
  abstract	= {The field of Natural Language Processing (NLP) has
		  revolutionized the way human language interacts with
		  computer systems. NLP applications span machine
		  translation, information extraction, summarization, and
		  question answering, driven by vast computational resources
		  and big data methodologies. Despite these advancements, NLP
		  tools haven't fully integrated with Internet of Things
		  (IoT) devices, like audio recorders, hindering their
		  accessibility and usability. This paper introduces an
		  innovative solution: a method for audio transcription and
		  contextual summarization using NLP, addressing this gap and
		  enhancing comprehension. Our approach employs cutting-edge
		  NLP techniques, including word embedding methods and
		  knowledge-based graphs, to create a system that efficiently
		  converts audio content into written text and generates
		  coherent summaries. Unlike existing AI tools, our system's
		  summaries are not only accurate but also rich and deep,
		  providing insightful representations of the original
		  content. This depth is achieved through advanced linguistic
		  analysis, surpassing tools like ChatGPT. Furthermore, our
		  system breaks language barriers, enabling multilingual data
		  traversal, enhancing accessibility on a global scale. Our
		  research methodology ensures the system's adherence to
		  industry standards like Request for Comments (RFC) and
		  Constrained Application Protocol (CoAP), guaranteeing
		  interoperability and reliability. By incorporating
		  knowledge-based graphs, our system comprehensively
		  understands audio content, enhancing the accuracy of
		  summarization. This approach addresses the unmet need for
		  seamlessly integrating NLP with IoT devices, making the
		  technology accessible to a broader audience.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Information Management \&amp; Machine Intelligence},
  articleno	= {45},
  numpages	= {9},
  keywords	= {Constrained Application Protocol (CoAP), IOT, NLP, Request
		  for Comments (RFC), audio transcription, contextual
		  summarization, data summarization, knowledge based graphs,
		  word embedding methods},
  location	= {Jaipur, India},
  series	= {ICIMMI '23}
}

@InProceedings{	  10.1145/3372020.3391564,
  author	= {Chondamrongkul, Nacha and Sun, Jing and Warren, Ian and
		  Lee, Scott Uk-Jin},
  title		= {Semantic-based Architecture Smell Analysis},
  year		= {2020},
  isbn		= {9781450370714},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3372020.3391564},
  doi		= {10.1145/3372020.3391564},
  abstract	= {Software smells have negative impacts on the reliability
		  and modifiability of software systems. The smells in
		  architecture design can be cascaded down to the
		  implementation level and cause issues that require much
		  effort to fix. Therefore, early detection of the
		  architecture smells can benefit the overall quality of the
		  software system. This paper presents an integration of
		  methods that formally define the software architecture
		  design towards architecture smell detection. Our approach
		  serves as a framework that allows the architectural
		  structures and behaviours to be formally analysed based on
		  a coherent technique. We evaluated the accuracy and
		  performance of our approach with the models generated from
		  open source projects. The results show that our approach is
		  effective and functions well.},
  booktitle	= {Proceedings of the 8th International Conference on Formal
		  Methods in Software Engineering},
  pages		= {109–118},
  numpages	= {10},
  keywords	= {Software Architecture, Smell Detection, Ontology Web
		  Language, Model Checking, Architecture Smells},
  location	= {Seoul, Republic of Korea},
  series	= {FormaliSE '20}
}

@InProceedings{	  10.1145/3308558.3313476,
  author	= {Chen, Huiyuan and Li, Jing},
  title		= {Modeling Relational Drug-Target-Disease Interactions via
		  Tensor Factorization with Multiple Web Sources},
  year		= {2019},
  isbn		= {9781450366748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308558.3313476},
  doi		= {10.1145/3308558.3313476},
  abstract	= {Modeling the behaviors of drug-target-disease interactions
		  is crucial in the early stage of drug discovery and holds
		  great promise for precision medicine and personalized
		  treatments. The growing availability of new types of data
		  on the internet brings great opportunity of learning a more
		  comprehensive relationship among drugs, targets, and
		  diseases. However, existing methods often consider
		  drug-target interactions or drug-disease interactions
		  separately, which ignores the dependencies among these
		  three entities. Also, many of them cannot directly
		  incorporate rich heterogeneous information from diverse
		  sources. In this work, we investigate the utility of tensor
		  factorization to model the relationships of
		  drug-target-disease, specifically leveraging different
		  types of online data. Our motivation is two-fold. First, in
		  human metabolic systems, many drugs interact with protein
		  targets in cells to modulate target activities, which in
		  turn alter biological pathways to promote healthy functions
		  and to treat diseases. Instead of binary relationships of
		  &lt;drug, disease&gt; or &lt;drug, target&gt;, a tighter
		  triple relationships &lt;drug, target, disease&gt; should
		  be exploited to better understand drug mechanism of actions
		  (MoAs). Second, medical data could be collected from
		  different sources (i.e., drug's chemical structure,
		  target's sequence, or expression measurements). Therefore,
		  effectively exploiting the complementarity among multiple
		  sources is of great importance. Our method elegantly
		  explores a &lt;drug, target, disease&gt; tensor together
		  with complementarity among different data sources, thus
		  improves prediction accuracy. We achieve this goal by
		  formulating the problem into a coupled tensor-matrix
		  factorization problem and directly optimize it on the
		  nonlinear manifold. Experimental results on real-world
		  datasets show that the proposed model outperforms several
		  competitive methods. Our model opens up opportunities to
		  use large Web data to predict drugs' MoAs in
		  pharmacological studies.},
  booktitle	= {The World Wide Web Conference},
  pages		= {218–227},
  numpages	= {10},
  keywords	= {Tensor factorization, Multi-view learning, Manifold
		  optimization, Grassmann manifold, Drug discovery, Disease
		  analysis;},
  location	= {San Francisco, CA, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3323503.3360638,
  author	= {Rolim, Tulio Vidal and Vidal, V\^{a}nia Maria Ponte and
		  Avila, Caio Viktor S. and Cruz, Matheus Mayron Lima da and
		  Barrio, Matheus and Queiroz, Daniel},
  title		= {SemanticSefaz: an ontology-based semantic portal for the
		  government spending},
  year		= {2019},
  isbn		= {9781450367639},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3323503.3360638},
  doi		= {10.1145/3323503.3360638},
  abstract	= {Supervision in the public procurement process is
		  considered essential for society as a means of promoting
		  greater security and control against possible fraud and
		  illegal actions. However, the data available on government
		  procurement alone does not allow for the identification of
		  possible signed contracts or bidding processes won by unfit
		  or suspended companies, making it difficult to analyze and
		  supervise by employees of tax agencies such as SEFAZ. In
		  addition, data are often not available in the same common
		  format and differ in their vocabulary, making it difficult
		  for these professionals to find interesting information. As
		  a means of solving these problems, the present work
		  presents SemanticSefaz, a semantic portal for integration
		  between heterogeneous bases focused on the domain of public
		  procurement through a homogeneous view, allowing for
		  semantic queries and subsequent discovery of information
		  that priori were not possible. As a case study, the
		  databases with data on government procurement (SIASG),
		  unhealthy and suspenseful companies (CEIS) and punished
		  companies (CNEP) were used to construct semantic
		  integration. Subsequently, queries of interest to the tax
		  domain were conducted through SemanticSefaz, demonstrating
		  its efficiency for performing faceted queries and semantic
		  navigation. In the end, SemanticSefaz is characterized as a
		  timely tool for integration, visualization, discovery of
		  knowledge to facilitate the work of tax professionals.},
  booktitle	= {Proceedings of the 25th Brazillian Symposium on Multimedia
		  and the Web},
  pages		= {493–496},
  numpages	= {4},
  keywords	= {semantic web, linked data, government purchasing},
  location	= {Rio de Janeiro, Brazil},
  series	= {WebMedia '19}
}

@InProceedings{	  10.1145/3312714.3312721,
  author	= {Wu, Eureeka Haishang and Wu, Raymond},
  title		= {Collaborative Model of Emerging Technologies in Asia
		  Pacific},
  year		= {2019},
  isbn		= {9781450362351},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3312714.3312721},
  doi		= {10.1145/3312714.3312721},
  abstract	= {Emerging technologies became pervasive in this decade due
		  to its significant growth in terms of high-volume
		  transactions and sophisticated businesses. According to
		  Gartner, public cloud in Asia Pacific will continue its
		  high growth and hit a minimum of $11.5 billion by end of
		  2018. However the challenge can be arise from the other
		  side as only those enterprises who understand agility and
		  collaboration can be the winner. Market power for a single
		  firm is very low in competitive market furthermore, to
		  survive in uprising competition; companies need to
		  differentiate themselves by customizing their products and
		  services, and to quickly adapt themselves into a common
		  platform in the region.To envision the roadmap for regional
		  prosperity, a three-step approach was proposed, to align
		  culture, technologies, and economy into a transformation
		  process, and eventually to achieve a common model of
		  regional collaboration.},
  booktitle	= {Proceedings of the 5th International Conference on
		  E-Society, e-Learning and e-Technologies},
  pages		= {73–77},
  numpages	= {5},
  keywords	= {Innovation, Governance, Emerging Technologies,
		  Consolidation, Collaboration},
  location	= {Vienna, Austria},
  series	= {ICSLT '19}
}

@InProceedings{	  10.1145/3637528.3671793,
  author	= {Gyurek, Croix and Talukder, Niloy and Hasan, Mohammad Al},
  title		= {Binder: Hierarchical Concept Representation through Order
		  Embedding of Binary Vectors},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671793},
  doi		= {10.1145/3637528.3671793},
  abstract	= {For natural language understanding and generation,
		  embedding concepts using an order-based representation is
		  an essential task. Unlike traditional point vector based
		  representation, an order-based representation imposes
		  geometric constraints on the representation vectors for
		  explicitly capturing various semantic relationships that
		  may exist between a pair of concepts. In existing
		  literature, several approaches on order-based embedding
		  have been proposed, mostly focusing on capturing
		  hierarchical relationships; examples include vectors in
		  Euclidean space, complex, Hyperbolic, order, and Box
		  Embedding. Box embedding creates region-based rich
		  representation of concepts, but along the process it
		  sacrifices simplicity, requiring a custom-made optimization
		  scheme for learning the representation. Hyperbolic
		  embedding improves embedding quality by exploiting the
		  ever-expanding property of Hyperbolic space, but it also
		  suffers from the same fate as box embedding as gradient
		  descent like optimization is not simple in the Hyperbolic
		  space. In this work, we propose Binder, a novel approach
		  for order-based representation. Binder uses binary vectors
		  for embedding, so the embedding vectors are compact with an
		  order of magnitude smaller footprint than other methods.
		  Binder uses a simple and efficient optimization scheme for
		  learning representation vectors with a linear time
		  complexity. Our comprehensive experimental results show
		  that Binder is very accurate, yielding competitive results
		  on the representation task. But Binder stands out from its
		  competitors on the transitive closure link prediction task
		  as it can learn concept embeddings just from the direct
		  edges, whereas all existing order-based approaches rely on
		  the indirect edges. In particular, Binder achieves a
		  whopping 70\% higher F1-score than the second best method
		  (98.6\% vs 29\%) in our largest dataset, WordNet Nouns
		  (743,241 edges), when using only direct edges during
		  training.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {980–991},
  numpages	= {12},
  keywords	= {binary vector embedding, concept graph, hierarchical
		  embedding, order embedding},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3544549.3582750,
  author	= {Henriques, Ana O. and Rafael, S\'{o}nia and Almeida,
		  Victor M and Pinto, Jos\'{e} Gomes},
  title		= {The problem with gender-blind design and how we might
		  begin to address it: A model for intersectional feminist
		  ethical deliberation},
  year		= {2023},
  isbn		= {9781450394222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544549.3582750},
  doi		= {10.1145/3544549.3582750},
  abstract	= {Gender-blind design hinges upon an assumption that
		  designing equally is the same as designing for equality.
		  That, however, is inaccurate, as gender-blindness is merely
		  a synonym for neutrality. Neutrality, because it lacks a
		  concerted effort to subvert, favors hegemonic values and
		  epistemologies, which counters the purported aim of
		  equality. Supposedly objective methods of analysis, such as
		  data gathering and interpreting, are not deprived of this
		  hegemonic bias either. As such, through an acknowledgment
		  of ethics, the designer must recognize that they are,
		  indeed, imbuing their values into their designs, which
		  bears influence on the ways in which the user interacts and
		  interprets those designs, a notion which is especially
		  relevant to a field concerned with user experience. This
		  may be done deliberately or by accident, but it is always
		  inevitable. Ethics is, in this way, inextricable from the
		  design process, and, thus, the present article aims to
		  propose that designing for equality requires the designer
		  to act as an ethical agent — responsibly, consciously,
		  and knowingly — especially if one hopes to avoid a design
		  which embodies and communicates oppressive notions. In
		  particular, within the purview of ethics, and by making use
		  of some case-studies and examples, it argues that designing
		  toward gender equality requires not the more typical
		  gender-blind approach, but rather one which is specifically
		  gender-conscious. Further, this article also offers some
		  suggestions as to how we might begin to act as ethical
		  design agents and implement marginalized epistemologies
		  into the design process.},
  booktitle	= {Extended Abstracts of the 2023 CHI Conference on Human
		  Factors in Computing Systems},
  articleno	= {423},
  numpages	= {12},
  keywords	= {Conceptual Model, Ethics, Feminist Design, Gender-Blind
		  Design},
  location	= {Hamburg, Germany},
  series	= {CHI EA '23}
}

@InProceedings{	  10.1145/3486607.3486771,
  author	= {Steimann, Friedrich},
  title		= {The kingdoms of objects and values},
  year		= {2021},
  isbn		= {9781450391108},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486607.3486771},
  doi		= {10.1145/3486607.3486771},
  abstract	= {THE purpose of the following paper is to consider whether
		  there is a fundamental division of the [data] with which
		  [programming] is concerned into two classes, [objects] and
		  [values], or whether there is any method of overcoming this
		  dualism. My own opinion is that the dualism is ultimate; on
		  the other hand, many [colleagues] with whom, in the main, I
		  am in close agreement, hold that it is not ultimate.
		  (paraphrased after Bertrand Russell)},
  booktitle	= {Proceedings of the 2021 ACM SIGPLAN International
		  Symposium on New Ideas, New Paradigms, and Reflections on
		  Programming and Software},
  pages		= {125–135},
  numpages	= {11},
  keywords	= {values, universals, programming languages, particulars,
		  ontology of computing, object, metaphysics},
  location	= {Chicago, IL, USA},
  series	= {Onward! 2021}
}

@Article{	  10.1109/taslp.2022.3224286,
  author	= {Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
  title		= {Minimising Biasing Word Errors for Contextual ASR With the
		  Tree-Constrained Pointer Generator},
  year		= {2022},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3224286},
  doi		= {10.1109/TASLP.2022.3224286},
  abstract	= {Contextual knowledge is essential for reducing speech
		  recognition errors on high-valued long-tail words. This
		  paper proposes a novel tree-constrained pointer generator
		  (TCPGen) component that enables end-to-end ASR models to
		  bias towards a list of long-tail words obtained using
		  external contextual information. With only a small overhead
		  in memory use and computation cost, TCPGen can structure
		  thousands of biasing words efficiently into a symbolic
		  prefix-tree, and creates a neural shortcut between the tree
		  and the final ASR output to facilitate the recognition of
		  the biasing words. To enhance TCPGen, we further propose a
		  novel minimum biasing word error (MBWE) loss that directly
		  optimises biasing word errors during training, along with a
		  biasing-word-driven language model discounting (BLMD)
		  method during the test. All contextual ASR systems were
		  evaluated on the public Librispeech audiobook corpus and
		  the data from the dialogue state tracking challenges (DSTC)
		  with the biasing lists extracted from the dialogue-system
		  ontology. Consistent word error rate (WER) reductions were
		  achieved with TCPGen, which were particularly significant
		  on the biasing words with around 40% relative reductions in
		  the recognition error rates. MBWE and BLMD further improved
		  the effectiveness of TCPGen, and achieved more significant
		  WER reductions on the biasing words. TCPGen also achieved
		  zero-shot learning of words not in the audio training set
		  with large WER reductions on the out-of-vocabulary words in
		  the biasing list.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {345–354},
  numpages	= {10}
}

@InProceedings{	  10.1145/3282373.3282400,
  author	= {Shaaban, Abdelkader Magdy and Schmittner, Christoph and
		  Gruber, Thomas and Mohamed, A. Baith and Quirchmayr, Gerald
		  and Schikuta, Erich},
  title		= {CloudWoT - A Reference Model for Knowledge-based IoT
		  Solutions},
  year		= {2018},
  isbn		= {9781450364799},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3282373.3282400},
  doi		= {10.1145/3282373.3282400},
  abstract	= {Internet technology has changed how people work, live,
		  communicate, learn and entertain. The internet adoption is
		  rising rapidly, thus creating a new industrial revolution
		  named "Industry 4.0". Industry 4.0 is the use of automation
		  and data transfer in manufacturing technologies. It fosters
		  several technological concepts, one of these is the
		  Internet of Things (IoT). IoT technology is based on a big
		  network of machines, objects, or people called "things"
		  interacting together to achieve a common goal. These things
		  are continuously generating vast amounts of data. Data
		  understanding, processing, securing and storing are
		  significant challenges in the IoT technology which
		  restricts its development. This paper presents a new
		  reference IoT model for future smart IoT solutions called
		  Cloud Web of Things (CloudWoT). CloudWoT aims to overcome
		  these limitations by combining IoT with edge computing,
		  semantic web, and cloud computing. Additionally, this work
		  is concerned with the security issues which threatens data
		  in IoT application domains.},
  booktitle	= {Proceedings of the 20th International Conference on
		  Information Integration and Web-Based Applications \&amp;
		  Services},
  pages		= {272–281},
  numpages	= {10},
  keywords	= {Semantic Web, IoT, IACS, Edge Computing, CloudWoT, Cloud
		  Computing, CPPS},
  location	= {Yogyakarta, Indonesia},
  series	= {iiWAS2018}
}

@InProceedings{	  10.1145/3564746.3587001,
  author	= {Adatrao, Naga Sai Krishna and Gadireddy, Gowtham Reddy and
		  Noh, Jiho},
  title		= {A Survey on Conversational Search and Applications in
		  Biomedicine},
  year		= {2023},
  isbn		= {9781450399210},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3564746.3587001},
  doi		= {10.1145/3564746.3587001},
  abstract	= {This paper aims to provide a radical rundown on
		  Conversational Search (ConvSearch), an approach to enhance
		  the information retrieval (IR) method where users engage in
		  a dialogue for the information-seeking tasks. In this
		  survey, we predominantly focused on the human interactive
		  characteristics of the ConvSearch systems, highlighting the
		  operations of the action modules, likely the retrieval
		  system, question-answering, and recommender system. We
		  labeled various ConvSearch research problems in knowledge
		  bases, natural language processing, and dialogue management
		  systems with action modules. We further categorized the
		  framework to ConvSearch, and the application is directed
		  toward biomedical and healthcare fields for the utilization
		  of clinical social technology. Finally, we conclude by
		  talking through the challenges and issues of ConvSearch,
		  particularly in Bio-Medicine. Our main aim is to provide an
		  integrated and unified vision of the ConvSearch components
		  from different fields, which benefit the
		  information-seeking process in healthcare systems.},
  booktitle	= {Proceedings of the 2023 ACM Southeast Conference},
  pages		= {78–88},
  numpages	= {11},
  keywords	= {privacy concerns, biomedical convsearch, generative
		  language models, recommender systems, dialogue management
		  systems, knowledge base, question answering, conversational
		  search, information retrieval},
  location	= {Virtual Event, USA},
  series	= {ACMSE '23}
}

@Proceedings{	  10.1145/3719160,
  title		= {CUI '25: Proceedings of the 7th ACM Conference on
		  Conversational User Interfaces},
  year		= {2025},
  isbn		= {9798400715273},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1109/jcdl57899.2023.00063,
  author	= {Powell, James and Balakireva, Lyudmila},
  title		= {Measuring the Growth of Ideas in a Title Corpus},
  year		= {2024},
  isbn		= {9798350399318},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL57899.2023.00063},
  doi		= {10.1109/JCDL57899.2023.00063},
  abstract	= {Beyond bibliometrics, there is interest in characterizing
		  the evolution of the number of ideas in scientific papers,
		  or more generally, exploring the progress of science. We
		  use various tokenization and phrase extraction strategies
		  combined with lexical diversity metrics to analyze titles
		  in our corpus. We compared four lexical diversity metrics
		  for each corpora variants, to look for indications that new
		  concepts might be emerging over time.},
  booktitle	= {Proceedings of the 2023 ACM/IEEE Joint Conference on
		  Digital Libraries},
  pages		= {291–292},
  numpages	= {2},
  keywords	= {lexical diversity, natural language processing, word
		  embeddings, science of science},
  location	= {Santa Fe, New Mexico, USA},
  series	= {JCDL '23}
}

@InProceedings{	  10.1145/3442442.3451381,
  author	= {Mansar, Youness and Kang, Juyeon and Maarouf, Ismail El},
  title		= {The FinSim-2 2021 Shared Task: Learning Semantic
		  Similarities for the Financial Domain},
  year		= {2021},
  isbn		= {9781450383134},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442442.3451381},
  doi		= {10.1145/3442442.3451381},
  abstract	= {The FinSim-2 is a second edition of FinSim Shared Task on
		  Learning Semantic Similarities for the Financial Domain,
		  colocated with the FinWeb workshop. FinSim-2 proposed the
		  challenge to automatically learn effective and precise
		  semantic models for the financial domain. The second
		  edition of the FinSim offered an enriched dataset in terms
		  of volume and quality, and interested in systems which make
		  creative use of relevant resources such as ontologies and
		  lexica, as well as systems which make use of contextual
		  word embeddings such as BERT[4]. Going beyond the mere
		  representation of words is a key step to industrial
		  applications that make use of Natural Language Processing
		  (NLP). This is typically addressed using either
		  unsupervised corpus-derived representations like word
		  embeddings, which are typically opaque to human
		  understanding but very useful in NLP applications or
		  manually created resources such as taxonomies and
		  ontologies, which typically have low coverage and contain
		  inconsistencies, but provide a deeper understanding of the
		  target domain. Finsim is inspired from previous endeavours
		  in the Semeval community, which organized several
		  competitions on semantic/lexical relation extraction
		  between concepts/words. This year, 18 system runs were
		  submitted by 7 teams and systems were ranked according to 2
		  metrics, Accuracy and Mean rank. All the systems beat our
		  baseline 1 model by over 15 points and the best systems
		  beat the baseline 2 by over 1 ∼ 3 points in accuracy.},
  booktitle	= {Companion Proceedings of the Web Conference 2021},
  pages		= {288–292},
  numpages	= {5},
  keywords	= {Word embeddings, Natural Language Processing,
		  Hypernym-hyponym relation extraction, Financial documents
		  processing, Domain specific ontology},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3411170.3411261,
  author	= {Giallonardo, Ester and Poggi, Francesco and Rossi, Davide
		  and Zimeo, Eugenio},
  title		= {Making Smart Buildings and Personal Systems Cooperate via
		  Knowledge Base Overlays},
  year		= {2020},
  isbn		= {9781450375597},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411170.3411261},
  doi		= {10.1145/3411170.3411261},
  abstract	= {Reactive IoT applications often have to deal with the data
		  source Babel arising from their need to operate on context
		  information originated from different data sources.
		  Semantic knowledge bases can be fruitfully deployed to
		  alleviate this problem: they provide a unified access point
		  for context information including both long term (such as
		  the structure of the environment) and transient (such as
		  sensor readings) data thanks to their ability to host
		  elements responding to different schemas within the same
		  container. In previous works we introduced an architecture
		  to create reactive IoT systems based on a semantic
		  knowledge base that also hosts the definition of their
		  behavior and on an accompanying reactive machinery. In this
		  paper, we introduce the use of knowledge base overlays,
		  i.e. containers providing a live, unified view over (parts
		  of) different underlying knowledge bases, as a mechanism to
		  enable interoperation between multiple IoT semantics-based
		  systems. Specifically we explore the benefits of this
		  approach in a case study in which a semantic IoT system
		  governing a smart building interacts with the personal
		  semantic systems of the people entering the building.},
  booktitle	= {Proceedings of the 6th EAI International Conference on
		  Smart Objects and Technologies for Social Good},
  pages		= {181–186},
  numpages	= {6},
  keywords	= {Smart moving, Smart Environment, Semantic modeling,
		  Semantic Sensor Networks, Reactive systems, Ontologies,
		  Models@runtime, Internet of Things (IoT),
		  Context-awareness, Context modeling},
  location	= {Antwerp, Belgium},
  series	= {GoodTechs '20}
}

@InProceedings{	  10.1145/3344341.3368806,
  author	= {Arshad, Bilal and Anjum, Ashiq},
  title		= {High Performance Dynamic Graph Model for Consistent Data
		  Integration},
  year		= {2019},
  isbn		= {9781450368940},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3344341.3368806},
  doi		= {10.1145/3344341.3368806},
  abstract	= {In a distributed environment, data from heterogeneous
		  sources are brought together in a unified and consistent
		  manner for analytics and insights. Inconsistencies arising
		  due to the dynamic nature of sources such as
		  addition/deletion of column or merging of columns can
		  compromise the consistency of the distributed system. This
		  can lead to the linking of inaccurate records and faulty
		  data entries. Resulting in false reports and erroneous
		  analyses. Furthermore, issues such as performance
		  guarantees and scalability fuel the existing challenges. We
		  have proposed an alternate graph-based approach to
		  integrate data using an in-memory environment. The central
		  idea of the approach is the use of graphs to integrate
		  heterogeneous data sources in a distributed environment.
		  The underlying approach provides both high-performance and
		  scalability to address changes in a dynamic system for data
		  integration. This allows the generation of graphs from
		  individual source data and modifications in a consistent
		  manner so that the state of the overall distributed system
		  always remains coherent. It provides a novel way of
		  combining consistent data integration and performance in a
		  distributed sys-tem. Our system performs better than
		  existing graph systems for dynamic graph evolution ensuring
		  consistency and provides the necessary scalability
		  guarantees as the size of the data increases. Results also
		  show the correctness of the approach when integrating
		  disparate data-sets},
  booktitle	= {Proceedings of the 12th IEEE/ACM International Conference
		  on Utility and Cloud Computing},
  pages		= {263–272},
  numpages	= {10},
  keywords	= {scalability, performance, graphs, dynamic graphs, data
		  integration, consistency},
  location	= {Auckland, New Zealand},
  series	= {UCC'19}
}

@InProceedings{	  10.1145/3701716.3715459,
  author	= {Taffa, Tilahun Abedissa and Usbeck, Ricardo},
  title		= {Bridge-Generate: Scholarly Hybrid Question Answering},
  year		= {2025},
  isbn		= {9798400713316},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701716.3715459},
  doi		= {10.1145/3701716.3715459},
  abstract	= {Answering scholarly hybrid questions requires access to
		  bibliographic facts stored in structured data, such as a
		  Knowledge Graph (KG) and textual information. Existing
		  Scholarly Hybrid Question Answering (SHQA) approaches rely
		  on retrieving KG triples and documents from the Wikipedia
		  text corpus and prompt an LLM (Large Language Model) for
		  answers. However, the retrieval is heavily keyword-based,
		  introducing noise into the context. Furthermore, despite
		  detecting the entities in the question, the models do not
		  attempt any question analysis. Therefore, we propose a new
		  SHQA system that employs a bridge-generate approach. During
		  the bridge phase, our system recursively identifies
		  entity-encapsulating phrases within the question and
		  resolves the entities leveraging the underlying KGs. It
		  then formulates assertion statements based on the resolved
		  entities and their corresponding phrases. In the generation
		  phase, the system auto-generates context guided by the
		  question and the assertions. Finally, it returns an answer
		  prompting an LLM with the generated context, the
		  assertions, and the question. Our approach outperforms
		  previous approaches, addressing the identified gaps.},
  booktitle	= {Companion Proceedings of the ACM on Web Conference 2025},
  pages		= {1321–1325},
  numpages	= {5},
  keywords	= {hybrid question answering, question answering, scholarly
		  hybrid question answering, scholarly question answering},
  location	= {Sydney NSW, Australia},
  series	= {WWW '25}
}

@InProceedings{	  10.1145/3652620.3688216,
  author	= {Fadhlillah, Hafiyyan Sayyid and Greiner, Sandra and
		  Feichtinger, Kevin and Rabiser, Rick and Zoitl, Alois},
  title		= {Managing Variability of Cyber-Physical Production Systems:
		  Towards Consistency Management},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688216},
  doi		= {10.1145/3652620.3688216},
  abstract	= {Engineering Cyber-Physical Production Systems (CPPSs)
		  involves several different disciplines, where team members
		  range from mechanical, electrical, and automation
		  engineers, to control software engineers. When developing
		  variability-intensive software systems such as CPPSs,
		  engineers create heterogeneous engineering artifacts of
		  varying granularity, structure, and level of abstraction in
		  the problem and solution space, e.g., CAD drawings, delta
		  models, and control software artifacts. Managing
		  consistency among these heterogeneous artifacts is
		  essential during the development and maintenance of these
		  systems to reduce development costs and runtime failures.
		  Software product line engineering provides approaches to
		  manage the variability of heterogeneous artifacts. However,
		  these approaches must be adapted and extended to manage
		  consistency in CPPSs and address the additional
		  multidimensional challenges in CPPSs. In this short paper,
		  we outline these challenges, motivate them using a case
		  study, and discuss potential solutions to manage the
		  consistency of engineering artifacts expressing CPPS
		  control software variability. We thereby lay the grounds
		  for a deeper understanding of possible inconsistencies and
		  exploring new methods for managing consistency in control
		  software variability in CPPSs.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {945–949},
  numpages	= {5},
  keywords	= {cyber-physical production systems engineering,
		  heterogeneous multi-modeling, software modeling
		  consistency},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3589334.3645649,
  author	= {Gong, Jiaying and Eldardiry, Hoda},
  title		= {Multi-Label Zero-Shot Product Attribute-Value Extraction},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645649},
  doi		= {10.1145/3589334.3645649},
  abstract	= {E-commerce platforms should provide detailed product
		  descriptions (attribute values) for effective product
		  search and recommendation. However, attribute value
		  information is typically not available for new products. To
		  predict unseen attribute values, large quantities of
		  labeled training data are needed to train a traditional
		  supervised learning model. Typically, it is difficult,
		  time-consuming, and costly to manually label large
		  quantities of new product profiles. In this paper, we
		  propose a novel method to efficiently and effectively
		  extract unseen attribute values from new products in the
		  absence of labeled data (zero-shot setting). We propose
		  HyperPAVE, a multi-label zero-shot attribute value
		  extraction model that leverages inductive inference in
		  heterogeneous hypergraphs. In particular, our proposed
		  technique constructs heterogeneous hypergraphs to capture
		  complex higher-order relations (i.e. user behavior
		  information) to learn more accurate feature representations
		  for graph nodes. Furthermore, our proposed HyperPAVE model
		  uses an inductive link prediction mechanism to infer future
		  connections between unseen nodes. This enables HyperPAVE to
		  identify new attribute values without the need for labeled
		  training data. We conduct extensive experiments with
		  ablation studies on different categories of the MAVE
		  dataset. The results demonstrate that our proposed
		  HyperPAVE model significantly outperforms existing
		  classification-based, generation-based large language
		  models for attribute value extraction in the zero-shot
		  setting.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2259–2270},
  numpages	= {12},
  keywords	= {attribute value extraction, heterogeneous hypergraph,
		  inductive link prediction, zero-shot learning},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3167132.3167287,
  author	= {Autili, Marco and Di Salle, Amleto and Gallo, Francesco
		  and Pompilio, Claudio and Tivoli, Massimo},
  title		= {Model-driven adaptation of service choreographies},
  year		= {2018},
  isbn		= {9781450351911},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167132.3167287},
  doi		= {10.1145/3167132.3167287},
  abstract	= {Service choreographies represent a powerful and flexible
		  approach to compose software services in a fully
		  distributed way. A key enabler for the actual realization
		  of choreographies is the ability to automatically compose
		  services, and perform exogenous coordination and adaptation
		  of their interaction. This is a nontrivial and error prone
		  task. Automatic support for realizing choreographies is
		  needed. In this paper we focus on adapter generation and
		  describe our novel approach to the synthesis of service
		  Adapters. When needed, adapters permit to correctly bind
		  concrete services to (abstract) choreography roles by
		  solving possible protocol mismatches. Enterprise
		  Integration Patterns are used as adaptation primitives and
		  composed to realize complex adaptation policies.},
  booktitle	= {Proceedings of the 33rd Annual ACM Symposium on Applied
		  Computing},
  pages		= {1441–1450},
  numpages	= {10},
  keywords	= {service-oriented computing, service choreography,
		  model-driven, enterprise integration pattern, adaptation},
  location	= {Pau, France},
  series	= {SAC '18}
}

@InProceedings{	  10.1145/3594536.3595124,
  author	= {van Drie, Romy A. N. and de Boer, Maaike H. T. and Bakker,
		  Roos M. and Tolios, Ioannis and Vos, Daan},
  title		= {The Dutch Law as a Semantic Role Labeling Dataset},
  year		= {2023},
  isbn		= {9798400701979},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3594536.3595124},
  doi		= {10.1145/3594536.3595124},
  abstract	= {Legal documents, and specifically law texts, are not easy
		  to understand by humans. The specific terminology and
		  sentence constructions are particular, which also makes it
		  a difficult machine understanding task. In this paper, we
		  present a publicly available benchmark dataset containing
		  Dutch law texts which can be used to train AI models that
		  assist humans equipped with the task of interpreting legal
		  texts. However, the dataset can be used in a broader
		  context, such as semantic role labeling of Dutch (legal)
		  texts. Our dataset contains 4463 annotated sentences from
		  55 different Dutch laws, in which four roles are annotated
		  by human annotators: action, actor, object and recipient.
		  The inter-annotator agreement is substantial (κ=0.75). In
		  experiments with a rule-based and a transformer-based
		  method, results show that the transformer-based method
		  performs quite well on the dataset (accuracy &gt; 0.8).
		  These results indicate that we can reliably predict
		  actions, actors, objects and recipients in legal texts.
		  This can help people equipped with the task of formal
		  interpretation of legal texts.},
  booktitle	= {Proceedings of the Nineteenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {316–322},
  numpages	= {7},
  keywords	= {datasets, legal interpretation, legal text, natural
		  language processing, norms, semantic role labeling},
  location	= {Braga, Portugal},
  series	= {ICAIL '23}
}

@Article{	  10.1145/3585387,
  author	= {Garc\'{\i}a, Roberto and Cediel, Ana and Teixid\'{o},
		  Merc\`{e} and Gil, Rosa},
  title		= {Semantics and Non-fungible Tokens for Copyright Management
		  on the Metaverse and Beyond},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {7},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3585387},
  doi		= {10.1145/3585387},
  abstract	= {Recent initiatives related to the Metaverse focus on
		  better visualization, like augmented or virtual reality,
		  but also persistent digital objects. To guarantee real
		  ownership of these digital objects, open systems based on
		  public blockchains and Non-Fungible Tokens (NFTs) are
		  emerging together with a nascent decentralized and open
		  creator economy. To manage this emerging economy in a more
		  organized way, and fight the so common NFT plagiarism, we
		  propose CopyrightLY, a decentralized application for
		  authorship and copyright management. It provides means to
		  claim content authorship, including supporting evidence.
		  Content and metadata are stored in decentralized storage
		  and registered on the blockchain. A token is used to curate
		  these claims, and potential complaints, by staking it on
		  them. Staking is incentivized by the fact that the token is
		  minted using a bonding curve. The tokenomics include the
		  resolution of complaints and enabling the monetization of
		  curated claims. Monetization is achieved through licensing
		  NFTs with metadata enhanced by semantic technologies.
		  Semantic data makes explicit the reuse conditions
		  transferred with the token while keeping the connection to
		  the underlying copyright claims to improve the trustability
		  of the NFTs. Moreover, the semantic metadata is flexible
		  enough to enable licensing not just in the real world.
		  Licenses can refer to reuses in specific locations in a
		  metaverse, thus facilitating the emergence of creative
		  economies in them.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= mar,
  articleno	= {186},
  numpages	= {20},
  keywords	= {Metaverse, Non-Fungible Token, copyright, social media,
		  blockchain, ontology}
}

@Article{	  10.1145/3230713,
  author	= {Russo, Daniel and Ciancarini, Paolo and Falasconi, Tommaso
		  and Tomasi, Massimo},
  title		= {A Meta-Model for Information Systems Quality: A Mixed
		  Study of the Financial Sector},
  year		= {2018},
  issue_date	= {September 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {3},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3230713},
  doi		= {10.1145/3230713},
  abstract	= {Information Systems Quality (ISQ) is a critical source of
		  competitive advantages for organizations. In a scenario of
		  increasing competition on digital services, ISQ is a
		  competitive differentiation asset. In this regard,
		  managing, maintaining, and evolving IT infrastructures have
		  become a primary concern of organizations. Thus, a
		  technical perspective on ISQ provides useful guidance to
		  meet current challenges. The financial sector is
		  paradigmatic, since it is a traditional business, with
		  highly complex business-critical legacy systems, facing a
		  tremendous change due to market and regulation drivers. We
		  carried out a Mixed-Methods study, performing a Delphi-like
		  study on the financial sector. We developed a specific
		  research framework to pursue this vertical study. Data were
		  collected in four phases starting with a high-level
		  randomly stratified panel of 13 senior managers and then a
		  target panel of 124 carefully selected and well-informed
		  domain experts. We have identified and dealt with several
		  quality factors; they were discussed in a comprehensive
		  model inspired by the ISO 25010, 42010, and 12207
		  standards, corresponding to software quality, software
		  architecture, and software process, respectively. Our
		  results suggest that the relationship among quality,
		  architecture, and process is a valuable technical
		  perspective to explain the quality of an information
		  system. Thus, we introduce and illustrate a novel
		  meta-model, named SQuAP (Software Quality, Architecture,
		  Process), which is intended to give a comprehensive picture
		  of ISQ by abstracting and connecting detailed individual
		  ISO models.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= sep,
  articleno	= {11},
  numpages	= {38},
  keywords	= {software quality, software process, software architecture,
		  mixed methods, management information systems, delphi
		  study, Information systems quality}
}

@InProceedings{	  10.1145/3570945.3607343,
  author	= {Aicher, Annalena and Weber, Klaus and Andr\'{e}, Elisabeth
		  and Minker, Wolfgang and Ultes, Stefan},
  title		= {The Influence of Avatar Interfaces on Argumentative
		  Dialogues},
  year		= {2023},
  isbn		= {9781450399944},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570945.3607343},
  doi		= {10.1145/3570945.3607343},
  abstract	= {Humans form opinions and justify different points of view
		  by exchanging arguments and knowledge. Likewise to
		  human-human interaction, the way arguments are presented
		  influence the user's willingness to engage into a critical
		  reflection. Especially when interacting with conversational
		  agents the user's engagement and motivation are important
		  factors and highly influence the success or failure of such
		  a mixed team. To maintain the users' trust and
		  satisfaction, the users' perception of the respective
		  system is an important indicator. Thus, this work
		  investigates the design of a cooperative argumentative
		  dialogue system using a virtual avatar compared to a
		  non-avatar interface by evaluating a crowdsourcing study
		  conducted with 84 participants. The results indicate, that
		  the avatar system is perceived as significantly more
		  appealing and natural and thus, engaging which also
		  influences the acceptance and perception of the quality of
		  presented arguments. Furthermore, we found that the
		  presence of the avatar often led to an increase in the
		  anticipated level of conversational proficiency similar to
		  that of a human interlocutor. Therefore, this work provides
		  important insights for the design of future cooperative
		  argumentative virtual avatar interfaces.},
  booktitle	= {Proceedings of the 23rd ACM International Conference on
		  Intelligent Virtual Agents},
  articleno	= {24},
  numpages	= {8},
  keywords	= {Avatar Interface, Conversational Engagement, Crowdsourcing
		  Study, Human-Computer Interaction, User Trust},
  location	= {W\"{u}rzburg, Germany},
  series	= {IVA '23}
}

@InProceedings{	  10.1145/3689050.3704801,
  author	= {Bertmark, Anna My},
  title		= {Artefacts as Pedagogy for Futuring},
  year		= {2025},
  isbn		= {9798400711978},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689050.3704801},
  doi		= {10.1145/3689050.3704801},
  abstract	= {This PhD explores the potential of interactive artefacts
		  as tangible pedagogy for generating perspective shifts
		  towards ecocentric thinking and doing. Research has
		  highlighted how deliberate societal transformative change
		  is required to reach the targets of sustainability goals
		  and how perspective shifts for instigating these changes
		  may be supported and achieved. The research investigates
		  how design and HCI seek to challenge anthropocentric
		  approaches and perspectives, motivated by the growing
		  number of works in technology for the more-than-human and
		  feminist post-humanism. Speculative, more-than-human design
		  and sustainable HCI highlight ecosystem interdependence
		  through transmedia narratives and interactive ecocentric
		  artefacts. However, their inaccessibility and unexplored
		  impact limit their potential to bridge the knowledge-action
		  gap for local development to stay within necessary
		  earth-system boundaries. This paper emphasises the
		  necessity for perceiving how interactive artefacts generate
		  epistemological and ontological shifts and how this may be
		  utilised to advise future design towards ethics of care and
		  regenerative development.},
  booktitle	= {Proceedings of the Nineteenth International Conference on
		  Tangible, Embedded, and Embodied Interaction},
  articleno	= {120},
  numpages	= {9},
  keywords	= {Design ontology, Ecocentric design, Ecological HCI,
		  Embodied learning, Futuring, Interaction for
		  sustainability},
  location	= { },
  series	= {TEI '25}
}

@InProceedings{	  10.1145/3472306.3478360,
  author	= {Ishii, Ryo and Ren, Xutong and Muszynski, Michal and
		  Morency, Louis-Philippe},
  title		= {Multimodal and Multitask Approach to Listener's
		  Backchannel Prediction: Can Prediction of Turn-changing and
		  Turn-management Willingness Improve Backchannel Modeling?},
  year		= {2021},
  isbn		= {9781450386197},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3472306.3478360},
  doi		= {10.1145/3472306.3478360},
  abstract	= {The listener's backchannel has the important function of
		  encouraging a current speaker to hold their turn and
		  continue to speak, which enables smooth conversation. The
		  listener monitors the speaker's turn-management (a.k.a.
		  speaking and listening) willingness and his/her own
		  willingness to display backchannel behavior. Many studies
		  have focused on predicting the appropriate timing of the
		  backchannel so that conversational agents can display
		  backchannel behavior in response to a user who is speaking.
		  To the best of our knowledge, none of them added the
		  prediction of turn-changing and participants'
		  turn-management willingness to the backchannel prediction
		  model in dyad interactions. In this paper, we proposed a
		  novel backchannel prediction model that can jointly predict
		  turn-changing and turn-management willingness. We
		  investigated the impact of modeling turn-changing and
		  willingness to improve backchannel prediction. Our proposed
		  model is based on trimodal inputs, that is, acoustic,
		  linguistic, and visual cues from conversations. Our results
		  suggest that adding turn-management willingness as a
		  prediction task improves the performance of backchannel
		  prediction within the multi-modal multi-task learning
		  approach, while adding turn-changing prediction is not
		  useful for improving the performance of backchannel
		  prediction.},
  booktitle	= {Proceedings of the 21st ACM International Conference on
		  Intelligent Virtual Agents},
  pages		= {131–138},
  numpages	= {8},
  keywords	= {turn-management willingness, turn-changing, multitask
		  learning, multimodal signal processing, backchannel},
  location	= {Virtual Event, Japan},
  series	= {IVA '21}
}

@Article{	  10.1145/3539608,
  author	= {Anwar, Sibgha and Beg, Mirza Omer and Saleem, Kiran and
		  Ahmed, Zeeshan and Javed, Abdul Rehman and Tariq, Usman},
  title		= {Social Relationship Analysis Using State-of-the-art
		  Embeddings},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3539608},
  doi		= {10.1145/3539608},
  abstract	= {Detection of human relationships from their interactions
		  on social media is a challenging problem with a wide range
		  of applications in different areas, like targeted
		  marketing, cyber-crime, fraud, defense, planning, and human
		  resource, to name a few. All previous work in this area has
		  only dealt with the most basic types of relationships. The
		  proposed approach goes beyond the previous work to
		  efficiently handle the hierarchy of social relationships.
		  This article introduces a novel technique named
		  Quantifiable Social Relationship (QSR) analysis for
		  quantifying social relationships to analyze relationships
		  between agents from their textual conversations. QSR uses
		  cross-disciplinary techniques from computational
		  linguistics and cognitive psychology to identify
		  relationships. QSR utilizes sentiment and behavioral styles
		  displayed in the conversations for mapping them onto level
		  II relationship categories. Then, for identifying the level
		  III relationship categories, QSR uses level II
		  relationships, sentiments, interactions, and word
		  embeddings as key features. QSR employs natural language
		  processing techniques for feature engineering and
		  state-of-the-art embeddings generated by word2vec, global
		  vectors (glove), and bidirectional encoder representations
		  from transformers (bert). QSR combines the intrinsic
		  conversational features with word embeddings for
		  classifying relationships. QSR achieves an accuracy of up
		  to 89\% for classifying relationship subtypes. The
		  evaluation shows that QSR can accurately identify the
		  hierarchical relationships between agents by extracting
		  intrinsic and extrinsic features from textual conversations
		  between agents.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {138},
  numpages	= {21},
  keywords	= {machine learning, behavioral model, quantifiable
		  relationships, hierarchical relationship analysis, social
		  relationship, Agents interaction model}
}

@Article{	  10.1145/3556538,
  author	= {Benedetto, Luca and Cremonesi, Paolo and Caines, Andrew
		  and Buttery, Paula and Cappelli, Andrea and Giussani,
		  Andrea and Turrin, Roberto},
  title		= {A Survey on Recent Approaches to Question Difficulty
		  Estimation from Text},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3556538},
  doi		= {10.1145/3556538},
  abstract	= {Question Difficulty Estimation from Text (QDET) is the
		  application of Natural Language Processing techniques to
		  the estimation of a value, either numerical or categorical,
		  which represents the difficulty of questions in educational
		  settings. We give an introduction to the field, build a
		  taxonomy based on question characteristics, and present the
		  various approaches that have been proposed in recent years,
		  outlining opportunities for further research. This survey
		  provides an introduction for researchers and practitioners
		  into the domain of question difficulty estimation from text
		  and acts as a point of reference about recent research in
		  this topic to date.},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  articleno	= {178},
  numpages	= {37},
  keywords	= {student assessment, question calibration, Question
		  difficulty estimation}
}

@InProceedings{	  10.1145/3634737.3645000,
  author	= {Kumarasinghe, Udesh and Lekssays, Ahmed and Sencar, Husrev
		  Taha and Boughorbel, Sabri and Elvitigala, Charitha and
		  Nakov, Preslav},
  title		= {Semantic Ranking for Automated Adversarial Technique
		  Annotation in Security Text},
  year		= {2024},
  isbn		= {9798400704826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3634737.3645000},
  doi		= {10.1145/3634737.3645000},
  abstract	= {We introduce a novel approach for mapping attack behaviors
		  described in threat analysis reports to entries in an
		  adversarial techniques knowledge base. Our method leverages
		  a multi-stage ranking architecture to efficiently rank the
		  most related techniques based on their semantic relevance
		  to the input text. Each ranker in our pipeline uses a
		  distinct design for text representation. To enhance
		  relevance modeling, we leverage pretrained language models,
		  which we fine-tune for the technique annotation task. While
		  generic large language models are not yet capable of fully
		  addressing this challenge, we obtain very promising
		  results. We achieve a recall rate improvement of +35\%
		  compared to the previous state-of-the-art results. We
		  further create new public benchmark datasets for training
		  and validating methods in this domain, which we release to
		  the research community aiming to promote future research in
		  this important direction.},
  booktitle	= {Proceedings of the 19th ACM Asia Conference on Computer
		  and Communications Security},
  pages		= {49–62},
  numpages	= {14},
  keywords	= {threat intelligence, TTP annotation, text ranking, text
		  attribution},
  location	= {Singapore, Singapore},
  series	= {ASIA CCS '24}
}

@InBook{	  10.1145/3233795.3233805,
  author	= {Hornung, Rachel and Chen, Nutan and van der Smagt,
		  Patrick},
  title		= {Early integration for movement modeling in latent spaces},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan \&amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233805},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {305–345},
  numpages	= {41}
}

@InProceedings{	  10.1145/3543507.3587428,
  author	= {de Berardinis, Jacopo and Mero\~{n}o-Pe\~{n}uela, Albert
		  and Poltronieri, Andrea and Presutti, Valentina},
  title		= {The Harmonic Memory: a Knowledge Graph of harmonic
		  patterns as a trustworthy framework for computational
		  creativity},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3587428},
  doi		= {10.1145/3543507.3587428},
  abstract	= {Computationally creative systems for music have recently
		  achieved impressive results, fuelled by progress in
		  generative machine learning. However, black-box approaches
		  have raised fundamental concerns for ethics,
		  accountability, explainability, and musical plausibility.
		  To enable trustworthy machine creativity, we introduce the
		  Harmonic Memory, a Knowledge Graph (KG) of harmonic
		  patterns extracted from a large and heterogeneous musical
		  corpus. By leveraging a cognitive model of tonal harmony,
		  chord progressions are segmented into meaningful
		  structures, and patterns emerge from their comparison via
		  harmonic similarity. Akin to a music memory, the KG holds
		  temporal connections between consecutive patterns, as well
		  as salient similarity relationships. After demonstrating
		  the validity of our choices, we provide examples of how
		  this design enables novel pathways for combinational
		  creativity. The memory provides a fully accountable and
		  explainable framework to inspire and support creative
		  professionals – allowing for the discovery of
		  progressions consistent with given criteria, the
		  recomposition of harmonic sections, but also the
		  co-creation of new progressions.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {3873–3882},
  numpages	= {10},
  keywords	= {computational creativity, knowledge graphs, music
		  technology},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3592813.3592915,
  author	= {Izo, Flavio and Vereau, Luis Enrique Santos Prado and
		  Pirovani, Juliana Pinheiro Campos and Oliveira, Elias and
		  Badue, Claudine},
  title		= {An Intelligent Report Generator for Chemical Documents},
  year		= {2023},
  isbn		= {9798400707599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3592813.3592915},
  doi		= {10.1145/3592813.3592915},
  abstract	= {Context: Scientific articles and patents contain academic,
		  industrial, and scientific information. Automatically
		  retrieving information from these documents is necessary
		  for supporting upcoming scientific research development.
		  Problem: Difficulties in manually identifying and analyzing
		  the chemical information in documents make it nearly
		  impossible to access specific contents of chemical
		  investigations and generate reports to support ongoing
		  research. Solution: In this article, we present a system
		  that recognizes chemical entities (elements, classes,
		  compounds, methods, and equipment) and generates
		  intelligent reports from free texts. IS Theory: We
		  developed this work under the support of Soft Systems
		  Theory. Method: This research was evaluated through proof
		  of concept. We used 30 chemical patents from Brazilian
		  National Institute of Industrial Property and 20 scientific
		  articles from Revista Virtual de Qu\'{\i}mica (RVq). For
		  validation, we extracted the texts and recognized the named
		  entities through, for instance, the hybrid method
		  Conditional Random Field (CRF) + Local Grammar (LG). We
		  then apply rules to generate intelligent reports. Summary
		  of Results: The system can generate seven types of
		  intelligent reports, two of which are customized by the
		  user. For datasetPat our model obtained mean values of
		  98.96\% for Precision, 91.12\% for Recall, and 94.17\% for
		  F-Score. The datasetArt reached average values of 97.31\%,
		  86.94\%, and 91.29\% for Precision, Recall, and F-Score,
		  respectively. Contributions and Impact in the IS Area: This
		  research presents as the main contribution the availability
		  of an Information System for the generation of intelligent
		  reports from documents based on the recognition of named
		  entities in the chemical area. In addition the hybrid
		  method CRF+LG can contribute to the evolution of
		  Information Systems, helping people and organizations. The
		  model is described throughout the paper and can be
		  replicated in other contexts.},
  booktitle	= {Proceedings of the XIX Brazilian Symposium on Information
		  Systems},
  pages		= {276–283},
  numpages	= {8},
  keywords	= {Natural Language Processing, Named Entity Recognition,
		  Intelligent Report., Artificial Intelligence},
  location	= {Macei\'{o}, Brazil},
  series	= {SBSI '23}
}

@InProceedings{	  10.1145/3477314.3507256,
  author	= {Kanwal, Neel and Rizzo, Giuseppe},
  title		= {Attention-based clinical note summarization},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3507256},
  doi		= {10.1145/3477314.3507256},
  abstract	= {In recent years, the trend of deploying digital systems in
		  numerous industries has hiked. The health sector has
		  observed an extensive adoption of digital systems and
		  services that generate significant medical records.
		  Electronic health records contain valuable information for
		  prospective and retrospective analysis that is often not
		  entirely exploited because of the complicated dense
		  information storage. The crude purpose of condensing health
		  records is to select the information that holds most
		  characteristics of the original documents based on a
		  reported disease. These summaries may boost diagnosis and
		  save a doctor's time during a saturated workload situation
		  like the COVID-19 pandemic. In this paper, we are applying
		  a multi-head attention-based mechanism to perform
		  extractive summarization of meaningful phrases on clinical
		  notes. Our method finds major sentences for a summary by
		  correlating tokens, segments, and positional embeddings of
		  sentences in a clinical note. The model outputs attention
		  scores that are statistically transformed to extract
		  critical phrases for visualization on the heat-mapping tool
		  and for human use.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {813–820},
  numpages	= {8},
  keywords	= {ICD-9, MIMIC-III, clinical notes, deep learning,
		  electronic health records, extractive summarization,
		  information extraction, medical records, multi-head
		  attention, natural language processing, transformer
		  models},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@InProceedings{	  10.1145/3430984.3431049,
  author	= {R, Tharaniya Sairaj and S. R., Balasundaram},
  title		= {An Entailment Analysis Based Entity Mapping To Improve
		  Automatic Question Generation},
  year		= {2021},
  isbn		= {9781450388177},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3430984.3431049},
  doi		= {10.1145/3430984.3431049},
  abstract	= {Automatic generation of assessment questions to enhance
		  learning requires natural language understanding. To
		  enhance the process, the entities extracted from the
		  subject domain, may be mapped to form knowledge graphs. But
		  entity mapping faces the challenge of analysing entailment.
		  The proposed work analyses the semantic relevance in the
		  process and aims at generating heuristics to map
		  entities.},
  booktitle	= {Proceedings of the 3rd ACM India Joint International
		  Conference on Data Science \&amp; Management of Data (8th
		  ACM IKDD CODS \&amp; 26th COMAD)},
  pages		= {424},
  numpages	= {1},
  keywords	= {Ontology, Natural Language Understanding, Entity Mapping,
		  Entailment Analysis, E-Assessment},
  location	= {Bangalore, India},
  series	= {CODS-COMAD '21}
}

@InProceedings{	  10.1145/3731763.3731782,
  author	= {Dang, Dung Thi and Nguyen, Khoi Tan and Huynh, Hiep Xuan},
  title		= {Efficient Object Detection Using Total Energy Function: An
		  Alternative to Anchor Box},
  year		= {2025},
  isbn		= {9798400710841},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3731763.3731782},
  doi		= {10.1145/3731763.3731782},
  abstract	= {Bounding box regression is a popular technique for
		  fine-tuning or predicting the localization boxes in object
		  detection methods that are trained to regress from proposed
		  regions or fixed anchor boxes to bounding boxes. But
		  anchor-based methods face computational costs and
		  challenges in optimal anchor configuration, which constrain
		  their adaptability and performance. This paper proposes an
		  anchor-free object detection method using multi-component
		  energy optimization that integrates edge, region, size, and
		  shape information to determine the optimal bounding box
		  parameters. Experiments on PASCAL VOC 2012 show superior
		  performance with 85.7\% loss reduction for small objects
		  and 3-4 times faster convergence speed, demonstrating the
		  effectiveness of the energy optimization method.},
  booktitle	= {Proceedings of the 2025 10th International Conference on
		  Intelligent Information Technology},
  pages		= {28–34},
  numpages	= {7},
  keywords	= {Object Detection, Energy-Based Models, Anchor-free
		  Detection, Bounding Box Optimization, Multi-component
		  Energy Function},
  location	= { },
  series	= {ICIIT '25}
}

@InProceedings{	  10.1145/3575882.3575926,
  author	= {Pinem, Josua Geovani and Septiadi, Agung and Shaleha, Siti
		  and Alfin, Muhammad Reza and Subekti, Aulia Haritsuddin
		  Karisma Muhammad and Muliadi, Jemie and Wibowanto, Gembong
		  and Santosa, Agung and Uliniansyah, M. Teduh and Jarin,
		  Asril and Latief, Andi Djalal and Gunarso and Riza,
		  Hammam},
  title		= {Developing Semantic Annotation Representation of Social
		  Media Sentiments and Metadata as Resource Description
		  Framework: A Study of Indonesian New Capital Related Tweets
		  Written in Bahasa},
  year		= {2023},
  isbn		= {9781450397902},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3575882.3575926},
  doi		= {10.1145/3575882.3575926},
  abstract	= {Social Media has become a tool abiding the press in this
		  modern society. Everyone can write their minds and build
		  their mass media to publish opinions. Thus, in this
		  manuscript, we develop a resource description framework
		  scheme (RDFS) to enrich the information and metadata from
		  Indonesian tweets regarding their New Capitol. This work
		  focused on applying a popular method (i.e., the Tweetskb
		  scheme) to construct the RDF of those tweets. We also
		  developed the Schema to fulfill our need to contain all the
		  information to RDF. RDF Triples were generated by
		  connecting several established vocabularies to ensure the
		  connection between its related nodes has meaning. The
		  sentiment polarity (i.e., neutral, positive, and negative
		  sentiment) is used in this manuscript. Thus, our proposal
		  can be used as an initial work to make use of twitter's
		  metadata to predict how reliable a user is, how the
		  community interact with a certain topic, spam detection,
		  clustering, and even implementing machine learning and deep
		  learning sentiment analysis in a manner of knowledge
		  graph.},
  booktitle	= {Proceedings of the 2022 International Conference on
		  Computer, Control, Informatics and Its Applications},
  pages		= {229–234},
  numpages	= {6},
  keywords	= {Twitter, Sentiment Analysis RDF/S, Ontology, Natural
		  language processing, Knowledge Graph},
  location	= {Virtual Event, Indonesia},
  series	= {IC3INA '22}
}

@InProceedings{	  10.1145/3293881.3295782,
  author	= {Frezza, Stephen and Daniels, Mats and Pears, Arnold and
		  Cajander, \r{A}sa and Kann, Viggo and Kapoor, Amanpreet and
		  McDermott, Roger and Peters, Anne-Kathrin and Sabin,
		  Mihaela and Wallace, Charles},
  title		= {Modelling competencies for computing education beyond
		  2020: a research based approach to defining competencies in
		  the computing disciplines},
  year		= {2018},
  isbn		= {9781450362238},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3293881.3295782},
  doi		= {10.1145/3293881.3295782},
  abstract	= {How might the content and outcomes of tertiary education
		  programmes be described and analysed in order to understand
		  how they are structured and function? To address this
		  question we develop a framework for modelling graduate
		  competencies linked to tertiary degree programmes in the
		  computing disciplines. While the focus of our work is
		  computing the framework is applicable to education more
		  broadly. The work presented here draws upon the pioneering
		  curricular document for information technology (IT2017),
		  curricular competency frameworks, other related documents
		  such as the software engineering competency model (SWECOM),
		  the Skills Framework for the Information Age (SFIA),
		  current research in competency models, and elicitation
		  workshop results from recent computing conferences. The aim
		  is to inform the ongoing Computing Curricula (CC2020)
		  project, an endeavour supported by the Association for
		  Computing Machinery (ACM) and the IEEE Computer Society. We
		  develop the Competency Learning Framework (CoLeaF),
		  providing an internationally relevant tool for describing
		  competencies. We argue that this competency based approach
		  is well suited for constructing learning environments and
		  assists degree programme architects in dealing with the
		  challenge of developing, describing and including
		  competencies relevant to computer and IT professionals. In
		  this paper we demonstrate how the CoLeaF competency
		  framework can be applied in practice, and though a series
		  of case studies demonstrate its effectiveness and
		  analytical power as a tool for describing and comparing
		  degree programmes in the international higher education
		  landscape.},
  booktitle	= {Proceedings Companion of the 23rd Annual ACM Conference on
		  Innovation and Technology in Computer Science Education},
  pages		= {148–174},
  numpages	= {27},
  keywords	= {curriculum guidelines, Professional competencies,
		  Computing competencies, CC2020},
  location	= {Larnaca, Cyprus},
  series	= {ITiCSE 2018 Companion}
}

@InProceedings{	  10.1145/3544548.3581175,
  author	= {Benjamin, Jesse Josua and Biggs, Heidi and Berger, Arne
		  and Rukanskaitundefined, Julija and Heidt, Michael B. and
		  Merrill, Nick and Pierce, James and Lindley, Joseph},
  title		= {The Entoptic Field Camera as Metaphor-Driven
		  Research-through-Design with AI Technologies},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3581175},
  doi		= {10.1145/3544548.3581175},
  abstract	= {Artificial intelligence (AI) technologies are widely
		  deployed in smartphone photography; and prompt-based image
		  synthesis models have rapidly become commonplace. In this
		  paper, we describe a Research-through-Design (RtD) project
		  which explores this shift in the means and modes of image
		  production via the creation and use of the Entoptic Field
		  Camera. Entoptic phenomena usually refer to perceptions of
		  floaters or bright blue dots stemming from the
		  physiological interplay of the eye and brain. We use the
		  term entoptic as a metaphor to investigate how the material
		  interplay of data and models in AI technologies shapes
		  human experiences of reality. Through our case study using
		  first-person design and a field study, we offer
		  implications for critical, reflective, more-than-human and
		  ludic design to engage AI technologies; the
		  conceptualisation of an RtD research space which
		  contributes to AI literacy discourses; and outline a
		  research trajectory concerning materiality and design
		  affordances of AI technologies.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {178},
  numpages	= {19},
  keywords	= {GAN, artificial intelligence, image synthesis,
		  materiality, research through design, technological
		  mediation},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@Article{	  10.1145/3745026,
  author	= {Yitagesu, Sofonias and Xing, Zhenchang and Zhang, Xiaowang
		  and Feng, Zhiyong and Bi, Tingting and Han, Linyi and Li,
		  Xiaohong},
  title		= {Systematic Literature Review on Software Security
		  Vulnerability Information Extraction},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3745026},
  doi		= {10.1145/3745026},
  abstract	= {Background. Software vulnerabilities are increasing in
		  complexity and scale, posing great security risks to many
		  software systems. Extracting information about software
		  vulnerabilities is a critical area of research that aims to
		  identify and create a structured representation of
		  vulnerability-related information. This structured data
		  helps software systems better understand vulnerabilities
		  and provides security professionals with timely information
		  to mitigate the impact of rapidly growing vulnerabilities
		  while guiding future research to develop more secure
		  systems. However, this process relies on the effectiveness
		  of information extraction to transform manual vulnerability
		  analysis from security experts to digital solutions.
		  Despite its importance, the unique nature of vulnerability
		  information and the fast pace at which machine
		  learning-based extraction methods and techniques have
		  evolved make it challenging to assess the current
		  successes, failures, challenges, and opportunities within
		  this research area. This study presents a systematic
		  literature review aimed at clarifying this complex
		  landscape.Methods. In this study, we conduct a systematic
		  literature review (SLR) to explore existing research
		  focusing on extracting information about software security
		  vulnerabilities. We search for 829 primary studies on
		  security vulnerability information extraction from seven
		  widely used online digital libraries, focusing on top
		  peer-reviewed journals and conferences published between
		  2001 and 2024. After applying our inclusion and exclusion
		  criteria and the snowballing technique, we narrowed our
		  selection to 87 studies for in-depth analysis and addressed
		  four main research questions. We collect qualitative and
		  quantitative data from each study, identifying 34
		  components such as research problems, methods,
		  contributions, evaluation metrics, results, types of
		  extracted vulnerability information, challenges, and
		  limitations. We use meta-analysis, statistical machine
		  learning, and text-mining techniques to identify themes,
		  patterns, and trends across the primary studies and
		  visualize findings.Results. The study provides an overview
		  of the security vulnerability data landscape, identifies
		  key resources, and guides efforts to improve vulnerability
		  information extraction and analysis. The study finds a
		  diverse landscape of learning algorithms used in security
		  vulnerability information extraction, with Bidirectional
		  Encoder Representations from Transformers (BERT), Long
		  Short-term Memory (LSTM), and Support Vector Machine (SVM)
		  being the most dominant. The study identifies key
		  challenges, including feature engineering complexity, lack
		  of a gold-standard corpus, preprocessing errors, generating
		  accurate training data, addressing imbalanced data,
		  multimodality fusion, and graph sparsity in security
		  knowledge graphs.Insights for Future Research Directions.
		  The study underscores the need for advanced extraction
		  approaches, robust datasets, automated annotation methods,
		  and advanced machine learning algorithms to improve the
		  extraction of security vulnerability information. This
		  study also suggests using large language models (LLMs) and
		  transformer models to facilitate the automatic extraction
		  of security-related words, terms, concepts, and phrases and
		  introduce new filtering parameters for user requirements.
		  We provide all our implementations; it can be found at .},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jun,
  keywords	= {Software Vulnerability, Vulnerability Information
		  Extraction, systematic literature review (SLR),
		  Meta-analysis, Statistical Machine Learning, Text Mining}
}

@Article{	  10.1145/3555312,
  author	= {Asprino, Luigi and Daga, Enrico and Gangemi, Aldo and
		  Mulholland, Paul},
  title		= {Knowledge Graph Construction with a Fa\c{c}ade: A Unified
		  Method to Access Heterogeneous Data Sources on the Web},
  year		= {2023},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {1},
  issn		= {1533-5399},
  url		= {https://doi.org/10.1145/3555312},
  doi		= {10.1145/3555312},
  abstract	= {Data integration is the dominant use case for RDF
		  Knowledge Graphs. However, Web resources come in formats
		  with weak semantics (for example, CSV and JSON), or formats
		  specific to a given application (for example, BibTex, HTML,
		  and Markdown). To solve this problem, Knowledge Graph
		  Construction (KGC) is gaining momentum due to its focus on
		  supporting users in transforming data into RDF. However,
		  using existing KGC frameworks result in complex data
		  processing pipelines, which mix structural and semantic
		  mappings, whose development and maintenance constitute a
		  significant bottleneck for KG engineers. Such frameworks
		  force users to rely on different tools, sometimes based on
		  heterogeneous languages, for inspecting sources, designing
		  mappings, and generating triples, thus making the process
		  unnecessarily complicated. We argue that it is possible and
		  desirable to equip KG engineers with the ability of
		  interacting with Web data formats by relying on their
		  expertise in RDF and the well-established SPARQL query
		  language&nbsp;[2]. In this article, we study a unified
		  method for data access to heterogeneous data sources with
		  Facade-X, a meta-model implemented in a new data
		  integration system called SPARQL Anything. We demonstrate
		  that our approach is theoretically sound, since it allows a
		  single meta-model, based on RDF, to represent data from (a)
		  any file format expressible in BNF syntax, as well as (b)
		  any relational database. We compare our method to
		  state-of-the-art approaches in terms of usability
		  (cognitive complexity of the mappings) and general
		  performance. Finally, we discuss the benefits and
		  challenges of this novel approach by engaging with the
		  reference user community.},
  journal	= {ACM Trans. Internet Technol.},
  month		= feb,
  articleno	= {6},
  numpages	= {31},
  keywords	= {re-engineering, meta-model, RDF, SPARQL}
}

@InProceedings{	  10.1145/3715275.3732091,
  author	= {Ferrario, Andrea},
  title		= {A Trustworthiness-based Metaphysics of Artificial
		  Intelligence Systems},
  year		= {2025},
  isbn		= {9798400714825},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715275.3732091},
  doi		= {10.1145/3715275.3732091},
  abstract	= {Modern AI systems are man-made objects that leverage
		  machine learning to support our lives across a myriad of
		  contexts and applications. Despite extensive
		  epistemological and ethical debates, their metaphysical
		  foundations remain relatively under explored. The orthodox
		  view simply suggests that AI systems, as artifacts, lack
		  well-posed identity and persistence conditions—their
		  metaphysical kinds are no real kinds. In this work, we
		  challenge this perspective by introducing a theory of
		  metaphysical identity of AI systems. We do so by
		  characterizing their kinds and introducing identity
		  criteria—formal rules that answer the questions “When
		  are two AI systems the same?” and “When does an AI
		  system persist, despite change?” Building on Carrara and
		  Vermaas’ account of fine-grained artifact kinds, we argue
		  that AI trustworthiness provides a lens to understand AI
		  system kinds and formalize the identity of these artifacts
		  by relating their functional requirements to their physical
		  make-ups. The identity criteria of AI systems are
		  determined by their trustworthiness profiles—the
		  collection of capabilities that the systems must uphold
		  over time throughout their artifact histories, and their
		  effectiveness in maintaining these capabilities. Our
		  approach suggests that the identity and persistence of AI
		  systems is sensitive to the socio-technical context of
		  their design and utilization via their trustworthiness,
		  providing a solid metaphysical foundation to the
		  epistemological, ethical, and legal discussions about these
		  artifacts.},
  booktitle	= {Proceedings of the 2025 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {1360–1370},
  numpages	= {11},
  keywords	= {artificial intelligence, machine learning, deep learning,
		  identity, metaphysics, ontology, change, time},
  location	= { },
  series	= {FAccT '25}
}

@InProceedings{	  10.1145/3514094.3534178,
  author	= {Skorupa Parolin, Erick and Hosseini, MohammadSaleh and Hu,
		  Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio,
		  Javier and D'Orazio, Vito},
  title		= {Multi-CoPED: A Multilingual Multi-Task Approach for Coding
		  Political Event Data on Conflict and Mediation Domain},
  year		= {2022},
  isbn		= {9781450392471},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3514094.3534178},
  doi		= {10.1145/3514094.3534178},
  abstract	= {Political and social scientists monitor, analyze and
		  predict political unrest and violence, preventing (or
		  mitigating) harm, and promoting the management of global
		  conflict. They do so using event coder systems, which
		  extract structured representations from news articles to
		  design forecast models and event-driven continuous
		  monitoring systems. Existing methods rely on expensive
		  manual annotated dictionaries and do not support
		  multilingual settings. To advance the global conflict
		  management, we propose a novel model, Multi-CoPED
		  (Multilingual Multi-Task Learning BERT for Coding Political
		  Event Data), by exploiting multi-task learning and
		  state-of-the-art language models for coding multilingual
		  political events. This eliminates the need for expensive
		  dictionaries by leveraging BERT models' contextual
		  knowledge through transfer learning. The multilingual
		  experiments demonstrate the superiority of Multi-CoPED over
		  existing event coders, improving the absolute
		  macro-averaged F1-scores by 23.3\% and 30.7\% for coding
		  events in English and Spanish corpus, respectively. We
		  believe that such expressive performance improvements can
		  help to reduce harms to people at risk of violence.},
  booktitle	= {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {700–711},
  numpages	= {12},
  keywords	= {transfer learning, social conflict, political conflict,
		  natural language processing, event coding, artificial
		  intelligence and geopolitics},
  location	= {Oxford, United Kingdom},
  series	= {AIES '22}
}

@Article{	  10.1177/26339137231207634,
  author	= {L\'{e}vy, Pierre},
  title		= {Semantic computing with IEML},
  year		= {2023},
  issue_date	= {October-December 2023},
  publisher	= {Sage Publications, Inc.},
  address	= {USA},
  volume	= {2},
  number	= {4},
  url		= {https://doi.org/10.1177/26339137231207634},
  doi		= {10.1177/26339137231207634},
  abstract	= {This paper presents IEML, Information Economy
		  MetaLanguage, a constructed language with the same
		  expressive power as a natural language and with computable
		  semantics. Distinguished from pragmatic and referential
		  semantics, linguistic semantics have not yet been
		  completely formalized. Only its syntagmatic dimension has
		  been mathematized in the form of regular languages. Its
		  paradigmatic dimension remained to be formalized. In order
		  to complete the mathematizing of language, including its
		  paradigmatic dimension, I have coded linguistic semantics
		  with IEML. This article introduces its 3000-word
		  dictionary, its formal grammar, and its integrated tools
		  for building semantic graphs. For the future, IEML could
		  become a vector for a fluid calculation and communication
		  of meaning—semantic interoperability—capable of
		  de-compartmentalizing the digital memory, and of advancing
		  the progress of collective intelligence, artificial
		  intelligence, and digital humanities. I conclude by
		  indicating some research directions.},
  journal	= {Collective Intelligence},
  month		= nov,
  numpages	= {28},
  keywords	= {Ieml, artificial intelligence, collective intelligence,
		  semantics, Linguistics}
}

@InProceedings{	  10.1145/3539618.3591877,
  author	= {Zhan, Haolan and Li, Zhuang and Wang, Yufei and Luo,
		  Linhao and Feng, Tao and Kang, Xiaoxi and Hua, Yuncheng and
		  Qu, Lizhen and Soon, Lay-Ki and Sharma, Suraj and Zukerman,
		  Ingrid and Semnani-Azad, Zhaleh and Haffari, Gholamreza},
  title		= {SocialDial: A Benchmark for Socially-Aware Dialogue
		  Systems},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591877},
  doi		= {10.1145/3539618.3591877},
  abstract	= {Content Warning: this paper may contain content that is
		  offensive or upsetting.Dialogue systems have been widely
		  applied in many scenarios and are now more powerful and
		  ubiquitous than ever before. With large neural models and
		  massive available data, current dialogue systems have
		  access to more knowledge than any people in their life.
		  However, current dialogue systems still do not perform at a
		  human level. One major gap between conversational agents
		  and humans lies in their abilities to be aware of social
		  norms. The development of socially-aware dialogue systems
		  is impeded due to the lack of resources. In this paper, we
		  present the first socially-aware dialogue corpus --
		  SocialDial based on Chinese social culture. SocialDial
		  consists of two parts: 1,563 multi-turn dialogues between
		  two human speakers with fine-grained labels, and 4,870
		  synthetic conversations generated by ChatGPT. The human
		  corpus covers five categories of social norms, which have
		  14 sub-categories in total. Specifically, it contains
		  social factor annotations including social relation,
		  context, social distance, and social norms. However,
		  collecting sufficient socially-aware dialogues is costly.
		  Thus, we harness the power of ChatGPT and devise an
		  ontology-based synthetic data generation framework. This
		  framework is able to generate synthetic data at scale. To
		  ensure the quality of synthetic dialogues, we design
		  several mechanisms for quality control during data
		  collection. Finally, we evaluate our dataset using several
		  pre-trained models, such as BERT and RoBERTa. Comprehensive
		  empirical results based on state-of-the-art neural models
		  demonstrate that modeling of social norms for dialogue
		  systems is a promising research direction. To the best of
		  our knowledge, SocialDial is the first socially-aware
		  dialogue dataset that covers multiple social factors and
		  has fine-grained labels.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2712–2722},
  numpages	= {11},
  keywords	= {datasets, social norms, socially-aware dialogue},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Article{	  10.1145/3474829,
  author	= {Debruyne, Christophe and Munnelly, Gary and Kilgallon,
		  Lynn and O’Sullivan, Declan and Crooks, Peter},
  title		= {Creating a Knowledge Graph for Ireland’s Lost History:
		  Knowledge Engineering and Curation in the Beyond 2022
		  Project},
  year		= {2022},
  issue_date	= {June 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3474829},
  doi		= {10.1145/3474829},
  abstract	= {The Beyond 2022 project aims to create a virtual archive
		  by digitally reconstructing and digitizing historical
		  records lost in a catastrophic fire which consumed items in
		  the Public Record Office of Ireland in 1922. The project is
		  developing a knowledge graph (KG) to facilitate information
		  retrieval and discovery over the reconstructed items. The
		  project decided to adopt Semantic Web technologies to
		  support its distributed KG and reasoning. In this article,
		  we present our approach to KG generation and management. We
		  elaborate on how we help historians contribute to the KG
		  (via a suite of spreadsheets) and its ontology. We
		  furthermore demonstrate how we use named graphs to store
		  different versions of factoids and their provenance
		  information and how these are serviced in two different
		  endpoints. Modeling data in this manner allows us to
		  acknowledge that history is, to some extent, subjective and
		  different perspectives can exist in parallel. The
		  construction of the KG is driven by competency questions
		  elicited from subject matter experts within the consortium.
		  We avail of CIDOC-CRM as our KG’s foundation, though we
		  needed to extend this ontology with various qualifiers
		  (types) and relations to support the competency questions.
		  We illustrate how one can explore the KG to gain insights
		  and answer questions. We conclude that CIDOC-CRM provides
		  an adequate, albeit complex, foundation for the KG and that
		  named graphs and Linked Data principles are a suitable
		  mechanism to manage sets of factoids and their
		  provenance.},
  journal	= {J. Comput. Cult. Herit.},
  month		= apr,
  articleno	= {25},
  numpages	= {25},
  keywords	= {digital humanities, knowledge graph management, Knowledge
		  graph creation}
}

@Article{	  10.1145/3687486,
  author	= {Gagnon, Michel and Font, Ludovic and Zouaq, Amal},
  title		= {An Exploration of IFLA LRM for Literature Data
		  Representation},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3687486},
  doi		= {10.1145/3687486},
  abstract	= {The digital humanities have witnessed a clear development
		  in recent years due partly to their adoption of Semantic
		  Web and linked data technologies and the creation of
		  knowledge bases. In this work, we target the creation of an
		  ontology and knowledge base for literature data
		  representation based on the IFLA Library Reference Model
		  (LRM). IFLA LRM is the main model for book-related data,
		  allowing for a fine representation of the various layers
		  that constitute a book. However, by design, it doesn’t
		  deal with some aspects usually available in literature
		  databases, such as information about authors, literary
		  awards or book themes. As a result, LRM requires some
		  extensions to be able to represent ancillary data. Another
		  challenge is the querying of IFLA LRM knowledge bases, with
		  a performance cost that comes with the fine-grained
		  expressivity of the LRM model, which creates longer and
		  therefore typically slower SPARQL queries. In this work, we
		  propose an extension to the IFLA LRM ontology called IFLA
		  LRM* that targets these limitations including a connection
		  to the vocabulary Schema.org and to the taxonomies Thema
		  and Dewey Decimal, and the representation of literary
		  awards. We also present a practical case study on using our
		  extended model to create a Quebec literature knowledge
		  base, discussing the interest of our extensions.},
  journal	= {J. Comput. Cult. Herit.},
  month		= sep,
  articleno	= {50},
  numpages	= {21},
  keywords	= {Linked Open Data, IFLA LRM, Cultural Heritage,
		  Literature}
}

@InProceedings{	  10.1145/3701551.3708813,
  author	= {Huang, Hao and Vidal, Maria-Esther},
  title		= {HyKG-CF: A Hybrid Approach for Counterfactual Prediction
		  using Domain Knowledge},
  year		= {2025},
  isbn		= {9798400713293},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701551.3708813},
  doi		= {10.1145/3701551.3708813},
  abstract	= {Predictive models are gaining attention as powerful tools
		  for aiding clinicians in diagnosis, prognosis, and
		  treatment recommendations. However, their reliance on
		  associative patterns may raise concerns about reliability
		  of decision support, as association does not necessarily
		  imply causation. To address this limit, we propose HyKG-CF,
		  a hybrid approach to counterfactual prediction that
		  leverages data and domain knowledge encoded in knowledge
		  graph (KG). HyKG-CF integrates symbolic reasoning (on
		  knowledge) with numerical learning (on data) using large
		  language models (LLMs) and statistical models to learn
		  causal Bayesian networks (CBNs) for accurate counterfactual
		  prediction. Using data and knowledge, HyKG-CF improves the
		  accuracy of causal discovery and counterfactual prediction.
		  We evaluate HyKG-CF on a non-small cell lung cancer (NSCLC)
		  KG, demonstrating that it outperforms other baselines. The
		  results highlight the promise of combining domain knowledge
		  with causal models to improve counterfactual prediction.},
  booktitle	= {Proceedings of the Eighteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1104–1105},
  numpages	= {2},
  keywords	= {causality, counterfactual prediction, knowledge graphs},
  location	= {Hannover, Germany},
  series	= {WSDM '25}
}

@Article{	  10.1145/3708504,
  author	= {Tsakalakis, Niko and Stalla-Bourdillon, Sophie and Huynh,
		  Dong and Moreau, Luc},
  title		= {A typology of explanations to support
		  Explainability-by-Design},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {1},
  url		= {https://doi.org/10.1145/3708504},
  doi		= {10.1145/3708504},
  abstract	= {As automated decision-making permeates almost all aspects
		  of everyday life, capabilities to generate meaningful
		  explanations for various stakeholders (i.e.,
		  decision-makers, addressees of decisions including
		  individuals, auditors, and regulators) should be carefully
		  deployed. This article presents a typology of explanations
		  intended to support the first pillar of an
		  explainability-by-design strategy. Its production has been
		  achieved by pursuing a responsible innovation approach and
		  introducing a new persona within the research and
		  innovation process, i.e., a legal engineer, whose role is
		  to work at the interface of two teams, the compliance and
		  the engineering teams, and to oversee the process of
		  requirement elicitation, which is often opinionated and
		  narrowing. Once explanation requirements have been derived
		  from applicable regulatory requirements, compliance rules,
		  or business policies, they have been mapped to the
		  dimensions of the typology to produce fine-grained
		  explanation requirements, forming computable building
		  blocks that can then be translated into system requirements
		  during the technical design phase. The typology has been
		  co-created with industry partners operating in two sectors:
		  finance and education. Two pilot studies have thus been
		  conducted to test both the feasibility of the generation
		  and computation of explanations on the basis of the
		  typology and the usefulness of the outputs in the light of
		  the state-of-the-art. The typology comprises nine
		  hierarchical dimensions. It can be leveraged to operate a
		  stand-alone classifier of explanations that acts as
		  detective controls within a broader partially automated
		  compliance strategy. A machine-readable format of the
		  typology is provided in the form of a light ontology.},
  journal	= {ACM J. Responsib. Comput.},
  month		= feb,
  articleno	= {1},
  numpages	= {36},
  keywords	= {Artificial intelligence, explainability, typology, data
		  protection, automated decisions}
}

@InProceedings{	  10.1145/3726302.3729880,
  author	= {Cai, Yongxin and Qiu, Jing and Zhang, Fan and Li, Qiang
		  and Chen, Lei},
  title		= {A Knowledge Extraction Framework on Cyber Threat Reports
		  with Enhanced Security Profiles},
  year		= {2025},
  isbn		= {9798400715921},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726302.3729880},
  doi		= {10.1145/3726302.3729880},
  abstract	= {Knowledge extraction on Cyber Threat Reports (CTRs) is
		  critical for attack investigation and defenses. The
		  granularity and the usability of the knowledge are key
		  issues: the former is determined by entity recognition on
		  CTRs, whereas the latter mainly depends on proper relation
		  extraction. Nevertheless, in the state-of-the-art entity
		  recognition methods on CTRs using span representation, the
		  local semantics of behavior are not considered and the
		  sequential features of entity labels within behavior
		  descriptions are not utilized. Besides, domain-specific
		  definitions/forms of the relation types and knowledge
		  representations are also crucial for effective utilization
		  of knowledge. In this paper, we propose a novel knowledge
		  extraction framework on CTRs to address the above concerns.
		  The framework is formed by the Enhanced Security Profiles
		  (ESP) that can be directly utilized by security detection
		  devices. In the ESP framework, we propose 3 modules to
		  facilitate fine-grained and accurate knowledge extractions:
		  (1) The entity recognition module utilizes a label-aware
		  subsequence autoregressive algorithm to integrate local
		  semantic and label sequence features, enabling accurate
		  identification of cybersecurity entities; (2) The relation
		  extraction module employs LLM-based strategies with shared
		  partition representations to enhance semantic understanding
		  and domain relevance; and (3) The security profile
		  generation module leverages Chain-of-Thought reasoning and
		  In-Context Learning to produce machine-readable rules
		  executable in security detection systems. Extensive
		  experiments on 6 datasets demonstrate that the ESP
		  framework largely outperform the state-of-the-art solutions
		  e.g., the Micro-Fl scores on entity recognition and
		  relation extraction are at least 1.54\% and 13.12\% better,
		  respectively. Our code can be found in
		  https://github.com/YxinMiracle/ESP.},
  booktitle	= {Proceedings of the 48th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {326–336},
  numpages	= {11},
  keywords	= {entity recognition, knowledge extraction, relation
		  extraction},
  location	= {Padua, Italy},
  series	= {SIGIR '25}
}

@Article{	  10.1145/3704848,
  author	= {Choudhury, Vikraman and Gay, Simon J.},
  title		= {The Duality of λ-Abstraction},
  year		= {2025},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {POPL},
  url		= {https://doi.org/10.1145/3704848},
  doi		= {10.1145/3704848},
  abstract	= {In this paper, we develop and study the following
		  perspective -- just as higher-order functions give
		  exponentials, higher-order continuations give
		  coexponentials. From this, we design a language that
		  combines exponentials and coexponentials, producing a
		  duality of lambda abstraction. We formalise this language
		  by giving an extension of a call-by-value simply-typed
		  lambda-calculus with covalues, coabstraction, and
		  coapplication. We develop the semantics of this language
		  using the axiomatic structure of continuations, which we
		  use to produce an equational theory, that gives a complete
		  axiomatisation of control effects. We give a computational
		  interpretation to this language using speculative execution
		  and backtracking, and use this to derive the classical
		  control operators and computational interpretation of
		  classical logic, and encode common patterns of control flow
		  using continuations. By dualising functional completeness,
		  we further develop duals of first-order arrow languages
		  using coexponentials. Finally, we discuss the
		  implementation of this duality as control operators in
		  programming, and develop some applications.},
  journal	= {Proc. ACM Program. Lang.},
  month		= jan,
  articleno	= {12},
  numpages	= {30},
  keywords	= {category theory, classical logic, continuations, control
		  effects, control operators, curry-howard, denotational
		  semantics, duality, equational theory, lambda-calculus,
		  type theory}
}

@InProceedings{	  10.1145/3411763.3451619,
  author	= {Salminen, Joni and Jung, Soon-Gyo and Chhirang, Kamal and
		  Jansen, Bernard},
  title		= {Instilling Knowledge Claims of Personas from 346 Research
		  Articles},
  year		= {2021},
  isbn		= {9781450380959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411763.3451619},
  doi		= {10.1145/3411763.3451619},
  abstract	= {Our research goal is to summarize the body of persona
		  knowledge by identifying knowledge claims. This can aid HCI
		  researchers to (a) navigate persona knowledge to form an
		  understanding of what is known about personas quickly, (b)
		  identify central research gaps of what is not known (or
		  said) about personas, and (c) identify claims that are not
		  substantiated with strong empirical evidence and warrant
		  future work. To this end, we use computational and manual
		  techniques to extract 130 knowledge claims based on 9139
		  sentences from 346 persona articles and analyze whether the
		  existing literature supports these claims. The results,
		  clustered into four groups (“Definition”,
		  “Creation”, “Evaluation”, and “Use”), indicate
		  that claims regarding persona definition are characterized
		  by a higher degree of consensus. In contrast, persona
		  creation and use contain a high proportion of unverified
		  claims. There are few claims concerning evaluation.
		  Empirical research should address unverified claims and
		  develop the ontological understanding on persona
		  evaluation.},
  booktitle	= {Extended Abstracts of the 2021 CHI Conference on Human
		  Factors in Computing Systems},
  articleno	= {450},
  numpages	= {9},
  keywords	= {Personas, knowledge claims, natural language processing,
		  summary},
  location	= {Yokohama, Japan},
  series	= {CHI EA '21}
}

@InProceedings{	  10.1145/3587259.3630082,
  author	= {Van Erp, Marieke},
  title		= {Unflattening Knowledge Graphs},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3630082},
  doi		= {10.1145/3587259.3630082},
  abstract	= {Large general-purpose knowledge graphs (KGs) are a
		  critical component for knowledge-driven applications.
		  However, most KGs represent only a limited view of the
		  entities and concepts they describe. The concept coffee
		  can, for example, refer to the plant that yields coffee
		  seeds, the beverage ‘coffee’, and the activity of
		  drinking the beverage. Moreover, it has a long history that
		  is deeply connected to colonialism and status. All of these
		  notions are an intricate part of national identities, have
		  changed dramatically over time, and connect to many
		  different narratives with different opinions on them. This
		  complexity is not captured in current KGs. In this vision
		  paper, I present the three crucial challenges for
		  unflattening knowledge graphs and directions for future
		  work.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {223–224},
  numpages	= {2},
  keywords	= {digital humanities, knowledge graphs, language
		  technology},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3379597.3387448,
  author	= {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar,
		  Avraham and Mummert, Todd},
  title		= {AIMMX: Artificial Intelligence Model Metadata Extractor},
  year		= {2020},
  isbn		= {9781450375177},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3379597.3387448},
  doi		= {10.1145/3379597.3387448},
  abstract	= {Despite all of the power that machine learning and
		  artificial intelligence (AI) models bring to applications,
		  much of AI development is currently a fairly ad hoc
		  process. Software engineering and AI development share many
		  of the same languages and tools, but AI development as an
		  engineering practice is still in early stages. Mining
		  software repositories of AI models enables insight into the
		  current state of AI development. However, much of the
		  relevant metadata around models are not easily extractable
		  directly from repositories and require deduction or domain
		  knowledge. This paper presents a library called AIMMX that
		  enables simplified AI Model Metadata eXtraction from
		  software repositories. The extractors have five modules for
		  extracting AI model-specific metadata: model name,
		  associated datasets, references, AI frameworks used, and
		  model domain. We evaluated AIMMX against 7,998 open-source
		  models from three sources: model zoos, arXiv AI papers, and
		  state-of-the-art AI papers. Our platform extracted metadata
		  with 87\% precision and 83\% recall. As preliminary
		  examples of how AI model metadata extraction enables
		  studies and tools to advance engineering support for AI
		  development, this paper presents an exploratory analysis
		  for data and method reproducibility over the models in the
		  evaluation dataset and a catalog tool for discovering and
		  managing models. Our analysis suggests that while data
		  reproducibility may be relatively poor with 42\% of models
		  in our sample citing their datasets, method reproducibility
		  is more common at 72\% of models in our sample,
		  particularly state-of-the-art models. Our collected models
		  are searchable in a catalog that uses existing metadata to
		  enable advanced discovery features for efficiently finding
		  models.},
  booktitle	= {Proceedings of the 17th International Conference on Mining
		  Software Repositories},
  pages		= {81–92},
  numpages	= {12},
  keywords	= {Model Mining, Model Metadata, Model Catalog, Metadata
		  Extraction, Machine Learning, Artificial Intelligence},
  location	= {Seoul, Republic of Korea},
  series	= {MSR '20}
}

@Article{	  10.1109/taslp.2024.3426331,
  author	= {Gunasekara, Chulaka and Kim, Seokhwan and D'Haro, Luis
		  Fernando and Rastogi, Abhinav and Chen, Yun-Nung and Eric,
		  Mihail and Hedayatnia, Behnam and Gopalakrishnan, Karthik
		  and Liu, Yang and Huang, Chao-Wei and Hakkani-T\"{u}r,
		  Dilek and Li, Jinchao and Zhu, Qi and Luo, Lingxiao and
		  Liden, Lars and Huang, Kaili and Shayandeh, Shahin and
		  Liang, Runze and Peng, Baolin and Zhang, Zheng and Shukla,
		  Swadheen and Huang, Minlie and Gao, Jianfeng and Mehri,
		  Shikib and Feng, Yulan and Gordon, Carla and Alavi, Seyed
		  Hossein and Traum, David and Eskenazi, Maxine and Beirami,
		  Ahmad and Cho, Eunjoon and Crook, Paul A. and De, Ankita
		  and Geramifard, Alborz and Kottur, Satwik and Moon,
		  Seungwhan and Poddar, Shivani and Subba, Rajen},
  title		= {Overview of the Ninth Dialog System Technology Challenge:
		  DSTC9},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3426331},
  doi		= {10.1109/TASLP.2024.3426331},
  abstract	= {This paper introduces the Ninth Dialog System Technology
		  Challenge (DSTC-9). This edition of the DSTC focuses on
		  applying end-to-end dialog technologies for four distinct
		  tasks in dialog systems, namely, 1. Task-oriented dialog
		  Modeling with Unstructured Knowledge Access, 2.
		  Multi-domain task-oriented dialog, 3. Interactive
		  evaluation of dialog and 4. Situated interactive multimodal
		  dialog. This paper describes the task definition, provided
		  datasets, baselines, and evaluation setup for each track.
		  We also summarize the results of the submitted systems to
		  highlight the general trends of the state-of-the-art
		  technologies for the tasks.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jul,
  pages		= {4066–4076},
  numpages	= {11}
}

@InProceedings{	  10.1145/3490725.3490737,
  author	= {Huang, Chao and Di, Hui and Wang, Lina and Ouchi,
		  Kazushige},
  title		= {ECO-DST: An Efficient Cross-lingual Dialogue State
		  Tracking Framework},
  year		= {2022},
  isbn		= {9781450384247},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3490725.3490737},
  doi		= {10.1145/3490725.3490737},
  abstract	= {Data efficiency is a critical challenge for cross-lingual
		  task-oriented dialogue state tracking (DST) due to high
		  cost of collecting large amount of task-related labeled
		  training set for specific language. Therefore, we focus on
		  adapting high-performance source language DST to target
		  language by using only bilingual dictionary, without
		  accessing labeled target data. We propose a novel data
		  efficient cross-lingual DST framework (ECO-DST), which
		  consists of cross-lingual encoder and language independent
		  decoder. To support cross-lingual zero-shot adaptation, we
		  leverage two advanced methods in encoder: 1) pre-trained
		  cross-lingual model XLM-RoBERTa (XLM-R), 2) dynamic local
		  phrase code-switching data augmentation for cross-lingual
		  representation alignment. We evaluate the proposed method
		  on The Ninth Dialogue System Technology Challenge (DSTC9)
		  cross-lingual tasks. For target language DST, we compare
		  our proposed framework with submitted systems in DSTC9, our
		  model achieves state-of-the-art result on CrossWOZ dataset
		  and promising result on MultiWOZ 2.1 dataset. Meanwhile on
		  source language DST, the same model keeps competitive
		  performance compared with original source DST model.},
  booktitle	= {Proceedings of the 2021 4th International Conference on
		  Machine Learning and Machine Intelligence},
  pages		= {77–82},
  numpages	= {6},
  keywords	= {Task-oriented Dialogue, Dynamic Local Phrase
		  Code-Switching, Dialogue State Tracking, Data Efficiency,
		  Cross-Lingual Transfer},
  location	= {Hangzhou, China},
  series	= {MLMI '21}
}

@Article{	  10.1145/3539223,
  author	= {Malviya, Shrikant and Kumar, Piyush and Namasudra, Suyel
		  and Tiwary, Uma Shanker},
  title		= {Experience Replay-based Deep Reinforcement Learning for
		  Dialogue Management Optimisation},
  year		= {2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3539223},
  doi		= {10.1145/3539223},
  abstract	= {Dialogue policy is a crucial component in task-oriented
		  Spoken Dialogue Systems (SDSs). As a decision function, it
		  takes the current dialogue state as input and generates
		  appropriate system’s response. In this paper, we explore
		  the reinforcement learning approaches to solve this problem
		  in an Indic language scenario. Recently, Deep Reinforcement
		  Learning (DRL) has been used to optimise the dialogue
		  policy. However, many DRL approaches are not
		  sample-efficient. Hence, particular attention is given to
		  actor-critic methods based on off-policy reinforcement
		  learning that utilise the Experience Replay (ER) technique
		  for reducing the bias and variance to achieve high sample
		  efficiency. ER based actor-critic methods, such as
		  Advantage Actor-Critic Experience Replay (A2CER) are proven
		  to deliver competitive results in gaming environments that
		  are fully observable and have a very small action-set.
		  While, in SDSs, the states are not fully observable and
		  often have to deal with the large action space. Describing
		  the limitations of traditional methods, i.e., value-based
		  and policy-based methods, such as high variance, low
		  sample-efficiency, and often converging to local optima, we
		  firstly explore the use of A2CER in dialogue policy
		  learning. It is shown to beat the current state-of-the-art
		  deep learning methods for SDS. Secondly, to handle the
		  issues of early-stage performance, we utilise a
		  demonstration corpus to pre-train the models prior to
		  on-line policy learning. We thus experiment with the A2CER
		  on a larger action space and find it significantly faster
		  than the current state-of-the-art. Combining both
		  approaches, we present a novel DRL based dialogue policy
		  optimisation method, A2CER and its effectiveness for a
		  task-oriented SDS in the Indic language.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  keywords	= {deep reinforcement learning, dialogue management, Spoken
		  dialogue systems}
}

@InProceedings{	  10.1145/3502223.3502227,
  author	= {Kume, Satoshi and Kozaki, Kouji},
  title		= {Extracting Domain-specific Concepts from Large-scale
		  Linked Open Data},
  year		= {2022},
  isbn		= {9781450395656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3502223.3502227},
  doi		= {10.1145/3502223.3502227},
  abstract	= {We propose a methodology for extracting concepts for a
		  target domain from large-scale linked open data (LOD) to
		  support the construction of domain ontologies providing
		  field-specific knowledge and definitions. The proposed
		  method defines search entities by linking the LOD
		  vocabulary with technical terms related to the target
		  domain. The search entities are then used as a starting
		  point for obtaining upper-level concepts in the LOD, and
		  the occurrences of common upper-level entities and the
		  chain-of-path relationships are examined to determine the
		  range of conceptual connections in the target domain. A
		  technical dictionary index and natural language processing
		  are used to evaluate whether the extracted concepts cover
		  the domain. As an example of extracting a class hierarchy
		  from LOD, we used Wikidata to construct a domain ontology
		  for polymer materials and physical properties. The proposed
		  method can be applied to general datasets with class
		  hierarchies, and it allows ontology developers to create an
		  initial model of the domain ontology for their own
		  purposes.},
  booktitle	= {Proceedings of the 10th International Joint Conference on
		  Knowledge Graphs},
  pages		= {28–37},
  numpages	= {10},
  keywords	= {Wikidata, Ontology construction, Linked open data, Graph
		  analysis, Domain ontology},
  location	= {Virtual Event, Thailand},
  series	= {IJCKG '21}
}

@Article{	  10.1145/3652149,
  author	= {Singh, Upendra and Abhishek, Kumar and Azad, Hiteshwar
		  Kumar},
  title		= {A Survey of Cutting-edge Multimodal Sentiment Analysis},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3652149},
  doi		= {10.1145/3652149},
  abstract	= {The rapid growth of the internet has reached the fourth
		  generation, i.e., web 4.0, which supports Sentiment
		  Analysis (SA) in many applications such as social media,
		  marketing, risk management, healthcare, businesses,
		  websites, data mining, e-learning, psychology, and many
		  more. Sentiment analysis is a powerful tool for
		  governments, businesses, and researchers to analyse
		  users’ emotions and mental states in order to generate
		  opinions and reviews about products, services, and daily
		  activities. In the past years, several SA techniques based
		  on Machine Learning (ML), Deep Learning (DL), and other
		  soft computing approaches were proposed. However, growing
		  data size, subjectivity, and diversity pose a significant
		  challenge to enhancing the efficiency of existing
		  techniques and incorporating current development trends,
		  such as Multimodal Sentiment Analysis (MSA) and fusion
		  techniques. With the aim of assisting the enthusiastic
		  researcher to navigating the current trend, this article
		  presents a comprehensive study of various literature to
		  handle different aspects of SA, including current trends
		  and techniques across multiple domains. In order to clarify
		  the future prospects of MSA, this article also highlights
		  open issues and research directions that lead to a number
		  of unresolved challenges.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {227},
  numpages	= {38},
  keywords	= {Multimodal sentiment analysis, sentiment classifier,
		  machine learning, emotion detection, modelling techniques}
}

@Article{	  10.1145/3747321.3747323,
  author	= {Kostovska, Ana},
  title		= {Representing and Exploiting Benchmarking Data for
		  Optimisation and Learning},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {2},
  url		= {https://doi.org/10.1145/3747321.3747323},
  doi		= {10.1145/3747321.3747323},
  abstract	= {The rapid advancements in Machine Learning (ML) and
		  Black-Box Optimisation (BBO) have led to an increased
		  reliance on benchmarking data for evaluating and comparing
		  algorithms across diverse domain tasks. However, the
		  effective exploitation of this data is hindered by
		  challenges such as syntactic variability, semantic
		  ambiguity, and lack of standardization. In this
		  dissertation, we address these challenges by advocating for
		  formal semantic representation of benchmarking data through
		  the use of ontologies. By providing standardized
		  vocabularies and ontologies, we improve knowledge sharing
		  and promote data interoperability across studies in ML and
		  BBO. In the ML domain, focusing on multi-label
		  classification (MLC), we design an ontology-based framework
		  for semantic annotation of benchmarking data, facilitating
		  the creation of MLCBench - a semantic catalog that enhances
		  data accessibility and reusability. In the BBO domain, we
		  introduce the OPTION (OPTImization algorithm benchmarking
		  ONtology) ontology to formally represent benchmarking data,
		  including performance data, algorithm metadata, and problem
		  landscapes. This ontology enables the automatic integration
		  and interoperability of knowledge and data from diverse
		  benchmarking studies.},
  journal	= {SIGEVOlution},
  month		= jul,
  articleno	= {2},
  numpages	= {4}
}

@InProceedings{	  10.1145/3589334.3645631,
  author	= {Zhao, Rui and Zhao, Jun},
  title		= {Perennial Semantic Data Terms of Use for Decentralized
		  Web},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645631},
  doi		= {10.1145/3589334.3645631},
  abstract	= {In today's digital landscape, the Web has become
		  increasingly centralized, raising concerns about user
		  privacy violations. Decentralized Web architectures, such
		  as Solid, offer a promising solution by empowering users
		  with better control over their data in their personal
		  'Pods'. However, a significant challenge remains: users
		  must navigate numerous applications to decide which
		  application can be trusted with access to their data Pods.
		  This often involves reading lengthy and complex Terms of
		  Use agreements, a process that users often find daunting or
		  simply ignore. This compromises user autonomy and impedes
		  detection of data misuse. We propose a novel formal
		  description of Data Terms of Use (DToU), along with a DToU
		  reasoner. Users and applications specify their own parts of
		  the DToU policy with local knowledge, covering permissions,
		  requirements, prohibitions and obligations. Automated
		  reasoning verifies compliance, and also derives policies
		  for output data. This constitutes a "perennial'' DToU
		  language, where the policy authoring only occurs once, and
		  we can conduct ongoing automated checks across users,
		  applications and activity cycles. Our solution is built on
		  Turtle, Notation 3 and RDF Surfaces, for the language and
		  the reasoning engine. It ensures seamless integration with
		  other semantic tools for enhanced interoperability. We have
		  successfully integrated this language into the Solid
		  framework, and conducted performance benchmark. We believe
		  this work demonstrates a practicality of a perennial DToU
		  language and the potential of a paradigm shift to how users
		  interact with data and applications in a decentralized Web,
		  offering both improved privacy and usability.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2238–2249},
  numpages	= {12},
  keywords	= {automated reasoning, data terms of use, decentralized web,
		  formal modelling, notation 3, usage control},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3574318.3574338,
  author	= {Paul, Soumen and Saha, Rounak and Padhi, Swarup and
		  Majumdar, Srijoni and Das, Partha Pratim and Rao, K
		  Sreenivas},
  title		= {NrityaManch: An Annotation and Retrieval System for
		  Bharatanatyam Dance},
  year		= {2023},
  isbn		= {9798400700231},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3574318.3574338},
  doi		= {10.1145/3574318.3574338},
  abstract	= {This paper presents an annotation and retrieval
		  application named NrityaManch dedicated explicitly to the
		  Indian classical dance. We primarily choose Bharatanatyam
		  dance for the application development. We exploit ontology
		  technique which captures dance image’s annotation details
		  and structurally organizes the dance database. An OWL2
		  ontology is developed in Prot\'{e}g\'{e} 5.5.0 which is
		  validated using HermiT 1.4.3.456 reasoner to maintain
		  consistency. A user interface is provided for the manual
		  annotation of dance images. Initially, we focus on dancer
		  details, dance details, and elements of static dance
		  posture like hasta mudra during the annotation. All
		  annotation details are saved in RDF/XML file. A search
		  window is provided, which facilitates two types of search -
		  natural language query search and tight query search. Named
		  Entity Recognition (NER) pipeline mechanism is utilized in
		  this work which facilitates keyword extraction from natural
		  language queries. A SPARQL query is automatically generated
		  by the system which is applied to the RDF corpus in order
		  to retrieve distinct images. The NER pipeline mechanism
		  achieves an accuracy of 80\% for our dance dataset. The
		  system achieves an average f-score value of 0.8547 for the
		  retrieval functionality. The proposed system intends to
		  help dance learners to find dance resources in a dedicated
		  place and will also help in Indian classical dance
		  preservation.},
  booktitle	= {Proceedings of the 14th Annual Meeting of the Forum for
		  Information Retrieval Evaluation},
  pages		= {65–73},
  numpages	= {9},
  keywords	= {search, natural language query, dance retrieval,
		  annotation, Bharatanatyam},
  location	= {Kolkata, India},
  series	= {FIRE '22}
}

@Article{	  10.1109/tcbb.2016.2640303,
  author	= {Min, Wenwen and Liu, Juan and Zhang, Shihua},
  title		= {Network-Regularized Sparse Logistic Regression Models for
		  Clinical Risk Prediction and Biomarker Discovery},
  year		= {2018},
  issue_date	= {May 2018},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {15},
  number	= {3},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2016.2640303},
  doi		= {10.1109/TCBB.2016.2640303},
  abstract	= {Molecular profiling data e.g., gene expression has been
		  used for clinical risk prediction and biomarker discovery.
		  However, it is necessary to integrate other prior knowledge
		  like biological pathways or gene interaction networks to
		  improve the predictive ability and biological
		  interpretability of biomarkers. Here, we first introduce a
		  general regularized Logistic Regression LR framework with
		  regularized term $lambda Vert boldsymbol {w}Vert _1 + eta
		  boldsymbol {w}^Tboldsymbol {M}boldsymbol {w}$, which can
		  reduce to different penalties, including Lasso, elastic
		  net, and network-regularized terms with different
		  $boldsymbol {M}$. This framework can be easily solved in a
		  unified manner by a cyclic coordinate descent algorithm
		  which can avoid inverse matrix operation and accelerate the
		  computing speed. However, if those estimated $boldsymbol
		  {w}_i$ and $boldsymbol {w}_j$ have opposite signs, then the
		  traditional network-regularized penalty may not perform
		  well. To address it, we introduce a novel
		  network-regularized sparse LR model with a new penalty
		  $lambda Vert boldsymbol {w}Vert _1 + eta |boldsymbol
		  {w}|^Tboldsymbol {M}|boldsymbol {w}|$ to consider the
		  difference between the absolute values of the coefficients.
		  We develop two efficient algorithms to solve it. Finally,
		  we test our methods and compare them with the related ones
		  using simulated and real data to show their efficiency.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= may,
  pages		= {944–953},
  numpages	= {10}
}

@InProceedings{	  10.1145/3643657.3643910,
  author	= {Cabrera, Christian and Paleyes, Andrei and Lawrence, Neil
		  David},
  title		= {Self-sustaining Software Systems (S4): Towards Improved
		  Interpretability and Adaptation},
  year		= {2024},
  isbn		= {9798400705601},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643657.3643910},
  doi		= {10.1145/3643657.3643910},
  abstract	= {Software systems impact society at different levels as
		  they pervasively solve real-world problems. Modern software
		  systems are often so sophisticated that their complexity
		  exceeds the limits of human comprehension. These systems
		  must respond to changing goals, dynamic data, unexpected
		  failures, and security threats, among other variable
		  factors in real-world environments. Systems' complexity
		  challenges their interpretability and requires autonomous
		  responses to dynamic changes. Two main research areas
		  explore autonomous systems' responses: evolutionary
		  computing and autonomic computing. Evolutionary computing
		  focuses on software improvement based on iterative
		  modifications to the source code. Autonomic computing
		  focuses on optimising systems' performance by changing
		  their structure, behaviour, or environment variables.
		  Approaches from both areas rely on feedback loops that
		  accumulate knowledge from the system interactions to inform
		  autonomous decision-making. However, this knowledge is
		  often limited, constraining the systems' interpretability
		  and adaptability. This paper proposes a new concept for
		  interpretable and adaptable software systems:
		  self-sustaining software systems (S4). S4 builds knowledge
		  loops between all available knowledge sources that define
		  modern software systems to improve their interpretability
		  and adaptability. This paper introduces and discusses the
		  S4 concept.},
  booktitle	= {Proceedings of the 1st International Workshop on New
		  Trends in Software Architecture},
  pages		= {5–9},
  numpages	= {5},
  keywords	= {autonomous systems, software engineering, knowledge
		  graphs, data-oriented architectures, large language
		  models},
  location	= {Lisbon, Portugal},
  series	= {SATrends '24}
}

@InProceedings{	  10.1145/3216122.3216155,
  author	= {McClatchey, Richard},
  title		= {The Deployment of an Enhanced Model-Driven Architecture
		  for Business Process Management},
  year		= {2018},
  isbn		= {9781450365277},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3216122.3216155},
  doi		= {10.1145/3216122.3216155},
  abstract	= {Business systems these days need to be agile to address
		  the needs of a changing world. Business modelling requires
		  process management to be highly adaptable with the ability
		  to support dynamic workflows, inter-application integration
		  (potentially between businesses) and process
		  reconfiguration. Designing in the ability to cater for
		  evolution is critical to success. To handle change, systems
		  need the capability to adapt as and when necessary to
		  changes in users' requirements. Using our implementation of
		  a self-describing system, a so-called description-driven
		  approach, new versions of data structures or processes can
		  be created alongside older versions providing a log of
		  changes to the underlying data schema and enabling the
		  gathering of traceable ("provenance") data. The CRISTAL
		  software, which originated at CERN for handling physics
		  data, uses versions of stored descriptions to define data
		  and workflows which can be evolved over time and thereby to
		  handle evolving system needs. It has been customised for
		  use in business as the Agilium-NG product. This paper
		  reports on how the Agilium-NG software has enabled the
		  deployment of an unique business process management
		  solution that can be dynamically evolved to cater for
		  changing user requirements.},
  booktitle	= {Proceedings of the 22nd International Database Engineering
		  \&amp; Applications Symposium},
  pages		= {217–225},
  numpages	= {9},
  keywords	= {system evolution, business provenance, business process
		  management, Description-driven systems},
  location	= {Villa San Giovanni, Italy},
  series	= {IDEAS '18}
}

@InProceedings{	  10.1145/3575882.3575912,
  author	= {Wardani, Dewi and Susmawati, Mauluah},
  title		= {SESS: Utilization of SPIN for Ethnomedicine Semantic
		  Search},
  year		= {2023},
  isbn		= {9781450397902},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3575882.3575912},
  doi		= {10.1145/3575882.3575912},
  abstract	= {Indonesia has biodiversity which is very beneficial for
		  human life. Existing applications for ethnomedicine have
		  been developed using conventional methods that only
		  utilized SPARQL Protocol and RDF Query Language (SPARQL),
		  so they still have limitations in representing knowledge
		  and its retrieval. Those conventional methods are which
		  based of relational database and ontology that has not
		  utilized inference in its query process. Therefore, this
		  work proposed SPIN for Enthnomedicine Semantic Search
		  (SESS), a framework of the semantic search for medicinal
		  plants that were developed by using SPIN (SPARQL
		  Inferencing Notation). SESS has two main parts, the
		  ontology design included SPARQL Inferencing Notation (SPIN)
		  library and query process. The experiments were assessed in
		  terms of execution time, query variation and accuracy. The
		  obtained results showed a ratio of precision at 1, recall
		  at 0.98 and the average value of the f-measure was 0.99.
		  Utilizing SPIN also decrease the time consuming to obtain
		  the result by around .},
  booktitle	= {Proceedings of the 2022 International Conference on
		  Computer, Control, Informatics and Its Applications},
  pages		= {153–157},
  numpages	= {5},
  keywords	= {spin, sparql, semantic search, owl, ontology,
		  ethnomedicine},
  location	= {Virtual Event, Indonesia},
  series	= {IC3INA '22}
}

@InProceedings{	  10.1145/3625156.3625159,
  author	= {Wang, Hongwei and Zhang, Ziling and Liu, Xiuhua and Cao,
		  Mengyuan},
  title		= {Knowledge Graph Completion Using Multiple Embedding
		  Representations for Intelligent Information Extraction from
		  Technical Reports},
  year		= {2023},
  isbn		= {9798400708206},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625156.3625159},
  doi		= {10.1145/3625156.3625159},
  abstract	= {As a new data structure, knowledge graphs are widely used
		  in search engines, recommendation systems, question and
		  answer systems and other related fields. The knowledge
		  graph is helpful to realize intelligent and digital
		  knowledge service of nuclear power accidents, in which link
		  prediction can solve the discovery and restoration of
		  missing information in Knowledge graph. This is also one of
		  the research hotspot in the field of knowledge graph
		  applications. Currently, the text description of entity
		  information is rarely considered in the completion of
		  nuclear power Knowledge graph. In this paper, we propose
		  MEK-ConvKB (Multi-Embedding Knowledge Graph Prediction
		  based on ConvKB), a reasoning model combined with
		  multi-embedding techniques to improve performance. The
		  embedded expression of Knowledge graph is enhanced by text
		  description in nuclear power accident reports, which
		  improves the accuracy of link prediction and expands the
		  Knowledge graph of nuclear power accidents. The results
		  show that our model can effectively express the semantic
		  association between entities, Our model achieved the best
		  performance compared to the baseline methods., which can
		  provide a research basis for solving the discovery and
		  restoration of missing information in knowledge graphs.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Information Science and Systems},
  pages		= {15–21},
  numpages	= {7},
  location	= {Edinburgh, United Kingdom},
  series	= {ICISS '23}
}

@InProceedings{	  10.1145/3523227.3547412,
  author	= {Anelli, Vito Walter and Basile, Pierpaolo and de Melo,
		  Gerard and Donini, Francesco Maria and Ferrara, Antonio and
		  Musto, Cataldo and Narducci, Fedelucio and Ragone, Azzurra
		  and Zanker, Markus},
  title		= {Fourth Knowledge-aware and Conversational Recommender
		  Systems Workshop (KaRS)},
  year		= {2022},
  isbn		= {9781450392785},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3523227.3547412},
  doi		= {10.1145/3523227.3547412},
  abstract	= {In the last few years, a renewed interest of the research
		  community in conversational recommender systems (CRSs) has
		  been emerging. This is likely due to the massive
		  proliferation of Digital Assistants (DAs) such as Amazon
		  Alexa, Siri, or Google Assistant that are revolutionizing
		  the way users interact with machines. DAs allow users to
		  execute a wide range of actions through an interaction
		  mostly based on natural language utterances. However,
		  although DAs are able to complete tasks such as sending
		  texts, making phone calls, or playing songs, they still
		  remain at an early stage in terms of their recommendation
		  capabilities via a conversation. In addition, we have been
		  witnessing the advent of increasingly precise and powerful
		  recommendation algorithms and techniques able to
		  effectively assess users’ tastes and predict information
		  that may be of interest to them. Most of these approaches
		  rely on the collaborative paradigm (often exploiting
		  machine learning techniques) and neglect the huge amount of
		  knowledge, both structured and unstructured, describing the
		  domain of interest of a recommendation engine. Although
		  very effective in predicting relevant items, collaborative
		  approaches miss some very interesting features that go
		  beyond the accuracy of results and move in the direction of
		  providing novel and diverse results as well as generating
		  explanations for recommended items. Knowledge-aware side
		  information becomes crucial when a conversational
		  interaction is implemented, in particular for preference
		  elicitation, explanation, and critiquing steps.},
  booktitle	= {Proceedings of the 16th ACM Conference on Recommender
		  Systems},
  pages		= {663–666},
  numpages	= {4},
  keywords	= {Conversational Agents, Knowledge Graphs, Knowledge
		  Representation, Natural Language Processing, Recommender
		  systems, Semantic Web},
  location	= {Seattle, WA, USA},
  series	= {RecSys '22}
}

@InProceedings{	  10.5555/3586210.3586504,
  author	= {Weaver, Gabriel A. and Shusko, Jacob and Hasenbein, John
		  J. and Kutanoglu, Erhan and Martinez-Medina, Gonzalo and
		  Castillo-Villar, Krystel K. and Costa, Paulo C. G.},
  title		= {Simulating Energy and Security Interactions in
		  Semiconductor Manufacturing: Insights from the Intel
		  Minifab Model},
  year		= {2023},
  publisher	= {IEEE Press},
  abstract	= {Semiconductor manufacturing, particularly wafer
		  fabrication, is a highly complex system of processes and
		  workflows. Fabrication facilities must deal with re-entrant
		  flows to support multiple types of wafers being produced
		  simultaneously, each with their own deadlines and
		  specifications. The manufacturing process itself depends
		  upon the ability to control and programmatically adjust a
		  variety of environmental conditions. In addition, wafer
		  fabrication consumes large amounts of energy, particularly
		  electricity. Emerging technologies including networked
		  devices may help reduce the energy footprint but can
		  introduce cybersecurity risks. Therefore, this paper
		  presents its modeling and simulation framework to quantify
		  tradeoffs between operational measures of performance,
		  energy consumption, and cybersecurity risks. We augment the
		  Intel Minifab model with an Industrial Control Systems
		  (ICS) reference model based on the Purdue Enterprise
		  Reference Architecture (PERA) as well as tool-level energy
		  consumption data from a semiconductor manufacturing
		  testbed.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {3477–3488},
  numpages	= {12},
  location	= {Singapore, Singapore},
  series	= {WSC '22}
}

@Article{	  10.1145/3183639.3183641,
  author	= {Zhang, Jie and Mitrovic, Tanja and Chin, David and Chen,
		  Li},
  title		= {ACM UMAP 2018 - User Modeling, Adaptation and
		  Personalization: 8-11 July, 2018 at NTU, Singapore},
  year		= {2018},
  issue_date	= {Winter 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2018},
  number	= {Winter},
  issn		= {1931-1745},
  url		= {https://doi.org/10.1145/3183639.3183641},
  doi		= {10.1145/3183639.3183641},
  abstract	= {UMAP (User Modeling, Adaptation and Personalization) is
		  the premier international conference for researchers and
		  practitioners working on systems that adapt to individual
		  users, to groups of users, and that collect, represent, and
		  model user information. UMAP is the successor to the
		  biennial User Modeling (UM) and Adaptive Hypermedia and
		  Adaptive Web-based Systems (AH) conferences that were
		  merged in 2009. It is sponsored by ACM SIGCHI and SIGWEB,
		  and organized under the auspices of User Modeling Inc. The
		  proceedings are published by ACM and will be part of the
		  ACM Digital Library.},
  journal	= {SIGWEB Newsl.},
  month		= mar,
  articleno	= {2},
  numpages	= {5}
}

@InProceedings{	  10.1145/3632410.3632489,
  author	= {Mitra, Aniket and Venugopal, Vinu},
  title		= {Enhancing Region-Based Geometric Embedding for
		  Gene-Disease Associations},
  year		= {2024},
  isbn		= {9798400716348},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632410.3632489},
  doi		= {10.1145/3632410.3632489},
  abstract	= {In recent times, Geometric Knowledge Graph Embedding (KGE)
		  methods that focus on regions have proven valuable for
		  effectively representing structured knowledge, such as
		  ontologies, in various reasoning tasks. Nevertheless,
		  transforming the richly expressive semantics present in
		  ontologies may require a transition to less expressive
		  interim representations before initiating the actual
		  embedding process. In our research, we explore current
		  approaches and offer recommendations to enhance the
		  state-of-the-art, drawing from extensive experiments
		  conducted on the Human Phenotype Ontology (HPO) for
		  predicting gene-disease (g-d) associations. Our findings
		  indicate that incorporating these new suggestions can lead
		  to results that outperform other leading KGE models.},
  booktitle	= {Proceedings of the 7th Joint International Conference on
		  Data Science \&amp; Management of Data (11th ACM IKDD CODS
		  and 29th COMAD)},
  pages		= {584–585},
  numpages	= {2},
  keywords	= {EL++ OWL, Knowledge Graph Embedding, n-ball Embedding},
  location	= {Bangalore, India},
  series	= {CODS-COMAD '24}
}

@InProceedings{	  10.1145/3338501.3357365,
  author	= {Alperin, Kenneth and Wollaber, Allan and Ross, Dennis and
		  Trepagnier, Pierre and Leonard, Leslie},
  title		= {Risk Prioritization by Leveraging Latent Vulnerability
		  Features in a Contested Environment},
  year		= {2019},
  isbn		= {9781450368339},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3338501.3357365},
  doi		= {10.1145/3338501.3357365},
  abstract	= {Cyber network defenders face an overwhelming volume of
		  software vulnerabilities. Resource limitations preclude
		  them mitigating all but a small number of vulnerabilities
		  on an enterprise network, so proper prioritization of
		  defensive actions are of paramount importance. Current
		  methods of risk prioritization are predominantly
		  expert-based, and many include leveraging Common
		  Vulnerability Scoring System (CVSS) risk scores. These
		  scores are assigned by subject matter experts according to
		  conventional methods of qualifying risk. Vulnerability
		  mitigation strategies are then often applied in CVSS score
		  order. Our vulnerability assessment system, in contrast,
		  takes a predominantly data-driven approach. In general, we
		  associate a risk metric of vulnerabilities with existence
		  of corresponding exploits. Our assumption is that if an
		  entity has invested time and money to exploit a particular
		  vulnerability, this is a critical gauge of that
		  vulnerability's importance, and hence risk.Prior work
		  presented a model that allows for the creation of
		  prioritized vulnerabilities based on their
		  association-likelihood with exploits, outperforming
		  then-current methods. Because the initial approach only
		  leveraged one vulnerability feature, we extended the
		  vulnerability feature space by incorporating additional
		  features derived from natural language processing. The
		  importance metric is still given by a vulnerability-exploit
		  relationship, but by processing text descriptions and other
		  available information, our system became significantly more
		  accurate and predictive. We next propose a mechanism that
		  customizes vulnerability risks according to their
		  exploitation likelihood in a contested environment given
		  site-specific threat intelligence information, namely,
		  attacks by an Advanced Persistent Threat (APT) group.
		  Utilizing held-back data, we then demonstrate that latently
		  similar vulnerabilities, which could be targeted by the
		  same adversary, see higher risk ratings.},
  booktitle	= {Proceedings of the 12th ACM Workshop on Artificial
		  Intelligence and Security},
  pages		= {49–57},
  numpages	= {9},
  keywords	= {vulnerability, risk model, natural language processing,
		  machine learning, exploit},
  location	= {London, United Kingdom},
  series	= {AISec'19}
}

@InProceedings{	  10.1145/3686169.3686204,
  author	= {Quesnel, Denise T. and Losev, Tatiana and Stepanova,
		  Ekaterina R. and Carpendale, Sheelagh and Riecke, Bernhard
		  E.},
  title		= {The Inbetweeny Collective: Reflexive Dialogues on the
		  Liminality of Researchers' Lived Experiences},
  year		= {2024},
  isbn		= {9798400710421},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3686169.3686204},
  doi		= {10.1145/3686169.3686204},
  abstract	= {In this collaborative autoethnography, we critically
		  explore our lived experiences within a wider context of HCI
		  research and practice. We reflect on the epistemological
		  ways of knowing through ‘insider’ lived experience of,
		  and ‘outsider’ knowledge of our research topic(s) via
		  the concept of “liminal space” as a process ontology.
		  To embrace liminality entails inhabiting the space
		  ‘in-between’ these ways of knowing, suspended on a
		  threshold of uncertainty and transformative growth. All
		  authors identify as ‘inbetweenies’, because we are
		  neither just ‘insiders’ nor ‘outsiders’, and we
		  collect our respective stories to share. Drawing from these
		  stories and our dialogues, we discuss how ways of knowing
		  have historically been dichotomously categorized with their
		  associated subjective or objective characterizations,
		  resulting in power hierarchies and tensions. We propose
		  that for an ‘inbetweeny’ researcher, thoughtful
		  approaches to navigating this liminal space could
		  potentially bridge persistent tensions in HCI research and
		  practices toward personal and systemic transformation. Five
		  areas of reflection are discussed, with proposed learnings
		  that can be applied towards sustained practices for
		  individuals and collectives at any stage of their journey
		  and development.},
  booktitle	= {Proceedings of the Halfway to the Future Symposium},
  articleno	= {6},
  numpages	= {10},
  keywords	= {Autoethnography, Critical Reflexivity, Design Research,
		  Experiential Knowledge, Human-Computer Interaction, Insider
		  and Outsider Research, Liminality, Lived Experience,
		  Philosophy, Process Ontology, Research Methodology},
  location	= {Santa Cruz, CA, USA},
  series	= {HttF '24}
}

@InProceedings{	  10.1145/3620666.3651344,
  author	= {Bisbas, George and Lydike, Anton and Bauer, Emilien and
		  Brown, Nick and Fehr, Mathieu and Mitchell, Lawrence and
		  Rodriguez-Canal, Gabriel and Jamieson, Maurice and Kelly,
		  Paul H. J. and Steuwer, Michel and Grosser, Tobias},
  title		= {A shared compilation stack for distributed-memory
		  parallelism in stencil DSLs},
  year		= {2024},
  isbn		= {9798400703867},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3620666.3651344},
  doi		= {10.1145/3620666.3651344},
  abstract	= {Domain Specific Languages (DSLs) increase programmer
		  productivity and provide high performance. Their targeted
		  abstractions allow scientists to express problems at a high
		  level, providing rich details that optimizing compilers can
		  exploit to target current- and next-generation
		  supercomputers. The convenience and performance of DSLs
		  come with significant development and maintenance costs.
		  The siloed design of DSL compilers and the resulting
		  inability to benefit from shared infrastructure cause
		  uncertainties around longevity and the adoption of DSLs at
		  scale. By tailoring the broadly-adopted MLIR compiler
		  framework to HPC, we bring the same synergies that the
		  machine learning community already exploits across their
		  DSLs (e.g. Tensorflow, PyTorch) to the finite-difference
		  stencil HPC community. We introduce new HPC-specific
		  abstractions for message passing targeting distributed
		  stencil computations. We demonstrate the sharing of common
		  components across three distinct HPC stencil-DSL compilers:
		  Devito, PSyclone, and the Open Earth Compiler, showing that
		  our framework generates high-performance executables based
		  upon a shared compiler ecosystem.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems, Volume 3},
  pages		= {38–56},
  numpages	= {19},
  keywords	= {message passing, MPI, MLIR, SSA, domain-specific
		  languages, intermediate representations, stencil
		  computations},
  location	= {La Jolla, CA, USA},
  series	= {ASPLOS '24}
}

@InProceedings{	  10.1145/3511808.3557520,
  author	= {Fissore, Giancarlo and Vasiloglou, Nikolaos},
  title		= {Simulating Complex Problems Inside a Database},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557520},
  doi		= {10.1145/3511808.3557520},
  abstract	= {The standard way to store and interact with the large
		  amount of data that are central to the functioning of any
		  modern business is through the use of a relational
		  Knowledge Graph Management System (KGMS). In this paper we
		  show how the relational model can be successfully exploited
		  to model complex analytic scenarios while enjoying the same
		  characteristics of clarity and flexibility as when modeling
		  the data themselves. Using the Rel language, we simulate
		  the daily schedule of an airline company as an agentbased
		  system, and we will show how modeling this system through a
		  set of relationships and logical rules will let us focus
		  directly on the inherent complexity of our model, taking
		  away most of the incidental effort in actually implementing
		  our simulation.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {5086–5087},
  numpages	= {2},
  keywords	= {simulation, database, agent based modelling},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3660853.3660886,
  author	= {Tabaza, Abdulrahman and Quishawi, Omar and Yaghi,
		  Abdelrahman and Qawasmeh, Omar},
  title		= {Binding Text, Images, Graphs, and Audio for Music
		  Representation Learning},
  year		= {2024},
  isbn		= {9798400716928},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3660853.3660886},
  doi		= {10.1145/3660853.3660886},
  abstract	= {Abstract In the field of Information Retrieval and Natural
		  Language Processing, text embeddings play a significant
		  role in tasks such as classification, clustering, and topic
		  modeling. However, extending these embeddings to abstract
		  concepts such as music, which involves multiple modalities,
		  presents a unique challenge. Our work addresses this
		  challenge by integrating rich multi-modal data into a
		  unified joint embedding space. This space includes: (1)
		  textual, (2) visual, (3) acoustic, and (4) graph-based
		  modality features. By doing so, we mirror cognitive
		  processes associated with music interaction and overcome
		  the disjoint nature of individual modalities. The resulting
		  joint low-dimensional vector space facilitates retrieval,
		  clustering, embedding space arithmetic, and cross-modal
		  retrieval tasks. Importantly, our approach carries
		  implications for music information retrieval and
		  recommendation systems. Furthermore, we propose a novel
		  multi-modal model that integrates various data
		  types—text, images, graphs, and audio—for music
		  representation learning. Our model aims to capture the
		  complex relationships between different modalities,
		  enhancing the overall understanding of music. By combining
		  textual descriptions, visual imagery, graph-based
		  structures, and audio signals, we create a comprehensive
		  representation that can be leveraged for a wide range of
		  music-related tasks. Notably, our model demonstrates
		  promising results in music classification, and
		  recommendation systems. Code Availability: The source code
		  for the multi-modal music representation model described in
		  this paper is available on GitHub. Access and further
		  details can be found at the following repository link:
		  //github.com/a-tabaza/binding_music/},
  booktitle	= {Proceedings of the Cognitive Models and Artificial
		  Intelligence Conference},
  pages		= {139–146},
  numpages	= {8},
  location	= {undefinedstanbul, Turkiye},
  series	= {AICCONF '24}
}

@InProceedings{	  10.1145/3550356.3561602,
  author	= {Jeusfeld, Manfred and Mezei, Gergely and B\'{a}csi,
		  S\'{a}ndor},
  title		= {DeepTelos and DMLA: a contribution to the MULTI 2022
		  collaborative comparison challenge},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561602},
  doi		= {10.1145/3550356.3561602},
  abstract	= {The MULTI 2022 Collaborative Comparison Challenge was
		  created to promote in-depth discussion between multi-level
		  modeling approaches. This paper presents a comparison of
		  DeepTelos- and DMLA-based solutions in response to the
		  challenge. We first present each approach and solution
		  separately, and then list the similarities and differences
		  between the two solutions, discussing their relative
		  strengths and weaknesses.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {414–423},
  numpages	= {10},
  keywords	= {multi-level modeling, collaborative challenge, DeepTelos,
		  DMLA},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@InProceedings{	  10.1145/3639233.3639335,
  author	= {Nuipian, Vatinee and Chuaykhun, Jirawat},
  title		= {Book Recommendation System based on Course Descriptions
		  using Cosine Similarity},
  year		= {2024},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639233.3639335},
  doi		= {10.1145/3639233.3639335},
  abstract	= {Ensuring the retrieval of books that match users'
		  preferences is of paramount importance. A significant
		  challenge users encounter is uncertainty regarding their
		  choice of search terms, often stemming from a limited
		  understanding of the content or exposure to new concepts.
		  Offering users results that closely resemble their query
		  represents one potential solution. This research aims to
		  suggest books relevant to students' course topics,
		  utilizing cosine similarity to compute similarity values
		  within each document in the collection.Performance
		  evaluation using a similarity threshold greater than 0.1
		  revealed that the retrieved book results achieved an
		  average precision of 0.7 and a recall value of 0.73,
		  indicating substantial alignment with the search terms. The
		  anticipated benefits of the recommendation system encompass
		  the elimination of the need for manual book suggestions by
		  staff, the provision of personalized book recommendations
		  tailored to readers' preferences, a deeper understanding of
		  library user behavior, and the effective promotion of new
		  books that align with users' interests.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {273–277},
  numpages	= {5},
  keywords	= {Book Recommendation, Cosine similarity, Course
		  Descriptions, Text mining},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '23}
}

@InProceedings{	  10.5555/3712729.3712833,
  author	= {Anagnostou, Anastasia and Brailford, Sally and Eldabi,
		  Tillal and Mustafee, Navonil and Tako, Antuela},
  title		= {Ten Years of the Hybrid Simulation Track: Reflections and
		  Vision for the Future},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {The Hybrid Simulation (HS) track was included in the
		  Winter Simulation Conference (WSC) proceedings as a full
		  conference track for the first time in 2014. A decade has
		  passed since that inaugural track, and HS research and
		  practice has seen impressive advancements during this time.
		  This paper, based on a high-level review of the published
		  works in the last ten years of the HS track, reflects on
		  its successes and challenges and sets the scene for the
		  future of the field. The paper is authored by the HS track
		  organizers, both past and present, who report on the
		  track's history, the nature of HS applications, the
		  modeling tools and software available, as well as
		  implementation challenges and the users' perspective.
		  Finally, the paper discusses the future of HS.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1245–1259},
  numpages	= {15},
  location	= {Orlando, Florida, USA},
  series	= {WSC '24}
}

@InProceedings{	  10.1145/3591106.3592223,
  author	= {Nebbia, Giacomo and Kovashka, Adriana},
  title		= {Hypernymization of named entity-rich captions for
		  grounding-based multi-modal pretraining},
  year		= {2023},
  isbn		= {9798400701788},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3591106.3592223},
  doi		= {10.1145/3591106.3592223},
  abstract	= {Named entities are ubiquitous in text that naturally
		  accompanies images, especially in domains such as news or
		  Wikipedia articles. In previous work, named entities have
		  been identified as a likely reason for low performance of
		  image-text retrieval models pretrained on Wikipedia and
		  evaluated on named entities-free benchmark datasets.
		  Because they are rarely mentioned, named entities could be
		  challenging to model. They also represent missed learning
		  opportunities for self-supervised models: the link between
		  named entity and object in the image may be missed by the
		  model, but it would not be if the object were mentioned
		  using a more common term. In this work, we investigate
		  hypernymization as a way to deal with named entities for
		  pretraining grounding-based multi-modal models and for
		  fine-tuning on open-vocabulary detection. We propose two
		  ways to perform hypernymization: (1) a “manual”
		  pipeline relying on a comprehensive ontology of concepts,
		  and (2) a “learned” approach where we train a language
		  model to learn to perform hypernymization. We run
		  experiments on data from Wikipedia and from The New York
		  Times. We report improved pretraining performance on
		  objects of interest following hypernymization, and we show
		  the promise of hypernymization on open-vocabulary
		  detection, specifically on classes not seen during
		  training.},
  booktitle	= {Proceedings of the 2023 ACM International Conference on
		  Multimedia Retrieval},
  pages		= {67–75},
  numpages	= {9},
  keywords	= {grounding, hypernymization, named entities,
		  open-vocabulary detection},
  location	= {Thessaloniki, Greece},
  series	= {ICMR '23}
}

@Article{	  10.1145/3586075,
  author	= {Das, Ringki and Singh, Thoudam Doren},
  title		= {Multimodal Sentiment Analysis: A Survey of Methods,
		  Trends, and Challenges},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {13s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3586075},
  doi		= {10.1145/3586075},
  abstract	= {Sentiment analysis has come long way since it was
		  introduced as a natural language processing task nearly 20
		  years ago. Sentiment analysis aims to extract the
		  underlying attitudes and opinions toward an entity. It has
		  become a powerful tool used by governments, businesses,
		  medicine, marketing, and others. The traditional sentiment
		  analysis model focuses mainly on text content. However,
		  technological advances have allowed people to express their
		  opinions and feelings through audio, image and video
		  channels. As a result, sentiment analysis is shifting from
		  unimodality to multimodality. Multimodal sentiment analysis
		  brings new opportunities with the rapid increase of
		  sentiment analysis as complementary data streams enable
		  improved and deeper sentiment detection which goes beyond
		  text-based analysis. Audio and video channels are included
		  in multimodal sentiment analysis in terms of broadness.
		  People have been working on different approaches to improve
		  sentiment analysis system performance by employing complex
		  deep neural architectures. Recently, sentiment analysis has
		  achieved significant success using the transformer-based
		  model. This paper presents a comprehensive study of
		  different sentiment analysis approaches, applications,
		  challenges, and resources then concludes that it holds
		  tremendous potential. The primary motivation of this survey
		  is to highlight changing trends in the unimodality to
		  multimodality for solving sentiment analysis tasks.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {270},
  numpages	= {38},
  keywords	= {transfer learning, audio sentiment analysis, image
		  sentiment analysis, text sentiment analysis, Multimodal
		  sentiment analysis}
}

@InProceedings{	  10.1145/3706599.3716239,
  author	= {Sarkar, Advait},
  title		= {AI Could Have Written This: Birth of a Classist Slur in
		  Knowledge Work},
  year		= {2025},
  isbn		= {9798400713958},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706599.3716239},
  doi		= {10.1145/3706599.3716239},
  abstract	= {AI shaming is a social phenomenon in which negative
		  judgements are associated with the use of Artificial
		  Intelligence (AI). This includes comparing someone’s work
		  with AI-generated work as a means of disparagement, voicing
		  suspicion or alleging that someone has used AI to undermine
		  their reputation, or blaming the poor quality of an
		  artefact on AI use. Common justifications of AI shaming
		  include recourse to AI’s societal harms, its technical
		  limitations, and lack of creativity. I argue that, more
		  fundamentally than any of these, AI shaming arises from a
		  class anxiety induced in middle class knowledge workers,
		  and is a form of boundary work to maintain class solidarity
		  and limit mobility into knowledge work. I discuss the role
		  of AI shaming in protecting the privileged class of
		  knowledge work and its attendant harms.},
  booktitle	= {Proceedings of the Extended Abstracts of the CHI
		  Conference on Human Factors in Computing Systems},
  articleno	= {621},
  numpages	= {12},
  keywords	= {class identity, sumptuary laws, protectionism, epistemic
		  injustice, colonialism, sociotechnical imaginaries, moral
		  panic},
  location	= { },
  series	= {CHI EA '25}
}

@InProceedings{	  10.1145/3394486.3403330,
  author	= {Li, Ying and Zakhozhyi, Vitalii and Zhu, Daniel and
		  Salazar, Luis J.},
  title		= {Domain Specific Knowledge Graphs as a Service to the
		  Public: Powering Social-Impact Funding in the US},
  year		= {2020},
  isbn		= {9781450379984},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394486.3403330},
  doi		= {10.1145/3394486.3403330},
  abstract	= {Web and mobile technologies enable ubiquitous access to
		  information. Yet, it is getting harder, even for subject
		  matter experts, to quickly identify quality, trustworthy,
		  and reliable content available online through search
		  engines powered by advanced knowledge graphs. This paper
		  explores the practical applications of Domain Specific
		  Knowledge Graphs that allow for the extraction of
		  information from trusted published and unpublished sources,
		  to map the extracted information to an ontology defined in
		  collaboration with sector experts, and to enable the public
		  to go from single queries into ongoing conversations
		  meeting their knowledge needs reliably. We focused on
		  Social-Impact Funding, an area of need for over one million
		  nonprofit organizations, foundations, government entities,
		  social entrepreneurs, impact investors, and academic
		  institutions in the US.},
  booktitle	= {Proceedings of the 26th ACM SIGKDD International
		  Conference on Knowledge Discovery \&amp; Data Mining},
  pages		= {2793–2801},
  numpages	= {9},
  keywords	= {social-impact funding, domain specific knowledge graph,
		  domain ontology},
  location	= {Virtual Event, CA, USA},
  series	= {KDD '20}
}

@InProceedings{	  10.5555/3378680.3378700,
  author	= {Oliveira, Raquel and Arriaga, Patr\'{\i}cia and Correia,
		  Filipa and Paiva, Ana},
  title		= {The stereotype content model applied to human-robot
		  interactions in groups},
  year		= {2020},
  isbn		= {9781538685556},
  publisher	= {IEEE Press},
  abstract	= {In this paper we sought to understand how the display of
		  different levels of warmth and competence, as well as,
		  different roles (opponent versus partner) portrayed by a
		  robot, affect the display of emotional responses towards
		  robots and how they can be used to predict future intention
		  to work. For this purpose we devised an entertainment
		  card-game group scenario involving two humans and two
		  robots (n=54). The results suggest that different levels of
		  warmth and competence are associated with distinct
		  emotional responses from users and that these variables are
		  useful in predicting future intention to work, thus hinting
		  at the importance of considering warmth and competence
		  stereotypes in Human-Robot Interaction.},
  booktitle	= {Proceedings of the 14th ACM/IEEE International Conference
		  on Human-Robot Interaction},
  pages		= {123–132},
  numpages	= {10},
  keywords	= {stereotypes, human-robot interaction, emotions, autonomous
		  robots},
  location	= {Daegu, Republic of Korea},
  series	= {HRI '19}
}

@InProceedings{	  10.1145/3360901.3364428,
  author	= {Mecharnia, Thamer and Chibout Khelifa, Lydia and Pernelle,
		  Nathalie and Hamdi, Fay\c{c}al},
  title		= {An Approach Toward a Prediction of the Presence of
		  Asbestos in Buildings Based on Incomplete Temporal
		  Descriptions of Marketed Products},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364428},
  doi		= {10.1145/3360901.3364428},
  abstract	= {Since 1997, the production, import and sale of
		  asbestosfootnoteNaturally occurring mineral fibres which
		  were used due to their insulating properties. have been
		  banned in France. However, there are still millions of tons
		  scattered in factories, buildings, or hospitals. In this
		  paper we propose a method for predicting the presence of
		  asbestos products in buildings based on temporal data that
		  describes the probability of the presence of asbestos in
		  marketed products.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {239–242},
  numpages	= {4},
  keywords	= {uncertain information, temporal data, prediction,
		  ontology},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.1145/3594536.3595168,
  author	= {Habba, Eliya and Keydar, Renana and Bareket, Dan and
		  Stanovsky, Gabriel},
  title		= {The Perfect Victim: Computational Analysis of Judicial
		  Attitudes towards Victims of Sexual Violence},
  year		= {2023},
  isbn		= {9798400701979},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3594536.3595168},
  doi		= {10.1145/3594536.3595168},
  abstract	= {We develop computational models to analyze court
		  statements in order to assess judicial attitudes toward
		  victims of sexual violence in the Israeli court system. The
		  study examines the resonance of "rape myths" in the
		  criminal justice system's response to sex crimes, in
		  particular in judicial assessment of victim's credibility.
		  We begin by formulating an ontology for evaluating judicial
		  attitudes toward victim's credibility, with eight ordinal
		  labels and binary categorizations. Second, we curate a
		  manually annotated dataset for judicial assessments of
		  victim's credibility in the Hebrew language, as well as a
		  model that can extract credibility labels from court cases.
		  The dataset consists of 855 verdict decision documents in
		  sexual assault cases from 1990-2021, annotated with the
		  help of legal experts and trained law students. The model
		  uses a combined approach of syntactic and latent structures
		  to find sentences that convey the judge's attitude towards
		  the victim and classify them according to the credibility
		  label set. Our ontology, data, and models will be made
		  available upon request, in the hope they spur future
		  progress in this judicial important task.},
  booktitle	= {Proceedings of the Nineteenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {111–120},
  numpages	= {10},
  keywords	= {Judicial decision making, Rape myths, Sexual violence,
		  Witness credibility},
  location	= {Braga, Portugal},
  series	= {ICAIL '23}
}

@InProceedings{	  10.5555/3427510.3427540,
  author	= {Kon\'{e}, Youssouf and Ma\"{\i}ga, Oumar and Traor\'{e},
		  Mamadou K.},
  title		= {Using hills as a common concrete syntax for ses and devs:
		  application to microscopic simulation of traffic},
  year		= {2020},
  isbn		= {9781713814290},
  publisher	= {Society for Computer Simulation International},
  address	= {San Diego, CA, USA},
  abstract	= {We propose to use the High Level Language for System
		  Specification as a common visual language for DEVS-based
		  simulation model specification and SES-based domain
		  ontological description. In doing so, we ensure continuity
		  in knowledge representation and therefore reduces
		  accidental errors while compressing the time to obtain the
		  simulation model from the domain knowledge. This approach
		  also offers a documentation basis to objective-based
		  selection of elements of the ontological representation. We
		  use a traffic modeling case to illustrate the effectiveness
		  of our conceptual proposal.},
  booktitle	= {Proceedings of the 2020 Summer Simulation Conference},
  articleno	= {29},
  numpages	= {11},
  keywords	= {system entity structure /model base (SES/MB), microscopic
		  traffic modeling, high level language for system
		  specification (HiLLS), discrete event systems specification
		  (DEVS)},
  location	= {Virtual Event, Spain},
  series	= {SummerSim '20}
}

@InProceedings{	  10.1145/3539618.3591904,
  author	= {Bonisoli, Giovanni and Di Buono, Maria Pia and Po, Laura
		  and Rollo, Federica},
  title		= {DICE: a Dataset of Italian Crime Event news},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591904},
  doi		= {10.1145/3539618.3591904},
  abstract	= {Extracting events from news stories as the aim of several
		  Natural Language Processing (NLP) applications (e.g.,
		  question answering, news recommendation, news
		  summarization) is not a trivial task, due to the complexity
		  of natural language and the fact that news reporting is
		  characterized by journalistic style and norms. Those
		  aspects entail scattering an event description over several
		  sentences within one document (or more documents), applying
		  a mechanism of gradual specification of event-related
		  information. This implies a widespread use of co-reference
		  relations among the textual elements, conveying non-linear
		  temporal information. In addition to this, despite the
		  achievement of state-of-the-art results in several tasks,
		  high-quality training datasets for non-English languages
		  are rarely available.This paper presents our preliminary
		  study to develop an annotated Dataset for Italian Crime
		  Event news (DICE). The contribution of the paper are: (1)
		  the creation of a corpus of 10,395 crime news; (2) the
		  annotation schema; (3) a dataset of 10,395 news with
		  automatic annotations; (4) a preliminary manual annotation
		  using the proposed schema of 1000 documents. The first
		  tests on DICE have compared the performance of a manual
		  annotator with that of single-span and multi-span question
		  answering models and shown there is still a gap in the
		  models, especially when dealing with more complex
		  annotation tasks and limited training data. This
		  underscores the importance of investing in the creation of
		  high-quality annotated datasets like DICE, which can
		  provide a solid foundation for training and testing a wide
		  range of NLP models.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2985–2995},
  numpages	= {11},
  keywords	= {5ws, crime news, event extraction, nlp, question
		  answering},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3267851.3267895,
  author	= {Querrec, Ronan and Taoum, Joanna and Nakhal, Bilal and
		  Bevacqua, Elisabetta},
  title		= {Model for Verbal Interaction between an Embodied Tutor and
		  a Learner in Virtual Environments},
  year		= {2018},
  isbn		= {9781450360135},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3267851.3267895},
  doi		= {10.1145/3267851.3267895},
  abstract	= {This research work introduce virtual embodied tutors in
		  Virtual Environments for learning devoted to learning of
		  procedures for industrial systems. We present a
		  communicative behavior which, integrated in pedagogical
		  scenario, permits on the one hand to realize the
		  pedagogical communicative actions at a semantic level
		  (e.g., the tutor explains the goal of an action) and on the
		  other hand to realize such actions through human-like
		  communicative channels (i.e., the virtual tutor's voice,
		  facial expressions and gestures). The communicative
		  behavior relies on a taxonomy of questions in order to
		  interpret the learner's communicative actions and to
		  generate the tutor's own questions.},
  booktitle	= {Proceedings of the 18th International Conference on
		  Intelligent Virtual Agents},
  pages		= {197–202},
  numpages	= {6},
  keywords	= {Virtual Learning Environment, Verbal Interaction,
		  Interface, Intelligent Tutoring System, Embodied
		  Conversational Agent},
  location	= {Sydney, NSW, Australia},
  series	= {IVA '18}
}

@InProceedings{	  10.1145/3582768.3582780,
  author	= {Moharkar, Kunal and Kshirsagar, Kartik and Shrey, Suruchi
		  and Pasine, Neha and Kumar, Rishu and Radke, Mansi A.},
  title		= {Responding to customer queries automatically by customer
		  reviews’ based Question Answering},
  year		= {2023},
  isbn		= {9781450397629},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3582768.3582780},
  doi		= {10.1145/3582768.3582780},
  abstract	= {The entire world has been undergoing its own digital
		  transformation over the past few decades as technology has
		  advanced in leaps and bounds. Following this, an increase
		  in the number of people using digital platforms for buying
		  products online likewise increases the number of questions
		  or enquiries posted about a product on an online shopping
		  platform like Amazon on a day to day basis. Though we have
		  gone completely digital in posting these questions, the
		  answering of these questions is still manual. The forums
		  are rarely active. By the time the user gets an answer to
		  his question, either he has bought that product already
		  through offline means or has lost interest in buying that
		  product since it is time consuming. Moreover, the questions
		  which are asked are mostly repetitive. At times the answers
		  are already out there since they have already been given to
		  some other user who had asked the same question. Also, lot
		  of answers are embedded in the user reviews. Therefore, the
		  answers can be extracted from the existing product reviews.
		  This may lead to increase in sale and greater customer
		  satisfaction as his query is resolved in much lower
		  response time. We have review-based question answering
		  systems that aim at answering the questions from the
		  reviews given on the product by other customers. However,
		  the existing systems have certain drawbacks due to the use
		  of RNN, like missing attention mechanism etc. In this work,
		  we enhance the performance of the existing review based QA
		  systems by carrying out some prototypical experiments with
		  the basic models of NLP and then moving towards more
		  advanced Language Models while identifying and rectifying
		  the shortcomings of the existing model. Further, in this
		  work a thorough comparative analysis of the models and
		  approaches that have been worked on is presented. We have
		  enhanced the current state of the art existing review QA
		  systems by using BERT, BART and also applied various
		  heuristics for comparison. We achieved the best BLEU score
		  of 0.58 by using BERT, which is an improvement of 0.19 on
		  the current existing system.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {228–233},
  numpages	= {6},
  location	= {Bangkok, Thailand},
  series	= {NLPIR '22}
}

@Article{	  10.1145/3713073,
  author	= {Wang, Haijie and Jiao, Jiajia},
  title		= {Sentiment Analysis of MOOC Reviews Based on Knowledge
		  Dependency Tree},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3713073},
  doi		= {10.1145/3713073},
  abstract	= {As an important online learning resource, Massive Open
		  Online Courses have a large amount of comments, which can
		  be exploited by aspect-level sentiment analysis to optimize
		  MOOC teaching from different perspectives. However, there
		  are two essential problems. One is that there is no
		  open-source dataset on Chinese MOOC. The other problem is
		  semantic information confusion caused by inherent polysemy
		  of Chinese words and ambiguous expressions relatively
		  relying on the context. To further characterize the special
		  features of Chinese MOOC reviews, we build an open-source
		  dataset with clean 5,000 MOOC reviews and propose a
		  sentiment knowledge dependency tree–based graph neural
		  network. The proposed model first uses the latest term
		  frequency–inverse document frequency algorithm to extract
		  high-frequency words and combines it with the Semantic
		  Orientation Pointwise Mutual Information algorithm so a
		  sentiment dictionary in the field of Chinese MOOCs is
		  constructed. Then, the grammatical information of the
		  dependency tree is merged with the sentiment knowledge
		  information of the sentiment dictionary. Next, this novel
		  model uses GCN to capture the long-distance feature
		  information of the sentiment dependency tree and finally
		  adopts the softmax function for sentiment classification.
		  To further improve the model's performance, we also use
		  BERT to enhance the text representation for higher
		  accuracy. Meanwhile, the comparative experiments
		  demonstrate that our proposed model takes advantages of the
		  customized dependency tree by knowledge dictionary to
		  achieve more accurate sentiment analysis than the
		  state-of-the-art methods under different word embedding
		  approaches.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= mar,
  articleno	= {24},
  numpages	= {15},
  keywords	= {MOOCs, Sentiment dictionary, Sentiment dependency tree,
		  Graph convolutional networks}
}

@Article{	  10.1145/3199668,
  author	= {Ahmad, Kashif and Mekhalfi, Mohamed Lamine and Conci,
		  Nicola and Melgani, Farid and Natale, Francesco De},
  title		= {Ensemble of Deep Models for Event Recognition},
  year		= {2018},
  issue_date	= {May 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3199668},
  doi		= {10.1145/3199668},
  abstract	= {In this article, we address the problem of recognizing an
		  event from a single related picture. Given the large number
		  of event classes and the limited information contained in a
		  single shot, the problem is known to be particularly hard.
		  To achieve a reliable detection, we propose a combination
		  of multiple classifiers, and we compare three alternative
		  strategies to fuse the results of each classifier, namely:
		  (i) induced order weighted averaging operators, (ii)
		  genetic algorithms, and (iii) particle swarm optimization.
		  Each method is aimed at determining the optimal weights to
		  be assigned to the decision scores yielded by different
		  deep models, according to the relevant optimization
		  strategy. Experimental tests have been performed on three
		  event recognition datasets, evaluating the performance of
		  various deep models, both alone and selectively combined.
		  Experimental results demonstrate that the proposed approach
		  outperforms traditional multiple classifier solutions based
		  on uniform weighting, and outperforms recent
		  state-of-the-art approaches.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= may,
  articleno	= {51},
  numpages	= {20},
  keywords	= {multiple classifiers, multimedia indexing and retrieval,
		  genetic algorithms, fusion, deep neural networks, PSO,
		  IOWA, Event recognition, CNN}
}

@Proceedings{	  10.1145/3681772,
  title		= {IWCTS'24: Proceedings of the 17th ACM SIGSPATIAL
		  International Workshop on Computational Transportation
		  Science GenAI and Smart Mobility Session},
  year		= {2024},
  isbn		= {9798400711510},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The 17th International Workshop on Computational
		  Transportation Science (IWCTS 2024) will feature a Smart
		  Mobility track, emphasizing the growing relevance of human
		  mobility data from sources like cell phones, connected
		  vehicles, and volunteered geographic information. This data
		  integration is advancing smart city frameworks, intelligent
		  transportation systems, and urban planning. Managing and
		  analyzing large-scale datasets highlights the critical role
		  of advanced computational and AI techniques, including
		  Generative AI (GenAI), Large Language Models (LLMs), and
		  Retrieval-Augmented Generation (RAG). The workshop builds
		  on previous success, focusing on computational and
		  informatics approaches for optimized urban mobility. We
		  will build upon the success of previous workshops to
		  continue to focus on the computational and informatics
		  approaches for (not limited to):},
  location	= {Atlanta, GA, USA}
}

@InProceedings{	  10.1145/3290605.3300298,
  author	= {Choi, In Kwon and Childers, Taylor and Raveendranath,
		  Nirmal Kumar and Mishra, Swati and Harris, Kyle and Reda,
		  Khairi},
  title		= {Concept-Driven Visual Analytics: an Exploratory Study of
		  Model- and Hypothesis-Based Reasoning with Visualizations},
  year		= {2019},
  isbn		= {9781450359702},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3290605.3300298},
  doi		= {10.1145/3290605.3300298},
  abstract	= {Visualization tools facilitate exploratory data analysis,
		  but fall short at supporting hypothesis-based reasoning. We
		  conducted an exploratory study to investigate how
		  visualizations might support a concept-driven analysis
		  style, where users can optionally share their hypotheses
		  and conceptual models in natural language, and receive
		  customized plots depicting the fit of their models to the
		  data. We report on how participants leveraged these unique
		  affordances for visual analysis. We found that a majority
		  of participants articulated meaningful models and
		  predictions, utilizing them as entry points to sensemaking.
		  We contribute an abstract typology representing the types
		  of models participants held and externalized as data
		  expectations. Our findings suggest ways for rearchitecting
		  visual analytics tools to better support hypothesis- and
		  model-based reasoning, in addition to their traditional
		  role in exploratory analysis. We discuss the design
		  implications and reflect on the potential benefits and
		  challenges involved.},
  booktitle	= {Proceedings of the 2019 CHI Conference on Human Factors in
		  Computing Systems},
  pages		= {1–14},
  numpages	= {14},
  keywords	= {visual analytics, sensemaking, mental models, hypothesis-
		  and model-based reasoning},
  location	= {Glasgow, Scotland Uk},
  series	= {CHI '19}
}

@Article{	  10.1145/3606370,
  author	= {Chan, Chia-Pang and Yang, Jun-He},
  title		= {Instagram Text Sentiment Analysis Combining Machine
		  Learning and NLP},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3606370},
  doi		= {10.1145/3606370},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  keywords	= {word embedding technology, deep learning, machine
		  learning, natural language processing, Instagram}
}

@InProceedings{	  10.1145/3655497.3655513,
  author	= {Chen, Yan and Ma, Ding},
  title		= {Detection of greenwashing in ESG reports of Chinese listed
		  companies based on Word2vec and TF-IDF},
  year		= {2024},
  isbn		= {9798400709302},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3655497.3655513},
  doi		= {10.1145/3655497.3655513},
  abstract	= {With the growing emphasis on ESG (Environmental, Social,
		  and Governance) issues, the mandatory disclosure of ESG
		  reports is on the horizon. However, due to the lack of a
		  regulatory framework and a unified international ESG
		  evaluation, the phenomenon of greenwashing in corporate ESG
		  reporting is prevalent. We collected social responsibility
		  reports and actual ESG performance data from A-share
		  companies from 2011 to 2021 and innovatively employed text
		  mining techniques to quantitatively investigate the extent
		  of greenwashing in ESG reports. Our study initially
		  utilized the Word2Vec method, combined with Skip-gram and
		  Continuous Bag of Words models to train word vectors, and
		  built an ESG lexicon using seed words. ESG reports is
		  subsequently segmented based on a defined sentence
		  splitting function and TF-IDF algorithm is employed to
		  extract keywords. By matching the keywords with the ESG
		  lexicon, we precisely extracted the annual ESG discourse
		  for each company and conducted sentiment analysis to derive
		  a greenwashing score. Heterogeneity analysis reveals that
		  firm ownership has no significant impact on the level of
		  greenwashing, yet the industry and region in which the
		  enterprise operates considerably influence the greenwashing
		  level. This study holds implications for enhancing the
		  quality of ESG reporting and optimizing investment
		  decisions.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Innovation in Artificial Intelligence},
  pages		= {159–164},
  numpages	= {6},
  keywords	= {ESG, Greenwashing detection, TF-IDF algorithm, Text
		  mining, Word2vec},
  location	= {Tokyo, Japan},
  series	= {ICIAI '24}
}

@InProceedings{	  10.5555/3721488.3721623,
  author	= {Pritchard, Michael and Ratnayake, Kalana and Gamage,
		  Buddhi and Jayasuriya, Maleen and Herath, Damith},
  title		= {Capabilities2 for ROS2: Advanced Skill-Based Control for
		  Human-Robot Interaction},
  year		= {2025},
  publisher	= {IEEE Press},
  abstract	= {In the early days of the Open Source Robotics Foundation,
		  a lesser-known project aimed to design an ''app-able
		  robot'', leading to the creation of the ''Capabilities''
		  package for the Robot Operating System. Over a decade
		  later, formulating robot capabilities remains a significant
		  technical hurdle in bringing robots from the lab into
		  everyday life. This paper introduces Capabilities2, a
		  successor to the original Capabilities package, now
		  reimagined for ROS2. Capabilities2 enhances the original
		  design by enabling advancements in skill-based control
		  techniques and offering a more efficient, extensible
		  framework for defining and utilising robot capabilities. We
		  delve into its application in new real-world scenarios,
		  with a particular focus on human-robot interactions and the
		  deployment of collaborative mobile robots in human-centric
		  environments. Capabilities2 addresses challenges in
		  implementing intuitive, collaborative robots by introducing
		  an abstracted database handler, an object-relational
		  mapping for capability models, and a plugin architecture
		  for capability execution. These features support dynamic
		  capability representation, runtime adaptability, and
		  integration with modern AI techniques for skill-based task
		  planning. By providing a standardised yet flexible
		  framework, Capabilities2 reduces the integration effort
		  required to develop top-level controls for real-world
		  scenarios, facilitating rapid development and deployment.
		  Our contributions include the reimplementation of the
		  Capabilities package in ROS2, enhancements to support
		  contemporary robotic applications, and demonstrations of
		  new use cases enabled by Capabilities2. We believe that
		  Capabilities2 significantly advances the field of robotics
		  by equipping developers with tools to create more capable,
		  adaptable, and interactive robots. Capabilities2 is
		  available at
		  https://github.com/CollaborativeRoboticsLab/capabilities2},
  booktitle	= {Proceedings of the 2025 ACM/IEEE International Conference
		  on Human-Robot Interaction},
  pages		= {1067–1071},
  numpages	= {5},
  keywords	= {api, capability, communication, hri, interface, package,
		  provider, ros2, service, skill, task},
  location	= {Melbourne, Australia},
  series	= {HRI '25}
}

@InProceedings{	  10.1145/3549737.3549769,
  author	= {Konstantinidis, Ioannis and Maragoudakis, Manolis and
		  Magnisalis, Ioannis and Berberidis, Christos and
		  Peristeras, Vassilios},
  title		= {Knowledge-driven Unsupervised Skills Extraction for
		  Graph-based Talent Matching},
  year		= {2022},
  isbn		= {9781450395977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3549737.3549769},
  doi		= {10.1145/3549737.3549769},
  abstract	= {In human resource management of large organisations,
		  finding the best candidate for a job description requires
		  an extensive examination of a large number of resume
		  profiles. Even with the advent of Deep Information
		  Retrieval and the supported semantic similarity search,
		  identification of relevant skills within profiles requires
		  thorough investigation over several aspects, including
		  educational background, professional experience,
		  achievements, etc. However, these techniques are based on
		  the existence of domain-specific, human-annotated datasets,
		  a laborious task that portrays high cost and a slow
		  labeling progress. In this paper, we propose
		  Resume2Skill-SE, an end-to-end architecture for
		  interpretable skill-based talent matching. The solution
		  consists of two components. The first module uses an
		  unsupervised approach for skills extraction based on
		  state-of-the-art text embeddings and efficient semantic
		  similarity search. The second module creates a
		  profile-skills bipartite graph and uses a proposed ranking
		  formula for similar resume profiles, minimising the effect
		  of potential errors from the skills extraction module. The
		  optimal ranking formula was identified through an intuitive
		  and automated evaluation method for getting relevance
		  scores. The proposed technique delivers promising results
		  while also including an interpretability layer by showing
		  the common skills of a pair of resume profiles.},
  booktitle	= {Proceedings of the 12th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {28},
  numpages	= {7},
  keywords	= {unsupervised skills extraction, search engine, natural
		  language processing, graph analytics},
  location	= {Corfu, Greece},
  series	= {SETN '22}
}

@InProceedings{	  10.1145/3687123.3698284,
  author	= {Tsiligkaridis, Athanasios and Kalinowski, Nicholas and Li,
		  Zhongheng and Hou, Elizabeth},
  title		= {Encoding Agent Trajectories as Representations with
		  Sequence Transformers},
  year		= {2024},
  isbn		= {9798400711763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3687123.3698284},
  doi		= {10.1145/3687123.3698284},
  abstract	= {Spatiotemporal data faces many analogous challenges to
		  natural language text including the ordering of locations
		  (words) in a sequence, long range dependencies between
		  locations, and locations having multiple meanings. In this
		  work, we propose a novel model for representing high
		  dimensional spatiotemporal trajectories as sequences of
		  discrete locations and encoding them with a
		  Transformer-based neural network architecture. Similar to
		  language models, our Sequence Transformer for Agent
		  Representation Encodings (STARE) model can learn
		  representations and structure in trajectory data through
		  both supervisory tasks (e.g., classification), and
		  self-supervisory tasks (e.g., masked modelling). We present
		  experimental results on various synthetic and real
		  trajectory datasets and show that our proposed model can
		  learn meaningful encodings that are useful for many
		  downstream tasks including discriminating between labels
		  and indicating similarity between locations. Using these
		  encodings, we also learn relationships between agents and
		  locations present in spatiotemporal data.},
  booktitle	= {Proceedings of the 7th ACM SIGSPATIAL International
		  Workshop on AI for Geographic Knowledge Discovery},
  pages		= {38–49},
  numpages	= {12},
  keywords	= {Transformers, encoders, human mobility, spatiotemporal
		  data, trajectory modeling},
  location	= {Atlanta, GA, USA},
  series	= {GeoAI '24}
}

@Article{	  10.1145/3410569,
  author	= {Laatar, Rim and Aloulou, Chafik and Belguith, Lamia
		  Hadrich},
  title		= {Disambiguating Arabic Words According to Their Historical
		  Appearance in the Document Based on Recurrent Neural
		  Networks},
  year		= {2020},
  issue_date	= {November 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3410569},
  doi		= {10.1145/3410569},
  abstract	= {How can we determine the semantic meaning of a word in
		  relation to its context of appearance? We eventually have
		  to grabble with this difficult question, as one of the
		  paramount problems of Natural Language Processing (NLP). In
		  other words, this issue is commonly defined as Word Sense
		  Disambiguation (WSD). The latter is one of the crucial
		  difficulties within the NLP field. In this respect, word
		  vectors extracted from a neural network model have been
		  successfully applied for resolving the WSD problem.
		  Accordingly, this article presents an unprecedented method
		  to disambiguate Arabic words according to both their
		  contextual appearance in a source text and the era in which
		  they emerged. In fact, in the few previous decades, many
		  researchers have been grabbling with Arabic Word Sense
		  Disambiguation.It should be noted that the Arabic language
		  can be divided into three major historical periods: old
		  Arabic, middle-age Arabic, and contemporary Arabic.
		  Actually, contemporary Arabic has proved to be the greatest
		  concern of many researchers. The main gist of our work is
		  to disambiguate Arabic words according to the historical
		  period in which they appeared. To perform such a task, we
		  suggest a method that deploys contextualized word
		  embeddings to better gather valid syntactic and semantic
		  information of the same word by taking into account its
		  contextual uses. The preponderant thing is to convert both
		  the senses and the contextual uses of an ambiguous item to
		  vectors, then determine which of the possible conceptual
		  meanings of the target word is closer to the given
		  context.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  articleno	= {86},
  numpages	= {16},
  keywords	= {word sense disambiguation, recurrent neural networks, old
		  arabic, middle-age arabic, historical dictionary,
		  contextualized word embeddings, contemporary arabic,
		  Natural language processing}
}

@Article{	  10.1145/3699953,
  author	= {Dadure, Pankaj and Pakray, Partha and Bandyopadhyay,
		  Sivaji},
  title		= {Mathematical Information Retrieval: A Review},
  year		= {2024},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3699953},
  doi		= {10.1145/3699953},
  abstract	= {Mathematical formulas are commonly used to demonstrate
		  theories and basic fundamentals in the Science, Technology,
		  Engineering, and Mathematics (STEM) domain. The burgeoning
		  research in the STEM domain results in the mass production
		  of scientific documents that contain both textual and
		  mathematical terms. In scientific information, the
		  definition of mathematical formulas is expressed through
		  context and symbolic structure that adheres to strong
		  domain-specific notions. Whereas the retrieval of textual
		  information is well-researched, and numerous text-based
		  search engines are present. However, textual information
		  retrieval systems are inadequate for searching scientific
		  information containing mathematical formulas, including
		  simple symbols to complicated mathematical structures. The
		  retrieval of mathematical information is in its infancy,
		  and it requires the inclusion of new technologies and tools
		  to promote the retrieval of scientific information and the
		  management of digital libraries. This article provides a
		  comprehensive study of mathematical information retrieval
		  and highlights their challenges and future opportunities.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {61},
  numpages	= {34},
  keywords	= {Artificial intelligence, natural language processing,
		  information retrieval, formula retrieval, mathematical
		  knowledge discovery, digital libraries}
}

@InProceedings{	  10.1145/3646548.3676597,
  author	= {Rabiser, Rick},
  title		= {Industry Adoption of UVL: What We Will Need},
  year		= {2024},
  isbn		= {9798400705939},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3646548.3676597},
  doi		= {10.1145/3646548.3676597},
  abstract	= {Since 2018, in the Software Product Line community, the
		  MODEVAR initiative is working on coming up with a simple,
		  standard variability modeling language and has proposed the
		  Universal Variability Language (UVL). UVL has already been
		  integrated in multiple tools such as FeatureIDE and FLAMA
		  and it has also already been adopted by other academics
		  outside the MODEVAR initiative. This short position paper
		  outlines the challenges for industry adoption of UVL the
		  community needs to work on.},
  booktitle	= {Proceedings of the 28th ACM International Systems and
		  Software Product Line Conference},
  pages		= {46–49},
  numpages	= {4},
  keywords	= {Challenges, Industry Adoption, Variability Modeling},
  location	= {Dommeldange, Luxembourg},
  series	= {SPLC '24}
}

@InProceedings{	  10.1145/3611643.3616314,
  author	= {Win, Hsu Myat and Wang, Haibo and Tan, Shin Hwei},
  title		= {Towards Automated Detection of Unethical Behavior in
		  Open-Source Software Projects},
  year		= {2023},
  isbn		= {9798400703270},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3611643.3616314},
  doi		= {10.1145/3611643.3616314},
  abstract	= {Given the rapid growth of Open-Source Software (OSS)
		  projects, ethical considerations are becoming more
		  important. Past studies focused on specific ethical issues
		  (e.g., gender bias and fairness in OSS). There is little to
		  no study on the different types of unethical behavior in
		  OSS projects. We present the first study of unethical
		  behavior in OSS projects from the stakeholders’
		  perspective. Our study of 316 GitHub issues provides a
		  taxonomy of 15 types of unethical behavior guided by six
		  ethical principles (e.g., autonomy). Examples of new
		  unethical behavior include soft forking (copying a
		  repository without forking) and self-promotion (promoting a
		  repository without self-identifying as contributor to the
		  repository). We also identify 18 types of software
		  artifacts affected by the unethical behavior. The diverse
		  types of unethical behavior identified in our study (1)
		  call for attentions of developers and researchers when
		  making contributions in GitHub, and (2) point to future
		  research on automated detection of unethical behavior in
		  OSS projects. From our study, we propose Etor, an approach
		  that can automatically detect six types of unethical
		  behavior by using ontological engineering and Semantic Web
		  Rule Language (SWRL) rules to model GitHub attributes and
		  software artifacts. Our evaluation on 195,621 GitHub issues
		  (1,765 GitHub repositories) shows that Etor can
		  automatically detect 548 unethical behavior with 74.8\%
		  average true positive rate (up to 100\% true positive
		  rate). This shows the feasibility of automated detection of
		  unethical behavior in OSS projects.},
  booktitle	= {Proceedings of the 31st ACM Joint European Software
		  Engineering Conference and Symposium on the Foundations of
		  Software Engineering},
  pages		= {644–656},
  numpages	= {13},
  keywords	= {Ethics in Software Engineering, Open-source software
		  projects},
  location	= {San Francisco, CA, USA},
  series	= {ESEC/FSE 2023}
}

@InProceedings{	  10.1145/3485557.3485566,
  author	= {Wagih, Heba M. and Mokhtar, Hoda M. O.},
  title		= {Coronavirus: A Curse or A Bless ?},
  year		= {2021},
  isbn		= {9781450384186},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485557.3485566},
  doi		= {10.1145/3485557.3485566},
  abstract	= {Nowadays crime is one of the major threats that affect
		  human lives. The current pandemic has a great impact on
		  changing the criminal landscape. Extensive investigations
		  for crime and criminal behaviors have revealed new crime
		  patterns and led to the generation of a large amount of
		  data and relations that need to be presented in a proper
		  model. In this paper, we conduct several experiments on
		  different datasets representing some major cities in the
		  USA to study the effect of the current pandemic on crime
		  types, rates, and intensity which can be used in crime
		  prediction and prevention. we also introduce an ontology
		  model with its underlying description logics as the
		  knowledge representation model to represent crime
		  information.},
  booktitle	= {The 7th Annual International Conference on Arab Women in
		  Computing in Conjunction with the 2nd Forum of Women in
		  Research},
  articleno	= {9},
  numpages	= {4},
  keywords	= {ontology, knowledge representation model, description
		  logic},
  location	= {Sharjah, United Arab Emirates},
  series	= {ArabWIC 2021}
}

@InProceedings{	  10.1145/3486187.3490204,
  author	= {Ducatteeuw, Vincent},
  title		= {Developing an Urban Gazetteer: A Semantic Web Database for
		  Humanities Data},
  year		= {2021},
  isbn		= {9781450391023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486187.3490204},
  doi		= {10.1145/3486187.3490204},
  abstract	= {This talk discusses the development of a spatiotemporal
		  data model for an urban gazetteer. The function of
		  gazetteers is to obtain descriptions uniquely identifying
		  places referred to in discourse. Often, they are lists of
		  places containing place name, feature type and geographical
		  extent. Contemporary digital gazetteers (e.g. World
		  Historical Gazetteer and Pleiades) are valuable tools for
		  geographical knowledge of the past and the structuring of
		  humanities data. However, scholars and GLAM (Galleries,
		  Libraries, Archives and Museums) specialists often require
		  information about entities on an intra-city scale. This
		  presentation explores the model and implementation of an
		  urban gazetteer using CIDOC CRM as a top-level ontology.
		  The model will closely follow international gazetteer
		  standards (i.e. Linked Places Format) in order to ensure
		  interoperability with other gazetteer datasets. To move
		  towards a FAIR (Findable, Accessible, Interoperable, and
		  Reusable) approach, humanities data from the urban
		  gazetteer will be published as Linked Open Data (LOD) and
		  searchable via (Geo)SPARQL.},
  booktitle	= {Proceedings of the 5th ACM SIGSPATIAL International
		  Workshop on Geospatial Humanities},
  pages		= {36–39},
  numpages	= {4},
  keywords	= {urban history, urban gazetteer, spatiotemporal analysis,
		  spatial humanities, spatial history, semantic technologies,
		  modelling geohistorical data, linked open data, gazetteer
		  development, digital humanities, GeoSPARQL, CIDOC CRM},
  location	= {Beijing, China},
  series	= {GeoHumanities '21}
}

@InProceedings{	  10.1145/3708359.3712110,
  author	= {Bao, Calvin and Shiue, Yow-Ting and Carpuat, Marine and
		  Chan, Joel},
  title		= {Words as Bridges: Exploring Computational Support for
		  Cross-Disciplinary Translation Work},
  year		= {2025},
  isbn		= {9798400713064},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708359.3712110},
  doi		= {10.1145/3708359.3712110},
  abstract	= {Scholars often explore literature outside of their home
		  community of study. This exploration process is frequently
		  hampered by field-specific jargon. Past computational work
		  often focuses on supporting translation work by removing
		  jargon through simplification and summarization; here, we
		  explore a different approach that preserves jargon as
		  useful bridges to new conceptual spaces. Specifically, we
		  cast different scholarly domains as different
		  language-using communities, and explore how to adapt
		  techniques from unsupervised cross-lingual alignment of
		  word embeddings to explore conceptual alignments between
		  domain-specific word embedding spaces.We developed a
		  prototype cross-domain search engine that uses aligned
		  domain-specific embeddings to support conceptual
		  exploration, and tested this prototype in two case studies.
		  We discuss qualitative insights into the promises and
		  pitfalls of this approach to translation work, and suggest
		  design insights for future interfaces that provide
		  computational support for cross-domain information
		  seeking.},
  booktitle	= {Proceedings of the 30th International Conference on
		  Intelligent User Interfaces},
  pages		= {1598–1623},
  numpages	= {26},
  keywords	= {cross-domain information seeking, information foraging,
		  scholarly term translation},
  location	= { },
  series	= {IUI '25}
}

@InProceedings{	  10.1145/3339252.3339282,
  author	= {Mahaini, Mohamad Imad and Li, Shujun and Sa\u{g}lam,
		  Rahime Belen},
  title		= {Building Taxonomies based on Human-Machine Teaming: Cyber
		  Security as an Example},
  year		= {2019},
  isbn		= {9781450371643},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3339252.3339282},
  doi		= {10.1145/3339252.3339282},
  abstract	= {Taxonomies and ontologies are handy tools in many
		  application domains such as knowledge systematization and
		  automatic reasoning. In the cyber security field, many
		  researchers have proposed such taxonomies and ontologies,
		  most of which were built based on manual work. Some
		  researchers proposed the use of computing tools to automate
		  the building process, but mainly on very narrow sub-areas
		  of cyber security. Thus, there is a lack of general cyber
		  security taxonomies and ontologies, possibly due to the
		  difficulties of manually curating keywords and concepts for
		  such a diverse, inter-disciplinary and dynamically evolving
		  field.This paper presents a new human-machine teaming based
		  process to build taxonomies, which allows human experts to
		  work with automated natural language processing (NLP) and
		  information retrieval (IR) tools to co-develop a taxonomy
		  from a set of relevant textual documents. The proposed
		  process could be generalized to support non-textual
		  documents and to build (more complicated) ontologies as
		  well. Using the cyber security as an example, we
		  demonstrate how the proposed taxonomy building process has
		  allowed us to build a general cyber security taxonomy
		  covering a wide range of data-driven keywords (topics) with
		  a reasonable amount of human effort.},
  booktitle	= {Proceedings of the 14th International Conference on
		  Availability, Reliability and Security},
  articleno	= {30},
  numpages	= {9},
  keywords	= {visualization, taxonomy, ontology, online social network
		  (OSN), natural language processing (NLP), knowledge
		  representation, information retrieval (IR), cyber security,
		  Twitter},
  location	= {Canterbury, CA, United Kingdom},
  series	= {ARES '19}
}

@InProceedings{	  10.1145/3502223.3502236,
  author	= {Shimizu, Cogan and Zhu, Rui and Mai, Gengchen and Fisher,
		  Colby and Cai, Ling and Schildhauer, Mark and Janowicz,
		  Krzysztof and Hitzler, Pascal and Zhou, Lu and Stephen,
		  Shirly},
  title		= {A Pattern for Features on a Hierarchical Spatial Grid},
  year		= {2022},
  isbn		= {9781450395656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3502223.3502236},
  doi		= {10.1145/3502223.3502236},
  abstract	= {The integration of data along a common spatial component
		  remains an obstacle in many problem spaces. One promising
		  method for integrating data in such a way is through the
		  use of a common, underlying spatial reference system, such
		  as a Discrete Global Grid (e.g., the S2 Grid System), and
		  pre-computing spatial relations between features and the
		  constituent components at a spatial resolution appropriate
		  for the data and use case. That is, by emphasizing the
		  notion of the cell, we can examine what is in a cell,
		  predict contents of its parent and child cells, and quickly
		  get an overview of spatially co-located features and
		  regions of interest without having to directly compute
		  spatial interactions. This paper provides an ontology
		  design pattern, to be used as a structural template, for
		  modeling how features or regions map onto a hierarchical
		  grid system and addresses how the attributes of these
		  features may be inherited upwards or downwards through the
		  hierarchy. We furthermore provide a motivating example and
		  implementation.},
  booktitle	= {Proceedings of the 10th International Joint Conference on
		  Knowledge Graphs},
  pages		= {108–114},
  numpages	= {7},
  keywords	= {ontology engineering, ontology design pattern,
		  geoinformation science},
  location	= {Virtual Event, Thailand},
  series	= {IJCKG '21}
}

@InProceedings{	  10.1145/3543873.3587359,
  author	= {Alobaid, Ahmad and Toledo, Jhon and Corcho, Oscar and
		  Poveda-Villal\'{o}n, Mar\'{\i}a},
  title		= {Depicting Vocabulary Summaries with Devos},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587359},
  doi		= {10.1145/3543873.3587359},
  abstract	= {Communicating ontologies to potential users is still a
		  difficult and time-consuming task. Even for small ones,
		  users need to invest time to determine whether to reuse
		  them. Providing diagrams together with the ontologies
		  facilitates the task of understanding the model from a user
		  perspective. While some tools are available for depicting
		  ontologies, and the code could also be inspected using
		  ontology editors’ graphical interfaces, in many cases,
		  the diagrams are too big or complex. The main objective of
		  this demo is to present Devos, a system to generate
		  ontology diagrams based on different strategies for
		  summarizing the ontology.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {250–253},
  numpages	= {4},
  keywords	= {ontology diagrams, ontology engineering, ontology
		  summarization},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@Article{	  10.1145/3464383,
  author	= {Crovari, Pietro and Pid\`{o}, Sara and Pinoli, Pietro and
		  Bernasconi, Anna and Canakoglu, Arif and Garzotto, Franca
		  and Ceri, Stefano},
  title		= {GeCoAgent: A Conversational Agent for Empowering Genomic
		  Data Extraction and Analysis},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {1},
  url		= {https://doi.org/10.1145/3464383},
  doi		= {10.1145/3464383},
  abstract	= {With the availability of reliable and low-cost DNA
		  sequencing, human genomics is relevant to a growing number
		  of end-users, including biologists and clinicians. Typical
		  interactions require applying comparative data analysis to
		  huge repositories of genomic information for building new
		  knowledge, taking advantage of the latest findings in
		  applied genomics for healthcare. Powerful technology for
		  data extraction and analysis is available, but broad use of
		  the technology is hampered by the complexity of accessing
		  such methods and tools.This work presents GeCoAgent, a
		  big-data service for clinicians and biologists. GeCoAgent
		  uses a dialogic interface, animated by a chatbot, for
		  supporting the end-users’ interaction with computational
		  tools accompanied by multi-modal support. While the
		  dialogue progresses, the user is accompanied in extracting
		  the relevant data from repositories and then performing
		  data analysis, which often requires the use of statistical
		  methods or machine learning. Results are returned using
		  simple representations (spreadsheets and graphics), while
		  at the end of a session the dialogue is summarized in
		  textual format. The innovation presented in this article is
		  concerned with not only the delivery of a new tool but also
		  our novel approach to conversational technologies,
		  potentially extensible to other healthcare domains or to
		  general data science.},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= oct,
  articleno	= {3},
  numpages	= {29},
  keywords	= {genomic computing, natural language understanding,
		  Conversational agents}
}

@InProceedings{	  10.1145/3241653.3241655,
  author	= {Verbitskaia, Ekaterina and Kirillov, Ilya and Nozkin, Ilya
		  and Grigorev, Semyon},
  title		= {Parser combinators for context-free path querying},
  year		= {2018},
  isbn		= {9781450358361},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3241653.3241655},
  doi		= {10.1145/3241653.3241655},
  abstract	= {Transparent integration of a domain-specific language for
		  specification of context-free path queries (CFPQs) into a
		  general-purpose programming language as well as static
		  checking of errors in queries may greatly simplify the
		  development of applications using CFPQs. LINQ and ORM can
		  be used for the integration, but they have issues with
		  flexibility: query decomposition and reusing of subqueries
		  are a challenge. Adaptation of parser combinators technique
		  for paths querying may solve these problems. Conventional
		  parser combinators process linear input, and only the
		  Trails library is known to apply this technique for path
		  querying. Trails suffers the common parser combinators
		  issue: it does not support left-recursive grammars and also
		  experiences problems in cycles handling. We demonstrate
		  that it is possible to create general parser combinators
		  for CFPQ which support arbitrary context-free grammars and
		  arbitrary input graphs. We implement a library of such
		  parser combinators and show that it is applicable for
		  realistic tasks.},
  booktitle	= {Proceedings of the 9th ACM SIGPLAN International Symposium
		  on Scala},
  pages		= {13–23},
  numpages	= {11},
  keywords	= {Scala, Parser Combinators, Neo4j, Language-Constrained
		  Path Problem, Graph Databases, Generalized LL, GLL,
		  Context-Free Path Querying, Context-Free Language
		  Reachability},
  location	= {St. Louis, MO, USA},
  series	= {Scala 2018}
}

@Article{	  10.1145/3575865,
  author	= {Banar, Nikolay and Daelemans, Walter and Kestemont, Mike},
  title		= {Transfer Learning for the Visual Arts: The Multi-modal
		  Retrieval of Iconclass Codes},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {2},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3575865},
  doi		= {10.1145/3575865},
  abstract	= {Iconclass is an iconographic thesaurus, which is widely
		  used in the digital heritage domain to describe subjects
		  depicted in artworks. Each subject is assigned a unique
		  descriptive code, which has a corresponding textual
		  definition. The assignment of Iconclass codes is a
		  challenging task for computational systems, due to the
		  large number of available labels in comparison to the
		  limited amount of training data available. Transfer
		  learning has become a common strategy to overcome such a
		  data shortage. In deep learning, transfer learning consists
		  in fine-tuning the weights of a deep neural network for a
		  downstream task. In this work, we present a deep retrieval
		  framework, which can be fully fine-tuned for the task under
		  consideration. Our work is based on a recent approach to
		  this task, which already yielded state-of-the-art
		  performance, although it could not be fully fine-tuned yet.
		  This approach exploits the multi-linguality and
		  multi-modality that is inherent to digital heritage data.
		  Our framework jointly processes multiple input modalities,
		  namely, textual and visual features. We extract the textual
		  features from the artwork titles in multiple languages,
		  whereas the visual features are derived from photographic
		  reproductions of the artworks. The definitions of the
		  Iconclass codes, containing useful textual information, are
		  used as target labels instead of the codes themselves. As
		  our main contribution, we demonstrate that our approach
		  outperforms the state-of-the-art by a large margin. In
		  addition, our approach is superior to the M3P feature
		  extractor and outperforms the multi-lingual CLIP in most
		  experiments due to the better quality of the visual
		  features. Our out-of-domain and zero-shot experiments show
		  poor results and demonstrate that the Iconclass retrieval
		  remains a challenging task. We make our source code and
		  models publicly available to support heritage institutions
		  in the further enrichment of their digital collections.},
  journal	= {J. Comput. Cult. Herit.},
  month		= jun,
  articleno	= {32},
  numpages	= {16},
  keywords	= {multi-lingual retrieval, multi-modal retrieval, natural
		  language processing, deep learning, transfer learning,
		  cultural heritage, Iconclass}
}

@Article{	  10.1145/3610581,
  author	= {Osman, Taha and Khalil, Hussein and Miltan, Mohammed and
		  Shaalan, Khaled and Alfrjani, Rowida},
  title		= {Exploiting Functional Discourse Grammar to Enhance Complex
		  Arabic Relation Extraction using a Hybrid Semantic
		  Knowledge Base - Machine Learning Approach},
  year		= {2023},
  issue_date	= {August 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {8},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3610581},
  doi		= {10.1145/3610581},
  abstract	= {Relation extraction from unstructured Arabic text is
		  especially challenging due to the Arabic language complex
		  morphology and the variation in word semantics and lexical
		  categories. The research documented in this paper presents
		  a hybrid Semantic Knowledge base - Machine Learning (SKML)
		  approach for extracting complex Arabic relations from
		  unstructured Arabic documents; the proposed approach
		  exploits the principles of Functional Discourse Grammar
		  (FDG) to emphasise the semantic and pragmatic properties of
		  the language and facilitate the identification of relation
		  elements. At the initial phase, the novel FDG-SKML relation
		  extraction approach deploys a lexical-based mechanism that
		  utilises a purposely built domain-specific Semantic
		  Knowledge to encode the semantic association between the
		  identified relations’ elements. The evaluation of the
		  initial stage evidenced improved accuracy for extracting
		  most complex Arabic relations. The initial relation
		  extraction mechanism was further extended by integrating
		  its output into a Machine Learning classifier that
		  facilitated extracting especially complex relations with
		  significant disparity in the relation elements’ presence,
		  order, and correlation. Using Economics as the problem
		  domain, experimental evaluation evidenced the high accuracy
		  of our FDG-SKML approach in complex Arabic relation
		  extraction task and demonstrated its further improvement
		  upon integration with machine learning classifiers.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  articleno	= {214},
  numpages	= {30},
  keywords	= {hybrid knowledge-based machine learning classification,
		  Functional Discourse Grammar, semantic web base, Natural
		  Language Processing, Arabic relation extraction}
}

@InProceedings{	  10.1145/3599640.3599647,
  author	= {Lin, Longcheng and Wang, Fang},
  title		= {Adaptive Learning System Based on Knowledge Graph},
  year		= {2023},
  isbn		= {9781450399593},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3599640.3599647},
  doi		= {10.1145/3599640.3599647},
  abstract	= {Since the rapid development of "Internet+Education" ,
		  various artificial intelligence technologies have been
		  applied to teaching and learning. The development of
		  technology has brought great impetus and potential to
		  education. Based on the background of big data, how to use
		  AI technology to mine valuable information in massive data
		  to meet the adaptive learning needs of learners is an
		  important topic that deserves attention and research. This
		  paper builds learner ontology and course knowledge
		  ontology, links learning resources to course knowledge
		  ontology to build domain knowledge graph, designs and
		  implements an adaptive learning system based on knowledge
		  graph, including learner model, domain knowledge model,
		  adaptive learning engine and interactive interface.
		  Finally, Pellet inference engine is used to infer the
		  designed SWRL rules. The result shows that the adaptive
		  learning system proposed in this paper can recommend
		  appropriate learning paths according to the learners'
		  learning status, and present personalized learning
		  resources.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Education and Training Technologies},
  articleno	= {7},
  numpages	= {7},
  keywords	= {recommendation, personalized learning, ontology
		  construction, knowledge graph, Adaptive learning system},
  location	= {Macau, China},
  series	= {ICETT '23}
}

@InProceedings{	  10.1145/3579375.3579391,
  author	= {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla,
		  Sandeep Kumar},
  title		= {TTPHunter: Automated Extraction of Actionable Intelligence
		  as TTPs from Narrative Threat Reports},
  year		= {2023},
  isbn		= {9798400700057},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579375.3579391},
  doi		= {10.1145/3579375.3579391},
  abstract	= {With the proliferation of attacks from various Advanced
		  Persistent Threats (APT) groups, it is essential to
		  comprehend the threat actor’s attack patterns to
		  accelerate threat detection and response. The MITRE
		  ATT&amp;CK framework’s Tactics, Techniques, and
		  Procedures (TTPs) help to decipher attack patterns. The APT
		  reports, published by security firms, contain rich
		  information on tools and techniques used by threat actors.
		  These reports are available in unstructured and natural
		  language texts. There is a need for an automated tool to
		  extract TTPs present in natural language text. However,
		  there are few tools available in the literature, but their
		  performance is not very satisfactory. In this work, we
		  propose TTPHunter, to extract TTPs from APT reports by
		  mapping sentence context to relevant TTPs. We fine-tune
		  linear classifiers, which take input as BERT (Bidirectional
		  Encoder Representations from Transformers) embeddings of
		  sentences. We create two datasets: sentence-based (8,387
		  sentence samples) and document-based (50 threat reports) to
		  validate TTPHunter. TTPHunter achieves the F1-score of 88\%
		  and 75\% for both datasets, respectively. We compare the
		  TTPHunter with rcATT and AttacKG baseline models, and it
		  outperforms both baselines.},
  booktitle	= {Proceedings of the 2023 Australasian Computer Science
		  Week},
  pages		= {126–134},
  numpages	= {9},
  keywords	= {Threat Intelligence, TTP Extraction, Natural Language
		  Processing, MITRE ATT&amp;CK, Cybersecurity},
  location	= {Melbourne, VIC, Australia},
  series	= {ACSW '23}
}

@InProceedings{	  10.1145/3357384.3358025,
  author	= {Amsterdamer, Yael and Milo, Tova and Somech, Amit and
		  Youngmann, Brit},
  title		= {Declarative User Selection with Soft Constraints},
  year		= {2019},
  isbn		= {9781450369763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357384.3358025},
  doi		= {10.1145/3357384.3358025},
  abstract	= {In applications with large userbases such as
		  crowdsourcing, social networks or recommender systems,
		  selecting users is a common and challenging task. Different
		  applications require different policies for selecting
		  users, and implementing such policies is
		  applicationspecific and laborious. To this end, we
		  introduce a novel declarative framework that abstracts
		  common components of the user selection problem, while
		  allowing for domain-specific tuning. The framework is based
		  on an ontology view of user profiles, with respect to which
		  we define a query language for policy specification. Our
		  language extends SPARQL with means for capturing soft
		  constraints which are essential for worker selection. At
		  the core of our query engine is then a novel efficient
		  algorithm for handling these constraints. Our experimental
		  study on real-life data indicates the effectiveness and
		  flexibility of our approach, showing in particular that it
		  outperforms existing task-specific solutions in prominent
		  user selection tasks.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {931–940},
  numpages	= {10},
  keywords	= {user selection, sparql, semantic similarity},
  location	= {Beijing, China},
  series	= {CIKM '19}
}

@InProceedings{	  10.1145/3637528.3671542,
  author	= {Baughman, Aaron and Morales, Eduardo and Agarwal, Rahul
		  and Akay, Gozde and Feris, Rogerio and Johnson, Tony and
		  Hammer, Stephen and Karlinsky, Leonid},
  title		= {Large Scale Generative AI Text Applied to Sports and
		  Music},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671542},
  doi		= {10.1145/3637528.3671542},
  abstract	= {We address the problem of scaling up the production of
		  media content, including commentary and personalized news
		  stories, for large-scale sports and music events worldwide.
		  Our approach relies on generative AI models to transform a
		  large volume of multimodal data (e.g., videos, articles,
		  real-time scoring feeds, statistics, and fact sheets) into
		  coherent and fluent text. Based on this approach, we
		  introduce, for the first time, an AI commentary system,
		  which was deployed to produce automated narrations for
		  highlight packages at the 2023 US Open, Wimbledon, and
		  Masters tournaments. In the same vein, our solution was
		  extended to create personalized content for ESPN Fantasy
		  Football and stories about music artists for the GRAMMY
		  awards. These applications were built using a common
		  software architecture achieved a 15x speed improvement with
		  an average Rouge-L of 82.00 and perplexity of 6.6. Our work
		  was successfully deployed at the aforementioned events,
		  supporting 90 million fans around the world with 8 billion
		  page views, continuously pushing the bounds on what is
		  possible at the intersection of sports, entertainment, and
		  AI.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4784–4792},
  numpages	= {9},
  keywords	= {applied computing, generative ai, large scale computing,
		  neural networks, sports and entertainment},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3638062,
  author	= {Aouachria, Moufida and Leshob, Abderrahmane and Ghomari,
		  Abdessamed R\'{e}da and Aouache, Mustapha},
  title		= {A Process Mining Method for Inter-organizational Business
		  Process Integration},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {1},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3638062},
  doi		= {10.1145/3638062},
  abstract	= {Business process integration (BPI) allows organizations to
		  connect and automate their business processes in order to
		  deliver the right economic resources at the right time,
		  place, and price. BPI requires the integration of business
		  processes and their supporting systems across multiple
		  autonomous organizations. However, such integration is
		  complex and can face coordination complexities that occur
		  during the resource exchanges between the partners’
		  processes. This article proposes a new method called
		  Process Mining for Business Process Integration (PM4BPI)
		  that helps process designers to perform BPI by creating new
		  process models that cross the boundaries of multiple
		  organizations from a collection of process event logs.
		  PM4BPI uses federated process mining techniques to detect
		  incompatibilities before the integration of the partners’
		  processes. Then, it applies process adaptation patterns to
		  solve detected incompatibilities. Finally, organizations’
		  processes are merged to build a collaborative process model
		  that crosses the organizations’ boundaries. AdaptWF_Net,
		  an extension of a Petri net, is used to design
		  inter-organizational business processes and adaptation
		  patterns. An integrated care pathway is used as a case
		  study to assess the applicability and effectiveness of the
		  proposed method.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= mar,
  articleno	= {2},
  numpages	= {29},
  keywords	= {Business process integration, inter-organizational
		  business process model, process mining, adaptation
		  patterns, process modelling}
}

@InProceedings{	  10.1145/3501409.3501592,
  author	= {Shan, Ruikang and Jiang, Tao and Wang, Yetong},
  title		= {Research on the Construction of Domain Sentiment Lexicon
		  Based on Label Propagation Algorithm},
  year		= {2022},
  isbn		= {9781450384322},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3501409.3501592},
  doi		= {10.1145/3501409.3501592},
  abstract	= {With the rapid development of the Internet and the
		  explosive growth of online information, timely monitoring
		  and guidance of public opinion is the key to maintaining a
		  safe online environment. Sentiment lexicon is an important
		  corpus resource in fields such as opinion analysis, and
		  when faced with tasks in different domains, generic
		  sentiment lexicon has been difficult to meet the demand, so
		  researchers have remembered to focus on domain sentiment
		  lexicon. In this paper, we propose a method based on
		  improved label propagation to achieve automatic
		  construction of Chinese sentiment lexicon from the
		  sentence-level text. The key idea is to use lexical rules
		  and sentiment association corrections to take the
		  computational approach of optimizing mutual information of
		  points, both of which improve the algorithm's effectiveness
		  in analyzing complex sentences and thus enhance the
		  accuracy of sentiment word recognition. The experimental
		  results show that the method can automatically build a
		  sentiment lexicon according to the corpus, with high
		  quality and certain domain adaptability.},
  booktitle	= {Proceedings of the 2021 5th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {1024–1029},
  numpages	= {6},
  keywords	= {Domain sentiment lexicon, Label propagation, Pointwise
		  mutual information, Public opinion analysis, Sentiment
		  analysis},
  location	= {Xiamen, China},
  series	= {EITCE '21}
}

@InProceedings{	  10.1109/jcdl57899.2023.00038,
  author	= {Sierra-M\'{u}nera, Alejandro and Westphal, Jan and
		  Krestel, Ralf},
  title		= {Efficient Ultrafine Typing of Named Entities},
  year		= {2024},
  isbn		= {9798350399318},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL57899.2023.00038},
  doi		= {10.1109/JCDL57899.2023.00038},
  abstract	= {Ultrafine named entity typing (UFET) refers to the
		  assignment of predefined labels to entity mentions in a
		  given context. In contrast to traditional named entity
		  typing, the number of potential labels is in the thousands
		  and one mention can have more than one assigned type.
		  Previous approaches either depend on large training
		  datasets, or require inefficient encoding of all input-type
		  combinations. Therefore, there is a need for investigating
		  the efficiency during training and prediction of entity
		  typing models in the ultrafine-grained setting, considering
		  its distinctively bigger search space, compared to the
		  coarse- and fine-grained tasks. To efficiently solve UFET,
		  we propose Decent, a lightweight model that encodes, using
		  a pretrained language model, the input sentences separately
		  from the type labels. Additionally, we make use of negative
		  oversampling to speed up the training while improving the
		  generalization of unseen types. Using an openly available
		  UFET dataset, we evaluated the classification and runtime
		  performance of Decent and observed that training and
		  prediction runtime is orders of magnitude faster than the
		  current state-of-the-art approaches, while maintaining a
		  competitive classification performance.},
  booktitle	= {Proceedings of the 2023 ACM/IEEE Joint Conference on
		  Digital Libraries},
  pages		= {205–214},
  numpages	= {10},
  keywords	= {ultrafine enity typing, named entity recognition},
  location	= {Santa Fe, New Mexico, USA},
  series	= {JCDL '23}
}

@InProceedings{	  10.1145/3627673.3679175,
  author	= {Egami, Shusaku and Ugai, Takanori and Htun, Swe Nwe Nwe
		  and Fukuda, Ken},
  title		= {VHAKG: A Multi-modal Knowledge Graph Based on Synchronized
		  Multi-view Videos of Daily Activities},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679175},
  doi		= {10.1145/3627673.3679175},
  abstract	= {Multi-modal knowledge graphs (MMKGs), which ground various
		  non-symbolic data (e.g., images and videos) into symbols,
		  have attracted attention as resources enabling knowledge
		  processing and machine learning across modalities. However,
		  the construction of MMKGs for videos consisting of multiple
		  events, such as daily activities, is still in the early
		  stages. In this paper, we construct an MMKG based on
		  synchronized multi-view simulated videos of daily
		  activities. Besides representing the content of daily life
		  videos as event-centric knowledge, our MMKG also includes
		  frame-by-frame fine-grained changes, such as bounding boxes
		  within video frames. In addition, we provide support tools
		  for querying our MMKG. As an application example, we
		  demonstrate that our MMKG facilitates benchmarking
		  vision-language models by providing the necessary
		  vision-language datasets for a tailored task.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5360–5364},
  numpages	= {5},
  keywords	= {daily life video, event-centric knowledge graph,
		  multi-modal knowledge graph, synthetic data, visual
		  question answering},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3194658.3194668,
  author	= {Reda, Roberto and Piccinini, Filippo and Carbonaro,
		  Antonella},
  title		= {Towards Consistent Data Representation in the IoT
		  Healthcare Landscape},
  year		= {2018},
  isbn		= {9781450364935},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3194658.3194668},
  doi		= {10.1145/3194658.3194668},
  abstract	= {Nowadays, the enormous volume of health and fitness data
		  gathered from IoT wearable devices offers favourable
		  opportunities to the research community. For instance, it
		  can be exploited using sophisticated data analysis
		  techniques, such as automatic reasoning, to find patterns
		  and, extract information and new knowledge in order to
		  enhance decision-making and deliver better healthcare.
		  However, due to the high heterogeneity of data
		  representation formats, the IoT healthcare landscape is
		  characterised by an ubiquitous presence of data silos which
		  prevents users and clinicians from obtaining a consistent
		  representation of the whole knowledge. Semantic web
		  technologies, such as ontologies and inference rules, have
		  been shown as a promising way for the integration and
		  exploitation of data from heterogeneous sources. In this
		  paper, we present a semantic data model useful to: (1)
		  consistently represent health and fitness data from
		  heterogeneous IoT sources; (2) integrate and exchange them;
		  and (3) enable automatic reasoning by inference engines.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Digital Health},
  pages		= {5–10},
  numpages	= {6},
  keywords	= {semantic web technologies, ontology-based data
		  representation, internet of things, health informatics},
  location	= {Lyon, France},
  series	= {DH '18}
}

@InProceedings{	  10.1145/3573381.3596150,
  author	= {Robert, Florent and Wu, Hui-Yin and Sassatelli, Lucile and
		  Ramano\"{e}l, Stephen and Gros, Auriane and Winckler,
		  Marco},
  title		= {An Integrated Framework for Understanding Multimodal
		  Embodied Experiences in Interactive Virtual Reality},
  year		= {2023},
  isbn		= {9798400700286},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3573381.3596150},
  doi		= {10.1145/3573381.3596150},
  abstract	= {Virtual Reality (VR) technology enables “embodied
		  interactions” in realistic environments where users can
		  freely move and interact, with deep physical and emotional
		  states. However, a comprehensive understanding of the
		  embodied user experience is currently limited by the extent
		  to which one can make relevant observations, and the
		  accuracy at which observations can be interpreted. Paul
		  Dourish proposed a way forward through the characterisation
		  of embodied interactions in three senses: ontology,
		  intersubjectivity, and intentionality. In a joint effort
		  between computer and neuro-scientists, we built a framework
		  to design studies that investigate multimodal embodied
		  experiences in VR, and apply it to study the impact of
		  simulated low-vision on user navigation. Our methodology
		  involves the design of 3D scenarios annotated with an
		  ontology, modelling intersubjective tasks, and correlating
		  multimodal metrics such as gaze and physiology to derive
		  intentions. We show how this framework enables a more
		  fine-grained understanding of embodied interactions in
		  behavioural research.},
  booktitle	= {Proceedings of the 2023 ACM International Conference on
		  Interactive Media Experiences},
  pages		= {14–26},
  numpages	= {13},
  keywords	= {user experience analysis, task modeling, scene ontology,
		  navigation, interaction, immersion, Embodied experiences,
		  3D environments},
  location	= {Nantes, France},
  series	= {IMX '23}
}

@InProceedings{	  10.1145/3643662.3643962,
  author	= {Falcarin, Paolo and Dainese, Fabio},
  title		= {Building a Cybersecurity Knowledge Graph with CyberGraph},
  year		= {2024},
  isbn		= {9798400705656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643662.3643962},
  doi		= {10.1145/3643662.3643962},
  abstract	= {Software engineers and security professionals rely on a
		  variety of sources of information, including known
		  vulnerabilities, newly identified weaknesses, and threats,
		  as well as attack patterns and current mitigations. Such
		  information, spread across different places, results in an
		  increased effort for developers in following all the
		  cross-referenced data and finding appropriate solutions to
		  their security issues in a timely manner. Software
		  developers cannot have a good knowledge of the breadth of
		  the different issues and vulnerabilities that are
		  constantly increasing in time; the raising number of
		  security issues to tackle cannot be matched by software
		  developers which need more help from intelligent tools.
		  Therefore, in this work, we present CyberGraph, a tool to
		  automatically build and update a single, easily queryable
		  cybersecurity knowledge graph by automatically linking
		  heterogeneous data from different public repositories. The
		  resulting unique integrated dataset, thanks to its
		  magnitude, allows the execution of sophisticated queries
		  that can quickly provide new insights and valuable
		  perspectives.},
  booktitle	= {Proceedings of the 2024 ACM/IEEE 4th International
		  Workshop on Engineering and Cybersecurity of Critical
		  Systems (EnCyCriS) and 2024 IEEE/ACM Second International
		  Workshop on Software Vulnerability},
  pages		= {29–36},
  numpages	= {8},
  keywords	= {cybersecurity, knowledge graph, software vulnerabilities,
		  visualization, Neo4j, MITRE},
  location	= {Lisbon, Portugal},
  series	= {EnCyCriS/SVM '24}
}

@InProceedings{	  10.1145/3641584.3641793,
  author	= {Guan, Wei and Lian, Xiaoru and Ma, Li},
  title		= {CA-WGE: A two-view graph neural network-based knowledge
		  graph completion approach combining common sense
		  perception},
  year		= {2024},
  isbn		= {9798400707674},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3641584.3641793},
  doi		= {10.1145/3641584.3641793},
  abstract	= {The knowledge graph completion algorithm can make the
		  knowledge graph more complete and is currently a research
		  hotspot in the field of artificial intelligence. The
		  knowledge graph completion model is mainly defined in three
		  aspects, the way of negative example generation, the design
		  of scoring function and the design of loss function. The
		  previous knowledge graph completion models only rely on
		  factual view data to predict the missing links between
		  entities and ignore the valuable common sense knowledge,
		  and there is invalid negative sampling in knowledge graph
		  embedding techniques; on the other hand, the existing graph
		  neural network-based knowledge graph embedding models
		  mainly consider capturing the graph structure around
		  entities, and the relational representation is only used to
		  update the entity embedding, which may miss the potentially
		  useful information about the relational structure of
		  potentially useful information. To address the above
		  challenges, this paper proposes a two-view graph neural
		  network-based knowledge graph completion model combined
		  with common sense awareness. Common knowledge is first
		  automatically extracted from fact triples with entity
		  concepts to facilitate high-quality negative sampling, and
		  then positive and weighted negative triples are fed into
		  the two-view graph neural network-based knowledge graph
		  embedding model to capture entity- and relationship-centric
		  graph structures and learn vector representations of
		  entities and relationships, and then the learned entity and
		  relationship representations are fed into a weighted score
		  function to return the final the final score. Extensive
		  experimental and ablation studies on four datasets, FB15K,
		  FB15K237, NELL995, and DBpedia-242, show that the model
		  achieves better performance compared to the
		  state-of-the-art models.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Artificial Intelligence and Pattern Recognition},
  pages		= {1382–1389},
  numpages	= {8},
  keywords	= {Common Sense Awareness, Graph Neural Network, Knowledge
		  Graph Completion, Knowledge Graph Embedding, Negative
		  Sampling},
  location	= {Xiamen, China},
  series	= {AIPR '23}
}

@Article{	  10.1145/3754450,
  author	= {Lian, Xiaoli and Wu, Jiajun and Gao, Xiaoyun and Wang,
		  Shuaisong and Zhang, Li},
  title		= {Vision to Specification: Automating the Transition from
		  Conceptual Features to Functional Requirements},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3754450},
  doi		= {10.1145/3754450},
  abstract	= {The translation of high-level abstract features into
		  clear, and testable functional requirements (FRs) is a
		  crucial step in software development, bridging the gap
		  between user needs and technical specifications. In
		  engineering practice, significant expert effort is needed
		  for this translation. Our approach, EasyFR, streamlines the
		  process by recommending Semantic Role Labeling (SRL)
		  sequences for the given abstract features to guide
		  Pre-trained Language Models (PLMs) in producing cohesive FR
		  statements. By analyzing ten diverse datasets, we induce
		  two variable SRL templates, each including two configurable
		  parts. For concrete features, our proposed Key2Temp model
		  can construct the appropriate variant of the SRL template
		  by identifying a variable SRL template and placing the
		  feature tokens in the appropriate slots. In this way, our
		  approach reframes the process of requirement generation
		  into a structured slot-filling activity. Experimental
		  validation on four open datasets demonstrates that EasyFR
		  outperforms three advanced Natural language generation
		  (NLG) approaches, including GPT-4, particularly when
		  existing FRs are available for training. The positive
		  influence of our SRL template variant recommendations is
		  further confirmed through an ablation study. We believe
		  that our results indicate a notable step forward in the
		  realm of automated requirements synthesis, holding
		  potential to improve the process of requirements
		  specification in future software projects.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= sep,
  keywords	= {Software Requirements Synthesis, Features, Semantic
		  Templates, Slot Filling}
}

@InProceedings{	  10.1145/3395027.3419585,
  author	= {Alpizar-Chacon, Isaac and Sosnovsky, Sergey},
  title		= {Order out of Chaos: Construction of Knowledge Models from
		  PDF Textbooks},
  year		= {2020},
  isbn		= {9781450380003},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3395027.3419585},
  doi		= {10.1145/3395027.3419585},
  abstract	= {Textbooks are educational documents created, structured
		  and formatted by domain experts with the main purpose to
		  explain the knowledge in the domain to a novice. Authors
		  use their understanding of the domain when structuring and
		  formatting the content of a textbook to facilitate this
		  explanation. As a result, the formatting and structural
		  elements of textbooks carry the elements of domain
		  knowledge implicitly encoded by their authors. Our paper
		  presents an extendable approach towards automated
		  extraction of this knowledge from textbooks taking into
		  account their formatting rules and internal structure. We
		  focus on PDF as the most common textbook representation
		  format; however, the overall method is applicable to other
		  formats as well. The evaluation experiments examine the
		  accuracy of the approach, as well as the pragmatic quality
		  of the obtained knowledge models using one of their
		  possible applications -- semantic linking of textbooks in
		  the same domain. The results indicate high accuracy of
		  model construction on symbolic, syntactic and structural
		  levels across textbooks and domains, and demonstrate the
		  added value of the extracted models on the semantic
		  level.},
  booktitle	= {Proceedings of the ACM Symposium on Document Engineering
		  2020},
  articleno	= {8},
  numpages	= {10},
  keywords	= {textbook, model extraction, knowledge modeling, PDF
		  processing},
  location	= {Virtual Event, CA, USA},
  series	= {DocEng '20}
}

@InProceedings{	  10.1145/3691620.3695037,
  author	= {Guo, An and Zhou, Yuan and Tian, Haoxiang and Fang,
		  Chunrong and Sun, Yunjian and Sun, Weisong and Gao, Xinyu
		  and Luu, Anh Tuan and Liu, Yang and Chen, Zhenyu},
  title		= {SoVAR: Build Generalizable Scenarios from Accident Reports
		  for Autonomous Driving Testing},
  year		= {2024},
  isbn		= {9798400712487},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691620.3695037},
  doi		= {10.1145/3691620.3695037},
  abstract	= {Autonomous driving systems (ADSs) have undergone
		  remarkable development and are increasingly employed in
		  safety-critical applications. However, recently reported
		  data on fatal accidents involving ADSs suggests that the
		  desired level of safety has not yet been fully achieved.
		  Consequently, there is a growing need for more
		  comprehensive and targeted testing approaches to ensure
		  safe driving. Scenarios from real-world accident reports
		  provide valuable resources for ADS testing, including
		  critical scenarios and high-quality seeds. However,
		  existing scenario reconstruction methods from accident
		  reports often exhibit limited accuracy in information
		  extraction. Moreover, due to the diversity and complexity
		  of road environments, matching current accident information
		  with the simulation map data for reconstruction poses
		  significant challenges.In this paper, we design and
		  implement SoVAR, a tool for automatically generating
		  road-generalizable scenarios from accident reports. SoVAR
		  utilizes well-designed prompts with linguistic patterns to
		  guide the large language model (LLM) in extracting accident
		  information from textual data. Subsequently, it formulates
		  and solves accident-related constraints in conjunction with
		  the extracted accident information to generate accident
		  trajectories. Finally, SoVAR reconstructs accident
		  scenarios on various map structures and converts them into
		  test scenarios to evaluate its capability to detect defects
		  in industrial ADSs. We experiment with SoVAR, using the
		  accident reports from the National Highway Traffic Safety
		  Administration's (NHTSA) database to generate test
		  scenarios for the industrial-grade ADS Apollo. The
		  experimental findings demonstrate that SoVAR can
		  effectively generate generalized accident scenarios across
		  different road structures. Furthermore, the results confirm
		  that SoVAR identified 5 distinct safety violation types
		  that contributed to the crash of Baidu Apollo.},
  booktitle	= {Proceedings of the 39th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {268–280},
  numpages	= {13},
  keywords	= {software testing, automatic test generation, constraint
		  solving, autonomous driving system},
  location	= {Sacramento, CA, USA},
  series	= {ASE '24}
}

@InProceedings{	  10.1109/mise.2019.00019,
  author	= {Babur, \"{O}nder and Stephan, Matthew},
  title		= {MoCoP: towards a model clone portal},
  year		= {2019},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/MiSE.2019.00019},
  doi		= {10.1109/MiSE.2019.00019},
  abstract	= {Widespread and mature practice of model-driven engineering
		  is leading to a growing number of modeling artifacts and
		  challenges in their management. Model clone detection (MCD)
		  is an important approach for managing and maintaining
		  modeling artifacts. While its counterpart in traditional
		  source code development, code clone detection, is enjoying
		  popularity and more than two decades of development, MCD is
		  still in its infancy in terms of research and tooling. We
		  aim to develop a portal for model clone detection, MoCoP,
		  as a central hub to mitigate adoption barriers and foster
		  MCD research. In this short paper, we present our vision
		  for MoCoP and its features and goals. We discuss MoCoP's
		  key components that we plan on realizing in the short term
		  including public tooling, curated data sets, and a body of
		  MCD knowledge. Our longer term goals include a dedicated
		  service-oriented infrastructure, contests, and forums. We
		  believe MoCoP will strengthen MCD research, tooling, and
		  the community, which in turn will lead to better quality,
		  maintenance, and scalability for model-driven engineering
		  practices.},
  booktitle	= {Proceedings of the 11th International Workshop on
		  Modelling in Software Engineerings},
  pages		= {78–81},
  numpages	= {4},
  keywords	= {software maintenance, model-driven engineering, model
		  repositories, model management, model clone detection,
		  model analytics},
  location	= {Montreal, Quebec, Canada},
  series	= {MiSE '19}
}

@Article{	  10.1109/taslp.2023.3302232,
  author	= {Lim, Jungwoo and Whang, Taesun and Lee, Dongyub and Lim,
		  Heuiseok},
  title		= {Adaptive Multi-Domain Dialogue State Tracking on Spoken
		  Conversations},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3302232},
  doi		= {10.1109/TASLP.2023.3302232},
  abstract	= {The main objective of the task-oriented dialogue system is
		  to identify the intent and needs of human dialogue. Many
		  existing studies are conducted under the setting of written
		  dialogue, but there always exists a difficulty in coping
		  with real-world spoken dialogues. To this end, DSTC10
		  challenge organizers propose the task of building robust
		  dialogue state tracking (DST) models on spoken dialogues.
		  With the powerful existing DST model (i.e., MinTL), this
		  article suggests integral components for building a
		  dialogue state tracker; 1) Data augmentation effectively
		  enhances the capability of the model to catch the entities
		  that exist in the evaluation dataset. 2) Levenshtein
		  post-processing aims to prevent the distortion in model
		  prediction caused by automatic speech recognition errors.
		  To validate the effectiveness of our methods, we evaluate
		  our model on DSTC10 datasets and conduct qualitative
		  analysis by ablating each component of the model.
		  Experimental results show that our model significantly
		  outperforms baselines in all evaluation metrics and took
		  3rd place in the challenge.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {727–732},
  numpages	= {6}
}

@InProceedings{	  10.1145/3691620.3695019,
  author	= {Zhao, Jiuang and Yang, Zitian and Zhang, Li and Lian,
		  Xiaoli and Yang, Donghao and Tan, Xin},
  title		= {DRMiner: Extracting Latent Design Rationale from Jira
		  Issue Logs},
  year		= {2024},
  isbn		= {9798400712487},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691620.3695019},
  doi		= {10.1145/3691620.3695019},
  abstract	= {Software architectures are usually meticulously designed
		  to address multiple quality concerns and support long-term
		  maintenance. However, there may be a lack of motivation for
		  developers to document design rationales (i.e., the design
		  alternatives and the underlying arguments for making or
		  rejecting decisions) when they will not gain immediate
		  benefit, resulting in a lack of standard capture of these
		  rationales. With the turnover of developers, the
		  architecture inevitably becomes eroded. This issue has
		  motivated a number of studies to extract design knowledge
		  from open-source communities in recent years.
		  Unfortunately, none of the existing research has
		  successfully extracted solutions alone with their
		  corresponding arguments due to challenges such as the
		  intricate semantics of online discussions and the lack of
		  benchmarks for design rationale extraction.In this paper,
		  we propose a novel approach, named DRMiner, to
		  automatically mine latent design rationales from
		  developers' live discussion in open-source community (i.e.,
		  issue logs in Jira). To better identify solutions and their
		  relevant arguments, DRMiner skillfully decomposes the
		  problem into multiple text classification tasks and tackles
		  them using prompt tuning of large language models (LLMs)
		  and specific heuristic features. To evaluate DRMiner, we
		  acquire issue logs from Cassandra, Flink, and Solr
		  repositories in Jira and form a dataset for design
		  rationale mining. Experimental results show that DRMiner
		  outperforms all baselines and achieves F1 improvements of
		  24\%, 22\%, and 20\% for mining design rationales,
		  solutions, and arguments, respectively, compared to the
		  best baseline. Furthermore, we investigate the usefulness
		  of the design rationales mined by DRMiner for automated
		  program repair (APR) and find that advanced LLMs, when
		  prompted with these extracted rationales, generate
		  10\texttimes{}-18\texttimes{} more full-match patches and
		  achieve a 10\%-13\% gain in CodeBLEU scores.},
  booktitle	= {Proceedings of the 39th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {468–480},
  numpages	= {13},
  keywords	= {design rationale, issue logs, design discussion, design
		  recovery, program maintenance},
  location	= {Sacramento, CA, USA},
  series	= {ASE '24}
}

@Article{	  10.1145/3451219,
  author	= {Nayak, Stuti and Zaveri, Amrapali and Serrano, Pedro
		  Hernandez and Dumontier, Michel},
  title		= {Experience: Automated Prediction of Experimental Metadata
		  from Scientific Publications},
  year		= {2021},
  issue_date	= {December 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {4},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3451219},
  doi		= {10.1145/3451219},
  abstract	= {While there exists an abundance of open biomedical data,
		  the lack of high-quality metadata makes it challenging for
		  others to find relevant datasets and to reuse them for
		  another purpose. In particular, metadata are useful to
		  understand the nature and provenance of the data. A common
		  approach to improving the quality of metadata relies on
		  expensive human curation, which itself is time-consuming
		  and also prone to error. Towards improving the quality of
		  metadata, we use scientific publications to automatically
		  predict metadata key:value pairs. For prediction, we use a
		  Convolutional Neural Network (CNN) and a Bidirectional
		  Long-short term memory network (BiLSTM). We focus our
		  attention on the NCBI Disease Corpus, which is used for
		  training the CNN and BiLSTM. We perform two different kinds
		  of experiments with these two architectures: (1) we predict
		  the disease names by using their unique ID in the MeSH
		  ontology and (2) we use the tree structures of MeSH
		  ontology to move up in the hierarchy of these disease
		  terms, which reduces the number of labels. We also perform
		  various multi-label classification techniques for the
		  above-mentioned experiments. We find that in both cases CNN
		  achieves the best results in predicting the superclasses
		  for disease with an accuracy of 83\%.},
  journal	= {J. Data and Information Quality},
  month		= aug,
  articleno	= {21},
  numpages	= {11},
  keywords	= {natural language processing, quality, metadata, neural
		  networks, Datasets}
}

@Proceedings{	  10.1145/3643666,
  title		= {MO2RE 2024: Proceedings of the 1st IEEE/ACM Workshop on
		  Multi-disciplinary, Open, and RElevant Requirements
		  Engineering},
  year		= {2024},
  isbn		= {9798400705694},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Requirements engineering (RE) is a critical sub-field of
		  software engineering (SE) that deals with identifying,
		  specifying, modeling, analyzing, and validating the needs
		  of stakeholders and constraints of a system [1]. RE covers
		  human-related aspects, as stakeholders need to be involved
		  in eliciting and validating the requirements, as well as
		  more technical aspects, as requirements can be
		  systematically collected (e.g., from app reviews) using
		  data mining techniques and analyzed with natural language
		  processing (NLP) approaches, e.g., to identify quality
		  issues or trace links [2]. Despite the broad spectrum of
		  activities that RE covers, researchers from outside RE
		  often have a misconception that RE is limited to writing
		  and analyzing requirements specifications. Consequently,
		  many researchers in the SE community working on RE-relevant
		  problems (e.g., human-centric SE) are often unaware that
		  such problems belong to the RE research strands. Broadly
		  speaking, RE is under-represented and under-appreciated in
		  the SE community.Despite this limited presence, RE is more
		  and more fundamental to cope with the current state of SE,
		  especially considering the recent disruptive changes in
		  artificial intelligence (AI) and NLP caused by large
		  language models (LLMs) and their applications, ChatGPT
		  being a notable example. Given the increasing pervasiveness
		  of AI-based systems in our daily life, there is a growing
		  need for RE techniques to support sound and structured
		  development of AI systems [3], with a particular interest
		  in explainability, interpretability, reliability, fairness,
		  and other ethical concerns [4]. At the same time, current
		  developments in AI can solve long-standing RE problems,
		  such as automatic requirements tracing, completeness
		  checking, and modeling. AI can further create better
		  connections between RE and other automated SE fields.The
		  1st International Workshop on (Multi-disciplinary, Open,
		  and RElevant RE) (MO2RE) has the goal to address these
		  issues by raising awareness of RE's diverse aspects and
		  fostering collaboration within the SE community.},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3734436.3734439,
  author	= {Hasel Mehri, Gelareh and Morisset, Charles and Zannone,
		  Nicola},
  title		= {Towards Explainable Access Control [BlueSky Paper]},
  year		= {2025},
  isbn		= {9798400715037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3734436.3734439},
  doi		= {10.1145/3734436.3734439},
  abstract	= {Access control (AC) systems play an important role in
		  ensuring security by regulating how resources are accessed,
		  protecting sensitive information, and maintaining system
		  integrity. Their complexity arises not only from diverse
		  policies and mechanisms but also from the involvement of
		  multiple stakeholders, including resource owners,
		  administrators, and end-users. Taking inspiration from
		  explainable AI and explainable security, we define the
		  first model of access control explainability, as a quality
		  measure of the explanation graph constructed around the
		  decisions made within the AC system. We then explore the
		  literature to identify how existing work can be integrated
		  as explanatory processes. Finally, we leverage our
		  framework to articulate three open research challenges: the
		  collection and interpretation of AC decisions, the
		  effective construction of AC explanation graphs, and the
		  definition of meaningful and computationally efficient
		  explanation quality metrics.},
  booktitle	= {Proceedings of the 30th ACM Symposium on Access Control
		  Models and Technologies},
  pages		= {117–126},
  numpages	= {10},
  keywords	= {explainable access control, explainability framework,
		  policy comprehension},
  location	= {USA},
  series	= {SACMAT '25}
}

@InProceedings{	  10.1145/3526242.3526254,
  author	= {Chatzipanagiotou, Marita and Machotka, Ewa and
		  Pavlopoulos, John},
  title		= {Automated recognition of geographical named entities in
		  titles of Ukiyo-e prints},
  year		= {2022},
  isbn		= {9781450387361},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3526242.3526254},
  doi		= {10.1145/3526242.3526254},
  abstract	= {This paper investigates the application of Natural
		  Language Processing as a means to study the relationship
		  between topography and its visual renderings in early
		  modern Japanese ukiyo-e landscape prints. We introduce a
		  new dataset with titles of landscape prints that have been
		  annotated by an art historian for any included place-names.
		  The prints are hosted by the digital database of the Art
		  Research Center at the Ritsumeikan University, Kyoto, one
		  of the hubs of Digital Humanities in Japan. By applying,
		  calibrating and assessing a Named Entity Recognition (NER)
		  tool, we argue that ‘distant viewing’ or macroanalysis
		  of visual datasets can be facilitated, which is needed to
		  assist art historical studies of this rich, complex and
		  diverse research material. Experimental results indicated
		  that the performance of NER can be improved by 30\% and
		  reach 50\% precision, by using part of the introduced
		  dataset.},
  booktitle	= {Digital Humanities Workshop},
  pages		= {70–77},
  numpages	= {8},
  keywords	= {natural language processing, named entity recognition, art
		  history, Ukiyo-e prints},
  location	= {Kyiv, Ukraine},
  series	= {DHW 2021}
}

@InProceedings{	  10.1145/3421766.3421812,
  author	= {Zhu, Dengyun and Guo, Qi and Zhang, Dongjiao and Wan,
		  Fucheng},
  title		= {Research on the Labelling Technology of Morphology and
		  Syntax},
  year		= {2020},
  isbn		= {9781450375535},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3421766.3421812},
  doi		= {10.1145/3421766.3421812},
  abstract	= {This paper aimed the integration tagging and tree-bank
		  transformation of morphology and syntax on the basis of
		  phrase and syntax tree-bank, tagged the nested named entity
		  in combination with the ontological linguistic clues.
		  Finally, it integrates the named entity to carry out the
		  integrative experimental analysis; according to the
		  experimental results, both the accuracy rate and recall
		  rate have been improved somewhat.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Artificial Intelligence and Advanced Manufacture},
  pages		= {181–184},
  numpages	= {4},
  keywords	= {tagging, named entity identification, Integration of
		  morphology and syntax},
  location	= {Manchester, United Kingdom},
  series	= {AIAM2020}
}

@Article{	  10.1145/3543508,
  author	= {Opdahl, Andreas L. and Al-Moslmi, Tareq and Dang-Nguyen,
		  Duc-Tien and Gallofr\'{e} Oca\~{n}a, Marc and Tessem,
		  Bj\o{}rnar and Veres, Csaba},
  title		= {Semantic Knowledge Graphs for the News: A Review},
  year		= {2022},
  issue_date	= {July 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {7},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3543508},
  doi		= {10.1145/3543508},
  abstract	= {ICT platforms for news production, distribution, and
		  consumption must exploit the ever-growing availability of
		  digital data. These data originate from different sources
		  and in different formats; they arrive at different
		  velocities and in different volumes. Semantic knowledge
		  graphs (KGs) is an established technique for integrating
		  such heterogeneous information. It is therefore
		  well-aligned with the needs of news producers and
		  distributors, and it is likely to become increasingly
		  important for the news industry. This article reviews the
		  research on using semantic knowledge graphs for production,
		  distribution, and consumption of news. The purpose is to
		  present an overview of the field; to investigate what it
		  means; and to suggest opportunities and needs for further
		  research and development.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {140},
  numpages	= {38},
  keywords	= {literature review, Semantic Web, Linked Open Data, Linked
		  Data, semantic technologies, ontology, knowledge graphs,
		  news consumption, news distribution, news production,
		  journalism, News}
}

@InProceedings{	  10.1145/3673277.3673306,
  author	= {Peng, Zhen and Du, Ye and Chen, Qifang and Zheng,
		  Tianshuai},
  title		= {Research on Knowledge Graph Construction for Smart Grid
		  Cybersecurity},
  year		= {2024},
  isbn		= {9798400716959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3673277.3673306},
  doi		= {10.1145/3673277.3673306},
  abstract	= {This paper proposes a construction method for smart grid
		  cybersecurity knowledge graph and solves the difficulty of
		  multilingual entity extraction with a small amount of
		  labeled data. First, the construction method of smart grid
		  cybersecurity knowledge graph is proposed with the
		  multi-source heterogeneous data in the field of electric
		  power cybersecurity collected by subject crawlers. Then,
		  for the problems of insufficient labeled data and language
		  mixing in the electric power cybersecurity domain, a
		  DA-XLMR-BiLSTM-FC-CRF model based on a five-layer
		  architecture is proposed to realize the entity extraction
		  of multilingual unstructured text. Finally, comparative and
		  ablation experiments are designed to prove the
		  effectiveness of the proposed model, and the F1 value of
		  the model reaches 94.04\% and the accuracy rate reaches
		  94.48\%.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Cryptography, Network Security and Communication
		  Technology},
  pages		= {164–170},
  numpages	= {7},
  location	= {Harbin, China},
  series	= {CNSCT '24}
}

@InProceedings{	  10.1145/3649921.3659847,
  author	= {Li, Cynthia and Osborn, Joseph},
  title		= {Translating Between Game Generators with Asterism and
		  Ceptre},
  year		= {2024},
  isbn		= {9798400709555},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3649921.3659847},
  doi		= {10.1145/3649921.3659847},
  abstract	= {In this paper, we present in-progress work that converts
		  games made with Ceptre, a genre-agnostic game description
		  language, into graphical games using the framework of
		  operational logics. Our preliminary code targets the
		  translation of tilemap-based dungeon crawlers, but we
		  present strategies for generalizing this process to other
		  Ceptre games and Asterism engines. We gesture at the
		  potential of operational logics and Asterism as a tool to
		  communicate across the many frameworks surrounding game
		  development and playing.},
  booktitle	= {Proceedings of the 19th International Conference on the
		  Foundations of Digital Games},
  articleno	= {70},
  numpages	= {4},
  keywords	= {formal models, game generators, operational logics},
  location	= {Worcester, MA, USA},
  series	= {FDG '24}
}

@InProceedings{	  10.1145/3722237.3722276,
  author	= {Ming, Jing},
  title		= {The personalized university English learning system in
		  computer-driven research},
  year		= {2025},
  isbn		= {9798400712692},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3722237.3722276},
  doi		= {10.1145/3722237.3722276},
  abstract	= {The aim of this study is to develop a computer-assisted
		  personalised university English learning system to improve
		  learning efficiency and effectiveness. Machine learning
		  algorithms were used to analyse student data and build a
		  personalised learning model. Experimental results show that
		  the system significantly outperforms traditional methods in
		  vocabulary acquisition (22\% improvement), listening
		  comprehension (18\% improvement) and oral fluency (15\%
		  improvement). The conclusion suggests that a personalised
		  learning system based on artificial intelligence and big
		  data can effectively improve university English teaching,
		  and provide a new direction for the future development of
		  educational technology. During the Eastern Han period, the
		  production of everyday pottery, pottery sculptures,
		  architectural bricks and tiles, a variety of ceramic
		  sculptures, living statues, horses, acrobatics and other
		  funerary objects, as well as new products such as large
		  lofts, chariots and horses, became the main features of the
		  industry.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Artificial Intelligence and Education},
  pages		= {222–225},
  numpages	= {4},
  keywords	= {Artificial Intelligence, Adaptive Learning, Data Mining,
		  Language Acquisition, Educational Technology},
  location	= { },
  series	= {ICAIE '24}
}

@InProceedings{	  10.1145/3587828.3587846,
  author	= {Tsiounis, Konstantinos and Kontogiannis, Kostas},
  title		= {Goal Driven Code Generation for Smart Contract
		  Assemblies},
  year		= {2023},
  isbn		= {9781450398589},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587828.3587846},
  doi		= {10.1145/3587828.3587846},
  abstract	= {We are currently witnessing the proliferation of
		  blockchain environments to support a wide spectrum of
		  corporate applications through the use of smart contracts.
		  It is of no surprise that smart contract programming
		  language technology constantly evolves to include not only
		  specialized languages such as Solidity, but also general
		  purpose languages such as GoLang and JavaScript.
		  Furthermore, blockchain technology imposes unique
		  challenges related to the monetary cost of deploying smart
		  contracts, and handling roll-back issues when a smart
		  contract fails. It is therefore evident that the complexity
		  of systems involving smart contracts will only increase
		  over time thus making the maintenance and evolution of such
		  systems a very challenging task. One solution to these
		  problems is to approach the implementation and deployment
		  of such systems in a disciplined and automated way. In this
		  paper, we propose a model-driven approach where the
		  structure and inter-dependencies of smart contract, as well
		  as stakeholder objectives, are denoted by extended goal
		  models which can then be transformed to yield Solidity code
		  that conforms with those models. More specifically, we
		  present first a Domain Specific Language (DSL) to denote
		  extended goal models and second, a transformation process
		  which allows for the Abstract Syntax Trees of such a DSL
		  program to be transformed into Solidity smart contact
		  source code. The transformation process ensures that the
		  generated smart contract skeleton code yields a system that
		  is conformant with the model, which serves as a
		  specification of said system so that subsequent analysis,
		  understanding, and maintenance will be easier to achieve.},
  booktitle	= {Proceedings of the 2023 12th International Conference on
		  Software and Computer Applications},
  pages		= {112–121},
  numpages	= {10},
  keywords	= {Smart contracts, Modular Design, Model-driven engineering,
		  Goal models, Compliance, Code generation},
  location	= {Kuantan, Malaysia},
  series	= {ICSCA '23}
}

@InProceedings{	  10.1145/3610978.3640715,
  author	= {Wilson, Jason and Yang, Yuqi},
  title		= {Software Architecture to Generate Assistive Behaviors for
		  Social Robots},
  year		= {2024},
  isbn		= {9798400703232},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3610978.3640715},
  doi		= {10.1145/3610978.3640715},
  abstract	= {To facilitate the design of socially assistive robots
		  (SARs), we present an architecture to generate assistive
		  behavior for social robots given a high-level description
		  of the intent of the assistance. Our approach features an
		  ontology of assistive intents, a hierarchical task network
		  planner, and robot middleware. We demonstrate the behaviors
		  on two robot platforms and compare the behaviors. While
		  many of the behaviors are similar, challenges remain in
		  generating behaviors that will be presented consistently
		  across multiple platforms.},
  booktitle	= {Companion of the 2024 ACM/IEEE International Conference on
		  Human-Robot Interaction},
  pages		= {1119–1123},
  numpages	= {5},
  keywords	= {HTN planning, behavior generation, ontology, socially
		  assistive robot, software architecture},
  location	= {Boulder, CO, USA},
  series	= {HRI '24}
}

@InProceedings{	  10.1145/3490099.3511130,
  author	= {Smith, Ronnie and Dragone, Mauro},
  title		= {A Dialogue-Based Interface for Active Learning of
		  Activities of Daily Living},
  year		= {2022},
  isbn		= {9781450391443},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3490099.3511130},
  doi		= {10.1145/3490099.3511130},
  abstract	= {While Human Activity Recognition (HAR) systems may benefit
		  from Active Learning (AL) by allowing users to
		  self-annotate their Activities of Daily Living (ADLs), many
		  proposed methods for collecting such annotations are for
		  short-term data collection campaigns for specific datasets.
		  We present a reusable dialogue-based approach to user
		  interaction for active learning in HAR systems, which
		  utilises a dataset of natural language descriptions of
		  common activities (which we make publicly available) and
		  semantic similarity measures. Our approach involves
		  system-initiated dialogue, including follow-up questions to
		  reduce ambiguity in user responses where appropriate. We
		  apply our work to an existing CASAS dataset in an active
		  learning scenario, to demonstrate our work in context, in
		  which a natural language interface provides knowledge that
		  can help interpret other multi-modal sensor data. We
		  provide results highlighting the potential of our dialogue-
		  and semantic similarity-based approach. We evaluate our
		  work: (i) technically, as an effective way to seek users’
		  input for active learning of ADLs; and (ii) qualitatively,
		  through a user study in which users were asked to use our
		  approach and an established method, and to subsequently
		  compare the two. Results show the potential of our approach
		  as a user-friendly mechanism for annotation of sensor data
		  as part of an active learning system.},
  booktitle	= {Proceedings of the 27th International Conference on
		  Intelligent User Interfaces},
  pages		= {820–831},
  numpages	= {12},
  keywords	= {Active Learning (AL), Human Activity Recognition (HAR)
		  labelling, Human-in-the-Loop (HITL) annotation, natural
		  language, semantic similarity},
  location	= {Helsinki, Finland},
  series	= {IUI '22}
}

@Article{	  10.1109/taslp.2023.3240661,
  author	= {Liu, Hong and Cai, Yucheng and Lin, Zhenru and Ou, Zhijian
		  and Huang, Yi and Feng, Junlan},
  title		= {Variational Latent-State GPT for Semi-Supervised
		  Task-Oriented Dialog Systems},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3240661},
  doi		= {10.1109/TASLP.2023.3240661},
  abstract	= {Recently, two approaches, fine-tuning large pre-trained
		  language models and variational training, have attracted
		  significant interests, separately, for semi-supervised
		  end-to-end task-oriented dialog (TOD) systems. In this
		  paper, we propose Variational Latent-State GPT model
		  (VLS-GPT), which is the first to combine the strengths of
		  the two approaches. Among many options of models, we
		  propose the generative model and the inference model for
		  variational learning of the end-to-end TOD system, both as
		  auto-regressive language models based on GPT-2, which can
		  be further trained over a mix of labeled and unlabeled
		  dialog data in a semi-supervised manner. Variational
		  training of VLS-GPT is both statistically and
		  computationally more challenging than previous variational
		  learning works for sequential latent variable models, which
		  use turn-level first-order Markovian. The inference model
		  in VLS-GPT is non-Markovian due to the use of the
		  Transformer architecture. In this work, we establish
		  Recursive Monte Carlo Approximation (RMCA) to the
		  variational objective with non-Markovian inference model
		  and prove its unbiasedness. Further, we develop the
		  computational strategy of sampling-then-forward-computation
		  to realize RMCA, which successfully overcomes the memory
		  explosion issue of using GPT in variational learning and
		  speeds up training. Semi-supervised TOD experiments are
		  conducted on two benchmark multi-domain datasets of
		  different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is
		  shown to significantly outperform both supervised-only and
		  semi-supervised self-training baselines.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jan,
  pages		= {970–984},
  numpages	= {15}
}

@InProceedings{	  10.1145/3600100.3623737,
  author	= {Ramanathan, Ganesh and Mayer, Simon},
  title		= {Reasoning about Physical Processes in Buildings through
		  Component Stereotypes},
  year		= {2023},
  isbn		= {9798400702303},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600100.3623737},
  doi		= {10.1145/3600100.3623737},
  abstract	= {Buildings employ an ensemble of technical systems like
		  those for heating and ventilation and each of them
		  orchestrate complex physical processes. Ontologies such as
		  Brick, IFC, SSN/SOSA, and SAREF have been created to
		  describe the technical systems in a machine-understandable
		  manner. However, such ontologies focus largely on
		  describing system topology, whereas several use cases, such
		  as automated fault detection and diagnostics (AFDD), also
		  need knowledge about the physical processes. Physical
		  processes can be described using mathematical simulation
		  models, but this is practically too expensive for building
		  automation systems and their integration with mainstream
		  technical systems ontologies is still under-explored. We
		  propose to address these challenges by introducing the
		  concept of component stereotypes that describe the effect
		  of component actuation on the state its underlying physical
		  mechanism. These stereotypes are then linked to actual
		  component instances in the technical system description,
		  thereby accomplishing an integration of structural
		  description with knowledge about physical processes. We
		  contribute an ontology for such stereotypes and evaluate it
		  with respect to the coverage of HVAC components in Brick
		  and its ability to automatically infer relationships
		  between components in a real-world building. We show how
		  the resulting knowledge graph can be queried by AFDD
		  applications to know about expected consequences of an
		  action, or conversely, identify components that may be
		  responsible for an observed state of the process. While we
		  are able to report a coverage of 100\% of Brick HVAC
		  components, the automatic inference underreports component
		  dependencies in real-world installations. This points at a
		  group of concepts which we propose should be considered in
		  future versions of the Brick ontology.},
  booktitle	= {Proceedings of the 10th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {120–129},
  numpages	= {10},
  location	= {Istanbul, Turkey},
  series	= {BuildSys '23}
}

@InProceedings{	  10.1145/3459930.3469533,
  author	= {Noh, Jiho and Kavuluru, Ramakanth},
  title		= {Joint learning for biomedical NER and entity
		  normalization: encoding schemes, counterfactual examples,
		  and zero-shot evaluation},
  year		= {2021},
  isbn		= {9781450384506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459930.3469533},
  doi		= {10.1145/3459930.3469533},
  abstract	= {Named entity recognition (NER) and normalization (EN) form
		  an indispensable first step to many biomedical natural
		  language processing applications. In biomedical information
		  science, recognizing entities (e.g., genes, diseases, or
		  drugs) and normalizing them to concepts in standard
		  terminologies or thesauri (e.g., Entrez, ICD-10, or RxNorm)
		  is crucial for identifying more informative relations among
		  them that drive disease etiology, progression, and
		  treatment. In this effort we pursue two high level
		  strategies to improve biomedical ER and EN. The first is to
		  decouple standard entity encoding tags (e.g., "B-Drug" for
		  the beginning of a drug) into type tags (e.g., "Drug") and
		  positional tags (e.g., "B"). A second strategy is to use
		  additional counterfactual training examples to handle the
		  issue of models learning spurious correlations between
		  surrounding context and normalized concepts in training
		  data. We conduct elaborate experiments using the
		  MedMentions dataset, the largest dataset of its kind for ER
		  and EN in biomedicine. We find that our first strategy
		  performs better in entity normalization when compared with
		  the standard coding scheme. The second data augmentation
		  strategy uniformly improves performance in span detection,
		  typing, and normalization. The gains from counterfactual
		  examples are more prominent when evaluating in zero-shot
		  settings, for concepts that have never been encountered
		  during training.},
  booktitle	= {Proceedings of the 12th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {55},
  numpages	= {10},
  keywords	= {biomedical natural language processing, deep neural
		  networks, entity normalization, information extraction,
		  named entity recognition},
  location	= {Gainesville, Florida},
  series	= {BCB '21}
}

@Article{	  10.5555/3586589.3586815,
  author	= {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan
		  and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and
		  Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob
		  and Hoffman, Matthew D. and Hormozdiari, Farhad and
		  Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and
		  Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and
		  McLean, Cory and Mincu, Diana and Mitani, Akinori and
		  Montanari, Andrea and Nado, Zachary and Natarajan, Vivek
		  and Nielson, Christopher and Osborne, Thomas F. and Raman,
		  Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff,
		  Jessica and Seneviratne, Martin and Sequeira, Shannon and
		  Suresh, Harini and Veitch, Victor and Vladymyrov, Max and
		  Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and
		  Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  title		= {Underspecification presents challenges for credibility in
		  modern machine learning},
  year		= {2022},
  issue_date	= {January 2022},
  publisher	= {JMLR.org},
  volume	= {23},
  number	= {1},
  issn		= {1532-4435},
  abstract	= {Machine learning (ML) systems often exhibit unexpectedly
		  poor behavior when they are deployed in real-world domains.
		  We identify underspecification in ML pipelines as a key
		  reason for these failures. An ML pipeline is the full
		  procedure followed to train and validate a predictor. Such
		  a pipeline is underspecified when it can return many
		  distinct predictors with equivalently strong test
		  performance. Underspecification is common in modern ML
		  pipelines that primarily validate predictors on held-out
		  data that follow the same distribution as the training
		  data. Predictors returned by underspecified pipelines are
		  often treated as equivalent based on their training domain
		  performance, but we show here that such predictors can
		  behave very differently in deployment domains. This
		  ambiguity can lead to instability and poor model behavior
		  in practice, and is a distinct failure mode from previously
		  identified issues arising from structural mismatch between
		  training and deployment domains. We provide evidence that
		  underspecfication has substantive implications for
		  practical ML pipelines, using examples from computer
		  vision, medical imaging, natural language processing,
		  clinical risk prediction based on electronic health
		  records, and medical genomics. Our results show the need to
		  explicitly account for underspecification in modeling
		  pipelines that are intended for real-world deployment in
		  any domain.},
  journal	= {J. Mach. Learn. Res.},
  month		= jan,
  articleno	= {226},
  numpages	= {61},
  keywords	= {genomics, electronic health records, medical imaging,
		  natural language processing, computer vision,
		  identifiability, fairness, spurious correlation,
		  distribution shift}
}

@InProceedings{	  10.1145/3706598.3713715,
  author	= {D\"{u}ck, Moritz and Holter, Steffen and Chan, Robin Shing
		  Moon and Sevastjanova, Rita and El-Assady, Mennatallah},
  title		= {Finding Needles in Document Haystacks: Augmenting
		  Serendipitous Claim Retrieval Workflows},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713715},
  doi		= {10.1145/3706598.3713715},
  abstract	= {Preliminary exploration of vast text corpora for
		  generating and validating hypotheses, typical in academic
		  inquiry, requires flexible navigation and rapid validation
		  of claims. Navigating the corpus by titles, summaries, and
		  abstracts might neglect information, whereas identifying
		  the relevant context-specific claims through in-depth
		  reading is unfeasible with rapidly increasing publication
		  numbers. Our paper identifies three typical user pathways
		  for hypothesis exploration and operationalizes
		  sentence-based retrieval combined with effective
		  contextualization and provenance tracking in a unified
		  workflow. We contribute an interface that augments the
		  previously laborious tasks of claim identification and
		  consistency checking using NLP techniques while balancing
		  user control and serendipity. Use cases, expert interviews,
		  and a user study with 10 participants demonstrate how the
		  proposed workflow enables users to traverse literature
		  corpora in novel and efficient ways. For the evaluation, we
		  instantiate the tool within two independent domains,
		  providing novel insights into the analysis of political
		  discourse and medical research.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {1003},
  numpages	= {17},
  keywords	= {human-AI interaction, natural language processing,
		  provenance, serendipity, text data},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3632314.3632332,
  author	= {Guo, Dongdong and Ma, Haitao and Zhao, Can and Peng, Hao
		  and Du, Wenbo and Jiang, Zongrui and Zhang, Yan},
  title		= {Construction and Application of the Knowledge Graph Method
		  in Maintenance of Robot in Automotive Manufacturing
		  Industry},
  year		= {2023},
  isbn		= {9798400709401},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632314.3632332},
  doi		= {10.1145/3632314.3632332},
  abstract	= {Based on the spare parts and structure data of industrial
		  robots, the entity list of robot parts is established to
		  form a query dictionary, and entity annotation is performed
		  on the robot corpus by means of dictionary query, which
		  reduces the cost of manual annotation and ensures the
		  quality of annotation data. In the process of entity
		  recognition training, Bert+Bilstm+CRF model structure is
		  used to initially use 70\% of the dictionary data for
		  annotation, and the model is trained by iteratively
		  increasing the annotation data in a continuous cycle, so
		  that the model can extract all the entities in the robot
		  corpus as much as possible. In addition, the material
		  number/model information and PM maintenance
		  content/strategy of the entity have been used as attributes
		  of the entity. Meanwhile, the experience summarized by the
		  failure model and effect analysis of industrial robots is
		  fully utilized to connect the phenomena, causes and
		  measures through the entities in order to build the
		  industrial robot knowledge graph relationships. The
		  constructed knowledge graph relationship is stored in a
		  Neo4j graphical database, making it convenient for content
		  retrieval and inquiry of application systems.In the
		  industrial robot knowledge graph application side, the
		  field maintenance personnel requirements are collected
		  through a questionnaire survey and the requirements are
		  classified into intent. A Bert+TextCNN structure model is
		  built to realize the intention recognition of user
		  inquiries. By combining entity recognition models and
		  intent classification models, the system is able to better
		  understand user inquiry needs, leading to the
		  implementation of an intelligent maintenance system for
		  industrial robots.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Intelligent Sensing and Industrial Automation},
  articleno	= {15},
  numpages	= {9},
  keywords	= {Knowledge Graph, Robot Maintenance},
  location	= {Virtual Event, China},
  series	= {ISIA '23}
}

@InProceedings{	  10.1145/3664476.3664523,
  author	= {Ruman, \'{A}d\'{a}m and Dra\v{s}ar, Martin and Sadlek,
		  Luk\'{a}\v{s} and Yang, Shanchieh Jay and Celeda, Pavel},
  title		= {Adversary Tactic Driven Scenario and Terrain Generation
		  with Partial Infrastructure Specification},
  year		= {2024},
  isbn		= {9798400717185},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664476.3664523},
  doi		= {10.1145/3664476.3664523},
  abstract	= {Diverse, accurate, and up-to-date training environments
		  are essential for training cybersecurity experts and
		  autonomous systems. However, preparation of their content
		  is time-consuming and requires experts to provide detailed
		  specifications. In this paper, we explore the challenges of
		  automated generation of the content (composed of scenarios
		  and terrains) for these environments. We propose new models
		  to represent the cybersecurity domain and associated action
		  spaces. These models are used to create sound and complex
		  training content based on partial specifications provided
		  by users. We compare the results with a real-world complex
		  malware campaign to assess the realism of the synthesized
		  content. To further evaluate the correctness and
		  variability of the results, we utilize the kill-chain
		  attack graph generation for the generated training content
		  to asses the internal correspondence of its key components.
		  Our results demonstrate that the proposed approach can
		  create complex training content similar to advanced attack
		  campaigns, which passes evaluation for soundness and
		  practicality. Our proposed approach and its implementation
		  significantly contribute to the state of the art, enabling
		  novel approaches to cybersecurity training and autonomous
		  system development.},
  booktitle	= {Proceedings of the 19th International Conference on
		  Availability, Reliability and Security},
  articleno	= {33},
  numpages	= {11},
  keywords	= {adversary framework, attack scenario generation, cyber
		  terrain generation, cybersecurity model},
  location	= {Vienna, Austria},
  series	= {ARES '24}
}

@Article{	  10.1145/3480238,
  author	= {Finkel, Raphael and Kaufman, Daniel and Shamim, Ahmed},
  title		= {Analyzing Code-mixing in Linguistic Corpora Using
		  Kratylos},
  year		= {2022},
  issue_date	= {February 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {1},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3480238},
  doi		= {10.1145/3480238},
  abstract	= {Code-switching, code-mixing, and, more generally,
		  multilingualism pose technological challenges for language
		  documentation, the sub-discipline of linguistics that deals
		  with the annotation and basic analysis of field recordings
		  and other primary data. We focus here on a case study
		  involving code-mixing in the endangered Koda language,
		  which poses special problems for morphosyntactic analysis.
		  We offer a robust approach to multilingual annotations that
		  involves a combination of the popular open source software
		  FieldWorks Language Explorer (FLEx) with Kratylos, a
		  web-based corpus tool for display and query. Kratylos
		  exposes linguistic data from various formats to powerful
		  regular-expression queries that can exploit tier structure
		  and other aspects of interlinear glossed text. We show how
		  Kratylos can target mixed structures in our FLEx database
		  of Koda that cannot be easily identified within the
		  original FLEx software itself.},
  journal	= {J. Comput. Cult. Herit.},
  month		= jan,
  articleno	= {3},
  numpages	= {15},
  keywords	= {lexicons, interlinear glossed texts, linguistics, Language
		  archives}
}

@InProceedings{	  10.1145/3715335.3735456,
  author	= {Bou Nassar, Jessica and Anwar, Misita and Bartram, Lyn and
		  Sharp, Darren and Goodwin, Sarah},
  title		= {‘Unsolvable within existing regimes’: Using a Systems
		  Thinking Approach to Co-design for Data Governance in
		  Cities},
  year		= {2025},
  isbn		= {9798400714849},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715335.3735456},
  doi		= {10.1145/3715335.3735456},
  abstract	= {Despite people’s significant role in generating data in
		  cities, their involvement in data governance (DG) remains
		  limited, failing to address the inherent complexity of DG
		  and undermining their ’right to the city’. We propose a
		  collaborative systems thinking approach as a scoping tool
		  for co-design, enabling researchers and designers to
		  involve people in co-creating an understanding of the
		  systemic structures underpinning DG in cities and
		  developing prototypes and solutions informed by these
		  structures. Using causal loop diagrams, we facilitated the
		  development of a conceptual model of DG. Participants,
		  representing diverse perspectives, created individual
		  causal loop diagrams that were merged into a collaborative
		  causal loop diagram (C-CLD). This C-CLD was employed in an
		  interactive workshop to identify intervention points and
		  develop targeted solutions. Our findings demonstrate how
		  C-CLDs can accommodate multiplicity, foster agonism, and
		  enable participants to challenge political dimensions and
		  existing systemic structures. Moreover, the engagement
		  process revealed the complexity of DG in the city, as
		  perceived by the collective of participants, resulting in
		  three key submodules that highlight tensions between
		  citizen sensitisation to data collection, the private
		  sector’s role in fulfilling citizens’ needs, and the
		  struggles faced by local governments. This work draws on
		  and extends HCI research that engages with systems thinking
		  ontologies, contributing to an HCI that includes the
		  political, moves beyond solutionism, and advances social
		  justice-oriented approaches.},
  booktitle	= {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on
		  Computing and Sustainable Societies},
  pages		= {48–67},
  numpages	= {20},
  keywords	= {Data governance, Systems thinking, Causal loop diagram,
		  Co-design, Cities},
  location	= { },
  series	= {COMPASS '25}
}

@InProceedings{	  10.1145/3701571.3701608,
  author	= {Zargham, Nima and Dubiel, Mateusz and Desai, Smit and
		  Mildner, Thomas and Belz, Hanz-Joachim},
  title		= {Designing AI Personalities: Enhancing Human-Agent
		  Interaction Through Thoughtful Persona Design},
  year		= {2024},
  isbn		= {9798400712838},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701571.3701608},
  doi		= {10.1145/3701571.3701608},
  abstract	= {In the rapidly evolving field of artificial intelligence
		  (AI) agents, designing the agent’s characteristics is
		  crucial for shaping user experience. This workshop aims to
		  establish a research community focused on AI agent persona
		  design for various contexts, such as in-car assistants,
		  educational tools, and smart home environments. We will
		  explore critical aspects of persona design, such as voice,
		  embodiment, and demographics, and their impact on user
		  satisfaction and engagement. Through discussions and
		  hands-on activities, we aim to propose practices and
		  standards that enhance the ecological validity of agent
		  personas. Topics include the design of conversational
		  interfaces, the influence of agent personas on user
		  experience, and approaches for creating contextually
		  appropriate AI agents. This workshop will provide a
		  platform for building a community dedicated to developing
		  AI agent personas that better fit diverse, everyday
		  interactions.},
  booktitle	= {Proceedings of the International Conference on Mobile and
		  Ubiquitous Multimedia},
  pages		= {490–494},
  numpages	= {5},
  keywords	= {conversational user interfaces, AI Agents, Personas,
		  Speech Interfaces, Conversational Agents},
  location	= { },
  series	= {MUM '24}
}

@Article{	  10.1145/3631483.3631499,
  author	= {Lisboa Malaquias, Felipe and Giantamidis, Georgios and
		  Basagiannis, Stylianos and Fulvio Rollini, Simone and
		  Amundson, Isaac},
  title		= {Towards a Methodology to Design Provably Secure
		  Cyber-physical Systems},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {1},
  issn		= {1094-3641},
  url		= {https://doi.org/10.1145/3631483.3631499},
  doi		= {10.1145/3631483.3631499},
  abstract	= {The inordinate financial cost of mitigating
		  post-production cybersecurity vulnerabilities in
		  cyber-physical systems (CPS) is forcing the industry to
		  rethink systems design cycles: greater attention is being
		  given to the design phase - with the goal of reducing the
		  attack surface of systems at an early stage (i.e., before
		  silicon tape out). Fortunately, formal methods have
		  advanced to the point that they can address such needs and
		  contribute towards achieving security certification.
		  However, new methods and tools focusing on industrial
		  scalability and usability for systems engineers are
		  required. In this ongoing research paper, we describe a
		  framework that will help systems engineers to: a) design
		  cyber-assured CPS using a Model Based Engineering (MBE)
		  approach; b) formally map security requirements to
		  different hardware and software blocks in the model; and c)
		  formally verify security requirements. Based on the nature
		  of each requirement, our framework collects formal
		  correctness evidence from different tools: while high-level
		  architectural properties are suitable for a contract- or
		  ontology-based reasoning, more complex properties with rich
		  semantics require the use of model checking or theorem
		  proving techniques.},
  journal	= {Ada Lett.},
  month		= oct,
  pages		= {94–99},
  numpages	= {6}
}

@InProceedings{	  10.1145/3589335.3651263,
  author	= {Jain, Monika},
  title		= {Knowledge Enabled Relation Extraction},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651263},
  doi		= {10.1145/3589335.3651263},
  abstract	= {Relation extraction is the task of extracting
		  relationships from input text, where input can be a
		  sentence, document, or multiple documents. This task has
		  been popular for decades and is still of keen interest.
		  Various techniques have been proposed to solve the relation
		  extraction problem, among which the most popular are using
		  distant supervision, deep learning-based models,
		  reasoning-based models, and transformer-based models. We
		  propose three approaches (named ReOnto, DocRE-CLip, and
		  KDocRE) for relation extraction from text at three levels
		  of granularity (sentence, document and across documents).
		  These approaches embed knowledge in a deep learning based
		  model to improve performance. ReOnto and DocRE-CLip have
		  been evaluated and the source code is publicly available.
		  We are currently implementing and evaluating KDocRE.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1210–1213},
  numpages	= {4},
  keywords	= {graph neural network, knowledge graph, neurosymbolic ai,
		  ontology, relation extraction},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3624062.3624099,
  author	= {Mcquaigue, Matthew and Saule, Erik and Subramanian,
		  Kalpathi and Payton, Jamie},
  title		= {Data-Driven Discovery of Anchor Points for PDC Content},
  year		= {2023},
  isbn		= {9798400707858},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3624062.3624099},
  doi		= {10.1145/3624062.3624099},
  abstract	= {The Parallel and Distributed Computing community has been
		  interested in integrating PDC content into early CS
		  curriculum to prime the students for more advanced
		  materials and build a workforce able to leverage advanced
		  computing infrastructure. To deploy this strategy at scale,
		  it is important to identify anchor points in early CS
		  courses where we can insert PDC content. We present an
		  analysis of CS courses that primarily focuses on CS1 and
		  Data Structure courses. We collected data on course content
		  through in-person workshops, where instructors of courses
		  classified their course materials against standard
		  curriculum guidelines. By using these classification, we
		  make sense of how Computer Science is being taught. We
		  highlight different types of CS1 and Data Structure
		  courses. And we provide reflection on how that knowledge
		  can be used by PDC experts to identify anchoring points for
		  PDC content, while being sensitive to the needs of
		  instructors.},
  booktitle	= {Proceedings of the SC '23 Workshops of the International
		  Conference on High Performance Computing, Network, Storage,
		  and Analysis},
  pages		= {335–342},
  numpages	= {8},
  keywords	= {CS Education, Course Model, Curriculum Guidelines,
		  Integrating PDC in Early CS},
  location	= {Denver, CO, USA},
  series	= {SC-W '23}
}

@InProceedings{	  10.1145/3659677.3659827,
  author	= {Bystrov, Dmitriy},
  title		= {Information Retrieval Multi-Agent System Established on
		  the Metaphysics Lexical Database},
  year		= {2024},
  isbn		= {9798400709296},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3659677.3659827},
  doi		= {10.1145/3659677.3659827},
  abstract	= {The system, retrieving information from heterogeneous
		  sources is discussed. Architecture of the system is based
		  on multi-agent approach. The user's queries could be
		  presented in the native language. Retrieving process based
		  on using knowledge, representing via ontology scheme. For
		  this purpose the wordnet ontology is used. The development
		  process of the ontology is discussed. The presented system
		  works on a distributed environment, where component agents
		  collaborate via XML web services and SOAP protocols.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Networking, Intelligent Systems and Security},
  articleno	= {53},
  numpages	= {3},
  location	= {Meknes, AA, Morocco},
  series	= {NISS '24}
}

@InProceedings{	  10.1145/3603781.3603871,
  author	= {Huang, Pengcheng and Li, Li and Wu, Chunyan and Zhang,
		  Xiaoqian and Liu, Zhigui},
  title		= {A Study of Sentence-BERT Based Essay Off-topic Detection},
  year		= {2023},
  isbn		= {9798400700705},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603781.3603871},
  doi		= {10.1145/3603781.3603871},
  abstract	= {Automated essay scoring systems are widely used in
		  education, and essay off-topic detection is an integral
		  part of this. Traditionally off-topic essay detection is
		  based on text features represented as spatial vectors,
		  however, this approach only addresses the structure of
		  essay statements and requires the use of manual features.
		  This paper proposed to use the Sentence-BERT model to
		  detect off-topic essays, the method first obtains a large
		  amount of high-quality data to build a corpus of off-topic
		  essays, and two Siamese twin pre-trained models are used to
		  embed sentences in the essay topic, and the body of the
		  essay, generate semantically rich sentence vectors and then
		  use cosine similarity to calculate the similarity between
		  the topic and the body of the essay after averaging the
		  pooled sentence vectors, and select the optimal threshold
		  to determine off-topic essays through continuous training.
		  The experimental results show that the proposed method
		  improves the accuracy, recall, and F1 values by 9.5\%,
		  11.2\%, and 10.4\% respectively over the C-BGRU
		  (Convolutional-Bidirectional Gate Recurrent Unit) based
		  Siamese twin network and also has an excellent performance
		  in topics with different degrees of divergence.},
  booktitle	= {Proceedings of the 2023 4th International Conference on
		  Computing, Networks and Internet of Things},
  pages		= {515–519},
  numpages	= {5},
  keywords	= {Siamese network, Pre-training models, Off-topic essay
		  detection, Cosine similarity},
  location	= {Xiamen, China},
  series	= {CNIOT '23}
}

@Proceedings{	  10.1145/3689944,
  title		= {SCORED '24: Proceedings of the 2024 Workshop on Software
		  Supply Chain Offensive Research and Ecosystem Defenses},
  year		= {2024},
  isbn		= {9798400712401},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to ACM SCORED '24,
		  the third edition of the ACM Workshop on Software Supply
		  Chain Offensive Research and Ecosystem Defenses. This
		  edition is held in Salt Lake City, Utah, United States with
		  extensive support for in-person and virtual attendance.
		  This year's program includes exciting work along many
		  different dimensions of research on supply chain security:
		  the development of security policies for software supply
		  chains, the use of artificial intelligence and large
		  language models, approaches on software bills of materials,
		  and the proposals of risk mitigation techniques. Consistent
		  with its focus, SCORED brings researchers, legislators and
		  practitioners in both open- and closed-source ecosystems to
		  the center of current and emerging challenges and
		  opportunities in software supply chain security.},
  location	= {Salt Lake City, UT, USA}
}

@Article{	  10.1145/3642979.3642995,
  author	= {B\'{e}n\'{e}dict, Gabriel and Zhang, Ruqing and Metzler,
		  Donald and Yates, Andrew and Deffayet, Romain and Hager,
		  Philipp and Jullien, Sami},
  title		= {Report on the 1st Workshop on Generative Information
		  Retrieval (Gen-IR 2023) at SIGIR 2023},
  year		= {2024},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {2},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3642979.3642995},
  doi		= {10.1145/3642979.3642995},
  abstract	= {The first edition of the workshop on Generative
		  Information Retrieval (Gen-IR 2023) took place in July 2023
		  in a hybrid fashion, co-located with the ACM SIGIR
		  Conference 2023 in Taipei (SIGIR 2023). The aim was to
		  bring information retrieval researchers together around the
		  topic of generative AI that gathered attention in 2022 and
		  2023 with large language models and diffusion models. Given
		  the novelty of the topic, the workshop was focused around
		  multi-sided discussions, namely panels and poster sessions
		  of the accepted proceedings papers. Two main research
		  outcomes are the proceedings of the workshop1 and the
		  potential research directions discussed in this
		  report.Date: 27 July 2023.Website:
		  https://coda.io/@sigir/gen-ir.},
  journal	= {SIGIR Forum},
  month		= jan,
  articleno	= {13},
  numpages	= {23}
}

@InProceedings{	  10.1145/3724363.3729049,
  author	= {McDermott, Roger and Daniels, Mats and Brown, John N.A.
		  and Cajander, \r{A}sa},
  title		= {Determining the Scope of the Philosophy of Computing
		  Education},
  year		= {2025},
  isbn		= {9798400715679},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3724363.3729049},
  doi		= {10.1145/3724363.3729049},
  abstract	= {There are a number of different approaches to the
		  investigation of teaching and learning within the subject
		  of Computing Education. Many of the advances in pedagogy
		  that have taken place over the past thirty years have been
		  due to careful statistical analysis of empirical data,
		  enhancing the reputation of the subject within the broader
		  Computing discipline. Empirical, qualitative methodologies,
		  of the kinds used extensively in the Social Sciences, have
		  also appeared in the Computing Education literature, often
		  investigating the socio-cultural aspects of the subject.
		  More recently, there has been a proposal to develop a role
		  for philosophical inquiry in Computing Education, which
		  mirrors similar historical developments in Engineering
		  Education. Rather than focus on the quantitative or
		  qualitative analysis of the student experience,
		  philosophical investigation instead relies on the use of
		  conceptual analysis to investigate the detailed semantic
		  content of ideas raised in the practice of computing
		  education, careful analysis of the methodologies used to do
		  such work, and a critique of the assumptions that underlie
		  the subject.In this paper, we investigate ways in which an
		  understanding of the Philosophy of Computing Education can
		  assist research within the subject. We consider how it
		  emerges from basic questions about nature of the subject,
		  its scope, and how it can be applied fruitfully within the
		  discipline.},
  booktitle	= {Proceedings of the 30th ACM Conference on Innovation and
		  Technology in Computer Science Education V. 1},
  pages		= {403–409},
  numpages	= {7},
  keywords	= {axiology, conceptual analysis, epistemology, methodology,
		  ontology, philosophy of computing education},
  location	= {Nijmegen, Netherlands},
  series	= {ITiCSE 2025}
}

@Proceedings{	  10.1145/3622758,
  title		= {Onward! 2023: Proceedings of the 2023 ACM SIGPLAN
		  International Symposium on New Ideas, New Paradigms, and
		  Reflections on Programming and Software},
  year		= {2023},
  isbn		= {9798400703881},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 2023 ACM SIGPLAN International Symposium on
		  New Ideas, New Paradigms, and Reflections on Programming
		  and Software (Onward! 2023), the premier multidisciplinary
		  conference focused on everything to do with programming and
		  software, including processes, methods, languages,
		  communities and applications. Onward! is more radical, more
		  visionary, and more open than other conferences to ideas
		  that are well-argued but not yet fully proven. We welcome
		  different ways of thinking about, approaching, and
		  reporting on programming language and software engineering
		  research. Onward! 2023 is co-located with SPLASH 2023,
		  running from Sunday 22nd of October till Friday 27th of
		  October, in Cascais, Portugal. We are delighted to have
		  Felienne Hermans giving the Onward! keynote, on Wednesday
		  25th of October, on "Creating a learnable and inclusive
		  programming language". All papers and essays that lie here
		  before you received at least three reviews, leading to a
		  decision of accept, reject, or conditional accept. Authors
		  of conditionally accepted papers were provided with
		  explicit requirements for acceptance, and were carefully
		  re-reviewed in the second phase. The essays track received
		  six submissions, out of which four were accepted. The
		  papers track accepted nine out of nineteen submissions. We
		  hope that the papers and essays in these proceedings will
		  stimulate and challenge your thinking about programming and
		  software engineering, and we are looking forward to many
		  discussions at the conference.},
  location	= {Cascais, Portugal}
}

@InProceedings{	  10.1145/3685651.3686699,
  author	= {Nizamis, Alexandros and Ioannidis, Dimosthenis and Gkonis,
		  Panagiotis and Trakadas, Panagiotis},
  title		= {Introducing an Enhanced Metadata Broker for Manufacturing
		  Data Spaces},
  year		= {2024},
  isbn		= {9798400709845},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3685651.3686699},
  doi		= {10.1145/3685651.3686699},
  abstract	= {Nowadays, collaborative ecosystems and value networks have
		  been established based on data sharing mechanisms and
		  principles coming from concepts like Data Spaces. This
		  data-centric approach has also increased the need for
		  effective metadata management that enables entities
		  participating in data sharing scenarios to find and trust
		  available data. In this paper, a metadata broker for
		  manufacturing related Data Spaces is introduced. It is
		  based on an ontology that has been implemented to describe
		  data related to Industries 4.0 and 5.0 implementations. The
		  proposed broker is based on Data Spaces principles and
		  artefacts that it extends by enabling semantic-based
		  modeling and search capabilities.},
  booktitle	= {Proceedings of the 4th Eclipse Security, AI, Architecture
		  and Modelling Conference on Data Space},
  pages		= {37–40},
  numpages	= {4},
  keywords	= {Data Spaces, Industry 4.0 / 5.0, Metadata Broker, Metadata
		  Registry},
  location	= {Mainz, Germany},
  series	= {eSAAM '24}
}

@Article{	  10.1145/3648360,
  author	= {Yu, Yang and Qiu, Dong and Wan, Huanyu},
  title		= {Sentiment Analysis Method of Epidemic-related Microblog
		  Based on Hesitation Theory},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3648360},
  doi		= {10.1145/3648360},
  abstract	= {The COVID-19 pandemic in 2020 brought an unprecedented
		  global crisis. After two years of control efforts, life
		  gradually returned to the pre-pandemic state, but localized
		  outbreaks continued to occur. Toward the end of 2022,
		  COVID-19 resurged in China, leading to another disruption
		  of people’s lives and work. Many pieces of information on
		  social media reflected people’s views and emotions toward
		  the second outbreak, which showed distinct differences
		  compared to the first outbreak in 2020. To explore
		  people’s emotional attitudes toward the pandemic at
		  different stages and the underlying reasons, this study
		  collected microblog data from November 2022 to January 2023
		  and from January to June 2020, encompassing Chinese
		  reactions to the COVID-19 pandemic. Based on hesitancy and
		  the Fuzzy Intuition theory, we proposed a hypothesis:
		  hesitancy can be integrated into machine learning models to
		  select suitable corpora for training, which not only
		  improves accuracy but also enhances model efficiency. Based
		  on this hypothesis, we designed a hesitancy-integrated
		  model. The experimental results demonstrated the model’s
		  positive performance on a self-constructed database. By
		  applying this model to analyze people’s attitudes toward
		  the pandemic, we obtained their sentiments in different
		  months. We found that the most negative emotions appeared
		  at the beginning of the pandemic, followed by emotional
		  fluctuations influenced by social events, ultimately
		  showing an overall positive trend. Combining word cloud
		  techniques and the Latent Dirichlet Allocation (LDA) model
		  effectively helped explore the reasons behind the changes
		  in pandemic attitude.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {50},
  numpages	= {25},
  keywords	= {COVID-19, sentiment analysis, hesitancy, fuzzy intuition
		  theory, machine learning}
}

@InProceedings{	  10.1145/3726122.3726123,
  author	= {Zufarova, Nozima and Kasimova, Zilola and Aripkhodjaev,
		  Saidamir},
  title		= {Branding Strategies for Education Using NLP and Knowledge
		  Based Systems},
  year		= {2025},
  isbn		= {9798400711701},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3726122.3726123},
  doi		= {10.1145/3726122.3726123},
  abstract	= {Knowledge-based systems can provide educational
		  institutions with tailored branding strategies based on
		  their data-driven insights, so it is necessary to integrate
		  advanced computational methods. People are accustomed to
		  using informal and dynamic language on the internet to
		  express their preferences and expectations. In order to
		  obtain actionable insights from large-scale textual data,
		  it is necessary to employ natural language processing
		  techniques to extract meaningful patterns and trends.
		  Therefore, the study introduced TF-IDF analysis and Topic
		  Modeling for textual data analysis. At the same time, the
		  study introduced TF-IDF analysis and Topic Modeling for
		  identifying key branding themes and introduced a
		  knowledge-based system combining rule-based decision-making
		  models and ontological frameworks based on branding domain
		  knowledge, thereby constructing a comprehensive branding
		  framework. The innovation of the research lies in the
		  synergistic combination of traditional frequency-based
		  analysis and coherence-driven topic modeling feature weight
		  calculation methods with knowledge-based systems to
		  formulate adaptive branding strategies, thereby improving
		  the efficiency and precision of branding methodologies. The
		  outcomes indicated that the accuracy of the classification
		  model combining Topic Modeling and TF-IDF based on
		  educational branding datasets was 94\%, the coherence score
		  was as high as 0.724, and the term frequency recall was
		  89\%. Meanwhile, the classification error rate of the model
		  was only 6\%. In addition, the proposed branding system,
		  which adopted a knowledge-driven framework, had an average
		  of 2,500 daily visits per person, with an effective
		  browsing time of 12 minutes per person, and a daily
		  browsing page count of 15 pages per person. This indicates
		  that the knowledge-based branding framework has strong
		  practical application effects and provides reliable
		  technical support for current research in the field of
		  educational branding.},
  booktitle	= {Proceedings of the 8th International Conference on Future
		  Networks \&amp; Distributed Systems},
  pages		= {1–7},
  numpages	= {7},
  location	= { },
  series	= {ICFNDS '24}
}

@InProceedings{	  10.5555/3721488.3721611,
  author	= {Ferrini, Lorenzo and Lemaignan, S\'{e}verin},
  title		= {VDB-based Spatially Grounded Semantics for Interactive
		  Robots},
  year		= {2025},
  publisher	= {IEEE Press},
  abstract	= {This paper presents a new approach for representing
		  spatially-grounded semantics in interactive robots. The
		  method combines spatial and symbolic data to improve robot
		  interactions in human-occupied environments. A key feature
		  is a voxel-based data structure optimized for dynamic and
		  sparse information, along with a global lookup table to
		  manage and track spatially-grounded entities and their
		  relationships. The implementation, which is integrated into
		  a ROS 2-based framework, allows for seamless querying
		  through semantic web APIs such as SPARQL. Initial tests
		  demonstrate the efficiency of this system in supporting
		  advanced scenarios in human-robot interaction. All the
		  repositories developed as part of this contribution can be
		  found at github.com/RepresentationMaps.},
  booktitle	= {Proceedings of the 2025 ACM/IEEE International Conference
		  on Human-Robot Interaction},
  pages		= {1005–1009},
  numpages	= {5},
  keywords	= {human-robot interaction, interactive robots, knowledge
		  representation, semantic mapping},
  location	= {Melbourne, Australia},
  series	= {HRI '25}
}

@InProceedings{	  10.1145/3460210.3493564,
  author	= {Alghamdi, Ghadah and Schmidt, Renate A. and Del-Pinto,
		  Warren and Gao, Yongsheng},
  title		= {Upwardly Abstracted Definition-Based Subontologies},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493564},
  doi		= {10.1145/3460210.3493564},
  abstract	= {In this paper, we present a method for extracting
		  subontologies from $mathcalELH $ ontologies for a set of
		  symbols. The approach is focused on the generation of
		  upwardly abstracted definitions of concepts, which is a
		  technique for computing definitions expressed using closest
		  primitive ancestors. The subontologies returned by the
		  method are evaluated for quality and compared to extracts
		  computed with locality-based modularisation and uniform
		  interpolation. Our subontology generation method produces
		  promising results in terms of size and relevance to the
		  needs of domain experts.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {209–216},
  numpages	= {8},
  keywords	= {subontologies, snomed ct, ontology summarisation, ontology
		  modularisation, ontology engineering},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@Proceedings{	  10.1145/3737609,
  title		= {AAR Adjunct '25: Adjunct Proceedings of the Sixth
		  Decennial Aarhus Conference: Computing X Crisis},
  year		= {2025},
  isbn		= {9798400719684},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3374587.3374637,
  author	= {Bogacheva, Evgenia and Puchkovskaia, Antonina and
		  Smetannikov, Ivan},
  title		= {Named Entity Recognition for Russian Historical Texts},
  year		= {2020},
  isbn		= {9781450376273},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3374587.3374637},
  doi		= {10.1145/3374587.3374637},
  abstract	= {With the raise of big data, machine learning and
		  crowdsourcing, the volume of existing datasets for
		  different machine learning problems have greatly increased.
		  The natural language processing field is not an exception;
		  so, as a result, most of the researches have transitioned
		  into investigating and applying different deep
		  architectures for it. One of the main issues of this trend
		  is as follows: it is hard to adopt such approaches for
		  somewhat poorly studied languages, which do not have
		  training data enough as for natural language processing
		  perspective. In this paper, we investigate some modern
		  approaches to named entity recognition as for Russian
		  language and show that for historical texts their results
		  are much lower than for general ones. In addition, we
		  propose our own algorithm that improves the results of for
		  these historical texts.},
  booktitle	= {Proceedings of the 2019 3rd International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {13–17},
  numpages	= {5},
  keywords	= {named entity recognition, historical texts, Russian
		  language, Natural language processing},
  location	= {Normal, IL, USA},
  series	= {CSAI '19}
}

@InProceedings{	  10.1145/3727505.3727544,
  author	= {Yang, Yan and Yao, Wenxu and Zhu, Yuangeng and Gao,
		  Xiaoxin and Zheng, Yanrong and Liu, Yanan},
  title		= {Dynamic Construction and Application of Electric Power
		  Thesaurus Based on Semantic Analysis},
  year		= {2025},
  isbn		= {9798400713620},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3727505.3727544},
  doi		= {10.1145/3727505.3727544},
  abstract	= {In order to improve the efficiency of knowledge management
		  in the electric power industry, a lexicon of electric power
		  topics based on semantic parsing is established in response
		  to the dynamic changes of terminology and domain
		  characteristics. It also integrates the semantic network
		  with the ontology, constructs a multi-dimensional semantic
		  association model, and utilizes deep learning, natural
		  language processing, and other technologies to realize the
		  automatic identification and classification of the
		  vocabulary. Experiments prove that the method can
		  effectively improve the learning effect of the lexicon. In
		  particular, the coverage increases from 72\% to 92\% and
		  from 12 to 16 months. In addition, the number of correctly
		  recognized words has increased to 200 with a correction
		  accuracy of 95\%. This series of improvements not only
		  reflects the significant improvement of the new method in
		  terms of accuracy and applicability, but also enhances the
		  robustness of the new method in application-specific
		  domain-oriented semantic analysis of power systems.},
  booktitle	= {Proceedings of the 2025 International Conference on Big
		  Data, Communication Technology and Computer Applications},
  pages		= {226–232},
  numpages	= {7},
  keywords	= {electric power subject thesaurus, semantic analysis,
		  semantic relations},
  location	= { },
  series	= {BDCTA '25}
}

@Article{	  10.1109/taslp.2023.3313415,
  author	= {Wang, Ante and Song, Linfeng and Jin, Lifeng and Yao,
		  Junfeng and Mi, Haitao and Lin, Chen and Su, Jinsong and
		  Yu, Dong},
  title		= {D&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$^{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt;PSG:
		  Multi-Party Dialogue Discourse Parsing as Sequence
		  Generation},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3313415},
  doi		= {10.1109/TASLP.2023.3313415},
  abstract	= {Conversational discourse analysis aims to extract the
		  interactions between dialogue turns, which is crucial for
		  modeling complex multi-party dialogues. As the benchmarks
		  are still limited in size and human annotations are costly,
		  the current standard approaches apply pretrained language
		  models, but they still require randomly initialized
		  classifiers to make predictions. These classifiers usually
		  require massive data to work smoothly with the pretrained
		  encoder, causing severe data hunger issue. We propose two
		  convenient strategies to formulate this task as a sequence
		  generation problem, where classifier decisions are
		  carefully converted into sequence of tokens. We then adopt
		  a pretrained T5 [C. Raffel et al., 2020] model to solve
		  this task so that no parameters are randomly initialized.
		  We also leverage the descriptions of the discourse
		  relations to help model understand their meanings.
		  Experiments on two popular benchmarks show that our
		  approach outperforms previous state-of-the-art models by a
		  large margin, and it is also more robust in zero-shot and
		  few-shot settings.&lt;sup&gt;1&lt;/sup&gt;},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= sep,
  pages		= {4004–4013},
  numpages	= {10}
}

@InProceedings{	  10.1145/3183713.3193562,
  author	= {Basik, Fuat and H\"{a}ttasch, Benjamin and Ilkhechi, Amir
		  and Usta, Arif and Ramaswamy, Shekar and Utama, Prasetya
		  and Weir, Nathaniel and Binnig, Carsten and Cetintemel,
		  Ugur},
  title		= {DBPal: A Learned NL-Interface for Databases},
  year		= {2018},
  isbn		= {9781450347037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3183713.3193562},
  doi		= {10.1145/3183713.3193562},
  abstract	= {In this demo, we present DBPal, a novel data exploration
		  tool with a natural language interface. DBPal leverages
		  recent advances in deep models to make query understanding
		  more robust in the following ways: First, DBPal uses novel
		  machine translation models to translate natural language
		  statements to SQL, making the translation process more
		  robust to paraphrasing and linguistic variations. Second,
		  to support the users in phrasing questions without knowing
		  the database schema and the query features, DBPal provides
		  a learned auto-completion model that suggests to users
		  partial query extensions during query formulation and thus
		  helps to write complex queries.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Management of Data},
  pages		= {1765–1768},
  numpages	= {4},
  keywords	= {robust natural language interface, relational database,
		  nlidb, natural language to sql},
  location	= {Houston, TX, USA},
  series	= {SIGMOD '18}
}

@InProceedings{	  10.1145/3744367.3744383,
  author	= {Xie, Zhixian and Hong, Yina},
  title		= {AI + Art and Design Education: Research on Intelligent
		  Algorithm-Driven Educational Content Generation Mechanism},
  year		= {2025},
  isbn		= {9798400715068},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3744367.3744383},
  doi		= {10.1145/3744367.3744383},
  abstract	= {Targeting at building up a framework for producing
		  educational contents, this study provides a thorough
		  investigation of cutting-edge applications of artificial
		  intelligence technologies in art and design education with
		  the assistance of intelligent algorithms. Generative AI,
		  deep learning, natural language processing (NLP), computer
		  vision (CV) and other significant tools are employed in
		  this study to deal with varied issues in related areas such
		  as outdated content updates, insufficient personalized
		  teaching, and excessive teacher workload. In this study,
		  various technologies including domain knowledge graph
		  construction, AI-generated content optimization,
		  personalized learning path recommendation, and human-AI
		  collaborative creation mechanisms are all combined to form
		  a novel approach. Based on controlled experimental
		  analysis, this framework is proven to deliver great
		  improvements in teaching quality (p&lt;0.05), learning
		  efficiency by 32.7\%, and help build up the expression
		  abilities. Besides acting as the theoretical foundation for
		  the combined AI, art, and design education, the research
		  can also contribute to the intelligent transformation of
		  education, holding important value for advancing
		  educational paradigms toward greater precision and
		  personalization.},
  booktitle	= {Proceedings of the 2025 International Conference on
		  Artificial Intelligence and Educational Systems},
  pages		= {88–94},
  numpages	= {7},
  keywords	= {AI Collaborative Creativity, AI + Art and Design
		  Education, Adaptive Learning, Intelligent Algorithm-Driven
		  Content Generation},
  location	= { },
  series	= {ICAIES '25}
}

@Proceedings{	  10.1145/3616855,
  title		= {WSDM '24: Proceedings of the 17th ACM International
		  Conference on Web Search and Data Mining},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the 17th ACM
		  International Conference on Web Search and Data Mining -
		  WSDM 2024. WSDM is one of the premier conferences in the
		  fields of web search and data mining, with a dynamic and
		  growing community from academia and industry. After two
		  years of virtual conferences and in-person conferences in
		  Singapore, the 2024 edition is an in-person conference with
		  virtual elements. We hope you enjoy the conference at the
		  "Centro Internacional de Congresos de Yucatan (CIC)" in
		  Merida from March 4 to March 8, 2024.We are excited to kick
		  off the program with a dynamic mix of Tutorials and
		  Industry Day. Our seven tutorials will cover a broad range
		  of search and data mining topics. Industry Day will provide
		  valuable insights from leaders at major technology
		  companies. The core technical program continues WSDM's
		  tradition of a single-track format, featuring 109
		  thought-provoking papers from both academic and industry
		  experts. We're honored to have inspiring keynote speakers
		  each day: Nicolas Christin (CMU), Elizabeth Reid (Google),
		  and Saiph Savage (Civic A.I. Lab). Additionally, 17
		  interactive demonstrations will showcase the latest
		  prototypes and systems. The final day offers a stimulating
		  Doctoral Consortium and six engaging workshops on topics
		  including integrity in social networks, large language
		  model for society, psychology-informed information access
		  system, interactive and scalable information retrieval
		  system and machine learning on graphs. WSDM 2024 proudly
		  presents WSDM day on information retrieval and Web in the
		  region. WSDM Cup Day highlights finalists' presentations
		  addressing challenges in Conversational Multi-Doc QA. This
		  diverse and stimulating program promises to be an enriching
		  experience for all!.},
  location	= {Merida, Mexico}
}

@InProceedings{	  10.1145/3589335.3651238,
  author	= {Jiomekong, Azanzi and Auer, S\"{o}ren and Oelen, Allard},
  title		= {Linked Open Literature Review using the Neuro-symbolic
		  Open Research Knowledge Graph},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651238},
  doi		= {10.1145/3589335.3651238},
  abstract	= {The way scholarly knowledge and in particular literature
		  reviews are communicated today rather resembles static,
		  unstructured, pseudo-digitized articles, which are hardly
		  processable by machines and AI. This demo showcases a novel
		  way to create and publish scholarly literature reviews,
		  also called semantic reviews. The neuro-symbolic approach
		  consists of extracting key insights from scientific papers
		  leveraging neural models and organizing them using a
		  symbolic scholarly knowledge graph. The food information
		  engineering review case study will allow participants to
		  see how this approach is implemented using the Open
		  Research Knowledge Graph (ORKG). The real-time demo will
		  allow participants to play with the ORKG and create their
		  own living, semantic review.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1015–1018},
  numpages	= {4},
  keywords	= {fair principle, linked open data, literature review,
		  neuro-symbolic ai, scholarly knowledge graph},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3706598.3713317,
  author	= {Dwyer, Andrew C and Coles-Kemp, Lizzie and Heath, Claude P
		  R and Crivellaro, Clara},
  title		= {Friend or Foe? Navigating and Re-configuring “Snipers'
		  Alley“},
  year		= {2025},
  isbn		= {9798400713941},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706598.3713317},
  doi		= {10.1145/3706598.3713317},
  abstract	= {In a ‘digital by default’ society, essential services
		  must be accessed online. This opens users to digital
		  deception not only from criminal fraudsters but from a
		  range of actors in a marketised digital economy. Using
		  grounded empirical research from northern England, we show
		  how supposedly ‘trusted’ actors, such as governments,
		  (re)produce the insecurities and harms that they seek to
		  prevent. Enhanced by a weakening of social institutions
		  amid a drive for efficiency and scale, this has built a
		  constricted, unpredictable digital channel. We
		  conceptualise this as a “snipers’ alley”. Four key
		  snipers articulated by participants’ lived experiences
		  are examined: 1) Governments; 2) Business; 3) Criminal
		  Fraudsters; and 4) Friends and Family to explore how
		  snipers are differentially experienced and transfigure
		  through this constricted digital channel. We discuss
		  strategies to re-configure the alley, and how crafting and
		  adopting opportunity models can enable more equitable forms
		  of security for all.},
  booktitle	= {Proceedings of the 2025 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {210},
  numpages	= {15},
  keywords	= {Digital Access, Digital Economy, Security Models, Threat
		  Models, Dark Patterns},
  location	= { },
  series	= {CHI '25}
}

@InProceedings{	  10.1145/3706599.3719878,
  author	= {Vella, Kellie and Dobson, Madeleine and Brereton, Margot},
  title		= {"Hello, Mr Tree": Toying with Playful Conversational AI in
		  the Early Years},
  year		= {2025},
  isbn		= {9798400713958},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706599.3719878},
  doi		= {10.1145/3706599.3719878},
  abstract	= {The use of generative AI is increasingly integrated into
		  childhood education, primarily with text-to-image
		  generation and older age ranges. This late-breaking work
		  looks at the use of a prototype technology using voiced
		  conversational AI (CAI) to engage young children’s
		  interest in nature, through the role-play of a character:
		  the ‘Talking Tree’. Using research-through-design, we
		  conducted six interactive sessions with children aged 3 to
		  5 years. These drove the iterative development of the
		  device and provided insight into how CAI might be applied
		  within the context of early learning. We found that the
		  device operated within children’s performative social
		  interactions and within their imagination to prompt
		  recollections of nature and fantastic diversions. We
		  contribute insight into the use of conversational AI for
		  learning in the busy environments of early childhood
		  education centres and the use of CAI-performed fictional
		  characters to build children’s connection with nature.},
  booktitle	= {Proceedings of the Extended Abstracts of the CHI
		  Conference on Human Factors in Computing Systems},
  articleno	= {8},
  numpages	= {6},
  keywords	= {Child-computer interaction, design, Early childhood
		  education, Conversational agent, Voice recognition, Digital
		  play, Play-based learning, LLM, AI},
  location	= { },
  series	= {CHI EA '25}
}

@InProceedings{	  10.1145/3589132.3625615,
  author	= {Bilidas, Dimitris and Mantas, Anastasios and Yfantis,
		  Filippos and Stamoulis, George and Koubarakis, Manolis and
		  Kondylatos, Spyros and Prapas, Ioannis and Papoutsis,
		  Ioannis},
  title		= {Fire Risk Management using Data Cubes, Machine Learning
		  and OBDA systems},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589132.3625615},
  doi		= {10.1145/3589132.3625615},
  abstract	= {We present a fire risk management system which takes input
		  data from various sources (e.g., meteorological data,
		  satellite indicators for vegetation, historical burned
		  areas), produces a harmonized spatio-temporal data cube to
		  compute fire risk and enables semantic querying to assist
		  fire risk management. The distinguishing implementation
		  features of the system is the use of data cubes, machine
		  learning algorithms and, most importantly, geospatial
		  ontology-based data access technologies. The system has
		  been implemented in the European project DeepCube for the
		  geographic area of Greece and can be used operationally to
		  assist authorities to determine fire risk during the summer
		  fire season.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {34},
  numpages	= {4},
  keywords	= {fire risk, machine learning, ontology based data access,
		  data cubes},
  location	= {Hamburg, Germany},
  series	= {SIGSPATIAL '23}
}

@InProceedings{	  10.1145/3297001.3297032,
  author	= {Subramanian, Asha and RR, Pavan Kumar and Vikkurthi,
		  Manikanta and Buttigieg, Pier Luigi},
  title		= {Semantic Harmonisation of Numeric Data from Open
		  Government Data},
  year		= {2019},
  isbn		= {9781450362078},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3297001.3297032},
  doi		= {10.1145/3297001.3297032},
  abstract	= {Open tabular data published as part of the open government
		  initiatives typically contain a spatial dimension, a
		  temporal dimension and the actual numeric data capturing
		  information such as health indicators, pollution readings,
		  sanitation status etc. "Semantic Harmonisation" of numeric
		  data entails linking numeric data columns with
		  web-accessible semantic entities from an ontology - a
		  machine readable knowledge representation. These semantic
		  entities are embedded in a knowledge graph, allowing
		  integration of information from disparate sources under
		  common semantic definitions across spatial and temporal
		  dimensions. Multiple research efforts have contributed to
		  recovering semantics of numeric columns in tables, however
		  they are either restricted to a single domain or rely on
		  the existence of numeric data as linked data tuples in
		  known ontologies. We present a novel yet simple approach
		  using a supervised machine learning classifier (Random
		  Forests) and semantic web techniques to generate semantics
		  for numeric columns in tabular data. This approach has been
		  tested with encouraging results for over 100 tabular
		  datasets from data.gov.in (Indian Open Government Data
		  Portal) downloaded from multiple domains such as "Health
		  and Family Welfare", "Agriculture", "Environment" etc. We
		  also present a use case for this work, being implemented in
		  collaboration with the ministries of the Government of
		  Karnataka for knowledge aggregation and dissemination of
		  sustainable development data.},
  booktitle	= {Proceedings of the ACM India Joint International
		  Conference on Data Science and Management of Data},
  pages		= {238–244},
  numpages	= {7},
  keywords	= {Semantic Harmonisation, Ontologies, Government Data},
  location	= {Kolkata, India},
  series	= {CODS-COMAD '19}
}

@Article{	  10.1145/3625301,
  author	= {Cornut, Murielle and Raemy, Julien Antoine and Spiess,
		  Florian},
  title		= {Annotations as Knowledge Practices in Image Archives:
		  Application of Linked Open Usable Data and Machine
		  Learning},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3625301},
  doi		= {10.1145/3625301},
  abstract	= {We reflect on some of the preliminary findings of the
		  Participatory Knowledge Practices in Analogue and Digital
		  Image Archives (PIA) research project around annotations of
		  photographic archives from the Swiss Society for Folklore
		  Studies (SSFS) as knowledge practices, the underlying
		  technological decisions, and their impact. The aim is not
		  only to seek more information but to find new approaches of
		  understanding the way in which people’s memory relate to
		  the collective, public form of archival memory and
		  ultimately how users figure in and shape the digital
		  archive.We provide a proof-of-concept workflow based on
		  automatically generated annotations comprising 53,481
		  photos that were subjected to object detection using Faster
		  R-CNN Inception ResNet V2. Of the detected objects, 184,609
		  have a detection score greater than 0.5, 123,529 have a
		  score greater than 0.75, and 88,442 have a score greater
		  than 0.9. A threshold of 0.75 was set for the dissemination
		  of our annotations, compatible with the W3C Web Annotation
		  Data Model (WADM) and embedded in our IIIF Manifests.In the
		  near future, the workflow will be upgraded to allow for the
		  co-existence of various, and occasionally conflicting,
		  assertions made by both human and machine users. We believe
		  that Linked Open Usable Data (LOUD) standards should be
		  used to improve the sustainability of such an ecosystem and
		  to foster collaboration between actors in cultural
		  heritage.},
  journal	= {J. Comput. Cult. Herit.},
  month		= nov,
  articleno	= {80},
  numpages	= {19},
  keywords	= {web annotation data model, participatory knowledge
		  practices in analogue and digital image archives, object
		  detection, memory, machine learning, linked open usable
		  data, linked art, international image interoperability
		  framework, digital materiality, cultural heritage, cultural
		  anthropology, Citizen science}
}

@Article{	  10.1145/3564275,
  author	= {van der Linden, Sanne and Sevastjanova, Rita and Funk,
		  Mathias and El-Assady, Mennatallah},
  title		= {MediCoSpace: Visual Decision-Support for Doctor-Patient
		  Consultations using Medical Concept Spaces from EHRs},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3564275},
  doi		= {10.1145/3564275},
  abstract	= {Healthcare systems are under pressure from an aging
		  population, rising costs, and increasingly complex
		  conditions and treatments. Although data are determined to
		  play a bigger role in how doctors diagnose and prescribe
		  treatments, they struggle due to a lack of time and an
		  abundance of structured and unstructured information. To
		  address this challenge, we introduce MediCoSpace, a visual
		  decision-support tool for more efficient doctor-patient
		  consultations. The tool links patient reports to past and
		  present diagnoses, diseases, drugs, and treatments, both
		  for the current patient and other patients in comparable
		  situations. MediCoSpace uses textual medical data,
		  deep-learning supported text analysis and concept spaces to
		  facilitate a visual discovery process. The tool is
		  evaluated by five medical doctors. The results show that
		  MediCoSpace facilitates a promising, yet complex way to
		  discover unlikely relations and thus suggests a path toward
		  the development of interactive visual tools to provide
		  physicians with more holistic diagnoses and personalized,
		  dynamic treatments for patients.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= jan,
  articleno	= {15},
  numpages	= {20},
  keywords	= {electronic health records, interaction design, natural
		  language processing, Visual analytics}
}

@InProceedings{	  10.1145/3534678.3542634,
  author	= {Chua, Watson W.K. and Li, Lu and Goh, Alvina},
  title		= {Classifying Multimodal Data Using Transformers},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3542634},
  doi		= {10.1145/3534678.3542634},
  abstract	= {The increasing prevalence of multimodal data in our
		  society has led to the increased need for machines to make
		  sense of such data holistically. However, data scientists
		  and machine learning engineers aspiring to work on such
		  data face challenges fusing the knowledge from existing
		  tutorials which often deal with each mode separately.
		  Drawing on our experience in classifying multimodal
		  municipal issue feedback in the Singapore government, we
		  conduct a hands-on tutorial to help flatten the learning
		  curve for practitioners who want to apply machine learning
		  to multimodal data.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4780–4781},
  numpages	= {2},
  keywords	= {computer vision, deep learning, multimodal learning,
		  natural language processing, transformers, vision-language
		  representation},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3340531.3414073,
  author	= {Bowles, Juliana and Broccia, Giovanna and Nanni, Mirco},
  title		= {DataMod2020: 9th International Symposium "From Data to
		  Models and Back"},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3414073},
  doi		= {10.1145/3340531.3414073},
  abstract	= {DataMod 2020 aims to bring together practitioners and
		  researchers from academia, industry and research
		  institutions interested in the combined application of
		  computational modelling methods with data-driven techniques
		  from the areas of knowledge management, data mining and
		  machine learning. Modelling methodologies of interest
		  include automata, agents, Petri nets, process algebras and
		  rewriting systems. Application domains include social
		  systems, ecology, biology, medicine, smart cities,
		  governance, security, education, software engineering, and
		  any other field that deals with complex systems and large
		  amounts of data. Papers can present research results in any
		  of the themes of interest for the symposium as well as
		  application experiences, tools and promising preliminary
		  ideas. Papers dealing with synergistic approaches that
		  integrate modelling and knowledge management/discovery or
		  that exploit knowledge management/discovery to
		  develop/syntesise system models are especially welcome.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information \&amp; Knowledge Management},
  pages		= {3531–3532},
  numpages	= {2},
  keywords	= {text mining, processing mining, process calculi, machine
		  learning, formal methods, deep learning, big data
		  analytics},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@Article{	  10.1145/3728365,
  author	= {Long, Yuan and Rai, Arun},
  title		= {Decoding Digital Risk From Corporate Disclosure: A Neural
		  Network Approach},
  year		= {2025},
  issue_date	= {September 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3728365},
  doi		= {10.1145/3728365},
  abstract	= {Digital risk—or the likelihood of losses from key
		  digital activities (i.e., information system [IS] sourcing,
		  digital infrastructure, data management, IS applications,
		  IS use, and digital product offerings)—constitutes a key
		  consideration in firm valuation. Firms’ public
		  disclosures (e.g., 10-K reports, earnings conference calls)
		  are a key source of data to learn about digital risks.
		  Although text analytics approaches (e.g., word frequency,
		  topic modeling, and sentiment analysis) have been applied
		  to a firm's public disclosures to assess various types of
		  risk (e.g., political risk, tax risk, cybersecurity), they
		  do not consider the structural linguistic relations
		  embedded in the text that are potentially relevant in
		  measuring risk.We apply a neural network approach to
		  address this gap and extract linguistic relations from a
		  firm's 10-K disclosure (Section “Item 1A”). We develop
		  novel firm-level digital risk measures based on these
		  linguistic relations. Specifically, we measure firm-level
		  digital risk from three perspectives: (1) presence (whether
		  digital risk is mentioned or not), (2) intensity (text
		  coverage of digital risk relative to other issues), and (3)
		  diversity (the types of digital risk mentioned).We validate
		  our digital risk measures by demonstrating their
		  significant correlation with firm risk, proxied by stock
		  market volatility. Our research reveals that investors’
		  perceptions of digital risk diversity and digital risk
		  intensity differ between IT and non-IT companies. First,
		  across all firms, digital risk intensity is negatively
		  associated with firm risk, indicating that investors do not
		  incorporate intensity of digital risk when assessing firm
		  risk. Second, in non-IT firms, digital risk diversity is
		  positively associated with firm risk, suggesting that
		  managers in these firms may influence investor perceptions
		  through strategic disclosure of digital risk types.
		  Overall, our findings suggest that text-based digital risk
		  measurement is practically feasible, scalable, and
		  economically meaningful.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= may,
  articleno	= {22},
  numpages	= {44},
  keywords	= {Digital risk, Textual analysis, Linguistic structure, Deep
		  learning, Corporate disclosure}
}

@InProceedings{	  10.1145/3708319.3733657,
  author	= {Perez-Martinez, Roberto and Casas-Ortiz, Alberto and
		  Santos, Olga C.},
  title		= {Towards Cultural Preservation of Traditional Motion
		  Knowledge through Automated Annotations with MoRTELaban},
  year		= {2025},
  isbn		= {9798400713996},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708319.3733657},
  doi		= {10.1145/3708319.3733657},
  abstract	= {Movement disciplines like dance or martial arts are
		  carriers of cultural knowledge, identity, and tradition.
		  However, oral traditions and video recordings make the
		  preservation of this knowledge susceptible to being lost.
		  Expert movement notation, in turn, holds the potential for
		  precise capture and knowledge inheritance. However, motion
		  notation approaches are not widespread, the process is
		  often time-consuming, and the movements are hard to
		  visualize without expert knowledge. In this work, we use
		  Labanotation and Laban Movement Analysis (LMA), a notation
		  system and method originally developed for dance, as a
		  symbolic, interpretable framework for motion representation
		  and preservation. Our contribution resides in the expansion
		  of an existing annotation system, the LabanEditor, to
		  handle full-body motion and data from multiple sources, and
		  support the work of experts in annotating the movements.
		  Our development, called MoRTELaban, supports
		  motion-to-notation and inverse mapping from notation to
		  keyframes, enabling exchange between video, motion capture,
		  and Labanotation formats. This allows for the documentation
		  and reconstruction of traditional motion practices using
		  expert-readable scores and 3D skeletons.},
  booktitle	= {Adjunct Proceedings of the 33rd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {459–463},
  numpages	= {5},
  keywords	= {motion modeling, movement modeling, knowledge
		  representation, Labanotation, Laban Movement Analysis
		  (LMA), expert systems},
  location	= { },
  series	= {UMAP Adjunct '25}
}

@InProceedings{	  10.1145/3543507.3583457,
  author	= {Zhao, Mingjun and Wang, Mengzhen and Ma, Yinglong and Niu,
		  Di and Wu, Haijiang},
  title		= {CEIL: A General Classification-Enhanced Iterative Learning
		  Framework for Text Clustering},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583457},
  doi		= {10.1145/3543507.3583457},
  abstract	= {Text clustering, as one of the most fundamental challenges
		  in unsupervised learning, aims at grouping semantically
		  similar text segments without relying on human annotations.
		  With the rapid development of deep learning, deep
		  clustering has achieved significant advantages over
		  traditional clustering methods. Despite the effectiveness,
		  most existing deep text clustering methods rely heavily on
		  representations pre-trained in general domains, which may
		  not be the most suitable solution for clustering in
		  specific target domains. To address this issue, we propose
		  CEIL, a novel Classification-Enhanced Iterative Learning
		  framework for short text clustering, which aims at
		  generally promoting the clustering performance by
		  introducing a classification objective to iteratively
		  improve feature representations. In each iteration, we
		  first adopt a language model to retrieve the initial text
		  representations, from which the clustering results are
		  collected using our proposed Category Disentangled
		  Contrastive Clustering (CDCC) algorithm. After strict data
		  filtering and aggregation processes, samples with clean
		  category labels are retrieved, which serve as supervision
		  information to update the language model with the
		  classification objective via a prompt learning approach.
		  Finally, the updated language model with improved
		  representation ability is used to enhance clustering in the
		  next iteration. Extensive experiments demonstrate that the
		  CEIL framework significantly improves the clustering
		  performance over iterations, and is generally effective on
		  various clustering algorithms. Moreover, by incorporating
		  CEIL on CDCC, we achieve the state-of-the-art clustering
		  performance on a wide range of short text clustering
		  benchmarks outperforming other strong baseline methods.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1784–1792},
  numpages	= {9},
  keywords	= {Classification-enhanced Clustering, Iterative Framework,
		  Text Clustering},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3747227.3747237,
  author	= {Gao, Feifei and Zhang, Lin and Zhang, Bo and Wang, Wenfeng
		  and Liu, Wei and Zhang, Jingyi and Liu, Han and Qiu, Shi
		  and Huang, Kai and Zhang, Mingang},
  title		= {Research on construction technology and application of
		  knowledge graph in equipment fault Diagnosis domain},
  year		= {2025},
  isbn		= {9798400714382},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3747227.3747237},
  doi		= {10.1145/3747227.3747237},
  abstract	= {In recent years, the construction technology of general
		  knowledge graph (KG) has been developing continuously. The
		  medical industry, manufacturing industry, financial
		  industry and other industries have constructed domain KGs.
		  The research of KG in the equipment field is mainly focused
		  on general equipment knowledge, and the field of equipment
		  fault diagnosis also needs to build its own domain KG.
		  Combined with the definition of the general KG, the
		  definition and construction process of the equipment fault
		  diagnosis domain KG are expounded, the specific technical
		  methods of each link of the construction process are
		  summarized, and the application of the equipment fault
		  diagnosis domain KG is explained, and some positive
		  exploration is carried out for the subsequent construction
		  of the equipment fault diagnosis domain KG.},
  booktitle	= {Proceedings of the 2025 International Conference on
		  Machine Learning and Neural Networks},
  pages		= {61–69},
  numpages	= {9},
  keywords	= {Data acquisition, Domain knowledge graph, Equipment fault
		  diagnosis, Knowledge extraction, Knowledge processing,
		  Knowledge storage, Ontology building},
  location	= { },
  series	= {MLNN '25}
}

@InProceedings{	  10.1145/3571884.3604310,
  author	= {Mannekote, Amogh and Celepkolu, Mehmet and Wiggins, Joseph
		  B. and Boyer, Kristy Elizabeth},
  title		= {Exploring Usability Issues in Instruction-Based and
		  Schema-Based Authoring of Task-Oriented Dialogue Agents},
  year		= {2023},
  isbn		= {9798400700149},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571884.3604310},
  doi		= {10.1145/3571884.3604310},
  abstract	= {Platforms such as Google DialogFlow and Amazon Lex have
		  enabled easier development of conversational agents. The
		  standard approach to training these agents involve
		  collecting and annotating in-domain data in the form of
		  labelled utterances. However, obtaining in-domain data for
		  training machine learning models remains a bottleneck.
		  Schema-based dialogue, which involves laying out a
		  structured representation of the flow of a “typical”
		  dialogue, and prompt-based methods, which involve writing
		  instructions in natural language to large language models
		  such as GPT-3, are promising ways to tackle this problem.
		  However, usability issues when translating these methods
		  into practice are less explored. Our study takes a first
		  step towards addressing this gap by having 23 students who
		  had finished a graduate-level course on spoken dialogue
		  systems report their experiences as they defined structured
		  schemas and composed instruction-based prompts for two
		  task-oriented dialogue scenarios. Through inductive coding
		  and subsequent thematic analysis of the survey data, we
		  explored users’ authoring experiences with schema and
		  prompt-based methods. The findings provide insights for
		  future data collection and authoring tool design for
		  dialogue systems.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Conversational User Interfaces},
  articleno	= {41},
  numpages	= {6},
  keywords	= {dialogue systems, schema-based dialogue, user studies;,
		  zero-shot prompting},
  location	= {Eindhoven, Netherlands},
  series	= {CUI '23}
}

@InProceedings{	  10.1145/3641399.3641412,
  author	= {Balwani, Shivani and Tiwari, Saurabh and Dasgupta, Sourish
		  and Sharma, Akhilesh},
  title		= {An Approach for Providing Recommendation for Requirements
		  Non-Conformant with Requirement Templates (RTs)},
  year		= {2024},
  isbn		= {9798400717673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3641399.3641412},
  doi		= {10.1145/3641399.3641412},
  abstract	= {RTs generally possess a fixed syntactic structure and
		  comprise pre-defined slots, and requirements written in the
		  format of RTs must conform with the template structure.
		  Suppose the requirements do not conform to the RT. In that
		  case, manually verifying the conformity of requirements to
		  RTs becomes a tedious task due to the large size of
		  industry requirement documents and introduces the
		  possibility of errors. Furthermore, rewriting requirements
		  to conform to the template structure when they initially do
		  not conform presents a significant challenge. This paper
		  proposes a tool-based approach that automatically verifies
		  whether Functional Requirements (FRs) conform to RTs. It
		  recommends a Template Conformance (TC) requirement by
		  generating a semantically identical requirement that
		  Conforms to the template structure. Our study focused on
		  two well-known RTs, Easy Approach to Requirements Syntax
		  (EARS) and RUPPs, for checking conformance and making
		  recommendations. We utilized Natural Language Processing
		  (NLP) techniques and applied our approach to industrial and
		  publicly available case studies. Our results demonstrate
		  that the proposed tool-based approach facilitates
		  requirement analysis and aids in recommending requirements
		  based on their conformity with RTs. Our results show an
		  accuracy of 83.9\% for providing recommendations to
		  non-conformant requirements with RTs.},
  booktitle	= {Proceedings of the 17th Innovations in Software
		  Engineering Conference},
  articleno	= {9},
  numpages	= {11},
  keywords	= {Analysis, Natural Language Processing (NLP), Quality,
		  Recommendation, Requirement Templates (RTs)},
  location	= {Bangalore, India},
  series	= {ISEC '24}
}

@InProceedings{	  10.1145/3604915.3608759,
  author	= {Anelli, Vito Walter and Basile, Pierpaolo and De Melo,
		  Gerard and Donini, Francesco M and Ferrara, Antonio and
		  Musto, Cataldo and Narducci, Fedelucio and Ragone, Azzurra
		  and Zanker, Markus},
  title		= {Fifth Knowledge-aware and Conversational Recommender
		  Systems Workshop (KaRS)},
  year		= {2023},
  isbn		= {9798400702419},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604915.3608759},
  doi		= {10.1145/3604915.3608759},
  abstract	= {Recommender systems have become ubiquitous in daily life,
		  but their limitations in interacting with human users have
		  become evident. Deep learning approaches have led to the
		  development of data-driven algorithms that identify
		  connections between users and items, but they often miss a
		  critical actor in the loop - the end-user. Knowledge-based
		  approaches are gaining attention due to the availability of
		  knowledge-graphs, such as DBpedia and Wikidata, which
		  provide semantics-aware information on different knowledge
		  domains. These approaches are being used for recommendation
		  and challenges such as knowledge graph embeddings, hybrid
		  recommendation, and interpretable recommendation. Moreover,
		  the emergence of neural-symbolic systems, which combine
		  data-driven and symbolic methods, can significantly improve
		  recommendation systems. A growing number of research papers
		  on such topics demonstrate the growing interest and
		  research potential of these systems. Furthermore, content
		  features become crucial when interaction requires it. The
		  development of conversational recommender systems presents
		  new challenges, as they require multi-turn dialogues
		  between users and systems, blurring the line between
		  recommendation and retrieval. Evaluation of these systems
		  goes beyond simple accuracy metrics and is hampered by the
		  limited availability of datasets. While research and
		  development into conversational recommender systems has
		  been less prominent in the past, recent literature shows
		  growing interest and potential for these systems.},
  booktitle	= {Proceedings of the 17th ACM Conference on Recommender
		  Systems},
  pages		= {1259–1262},
  numpages	= {4},
  keywords	= {Conversational Agents, Knowledge Graphs, Knowledge
		  Representation, Natural Language Processing,
		  Neural-Symbolic Reasoning, Recommender systems, Semantic
		  Web},
  location	= {Singapore, Singapore},
  series	= {RecSys '23}
}

@Article{	  10.1162/coli_a_00354,
  author	= {Ustalov, Dmitry and Panchenko, Alexander and Biemann,
		  Chris and Ponzetto, Simone Paolo},
  title		= {Watset: Local-Global Graph Clustering with Applications in
		  Sense and Frame Induction},
  year		= {2019},
  issue_date	= {September 2019},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA},
  volume	= {45},
  number	= {3},
  issn		= {0891-2017},
  url		= {https://doi.org/10.1162/coli_a_00354},
  doi		= {10.1162/coli_a_00354},
  abstract	= {We present a detailed theoretical and computational
		  analysis of the Watset meta-algorithm for fuzzy graph
		  clustering, which has been found to be widely applicable in
		  a variety of domains. This algorithm creates an
		  intermediate representation of the input graph, which
		  reflects the “ambiguity” of its nodes. Then, it uses
		  hard clustering to discover clusters in this
		  “disambiguated” intermediate graph. After outlining the
		  approach and analyzing its computational complexity, we
		  demonstrate that Watset shows competitive results in three
		  applications: unsupervised synset induction from a synonymy
		  graph, unsupervised semantic frame induction from
		  dependency triples, and unsupervised semantic class
		  induction from a distributional thesaurus. Our algorithm is
		  generic and can also be applied to other networks of
		  linguistic data.},
  journal	= {Comput. Linguist.},
  month		= sep,
  pages		= {423–479},
  numpages	= {57}
}

@InProceedings{	  10.1145/3640543.3645208,
  author	= {Bendeck, Alexander and Bromley, Dennis and Setlur, Vidya},
  title		= {SlopeSeeker: A Search Tool for Exploring a Dataset of
		  Quantifiable Trends},
  year		= {2024},
  isbn		= {9798400705083},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640543.3645208},
  doi		= {10.1145/3640543.3645208},
  abstract	= {Natural language and search interfaces intuitively
		  facilitate data exploration and provide visualization
		  responses to diverse analytical queries based on the
		  underlying datasets. However, these interfaces often fail
		  to interpret more complex analytical intents, such as
		  discerning subtleties and quantifiable differences between
		  terms like “bump’’ and “spike’’ in the context
		  of COVID cases, for example. We address this gap by
		  extending the capabilities of a data exploration search
		  interface for interpreting semantic concepts in time series
		  trends. We first create a comprehensive dataset of semantic
		  concepts by mapping quantifiable univariate data trends
		  such as slope and angle to crowdsourced, semantically
		  meaningful trend labels. The dataset contains quantifiable
		  properties that capture the slope-scalar effect of semantic
		  modifiers like “sharply” and “gradually,” as well
		  as multi-line trends (e.g., “peak,” “valley”). We
		  demonstrate the utility of this dataset in SlopeSeeker, a
		  tool that supports natural language querying of
		  quantifiable trends, such as “show me stocks that tanked
		  in 2010.” The tool incorporates novel scoring and ranking
		  techniques based on semantic relevance and visual
		  prominence to present relevant trend chart responses
		  containing these semantic trend concepts. In addition,
		  SlopeSeeker provides a faceted search interface for users
		  to navigate a semantic hierarchy of concepts from general
		  trends (e.g., “increase’’) to more specific ones
		  (e.g., “sharp increase’’). A preliminary user
		  evaluation of the tool demonstrates that the search
		  interface supports greater expressivity of queries
		  containing concepts that describe data trends. We identify
		  potential future directions for leveraging our publicly
		  available quantitative semantics dataset in other data
		  domains and for novel visual analytics interfaces.},
  booktitle	= {Proceedings of the 29th International Conference on
		  Intelligent User Interfaces},
  pages		= {817–836},
  numpages	= {20},
  keywords	= {Semantics, quantifiable metadata, search, trends, visual
		  analysis.},
  location	= {Greenville, SC, USA},
  series	= {IUI '24}
}

@InProceedings{	  10.1145/3658644.3670318,
  author	= {Al Rahat, Tamjid and Feng, Yu and Tian, Yuan},
  title		= {AuthSaber: Automated Safety Verification of OpenID Connect
		  Programs},
  year		= {2024},
  isbn		= {9798400706363},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3658644.3670318},
  doi		= {10.1145/3658644.3670318},
  abstract	= {Single Sign-On (SSO)-based authentication protocols, like
		  OpenID Connect (OIDC), play a crucial role in enhancing
		  security and privacy in today's interconnected digital
		  world, gaining widespread adoption among the majority of
		  prominent authentication service providers. These protocols
		  establish a structured framework for verifying and
		  authenticating the identities of individuals,
		  organizations, and devices, while avoiding the necessity of
		  sharing sensitive credentials (e.g., passwords) with
		  external entities. However, the security guarantees of
		  these protocols rely on their proper implementation, and
		  real-world implementations can, and indeed often do,
		  contain logical programming errors leading to severe
		  attacks, including authentication bypass and user account
		  takeover. In response to this challenge, we present
		  AuthSaber, an automated verifier designed to assess the
		  real-world OIDC protocol implementations against their
		  standard safety specifications in a scalable manner.
		  AuthSaber addresses the challenges of expressiveness for
		  OIDC properties, modeling multi-party interactions, and
		  automation by first designing a novel specification
		  language based on linear temporal logic, leveraging an
		  automaton-based approach to constrain the space of possible
		  interactions between OIDC entities, and incorporating
		  several domain-specific transformations to obtain programs
		  and properties that can be directly reasoned about by
		  software model checkers. We evaluate AuthSaber on the 15
		  most popular and widely used OIDC libraries and discover 16
		  previously unknown vulnerabilities, all of which are
		  responsively disclosed to the developers. Five categories
		  of these vulnerabilities also led to new CVEs.},
  booktitle	= {Proceedings of the 2024 on ACM SIGSAC Conference on
		  Computer and Communications Security},
  pages		= {2949–2962},
  numpages	= {14},
  keywords	= {authentication, authorization, automated analysis, openid
		  connect security, safety verification, single sign-on},
  location	= {Salt Lake City, UT, USA},
  series	= {CCS '24}
}

@InProceedings{	  10.1145/3390557.3394128,
  author	= {Wang, Jiawei and Cui, Guorong and Zhu, Xiaoke and Liu,
		  Huijian and Liu, Junsong and Jia, Xuebin},
  title		= {GSR: A Resource Model and Semantics-based API
		  Recommendation Algorithm},
  year		= {2020},
  isbn		= {9781450376587},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3390557.3394128},
  doi		= {10.1145/3390557.3394128},
  abstract	= {With the rapid development of Web services, more and more
		  Web services are published on the Internet. A Mashup
		  application that aggregates multiple Web APIs is also
		  becoming more popular. But it also brings a problem that is
		  how to find a suitable API among a wide variety of APIs has
		  become a challenge. To this end, this paper proposes a web
		  service recommendation algorithm that combines graph
		  databases and semantics. In this algorithm, we propose to
		  use graph database to build a two-layer structure resource
		  model. First, we use LDA for topic classification and
		  classify Mashup and API of the same classification into the
		  same category respectively. This helps reduce the number of
		  searches for Mashup and API. When a user enters a
		  requirement document, Word2vec and WMD algorithms are used
		  to find similar Web API description text. Finally, we use
		  similarity and API history invokes to propose a ranking
		  algorithm to generate a recommendation list. Through
		  real-world data, this experiment has a better-recommended
		  performance.},
  booktitle	= {Proceedings of the 2020 the 4th International Conference
		  on Innovation in Artificial Intelligence},
  pages		= {184–188},
  numpages	= {5},
  keywords	= {Word Mover's Distance, Resource Model, LDA, API
		  recommendation},
  location	= {Xiamen, China},
  series	= {ICIAI '20}
}

@InProceedings{	  10.1145/3497775.3503685,
  author	= {Conrad, Esther and Titolo, Laura and Giannakopoulou,
		  Dimitra and Pressburger, Thomas and Dutle, Aaron},
  title		= {A compositional proof framework for FRETish requirements},
  year		= {2022},
  isbn		= {9781450391825},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3497775.3503685},
  doi		= {10.1145/3497775.3503685},
  abstract	= {Structured natural languages provide a trade space between
		  ambiguous natural languages that make up most written
		  requirements, and mathematical formal specifications such
		  as Linear Temporal Logic. FRETish is a structured natural
		  language for the elicitation of system requirements
		  developed at NASA. The related open-source tool Fret
		  provides support for translating FRETish requirements into
		  temporal logic formulas that can be input to several
		  verification and analysis tools. In the context of
		  safety-critical systems, it is crucial to ensure that a
		  generated formula captures the semantics of the
		  corresponding FRETish requirement precisely. This paper
		  presents a rigorous formalization of the FRETish language
		  including a new denotational semantics and a proof of
		  semantic equivalence between FRETish specifications and
		  their temporal logic counterparts computed by Fret. The
		  complete formalization and the proof have been developed in
		  the Prototype Verification System (PVS) theorem prover.},
  booktitle	= {Proceedings of the 11th ACM SIGPLAN International
		  Conference on Certified Programs and Proofs},
  pages		= {68–81},
  numpages	= {14},
  keywords	= {Structured Natural Language, Requirements, PVS, Metric
		  Temporal Logic, Formal Proofs},
  location	= {Philadelphia, PA, USA},
  series	= {CPP 2022}
}

@Article{	  10.1145/3485847,
  author	= {Costa, L\'{a}zaro and Freitas, Nuno and da Silva, Jo\~{a}o
		  Rocha},
  title		= {An Evaluation of Graph Databases and Object-Graph Mappers
		  in CIDOC CRM-Compliant Digital Archives},
  year		= {2022},
  issue_date	= {September 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3485847},
  doi		= {10.1145/3485847},
  abstract	= {The Portuguese General Directorate for Book, Archives and
		  Libraries (DGLAB) has selected CIDOC CRM as the basis for
		  its next-generation digital archive management software.
		  Given the ontological foundations of the Conceptual
		  Reference Model (CRM), a graph database or a triplestore
		  was seen as the best candidate to represent a CRM-based
		  data model for the new software. We thus decided to compare
		  several of these databases, based on their maturity,
		  features, performance in standard tasks and, most
		  importantly, the Object-Graph Mappers (OGM) available to
		  interact with each database in an object-oriented way. Our
		  conclusions are drawn not only from a systematic review of
		  related works but from an experimental scenario. For our
		  experiment, we designed a simple CRM-compliant graph
		  designed to test the ability of each OGM/database
		  combination to tackle the so-called “diamond-problem”
		  in Object-Oriented Programming (OOP) to ensure that
		  property instances follow domain and range
		  constraints.&nbsp;&nbsp;Our results show that (1)
		  ontological consistency enforcement in graph databases and
		  triplestores is much harder to achieve than in a relational
		  database, making them more suited to an analytical rather
		  than a transactional role; (2) OGMs are still rather
		  immature solutions; and (3) neomodel, an OGM for the Neo4j
		  graph database, is the most mature solution in the study as
		  it satisfies all requirements, although it is also the
		  least performing.},
  journal	= {J. Comput. Cult. Herit.},
  month		= sep,
  articleno	= {44},
  numpages	= {18},
  keywords	= {comparison, CIDOC CRM, digital archives, graph databases,
		  Object-graph mapping}
}

@InProceedings{	  10.1109/ase56229.2023.00150,
  author	= {Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai,
		  Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
  title		= {LiSum: Open Source Software License Summarization with
		  Multi-Task Learning},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00150},
  doi		= {10.1109/ASE56229.2023.00150},
  abstract	= {Open source software (OSS) licenses regulate the
		  conditions under which users can reuse, modify, and
		  distribute the software legally. However, there exist
		  various OSS licenses in the community, written in a formal
		  language, which are typically long and complicated to
		  understand. In this paper, we conducted a 661-participants
		  online survey to investigate the perspectives and practices
		  of developers towards OSS licenses. The user study revealed
		  an indeed need for an automated tool to facilitate license
		  understanding. Motivated by the user study and the fast
		  growth of licenses in the community, we propose the first
		  study towards automated license summarization.
		  Specifically, we released the first high quality text
		  summarization dataset and designed two tasks, i.e., license
		  text summarization (LTS), aiming at generating a relatively
		  short summary for an arbitrary license, and license term
		  classification (LTC), focusing on the attitude inference
		  towards a predefined set of key license terms (e.g.,
		  Distribute). Aiming at the two tasks, we present LiSum, a
		  multi-task learning method to help developers overcome the
		  obstacles of understanding OSS licenses. Comprehensive
		  experiments demonstrated that the proposed jointly training
		  objective boosted the performance on both tasks, surpassing
		  state-of-the-art baselines with gains of at least 5 points
		  w.r.t. F1 scores of four summarization metrics and
		  achieving 95.13\% micro average F1 score for classification
		  simultaneously. We released all the datasets, the
		  replication package, and the questionnaires for the
		  community.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {787–799},
  numpages	= {13},
  keywords	= {open source software licenses, multi-task learning,
		  license comprehension},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@InBook{	  10.1145/3729706.3729796,
  author	= {Wang, Yu and Wang, Liguang and Ma, Jun},
  title		= {Research on the Construction and Application of the
		  Knowledge Graph of Qin Dynasty Historical Celebrities'
		  Deeds by Spatio-Temporal Integration},
  year		= {2025},
  isbn		= {9798400712715},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3729706.3729796},
  abstract	= {[Objective] Construct a spatial and temporal integration
		  of the Qin Dynasty historical celebrities knowledge map, in
		  order to lay the foundation for information retrieval and
		  visualization when customers learn the Qin Dynasty
		  historical knowledge. [Methods] We use the “seven-step
		  method” and the Prot\'{e}g\'{e} tool to construct the
		  ontology model of the Qin Dynasty historical celebrities
		  with spatial and temporal correlation from the top down,
		  and then construct the spatial and temporal fusion
		  knowledge map instance through knowledge extraction,
		  attribute fusion and knowledge storage, and finally implant
		  the model into the network, so that we can provide the
		  users with an accurate, intuitive, high-capacity, fast, and
		  in-depth spatial and temporal correlation historical
		  knowledge map. Finally, the model is implanted into the
		  network to provide users with accurate, intuitive,
		  high-capacity, fast, and deeply spatio-temporally related
		  popularization services. [Results] The constructed
		  knowledge graph is stored and visualized by Neo4j, supports
		  Cypher language for knowledge query, and realizes accurate
		  promotion in three ways: webpage, SVG vector image of
		  knowledge graph, and short video. [Limitations] As the
		  historical knowledge is more cumbersome, data acquisition
		  takes a long time while ensuring correctness. [Conclusion]
		  Compared with traditional knowledge mapping, knowledge
		  mapping based on spatio-temporal fusion integrates the
		  spatio-temporal factors associated with events into the
		  model, and the events shown in the mapping are more
		  abundant, so that the users can understand the events and
		  their spatio-temporal backgrounds clearly, accurately,
		  intuitively, and quickly through various network terminals,
		  and it is more efficient and more acceptable than the
		  traditional popularization methods.},
  booktitle	= {Proceedings of the 2025 4th International Conference on
		  Cyber Security, Artificial Intelligence and the Digital
		  Economy},
  pages		= {565–571},
  numpages	= {7}
}

@Article{	  10.1109/taslp.2021.3138670,
  author	= {Li, Qian and Peng, Hao and Li, Jianxin and Wu, Jia and
		  Ning, Yuanxing and Wang, Lihong and Yu, Philip S. and Wang,
		  Zheng},
  title		= {Reinforcement Learning-Based Dialogue Guided Event
		  Extraction to Exploit Argument Relations},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3138670},
  doi		= {10.1109/TASLP.2021.3138670},
  abstract	= {Event extraction is a ftask for natural language
		  processing. Finding the roles of event arguments like event
		  participants is essential for event extraction. However,
		  doing so for real-life event descriptions is challenging
		  because an argument’s role often varies in different
		  contexts. While the relationship and interactions between
		  multiple arguments are useful for settling the argument
		  roles, such information is largely ignored by existing
		  approaches. This paper presents a better approach for event
		  extraction by explicitly utilizing the relationships of
		  event arguments. We achieve this through a carefully
		  designed task-oriented dialogue system. To model the
		  argument relation, we employ reinforcement learning and
		  incremental learning to extract multiple arguments via a
		  multi-turned, iterative process. Our approach leverages
		  knowledge of the already extracted arguments of the same
		  sentence to determine the role of arguments that would be
		  difficult to decide individually. It then uses the newly
		  obtained information to improve the decisions of previously
		  extracted arguments. This two-way feedback process allows
		  us to exploit the argument relations to effectively settle
		  argument roles, leading to better sentence understanding
		  and event extraction. Experimental results show that our
		  approach consistently outperforms seven state-of-the-art
		  event extraction methods for the classification of events
		  and argument role and argument identification.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= dec,
  pages		= {520–533},
  numpages	= {14}
}

@InProceedings{	  10.1109/jcdl57899.2023.00065,
  author	= {Engel, Felix and Krdzavac, Nenad and Tuncay, Erhun Giray
		  and Klinger, Axel and Hughes, John},
  title		= {Semantification of Space Data - A Feasibility Study},
  year		= {2024},
  isbn		= {9798350399318},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL57899.2023.00065},
  doi		= {10.1109/JCDL57899.2023.00065},
  abstract	= {This paper presents a new approach to the semantic
		  representation of NASA planetary mission data. The Open
		  Archival Information Systems (OAIS) Information Model (IM)
		  is used as a model to represent this data in a Knowledge
		  Graph (KG). To prepare the data for this requirement, a
		  machine learning approach is used to extract information
		  (entities) and connect to Wikidata. A demo shows the
		  advantages of using federated SPARQL queries over the
		  created KG and Wikidata.},
  booktitle	= {Proceedings of the 2023 ACM/IEEE Joint Conference on
		  Digital Libraries},
  pages		= {295–296},
  numpages	= {2},
  keywords	= {knowledge graph, natural language processing, extracting
		  semantics},
  location	= {Santa Fe, New Mexico, USA},
  series	= {JCDL '23}
}

@Article{	  10.1145/3440755,
  author	= {Chandrasekaran, Dhivya and Mago, Vijay},
  title		= {Evolution of Semantic Similarity—A Survey},
  year		= {2021},
  issue_date	= {March 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3440755},
  doi		= {10.1145/3440755},
  abstract	= {Estimating the semantic similarity between text data is
		  one of the challenging and open research problems in the
		  field of Natural Language Processing (NLP). The versatility
		  of natural language makes it difficult to define rule-based
		  methods for determining semantic similarity measures. To
		  address this issue, various semantic similarity methods
		  have been proposed over the years. This survey article
		  traces the evolution of such methods beginning from
		  traditional NLP techniques such as kernel-based methods to
		  the most recent research work on transformer-based models,
		  categorizing them based on their underlying principles as
		  knowledge-based, corpus-based, deep neural network–based
		  methods, and hybrid methods. Discussing the strengths and
		  weaknesses of each method, this survey provides a
		  comprehensive view of existing systems in place for new
		  researchers to experiment and develop innovative ideas to
		  address the issue of semantic similarity.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {41},
  numpages	= {37},
  keywords	= {word embeddings, supervised and unsupervised methods,
		  linguistics, knowledge-based methods, corpus-based methods,
		  Semantic similarity}
}

@InProceedings{	  10.1145/3593013.3594011,
  author	= {Kang, Edward B.},
  title		= {On the Praxes and Politics of AI Speech Emotion
		  Recognition},
  year		= {2023},
  isbn		= {9798400701924},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3593013.3594011},
  doi		= {10.1145/3593013.3594011},
  abstract	= {There is no scientific consensus on what is meant by
		  “emotion” – researchers have examined various
		  phenomena spanning brain modes, feelings, sensations, and
		  cognitive structures, among others, in their study of
		  emotional experiences. For the purposes of developing an AI
		  speech emotion recognition (SER) system, however, emotion
		  must be defined, bounded, and instantiated as ground truth
		  in the training data. This means practical choices must be
		  made in which particular emotional ontologies are
		  prioritized over others in the construction of SER
		  datasets. In this paper, I explore these tensions around
		  fairness, accountability, and transparency by analyzing
		  open-source datasets used for SER applications along with
		  their accompanying methodology papers. Specifically, I
		  critique the centrality of discrete emotion theory in SER
		  applications as a contestable emotional framework that is
		  invoked primarily for its practical utility and alignment
		  – as opposed to scientific rigor – with machine
		  learning epistemologies. In so doing, I also shed light on
		  the role of the dataset creators as emotional designers in
		  their attempt to produce, elicit, record, and index
		  emotional expressions for the purposes of crafting SER
		  training datasets. Ultimately, by further querying SER
		  through the aperture of Critical Disability Studies, I use
		  this empirical work to examine the sociopolitical stakes of
		  SER as a normative and regulatory technology that siphons
		  emotion into a broader agenda of capitalistic productivity
		  in the context of call center optimization.},
  booktitle	= {Proceedings of the 2023 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {455–466},
  numpages	= {12},
  location	= {Chicago, IL, USA},
  series	= {FAccT '23}
}

@InProceedings{	  10.1145/3375462.3375515,
  author	= {Peri, Sai Santosh Sasank and Chen, Bodong and Dougall,
		  Angela Liegey and Siemens, George},
  title		= {Towards understanding the lifespan and spread of ideas:
		  epidemiological modeling of participation on Twitter},
  year		= {2020},
  isbn		= {9781450377126},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3375462.3375515},
  doi		= {10.1145/3375462.3375515},
  abstract	= {How ideas develop and evolve is a topic of interest for
		  educators. By understanding this process, designers and
		  educators are better able to support and guide
		  collaborative learning activities. This paper presents an
		  application of our Lifespan of an Idea framework to measure
		  engagement patterns among individuals in communal
		  socio-technical spaces like Twitter. We correlated
		  engagement with social participation, enabling the process
		  of idea expression, spread, and evolution. Social
		  participation leads to transmission of ideas from one
		  individual to another and can be gauged in the same way as
		  evaluating diseases. The temporal dynamics of the social
		  participation can be modeled through the lens of
		  epidemiological modeling. To test the plausibility of this
		  framework, we investigated social participation on Twitter
		  using the tweet posting patterns of individuals in three
		  academic conferences and one long term chat space. We used
		  a basic SIR epidemiological model, where the rate
		  parameters were estimated through Euler's solutions to SIR
		  model and non-linear least squares optimization technique.
		  We discuss the differences in the social participation
		  among individuals in these spaces based on their transition
		  behavior into different categories of the SIR model. We
		  also made inferences on how the total lifetime of these
		  different twitter spaces affects the engagement among
		  individuals. We conclude by discussing implications of this
		  study and planned future research of refining the Lifespan
		  of an Idea Framework.},
  booktitle	= {Proceedings of the Tenth International Conference on
		  Learning Analytics \&amp; Knowledge},
  pages		= {197–202},
  numpages	= {6},
  keywords	= {networked learning, knowledge creation, ideas,
		  epidemiology, engagement patterns, connectivism},
  location	= {Frankfurt, Germany},
  series	= {LAK '20}
}

@InProceedings{	  10.1145/3543873.3587585,
  author	= {Timmer, Roelien C. and Mark, Megan and Khoo, Fech Scen and
		  Ribeiro Martins, Marcella Scoczynski and Berea, Anamaria
		  and Renard, Gregory and Bugbee, Kaylin},
  title		= {NASA Science Mission Directorate Knowledge Graph
		  Discovery},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587585},
  doi		= {10.1145/3543873.3587585},
  abstract	= {The size of the National Aeronautics and Space
		  Administration (NASA) Science Mission Directorate (SMD)
		  data catalog is growing exponentially, allowing researchers
		  to make discoveries. However, making discoveries is
		  challenging and time-consuming due to the size of the data
		  catalogs, and as many concepts and data are indirectly
		  connected. This paper proposes a pipeline to generate
		  knowledge graphs (KGs) representing different NASA SMD
		  domains. These KGs can be used as the basis for dataset
		  search engines, saving researchers time and supporting them
		  in finding new connections. We collected textual data and
		  used several modern natural language processing (NLP)
		  methods to create the nodes and the edges of the KGs. We
		  explore the cross-domain connections, discuss our
		  challenges, and provide future directions to inspire
		  researchers working on similar challenges.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {795–799},
  numpages	= {5},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@Article{	  10.1145/3399630,
  author	= {Tao, Jie and Zhou, Lina},
  title		= {A Weakly Supervised WordNet-Guided Deep Learning Approach
		  to Extracting Aspect Terms from Online Reviews},
  year		= {2020},
  issue_date	= {September 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {11},
  number	= {3},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3399630},
  doi		= {10.1145/3399630},
  abstract	= {The unstructured nature of online reviews makes it
		  inefficient and inconvenient for prospective consumers to
		  research and use in support of purchase decision making.
		  The aspects of products provide a fine-grained meaningful
		  perspective for understanding and organizing review texts.
		  Traditional aspect term extraction approaches rely on
		  discrete language models that treat words in isolation.
		  Despite that continuous-space language models have
		  demonstrated promise in addressing a wide range of
		  problems, their application in aspect term extraction faces
		  significant challenges. For instance, existing
		  continuous-space language models typically require large
		  collections of labeled data, which remain difficult to
		  obtain in many domains. More importantly, previous methods
		  are largely data driven but overlook the role of human
		  knowledge in guiding model development. To address these
		  limitations, this study designs and develops weakly
		  supervised WordNet-guided deep learning to aspect term
		  extraction. The approach draws on deep-level semantic
		  information from WordNet to guide not only the selection
		  representative seed terms but also the pruning of aspect
		  candidate terms. The weak supervision is provided by a very
		  small set of labeled data. We conduct a comprehensive
		  evaluation of the proposed method using both direct and
		  indirect methods. The evaluation results with Yelp
		  restaurant reviews demonstrate that our proposed method
		  consistently outperforms all baseline methods including
		  discrete models and the state-of-the-art continuous-space
		  language models for aspect term extraction across both
		  direct and indirect evaluations. The research findings have
		  broad research, technical, and practical implications for
		  various stakeholders of online reviews.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= jul,
  articleno	= {13},
  numpages	= {22},
  keywords	= {text analytics, semantic knowledge, deep learning,
		  continuous-space language model, Aspect term extraction}
}

@InProceedings{	  10.1145/3652620.3688212,
  author	= {K\"{u}hne, Thomas and Maier, Pierre},
  title		= {FMMLx and DLM -- A Contribution to the MULTI Collaborative
		  Comparison Challenge},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688212},
  doi		= {10.1145/3652620.3688212},
  abstract	= {This paper is a response to the MULTI 2022 Collaborative
		  Comparison Challenge [23]. We compare FMMLx- and DLM-based
		  solutions. We first present each approach and solution
		  separately, and then discuss trade-offs of both the
		  solutions and the approaches.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {800–809},
  numpages	= {10},
  keywords	= {MLM, modeling challenge, FMMLx, DLM},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3243907.3243908,
  author	= {Viola, Fabio and Stolfi, Ariane and Milo, Alessia and
		  Ceriani, Miguel and Barthet, Mathieu and Fazekas,
		  Gy\"{o}rgy},
  title		= {Playsound.space: enhancing a live music performance tool
		  with semantic recommendations},
  year		= {2018},
  isbn		= {9781450364959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3243907.3243908},
  doi		= {10.1145/3243907.3243908},
  abstract	= {Playsound is a simple and intuitive web-based tool for
		  music composition based on sounds from Freesound, an online
		  repository of diverse audio content with Creative Commons
		  licenses. In this paper, we present an approach based on
		  Semantic Web technologies to provide recommendations to
		  Playsound users. A Semantic Web of Things architecture is
		  outlined, showing loosely coupled, independent software
		  agents interoperating by means of a semantic
		  publish/subscribe platform and a set of ontologies to
		  describe agents, audio contents, input/output of audio
		  analytics tools and recommendations. Preliminary tests
		  confirm that the designed architecture adapts well to
		  environments where services can be discovered and
		  seamlessly orchestrated on the fly, resulting in a dynamic
		  workflow.},
  booktitle	= {Proceedings of the 1st International Workshop on Semantic
		  Applications for Audio and Music},
  pages		= {46–53},
  numpages	= {8},
  keywords	= {Web of Things, Semantic Web, Recommendations, Ontologies},
  location	= {Monterey, CA, USA},
  series	= {SAAM '18}
}

@InProceedings{	  10.1145/3544538.3544668,
  author	= {Kourtiche, Ali and Felici-Castell, S. and Perez Solano, J.
		  J. and Segura-Garcia, J. and Soriano-Asensi, A. and
		  Navarro-Camba, E. and Pinto, J.},
  title		= {Internet of Things under a semantic perspective with user
		  profiles},
  year		= {2022},
  isbn		= {9781450397384},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544538.3544668},
  doi		= {10.1145/3544538.3544668},
  abstract	= {Internet of Things (IoT) is a network made up of different
		  types of devices integrated into the Internet. For the use
		  of IoT in cities to assist humans beings in their daily
		  activities, it is necessary to extract and process as much
		  information as possible and therefore, semantic web
		  technologies are a key point. This new environment is known
		  as the Semantic Web of Things (SWoT). The application of
		  semantic techniques to IoT can improve interoperability,
		  effective access to data, discovery of integration
		  resources, reasoning and processing of knowledge extraction
		  from data, opening up opportunities for new applications.
		  We show an overview of the most relevant semantic
		  technologies, focusing on SWoT and well-accepted ontologies
		  for IoT and we stress that considering users’
		  preferences, interests and needs are of vital importance,
		  as a complement to the semantic information extracted. This
		  new scenario enables the development of advanced
		  applications and services to meet user requirements and
		  needs, in particular for smart city applications.},
  booktitle	= {Proceedings of the 11th Euro American Conference on
		  Telematics and Information Systems},
  articleno	= {19},
  numpages	= {4},
  keywords	= {web of things, user profiles, smart cities, semantic web,
		  ontology, e-public, citizens, IoT},
  location	= {Aveiro, Portugal},
  series	= {EATIS '22}
}

@Article{	  10.1145/3522586,
  author	= {Tama\v{s}auskaitundefined, Gytundefined and Groth, Paul},
  title		= {Defining a Knowledge Graph Development Process Through a
		  Systematic Review},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {32},
  number	= {1},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3522586},
  doi		= {10.1145/3522586},
  abstract	= {Knowledge graphs are widely used in industry and studied
		  within the academic community. However, the models applied
		  in the development of knowledge graphs vary. Analysing and
		  providing a synthesis of the commonly used approaches to
		  knowledge graph development would provide researchers and
		  practitioners a better understanding of the overall process
		  and methods involved. Hence, this article aims at defining
		  the overall process of knowledge graph development and its
		  key constituent steps. For this purpose, a systematic
		  review and a conceptual analysis of the literature was
		  conducted. The resulting process was compared to case
		  studies to evaluate its applicability. The proposed process
		  suggests a unified approach and provides guidance for both
		  researchers and practitioners when constructing and
		  managing knowledge graphs.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= feb,
  articleno	= {27},
  numpages	= {40},
  keywords	= {information integration, development process semantic
		  network, knowledge graph construction, Knowledge graphs}
}

@InProceedings{	  10.1145/3589335.3651454,
  author	= {Ragab, Mohamed and Savateev, Yury and Oliver, Helen and
		  Tiropanis, Thanassis and Poulovassilis, Alexandra and
		  Chapman, Adriane and Roussos, George},
  title		= {Unlocking the Potential of Health Data with Decentralised
		  Search in Personal Health Datastores},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651454},
  doi		= {10.1145/3589335.3651454},
  abstract	= {In the digital age, where health data and digital lives
		  converge, data privacy and control are crucial. The advent
		  of AI and Large Language Models (LLMs) brings advanced data
		  analysis and healthcare predictions, but also privacy
		  concerns. The ESPRESSO project 1 asserts that for AI to be
		  trustworthy and effective in healthcare, it must prioritize
		  user control over corporate interests. The shift towards
		  decentralized personal online datastores (pods) and Solid 2
		  principles represents a new era of private, controllable
		  Web interactions, balancing AI data protection and machine
		  intelligence. This balance is particularly important for
		  applications involving health data. However,
		  decentralization poses challenges, particularly in secure,
		  efficient data search and data retrieval, that need to be
		  addressed first. We argue that a decentralized search
		  system that provides a large-scale search across Solid
		  pods, while considering data owners' control of their data
		  and users' different access rights, is crucial for this new
		  paradigm. In this paper, we describe how our current
		  decentralized search system's prototype (ESPRESSO) helps to
		  query structured and unstructured personal health data in
		  Solid servers. The paper also describes a search scenario
		  that shows how ESPRESSO can search health data combined
		  with fitness personal data stored in different personal
		  datastores},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1154–1157},
  numpages	= {4},
  keywords	= {decentralized web search, health and well-being data,
		  linked data, personal online datastores, solid framework},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3644523.3644601,
  author	= {Wang, Kaijie and Wang, Tiejun and Lu, Ziling},
  title		= {Construction of Gesar Epic Event Graph Based on Event
		  Extraction},
  year		= {2024},
  isbn		= {9798400709517},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644523.3644601},
  doi		= {10.1145/3644523.3644601},
  abstract	= {The Biography of King Gesar is the largest heroic epic in
		  world history and also one of the world's intangible
		  cultural heritage sites. The Tibetan culture it carries is
		  an important component of Chinese civilization. In order to
		  better showcase and protect this intangible cultural
		  heritage, and provide data support for research on digital
		  retrieval, intelligent Q&amp;A, and reading comprehension
		  of Gesar cultural resources, the construction of Gesar
		  event graph was studied. Firstly, the BTCNN event
		  federation model is used to extract event trigger words and
		  event elements from semi structured and unstructured event
		  texts, assisting in manually customized event
		  relationships, and completing the preliminary construction
		  of the Gesar event graph. Based on the constructed event
		  graph, a visualization system for the graph was built, and
		  the graph retrieval function was designed and implemented,
		  supporting entity query, relationship query, and entity
		  relationship hybrid query.},
  booktitle	= {Proceedings of the 2023 4th International Conference on
		  Computer Science and Management Technology},
  pages		= {431–436},
  numpages	= {6},
  location	= {Xi'an, China},
  series	= {ICCSMT '23}
}

@InProceedings{	  10.1145/3243907.3243914,
  author	= {Nurmikko-Fuller, Terhi and Bangert, Daniel and Hao, Yun
		  and Downie, J. Stephen},
  title		= {Swinging Triples: Bridging Jazz Performance Datasets using
		  Linked Data},
  year		= {2018},
  isbn		= {9781450364959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3243907.3243914},
  doi		= {10.1145/3243907.3243914},
  abstract	= {The jazz performance metadata prototype JazzCats:Jazz
		  Collection of Aggregated Triples uses Linked Data to bridge
		  four discrete jazz music datasets: Linked Jazz, with
		  prosopographical and interpersonal information about
		  musicians; the Weimar Jazz Database (WJazzD), containing
		  musicological metadata; a discography of the jazz standard
		  Body&amp;Soul; and J-DISC, a fourth independent but
		  complementary and extensive discographic project. Through
		  the use of custom-built ontological structures the data,
		  originally stored in various different information
		  structures, has been converted to RDF and merged together
		  in a single triplestore. The result is a new digital
		  resource that can be used to support and enrich scholarship
		  and research in musicology and performance studies.},
  booktitle	= {Proceedings of the 1st International Workshop on Semantic
		  Applications for Audio and Music},
  pages		= {42–45},
  numpages	= {4},
  keywords	= {semantic web, performance, ontologies, metadata, jazz,
		  digital musicology, SPARQL, Linked Data},
  location	= {Monterey, CA, USA},
  series	= {SAAM '18}
}

@InProceedings{	  10.1145/3371140.3371147,
  author	= {Yousaf, Madiha and Wolter, Diedrich},
  title		= {How to identify appropriate key-value pairs for querying
		  OSM},
  year		= {2019},
  isbn		= {9781450372602},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3371140.3371147},
  doi		= {10.1145/3371140.3371147},
  abstract	= {This paper presents a study on how natural language words
		  that designate types of spatial entities (metropolis, city,
		  creek, etc.) can automatically be translated to the entity
		  classification used in OpenStreetMap (OSM) that assigns
		  key-value tags to entities. The problem of identifying
		  key-value pairs for querying OSM occurs in geographic
		  information retrieval based on natural language text and is
		  difficult for three reasons: Conceptualisation of entities
		  in natural language text and in OSM often differs. Even
		  classification of a single entity type is subject to
		  variations throughout the OSM database. Language is rich
		  and offers many words to communicate nuances of a single
		  entity type. The contribution of this paper is to analyse
		  the contribution of semantic word similarity using Word-Net
		  to identify a mapping from natural language to OSM tags. We
		  present a strategy to identify key-value pairs for natural
		  language words using WordNet and analyse its
		  effectiveness.},
  booktitle	= {Proceedings of the 13th Workshop on Geographic Information
		  Retrieval},
  articleno	= {7},
  numpages	= {6},
  keywords	= {semantics of spatial language, geo-referencing,
		  OpenStreetMap (OSM)},
  location	= {Lyon, France},
  series	= {GIR '19}
}

@Proceedings{	  10.1145/3689492,
  title		= {Onward! '24: Proceedings of the 2024 ACM SIGPLAN
		  International Symposium on New Ideas, New Paradigms, and
		  Reflections on Programming and Software},
  year		= {2024},
  isbn		= {9798400712159},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 2024 ACM SIGPLAN International Symposium on
		  New Ideas, New Paradigms, and Reflections on Programming
		  and Software (Onward! 2024), the premier multidisciplinary
		  conference focused on everything to do with programming and
		  software, including processes, methods, languages,
		  communities and applications. Onward! is more radical, more
		  visionary and more open than other conferences to ideas
		  that are well-argued but not yet proven. We welcome
		  different ways of thinking about, approaching, and
		  reporting on programming language and software engineering
		  research.},
  location	= {Pasadena, CA, USA}
}

@InProceedings{	  10.1145/3276954.3276961,
  author	= {Gavran, Ivan and Mailahn, Ortwin and M\"{u}ller, Rainer
		  and Peifer, Richard and Zufferey, Damien},
  title		= {Tᴏᴏʟ: accessible automated reasoning for human robot
		  collaboration},
  year		= {2018},
  isbn		= {9781450360319},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3276954.3276961},
  doi		= {10.1145/3276954.3276961},
  abstract	= {We present an expressive, concise, and extendable domain
		  specific language for planning of assembly systems, such as
		  industrial human robot cooperation. Increased flexibility
		  requirements in manufacturing processes call for more
		  automation at the description and planning stages of
		  manufacturing. Procedural models are good candidates to
		  meet this demand as programs offer a high degree of
		  flexibility and are easily composed. Furthermore, we aim to
		  make our programs close to declarative specification and
		  integrate automatic reasoning tools to help the users. The
		  constraints come both from specific programs and
		  preexisting knowledge base from the target domain. The case
		  of human robot collaboration is interesting as there is a
		  number of constraints and regulations around this domain.
		  Unfortunately, automated reasoners are often too
		  unpredictable and cannot be used directly by non-experts.
		  In this paper, we present our domain specific language
		  ``Tool Ontology and Optimization Language'' (Tool) and
		  describe how we integrated automated reasoners and planners
		  in a way that makes them accessible to users which have
		  little programming knowledge, but expertise in
		  manufacturing domain and no previous experience with or
		  knowledge about the underlying reasoners. We present
		  encouraging results by applying Tool to a case study from
		  the automotive and aerospace industry.},
  booktitle	= {Proceedings of the 2018 ACM SIGPLAN International
		  Symposium on New Ideas, New Paradigms, and Reflections on
		  Programming and Software},
  pages		= {44–56},
  numpages	= {13},
  keywords	= {robotics and automation, knowledge integration, industry
		  4.0, human-robot cooperation, domain specific language,
		  cyber-physical systems, automated reasoning, assembly
		  planning},
  location	= {Boston, MA, USA},
  series	= {Onward! 2018}
}

@Proceedings{	  10.1145/3640794,
  title		= {CUI '24: Proceedings of the 6th ACM Conference on
		  Conversational User Interfaces},
  year		= {2024},
  isbn		= {9798400705113},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Luxembourg, Luxembourg}
}

@Article{	  10.1145/3152889,
  author	= {Dumitrache, Anca and Aroyo, Lora and Welty, Chris},
  title		= {Crowdsourcing Ground Truth for Medical Relation
		  Extraction},
  year		= {2018},
  issue_date	= {June 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {2},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3152889},
  doi		= {10.1145/3152889},
  abstract	= {Cognitive computing systems require human labeled data for
		  evaluation and often for training. The standard practice
		  used in gathering this data minimizes disagreement between
		  annotators, and we have found this results in data that
		  fails to account for the ambiguity inherent in language. We
		  have proposed the CrowdTruth method for collecting ground
		  truth through crowdsourcing, which reconsiders the role of
		  people in machine learning based on the observation that
		  disagreement between annotators provides a useful signal
		  for phenomena such as ambiguity in the text. We report on
		  using this method to build an annotated data set for
		  medical relation extraction for the cause and treat
		  relations, and how this data performed in a supervised
		  training experiment. We demonstrate that by modeling
		  ambiguity, labeled data gathered from crowd workers can (1)
		  reach the level of quality of domain experts for this task
		  while reducing the cost, and (2) provide better training
		  data at scale than distant supervision. We further propose
		  and validate new weighted measures for precision, recall,
		  and F-measure, which account for ambiguity in both human
		  and machine performance on this task.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= jul,
  articleno	= {11},
  numpages	= {20},
  keywords	= {relation extraction, natural language ambiguity,
		  inter-annotator disagreement, crowdtruth, crowd truth,
		  clinical natural language processing, Ground truth}
}

@InProceedings{	  10.1145/3227609.3227654,
  author	= {Mandi\'{c}, Milinko},
  title		= {Semantic Web based software platform for curriculum
		  harmonization},
  year		= {2018},
  isbn		= {9781450354899},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3227609.3227654},
  doi		= {10.1145/3227609.3227654},
  abstract	= {This paper presents a software platform for comparing
		  informatics teacher education curricula. The ontological
		  model of the chosen informatics teachers' curriculum from
		  the Republic of Serbia and the reference informatics
		  teachers' curriculum model were created. The semi-automatic
		  software platform is based on the standard techniques and
		  methods of ontology matching. The created ontological
		  models of the teacher education curricula are compared
		  using the developed software. Analysis of the results of
		  comparison includes consideration of classes' matching,
		  obtained system evaluation (by the expert team) and a
		  harmonization of the Revised Bloom's taxonomy categories.
		  Also, the obtained results are compared with the results
		  obtained for other combinations of input ontological models
		  (secondary informatics and informatics teacher education
		  curricula models). The analysis of the results revealed the
		  need to improve the content and structure of the observed
		  model curricula as well as the limits of the developed
		  system and possibilities of improving the software
		  platform.},
  booktitle	= {Proceedings of the 8th International Conference on Web
		  Intelligence, Mining and Semantics},
  articleno	= {31},
  numpages	= {9},
  keywords	= {teacher education curriculum, ontology, matching,
		  informatics, alignment},
  location	= {Novi Sad, Serbia},
  series	= {WIMS '18}
}

@Article{	  10.1145/3527635,
  author	= {Steimann, Friedrich},
  title		= {Containerless Plurals: Separating Number from Type in
		  Object-Oriented Programming},
  year		= {2022},
  issue_date	= {December 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {44},
  number	= {4},
  issn		= {0164-0925},
  url		= {https://doi.org/10.1145/3527635},
  doi		= {10.1145/3527635},
  abstract	= {To let expressions evaluate to no or many objects, most
		  object-oriented programming languages require the use of
		  special constructs that encode these cases as single
		  objects or values. While the requirement to treat these
		  standard situations idiomatically seems to be broadly
		  accepted, I argue that its alternative, letting expressions
		  evaluate to any number of objects directly, has several
		  advantages that make it worthy of consideration. As a proof
		  of concept, I present a core object-oriented programming
		  language, dubbed Num, which separates number from type so
		  that the type of an expression is independent of the number
		  of objects it may evaluate to, thus removing one major
		  obstacle to using no, one, and many objects uniformly.
		  Furthermore, Num abandons null references, replaces the
		  nullability of reference types with the more general notion
		  of countability, and allows methods to be invoked on any
		  number of objects, including no object. To be able to adapt
		  behavior to the actual number of receivers, Num complements
		  instance methods with plural methods, that is, with methods
		  that operate on a number of objects jointly and that
		  replace static methods known from other languages. An
		  implementation of Num in Prolog and accompanying type and
		  number safety proofs are presented.},
  journal	= {ACM Trans. Program. Lang. Syst.},
  month		= sep,
  articleno	= {21},
  numpages	= {56},
  keywords	= {object-relational programming, null-safety, bunches,
		  collections, Multiplicities in programming}
}

@InProceedings{	  10.1145/3734947.3734958,
  author	= {Katwe, Praveen Kumar and Balabantaray, Rakesh Chandra and
		  Vittala, Kali Prasad},
  title		= {Evaluating Relation Hallucination in Text Summarization:
		  An Introduction to the Relation Hallucination Index},
  year		= {2025},
  isbn		= {9798400713187},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3734947.3734958},
  doi		= {10.1145/3734947.3734958},
  abstract	= {Text summarization involves generating a concise, precise,
		  and coherent summary that captures the essence of a longer
		  document. Relation Hallucination refers to the generation
		  of summary sentences that imply or state relationships
		  between entities or events that were not present or implied
		  in the original document. Relation Hallucination is a
		  pivotal concern in abstractive text summarization, where
		  models fabricate or exaggerate connections, leading to
		  misleading summaries. This paper delves deep into the
		  evaluation of Relation Hallucination in abstractive
		  summarization. We introduce a novel metric, the Relation
		  Hallucination Index, designed to evaluate and quantify
		  Relation hallucinations across various state-of-the-art
		  models. Emphasizing the paramount importance of maintaining
		  accurate relational context in summaries, this article
		  showcases the efficacy of the Relation hallucination index
		  in discerning fabricated relationships. The Relation
		  hallucination index provides a quantitative measure of the
		  various levels of Relation Hallucination present in
		  generated summaries, enabling practitioners from industry
		  as well as Academia to select models aligned with desired
		  hallucination parameters for their tailored applications.},
  booktitle	= {Proceedings of the 16th Annual Meeting of the Forum for
		  Information Retrieval Evaluation},
  pages		= {88–94},
  numpages	= {7},
  keywords	= {Hallucination;Abstractive summarization; Relation Tuples;
		  Positive Hallucination; Negative Hallucination; Lost Focus;
		  Extractiveness Factor;Over Focus},
  location	= { },
  series	= {FIRE '24}
}

@InProceedings{	  10.1145/3453483.3454047,
  author	= {Chen, Qiaochu and Lamoreaux, Aaron and Wang, Xinyu and
		  Durrett, Greg and Bastani, Osbert and Dillig, Isil},
  title		= {Web question answering with neurosymbolic program
		  synthesis},
  year		= {2021},
  isbn		= {9781450383912},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3453483.3454047},
  doi		= {10.1145/3453483.3454047},
  abstract	= {In this paper, we propose a new technique based on program
		  synthesis for extracting information from webpages. Given a
		  natural language query and a few labeled webpages, our
		  method synthesizes a program that can be used to extract
		  similar types of information from other unlabeled webpages.
		  To handle websites with diverse structure, our approach
		  employs a neurosymbolic DSL that incorporates both neural
		  NLP models as well as standard language constructs for tree
		  navigation and string manipulation. We also propose an
		  optimal synthesis algorithm that generates all DSL programs
		  that achieve optimal F1 score on the training examples. Our
		  synthesis technique is compositional, prunes the search
		  space by exploiting a monotonicity property of the DSL, and
		  uses transductive learning to select programs with good
		  generalization power. We have implemented these ideas in a
		  new tool called WebQA and evaluate it on 25 different tasks
		  across multiple domains. Our experiments show that WebQA
		  significantly outperforms existing tools such as
		  state-of-the-art question answering models and wrapper
		  induction systems.},
  booktitle	= {Proceedings of the 42nd ACM SIGPLAN International
		  Conference on Programming Language Design and
		  Implementation},
  pages		= {328–343},
  numpages	= {16},
  keywords	= {Web Information Extraction, Programming by Example,
		  Program Synthesis},
  location	= {Virtual, Canada},
  series	= {PLDI 2021}
}

@InProceedings{	  10.1145/3672758.3672868,
  author	= {Wang, Yi Long and Wen, Ying Zi and Wang, Yao Hui and She,
		  Xin Peng and Sun, Xiao Hu and Lyu, Xue Qiang and Hao,
		  Qiang},
  title		= {Named entity recognition method for mine electromechanical
		  equipment field},
  year		= {2024},
  isbn		= {9798400716942},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672758.3672868},
  doi		= {10.1145/3672758.3672868},
  abstract	= {Aiming at the lack of annotated corpus in the field of
		  mine electromechanical equipment, the high similarity
		  between different categories of entities and the long names
		  of some entities, this paper proposed an entity extraction
		  method of mine electromechanical equipment based on the
		  fusion of technical words and comparative learning.
		  Firstly, an ontology library was constructed according to
		  the characteristics of mine electromechanical equipment,
		  Word2Vec was used to obtain the vector representation of
		  characters and words, and the term words were obtained by
		  matching in the ontology library. The multi-term multi-head
		  attention mechanism was used to assign larger weights to
		  these term words and then they were fused with the
		  character vector. Then, the Bi-LSTM model was used for
		  feature extraction, and a contrastive learning strategy
		  based on R-drop was used to reduce the recognition bias
		  error caused by the similarity of entity names. We also
		  improved the loss function by using the relative entropy
		  loss calculated by the Bi-LSTM layer as the regularization
		  term of the loss in the CRF layer to form a constraint on
		  the loss function and enhance the robustness of the model
		  to Dropout. Finally, we used the CRF model to decode to
		  obtain the optimal label. Experimental results show that
		  compared with the existing mainstream baseline method
		  Lattice-LSTM, our proposed method achieves better results
		  on the self-constructed mine electromechanical equipment
		  corpus. The precision, recall and F1 value are improved by
		  4.73, 5.5 and 5.09 percentage points, respectively.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Computer, Artificial Intelligence and Control Engineering},
  pages		= {660–664},
  numpages	= {5},
  location	= {Xi' an, China},
  series	= {CAICE '24}
}

@InProceedings{	  10.1145/3366650.3366661,
  author	= {Rabut, Benedict A. and Fajardo, Arnel C. and Medina, Ruji
		  P.},
  title		= {Multi-class Document Classification Using Improved Word
		  Embeddings},
  year		= {2019},
  isbn		= {9781450372909},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366650.3366661},
  doi		= {10.1145/3366650.3366661},
  abstract	= {In this paper, we conducted an experiment to build a
		  classification model that combines different techniques in
		  most of the Natural Language Processing Tasks. We used the
		  word embedding method to transform every word in the
		  dataset and to obtain the custom-built word embedding
		  vectors. This is in contrast to the approaches in the
		  previous literature that implement word embedding using the
		  pre-trained word embedding vectors. We enriched the
		  custom-built word embedding vectors by incorporating
		  Part-of-Speech (POS) tag vectors to provide additional
		  semantic information about the word to be used in training
		  our proposed classification model. The proposed model was
		  built using the neural network approach, which is
		  considered to be more efficient and reliable in solving
		  real problems for document classification tasks. We
		  fine-tuned the parameters during the training of our neural
		  network classification model with our aim to increase the
		  performance in terms of classification accuracy. The
		  experimental result demonstrates that our model performs
		  remarkably well and increase the percentage accuracy up to
		  1.7\% compared to the accuracy results obtained by the
		  previous baseline word embedding methods using the same
		  dataset. It was also observed that our model outperforms
		  some other traditional classification models implemented
		  using different techniques and machine learning
		  algorithms.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Computing and Big Data},
  pages		= {42–46},
  numpages	= {5},
  keywords	= {Document Classification, Natural Language Processing, Word
		  Embeddings},
  location	= {Taichung, Taiwan},
  series	= {ICCBD '19}
}

@InProceedings{	  10.5555/3643142.3643364,
  author	= {Wilsdorf, Pia and Zuska, Marian and Andelfinger, Philipp
		  and Uhrmacher, Adelinde M. and Peters, Florian},
  title		= {Validation Without Data - Formalizing Stylized Facts of
		  Time Series},
  year		= {2024},
  isbn		= {9798350369663},
  publisher	= {IEEE Press},
  abstract	= {A stylized fact is a simplified presentation of an
		  empirical finding. When modeling and simulating complex
		  systems and real data are sparse, stylized facts have
		  become a key instrument for building trust in a model as
		  they represent important requirements regarding the model's
		  behavior. However, automatically validating stylized facts
		  has remained limited as they are usually expressed in
		  natural language. Therefore, we develop a formal language
		  with a custom syntax and tailored predicates allowing
		  modelers to unambiguously and succinctly describe important
		  (temporal) characteristics of simulation traces or
		  relationships between multiple traces via statistical
		  tests. The proposed formal language is able to express
		  numerous facts from the literature in different application
		  domains, as well as to automatically check stylized facts.
		  If stylized facts are defined at the beginning of a
		  simulation study, formally expressing and checking them can
		  streamline and guide the development of simulation models
		  and their successive revisions.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2674–2685},
  numpages	= {12},
  location	= {San Antonio, Texas, USA},
  series	= {WSC '23}
}

@InProceedings{	  10.1145/3303772.3303834,
  author	= {Fiallos, Angel and Ochoa, Xavier},
  title		= {Semi-Automatic Generation of Intelligent Curricula to
		  Facilitate Learning Analytics},
  year		= {2019},
  isbn		= {9781450362566},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3303772.3303834},
  doi		= {10.1145/3303772.3303834},
  abstract	= {Several Learning Analytics applications are limited by the
		  cost of generating a computer understandable description of
		  the course domain, what is called an Intelligent
		  Curriculum. The following work contributes a novel approach
		  to (semi-)automatically generate Intelligent Curriculum
		  through ontologies extracted from existing learning
		  materials such as digital books or web content. Through a
		  series of natural language processing steps, the
		  semi-structured information present in existing content is
		  transformed into a concept-graph. This work also evaluates
		  the proposed methodology by applying it to learning content
		  for two different courses and measuring the quality of the
		  extracted ontologies against manually generated ones. The
		  results obtained suggest that the technique can be readily
		  used to provide domain information to other Learning
		  Analytics tools.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Learning Analytics \&amp; Knowledge},
  pages		= {46–50},
  numpages	= {5},
  keywords	= {ontologies, intelligent curriculum, NLP},
  location	= {Tempe, AZ, USA},
  series	= {LAK19}
}

@InProceedings{	  10.5555/3721488.3721682,
  author	= {Jokinen, Kristiina and Wilcock, Graham},
  title		= {Towards Domain Graphs and Dialogue Graphs for
		  Conversational Grounding in HRI},
  year		= {2025},
  publisher	= {IEEE Press},
  abstract	= {Knowledge graphs have been used to improve robot dialogues
		  by providing more sophisticated world knowledge. We now
		  propose a new role for knowledge graphs in GenAI-based HRI
		  that aims to reduce dialogue errors by better
		  conversational grounding. This approach uses both domain
		  knowledge graphs and dialogue history graphs, constructing
		  shared knowledge via entity linking. We present first steps
		  towards these aims, and also address sustainability by
		  supporting the use of smaller models.},
  booktitle	= {Proceedings of the 2025 ACM/IEEE International Conference
		  on Human-Robot Interaction},
  pages		= {1373–1377},
  numpages	= {5},
  keywords	= {conversational grounding, human-robot dialogues, knowledge
		  graphs, sustainability},
  location	= {Melbourne, Australia},
  series	= {HRI '25}
}

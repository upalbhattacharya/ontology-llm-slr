
@article{jha_prediction_2023,
	title = {Prediction of {Protein}-{Protein} {Interactions} {Using} {Vision} {Transformer} and {Language} {Model}},
	volume = {20},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2023.3248797},
	doi = {10.1109/TCBB.2023.3248797},
	abstract = {The knowledge of protein-protein interaction (PPI) helps us to understand proteins’ functions, the causes and growth of several diseases, and can aid in designing new drugs. The majority of existing PPI research has relied mainly on sequence-based approaches. With the availability of multi-omics datasets (sequence, 3D structure) and advancements in deep learning techniques, it is feasible to develop a deep multi-modal framework that fuses the features learned from different sources of information to predict PPI. In this work, we propose a multi-modal approach utilizing protein sequence and 3D structure. To extract features from the 3D structure of proteins, we use a pre-trained vision transformer model that has been fine-tuned on the structural representation of proteins. The protein sequence is encoded into a feature vector using a pre-trained language model. The feature vectors extracted from the two modalities are fused and then fed to the neural network classifier to predict the protein interactions. To showcase the effectiveness of the proposed methodology, we conduct experiments on two popular PPI datasets, namely, the human dataset and the \&lt;italic\&gt;S. cerevisiae\&lt;/italic\&gt; dataset. Our approach outperforms the existing methodologies to predict PPI, including multi-modal approaches. We also evaluate the contributions of each modality by designing uni-modal baselines. We perform experiments with three modalities as well, having gene ontology as the third modality.},
	number = {5},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
	month = feb,
	year = {2023},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	keywords = {Language model, Deep learning, Gene Ontology, Transformers, Feature extraction, Neural Networks, Forecasting, Transformer, Computational linguistics, Vision transformer, protein-protein interaction, vision transformer, Proteins, chemistry, metabolism, human, protein, Protein sequence, Three-dimensional displays, Amino acids, artificial neural network, Humans, amino acid sequence, Computer, Protein sequences, Protein-protein interactions, Amino Acid Sequence, Saccharomyces cerevisiae, Features extraction, multiomics, Three dimensional displays, Three-dimensional display, Amino-acids, 3D Structure, Multiomics},
	pages = {3215--3225},
	annote = {Cited by: 7},
}

@inproceedings{tu_reusable_2024,
	address = {San Antonio, Texas, USA},
	series = {{WSC} '23},
	title = {Reusable {Ontology} {Generation} and {Matching} from {Simulation} {Models}},
	isbn = {979-8-3503-6966-3},
	doi = {10.1109/WSC60868.2023.10407599},
	abstract = {As simulating semiconductor manufacturing grows complex, model reuse becomes appealing since it can reduce the time incurred in developing future models. Also, considering a large network of the semiconductor supply chain, knowledge sharing can enable the efficient development of simulation models in a collaborative organization. Such necessity of reusability and interoperability of simulation models motivates this paper. We will address these challenges through ontological modeling and linking of the simulation components. The first application is generating reusable ontologies from simulation models. Another discussed application is ontology matching for knowledge sharing between simulation components and a meta-model of the semiconductor supply chain. The proposed approach succeeds in automatically transforming simulation into reusable knowledge and identifying interconnection in a semiconductor manufacturing system.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Tu, Ming-Yu and Ehm, Hans and Ismail, Abdelgafar and Ulrich, Philipp},
	year = {2024},
	note = {ISSN: 1558-4305},
	keywords = {Ontologies, Interoperability, Knowledge engineering, Organizations, Supply chains, Semiconductor device manufacture, Semiconductor device modeling},
	pages = {2298--2309},
}

@inproceedings{dimitrakopoulos_enhanced_2025,
	address = {Orlando, Florida, USA},
	series = {{WSC} '24},
	title = {Enhanced {Ontology} {Extraction}: {Integrating} {GPT} {AI} with {Human} {Knowledge} on the {Example} of {EU} {Standards} {Related} to {Semiconductor} {Supply} {Chains}},
	isbn = {979-8-3315-3420-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217617379&doi=10.1109%2FWSC63780.2024.10838760&partnerID=40&md5=8e67c6383c556395af5749406deb7bf3},
	doi = {10.1109/WSC63780.2024.10838760},
	abstract = {This paper addresses challenges in creating ontologies for the semiconductor supply chain. Ontologies are crucial for seamless data exchange within the semantic web, enabling initiatives like GAIA-X and CatenA-X. Traditionally, ontology creation is complex. Here, we propose a novel AI-assisted method using large language models (LLMs) like ChatGPT 4 Turbo to support human experts. This collaboration aims to expedite ontology generation while maintaining quality. While initial tests show promise, refining the human-AI interface for clear content generation remains a focus. By improving this collaboration, we expect to create more accurate and complete ontologies, fostering efficient information sharing and strengthening the meaningfulness of standards within the semiconductor supply chain.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Dimitrakopoulos, George and Ehm, Hans and Tsaousi, Eleni},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Ontology, Language model, Semantic Web, Semantics, Modeling languages, Reliability, Ontology generation, Standards, Collaboration, Accuracy, Chatbots, Ontology Extraction, Supply chains, Information sharing, Ontology's, Semantic-Web, Human knowledge, Human expert, Ontology creations, Semiconducting indium phosphide, Semiconductor supply chain, Web-enabling},
	pages = {1955--1965},
	annote = {Cited by: 0},
}

@inproceedings{phokela_smart_2024,
	address = {Echternach, Luxembourg},
	series = {{ASE} '23},
	title = {Smart {Prompt} {Advisor}: {Multi}-{Objective} {Prompt} {Framework} for {Consistency} and {Best} {Practices}},
	isbn = {979-8-3503-2996-4},
	url = {https://doi.org/10.1109/ASE56229.2023.00019},
	doi = {10.1109/ASE56229.2023.00019},
	abstract = {Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions, and basic control and data flow are met.},
	booktitle = {Proceedings of the 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi, Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Large language model, Ontology, LLM, Natural language processing, Language model, Semantics, artificial intelligence, Deep learning, ontology, Prompt Engineering, Deep Learning, deep learning, Artificial Intelligence, Prompt engineering, prompt engineering, Visualization, Language processing, Best practices, Natural languages, Codes, Costs, Task analysis, Time to market, Ontology's, Natural language processing systems, Codes (symbols), Economic and social effects, Trade off, Multi objective},
	pages = {1846--1848},
	annote = {Cited by: 5},
}

@inproceedings{chondamrongkul_semantic-based_2020,
	address = {New York, NY, USA},
	series = {{FormaliSE} '20},
	title = {Semantic-based {Architecture} {Smell} {Analysis}},
	isbn = {978-1-4503-7071-4},
	url = {https://doi.org/10.1145/3372020.3391564},
	doi = {10.1145/3372020.3391564},
	abstract = {Software smells have negative impacts on the reliability and modifiability of software systems. The smells in architecture design can be cascaded down to the implementation level and cause issues that require much effort to fix. Therefore, early detection of the architecture smells can benefit the overall quality of the software system. This paper presents an integration of methods that formally define the software architecture design towards architecture smell detection. Our approach serves as a framework that allows the architectural structures and behaviours to be formally analysed based on a coherent technique. We evaluated the accuracy and performance of our approach with the models generated from open source projects. The results show that our approach is effective and functions well.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Formal} {Methods} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chondamrongkul, Nacha and Sun, Jing and Warren, Ian and Lee, Scott Uk-Jin},
	year = {2020},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {Ontologies, OWL, Cognition, Ontology Web Language, Software architecture, Model checking, Software systems, Architecture Smells, Model Checking, Smell Detection, Software Architecture, Computer architecture},
	pages = {109--118},
}

@article{sun_minimising_2022,
	title = {Minimising {Biasing} {Word} {Errors} for {Contextual} {ASR} {With} the {Tree}-{Constrained} {Pointer} {Generator}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2022.3224286},
	doi = {10.1109/TASLP.2022.3224286},
	abstract = {Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40\% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
	month = nov,
	year = {2022},
	note = {Publisher: IEEE Press},
	keywords = {Language model, Decoding, Context modeling, Contextual knowledge, Training, Computational linguistics, Zero-shot learning, Error analysis, Speech recognition, end-to-end, Generators, Speech processing, Contextual speech recognition, Hidden Markov models, language model discounting, minimum Bayes' risk, Pointer generator, End to end, Errors, Recognition error, Language model discounting, Long tail, Minimum bayes risk, Speech communication, Word error rate reductions},
	pages = {345--354},
	annote = {Cited by: 11; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{powell_measuring_2024,
	address = {Santa Fe, New Mexico, USA},
	series = {{JCDL} '23},
	title = {Measuring the {Growth} of {Ideas} in a {Title} {Corpus}},
	isbn = {979-8-3503-9931-8},
	url = {https://doi.org/10.1109/JCDL57899.2023.00063},
	doi = {10.1109/JCDL57899.2023.00063},
	abstract = {Beyond bibliometrics, there is interest in characterizing the evolution of the number of ideas in scientific papers, or more generally, exploring the progress of science. We use various tokenization and phrase extraction strategies combined with lexical diversity metrics to analyze titles in our corpus. We compared four lexical diversity metrics for each corpora variants, to look for indications that new concepts might be emerging over time.},
	booktitle = {Proceedings of the 2023 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Powell, James and Balakireva, Lyudmila},
	year = {2024},
	note = {ISSN: 2575-8152},
	keywords = {Ontologies, Semantics, natural language processing, word embeddings, Measurement, Tokenization, Ecology, lexical diversity, science of science, Ecosystems, Libraries},
	pages = {291--292},
}

@inproceedings{sierra-munera_efficient_2024,
	address = {Santa Fe, New Mexico, USA},
	series = {{JCDL} '23},
	title = {Efficient {Ultrafine} {Typing} of {Named} {Entities}},
	isbn = {979-8-3503-9931-8},
	url = {https://doi.org/10.1109/JCDL57899.2023.00038},
	doi = {10.1109/JCDL57899.2023.00038},
	abstract = {Ultrafine named entity typing (UFET) refers to the assignment of predefined labels to entity mentions in a given context. In contrast to traditional named entity typing, the number of potential labels is in the thousands and one mention can have more than one assigned type. Previous approaches either depend on large training datasets, or require inefficient encoding of all input-type combinations. Therefore, there is a need for investigating the efficiency during training and prediction of entity typing models in the ultrafine-grained setting, considering its distinctively bigger search space, compared to the coarse- and fine-grained tasks. To efficiently solve UFET, we propose Decent, a lightweight model that encodes, using a pretrained language model, the input sentences separately from the type labels. Additionally, we make use of negative oversampling to speed up the training while improving the generalization of unseen types. Using an openly available UFET dataset, we evaluated the classification and runtime performance of Decent and observed that training and prediction runtime is orders of magnitude faster than the current state-of-the-art approaches, while maintaining a competitive classification performance.},
	booktitle = {Proceedings of the 2023 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Sierra-Múnera, Alejandro and Westphal, Jan and Krestel, Ralf},
	year = {2024},
	note = {ISSN: 2575-8152},
	keywords = {Semantics, Vocabulary, named entity recognition, Training, Computational modeling, ultrafine enity typing, Limiting, Predictive models, Runtime},
	pages = {205--214},
}

@inproceedings{li_lisum_2024,
	address = {Echternach, Luxembourg},
	series = {{ASE} '23},
	title = {{LiSum}: {Open} {Source} {Software} {License} {Summarization} with {Multi}-{Task} {Learning}},
	isbn = {979-8-3503-2996-4},
	url = {https://doi.org/10.1109/ASE56229.2023.00150},
	doi = {10.1109/ASE56229.2023.00150},
	abstract = {Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13\% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.},
	booktitle = {Proceedings of the 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai, Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
	year = {2024},
	note = {ISSN: 2643-1572},
	keywords = {Formal languages, Training, Surveys, Measurement, multi-task learning, license comprehension, open source software licenses, Learning systems, License comprehension, Licenses, Multi-Task Learning, Multitasking, Open Source Software Licenses},
	pages = {787--799},
}

@article{garimella_building_2025,
	title = {Building {Multimodal} {Knowledge} {Graphs}: {Automation} for {Enterprise} {Integration}},
	volume = {29},
	issn = {1941-0131},
	doi = {10.1109/MIC.2025.3588546},
	abstract = {As enterprises increasingly aim to incorporate artificial intelligence into their workflows to tackle complex, multimodal tasks, the demand for intelligent, robust, and trustworthy systems is paramount. While multimodal large language models offer initial capabilities for processing diverse data streams, their dependence on embedding-based representations limit their effectiveness in delivering semantically grounded explanations and reasoning, as well as qualities essential for enterprise-grade applications. Neurosymbolic approaches provide a promising alternative by enabling traceable, context-aware decision making. However, constructing enterprise-level multimodal knowledge graphs (MMKGs) that enable neurosymbolic approaches remains largely impractical. Although prior efforts have explored MMKG construction, they fall short in addressing the scalability, modularity, and integration that are necessary for any enterprise-grade application. We present a fully automated MMKG construction framework tailored to real-world enterprise environments. Our system features a modular, self-refining lifecycle with a support for human-in-the-loop feedback, enabling scalable, cost-effective, and task-aligned MMKG generation. We demonstrate the practical value of our framework through a real-world case study, showcasing its ability to transform unstructured multimodal data into actionable, semantically grounded knowledge assets for enterprise use.},
	number = {3},
	journal = {IEEE Internet Computing},
	author = {Garimella, Ritvik and Yip, Hong Yung and Venkataramanan, Revathy and Sheth, Amit P.},
	month = may,
	year = {2025},
	keywords = {Knowledge graphs, Artificial intelligence, Large language models, Semantics, Decision making, Cognition, Scalability, Internet, Human in the loop, Iterative methods, Multimodal sensors, Transforms},
	pages = {76--84},
}

@inproceedings{duric_framework_2025,
	title = {A {Framework} for {Ontology}-{Driven} {Multiagent} {System} {Generation}},
	doi = {10.1109/MIPRO65660.2025.11131908},
	abstract = {This paper presents a novel framework for developing multiagent systems using ontology-based modelling consisting of an ontology and an executable module to translate ontology to implementation templates. It systematically maps concepts and relationships to agent roles, behaviours, and interactions by capturing domain knowledge in an ontology. Furthermore, it automatically generates implementation templates for the modelled multiagent system. This way, the need for extensive manual coding may be somewhat reduced, ensuring that conceptual integrity is maintained throughout development. A case study showcases how the proposed framework may speed up the design cycle, reduce coding errors, and enhance system extensibility. Using ontology-driven generation ensures consistency and interoperability, as each agent artefact follows the established domain semantics. This approach may be especially beneficial in complex, knowledge-intensive environments that require collaborative and distributed solutions. Ultimately, it may lead to a flexible and scalable process from domain modelling to multiagent implementation. Our framework provides a way that might help bridge the gap between conceptual design and agent-based execution, streamlining multiagent system development and ensuring robust alignment with domainspecific ontologies.},
	booktitle = {2025 {MIPRO} 48th {ICT} and {Electronics} {Convention}},
	author = {Đurić, B. Okreša and Carrascosa, C.},
	month = jun,
	year = {2025},
	note = {ISSN: 1847-3938},
	keywords = {Ontologies, Interoperability, Artificial intelligence, Semantics, artificial intelligence, knowledge representation, Multi-agent systems, Software, ontology engineering, Collaboration, Encoding, Extensibility, Manuals, multiagent system, software frameworks},
	pages = {198--203},
}

@inproceedings{paik_integrating_2025,
	title = {Integrating {Ontology} {Rules} with {Large} {Language} {Models} for {Enhanced} {Reasoning}},
	doi = {10.1109/ITC-CSCC66376.2025.11137761},
	abstract = {Ontology-based reasoning enables structured inference and logical consistency in specialized domains. However, traditional ontology reasoning frameworks rely heavily on predefined inference engines and XML-based structured data, limiting their integration with Large Language Models (LLMs). Despite their strong general reasoning capabilities, LLMs struggle with structured ontological knowledge due to their reliance on unstructured text.This study proposes a novel approach to integrating ontology-based reasoning into LLMs by transforming ontological concepts and rules into structured natural language. By systematically defining ontology components—such as classes, properties, and rules—using a natural language-friendly format, we enable LLMs to process and infer knowledge from domain-specific ontologies without dedicated inference engines. The feasibility of this transformation is demonstrated through experiments on a domain-specific cocktail ontology. Our findings highlight the potential of natural language-driven ontology representation in bridging the gap between rule-based inference and flexible LLM reasoning.},
	booktitle = {2025 {International} {Technical} {Conference} on {Circuits}/{Systems}, {Computers}, and {Communications} ({ITC}-{CSCC})},
	author = {Paik, Incheon},
	month = jul,
	year = {2025},
	note = {ISSN: 2997-741X},
	keywords = {Ontologies, Ontology, Large language models, Large Language Model, Cognition, RAG, Optimization, Scalability, Computer architecture, Computers, Engines, Limiting, Natural languages, Ontology in Natural Language, Ontology Reasoning},
	pages = {1--8},
}

@inproceedings{saucedo_estimating_2025,
	title = {Estimating {Commonsense} {Scene} {Composition} on {Belief} {Scene} {Graphs}},
	doi = {10.1109/ICRA55743.2025.11127920},
	abstract = {This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types. For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/f0tqtPVFZ2A},
	booktitle = {2025 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Saucedo, Mario A.V. and Viswanathan, Vignesh Kottayam and Kanellakis, Christoforos and Nikolakopoulos, George},
	month = may,
	year = {2025},
	keywords = {Ontologies, Large language models, Semantics, Graph convolutional networks, Distribution functions, Graphical models, Indoor environment, Probability distribution, Robotics and automation, Videos},
	pages = {2861--2867},
}

@inproceedings{han_reasoning_2025,
	title = {A {Reasoning} {Method} for {Micro} {Assembly} {Sequence} {Based} on {Knowledge} {Graph} and {Large} {Language} {Model}},
	doi = {10.1109/RAIIE65740.2025.11139975},
	abstract = {For the assembly of multi-species laser inertial confinement fusion microtargets, micro-assembly robots cannot adaptively respond to changes in the structure of the microtargets and generate assembly sequences autonomously, and manual intervention is needed to reset the assembly process, so there are the problems of autonomous configuration capability and low rapid response capability of the robot system for the assembly of new microtargets and the problem of low response capability, which can not satisfy the needs of small batch and multi-species assembly and rapid response. For this reason, this paper introduces Large-scale Language Model (LLM) and Knowledge Graph (KG) in the assembly sequence generation of micro-assembly robots, and puts forward the method of "Thinking with Knowledge Graph" (TwKG). By integrating external knowledge such as manual assembly experience, a priori information of parts, and robot information, the method enables the micro-assembly robot to generate assembly sequences autonomously, optimize the sequences based on assembly experience, and update the sequences autonomously in case of environmental changes. Since the sequences directly generated by this method cannot be directly used for device control, a Reactive Behavioral Trees method (RBT) based on dynamic behavioral trees is designed based on the analysis of rationality relationships among assembly sequences for automatically creating and updating assembly sequences to cope with abnormal situations. Finally, the effectiveness of the proposed method in improving assembly efficiency and sequence rationality is verified by comprehensive tests in the equipment platform.},
	booktitle = {2025 4th {International} {Symposium} on {Robotics}, {Artificial} {Intelligence} and {Information} {Engineering} ({RAIIE})},
	author = {Han, Qichang and Zhang, Juan and Bi, Lie and Lu, Xi},
	month = jun,
	year = {2025},
	keywords = {Knowledge graphs, Large language models, Knowledge engineering, knowledge graph, Cognition, information fusion, Training, large language modeling, Visualization, Assembly, Planning, Manuals, assembly sequence planning, micro-target, Robots},
	pages = {216--224},
}

@inproceedings{sridharan_toward_2025,
	title = {Toward a {Hybrid} {Ontology} {Framework} for {Semantic} {Data} {Understanding} in {AI}-{Augmented} {Data} {Warehousing}},
	doi = {10.1109/BigDataService65758.2025.00033},
	abstract = {The growing complexity of hybrid data infrastructures, combining structured, semi-structured, and unstructured sources, poses significant challenges for achieving semantic data understanding in enterprise environments. While Large Language Models (LLMs) offer promising capabilities for data exploration, their effectiveness is limited without a unified semantic layer that bridges business meaning and technical metadata. We present a hybrid ontology framework that integrates deterministic schema metadata (such as tables, keys, and ER diagrams) with flexible semantic enrichment derived from documentation, ETL scripts, and business glossaries. This framework enables the construction of a layered knowledge graph aligned with metadata catalogs like OpenMetadata and DataHub, supporting both metadata ingestion and semantic feedback loops. Although named an ontology, our prototype is a Neo4j like property graph; conversion to RDF/OWL is deferred because SHACL (Shapes Constraint Language) normalization is nontrivial, our graph uses rich property types, while OWL/RDF requires explicit classes/relations. Our implementation demonstrates improvements in entity disambiguation, catalog enrichment, and cross-system understanding.},
	booktitle = {2025 {IEEE} 11th {International} {Conference} on {Big} {Data} {Computing} {Service} and {Machine} {Learning} {Applications} ({BigDataService})},
	author = {Sridharan, Hayagriv and Sarkar, Saurabh and Paul, Parag and Cheng, Eugene},
	month = jul,
	year = {2025},
	note = {ISSN: 2690-828X},
	keywords = {Ontologies, Knowledge graphs, Ontology, Semantics, Machine learning, Big Data, Knowledge Graph, Metadata, Business, Data Warehouse, Hybrid Infrastructure, Metadata Catalog, Prototypes, Semantic Data Understanding, Semantic Retrieval Techniques, Shape, Warehousing},
	pages = {174--175},
}

@inproceedings{ospan_llm_2025,
	title = {{LLM} {Agents} for {Enhanced} {Tabular} {Data} {Interpretation}: {A} {Perspective}},
	doi = {10.1109/SIST61657.2025.11139242},
	abstract = {The task of interpreting tabular data semantically is central to domains ranging from biomedical research to finance, where structured tables must be linked to rich domain knowledge. Ontology-driven methods, such as the iterative approaches could provide a structured, interpretable framework for entity recognition and semantic enrichment. However, these methods often struggle with handling ambiguous terms, evolving taxonomies, and rapid domain shifts. Simultaneously, recent advances in Large Language Models (LLMs) have demonstrated remarkable flexibility and context-aware reasoning capabilities, making them attractive complementary tools. In this perspective paper, we review the ontology-driven semantic analysis landscape, highlight its limitations, and discuss how LLM-based agents can address these challenges. We then outline a conceptual framework for integrating LLM reasoning with ontology-driven pipelines, enabling dynamic ontology extension, robust entity disambiguation, and scalable cross-domain adaptation. By bridging symbolic knowledge structures with the adaptive intelligence of LLMs, we propose a pathway to more flexible, accurate, and future-proof semantic interpretation of tabular data.},
	booktitle = {2025 {IEEE} 5th {International} {Conference} on {Smart} {Information} {Systems} and {Technologies} ({SIST})},
	author = {Ospan, Assel and Mussa, Aman and Mansurova, Madina and Sarsembayeva, Talshyn},
	month = may,
	year = {2025},
	keywords = {large language model, Ontologies, Terminology, Large language models, Semantics, OWL ontology, deep learning, Taxonomy, Cognition, Accuracy, semantic analysis, table interpretation, Stability analysis, Thermal stability, Tuning},
	pages = {1--6},
}

@inproceedings{yu_ontology-based_2025,
	title = {Ontology-based {Adaptive} {Knowledge} {System} ({OAKS}): {Adaptive} and {Consistent} {Knowledge} {Acquisition} through {LLMs} for {Diverse} {User} {Backgrounds}},
	doi = {10.1109/COMPSAC65507.2025.00081},
	abstract = {Although most of the research on large language models (LLMs) focuses on their development and validation against datasets, significant gaps remain in their application to real-world knowledge-intensive tasks. This research addresses key challenges in using LLMs for extracting and synthesizing knowledge from unstructured sources, focusing on applications where the validity and consistency of the results are critical. We propose Ontology-based Adaptive Knowledge System (OAKS) as a holistic approach to manage the complexities of acquiring unstructured knowledge, varying user expertise, and dynamic query formulation. This research provides practical value for enabling the domain user community to leverage their technical documentation and expertise and accelerate ongoing working projects through improved literature review and cross-disciplinary insight discovery. Validated through empirical studies, our findings offer insight into best practices for the deployment of OAKS, bridging the gaps between AI capabilities and real-world needs in knowledge acquisition and research.},
	booktitle = {2025 {IEEE} 49th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Yu, Muran and Wang, Jie and Chen, Yirong and Lepech, Micheal D. and Liu, Ying and Law, Kincho H.},
	month = jul,
	year = {2025},
	note = {ISSN: 2836-3795},
	keywords = {Knowledge acquisition, Ontologies, Semantics, Systematic literature review, Problem-solving, knowledge acquisition, Reliability, Software, Knowledge based systems, Adaptive systems, Human-LLM interaction, nonmonotonic reasoning, ontology-based dynamic query, Real-time systems},
	pages = {593--599},
}

@inproceedings{chowdhury_enhanced_2025,
	title = {Enhanced tracking and reporting of missing persons using {Knowledge} {Graph} and {Ontology} {Engineering}},
	doi = {10.1109/COMPSAC65507.2025.00084},
	abstract = {The issue of missing persons remains a significant concern in the United States, with thousands of cases reported annually. Addressing this challenge requires innovative methods to integrate and analyze disparate data sources to uncover patterns in disappearances. This research explores the potential of Ontology Engineering and Knowledge Graphs to provide a structured and interconnected perspective on missing person data. Ontologies enable the representation and linking of real-world entities within a unified framework, facilitating more meaningful data relationships.In this study, a knowledge graph is constructed to capture key details about missing persons, including the circumstances of their disappearance, personal characteristics, and last known locations. Additionally, a Large Language Model (LLM) is integrated to summarize query results, enhancing data interpretation. This knowledge graph serves as a foundation for advanced data analysis, enabling real-time insights, supporting proactive interventions, and aiding in case resolution.},
	booktitle = {2025 {IEEE} 49th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Chowdhury, Suparno Roy and Desai, Ayush and Kapure, Mrunal and Shikalgar, Mustakim and Venugopal, Ramchander and Bansal, Srividya},
	month = jul,
	year = {2025},
	note = {ISSN: 2836-3795},
	keywords = {Ontologies, Knowledge graphs, LLM, Large language models, Semantic web, Semantic Web, Knowledge Graphs, Data Integration, Data integration, Data analysis, Open data, Databases, Soft sensors, Real-time systems, Missing people, NamUS, Risk factors},
	pages = {619--627},
}

@inproceedings{busany_automating_2025,
	title = {Automating {Business} {Intelligence} {Requirements} with {Generative} {AI} and {Semantic} {Search}},
	doi = {10.1109/COMPSAC65507.2025.00260},
	abstract = {Eliciting Business Intelligence (BI) requirements is challenging, especially in dynamic business environments. This paper introduces AutoBIR, an AI-driven system that uses semantic search and Large Language Models (LLMs) to automate BI specification and prototyping. Through a conversational interface, it translates user inputs into analytic code, descriptions, and data dependencies while generating test-case reports with optional visuals. AutoBIR refines BI reporting via feedback, accelerating data-driven decision-making. We also explore the broader potential of generative AI in transforming BI development, illustrating its role in enhancing data engineering practice for large-scale, evolving systems.},
	booktitle = {2025 {IEEE} 49th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Busany, Nimrod and Hadar, Ethan and Hadad, Hananel and Rosenblum, Gil and Maszlanka, Zofia and Akhigbe, Okhaide and Amyot, Daniel},
	month = jul,
	year = {2025},
	note = {ISSN: 2836-3795},
	keywords = {Large Language Models, Large language models, Generative AI, Requirements, Semantic search, Business intelligence, Software, Decision making, Data engineering, Translation, Visualization, Semantic Search, AI-Driven Data Engineering, Business Intelligence, Codes, Ontology-Based Query Generation, Prototyping, Text-to-SQL},
	pages = {1891--1898},
}

@inproceedings{de_villiers_evaluation_2025,
	title = {Evaluation of {LLM} {Reasoning} {Under} {Uncertainty}: {An} {Atomic} {Comparison} to {Normative} {Approaches}},
	doi = {10.23919/FUSION65864.2025.11123952},
	abstract = {Evaluating uncertainty in large language model (LLM) reasoning is challenging due to their vast parameter space, abstract knowledge representation, and limited transparency regarding training data. While normative formalisms, such as deductive logic, clearly define sound reasoning in the absence of uncertainty, reasoning under uncertainty admits multiple approaches, including probabilistic reasoning (e.g. Bayesian), belief function reasoning (e.g. Dempster-Shafer), or fuzzy logic, to name a few. This paper examines how LLMs handle uncertainty by analyzing outcomes based on an atomic fusion and reasoning problem. We establish a point of reference using the simplest of fusion topologies to facilitate transparency and understanding of how LLMs align with established theories. The reasoning approaches of different LLMs with varying complexities are compared to established normative frameworks, providing insights into which formalism best aligns with LLM reasoning and assessing its soundness and consistency. A deviation function for assessment is developed, and the results indicate that the tested LLMs' reasoning under uncertainty does not consistently align with established theories, even for the simplest information fusion topologies. These preliminary results form the basis for further investigations and LLM refinements.},
	booktitle = {2025 28th {International} {Conference} on {Information} {Fusion} ({FUSION})},
	author = {de Villiers, J.P. and de Freitas, A. and Jousselme, A-L. and Kaplan, L. and Blasch, E. and Laudy, C. and Costa, P. C.},
	month = jul,
	year = {2025},
	keywords = {Ontologies, Large language models, Uncertainty, Cognition, Standards, evaluation criteria, evaluation, Measurement, Topology, Probabilistic logic, reasoning models, reasoning under uncertainty, Reliability theory, Training data, URREF},
	pages = {1--8},
}

@inproceedings{cheng_ctinexus_2025,
	title = {{CTINexus}: {Automatic} {Cyber} {Threat} {Intelligence} {Knowledge} {Graph} {Construction} {Using} {Large} {Language} {Models}},
	doi = {10.1109/EuroSP63326.2025.00057},
	abstract = {Textual descriptions in cyber threat intelligence (CTI) reports, such as security articles and news, are rich sources of knowledge about cyber threats, crucial for organizations to stay informed about the rapidly evolving threat landscape. However, current CTI knowledge extraction methods lack flexibility and generalizability, often resulting in inaccurate and incomplete knowledge extraction. Syntax parsing relies on fixed rules and dictionaries, while model fine-tuning requires large annotated datasets, making both paradigms challenging to adapt to new threats and ontologies. To bridge the gap, we propose CTINexus, a novel framework leveraging optimized in-context learning (ICL) of large language models (LLMs) for data-efficient CTI knowledge extraction and high-quality cybersecurity knowledge graph (CSKG) construction. Unlike existing methods, CTINexus requires neither extensive data nor parameter tuning and can adapt to various ontologies with minimal annotated examples. This is achieved through: (1) a carefully designed automatic prompt construction strategy with optimal demonstration retrieval for extracting a wide range of cybersecurity entities and relations; (2) a hierarchical entity alignment technique that canonicalizes the extracted knowledge and removes redundancy; (3) an long-distance relation prediction technique to further complete the CSKG with missing links. Our extensive evaluations using 150 real-world CTI reports collected from 10 platforms demonstrate that CTINexus significantly outperforms existing methods in constructing accurate and complete CSKG, highlighting its potential to transform CTI analysis with an efficient and adaptable solution for the dynamic threat landscape.},
	booktitle = {2025 {IEEE} 10th {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	author = {Cheng, Yutong and Bajaber, Osama and Tsegai, Saimon Amanuel and Song, Dawn and Gao, Peng},
	month = jun,
	year = {2025},
	note = {ISSN: 2995-1356},
	keywords = {Ontologies, Knowledge graphs, Large language models, Large Language Model, Cyber threat intelligence, Redundancy, Organizations, Cybersecurity Knowledge Graph, Cyber Threat Intelligence, Transforms, Tuning, Computer security, In-Context Learning, Syntactics},
	pages = {923--938},
}

@inproceedings{alshomar_teaching_2025,
	title = {Teaching {LLMs} {Non}-{Functional} {Requirements} {Modeling}: {A} {Grammar} and {RAG} {Approach}},
	doi = {10.1109/SSE67621.2025.00016},
	abstract = {A picture is worth a thousand words. Non-Functional Requirements (NFRs), such as security and usability, are modeled using Softgoal Interdependency Graphs (SIGs) to capture potential conflicts and synergies. However, the practice of NFR modeling remains limited, partly due to unfamiliarity with modeling languages like SIG and insufficient understanding of relevant NFRs. Large Language Models (LLMs) show some knowledge of NFRs and SIG concepts, such as goal decomposition and operationalization, but often lack precise knowledge of formal SIG syntax. We introduce SIG-GPT, a GPT-4-based LLM augmented with SIG knowledge using text-based grammar supplied and Retrieval Augmented Generation (RAG). RAG enhancs LLM responses by retrieving relevant external knowledge, while the grammar enforces correct syntax, guiding the LLM to generate SIGs align with formal notation. To help practitioners better understand SIG modeling, reduce time and effort, and enhance NFR proficiency, we apply textual grammar to SIG-GPT, ensuring it is ready for seamless integration with visual modeling tools like RE- Tool, enabling the LLM to generate correct SIG structures without requiring a large dataset of SIG examples. Results show that SIG-GPT with grammar and RAG achieves 100\% syntactic accuracy, 95\% semantic accuracy, and 98\% cohesion (CCR) while aligning with Bloom's Taxonomy to enhance structured reasoning in SIG modeling.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Software} {Services} {Engineering} ({SSE})},
	author = {AlShomar, Ahmad and Supakkul, Sam and Bao Pham, To Kim and Hill, Tom and Chung, Lawrence},
	month = jul,
	year = {2025},
	keywords = {Large language models, Semantics, Retrieval augmented generation, Large Language Models (LLMs), Taxonomy, Software, Usability, Visualization, Accuracy, Syntactics, Goal Modeling and Goal-oriented Requirement Language, Grammar, Non-Functional Requirements (NFRs), Retrieval Augmented Generation (RAG), Softgoal Interdependency Graph (SIG), Visual Modeling Languages},
	pages = {57--66},
}

@inproceedings{goel_multilingual_2025,
	title = {A {Multilingual} {Ontology} and {Knowledge} {Graph} for {Recipes}},
	doi = {10.1109/ICDEW67478.2025.00040},
	abstract = {In recent years, the fusion of artificial intelligence and semantic web technologies has paved the way for innovative approaches to managing and utilizing information. With the growing demand for structured gastronomical data, there is a need for well-defined ontologies that facilitate recipe organization, ingredient classification, nutritional insights, and personalized diet recommendations. This article presents a multilingual recipe ontology and knowledge graph, capturing critical relationships between ingredients, nutrition, cooking actions, and recipe planning. Our ontology supports interoperability across different languages (English, Hindi, and Japanese), facilitating AI applications such as ingredient substitution and personalized recommendations. The proposed knowledge graph leverages semantic web technologies to enhance structured data accessibility and machine-readable representations, ultimately contributing to computational gastronomy and food informatics.},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} {Workshops} ({ICDEW})},
	author = {Goel, Mansi and Andres, Frederic},
	month = may,
	year = {2025},
	note = {ISSN: 2473-3490},
	keywords = {Ontologies, Knowledge graphs, Interoperability, Semantic Web, Knowledge Graph, Cognition, Ontology Construction, Informatics, Redundancy, Planning, Real-time systems, Food Informatics, Multilingual, Multilingual Data, Recipe Ontology},
	pages = {282--288},
}

@inproceedings{sriharsha_image--label--answer_2025,
	title = {Image-to-{Label}-to-{Answer}: {A} {Simplified} {LLM} {Framework} {Medical} {Visual} {Question} {Answering}},
	doi = {10.1109/ICOCT64433.2025.11118904},
	abstract = {Medical Visual Question Answering (MVQA) has emerged as a promising approach for assisting clinicians in interpreting complex medical images by providing accurate and context-aware answers to specific visual queries. In this project we introduce an innovative framework, "Image to Label to Answer" (ILA) designed to enhance the efficiency and accuracy of clinical applications in MVQA. The proposed project will involve a setup of integrated image recognition techniques along with advanced natural language processing models that systematically convert the medical images into interpretable labels, which in turn generate the correct answers to clinical questions. The ILA framework will operate in three stages: image labelling, context generation, and answer synthesis. First, the system uses deep learning-based image segmentation and classification models to extract relevant features and assign diagnostic labels to medical images. These labels provide the basis for generating contextual understanding, which is crucial for the subsequent stage. In the context generation phase, the framework uses ontology-based reasoning in combination with language models to interpret the labelled data within the clinical context. As seen, the synthesis module can build full and accurate replies after the contextual understanding created over the input queries.},
	booktitle = {2025 {International} {Conference} on {Computing} {Technologies} ({ICOCT})},
	author = {Sriharsha, A V and Ganesh, C Sai and Teja, Gudi and Sumanth, K and Sindhuja, G and Ajay, C},
	month = jun,
	year = {2025},
	keywords = {Feature extraction, Context modeling, Image recognition, Visualization, Accuracy, Convolutional Neural Networks, Image segmentation, Context Generation, Image Segmentation, Labeling, Medical diagnostic imaging, Medical Images, Medical Visual Question Answering, Optical character recognition, Optical Character Recognition, Question answering (information retrieval)},
	pages = {1--5},
}

@article{triantafyllopoulos_computer_2025,
	title = {Computer {Audition}: {From} {Task}-{Specific} {Machine} {Learning} to {Foundation} {Models}},
	volume = {113},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2025.3593952},
	abstract = {Foundation models (FMs) are increasingly spearheading recent advances on a variety of tasks that fall under the purview of computer audition—i.e., the use of machines to understand sounds. They feature several advantages over traditional pipelines: among others, the ability to consolidate multiple tasks in a single model, the option to leverage knowledge from other modalities, and the readily available interaction with human users. Naturally, these promises have created substantial excitement in the audio community and have led to a wave of early attempts to build new, generalpurpose FMs for audio. In the present contribution, we give an overview of computational audio analysis as it transitions from traditional pipelines toward auditory FMs. Our work highlights the key operating principles that underpin those models and showcases how they can accommodate multiple tasks that the audio community previously tackled separately.},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {Triantafyllopoulos, Andreas and Tsangko, Iosif and Gebhard, Alexander and Mesaros, Annamaria and Virtanen, Tuomas and Schuller, Björn W.},
	month = apr,
	year = {2025},
	keywords = {Ontologies, Machine learning, Foundation models, Training, Computational modeling, artificial intelligence (AI), Acoustic scene classification, Acoustics, audio captioning (AC), Automobiles, computational audio analysis, computer audition, foundation models (FMs), Frequency modulation, large audio models, machine listening, Market research, sound event detection (SED), Tagging},
	pages = {317--343},
}

@inproceedings{alex_leveraging_2025,
	title = {Leveraging {Large} {Language} {Models} for {Automated} {XR} {Instructional} {Content} {Generation}},
	doi = {10.1109/ICE/ITMC65658.2025.11106622},
	abstract = {This paper presents a study in which authors examine the potential of leveraging large language models to generate instructional content for eXtended Reality environments. Considering the IEEE ARLEM standard as a framework for structuring data, it could be integrated and interpreted by existing authoring tools. In terms of methods, authors have adopted an exploratory approach in testing various strategies. A case study focusing on the use of an eXtended Reality authoring tool for teaching operating procedures is presented. Finally, this exploratory work shows that while simple prompts can produce scenarios with satisfactory quality, imposing a structured schema through more complex prompts leads to less reliable outcomes.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Engineering}, {Technology}, and {Innovation} ({ICE}/{ITMC})},
	author = {Alex, Gabriel},
	month = jun,
	year = {2025},
	note = {ISSN: 2693-8855},
	keywords = {Ontologies, Large language models, ontology, Large Language Model, Artificial Intelligence, Reliability, Standards, Technological innovation, engineering education, Authoring systems, Extended reality, Testing, Engineering education, authoring tool, extended reality, Focusing},
	pages = {1--9},
}

@inproceedings{wan_facilitating_2025,
	title = {Facilitating {Design} for {Additive} {Manufacturing} with {KG}-based {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICE/ITMC65658.2025.11106597},
	abstract = {Additive manufacturing (AM), or 3D printing, enables the production of complex geometries and highly customized components. However, design for AM (DfAM) requires specialised and comprehensive knowledge of process constraints, material behaviours, and performance parameters. While knowledge graphs (KGs) have been utilized to organize and integrate DfAM knowledge, they do not understand natural language and have limited reasoning capabilities, which make them less accessible to non-experts and ineffective in handling complex and context-dependent design queries. Large language models (LLMs) offer powerful language processing and generalizability but suffer from hallucinations when handling specialized domains. To address these limitations, this study proposes a KG-based retrieval-augmented generation (RAG) approach to develop a domain-specific question-answering (Q\&A) in DfAM. By integrating structured knowledge from a DfAM KG with the strengths of LLMs, the proposed approach improves response accuracy and relevance. Comparative experiments evaluated LLMs with non-RAG and KG-based RAG using generic and domain-specific metrics. Results demonstrated that KG-based RAG enhances information retrieval and response quality, reduces hallucinations, and ensures alignment with domain knowledge.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Engineering}, {Technology}, and {Innovation} ({ICE}/{ITMC})},
	author = {Wan, Yuwei and Liu, Ying and Zammit, Joseph Paul and Chen, Zheyuan and Li, Li and Francalanza, Emmanuel},
	month = jun,
	year = {2025},
	note = {ISSN: 2693-8855},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, Large language models, Retrieval augmented generation, Additive manufacturing, Retrieval-augmented generation, Prompt engineering, Design for additive manufacturing, Stakeholders, Technological innovation, Accuracy, Production, Pipelines, Three-dimensional printing},
	pages = {1--8},
}

@inproceedings{buschelberger_digital_2025,
	title = {Digital {Products} {Based} on {Large} {Language} {Models} for the {Exploration} of {Graph}-{Databases} in {Materials} {Science} and {Manufacturing}},
	doi = {10.1109/ICE/ITMC65658.2025.11106608},
	abstract = {Semantic technologies are gaining traction in materials science and manufacturing. Specifically, the integration of graph databases with ontologies facilitates the harmonization of typically heterogeneous materials and process data, as well as the representation of complex workflows in the field (e.g., processing experimental and simulation data or transferring and tracking data along process chains). This approach enables previously inaccessible data for scientists and engineers to be made available in a FAIR (findable, accessible, interoperable, reusable) manner. On this basis, both science and industry, anticipate a significant boost in materials and process innovation, leading to a more resilient and sustainable production. Nevertheless, one of the main challenges in making semantic technologies usable for engineers is enabling navigation and exploration of the typically complex and flexible graph-based data structures. This work presents two approaches for data exploration in graph-databases using large language models (LLMs), namely LLM-CypherGen and SPARQL-Agent, and their application in two digital products developed within the EU research project DiMAT demonstrated across different use cases in materials science and manufacturing.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Engineering}, {Technology}, and {Innovation} ({ICE}/{ITMC})},
	author = {Büschelberger, Matthias and Tsitseklis, Konstantinos and Morand, Lukas and Zafeiropoulos, Anastasios and Nahshon, Yoav and Papavassiliou, Symeon and Helm, Dirk},
	month = jun,
	year = {2025},
	note = {ISSN: 2693-8855},
	keywords = {Ontologies, Knowledge graphs, Ontology, LLM, Large language models, AI, SPARQL, Manufacturing, Knowledge Graph, Semantic technology, Technological innovation, Production, Cypher, Syntactics, Ciphers, Material Science, Materials science and technology},
	pages = {1--9},
}

@inproceedings{li_veu-bench_2025,
	title = {{VEU}-{Bench}: {Towards} {Comprehensive} {Understanding} of {Video} {Editing}},
	doi = {10.1109/CVPR52734.2025.01276},
	abstract = {Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars1, a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3\% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3\% across nine reasoning tasks. The code and data are available at project page},
	booktitle = {2025 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Li, Bozheng and Wu, Yongliang and Lu, Yi and Yu, Jiashuo and Tang, Licheng and Cao, Jiawang and Zhu, Wenqing and Sun, Yuyang and Wu, Jay and Zhu, Wenbo},
	month = jun,
	year = {2025},
	note = {ISSN: 2575-7075},
	keywords = {Large language models, Annotations, Cognition, Knowledge based systems, Face recognition, Soft sensors, video understanding, Videos, Pipelines, Benchmark testing, Performance gain, video benchmark, video editing, videollm},
	pages = {13671--13680},
}

@inproceedings{teslya_using_2025,
	title = {Using {NLP} {Tools} for {Linking} {Materials} {Within} the “{Pushkin} {Digital}” {Resource}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014166911&doi=10.1109%2FEDM65517.2025.11096639&partnerID=40&md5=f3937f289389215b65b7cfc22e96a806},
	doi = {10.1109/EDM65517.2025.11096639},
	abstract = {The development of the scientific and educational resource “Pushkin Digital” requires processing a substantial volume of documents to create an ontology of A. S. Pushkin's literary heritage. The works of A. S. Pushkin and related texts contain mentions of entities, such as historical figure names, geographical locations, dates, and references biographical sources. All these entities are the source of links in the ontology. The paper presents a description of the natural language processing techniques used in processing the materials featured on the Pushkin Digital resource in order to create links between them. The proposed system system utilizes state-of-the-art NLP techniques including BERT for robust named entity recognition, identifying and classifying key entities within the text, SBERT for enabling the system to discern relationships and connections between entities even when expressed with different wording, and LLMs for complex text analysis. Regular expressions are employed for identifying and processing structured text elements, such as dates and bibliographic references, ensuring data consistency and accuracy. This combination of techniques allows for the automated construction of a rich and interconnected ontology, facilitating indepth exploration of Pushkin's literary heritage and its broader cultural significance.},
	booktitle = {2025 {IEEE} 26th {International} {Conference} of {Young} {Professionals} in {Electron} {Devices} and {Materials} ({EDM})},
	author = {Teslya, Nikolay},
	month = jun,
	year = {2025},
	note = {ISSN: 2325-419X},
	keywords = {Ontologies, Information extraction, Ontology, Natural language processing, Named entity recognition, Large language models, Semantics, Text mining, natural language processing, text mining, ontology, Information retrieval, Information systems, Training, Digital resources, Text analysis, information extraction, Language processing, Text-mining, NLP tools, Natural languages, Adaptation models, Cultural differences, document analysis, Ontology's, Natural language processing systems, Arts computing, Bibliographic retrieval systems, Documents analysis, Scientific resources},
	pages = {1540--1543},
	annote = {Cited by: 0},
}

@inproceedings{kotenko_detecting_2025,
	title = {Detecting and {Analysing} {Cyber} {Attacks} {Based} on {Graph} {Neural} {Networks}, {Ontologies} and {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014152166&doi=10.1109%2FEDM65517.2025.11096818&partnerID=40&md5=224d4609693b5bb1d5524bde9e9d6946},
	doi = {10.1109/EDM65517.2025.11096818},
	abstract = {This paper presents an intelligent system to automate the process of detecting and analysing cyber-attacks using Suricata logs, graph neural networks (GNNs), and large language models (LLMs). The proposed approach is based on several key components: collecting and preprocessing network events from Suricata, building an ontological model of attacks using MITRE ATT\&CK, applying graph neural networks to identify relationships between events, and finally integrating a language model for dialogue interaction with the operator and generating attack hypotheses. Experimental results demonstrate high accuracy in detecting anomalous network patterns and operator friendliness and indicate the potential for further development of the system for use in high-load and distributed infrastructures.},
	booktitle = {2025 {IEEE} 26th {International} {Conference} of {Young} {Professionals} in {Electron} {Devices} and {Materials} ({EDM})},
	author = {Kotenko, Igor and Abramenko, Georgii},
	month = jun,
	year = {2025},
	note = {ISSN: 2325-419X},
	keywords = {Ontologies, Large language model, Ontology, LLM, Language model, Large language models, ontologies, anomaly detection, Neural networks, Ontological modeling, RAG, Optimization, Intelligent systems, GNN, Graph neural networks, Computational linguistics, Anomaly detection, Accuracy, Computational efficiency, Buildings, Cyberattack, Electron devices, process automation, Suricata, Learning systems, Ontology's, Distributed computer systems, Network security, Cyber-attacks, Process automation},
	pages = {1460--1464},
	annote = {Cited by: 0},
}

@inproceedings{tomasovic_semantic_2025,
	title = {Semantic {Usability} of {Digital} {Images} -{Ontology}-{Based} {Readability} {Assessment}},
	doi = {10.23919/SpliTech65624.2025.11091706},
	abstract = {This paper introduces an ontology-driven method for assessing the semantic usability of digitized images, with a focus on readability and layout readiness, emphasizing practical metrics relevant for text extraction and annotation workflows: Optical Character Recognition (OCR) performance, line segmentation quality, script and annotation compatibility. While previous work has addressed blind Image Quality Assessment (IQA) using perceptual and statistical methods, such approaches often overlook a critical dimension - the semantic usability of images. The method assesses digitized images using a combination of objective metrics (e.g., resolution, noise, compression) and semantic tests (e.g. OCR accuracy, layout analysis) with future goal of implementing metadata completeness, Linked Open Data and International Image Interoperability Framework (LOD/IIIF) compatibility in order to provide a composite usability score. The proposed model is grounded in the Image Authenticity and Quality Ontology (IAQO), a lightweight ontology designed to represent assessment outputs in a standardized, machine-readable format. A dataset of 800 public-domain manuscript images was analyzed, yielding average semantic usability scores ranging from 73.8 to 85.4 (out of 100) across repositories. Results demonstrate that high visual fidelity does not always imply high semantic usability but also the framework’s potential to improve digitizing practices. This approach supports FAIR (Findable, Accessible, Interoperable, Reusable)-compliant image assessment.},
	booktitle = {2025 10th {International} {Conference} on {Smart} and {Sustainable} {Technologies} ({SpliTech})},
	author = {Tomasović, Željka and Tomić, Marijana},
	month = jun,
	year = {2025},
	keywords = {Ontologies, Semantics, Annotations, Ontology Modeling, Usability, Digital images, Image quality, Digital Humanities, JSON-LD, Optical character recognition, Cultural differences, Cultural Heritage Repositories, Digital Image Quality Assessment, Image resolution, Layout, Semantic Usability},
	pages = {1--6},
}

@inproceedings{heppner_refining_2025,
	title = {Refining {NLP} {Semantic} {Matches} {Through} {Dialogue} with {Large} {Language} {Models}},
	doi = {10.1109/ICPS65515.2025.11087870},
	abstract = {Advancements in digitization are fostering innovation within Industrie 4.0 (I4.0) ecosystems. The 2030 vision for I4.0 prioritizes sovereignty, sustainability, and interoperability to transform value chains into dynamic networks. While many physical and syntactical interoperability challenges have been addressed, there are still open questions regarding semantic interoperability. Natural Language Processing (NLP)-based ranking by semantic similarity offers a straightforward approach for matching semantically heterogeneous data but often lacks informative rationale and usability, especially when relevant matches are buried deep in the rankings. In this paper, we explore how we can further improve the ranking to obtain more precise semantical matches by leveraging Large Language Model (LLM), in particular Retrieval-Augmented Generation (RAG). Since comprehensibility is crucial for the end user when selecting the matches, we employ the LLM to additionally explain the rankings and enable follow-up queries. In a series of experiments, we show that this not only improves the usability, but also ensures that users can navigate complex semantic matches effectively.},
	booktitle = {2025 {IEEE} 8th {International} {Conference} on {Industrial} {Cyber}-{Physical} {Systems} ({ICPS})},
	author = {Heppner, Sebastian and Miny, Torben and Kleinert, Tobias and Zholdybayev, Ayan and Ristin, Marko and van de Venn, Hans Wernher},
	month = may,
	year = {2025},
	note = {ISSN: 2769-3899},
	keywords = {Interoperability, Natural language processing, Large language models, Semantics, Retrieval augmented generation, Large Language Model, Retrieval-Augmented Generation, Usability, Databases, Transforms, Biological system modeling, Industrie 4.0, Industries, Semantic Matching},
	pages = {1--6},
}

@inproceedings{peng_large_2025,
	title = {Large {Language} {Models} and {Knowledge} {Graphs} {Synergistically} {Enhancing} {Personalized} {Learning}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013538581&doi=10.1109%2FCSTE64638.2025.11091995&partnerID=40&md5=ab6ad634404ebe7444d3a7fe809a69ad},
	doi = {10.1109/CSTE64638.2025.11091995},
	abstract = {The rapid development of artificial intelligence technology has promoted the intelligence of the education field. The collaborative work of large language models (LLMs) and knowledge graphs (KGs) can effectively develop personalized learning paths. Personalized learning in programming courses is crucial for meeting students' different needs and proficiency levels. This article proposes a method for personalized teaching of programming courses using LLMs and KGs. The LLMs is used to analyze students' learning status, provide real-time feedback, and generate customized learning materials, while the KGs provides a structured knowledge base to plan essential skills and learning progress, highly integrating the advantages of the LLMs and the logical connections of knowledge from various disciplines. The application results show that designing a knowledge system ontology through disciplinary knowledge analysis, retraining and fine-tuning the LLMs, forms the final personalized knowledge system, improves the accuracy of learning path recommendations and the relevance of generated exercises, and can be easily promoted to other disciplinary fields.},
	booktitle = {2025 7th {International} {Conference} on {Computer} {Science} and {Technologies} in {Education} ({CSTE})},
	author = {Peng, Yuying and Zhao, Siqi and Luo, Ximeng and Ning, Yongzhi},
	month = apr,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, Ontology, Language model, Large language models, Semantics, knowledge graphs, large language models, Education, Knowledge organization, Knowledge based systems, Students, Knowledge system, Accuracy, Learning paths, Engineering education, personalized learning, Real-time systems, artificial intelligence technology, Atmospheric modeling, Learning (artificial intelligence), programming courses, Programming profession, Learning systems, Collaborative Work, Curricula, Personalized learning, Teaching, Artificial intelligence technologies, Education computing, Education field, Programming course},
	pages = {1115--1119},
	annote = {Cited by: 0},
}

@inproceedings{yang_knowledge_2025,
	title = {Knowledge graph-based dual-modal collaborative {QA} framework for hydropower operation and maintenance},
	volume = {2025},
	doi = {10.1049/icp.2025.2348},
	abstract = {Maintenance of hydropower equipment is essential for ensuring a reliable supply of renewable energy. However, in practical maintenance operations, technicians rely heavily on personal experience and extensive domain manuals, which can reduce the accuracy and efficiency of decision-making processes. This paper proposes a knowledge graph-based dual-modal collaborative QA (KGD-QA) framework for hydropower operation and maintenance. Firstly, a multi-granularity segmentation strategy is used to divide domain manuals into thematically grouped text blocks and path-labeled sentence units. Secondly, a LoRA-fine-tuned large language model (LLM), guided by domain ontologies and chain-of-thought prompts, extracts entity-relation triples. Finally, a dual-modal QA mechanism is designed to integrate knowledge graph subgraph queries and text vector retrieval, enabling information acquisition in both detailed and global modes. Experimental results show that, compared with baseline model, this approach improves the accuracy of knowledge extraction while the dual-modal collaborative QA retrieval method provides more traceable and evidence-backed responses to maintenance-related queries. This framework offers a novel paradigm for advancing intelligent operation and maintenance technologies for hydropower equipment.},
	booktitle = {15th {Prognostics} and {System} {Health} {Management} {Conference} ({PHM} 2025)},
	author = {Yang, Ye and Duan, Ran and Yi, Xinyang and Ma, Qicheng and Liu, Jie and Hu, Zhongxu and Hu, Youmin},
	month = jun,
	year = {2025},
	pages = {152--157},
}

@inproceedings{he_static_2025,
	title = {Static and {Dynamic} {Embedding} {Approaches} to {Identify} {Is}-{A} {Relations} in {SNOMED} {CT}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012760706&doi=10.1109%2FICHI64645.2025.00050&partnerID=40&md5=483223ede657cc39d5d7756f354e5d48},
	doi = {10.1109/ICHI64645.2025.00050},
	abstract = {Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) is a comprehensive clinical terminology with over 350,000 unique concepts and 1 million relationships. It serves as an essential resource for both clinical practice and medical research. Approximately 40\% of these relationships are of the is-a type, which is key to classifying concepts as subtypes or subclasses within broader categories and is fundamental for decision support systems and automated reasoning in clinical environments. Identifying is-a relationships in SNOMED CT is crucial for enhancing the accuracy of patient cohort queries, which form the backbone of clinical and research applications. Our study aims to use deep learning-based neural networks to determine whether a relationship between two concepts falls under the is-a or non-is-a categories. This approach can help detect misclassified or undefined is-a concept pairs, ultimately improving the quality of SNOMED CT ontology. We construct a node-edge-node structure for concept pairs and their relationships, which we then split into two categories: is-a and non-is-a classes. In this study, we propose two classification approaches. In the first approach using embeddingBag, the data are mapped into vector representations and used as input for a fully connected neural network to learn the patterns of the is-a relationship. During model training, 80\% of the data is used for training and the remaining 20\% for testing.We achieved a precision of 0.903, recall of 0.894, and F1 Score of 0.898 in predicting the is-a relation among the associated medical concepts. The second approach involved transformer-based models like BERT(Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. [1], which dramatically advanced various natural language processing (NLP) tasks by offering deeply contextualized word embeddings. And RoBERTa(Robustly optimized BERT approach)[2], both BERT and RoBERTa models have significantly advanced the field of NLP. This work uses them as encoders connected to a classifier head to predict is-a or non-is-a categories. We fine-tune our models using a historical SNOMED CT dataset from 2017, while evaluation is carried out on new data introduced after 2023 in the 2024 release. With a recall of 0.933, precision of 0.933, F1 score of 0.932, accuracy of 0.933, and ROC of 0.972, RoBERTa outperforms the other baseline models (Support Vector Machine(SVM), K-Nearest Neighbors(KNN), Naive Bayes, and Multilayer Perceptron(MLP)) across all metrics. This work demonstrates that our implementation can effectively identify unknown is-a relations in future datasets.},
	booktitle = {2025 {IEEE} 13th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {He, Bofan and Cheng, Jerry Q. and Gu, Huanying},
	month = jun,
	year = {2025},
	note = {ISSN: 2575-2634},
	keywords = {Ontologies, Large language model, Ontology, Terminology, Natural language processing, Language model, BERT, Deep learning, Information retrieval, Deep Learning, Natural Language Processing, Large Language Model, Neural networks, Transformers, Clinical terms, Data models, Training, SNOMED-CT, Deep neural networks, Computational linguistics, Information Retrieval, Accuracy, Medical imaging, Language processing, Bidirectional encoder representation from transformer, Encoding, Natural languages, Bidirectional control, Classification of Multi Sentence, Vectors, Learning systems, Natural language processing systems, Classification (of information), Clinical research, Learning algorithms, Classification of multi sentence},
	pages = {369--378},
	annote = {Cited by: 1},
}

@inproceedings{cox_thought_2025,
	title = {Thought {Graph}: {Balancing} {Specificity} and {Uncertainty} in {LLM}-{Based} {Gene} {Set} {Annotation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012713321&doi=10.1109%2FICHI64645.2025.00060&partnerID=40&md5=ce6215766e49f3b8e469f42dc1e59e6c},
	doi = {10.1109/ICHI64645.2025.00060},
	abstract = {Accurate predictive reasoning is a cornerstone of biomedical decision-making, particularly in precision oncology, where elucidating the intricate relationships between disease-risk genes and biological processes is critical. This study presents a novel "Thought Graph" methodology, an advancement of the Tree of Thoughts framework, to systematically generate and refine biological process representations derived from gene sets while addressing the trade-off between specificity and uncertainty. Balancing these factors is essential for robust and interpretable gene set analyses, as it accounts for the complexity, variability, and overlapping functions of biological pathways. Furthermore, we introduce a quantitative metric that integrates specificity and uncertainty, thereby enhancing the rigor and transparency of the inference process. Using a subset of the Gene Ontology database, we evaluate the effectiveness of our system in generating biologically meaningful terms that accurately describe the underlying biological processes of gene sets. We compare its performance against a domain-specific tool (GSEA) and five LLM baselines across multiple metrics. Our system achieves the highest cosine similarity (64.00\%) and specificity percentile (96.40\%), highlighting its capacity to generate terms closely aligned with human annotations while maintaining a balance between specificity and accuracy. By advancing the artificial intelligence driven analyses, this work facilitates more informed decision-making in biomedical research, precision oncology, and related fields.},
	booktitle = {2025 {IEEE} 13th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {Cox, Kyle and Qu, Gang and Hsu, Chi-Yang and Xu, Jiawei and Zhou, Yingtong and Tan, Zhen and Hu, Mengzhou and Chen, Tianlong and Hu, Ziniu and Zhao, Zhongming and Ding, Ying},
	month = jun,
	year = {2025},
	note = {ISSN: 2575-2634},
	keywords = {Large language model, Language model, Artificial intelligence, Large language models, Gene Ontology, Uncertainty, Decision making, Cognition, Cancer, Measurement, Accuracy, Precision oncology, Genetics, Oncology, Genes, Biological processes, Gene set analysis, Medical reasoning, Thought graph, Gene sets, Biological process, Behavioral research, Decisions makings, Economic and social effects, Gene set analyze, Medical reasonings},
	pages = {462--470},
	annote = {Cited by: 0},
}

@inproceedings{khalov_automatic_2025,
	title = {Automatic {Mapping} of {Upper}-{Level} {Ontology} {Classes} ({DOLCE}) and {Domain}-{Specific} {Ontology} {ITSMO}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012762960&doi=10.1109%2FICAIBD64986.2025.11082040&partnerID=40&md5=03a58282e7ad85da2118c7855a3ebada},
	doi = {10.1109/ICAIBD64986.2025.11082040},
	abstract = {This paper proposes a method for extending the top-level ontology DOLCE (DOLCE-lite version, referred to as TLO) to the domain of IT services without expert involvement. The main challenge addressed is the automatic mapping of classes in conditions of a small number of objects ({\textless}100) and the absence of annotated data. A review of existing approaches is conducted, their limitations are identified, and novel mapping methods are proposed, integrating embeddings and large language models. The suggested method achieved an 82.35\% mapping accuracy when integrating DOLCE and ITSMO ontologies. As a result, the ITO-seed ontology was developed, containing linked classes from DOLCE and ITSMO, which can be utilized in further research and in building knowledge graphs for IT Service Management (ITSM) systems.},
	booktitle = {2025 8th {International} {Conference} on {Artificial} {Intelligence} and {Big} {Data} ({ICAIBD})},
	author = {Khalov, Andrey and Ataeva, Olga},
	month = may,
	year = {2025},
	note = {ISSN: 2769-3554},
	keywords = {Ontologies, Knowledge graphs, Ontology, Artificial intelligence, Large language models, GPT, BERT, Knowledge management, OWL, RDF, Resource description framework, Transformers, Prompt engineering, Information systems, prompt engineering, Reviews, mapping, clustering, Mapping, Graph neural networks, DOLCE, ITIL, owl2vec, rdf2vec, Ontology's, Birds, Clusterings, Owl2vec, Rdf2vec},
	pages = {795--802},
	annote = {Cited by: 0},
}

@inproceedings{wang_knowledge_2025,
	title = {Knowledge {Graphs} {Combined} with {ChatGPT} in the {Field} of {Acupuncture}: {A} {Case} {Study} on the {Treatment} of {Depression} 1},
	doi = {10.1109/IC4e65071.2025.11075342},
	abstract = {This study integrates knowledge graphs (KGs) and large language models (LLMs) to enhance acupuncture-based depression treatment through structured knowledge sharing. We developed a depression-specific KG using traditional acupuncture principles and modern IT, leveraging LLMs to optimize knowledge integration and global accessibility for researchers and clinicians.},
	booktitle = {2025 16th {International} {Conference} on {E}-{Education}, {E}-{Business}, {E}-{Management} and {E}-{Learning} ({IC4e})},
	author = {Wang, Xi and Shen, Tian and Chen, Xi and Zhang, Jundong},
	month = apr,
	year = {2025},
	keywords = {Knowledge graphs, Knowledge graph, Large language models, Depression, Acupuncture, Chatbots, Electronic learning},
	pages = {216--220},
}

@inproceedings{neupane_securing_2025,
	title = {Securing {Inverter}-{Based} {Resources} via {Knowledge}-{Driven} {Threat} {Modeling}, {Analysis}, and {Mitigation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012224579&doi=10.1109%2FNOMS57970.2025.11073642&partnerID=40&md5=10a92592b3f95da8348b310137c47621},
	doi = {10.1109/NOMS57970.2025.11073642},
	abstract = {Inverter-based Resources (IBRs) present unique cybersecurity challenges due to their digital control systems and connection to the electric grid. They rely on digital communications and control systems, making them vulnerable to cyberattacks. These attacks can disrupt grid operations and stability, compromise data, or cause physical damage to equipment. To address these challenges, it is essential to establish robust cybersecurity measures that meet and exceed existing industry standards. In this paper, we describe a comprehensive strategy to bolster the cybersecurity of IBRs through cutting-edge applications and technologies via a cybersecurity framework called “CIBR-Fort”, a knowledge-driven, interoperable, scalable, and manageable framework for modeling, analysis, and mitigation of cyber threats disrupting different components of IBR systems. Our knowledge-driven analysis consists of a fusion of knowledge graphs (KGs) in cybersecurity and the electric grid, achieved through link prediction leveraging Large Language Models (LLMs) and cosine similarity, attributed towards informed decision-making for threat mitigation. The evaluation results show how we can automate LLM-driven link prediction based on the fusion of two distantly separated ontologies, generating a dataset that can be used for scaling via graph learning that can be utilized for further security analyses of IBR systems. In addition, we show our knowledge-driven threat analysis can predict different attacks with 91.88\% maximum accuracy. Lastly, we show how we can achieve real-time end-to-end threat mitigation with an average of 40 ms per traffic flow.},
	booktitle = {{NOMS} 2025-2025 {IEEE} {Network} {Operations} and {Management} {Symposium}},
	author = {Neupane, Roshan Lal and Pusapati, Vamsi and Edara, Lakshmi Srinivas and Cheng, Xiyao and Neupane, Kiran and Chintapatla, Harshavardhan and Mitra, Reshmi and Korkali, Mert and Suk Na, Hyeong and Srinivas, Sharan and Calyam, Prasad},
	month = may,
	year = {2025},
	note = {ISSN: 2374-9709},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, Interoperability, Language model, Large language models, Retrieval augmented generation, large language models, Cyber security, Cybersecurity, Modeling languages, knowledge graph, Standards, cybersecurity, Forecasting, retrieval augmented generation, Threat modeling, Real-time systems, Computer security, inverter-based resources, Power system stability, Prevention and mitigation, Stability criteria, Control systems, Network security, Digital communication systems, Electric inverters, Electric machine control, Inverter-based, Inverter-based resource, Modeling analyzes, Threats analysis, Threats mitigations},
	pages = {1--9},
	annote = {Cited by: 0},
}

@inproceedings{blasch_space_2025,
	title = {Space and {Air} {Traffic} {Management} {Situation} {Awareness} with {Notices}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012195937&doi=10.1109%2FCogSIMA64436.2025.11079470&partnerID=40&md5=1baa5281582c55021deba13c960314cc},
	doi = {10.1109/CogSIMA64436.2025.11079470},
	abstract = {Air traffic management has long been associated with situation awareness, especially supporting pilots in assessment and response to challenging situations. For multi-domain air and space operations, artificial intelligence and recently large language models (LLMs) can increase semantic understanding. In this paper, we focus on LLM aerospace analysis of Notice to Airman (NOTAM) and Notice to Space Operators (NOTSO). The notices afford a communication of the situation that can be extracted as an ontology to support human operators. Results show that LLM-based clustering can facilitate cognitive situation awareness.},
	booktitle = {2025 {IEEE} {Conference} on {Cognitive} and {Computational} {Aspects} of {Situation} {Management} ({CogSIMA})},
	author = {Blasch, Erik and Insaurralde, Carlos C.},
	month = jun,
	year = {2025},
	note = {ISSN: 2379-1675},
	keywords = {Ontologies, Ontology, Language model, Artificial intelligence, Large language models, Digital twins, Semantics, Information management, Data integration, Uncertainty, Data fusion, Air traffic control, Situation awareness, Air traffic Management, Data Fusion, Orbits, Space Traffic Management, Space vehicles, Uncertainty Ontology, Ontology's, Air navigation, Air operation, Air Traffic Management, Air transportation, Digital avionics, Multi-domains, Space operations, Space traffic management, Uncertainty ontology},
	pages = {192--199},
	annote = {Cited by: 0},
}

@inproceedings{li_ontocons_2025,
	title = {{OntoCons}: {A} {Method} for {Intelligent} {Construction} of {Domain} {Ontology} {Models} for {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012094740&doi=10.1109%2FNNICE64954.2025.11063905&partnerID=40&md5=99fdc72416564aaccebf00ec8b2b1c76},
	doi = {10.1109/NNICE64954.2025.11063905},
	abstract = {With the development of large language models technology, its application in the field of education has been steadily expanding. However, due to limitations in understanding specialized literature in vertical fields, large language models face serious issues of “hallucination” and prior bias when responding to teachers' questions about specialized domains. To address this, this paper proposes an intelligent construction framework for ontology models based on knowledge tuple extraction, called OntoCons, which introduces entity extraction and relation extraction methods into the field of ontology model construction. OntoCons transforms the ontology model construction problem into a knowledge tuple extraction problem, enhancing the transparency and interpretability of ontology model construction methods, and introduces subgraph encoding strategies to improve the accuracy of relation extraction. This research provides a new method for the intelligent construction of ontology models in vertical fields, improving the understanding and response quality of large language models in specialized domains by combining machine learning and manual analysis.},
	booktitle = {2025 5th {International} {Conference} on {Neural} {Networks}, {Information} and {Communication} {Engineering} ({NNICE})},
	author = {Li, Yi and Lu, Wenxin and Xiang, Xiuzhen},
	month = jan,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {large language model, Ontologies, Large language model, Ontology, Ontology model, Language model, Relation extraction, Large language models, Machine learning, Knowledge extraction, Knowledge engineering, ontology model, Data mining, relation extraction, Extraction, knowledge extraction, entity extraction, Accuracy, Engineering education, Transforms, Encoding, Manuals, Reliability theory, Learning systems, Domain Knowledge, Entity extractions, Teaching, Intelligent constructions, Model construction, Tuples extraction, Vertical fields},
	pages = {706--712},
	annote = {Cited by: 0},
}

@inproceedings{zhou_design_2025,
	title = {Design and {Implementation} of a {Modern} {Chinese} {History} {QA} {System} {Based} on {Knowledge} {Graphs}},
	doi = {10.1109/CNIOT65435.2025.11070747},
	abstract = {To address the challenge of quickly obtaining accurate answers while exploring modern historical knowledge, this study develops an intelligent question-answering system based on knowledge graphs, focusing on modern Chinese history. The system leverages the Robustly Optimized BERT Approach (RoBERTa) model for word embedding construction and integrates an adversarial training mechanism to enhance robustness. BiLSTM is employed to capture the contextual features of input text, while Conditional Random Fields (CRF) are used to generate the optimal prediction sequence. Experimental results demonstrate that the model achieves an accuracy of 92.2\%, a recall of 94.2\%, and an F1-score of 93.0\% on the modern history dataset, significantly outperforming other approaches.},
	booktitle = {2025 6th {International} {Conference} on {Computing}, {Networks} and {Internet} of {Things} ({CNIOT})},
	author = {Zhou, Liyun},
	month = may,
	year = {2025},
	keywords = {Ontologies, Knowledge graphs, knowledge graph, domain ontology, Training, Robustness, Accuracy, History, Conditional random fields, Text recognition, Question answering (information retrieval), Market research, modern Chinese history, question answering system},
	pages = {1--5},
}

@inproceedings{pokkuluri_unified_2025,
	title = {A {Unified} {Knowledge} {Base} for {Drug} {Label} {Analysis} {Using} {Learning} {Models}, {NLP}, and {IoT} {Tasks}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012096608&doi=10.1109%2FOTCON65728.2025.11070706&partnerID=40&md5=d6df086f35ad5793a3bfb981ad843c0f},
	doi = {10.1109/OTCON65728.2025.11070706},
	abstract = {Linguistic transmission advanced humanity. Voice, text, and pictures affect knowledge transmission. Techniques for interpolating data may retain knowledge by determining semantic equivalents based on its context, the lesson, and connections without sacrificing data integrity. Knowledge has affected the security and privacy of data, showing vulnerability on every side of privacy and security violations with numerous legislation and standards. linguistic models convert voice impulses to digital data to examine phonological, prosodic, phonotactic, and lexical features. In contrast, embedding text in pictures and voice patterns modify word meaning via resolution, simplicity, and pitch adjustment. NLP uses vectors to detect word similarity, ensuring anonymity for ontology-based understanding with compatibility while deciphering control features. Using text-based semantic similarities analysis, visual text is classed by arrangement and topic. Every individual's linguistic dialect is distinct, with a tendency toward the local dialect. Due of rapid modifications to the environment, methods' acoustic qualities are difficult to transmit. Voice translating syllables might or might not communicate word feeling according to phonetical modification. A learnt variables taxonomy with prosody properties helps knowledge bases find relevant material without intermediary stages. For successful information transmission, integration, and edge manufacturing, edge learning requires context. An ontology of learnt connections from text, speech, and pictures shows information resemblance usefulness, and interest. The suggested XTI-CNN technique excels in all four metrics and has 93.2\% accuracy, making it ideal for hybrid analysis of data in related to drugs data extraction applications.},
	booktitle = {2025 4th {OPJU} {International} {Technology} {Conference} ({OTCON}) on {Smart} {Computing} for {Innovation} and {Advancement} in {Industry} 5.0},
	author = {Pokkuluri, Kiran Sree and jain, Kirti and Rajya Lakshmi, V Sravani and Pastariya, Rishab and Lathigara, Amit and Navanitha, Dubbaka},
	month = apr,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Interoperability, Ontology, Language model, Semantics, Language models, Knowledge management, Security, Data mining, Semantic Similarity, Knowledge based systems, Semantic similarity, Semantic reasoning, Data models, Data privacy, Computational linguistics, Visualization, Drugs, Linguistics, Knowledge transfer, Speech recognition, Context Learning, Data integrity, Privateness, Semantic Reasoning, Text Extraction, Voice activity detection, Voice cloning, Text processing, Ontology's, Natural language processing systems, Context learning, Learn+, Character recognition, Laws and legislation, Speech transmission, Text extraction, Voice-activity detections},
	pages = {1--6},
	annote = {Cited by: 0},
}

@inproceedings{johnson_ontological_2025,
	title = {Ontological {Methods} of {Functional} {Analysis} for {Aerospace} {Concepts}},
	doi = {10.1109/AERO63441.2025.11068551},
	abstract = {Functional analysis and decomposition are crucial steps in the development of complex aerospace systems, involving the translation of high-level system requirements into high-level functions, their breakdown into lower-level functions, and linking these functions to system elements. Traditionally, these processes are conducted informally, relying on the engineering judgment of developers, which may limit consistency and reduce the potential for automated assistance. To formalize these processes, we propose using an ontology-a structured conceptualization of terms and relations within a domain. This paper presents a function sub-ontology, part of an ongoing ontology development effort in association with the Advanced Concepts Office at Marshall Space Flight Center, aimed at modeling early stage space system concepts. The function sub-ontology defines types of functions, their interactions, and decomposition rules using the Ontological Modeling Language (OML). This structured approach supports automated reasoning, facilitating more consistent and traceable functional decomposition. The ontology's reasoning capabilities are demonstrated through several examples, showcasing its utility in space system concept development. These examples highlight how the ontology can help ensure consistency between functions, determine equivalence of functions, suggest functional decompositions, and recommend necessary system elements. While the ontology shows promise in enhancing functional analysis in aerospace systems, it is a work in progress. Future work will focus on refining the formal theory underpinning the ontology and further expanding its capabilities.},
	booktitle = {2025 {IEEE} {Aerospace} {Conference}},
	author = {Johnson, Hamilton and SureshKumar, Mayuranath and Thomas, L. Dale and Kannan, Hanumanthrao},
	month = mar,
	year = {2025},
	note = {ISSN: 2996-2358},
	keywords = {Ontologies, Modeling, Cognition, Translation, Functional analysis, Aerospace engineering, Observatories, Propulsion, Refining, Subject matter experts},
	pages = {1--13},
}

@inproceedings{sharma_lynx-rna_2025,
	title = {{LYNX}-{RNA}: {A} {Scalable} {Nextflow} {Workflow} for {RNA}-{Seq} {Analysis} with {Integrated} {Large} {Language} {Models} for {Comprehensive} {Result} {Interpretation}},
	volume = {3},
	doi = {10.1109/ICCSAI64074.2025.11064138},
	abstract = {RNA sequencing has become an essential technique in the life sciences, providing a comprehensive methodology. RNA sequencing has become an essential method in the life sciences, providing a comprehensive means to quantitatively evaluate RNA levels in tissues and cells. From alignment to subsequent pathway analysis, the increasing usage of RNA-seq has inspired continuous creation of creative tools for all phases of study. For non-experts especially, it is difficult to use these analytical approaches in a scalable, repeatable, and easily available manner. We have established an intuitive, rapid, efficient, and thorough process for RNA-seq analysis using the workflow management system “Nextflow.” LYNX-RNA (Language-augmented Yield for Nextflow-based RNA eXpression analysis) enhances RNA-seq analysis involves the processing of raw sequencing data through alignment, quality control, and subsequent differential expression and pathway analysis. LYNX-RNA incorporates a distinctive integration with a Large Language Model (LLM) that autonomously produces concise, comprehensible summaries of results, thereby rendering RNA-seq data analysis accessible to a diverse audience, including clinicians, biomedical researchers, pharmaceutical scientists, public health professionals, educators, and students. This enhanced accessibility enables other sectors to extract relevant insights from RNA-seq data without necessitating substantial bioinformatics knowledge. The pipeline was verified with patient-derived xenografts from leukemia and lymphoma, identifying differentially expressed genes and enriched pathways pertinent to disease development and therapy response. LYNX-RNA's applications encompass oncology, virology, immunology, personalized medicine, neuroscience, and agriculture, facilitating biomarker discovery, therapeutic target identification, and sustainable agricultural innovations, thereby rendering RNA-seq analysis both accessible and significant across various fields. Looking forward, LYNX-RNA aims to incorporate multi-omics integration, single-cell and spatial transcriptomics support, and machine learning-based predictive analytics. Planned enhancements include cloud deployment, improved visualization, compatibility with long-read sequencing, and clinical application readiness. These advancements will broaden LYNX-RNA's utility in biomarker discovery, personalized medicine, and agricultural innovations, solidifying its role as a comprehensive RNA-seq analysis tool across various fields.},
	booktitle = {2025 3rd {International} {Conference} on {Communication}, {Security}, and {Artificial} {Intelligence} ({ICCSAI})},
	author = {Sharma, Devanshi and Das, Asmita},
	month = apr,
	year = {2025},
	keywords = {Large language models, Precision medicine, Life sciences, Technological innovation, Process control, RNA, personalized medicine, biomarker discovery, large language models (LLM), LYNX-RNA, Nextflow, Peline, Rendering (computer graphics), RNA sequencing (RNA-seq), Sequential analysis, Virology, Workflow management software},
	pages = {334--342},
}

@inproceedings{molina_robot_2025,
	title = {Robot {Situation} and {Task} {Awareness} {Using} {Large} {Language} {Models} and {Ontologies}},
	doi = {10.1109/DSN-W65791.2025.00045},
	abstract = {Robot situation and task awareness requires a deep understanding of the environment, the domain knowledge, and task planning. We present a novel framework that integrates ontologies, Large Language Models (LLMs), and the Planning Domain Definition Language (PDDL) to enhance the comprehension capabilities of robotic systems. The framework employs an LLM to extract structured knowledge from natural language descriptions provided by a human user, populating an OWL ontology that captures relevant objects, properties, and relations. This populated ontology is then used to parse a PDDL Domain file and generate a corresponding PDDL Problem file to solve particular planning problems. This research contributes to the intersection of knowledge representation, natural language processing, and automated planning, providing a solution for intuitive human-robot interaction through LLMs.},
	booktitle = {2025 55th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN}-{W})},
	author = {Molina, Victor and Ruiz-Celada, Oriol and Suarez, Raul and Rosell, Jan and Zaplana, Isiah},
	month = jun,
	year = {2025},
	note = {ISSN: 2325-6664},
	keywords = {Ontologies, Large Language Models, Natural language processing, Large language models, ontologies, OWL, Planning, Human-robot interaction, Robots, Conferences, robotic manipulation, task planning},
	pages = {96--103},
}

@article{niu_pluggable_2025,
	title = {A {Pluggable} {Common} {Sense}-{Enhanced} {Framework} for {Knowledge} {Graph} {Completion}},
	issn = {2332-7790},
	doi = {10.1109/TBDATA.2025.3588081},
	abstract = {Knowledge graph completion (KGC) tasks aim to infer missing facts in a knowledge graph (KG) for many knowledgeintensive applications. However, existing embedding-based KGC approaches primarily rely on factual triples, potentially leading to outcomes inconsistent with common sense. Besides, generating explicit common sense is often impractical or costly for a KG. To address these challenges, we propose a pluggable common sense-enhanced KGC framework that incorporates both fact and common sense for KGC. This framework is adaptable to different KGs based on their entity concept richness and has the capability to automatically generate explicit or implicit common sense from factual triples. Furthermore, we introduce common senseguided negative sampling and a coarse-to-fine inference approach for KGs with rich entity concepts. For KGs without concepts, we propose a dual scoring scheme involving a relation-aware concept embedding mechanism. Importantly, our approach can be integrated as a pluggable module for many knowledge graph embedding (KGE) models, facilitating joint common sense and fact-driven training and inference. The experiments illustrate that our framework exhibits good scalability and outperforms existing models across various KGC tasks.},
	journal = {IEEE Transactions on Big Data},
	author = {Niu, Guanglin and Li, Bo and Feng, Siling},
	year = {2025},
	keywords = {Ontologies, Knowledge graphs, Large language models, Uncertainty, Knowledge graph completion, Data models, Training, Translation, Visualization, Internet, common sense, entity concepts, negative sampling, pluggable framework, Tensors},
	pages = {1--18},
}

@article{sheth_composite_2025,
	title = {Composite {AI} {With} {Custom}, {Compact}, {Neurosymbolic} {Models}: {The} {Emergent} {Enterprise} {Artificial} {Intelligence} {Paradigm}},
	volume = {29},
	issn = {1941-0131},
	doi = {10.1109/MIC.2025.3570554},
	abstract = {Artificial-intelligence architectures are shifting from monolithic, internet-scale models toward multi-component composite systems that must perform reliably in high-stakes settings. We introduce Custom, Compact and Composite AI with a Neurosymbolic approach (C3AN), a fourth-generation framework that integrates data, domain knowledge and human expertise through 14 foundation elements spanning reliability, grounding and safety. Custom focuses on right-sized, domain-specific data and workflows; Compact emphasizes resource-conscious models that can run on edge or mobile hardware; Composite unifies neural, symbolic and feedback modules for end-to-end reasoning. Three pilot systems illustrate the paradigm: Nourich (diabetes-aware dietary guidance) outperforms nine LLM baselines on recipe suitability; MAIC (K–12 mental-health triage) matches PHQ-9 gold-standard accuracy while halving compute; and SmartPilot (edge manufacturing copilot) achieves 93\% anomaly accuracy and 21\% forecasting gain over LSTM baselines. C3AN demonstrates that domain-aligned, neurosymbolic design can deliver transparent, trustworthy enterprise AI without extreme scale.},
	number = {2},
	journal = {IEEE Internet Computing},
	author = {Sheth, Amit P. and Roy, Kaushik and Venkataramanan, Revathy and Nadimuthu, Venkatesan and Shyalika, Chathurangi},
	month = mar,
	year = {2025},
	keywords = {Artificial intelligence, Manufacturing, Safety, Reliability, Cognition, Computational modeling, Long short term memory, Mission critical systems, Neural engineering},
	pages = {37--49},
}

@article{vereno_sgam_2025,
	title = {{SGAM} {Toolbox} {Revisited}: {A} {Standards}-{Based} {Domain}-{Specific} {Modeling} {Language} and {Toolset}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3586722},
	abstract = {The SGAM Toolbox has established itself as a valuable modeling tool in the energy sector, particularly for interdisciplinary system-of-systems use cases. Built on a domain-specific modeling language that is anchored in European smart grid standardization, the toolbox has been adopted across academia and industry. Drawing on this extensive experience, we introduce the new and improved SGAM Toolbox. First, we review published applications of the tool; it has been widely used for high-level architecture modeling, often alongside other software in areas such as security, privacy, and e-mobility integration. Based on our findings, the key updates for the new toolbox include a strong formal foundation, a clear definition on how the tool should interface with requirements engineering, comprehensive semantics, and a viewpoint structure that segregates logical from technical aspects; these improvements enhance usability and real-world applicability. Second, the updated toolbox is presented in accordance with the TILO language-engineering stack; the specification includes the underlying ontology, a MOF-conformant metamodel, a UML-based implementation, and a modeling add-in for Enterprise Architect for advanced features. Third, we offer a practical demonstration of the toolbox, showing use case–driven architecture modeling. The SGAM Toolbox aims to strengthen its role as a platform for collaborating on system-of-systems use cases, bringing together the diverse stakeholders in smart grid projects.},
	journal = {IEEE Access},
	author = {Vereno, Dominik and Neureiter, Christian and Eschlberger, Simon and Millaku, Mergim and Kuchenbuch, René and Uslar, Mathias},
	year = {2025},
	keywords = {Ontologies, Modeling, Unified modeling language, systems engineering, domain-specific language, Stakeholders, model-based systems engineering, DSL, smart grid, architecture, Computer architecture, Syntactics, Object oriented modeling, Smart grid architecture model, Smart grids, system of systems, Systems engineering and theory, use case},
	pages = {119243--119261},
}

@inproceedings{shyalika_smartpilot_2025,
	title = {{SmartPilot}: {A} {Multiagent} {CoPilot} for {Adaptive} and {Intelligent} {Manufacturing}},
	doi = {10.1109/CAI64502.2025.00007},
	abstract = {In the dynamic landscape of Industry 4.0, achieving efficiency, precision, and adaptability is essential to optimize manufacturing operations. Industries suffer due to supply chain disruptions caused by anomalies, which are being detected by current AI models but leaving domain experts uncertain without deeper insights into these anomalies. Additionally, operational inefficiencies persist due to inaccurate production forecasts and the limited effectiveness of traditional AI models for processing complex sensor data. Despite these advancements, existing systems lack the seamless integration of these capabilities needed to create a truly unified solution for enhancing production and decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot designed for advanced reasoning and contextual decision-making to address these challenges. SmartPilot processes multimodal sensor data and is compact to deploy on edge devices. It focuses on three key tasks: anomaly prediction, production forecasting, and domain-specific question answering. By bridging the gap between AI capabilities and real-world industrial needs, SmartPilot empowers industries with intelligent decision-making and drives transformative innovation in manufacturing. The demonstration video, datasets, and supplementary materials are available at https://github.com/ChathurangiShyalika/SmartPilot.},
	booktitle = {2025 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Shyalika, Chathurangi and Prasad, Renjith and Al Ghazo, Alaa and Eswaramoorthi, Darssan and Kaur, Harleen and Muthuselvam, Sara Shree and Sheth, Amit},
	month = may,
	year = {2025},
	keywords = {Artificial intelligence, Multimodal data, Smart manufacturing, Neurosymbolic AI, Decision making, Technological innovation, Smart Manufacturing, Multiagent, Production, Multimodal sensors, Question answering (information retrieval), Industries, CoPilot, Predictive models, Supply chains},
	pages = {1--8},
}

@inproceedings{tong_first-principles_2025,
	title = {A {First}-{Principles} {Based} {Risk} {Assessment} {Framework} and the {IEEE} {P3396} {Standard}},
	doi = {10.1109/CAI64502.2025.00237},
	abstract = {Generative Artificial Intelligence (AI) is enabling unprecedented automation in content creation and decision support, but it also raises novel risks. This paper presents a first-principles risk assessment framework underlying the IEEE P3396 Recommended Practice for AI Risk, Safety, Trustworthiness, and Responsibility. We distinguish between process risks (risks arising from how AI systems are built or operated) and outcome risks (risks manifest in the AI system's outputs and their real-world effects), arguing that generative AI governance should prioritize outcome risks. Central to our approach is an information-centric ontology that classifies AI-generated outputs into four fundamen-tal categories: (1) Perception-level information, (2) Knowledge-level information, (3) Decision/Action plan information, and (4) Control tokens (access or resource directives). This classification allows systematic identification of harms and more precise attribution of responsibility to stakeholders (developers, deployers, users, regulators) based on the nature of the information produced. We illustrate how each information type entails distinct outcome risks (e.g, deception, misinformation, unsafe recommendations, security breaches) and requires tailored risk metrics and mitigations. By grounding the framework in the essence of information, human agency, and cognition, we align risk evaluation with how AI outputs influence human understanding and action. The result is a principled approach to AI risk that supports clear accountability and targeted safeguards, in contrast to broad application-based risk categorizations. We include example tables mapping information types to risks and responsibilities. This work aims to inform the IEEE P3396 Recommended Practice and broader AI governance with a rigorous, first-principles foundation for assessing generative AI risks while enabling responsible innovation.},
	booktitle = {2025 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Tong, Richard J. and Cortês, Marina and DeFalco, Jeanine A. and Underwood, Mark and Zalewski, Janusz},
	month = may,
	year = {2025},
	keywords = {Generative AI, Security, Cognition, Ethics, Stakeholders, Organizations, Technological innovation, Risk, Fake news, AI Agency, GenAl, Human Agency, Information Categorization, Regulators, Standards organizations},
	pages = {1588--1595},
}

@inproceedings{xu_architectural_2025,
	title = {An {Architectural} {Framework} for {Educational} {Knowledge} {Graphs} ({IEEE} {P2807}.6): {Ontology} {Design}, {Llm} {Integration}, and {Adaptive} {Learning} {Applications}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011260890&doi=10.1109%2FCAI64502.2025.00249&partnerID=40&md5=d8569498bbc3f1e4f5d1adbc9ebd141f},
	doi = {10.1109/CAI64502.2025.00249},
	abstract = {The IEEE P2807.6 Education Knowledge Graph (EduKG) standard defines a semantic infrastructure to represent educational knowledge, resources, and pedagogy in a unified graph format. This paper expands on the core EduKG architecture, detailing its ontology design and key entities-Learning Points, Resource Items, and Pedagogical Rules-that collectively model the domain, content, and instructional strategies of learning systems. We further explore how EduKG can be integrated with advanced AI technologies, including large language models (LLMs) and retrieval-augmented generation (Graph-RAG) via embedding databases, to enable intelligent behavior such as semantic search, question answering, and dynamic content generation. These integrations position EduKG as a central component in next-generation smart education systems, wherein knowledge graphs work in concert with intelligent agents and adaptive instructional systems to deliver fully automated, personalized, and interactive learning experiences. By leveraging the standardized graph-structured representation and semantic reasoning capabilities of EduKG, such systems can achieve interoperability across platforms and support complex AI-driven tutoring and training scenarios. This work provides a comprehensive overview of the EduKG framework and highlights its role in empowering adaptive, cognitive, and collaborative learning solutions for the future of digital education.},
	booktitle = {2025 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Xu, Bin and Tong, Richard J and Li, Yanyan and Chen, Penghe and Li, Hanming and Liang, Joleen and Fan, Xing and Tong, Jessie},
	month = may,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Large Language Models, Knowledge graph, Large language model, Interoperability, Ontology, Language model, Artificial intelligence, Large language models, Semantics, Retrieval augmented generation, Search engines, Knowledge management, Retrieval-augmented generation, Retrieval-Augmented Generation, E-learning, Standards, Graph theory, Adaptive systems, Cognitive systems, Intelligent agents, Engineering education, Adaptive Instructional Systems, Education Knowledge Graph, Intelligent Tutoring, Learning systems, Computer aided instruction, Graphic methods, Ontology's, Educational knowledge, Graph structures, Teaching, Education computing, Adaptive instructional system, Collaborative learning, Education knowledge graph, Instructional system, Intelligent tutoring},
	pages = {1610--1616},
	annote = {Cited by: 0},
}

@inproceedings{do_mapping_2025,
	title = {Mapping {Biomedical} {Ontology} {Terms} to {Ids}: {Effect} of {Domain} {Prevalence} on {Prediction} {Accuracy}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011289811&doi=10.1109%2FCAI64502.2025.00101&partnerID=40&md5=6e57aef3b13e1dde5d7b5468be8c8a9b},
	doi = {10.1109/CAI64502.2025.00101},
	abstract = {This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship. In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95 \%) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications.},
	booktitle = {2025 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Do, Thanh Son and Hier, Daniel B. and Obafemi-Ajayi, Tayo},
	month = may,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Large language model, Terminology, Language model, Large language models, Ontology mapping, Human phenotype ontology, large language models, Gene Ontology, Lexicalization, Gene ontology, Human Phenotype Ontology, Mapping, Training, Phenotypes, Proteins, Accuracy, Genes, Training data, lexicalization, machine codes, Protein engineering, Symbols, UniProt KB, Zipf's Law, Ontology's, Systems analysis, Codes (symbols), Machine codes, Medical applications, Uniprot, Uniprot KB, Zipf Law},
	pages = {1--6},
	annote = {Cited by: 0},
}

@inproceedings{al_khatib_patient_2025,
	title = {From {Patient} {Consultations} to {Graphs}: {Leveraging} {LLMs} for {Patient} {Journey} {Knowledge} {Graph} {Construction}},
	doi = {10.1109/CAI64502.2025.00075},
	abstract = {The shift toward patient-centric healthcare requires understanding comprehensive patient journeys. Current healthcare data systems often fail to provide holistic representations, hindering coordinated care. Patient Journey Knowledge Graphs (PJKGs) solve this by integrating diverse patient information into unified, structured formats. This paper presents a methodology for constructing PJKGs using Large Language Models (LLMs) to process both clinical documentation and patient-provider conversations. These graphs capture temporal and causal relationships between clinical events, enabling advanced reasoning and personalized insights. Our evaluation of four LLMs (Claude 3.5, Mistral, Llama 3.1, ChatGPT4o) shows all achieved perfect structural compliance but varied in medical entity processing, computational efficiency, and semantic accuracy. This work advances patient-centric healthcare through actionable knowledge graphs (KGs) that enhance care coordination and outcome prediction.},
	booktitle = {2025 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Al Khatib, Hassan S. and Mittal, Sudip and Rahimi, Shahram and Marhamati, Nina and Bozorgzad, Sean},
	month = may,
	year = {2025},
	keywords = {Knowledge graphs, Large language models, Semantics, Large Language Model (LLM), Knowledge Graph, Cognition, Accuracy, Computational efficiency, Documentation, Data systems, Healthcare Journey Mapping, Medical services, Oral communication, Patient-Centric Healthcare, Temporal and Causal Reasoning},
	pages = {410--415},
}

@inproceedings{menad_enhancing_2025,
	title = {Enhancing the {Description}-{Detection} {Framework} with {Semantic} {Clustering} {Using} {Biostransformers}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010645286&doi=10.1109%2FCBMS65348.2025.00135&partnerID=40&md5=2a11e2829831cc19ff8dc016f05c6124},
	doi = {10.1109/CBMS65348.2025.00135},
	abstract = {Event-Based Surveillance Systems (EBS) are crucial for detecting emerging public health threats. However, these systems face significant challenges, including overreliance on manual expert intervention, limited handling of heterogeneous textual data, etc. The Description-Detection Framework (DDF) addresses some of these limitations by leveraging PropaPhen (Core Propagation Phenomenon Ontology), UMLS, and OpenStreetMaps to detect suspicious health-related cases using spatiotemporal and textual data. However, DDF is restricted to detection and lacks the ability to classify the detected observations into meaningful categories. To adress this limitation, we propose to enhance DDF by incorporating a clustering-based classification process. This enhancement employs BioSTransformers, a pretrained biomedical language model built on Sentence Transformers trained on PubMed data, to compute semantic similarity between observations. By capturing domain-specific semantic relationships, BioSTransformers enables clustering that integrates biological semantics with spatiotemporal context, outperforming traditional methods from the literature in observation classification. Our proposed approach reduces the dependency on manual expert effort, improves the system's ability to process heterogeneous data, and enhances the accuracy and contextual relevance of case classification. The results demonstrate the potential of this method to advance EBS systems, providing a scalable and automated solution to public health surveillance challenges.},
	booktitle = {2025 {IEEE} 38th {International} {Symposium} on {Computer}-{Based} {Medical} {Systems} ({CBMS})},
	author = {Menad, Safaa and Medeiros, Gabriel H. A. and Soualmia, Lina F.},
	month = jun,
	year = {2025},
	note = {ISSN: 2372-9198},
	keywords = {Ontologies, Ontology, Language model, Semantics, Transformers, Unified modeling language, Context modeling, Surveillance, Computational linguistics, Computational modeling, Public health, Biological system modeling, Biomedical Language Models, Core Propagation Phenomenon Ontology, Description Detection Framework, Public Health Surveillance, Public healthcare, Spatiotemporal phenomena, Spatiotemporal Reasoning, Ontology's, Natural language processing systems, Classification (of information), Biomedical language model, Core propagation phenomenon ontology, Description detection framework, Detection framework, Event-based, Health risks, Public health surveillances, Security systems, Spatio-temporal reasoning, Surveillance systems},
	pages = {654--659},
	annote = {Cited by: 0},
}

@inproceedings{menad_predicting_2025,
	title = {Predicting {Similarities} {Between} {Biomedical} {Ontologies} : {The} {UMLs} {Use}-{Case}},
	doi = {10.1109/CBMS65348.2025.00134},
	abstract = {Biomedical ontologies are crucial for organizing domain-specific knowledge, yet traditional alignment methods relying on lexical matching often fail to capture complex semantic relationships. To address this limitation, we propose a novel approach leveraging siamese neural networks and transformerbased models to enhance ontology alignment within the biomedical domain. Our method applies self-supervised contrastive learning to biomedical literature, optimizing the prediction of semantic similarities between concepts in the UMLS Metathesaurus. The results demonstrate that this approach surpasses lexical-based techniques by identifying contextual relationships and uncovering new interconnections among UMLS terminologies. This highlights the potential of our models in improving ontology alignment and enriching biomedical knowledge integration.},
	booktitle = {2025 {IEEE} 38th {International} {Symposium} on {Computer}-{Based} {Medical} {Systems} ({CBMS})},
	author = {Menad, Safaa and Abdeddaïm, Saïd and Soualmia, Lina F.},
	month = jun,
	year = {2025},
	note = {ISSN: 2372-9198},
	keywords = {Ontologies, Ontology, Terminology, Semantics, Biomedical Ontology, Neural networks, Transformers, Vocabulary, Semantic Similarity, Unified modeling language, Contrastive learning, Alignment, UMLS Metathesaurus, Biological system modeling, Vectors, Matching, Sentence Embeddings, Siamese Neural Network},
	pages = {648--653},
}

@inproceedings{zlobin_domain-specific_2025,
	title = {Domain-{Specific} {Language} {Models} for {Continuous} {Learning}},
	doi = {10.1109/NeuroNT66873.2025.11049984},
	abstract = {The paper examines the issue of building domain-specific (domain-oriented) language models and the possibility of their continuous learning. A concept is proposed that provides for the introduction of additional cognitive memory with domain administration, designed for the gradual accumulation of new information obtained after the termination of learning the language model. The issue of transition from the inference mode to a new model learning is considered.},
	booktitle = {2025 {VI} {International} {Conference} on {Neural} {Networks} and {Neurotechnologies} ({NeuroNT})},
	author = {Zlobin, O. N. and Litvinov, V. L. and Filippov, F. V.},
	month = jun,
	year = {2025},
	keywords = {Ontologies, artificial intelligence, Knowledge engineering, language models, Data mining, Context modeling, Training, Domain specific languages, embeddings, Tuning, Buildings, cognitive memory, domains, Neurotechnology, ontology concepts, Search problems, update context, verbal memory},
	pages = {3--5},
}

@inproceedings{gueddes_bert-based_2025,
	title = {{BERT}-{Based} {Knowledge} {Graph} {Construction} from {Social} {Media}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011341967&doi=10.1109%2FIWCMC65282.2025.11059459&partnerID=40&md5=8d43dc25e5b830cec8148a9b3faeb1f3},
	doi = {10.1109/IWCMC65282.2025.11059459},
	abstract = {Social media platforms serve as massive repositories of textual data, reflecting diverse human interactions and preferences. However, the unstructured nature of this content poses significant challenges for extracting semantically rich insights. This paper introduces a novel methodology for the automated construction of knowledge graphs (KGs) from social media discourse, specifically focusing on Twitter tweets. Our approach synergistically integrates large language models (LLMs), specifically a fine-tuned BERT model, with an ontology-driven framework. First, we define a detailed ontology of online communication concepts. The pre-trained BERT model is then fine-tuned using a multi-task learning approach on a curated dataset of anonymized and segmented Twitter discussions, thereby aligning its semantic representations with the predefined ontology. The fine-tuned LLM is leveraged for several critical tasks including entity and relation extraction, sentiment analysis, intention classification and the inference of contextual information and discussion styles. Furthermore, a mechanism is introduced to infer inter-user relationships and shared interests using graph neural networks (GNNs), analyzing patterns in interaction and language use. This multi-faceted extracted and inferred data is subsequently employed to build a knowledge graph, stored and queried via the Neo4j graph database management system. This study presents several contributions such as the integration of a ontology with an LLM method and the innovative user relationship and shared interest extraction using graph neural networks. The proposed methodology was rigorously evaluated using a real-world dataset of Twitter discussions, showcasing its ability to capture semantic content, and elucidate inter-user relationships effectively and revealing shared interests of the users involved. Furthermore, an ablation study is included which further demonstrates each of the method component contribution and demonstrates the importance of such integrations. Our findings highlight the potential for various downstream applications such as in community structure analysis and sentiment analysis to improve information management within online social networks.},
	booktitle = {2025 {International} {Wireless} {Communications} and {Mobile} {Computing} ({IWCMC})},
	author = {Gueddes, Abdelweheb and Fathallah, Wyssem and Mahjoub, Mohamed Ali},
	month = may,
	year = {2025},
	note = {ISSN: 2376-6506},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Large language model, Ontology, Language model, Large language models, Semantics, BERT, Information management, Large Language Models (LLMs), Social media analysis, Knowledge Graph, Sentiment analysis, Social media, Ethics, Scalability, Graph neural networks, Extraction, Management information systems, Social Media Analysis, Blogs, Ontology-Driven Methodology, Social networking (online), Ontology's, Classification (of information), Ontology-driven methodology, Tweets, Users' relationships},
	pages = {461--466},
	annote = {Cited by: 0},
}

@inproceedings{shukla_protein_2025,
	title = {Protein {Matching} for {Function} {Prediction} using {Siamese} {Neural} {Network}},
	doi = {10.1109/ICoICC64033.2025.11052053},
	abstract = {Protein similarity lies at the heart of many critical tasks in bio-informatics, from predicting protein function to drug discovery and evolutionary studies. The discipline has deployed traditional approaches such as sequence alignment and structural comparison to serve the field for decades, their limitations in scalability, sensitivity to remote homologs, and reliance on handcrafted features have become increasingly apparent in the era of high-throughput data. In this study, one of the concept of deep learning, specifically Siamese neural networks for similarity matching, is explored for protein matching. By leveraging pretrained protein sequence embeddings and biologically informed functional annotations, a framework is developed that can learn nuanced relationships between protein pairs.},
	booktitle = {2025 {International} {Conference} on {Intelligent} and {Cloud} {Computing} ({ICoICC})},
	author = {Shukla, Varsha and Pradhan, Rahul},
	month = may,
	year = {2025},
	keywords = {Deep learning, Cloud computing, Scalability, Graph neural networks, Convolutional neural networks, Graph neural network (GNN), Drug discovery, Biology, Heart, Convolutional neural network (CNN), Sensitivity, Protein sequence, Siamese neural network (SNN)},
	pages = {1--5},
}

@inproceedings{loevenich_automating_2025,
	title = {Automating {Cyber} {Threat} {Intelligence} and {Attack} {Chain} {Generation} using {Cyber} {Security} {Knowledge} {Graphs} and {Large} {Language} {Models}},
	doi = {10.1109/ICMCIS64378.2025.11047951},
	abstract = {Modern cyberattacks are increasingly complex, using sophisticated tactics, techniques and procedures (TTPs) to evade detection and compromise systems. Effective cyber defence relies on real-time and accurate Cyber Threat Intelligence (CTI), which is often challenged by data quality, completeness and accessibility. While traditional methods and manually maintained knowledge bases provide valuable insights, they struggle to adapt to the rapidly evolving threat landscape. To address these challenges, we propose an architecture that uses Large Language Models (LLMs) for automated annotation of CTI reports and construction of Cybersecurity Knowledge Graphs (CSKG) to build sophisticated attack chains. Building on our previous research, we extend the capabilities of Autonomous Cyber Defence (ACD) agents to improve situational awareness and defence mechanisms in dynamic environments. Experimental results demonstrate the effectiveness of our approach in improving CTI accessibility, accuracy, and integration into defence strategies. Our experimental results highlight the potential of combining LLM, knowledge graphs and automated planning to improve proactive cyber defence and attack simulation methodologies.},
	booktitle = {2025 {International} {Conference} on {Military} {Communication} and {Information} {Systems} ({ICMCIS})},
	author = {Loevenich, Johannes F. and Adler, Erik and Hürten, Tobias and Spelter, Florian and Roncevic, Damian and Lopes, Roberto Rigolin F.},
	month = may,
	year = {2025},
	note = {ISSN: 2993-4974},
	keywords = {Knowledge graphs, Large language models, Knowledge Graphs, Cybersecurity, Annotations, Large Language Model, Cyber threat intelligence, Knowledge based systems, Autonomous Cyber Defence, Accuracy, Planning, Soft sensors, Real-time systems, Military communication},
	pages = {1--10},
}

@article{nagata_semantipack_2025,
	title = {{SemantiPack}: {An} {Efficient} {Real}-{World} {Data} {Compressor} {Using} {Structural} and {Semantic} {Metadata}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3583829},
	abstract = {The exponential growth of Real-World Data (RWD), primarily collected from IoT sensors and spanning domains such as mobility, environment, and energy consumption, presents critical challenges due to its scale, heterogeneity, and structural variability. Traditional compression methods often fail to adapt efficiently to these complexities, leading to sub-optimal storage and analysis performance. Additionally, while several metadata schemas exist to enhance the availability of RWD, smaller organizations often lack the resources to create and manage metadata effectively. This paper introduces RWD Profile, an automatically generatable metadata schema for RWD, and SemantiPack, which applies tailored compression to data fields in the individual RWD based on RWD Profile. RWD Profile has two kinds of metadata, Structural Profile and Semantic Profile. Structural Profile is generated through rule-based systems, while Semantic Profile is generated using large language models (LLMs) to capture semantic data properties. On the other hand, SemantiPack performs compression at the data field level in RWD using RWD Profile to achieve higher compression ratios. Experimental results demonstrate up to 23.2\% improved compression rate for JSON and 19.3\% for CSV compared to conventional methods while maintaining a faster or equivalent processing speed compared to conventional methods. Furthermore, SemantiPack supports lossy compression for applications prioritizing storage over precision. This research not only improves compression efficiency but also establishes a scalable solution for automated analysis and sustainable data utilization, paving the way for advancements in RWD management.},
	journal = {IEEE Access},
	author = {Nagata, Yoshiteru and Kohama, Daiki and Watanabe, Yoshiki and Katayama, Shin and Urano, Kenta and Yonezawa, Takuro and Kawaguchi, Nobuo},
	year = {2025},
	keywords = {Ontologies, Semantics, Resource description framework, semantic web, Metadata, linked data, real-world data, Data compression, Standards organizations, Image coding, Sensor phenomena and characterization, Transform coding, Wireless sensor networks},
	pages = {114159--114178},
}

@inproceedings{s_advancements_2025,
	title = {Advancements in {Context}-{Aware} {Recommender} {Systems}: {An} {Exhaustive} {Exploration} of {Ontology} and {LLM} in improving ranking accuracy},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010522965&doi=10.1109%2FRMKMATE64874.2025.11042856&partnerID=40&md5=3f906f4128e0b16c696a6b4f12321eb3},
	doi = {10.1109/RMKMATE64874.2025.11042856},
	abstract = {Recommendation systems have become a ubiquitous presence across various domains such as e-commerce, entertainment (movies and music), tourism, news, advertising, stock markets and social networks. The integration of Ontology and Large Language Models (LLMs) presents a transformative approach for enhancing recommender systems by improving contextual intelligence, personalization, and explainability. Traditional recommendation algorithms often suffer from data sparsity, cold-start problems, and limited semantic understanding, which hinder their ability to deliver highly relevant and interpretable suggestions. This research provides an exhaustive exploration of how ontology-driven knowledge representation can be seamlessly combined with LLMs to refine user-item interactions, enrich embeddings, and improve reasoning in recommendation pipelines. We propose a hybrid framework where ontological structures encode domain-specific relationships while LLMs process, infer, and enhance recommendations through contextual embeddings.This synergy enables semantic-aware recommendations, explainable item suggestions, and adaptive learning mechanisms that adjust based on user behavior and external context. Furthermore, we investigate novel ontology injection techniques, including knowledge graph-based reinforcement, to improve long-tail item exposure and enhance recommendation diversity. Through rigorous evaluation on real-world datasets, we demonstrated how ontology-LLM fusion significantly outperforms baseline models in accuracy, novelty, and interpretability. This study establishes a foundation for the next generation of context-aware, knowledge-enhanced recommender systems that leverage LLMs for reasoning and ontologies for structured knowledge representation, paving the way for more intelligent and user-centric recommendations.},
	booktitle = {2025 2nd {International} {Conference} on {Research} {Methodologies} in {Knowledge} {Management}, {Artificial} {Intelligence} and {Telecommunication} {Engineering} ({RMKMATE})},
	author = {S, Jenny Kalaiarasi. and K, Nimala.},
	month = may,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Large language model, Ontology, LLM, Language model, Semantics, Knowledge representation, Knowledge management, Embeddings, Cognition, Electronic commerce, Recommender systems, Graph neural networks, Accuracy, Computational intelligence, Social networking (online), Context-Aware Recommendation Systems, Diversity reception, Heavily-tailed distribution, semantic-aware, Stock markets, User interfaces, Graphic methods, Ontology's, Behavioral research, Knowledge-representation, In contexts, Context-aware recommendation systems, Context-aware recommender systems, E- commerces, Electronic trading, Semantic-aware},
	pages = {1--7},
	annote = {Cited by: 0},
}

@inproceedings{shi_dynamic_2025,
	title = {Dynamic {Assessment} and {Early} {Warning} of {Cross}-{Border} {Pipeline} {Risks} {Based} on {Knowledge} {Graph}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010558936&doi=10.1109%2FAEMCSE65292.2025.11042435&partnerID=40&md5=fe90113108f54be993c45e3f1190d29c},
	doi = {10.1109/AEMCSE65292.2025.11042435},
	abstract = {This paper introduces a novel framework for the dynamic assessment and early warning of cross-border pipeline risks, leveraging the zero-shot learning capabilities of the GPT-4 large language model (LLM) to construct and utilize a domain-specific knowledge graph. The approach circumvents the need for extensive pre-training or fine-tuning, thereby enabling rapid development and deployment in a domain characterized by data scarcity. The methodology begins with the construction of a knowledge graph, which involves a comprehensive ontology design, followed by knowledge extraction from unstructured text sources, and culminates in knowledge representation and storage in a structured format. Subsequently, the framework facilitates dynamic risk understanding and analysis, encompassing risk factor identification, risk event inference, and the analysis of complex relationships between these elements, all supported by the structured knowledge base. Finally, the system provides knowledge-based risk interpretation and decision-making support, generating explanations for risk events, assessing their potential impacts, and offering actionable recommendations to stakeholders. By employing carefully crafted prompts and the inherent natural language processing capabilities of GPT-4, the framework facilitates the extraction of entities and relationships from unstructured textual data, effectively populating the knowledge graph. This knowledge graph, representing entities, attributes, and relationships pertinent to cross-border pipeline risks, serves as a foundation for dynamic risk assessment. This method presents a scalable, data-efficient approach tailored to the specialized domain of cross-border pipeline risk management, particularly advantageous in contexts with limited resources. The proposed framework demonstrates the potential of LLMs, specifically GPT-4, in conjunction with knowledge graph methodologies, to advance risk management practices in complex, data-sparse domains.},
	booktitle = {2025 8th {International} {Conference} on {Advanced} {Electronic} {Materials}, {Computers} and {Software} {Engineering} ({AEMCSE})},
	author = {Shi, Ruhui and Chen, Tianran and Lu, Chunyu and Shang, Duo and Luo, Jun and Hui, Xin and Li, Haoran and He, Huihong},
	month = may,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Large language model, Ontology, LLM, Language model, Domain-specific knowledge, Large language models, Knowledge management, Risk assessment, Software engineering, Risk management, knowledge graph, Data mining, Safety, Decision making, Knowledge based systems, Stakeholders, Risk analysis, Graph theory, Extraction, Early warning, Pipelines, China-Russia pipeline, Zero shot learning, Domain Knowledge, Graphic methods, Natural language processing systems, Learning capabilities, China-russia pipeline, Cross-border, Digital storage, Dynamic assessment, Risk-based},
	pages = {374--377},
	annote = {Cited by: 0},
}

@inproceedings{wang_construction_2025,
	title = {Construction and application of power transformer fault knowledge graph based on {Sentence}-{BERT}},
	doi = {10.1109/ISEAE64934.2025.11041826},
	abstract = {With the rapid development of smart grid, the relevance of multidimensional information in power grid is weak, and the decision-making generation efficiency in operation and maintenance process is low. At present, knowledge graph has innovative applications in various fields, which improves the efficiency of knowledge query in related professional fields. This paper proposes a power transformer fault diagnosis system driven by knowledge graph, which aims to solve the problems of weak information relevance and low decision-making efficiency in transformer operation and maintenance; The system is composed of three modules: data acquisition, data analysis and data service. By constructing the knowledge graph and model of fault diagnosis, the intelligent query and diagnosis of transformer fault are realized; The construction of knowledge graph is based on expert knowledge and fault data, and the entity extraction is carried out by using the Sentence-BERT-BiLSTM-CRF model, which improves the extraction accuracy of transformer fault information; The three tuple data is stored in the neo4j graph database, and the cypher language is used for efficient query, providing accurate fault diagnosis and maintenance strategies for operation and maintenance personnel. Experimental results show that the proposed method can effectively improve the accuracy and efficiency of fault diagnosis.},
	booktitle = {2025 7th {International} {Conference} on {Information} {Science}, {Electrical} and {Automation} {Engineering} ({ISEAE})},
	author = {Wang, Shenghui and Li, Mengjiao and Miao, Yuxin and Liu, Nan and Sun, Zihao},
	month = apr,
	year = {2025},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Knowledge extraction, Maintenance, Fault diagnosis, Data mining, Decision making, Data models, Accuracy, Power transformer, Bert model, Power transformers, Visual databases},
	pages = {841--846},
}

@inproceedings{zhao_dmm-gpt_2025,
	title = {{DMM}-{GPT}: {A} {Dual} {Modeling} {Mechanism}-{Based} {Generic} {Protocol} {Translation} {Framework}},
	doi = {10.1109/AINIT65432.2025.11035002},
	abstract = {With the increasing complexity of heterogeneous communication environments, the problem of data sharing caused by multiple heterogeneous protocols has become the core challenge restricting interoperability. Aiming at the problems of poor scalability and automation ability of existing protocol translation methods, this study proposes a Dual Model Mechanism-based Generic Protocol Translation (DMM-GPT) framework. The framework creatively combines ontology-based semantic modeling with state-machine-based behavioral modeling, enabling automated identification of protocol translation bridging points and autonomous construction of a unified protocol state transition model. Firstly, a protocol ontology model is constructed to model the semantic associations between protocol entities. Based on the ontology, semantic information is extracted. Semantic similarities from message level to field level are computed, thereby sequentially identifying message mapping and field mapping relationships. Secondly, modeling protocol behavior through state machine models. Based on the identified mapping relationships, the heterogeneous protocol bridge points are constructed, and a unified state transition model is synthesized automatically. Thus, interoperable behavior between heterogeneous protocols is implemented. Finally, the complete closed loop of protocol interoperability is implemented by applying semantic alignment results to the state transition model. To validate the effectiveness of DMM-GPT, experiments were conducted in two representative scenarios: service discovery (SLP-Bonjour) and IoT communication (HTTP-CoAP). The results demonstrate its effectiveness in enabling interoperability between heterogeneous protocols.},
	booktitle = {2025 {IEEE} 6th {International} {Seminar} on {Artificial} {Intelligence}, {Networking} and {Information} {Technology} ({AINIT})},
	author = {Zhao, Wei and Bai, Wei and Hu, Zhi-Yuan and He, Chao-Xin},
	month = apr,
	year = {2025},
	keywords = {Ontologies, Interoperability, Semantics, Data mining, semantic similarity, Scalability, Translation, Computational modeling, state machine, ontology modeling, Information technology, protocol translation, Protocols, Seminars},
	pages = {1--12},
}

@inproceedings{matta_answering_2025,
	title = {Answering {Application} of {Generative} {AI} in {Industry}: {Integration} of {Semantic} {Representation}},
	doi = {10.1109/CSCWD64889.2025.11033426},
	abstract = {Generative AI acts as an effective guide in decision-making process. This paper is designed to outline the main challenges in applying this technique within businesses and for specific activities. Semantic representation as ontology can be one solution for these challenges. Our first work to link ontology to LLM algorithms is illustrated in financial institutions},
	booktitle = {2025 28th {International} {Conference} on {Computer} {Supported} {Cooperative} {Work} in {Design} ({CSCWD})},
	author = {Matta, Nada and Pfundstein, Patrick and Larde, Camille and Ismedon, Baptiste},
	month = may,
	year = {2025},
	note = {ISSN: 2768-1904},
	keywords = {ChatGPT, Ontologies, Ontology, LLM, Semantics, Generative AI, Retrieval augmented generation, Federated learning, Finance, Machine learning algorithms, Text recognition, Industries, Bard, Companies, finance},
	pages = {825--828},
}

@inproceedings{onozuka_analysis_2025,
	title = {Analysis of {LLMs} for {RDF} {Triple} {Generation}: {Semantic} and {Syntactic} {Evaluation} {Using} {WebNLG}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009459996&doi=10.1109%2FICSC64641.2025.00025&partnerID=40&md5=a0086a1e9ef98246a60e66d2507f9e5a},
	doi = {10.1109/ICSC64641.2025.00025},
	abstract = {Using the WebNLG dataset as ground truth, we evaluate the capability of large language models (LLMs) to generate resource description framework triples from natural language input. The proposed method employs two complementary evaluation metrics: cosine similarity for assessing semantic proximity and graph edit distance for comparing structural aspects of triple sets. The analysis demonstrates that these metrics provide distinct yet complementary perspectives on semantic evaluation, enabling a comprehensive assessment of the natural language understanding capabilities of LLMs. Through this approach, we demonstrate that modern LLMs exhibit sophisticated abilities in integrated syntax and semantics processing by utilizing distributed representations where both types of information coexist within high-dimensional vector spaces. This integration suggests that understanding of language structure of LLMs transcends simple pattern recognition to achieve meaningful semantic comprehension.},
	booktitle = {2025 19th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Onozuka, Soichi and Ohnishi, Takaaki},
	month = feb,
	year = {2025},
	note = {ISSN: 2472-9671},
	keywords = {Ontologies, Large language model, Ontology, Natural language processing, Language model, Large language models, Semantics, RDF, Resource description framework, Similarity, LLMs, Computational linguistics, Resource Description Framework (RDF), Latent semantic analysis, Measurement, Evaluation metrics, Ground truth, Natural languages, Syntactics, Vectors, Pattern recognition, Ontology's, Natural language processing systems, Large datasets, Vector spaces, RDF triples, Resources description frameworks},
	pages = {136--143},
	annote = {Cited by: 0},
}

@inproceedings{johnson_improving_2025,
	title = {Improving {Tabular} {Reusability} {Through} {Data} {Dictionary} {Descriptions}},
	doi = {10.1109/ICSC64641.2025.00035},
	abstract = {Tables have become a ubiquitous standard for capturing, storing, and sharing data on the web. This is primarily due to the semi-structured nature of tables, where relationships between data are often ambiguously encoded using locality. While this format can be easy for humans to interpret in simple cases, as table complexity increases, so does the difficulty in interpretability. To bridge this context gap, many data publishers provide a data dictionary to capture schema elements' meaning through text descriptions. Existing work compounds the need for data dictionaries to improve tabular interoperability, but few provide detailed requirements for data dictionary descriptions. This paper identifies and defines three common types of data dictionary descriptions in the biomedical domain. We then compare the effectiveness of each description type by normalizing data dictionary descriptions to a single type using large language models and measuring their performance using a semantic tabular interpretation algorithm. Our experiments show that intensional descriptions, which describe the general properties a column member should have, are most effective for tabular alignment and improve the reusability of data dictionaries.},
	booktitle = {2025 19th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Johnson, Matthew and Rashid, Sabbir M. and McGuinness, Deborah L.},
	month = feb,
	year = {2025},
	note = {ISSN: 2472-9671},
	keywords = {large language model, Interoperability, Large language models, Semantics, Standards, Reviews, Dictionaries, Complexity theory, semantic annotation, tabular data, Compounds, data dictionary, Information sharing, Lenses, semantic data dictionary},
	pages = {209--216},
}

@inproceedings{jiang_ontology-guided_2025,
	title = {Ontology-{Guided}, {Hybrid} {Prompt} {Learning} for {Generalization} in {Knowledge} {Graph} {Question} {Answering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009456479&doi=10.1109%2FICSC64641.2025.00010&partnerID=40&md5=19916ad8cf2702f3905c9c621d6d8ec4},
	doi = {10.1109/ICSC64641.2025.00010},
	abstract = {Most existing Knowledge Graph Question Answering (KGQA) approaches are designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the heterogeneity of the underlying graph schema, topology and assertions, most KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without resource-intensive training data. We present OntoSCPrompt, a novel Large Language Model (LLM)-based KGQA approach with a two-stage architecture that separates semantic parsing from KG-dependent interactions. OntoSCPrompt first generates a SPARQL query structure (including SPARQL keywords such as SELECT, ASK, WHERE and placeholders for missing tokens) and then fills them with KG-specific information. To enhance the understanding of the underlying KG, we present an ontology-guided, hybrid prompt learning strategy that integrates KG ontology into the learning process of hybrid prompts (e.g., discrete and continuous vectors). We also present several task-specific decoding strategies to ensure the correctness and executability of generated SPARQL queries in both stages. Experimental results demonstrate that OntoSCPrompt performs as well as SOTA approaches without retraining on a number of KGQA datasets such as CWQ, WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG 11Code: https://github.com/LongquanJiang/OntoSCPrompt.},
	booktitle = {2025 19th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Jiang, Longquan and Huang, Junbo and Möller, Cedric and Usbeck, Ricardo},
	month = feb,
	year = {2025},
	note = {ISSN: 2472-9671},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Large language model, Ontology, LLM, Language model, Large language models, Semantics, Question answering, Decoding, Query processing, QA, Question Answering, Knowledge transfer, Topology, Generalization, Training data, Question answering (information retrieval), Vectors, Predictive models, Learning systems, KGQA, Ontology's, Dbpedia, Generalisation, Knowledge graph question answering, Specific knowledge},
	pages = {28--35},
	annote = {Cited by: 0},
}

@inproceedings{robert_case_2025,
	title = {Case {Study}: {Apply} {Ontology} and {AIGC} for {Task} {Appoint} in {Smart} {Building} {Control} {Centers}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011028437&doi=10.1109%2FICAID65275.2025.11034402&partnerID=40&md5=7bd2eca04f0754580c1d22ce24da8ff6},
	doi = {10.1109/ICAID65275.2025.11034402},
	abstract = {Since the amazing effect of ChatGPT has been recognized by the industry, towards integrating with employees’ workflow to create more business applications. At the same time, many artificial intelligence scientists are actively looking for methods and platforms that are highly efficient and can quickly embed LLM and gain business value quickly. This study proposes two important innovations. First, it uses historical operating data to quickly and automatically generate LLMs in the company’s exclusive domain. Second, to quickly embed those models in ontology maps that to generate thinking and decision-making documents (as AIGC). In the paper, first step is use auto-insight engine to transfer input data from operational datasets. Secondly, to adjust parameters the training in deep learning framework to output models. Finally, integrate more API that they like conversation tools or other services. It can automatically generate and output high-precision event determination results in real time and can automatic notification and complete the processing flow that integrate daily report for frontline partners. In future, it will be able to develop more follow-up applications, such as artificial intelligence assistant for the security control center partner, which can determine the high level of daily incidents and automatically complete high-complexity daily operation procedures in a closed-loop security environment, and moderately connect high-level voice assistants to achieve important indicators such as health care, efficiency improvement and cost reduction.},
	booktitle = {2025 4th {International} {Conference} on {Artificial} {Intelligence}, {Internet} and {Digital} {Economy} ({ICAID})},
	author = {Robert, Kuo-Chung Lin and Jessica, Wen-Jie Lin},
	month = apr,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Ontology, LLM, Large language models, Deep learning, LLMs, Security, Decision making, Data models, Training, AIGC, Smart buildings, Biological system modeling, Industries, Predictive models, Costs, Smart Building, Personnel, Ontology's, Case-studies, Work-flows, Building controls, Business applications, Business value, Control centre, Cost reduction, Intelligent buildings, Operating data},
	pages = {22--26},
	annote = {Cited by: 0},
}

@inproceedings{lee_visual_2025,
	title = {Visual {Text} {Mining} with {Progressive} {Taxonomy} {Construction} for {Environmental} {Studies}},
	doi = {10.1109/PacificVis64226.2025.00037},
	abstract = {Environmental experts have developed the DPSIR (Driver, Pressure, State, Impact, Response) framework to systematically study and communicate key relationships between society and the environment. Using this framework requires experts to construct a DPSIR taxonomy from a corpus, annotate the documents, and identify DPSIR variables and relationships, which is laborious and inflexible. Automating it with conventional text mining faces technical challenges, primarily because the taxonomy often begins with abstract definitions, which experts progressively refine and contextualize as they annotate the corpus. In response, we develop GreenMine, a system that supports interactive text mining with prompt engineering. The system implements a prompting pipeline consisting of three simple and evaluable subtasks. In each subtask, the DPSIR taxonomy can be defined in natural language and iteratively refined as experts analyze the corpus. To support users evaluate the taxonomy, we introduce an uncertainty score based on response consistency. Then, we design a radial uncertainty chart that visualizes uncertainties and corpus topics, which supports interleaved evaluation and exploration. Using the system, experts can progressively construct the DPSIR taxonomy and annotate the corpus with LLMs. Using real-world interview transcripts, we present a case study to demonstrate the capability of the system in supporting interactive mining of DPSIR relationships, and an expert review in the form of collaborative discussion to understand the potential and limitations of the system. We discuss the lessons learned from developing the system and future opportunities for supporting interactive text mining in knowledge-intensive tasks for other application scenarios.},
	booktitle = {2025 {IEEE} 18th {Pacific} {Visualization} {Conference} ({PacificVis})},
	author = {Lee, Sam Yu-Te and Hung, Cheng-Wei and Yuan, Mei-Hua and Ma, Kwan-Liu},
	month = apr,
	year = {2025},
	note = {ISSN: 2165-8773},
	keywords = {Text mining, Uncertainty, Taxonomy, Data Mining, Information systems, Reviews, Data visualization, Visualization, Human-centered computing, Green products, Pipelines, Information systems applications, Systems support, Visualization systems and tools},
	pages = {307--317},
}

@inproceedings{guryanov_approach_2025,
	title = {An {Approach} to {Automated} {Ontology} {Extraction} {From} {Technological} {Documentation} {Using} {NLP} and {LLM}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009220232&doi=10.1109%2FICIEAM65163.2025.11028174&partnerID=40&md5=ae8cbf6929836ebcd5106cf7818cd65b},
	doi = {10.1109/ICIEAM65163.2025.11028174},
	abstract = {The paper presents an approach to constructing a formal description of a subject area using automated analysis of technological documentation. The first stage of the proposed approach is extracting data (terms, objects of the subject area) from technical documentation on the subject area. Then, a natural language query is formed for a large language model (LLM), which helps to extract relations between the extracted terms. At the final stage, an OWL ontology is formed by postprocessing the obtained data. Evaluation experiments were conducted on 16 documents (ISO) on the topic of “Space industry”. Experiments were also conducted to compare algorithms for extracting terms and relations using LLM and the C-value algorithm. The proposed approach has proven its effectiveness on strictly formalized texts.},
	booktitle = {2025 {International} {Conference} on {Industrial} {Engineering}, {Applications} and {Manufacturing} ({ICIEAM})},
	author = {Guryanov, A. V. and Moshkin, V. S. and Dyrnochkin, A. A.},
	month = may,
	year = {2025},
	note = {ISSN: 2993-4060},
	keywords = {large language model, Ontologies, Large language model, Ontology, Natural language processing, Language model, Large language models, Manufacturing, natural language processing, ontology, OWL, Term, Language processing, Ontology Extraction, Documentation, Natural languages, Prototypes, Industries, design documentation, Industrial engineering, ISO Standards, technical documentation, term, Ontology's, Natural language processing systems, Query languages, C (programming language), Design documentation, Technical documentations},
	pages = {1011--1016},
	annote = {Cited by: 0},
}

@inproceedings{ahmed_smatch-m-llm_2025,
	title = {{SMATCH}-{M}-{LLM}: {Semantic} {Similarity} in {Metamodel} {Matching} {With} {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009099546&doi=10.1109%2FMSR66628.2025.00040&partnerID=40&md5=99eebac4ac67ecc52b6190e9eb76e74f},
	doi = {10.1109/MSR66628.2025.00040},
	abstract = {Metamodel matching plays a crucial role in defining transformation rules in model-driven engineering by identifying correspondences between different metamodels, forming the foundation for effective transformations. Current techniques face significant challenges due to syntactical and structural heterogeneity. To address this, matching techniques often employ semantic similarity to identify correspondences. Traditional semantic matchers, however, rely on ontology matching tools or lexical databases, which often struggle when metamodels use different terminologies or hierarchical structures. Inspired by the contextual understanding capabilities of Large Language Models (LLMs), this paper explores the capability of GPT-4 potentials as a semantic matcher and alternative to existing methods for metamodel matching. However, metamodels can be large, which can overwhelm LLMs if provided in a single prompt, leading to reduced accuracy. Therefore, we propose prompting LLMs with fragments of the source and target metamodels, identifying correspondences through an iterative process. The fragments to be provided in the prompt are identified based on an initial mapping derived from their elements’ definitions. Through experiments with 10 metamodels, our results show that our LLMbased approach improves the accuracy of metamodel matching, achieving an average F-measure of {\textbackslash}approx 91 \%, outperforming both the baseline and hybrid approaches, which have a maximum average F-measure of {\textbackslash}approx {\textbackslash}mathbf2 9 \% and {\textbackslash}approx {\textbackslash}mathbf7 4 \%, respectively. Moreover, our approach surpasses single-prompt LLM-based matching, which has an average {\textbackslash}mathbfF-measure of {\textbackslash}mathbf8 0 \%, by approximately {\textbackslash}mathbf1 1 \%.},
	booktitle = {2025 {IEEE}/{ACM} 22nd {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Ahmed, Nafisa and Kwok, Hin Chi and Hamdaqa, Mohammad and Assunção, Wesley K. G.},
	month = apr,
	year = {2025},
	note = {ISSN: 2574-3864},
	keywords = {Ontologies, Ontology, Terminology, Language model, Large language models, Semantics, Software engineering, Software, Semantic similarity, Databases, Computational linguistics, Latent semantic analysis, Model-driven Engineering, Accuracy, Iterative methods, Costs, Domain-Specific Languages, Metamodel Matching, Model driven engineering, Model Migration, Domains specific languages, F measure, Meta model, Meta-model matching, Model migrations, Model semantics, Transformation rules},
	pages = {199--210},
	annote = {Cited by: 0},
}

@inproceedings{pruski_enhancing_2025,
	title = {Enhancing {ESCO} with {Generative} {AI}: {A} {Dynamic} {Approach} to {Supporting} 21st {Century} {Education}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008223380&doi=10.1109%2FEDUCON62633.2025.11016516&partnerID=40&md5=b97ab1cd1b30a3fd2cf619db06f6e211},
	doi = {10.1109/EDUCON62633.2025.11016516},
	abstract = {In the rapidly evolving landscape of engineering education, upskilling and lifelong learning have become critical to maintaining competitiveness and fostering innovation. The use of ontologies, such as the European Skills, Competences, Qualifications, and Occupations (ESCO), plays a crucial role in organizing and managing the skills required for modern engineering roles. However, the slow pace of ontology updates and the lack of contextual adaptability present significant challenges, leading to outdated and irrelevant information for educators, learners, and industry professionals. This paper explores the potential of integrating Large Language Models (LLMs) with knowledge engineering to accelerate the process of updating ontologies like ESCO. By dynamically analyzing data and incor-porating contextual information, LLMs offer promising avenues for enhancing the evolution and precision of these ontologies. We discuss the potential impact of this approach in engineering education, particularly in aligning ups killing and reskilling efforts with the demands of emerging technologies such as AI -driven automation and digital engineering. This paper aims to highlight how LLMs can support the creation of more responsive, context-aware learning frameworks, ultimately sustaining educational ex-cellence and fostering critical thinking in engineering education.},
	booktitle = {2025 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
	author = {Pruski, Cédric and Gallais, Marie and Da Silveira, Marcos},
	month = apr,
	year = {2025},
	note = {ISSN: 2165-9567},
	keywords = {Ontologies, Large language model, Ontology, LLM, Language model, Large language models, Knowledge engineering, Ontology evolution, Robustness, Technological innovation, Contextualization, Engineering education, Soft sensors, lifelong learning, Multilingual, Industries, Qualifications, Upskilling, Ontology's, Dynamic approaches, Industry professionals, Life long learning, Modern engineering},
	pages = {1--5},
	annote = {Cited by: 0},
}

@inproceedings{abu-rasheed_llm-assisted_2025,
	title = {{LLM}-{Assisted} {Knowledge} {Graph} {Completion} for {Curriculum} and {Domain} {Modelling} in {Personalized} {Higher} {Education} {Recommendations}},
	doi = {10.1109/EDUCON62633.2025.11016377},
	abstract = {While learning personalization offers great potential for learners, modern practices in higher education require a deeper consideration of domain models and learning contexts, to develop effective personalization algorithms. This paper introduces an innovative approach to higher education curriculum modelling that utilizes large language models (LLMs) for knowledge graph (KG) completion, with the goal of creating personalized learning-path recommendations. Our research focuses on modelling university subjects and linking their topics to corresponding domain models, enabling the integration of learning modules from different faculties and institutions in the student's learning path. Central to our approach is a collaborative process, where LLMs assist human experts in extracting high-quality, fine-grained topics from lecture materials. We develop a domain, curriculum, and user models for university modules and stakeholders. We implement this model to create the KG from two study modules: Embedded Systems and Development of Embedded Systems Using FPGA. The resulting KG structures the curriculum and links it to the domain models. We evaluate our approach through qualitative expert feedback and quantitative graph quality metrics. Domain experts validated the relevance and accuracy of the model, while the graph quality metrics measured the structural properties of our KG. Our results show that the LLM-assisted graph completion approach enhances the ability to connect related courses across disciplines to personalize the learning experience. Expert feedback also showed high acceptance of the proposed collaborative approach for concept extraction and classification.},
	booktitle = {2025 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
	author = {Abu-Rasheed, Hasan and Jumbo, Constance and Al Amin, Rashed and Weber, Christian and Wiese, Veit and Obermaisser, Roman and Fathi, Madjid},
	month = apr,
	year = {2025},
	note = {ISSN: 2165-9567},
	keywords = {Ontologies, Knowledge graphs, Large language models, Stakeholders, Collaboration, Higher education, Knowledge graph (KG), Recommender systems, Embedded systems, Large language models (LLM), Measurement, domain model, Pipelines, curriculum model, Field programmable gate arrays},
	pages = {1--5},
}

@inproceedings{garzo_human---loop_2025,
	title = {Human-in-the-{Loop}: {Legal} {Knowledge} {Formalization} in {Attempto} {Controlled} {English}},
	doi = {10.1109/ISDFS65363.2025.11011971},
	abstract = {Automating legal knowledge through the use of computational languages presents a series of challenges, particularly in translating complex normative texts into formalized representations that are semantically faithful and computationally valid. A salient concern is reconciling the ambiguity inherent in natural legal language with the syntactic constraints of controlled languages. This work proposes a human-in-the-loop methodology for formalizing normative texts in the controlled language Attempto Controlled English (ACE), centered on the interaction between legal experts and computational constraints. Specifically, key legal provisions and fundamental case-law principles were translated into ACE. The findings reveal that even ostensibly straightforward legal provisions necessitate a substantial decomposition, abstraction, and adaptation process. The analysis also identified recurrent linguistic patterns (modal, temporal, and conditional) and reusable translation strategies.},
	booktitle = {2025 13th {International} {Symposium} on {Digital} {Forensics} and {Security} ({ISDFS})},
	author = {Garzo, Grazia and Palumbo, Alessandro},
	month = apr,
	year = {2025},
	note = {ISSN: 2768-1831},
	keywords = {Ontologies, Semantics, Security, Cognition, Law, Translation, Europe, Human in the loop, Syntactics, Attempto Controlled English, Computable Law, Controlled Language, Human-Machine Interaction, Systematics},
	pages = {1--6},
}

@inproceedings{friedenberger_train_2025,
	title = {A {Train} {Dispatcher} in the {Cloud} {Generated} from {RDF} {Models}},
	doi = {10.1109/ICSA-C65153.2025.00023},
	abstract = {In the context of fast systems and software development, combining model-based systems engineering with automated code generation is essential for managing complexity while enhancing efficiency and adaptability. This paper presents a model-based approach, where a composite RDF model is created, which serves as a flexible basis for the subsequent generation of various artifacts. The artifacts include not only source code but also technical configuration files (e.g. Dockerfiles, Kubernetes objects), CI/CD pipeline configurations, and documentation. The approach has been successfully applied to the Train Dispatcher in the Cloud (ZLiC), a cloud-based approach to digitalize the German Zugleitbetrieb. An iterative development process enabled continuous system expansion and adaptation to specific project requirements. The generated prototype has been validated through simulations and field tests, confirming the robustness and practical applicability of the approach.},
	booktitle = {2025 {IEEE} 22nd {International} {Conference} on {Software} {Architecture} {Companion} ({ICSA}-{C})},
	author = {Friedenberger, Dirk and Pirl, Lukas and Boockmeyer, Arne and Schmid, Robert and Polze, Andreas},
	month = mar,
	year = {2025},
	note = {ISSN: 2768-4288},
	keywords = {Model-based systems engineering, Resource description framework, Modeling, code generation, Documentation, Iterative methods, Computer architecture, Prototypes, Codes, Adaptation models, Pipelines, ontology-based systems engineering, railway, Source coding},
	pages = {111--119},
}

@inproceedings{davila-andino_identifying_2025,
	title = {Identifying {Vagueness} in {Model}-{Based} {Systems} {Engineering} {Theory} {Through} a {Materialist} {Ontology}},
	doi = {10.1109/SysCon64521.2025.11014868},
	abstract = {Model-Based Systems Engineering (MBSE) has had inconsistent implementation across industry and academia. This inconsistency is caused by different definitions of the term “system” have led to theoretical vagueness and contradictions that undermine the effectiveness of MBSE practices including metamodeling. By grounding MBSE in materialist philosophy, we aim to analyze vagueness in current definitions of “system.” We analyze the current definitions by using semantic analysis of decomposing their reference classes and intensions. Where reference classes determine the set of objects that a theory refers to, and intensions are the logical decomposition of a statement as it is written. Our findings reveal that the vagueness in current systems engineering theories is mostly due to the inability to resolve the ontological status of information. That is, current definitions do not clearly state how information interacts in the material world. Moreover, we have found internal inconsistencies and contradictions in current foundational theories, particularly with regard to emergence, interactions, and properties in both real and conceptual systems. We conclude that current systems engineering theories are not suitable for scientific practice. Moreover, we argue that adopting a materialist stance provides a precise ontological basis for defining systems, thereby reducing inconsistencies and enhancing the rigor of MBSE theories.},
	booktitle = {2025 {IEEE} {International} systems {Conference} ({SysCon})},
	author = {Davila-Andino, Arturo J. and Huang, Edward and Zaidi, Abbas K.},
	month = apr,
	year = {2025},
	note = {ISSN: 2472-9647},
	keywords = {Ontologies, Ontology, Semantics, Modeling, Systems Engineering, Model-Based Systems Engineering (MBSE), Mathematical models, Metamodeling, Semantic Analysis, Grounding, Industries, Systems engineering and theory, Philosophical considerations, System analysis and design, Systems Theory},
	pages = {1--8},
}

@inproceedings{xu_mbse-enhanced_2025,
	title = {A {MBSE}-{Enhanced} {Semantically} {Integration} {Method} for {Populating} {MDAO} {Design} {Processes}},
	doi = {10.1109/SysCon64521.2025.11014844},
	abstract = {With the development of the automotive market, the complexity of product portfolios is significantly increasing, while the frequency of market requirement changes is also rising. In this context, automotive OEMs face significant challenges in exploring feasible solutions for the next generation of vehicle development. Model-Based Systems Engineering (MBSE) effectively addresses these challenges by managing product complexity and establishing connections between requirements and solutions. However, an alternative solution often necessitates early verification \& validation (V\&V) and trade-off analysis across multiple disciplines. Multidisciplinary Design Analysis and Optimization (MDAO) has already been applied in solving such multidisciplinary challenges. MBSE is an effective approach for demonstrating multidisciplinary coupling relationships needed to meet specific requirements. By integrating MBSE with MDAO, it is possible to realize traceability from MDAO elements and results to the corresponding system elements and requirements, and to keep data consistency. This paper discusses a method of semantically linking MBSE and MDAO by establishing a MDAO domainspecific metamodel as a SysML profile. This approach positions MDAO as a viewpoint for vehicle development. MBSE serves as the overarching framework throughout different lifecycle stages, ensuring the consistency of MDAO specification. An instantiation of MDAO metamodel has been established to present a coupled multidisciplinary problem structure.},
	booktitle = {2025 {IEEE} {International} systems {Conference} ({SysCon})},
	author = {Xu, Tianxiao and Moalla, Néjib and Bentaha, Mohand-Lounes and Aktekin, Hazal and Agostinelli, Claudia},
	month = apr,
	year = {2025},
	note = {ISSN: 2472-9647},
	keywords = {Semantics, Modeling, Optimization, Data models, Model-Based Systems Engineering (MBSE), Metamodel, SysML, Complexity theory, Analytical models, Automotive engineering, Couplings, Early verification \& validation (V\&V), Hands, Multidisciplinary Design Analysis and Optimization (MDAO), Next generation networking},
	pages = {1--8},
}

@article{donald_semantic_2025,
	title = {A {Semantic} {Approach} for {Linked} {Model}, {Data}, and {Dataspace} {Cards}},
	volume = {13},
	issn = {2169-3536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005595465&doi=10.1109%2FACCESS.2025.3572211&partnerID=40&md5=39260fad811a4417f9ad46d34949c5fc},
	doi = {10.1109/ACCESS.2025.3572211},
	abstract = {In artificial intelligence, the significance of thorough documentation of models and datasets for publication is underestimated. However, due to the rising trend in the explainability and fairness of AI models, frameworks like Model Cards, Service Cards and Data Cards have emerged to facilitate understanding and reusing those models and datasets. Moreover, the Dataspace concept integrates these resources into Dataspace Cards, a comprehensive framework that systematically captures and organises crucial information to guide model and data selection for a specific application. This paper advocates a Semantic Web approach for transforming Model/Data Cards into Linked Data or knowledge graphs within a Dataspace, rendering them machine-readable and interoperable. A significant contribution is the development of a vocabulary that unifies Data, Model and Dataspace Card ontologies, enhancing consistent documentation and understanding of the Dataspace design. The paper further demonstrates the applicability of the proposed schema in various use cases, including bias detection in BERT-base-uncased and Large Language Models. Additionally, we propose a conceptual semantic approach, examined in-depth for sentiment and emotion analysis, to highlight how extended Dataspace Cards can improve applicability and outcomes. We found that this unified, ontology-driven approach results in more consistent metadata linking and more fine-grained bias detection in BERT-based-uncased than standalone documentation tools relying solely on Model or Data Cards. Furthermore, compared to existing frameworks, the richer interlinking capabilities of our proposed Dataspace Cards also facilitated easier traceability of performance outcomes, thereby ultimately fostering higher trustworthiness and reusability of AI resources.},
	journal = {IEEE Access},
	author = {Donald, Andy and Galanopoulos, Apostolos and Curry, Edward and Muñoz, Emir and Ullah, Ihsan and Waskow, M. A. and Kalra, Manan and Saxena, Sagar and Iqbal, Talha},
	year = {2025},
	note = {Type: Article},
	keywords = {Artificial intelligence, Semantic web, Linked data, Semantic Web, Semantics, Semantic approach, Vocabulary, Data models, Training, Data space, Europe, Documentation, Adaptation models, AI documentation, data cards, dataspace cards, model cards, service cards, Ontology's, Semantic-Web, Data cards, Dataspace card, Model card, Modeling data, Service card},
	pages = {110194--110207},
	annote = {Cited by: 0},
}

@inproceedings{almoqren_smart_2025,
	title = {A {Smart} {Framework} for {Optimizing} {User} {Feedback} {Prioritization} in {Application} {Development}},
	doi = {10.1109/ITIKD63574.2025.11004934},
	abstract = {In mobile app development, user reviews are a significant source of requirements. Users frequently report bugs, request new features, or suggest enhancements. Mobile app vendors aim to maximize user satisfaction by addressing these continuous comments and requests as early as possible. Typically, they prioritize delivering the most promising features, extracted from user reviews, in early releases while deferring fewer promising ones to later stages. However, due to the massive volume of reviews, redundancy, and conflicts among them, manually extracting requirements is inefficient and often challenging, making requirement prioritization even more difficult. Therefore, automating this process is essential. This paper presents a conceptual framework for the requirements prioritization process's automation, continuity, and scalability. The proposed framework follows a hybrid approach that integrates multiple advanced techniques: generative artificial intelligence, active learning, ontologies, and optimization algorithms. Generative artificial intelligence enables the identification of important patterns and the automatic extraction of requirements and their properties, which aids in assessing properties to determine requirement priorities. The generative artificial intelligence framework integrates with an active learning system to improve annotation efficiency. Ontologies help comprehend relationships, properties, and dependencies among requirements, aligning them with domain-specific knowledge. Optimization methods playa crucial role in the requirements prioritization process by computing the weights of various properties to identify the most effective combination of requirements and determine the optimal order for implementation. Consequently, this research presents a smart theoretical framework for enhancing user-driven maintenance and development of mobile applications. Researchers are tackling several critical challenges that remain unresolved in the field, with future directions focusing on evaluating its applicability and effectiveness in real-world scenarios.},
	booktitle = {2024 {International} {Conference} on {IT} {Innovation} and {Knowledge} {Discovery} ({ITIKD})},
	author = {Almoqren, Nuha and Alrashoud, Mubarak},
	month = apr,
	year = {2025},
	keywords = {Active learning, Ontologies, Generative AI, Feature extraction, Software, Optimization, Reviews, Mobile applications, Technological innovation, Smart Framework, Software Requirements, Mobile Applications, App Reviews, Software development management, Software measurement},
	pages = {1--8},
}

@article{ouriques_toward_2025,
	title = {Toward an {Ontology} of {Wargame} {Design}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3566249},
	abstract = {Governments rely on military power to address conflicts, manage crises, and adapt to the evolving nature of modern warfare, which is often characterized by uncertainty and disorder. Wargames, traditionally military tools for simulating conflicts and decision-making, have gained prominence in civilian applications, including business, cybersecurity, disaster management, and critical infrastructure protection. Despite their utility, designing wargames is a time-intensive process with significant challenges, such as scenario creation and decision modeling, necessitating structured and systematic approaches. This research formalizes wargame design through ontology-driven conceptual modeling, structuring its key concepts, characteristics, and elements. Ontologies provide a structured representation of knowledge, facilitating communication, knowledge management, and collaborative design. As a result, we developed core ontologies for wargame design based on the Unified Foundational Ontology (UFO) and implemented them using OntoUML. Our innovation lies in analyzing wargame design processes across various countries and military organizations to develop a comprehensive reference model for wargame design. Additionally, we are the first to apply UFO for conceptual modeling of the wargame domain. These ontologies enhance wargame design by fostering standardization, adaptability, and support for intelligent systems, enabling dynamic and responsive scenarios. These contributions enhance wargame design efficiency and effectiveness, applicable to military and civilian contexts.},
	journal = {IEEE Access},
	author = {Ouriques, Leandro and Barbosa, Carlos Eduardo and Kritz, Joshua and Xexéo, Geraldo},
	year = {2025},
	keywords = {Ontologies, Ontology, OntoUML, Uncertainty, Decision making, Training, Object recognition, Planning, Adaptation models, Systematics, Games, ontology-driven conceptual modeling (ODCM), Personnel, unified foundational ontology (UFO), wargame, wargame design, wargaming},
	pages = {78928--78958},
}

@inproceedings{cooper_demonstration_2025,
	title = {Demonstration of an {Open}-{Source} {ROS} 2 {Framework} and {Simulator} for {Situated} {Interactive} {Social} {Robots}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004875524&doi=10.1109%2FHRI61500.2025.10974109&partnerID=40&md5=15b7a25432821b279b8d4ef7c50051b2},
	doi = {10.1109/HRI61500.2025.10974109},
	abstract = {We introduce an open-source ROS 2 architecture for situated social robots, along with a simulator that allows mixed-reality development and interactions. The architecture is a hybrid symbolic/subsymbolic system that integrates explicit ontology semantics for perception, reasoning, and execution, with LLMs. It features multimodal social perception by leveraging the open source ROS4HRI framework; LLMs (both edge- and cloud-based) to facilitate natural language interaction between the user and system; KnowledgeCore, an open-source knowledge base, to reason about facts in the world; and an intent-based controller to supervise the execution of parallel/sequential tasks and skills. We demonstrate our system architecture with a social robot running the mixed-reality system.},
	booktitle = {2025 20th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	author = {Cooper, Sara and Ros, Raquel and Lemaignan, Séverin and Gebellí, Ferran and Ferrini, Lorenzo and Juričić, Luka},
	month = mar,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Knowledge acquisition, Ontologies, Semantics, Knowledge representation, Virtual reality, Mixed reality, Cognition, Knowledge based systems, Expert systems, Social robots, Multi-modal, Industrial robots, mixed-reality simulator, Natural languages, ROS 2 framework, Situated social robots, Systems architecture, Intelligent robots, Domain Knowledge, Mixed-reality simulator, Ontology semantics, Open-source, Situated social robot, Social perception, Sub-symbolic systems},
	pages = {1770--1772},
	annote = {Cited by: 0},
}

@article{kim_llm-assisted_2025,
	title = {{LLM}-{Assisted} {Ontology} {Restriction} {Verification} {With} {Clustering}-{Based} {Description} {Generation}},
	volume = {13},
	issn = {2169-3536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003435799&doi=10.1109%2FACCESS.2025.3562560&partnerID=40&md5=7fb7ff39e7103d7aba92020f3d6a17d9},
	doi = {10.1109/ACCESS.2025.3562560},
	abstract = {An ontology is a scheme for structuring relationships between concepts in a domain, promoting data interoperability and system integration. However, poorly designed ontologies can lead to errors and performance issues. While systems engineering has standardized evaluation guidelines (e.g., ISO/IEC), ontology engineering lacks such standards, leading to various independent evaluation methods. One frequent issue among novice developers is the misuse of ontology restrictions, particularly ‘allValuesFrom’ and ‘someValuesFrom’, which can significantly impact the correctness and reliability of ontologies. However, existing studies have not adequately addressed effective methods for detecting such errors. To address this gap, we propose a context-aware verification framework utilizing large language models to detect and correct misuse in ontology restrictions. Unlike conventional methods, our framework integrates contextual descriptions derived from ontological axioms, enabling more accurate verification. Additionally, we introduce a clustering-based description generation method that systematically organizes contextual information, further enhancing verification accuracy. Experimental evaluation conducted on diverse ontology datasets suggests that contextual integration improves verification performance. Moreover, the clustering-based description generation improves restriction misuse detection and correction compared to traditional approaches. By automating ontology restriction verification, this study contributes significantly to enhancing the reliability of ontology evaluation and provides a foundation for developing more scalable and standardized verification techniques.},
	journal = {IEEE Access},
	author = {Kim, Seungyeon and Kim, Donghyun and Hwang, Seokju and Lee, Kyong-Ho and Lee, Kyunghwa},
	year = {2025},
	note = {Type: Article},
	keywords = {Ontologies, Interoperability, Ontology, Semantics, Data interoperability, Ontology evaluation, System integration, Reliability, Software, Quality assessment, clustering, Scalability, Translation, Accuracy, text generation, Data systems, ISO Standards, IEC Standards, ontology restriction verification, Ontology's, Relationship between concepts, Ontology evaluations, Text generations, Clusterings, Ontology restriction verification, Performance issues},
	pages = {73603--73618},
	annote = {Cited by: 0},
}

@article{mishra_context-aware_2025,
	title = {Context-{Aware} {Embedded} {Language} {Transformers} for {Evaluating} {Climate} {Change}-{Based} {Sustainable} {Development} {Goals}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3559548},
	abstract = {This research work addresses the pressing issue of climate change and the urgent need for a comprehensive and reliable dataset that can assist stakeholders, policymakers and researchers in making informed and data-driven decisions. We propose the process for leveraging publicly available raw data such as news articles and social media posts and help to create meaningful ontologies. The study demonstrates the use of advanced Natural Language Processing and Deep Learning techniques to transform raw data into potential insights. Additionally, we present a methodology for utilizing these generated ontologies to anticipate the impacts of decisions on climate change. The primary objective of this research is to present a Context-Aware Embedded Language Transformers model that can be easily integrated into various pipelines by generating meaningful ontologies to support more informed decision-making. These ontologies will serve as knowledge graphs and contribute to a large dataset for future research in the field and address the current lack of comprehensive resources.},
	journal = {IEEE Access},
	author = {Mishra, Pratham and Narayanasamy, Senthil Kumar and Srinivasan, Kathiravan},
	year = {2025},
	keywords = {Ontologies, knowledge graphs, Sustainable development, Transformers, Decision making, Data models, Climate change, Databases, transformers, Surveys, natural language processing (NLP), Social networking (online), graph convolution networks (GCNs), Meteorology, sustainable development goals},
	pages = {65757--65775},
}

@inproceedings{coffelt_implementation_2025,
	title = {Implementation and {Application} of a {Knowledge} {Service} for {AUV} {Mission} {Explainability}},
	doi = {10.1109/UT61067.2025.10947388},
	abstract = {This paper presents a knowledge service aimed at enhancing mission explainability in subsea robotics operations. The proposed system consists of an AI agent that chains together specialized large language models (LLMs) and a graph database to enable natural language querying and interactive visualization. The graph database models entities and relationships relevant to subsea inspection and maintenance, such as clients, industries, sites, vehicles, sensors, and underwater scene elements, including pipeline components and seafloor characteristics. The browser-based GUI aims to allow stakeholders—including field teams, robot developers, industry clients, and regulatory agencies—to intuitively interact with mission data, supporting post-mission analysis and explainability. Using pipeline inspections as a case study, we illustrate the potential of this approach and discuss future developments needed to advance this framework toward a practical solution for subsea robotics.},
	booktitle = {2025 {IEEE} {Underwater} {Technology} ({UT})},
	author = {Coffelt, Jeremy Paul and Kampmann, Peter and Beetz, Michael},
	month = mar,
	year = {2025},
	keywords = {AI agent, Large language models, Inspection, knowledge representation and reasoning, Service robots, graph database, Robot sensing systems, Natural languages, Pipelines, Industries, Sensor phenomena and characterization, Visual databases, mission explainability, subsea robotics, Underwater technology},
	pages = {1--7},
}

@inproceedings{singh_llm-rspf_2025,
	title = {{LLM}-{RSPF}: {Large} {Language} {Model}-{Based} {Robotic} {System} {Planning} {Framework} for {Domain} {Specific} {Use}-cases},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003629658&doi=10.1109%2FWACV61041.2025.00707&partnerID=40&md5=f936765f3f0bf5e3170914af18fe83b6},
	doi = {10.1109/WACV61041.2025.00707},
	abstract = {The employment of large language models (LLMs) for task planning and reasoning has emerged as a focal point of interest within the robotics research community. However, directly applying LLMs, even with large token-sized prompts, does not achieve the task planning performance required for an industrial-grade domain-specific use-case (DSU). This work aims to overcome the obstacles of a robotic task planner for DSUs by introducing a novel planning framework, LLM-RSPF (Large Language Model-based Robotic System Planning Framework). Central to the LLM-RSPF is a novel robotic system ontology that organizes the components of the robotic system in a coherent and a systematic manner. The ontology empowers the LLM-RSP F to efficiently capture a contextual representation of the DSU using the LLMs. Subsequently, the research introduces a LLM-tuning regimen referred as chain of hierarchical thought (CoHT), specifically crafted to complement the proposed system ontology. Integrating these two components, the LLM-RSPF aims to enhance the accuracy, robustness, and throughput of a robotic system in a cost-effective manner. In addition, the research presents an empirical methodology to generate the LLM-tuning dataset size for a guaranteed performance. The LLM-RSPF is validated on a retail order-fulfillment use-case thereby, illustrating the efficacy of the framework. Through rigorous evaluation, the LLM-RSPF demonstrates exceptional performance on the generated dataset, effectively meeting the DSU objectives.},
	booktitle = {2025 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Singh, Chandan Kumar and Kumar, Devesh and Sanap, Vipul and Sinha, Rajesh},
	month = feb,
	year = {2025},
	note = {ISSN: 2642-9381},
	keywords = {large language model, Ontologies, Large language model, Language model, Task planning, Service robots, Robustness, Motion planning, Accuracy, Industrial robots, Planning, Robots, task planning, Systematics, coht, domain-specific use-case, robotic system ontology, Solid modeling, Three-dimensional displays, Throughput, Intelligent robots, Robotic systems, Domain specific, Coht, Domain-specific use-case, Planning framework, Robotic system ontology, System ontology},
	pages = {7277--7286},
	annote = {Cited by: 0},
}

@inproceedings{abolhasani_ontokgen_2025,
	title = {{OntoKGen}: {A} {Genuine} {Ontology} and {Knowledge} {Graph} {Generator} {Using} {Large} {Language} {Model}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002271575&doi=10.1109%2FRAMS48127.2025.10935139&partnerID=40&md5=a1dda94eab8edc38426e8bfe96221f84},
	doi = {10.1109/RAMS48127.2025.10935139},
	abstract = {Extracting relevant and structured knowledge from large, complex technical documents within the Reliability and Maintainability (RAM) domain is labor-intensive and prone to errors. Our work addresses this challenge by presenting OntoKGen, a Genuine pipeline for Ontology extraction and Knowledge Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through an interactive user interface guided by our adaptive iterative Chain of Thought (CoT) algorithm to ensure that the ontology extraction process and, thus, KG generation align with user-specific requirements. Although KG generation follows a clear, structured path based on the confirmed ontology, there is no universally correct ontology as it is inherently based on the user's preferences. OntoKGen recommends an ontology grounded in best practices, minimizing user effort and providing valuable insights that may have been overlooked, all while giving the user complete control over the final ontology. Having generated the KG based on the confirmed ontology, OntoKGen enables seamless integration into schemeless, non-relational databases like Neo4j. This integration allows for flexible storage and retrieval of knowledge from diverse, unstructured sources, facilitating advanced querying, analysis, and decision-making. Moreover, the generated KG serves as a robust foundation for future integration into Retrieval-Augmented Generation (RAG) systems, offering enhanced capabilities for developing domain-specific intelligent applications.},
	booktitle = {2025 {Annual} {Reliability} and {Maintainability} {Symposium} ({RAMS})},
	author = {Abolhasani, Mohammad Sadeq and Pan, Rong},
	month = jan,
	year = {2025},
	note = {ISSN: 2577-0993},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Large language model, Ontology, Language model, Large language models, Retrieval augmented generation, Prompt Engineering, Large Language Model, Neo4j, Prompt engineering, Ontology Extraction, Pipelines, Generators, Neo4J, Ontology and Knowledge Graph Generator, Random access memory, Reliability engineering, User interfaces, Structured Query Language, Ontology's, Ontology graphs, Query languages, Relational database systems, Graph generation, Ontology and knowledge graph generator},
	pages = {1--6},
	annote = {Cited by: 0},
}

@inproceedings{naik_protgat_2025,
	title = {{ProtGAT}: {Refining} {Antimicrobial} {Resistance} {Detection} with {Graph}-{Based} {Deep} {Learning}*},
	doi = {10.1109/ICAET63349.2025.10932144},
	abstract = {The rise of antimicrobial resistance (AMR) is a critical threat to global health, necessitating urgent advancements in diagnostic capabilities. Current methods for detecting AMR are hampered by slow processing times and a lack of precision, making early and accurate detection challenging. This paper introduces ProtGAT, a novel computational framework that combines ProteinBERT and Graph Attention Networks (GAT) to enhance the detection of AMR. Our model leverages deep learning to analyze complex protein sequences and their interactions, improving the accuracy of AMR predictions. The integration of ProteinBERT provides a robust feature extraction from protein sequences, while GAT focuses on identifying key relational features within these sequences. Early testing of ProtGAT demonstrates its superior performance over traditional models, particularly in identifying novel AMR sequences that evade conventional detection methods. This study highlights the potential of advanced computational approaches to revolutionize AMR diagnostics, offering faster and more reliable tools for healthcare providers in combating this growing public health concern.},
	booktitle = {2025 1st {International} {Conference} on {AIML}-{Applications} for {Engineering} \& {Technology} ({ICAET})},
	author = {Naik, Anushka and Naik, Akanksha and Deshmukh, Pratiksha},
	month = jan,
	year = {2025},
	keywords = {Deep learning, Feature extraction, Reliability, Computational modeling, Immune system, Accuracy, Testing, Refining, Public healthcare, Protein sequence, Antimicrobial Resistance, Computational Biology, Graph Attention Networks, Protein Sequence Analysis, ProteinBERT},
	pages = {1--6},
}

@inproceedings{desai_unleashing_2025,
	title = {Unleashing the {Potential} of {Ontology} in {Skill} {Extraction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002444956&doi=10.1109%2FICAET63349.2025.10932270&partnerID=40&md5=7af1c210443a2102ec5a3f838fb63953},
	doi = {10.1109/ICAET63349.2025.10932270},
	abstract = {Research reveals ontology's ability to extract information about skills from online job sites by giving a structured and semantically rich representation of skills. The study talks about more accurate and thorough skill profiling by systematically building ontological models that allow for an indepth knowledge of the complex links between skills, abilities, and domains. In this research, we study papers from different methods like supervised learning, unsupervised learning, and LLMs. The paper begins by providing not only an overview what is new in ontology generation but also its application in the context of skill extraction. It then delves into the challenges and opportunities associated with ontology-based skill extraction, highlighting the ways of processing natural language in bridging the gap between unstructured text and the formal representation of skills and competencies. Furthermore, the paper presents a comprehensive framework for ontology-driven skill extraction, emphasizing the importance of contextual awareness and the identification of implicit skills that may not be explicitly stated in the source text. The potential implications of this approach are manifold, as it could significantly impact various aspects of the talent management ecosystem.},
	booktitle = {2025 1st {International} {Conference} on {AIML}-{Applications} for {Engineering} \& {Technology} ({ICAET})},
	author = {Desai, Arun and Kulkarni, Anagha},
	month = jan,
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Ontology, Machine learning, Machine Learning, Data mining, Ontological modeling, Ontology generation, Skill, Federated learning, Self-supervised learning, Supervised learning, Unsupervised learning, Accuracy, Adversarial machine learning, skill extraction, skills, Natural languages, Biological system modeling, Ecosystems, Jobs, Manifolds, Ontology's, Machine-learning, Contrastive Learning, Extract informations, In-depth knowledge, Job, Online job sites, Skill extractions},
	pages = {1--6},
	annote = {Cited by: 0},
}

@article{xiahou_knowledge_2025,
	title = {Knowledge {Management} in {Construction} {Quality} {Management}: {Current} {State}, {Challenges}, and {Future} {Directions}},
	volume = {72},
	issn = {1558-0040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003157608&doi=10.1109%2FTEM.2025.3550354&partnerID=40&md5=bff39b79802f667a4826a8a3246f4bbf},
	doi = {10.1109/TEM.2025.3550354},
	abstract = {Construction quality management (CQM), as one of the major activities in construction project management, relies heavily on knowledge. Unfortunately, the knowledge of CQM is diverse in format and scattered in different stakeholders within the whole construction processes. Therefore, knowledge management (KM) of CQM is underinvestigated. To offering a comprehensive view of KM in CQM, this article employed a mixed review method to critically review 87 related articles. The results indicate 1) building information modeling, ontology, and natural language processing are identified as critical technologies in KM, 2) expert system and decision support, structural health monitoring, and project management are the major application domains. This article conducts an in-depth analysis of the literature based on the three phases of quality control: pre-construction, in-construction, and post-construction. The results are discussed to critically assess the critical technologies in KM. A framework is proposed to guide the effective implementation of KM in CQM, alongside a discussion of the current challenges and opportunities. The article further identifies potential development directions for KM in CQM, including total quality management, digital twins, development of large language models, construction of “No-cost” KM platforms, uniform evaluation and standardization mechanisms, tacit knowledge capture, and confidentiality and security. A novel paradigm for knowledge-driven quality management decision-making is first introduced. This article offers a comprehensive perspective on the application of KM in CQM, which will significantly enhance the effectiveness of CQM implementation in the future.},
	journal = {IEEE Transactions on Engineering Management},
	author = {Xiahou, Xiaer and Chen, Gaotong and Li, Zirui and Xu, Xin and Li, Qiming},
	year = {2025},
	note = {Type: Article},
	keywords = {Project management, Data mining, Decision making, Reviews, Stakeholders, Training, Building Information Modelling, Quality management, Complexity theory, quality control, Engineering management, Industries, Costs, Construction quality management (CQM), knowledge management (KM), knowledge-driven quality management, mixed review, Total quality management, Natural language processing systems, 'current, Construction process, Construction project management, Construction quality management, Critical technologies, Knowledge-driven quality management, Management IS, Mixed review, Ontology language},
	pages = {1069--1088},
	annote = {Cited by: 3},
}

@article{hosseini_leveraging_2025,
	title = {Leveraging {LLMs} and {Knowledge} {Graphs} to {Design} {Secure} {Automation} {Systems}},
	volume = {6},
	issn = {2644-1284},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003034847&doi=10.1109%2FOJIES.2025.3545811&partnerID=40&md5=78e6bb987f64de17e8595c90d94b9c7f},
	doi = {10.1109/OJIES.2025.3545811},
	abstract = {The digital transformation of Industrial Control Systems (ICSs) within the Industry 4.0 paradigm is essential for industrial organizations to remain competitive, while cybersecurity is an enabler. However, security measures, often implemented late in the engineering process, lead to costly and complicated implementations. Thus, this article is concerned with the “security by design” principle in ICSs and facilitates compliance with ICS security standards, which can be legally mandated for some critical systems or adopted by asset owners to protect their assets. Current methods for compliance demand manual efforts from security experts, making the compliance process time-consuming and costly. To address this, we propose a framework for leveraging large language models (LLMs) combined with knowledge graphs to automate the interpretation of security requirements and system architecture as two main elements of the design phase. Our knowledge graph-augmented LLM framework converts system architectures into human natural language, enhancing the automation of various security analyses, especially those that need to handle textual requirements. The framework enables validating applicable security requirements provided by IEC 62443-3-3 (a widely-used ICS security standard) concerning system designs through a question-and-answer interface. To evaluate the framework, various questions with reference responses from human experts were prepared in the context of a use case, and the quality of the LLMs' responses was measured across various metrics. Moreover, we compared the framework with a baseline approach based on formal queries. The results show that the proposed framework effectively automates security tasks and offers a user-friendly interface accessible to nonexperts.},
	journal = {IEEE Open Journal of the Industrial Electronics Society},
	author = {Hosseini, Ali M. and Kastner, Wolfgang and Sauter, Thilo},
	year = {2025},
	note = {Type: Article},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Large language model, Language model, Large language models, ontology, knowledge graph, Security, Security requirements, Requirements engineering, Cognition, Industrial control systems, Competition, Security by design, large language model (LLM), security by design, Natural languages, Computer security, Cyberattack, Systems architecture, IEC Standards, Industrial control system (ICS), Ontology's, Chemical plants, Concrete construction, Control system security, Industrial control system, Petroleum products, Security standards},
	pages = {380--395},
	annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{yang_cmvc_2025,
	title = {{CMVC}+: {A} {Multi}-{View} {Clustering} {Framework} for {Open} {Knowledge} {Base} {Canonicalization} {Via} {Contrastive} {Learning}},
	volume = {37},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2025.3543423},
	abstract = {Open information extraction (OIE) methods extract plenty of OIE triples {\textless} {\textless}noun phrase, relation phrase, noun phrase{\textgreater} {\textgreater} from unstructured text, which compose large open knowledge bases (OKBs). Noun phrases and relation phrases in such OKBs are not canonicalized, which leads to scattered and redundant facts. It is found that two views of knowledge (i.e., a fact view based on the fact triple and a context view based on the fact triple's source context) provide complementary information that is vital to the task of OKB canonicalization, which clusters synonymous noun phrases and relation phrases into the same group and assigns them unique identifiers. In order to leverage these two views of knowledge jointly, we propose CMVC+, a novel unsupervised framework for canonicalizing OKBs without the need for manually annotated labels. Specifically, we propose a multi-view CHF K-Means clustering algorithm to mutually reinforce the clustering of view-specific embeddings learned from each view by considering the clustering quality in a fine-grained manner. Furthermore, we propose a novel contrastive learning module to refine the learned view-specific embeddings and further enhance the canonicalization performance. We demonstrate the superiority of our framework through extensive experiments on multiple real-world OKB data sets against state-of-the-art methods.},
	number = {5},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Yang, Yang and Shen, Wei and Shu, Junfeng and Liu, Yinan and Curry, Edward and Li, Guoliang},
	month = may,
	year = {2025},
	keywords = {Ontologies, Information retrieval, Data mining, Knowledge based systems, Contrastive learning, Training, Organizations, contrastive learning, Clustering algorithms, Electronic mail, Indexes, multi-view clustering, Open knowledge base canonicalization},
	pages = {2296--2310},
}

@article{huang_semantic_2025,
	title = {A {Semantic} and {Intelligent} {Focused} {Crawler} based on {BERT} {Semantic} {Vector} {Space} {Model} and {Hybrid} {Algorithm} ({October} 2024)},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3542064},
	abstract = {The goal of a focused crawler is to selectively fetch pages that are relevant to a given topic. Previous crawlers use text content to determine text topic relevance and manually determined weighting factors to predict the priority of unvisited URLs. However, there are still some problems in the above focused crawler methods, the calculation formula of semantic similarity between words is flawed. The weighting factor for the priority of unvisited URLs is determined arbitrarily. In order to solve the above problems, this paper proposes a semantic and intelligent focused crawler based on BERT semantic vector space model and hybrid algorithm. This method used BERT semantic vector space model to calculate the topic relevance of documents, and used a hybrid algorithm to optimize the weighting factor of unvisited URL priority. The experimental results show that the proposed BSVSM-HA crawler can obtain better evaluation indicators compared with the other three crawlers including Word2vec crawler, ELMO crawler and BSVSM crawler. In conclusion, the semantic and intelligent crawler proposed in this paper makes the semantic similarity between terms more accurate, and improves the topic relevance of the text, and the optimized weighting factor makes the priority evaluation of unvisited URLs more accurate.},
	journal = {IEEE Access},
	author = {Huang, Wenhao and Zhang, Jiahao and Li, Xin and Zhou, Xiao and Qi, Deyu and Xi, Jianqing and Liu, Wenjun},
	year = {2025},
	keywords = {Ontologies, Semantics, Training, Accuracy, Hybrid Algorithm, Vectors, Search problems, Crawlers, Focused Crawler, Hypertext systems, Semantic Vector Space Model, Uniform resource locators, Web pages},
	pages = {1--1},
}

@article{kim_eileen_2025,
	title = {{EILEEN}: {A} {Multi}-{Modal} {Framework} for {Extracting} {Alcohol} {Consumption} {Patterns} {From} {Bilingual} {Clinical} {Notes}},
	volume = {13},
	issn = {2169-3536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217581897&doi=10.1109%2FACCESS.2025.3538803&partnerID=40&md5=f710ec51d3820a3fdbe46f672ad3bd96},
	doi = {10.1109/ACCESS.2025.3538803},
	abstract = {In this work, we introduce EILEEN (Efficient Inference for Language-based Extraction of EHR Notes), a novel multi-modal natural language processing (NLP) framework designed to extract various alcohol consumption patterns from unstructured clinical notes, particularly in bilingual and non-English contexts. Recent advances in NLP have significantly improved information extraction capability across various domains. However, identifying patterns of alcohol consumption in medical documents remains underexplored, with existing approaches heavily relying on traditional NLP methods such as bag-of-words models that require extensive text preprocessing. These methods are often limited to English-language clinical settings, where robust medical ontologies and NLP toolkits are available to support preprocessing tasks. Therefore, this limitation hinders their use in multilingual healthcare settings and in environments lacking robust NLP toolkits to facilitate preprocessing. Motivated by the need for a more generalizable and accurate approach, this paper investigates the impact of large language models (LLMs) in advancing alcohol consumption pattern extraction from clinical notes. By reducing the need for manual preprocessing and improving adaptability to multilingual clinical notes, this work aims to enable broader, more practical applications of NLP models in extracting alcohol consumption patterns from clinical notes. By fine-tuning multilingual language models along with additional data sources, EILEEN effectively analyzes unstructured electronic health records (EHR) without relying on traditional concept normalization or extensive text preprocessing resources. Furthermore, the multi-modal component of EILEEN enables it to integrate and leverage diverse types of alcohol-related information, such as various types and amounts of alcohol consumed by a patient, thereby improving its pattern extraction accuracy. Our experiments, conducted in two different medical institutions in Korea, demonstrate that EILEEN significantly outperforms existing NLP methods in accurately identifying clinically relevant alcohol consumption patterns. By providing accurate, detailed, and clinically useful alcohol consumption patterns from unstructured clinical notes, EILEEN empowers healthcare practitioners with actionable insights essential for informed clinical decision-making.},
	journal = {IEEE Access},
	author = {Kim, Han Kyul and Park, Yujin and Kim, Yoon Ji and Yi, Seungah and Park, Yeju and So, Sujin and Lee, Hyeon-Ji and Bae, Ye Seul},
	year = {2025},
	note = {Type: Article},
	keywords = {Ontologies, Ontology, Natural language processing, natural language processing, Multi-modal learning, Transformers, Feature extraction, Electronic health record, Data mining, Unified modeling language, Clinical notes, Accuracy, Clinical informatics, Language processing, multimodal learning, Natural languages, Multilingual, Vectors, alcohol information extraction, Hospitals, multilingual transformers, Natural language processing systems, Alcohol consumption, Alcohol information extraction, Consumption patterns, Multilingual transformer},
	pages = {25741--25751},
	annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{yhdego_automated_2025,
	title = {Automated {Ontology} {Generation} for {Zero}-shot {Defect} {Identification} in {Manufacturing}},
	issn = {1558-3783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217032682&doi=10.1109%2FTASE.2025.3537463&partnerID=40&md5=e5ab810de0ed81cdcfa98e466137c161},
	doi = {10.1109/TASE.2025.3537463},
	abstract = {A lack of labeled data presents a significant challenge to automatic defect identification in manufacturing, which is a crucial step in process control and certification during process development. State-of-the-art transfer learning is incapable of handling such zero-shot learning (ZSL) when defect labels are absent in training datasets. The latest research on ZSL leverages natural language processing (NLP) based on large language models (LLM) and shows promise by supplementing information to generate labels. However, its performance is hampered by the supporting LLMs pre-trained on generic vocabulary that failed to characterize manufacturing defects accurately. This paper establishes a methodology to automatically extract multi-level attributes from literature to improve defect representation, thereby facilitating ZSL. The extracted attributes contribute to a hierarchical knowledge graph, called defect ontology, to characterize multiple aspects of manufacturing defects. The proposed algorithm takes the defect images and associated text from the literature as input and develops an unsupervised method to identify the hierarchical relationships among the tokenized information extracted from the input text-feature corpora. The hierarchical graph is refined to retain the most relevant information by a pruning algorithm based on a minimum path search. A walk algorithm, along with NLP, parsed the generated ontology to create embedding of defects to enable zero-shot attribute learning to identify defects. The proposed method advances the ZSL methodology by automatically creating a hierarchical knowledge representation from literature and images to replace generic vocabulary in LLM adopted by ZSL algorithms, thus improving defect representation. The case studies are among the earlier attempts to demonstrate the feasibility of using literature data from public sources to extract attributes automatically to identify defects in a real additive manufacturing process based on direct-ink-writing.},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Yhdego, Tsegai O. and Wang, Hui},
	year = {2025},
	note = {Type: Article},
	keywords = {Ontologies, Knowledge graph, Ontology, Language model, Automation, Manufacturing, Certification, Vocabulary, Data mining, Smart manufacturing, Ontology generation, Transfer learning, Graph embeddings, Self-supervised learning, Semi-supervised learning, Process control, Unsupervised learning, Zero-shot learning, Accuracy, Language processing, Natural languages, Zero shot learning, defect identification, manufacturing automation, self-supervised learning, Ontology's, Natural language processing systems, Labeled data, Network security, Manufacturing data processing, Manufacturing Automation, Defect identification, Gluing, Hierarchical knowledge, Manufacturing defects},
	pages = {1--1},
	annote = {Cited by: 1},
}

@article{wang_enhancing_2025,
	title = {Enhancing {Protein} {Function} {Prediction} {Through} the {Fusion} of {Multi}-{Type} {Biological} {Knowledge} {With} {Protein} {Language} {Model} and {Graph} {Neural} {Network}},
	volume = {22},
	issn = {2998-4165},
	doi = {10.1109/TCBBIO.2025.3529301},
	abstract = {Proteins play crucial roles in diverse biological functions. Accurately annotating their functions is essential for understanding cellular mechanisms and developing therapies for complex diseases. Computational methods have been proposed as alternatives to labor-intensive and expensive experimental approaches. Existing computational methods have demonstrated that protein evolution information and Protein-Protein Interactions (PPIs) are essential for protein function prediction. However, traditional computational approaches for generating evolution information are time-consuming. On the other hand, proteins lacking interactions are ignored in previous studies. To address these limitations, we propose a novel deep learning framework, named DeepFMB, which incorporates multi-type biological knowledge. DeepFMB leverages a pre-trained protein language model to extract evolution information. Moreover, DeepFMB generates PPI-related features and orthology-related features using graph neural networks on the constructed PPI and orthology networks. Then, these multi-type features are fused adaptively for protein function prediction. Compared to eight state-of-the-art methods, DeepFMB outperforms all of them in terms of F-max and AUPR. Additionally, with the combination of sequence similarity-based inference, our predicted model predicts protein functions more accurately. Experimental results also validate the superior performance of our methods in predicting low-frequency GO terms. Ablation studies demonstrate that the multi-type biological knowledge we use is highly relevant to protein functions.},
	number = {2},
	journal = {IEEE Transactions on Computational Biology and Bioinformatics},
	author = {Wang, Wenkang and Shuai, Yunyan and Li, Yiming and Zeng, Min and Li, Min},
	month = mar,
	year = {2025},
	keywords = {Knowledge engineering, Feature extraction, Databases, Protein function prediction, Proteins, graph neural network, Biological system modeling, Predictive models, Protein sequence, Aggregates, Biological information theory, Evolution (biology), orthology relations, pre-trained protein language model, protein-protein interactions},
	pages = {581--590},
}

@article{wu_generative_2025,
	title = {A {Generative} {Modeling} {Method} for {Digital} {Twin} {Shop} {Floor}},
	volume = {29},
	issn = {1941-0131},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003040038&doi=10.1109%2FMIC.2024.3522301&partnerID=40&md5=536a6181f3b0e7ca577d71375281ee20},
	doi = {10.1109/MIC.2024.3522301},
	abstract = {Digital twin (DT) as a key enabling technology for achieving digitization, flexibility, and customization in shop floors has attracted significant attention. However, the shop floor involves diverse assets across multiple dimensions, scales, and interdisciplinary fields, making the modeling process complex. To address this issue, this article analyzes the construction process of ontology-based information models and proposes a generative modeling method for digital twin shop floor driven by large language models (LLMs). First, LLMs are utilized to analyze user intentions, acquiring the hierarchical object structure of DT models. Second, by combining an analysis–retrieval method to extract domain knowledge and generate dynamic prompts, LLMs are guided to realize the creation and fusion of objects and construct structured and semantically enriched DT models. Finally, the effectiveness of the proposed method is validated through examples of shop floor resource scheduling.},
	number = {1},
	journal = {IEEE Internet Computing},
	author = {Wu, Yanting and Sun, Yicheng and Wen, Xiaojian and Liu, Xiaoqiang and Bao, Jinsong and Wang, Sen},
	month = jan,
	year = {2025},
	note = {Type: Article},
	keywords = {Ontologies, Language model, Digital twins, Semantics, Context modeling, Data models, Computational modeling, Digitisation, Generative model, Internet, Natural languages, Object oriented modeling, Analytical models, Industrial facilities, Job shop scheduling, Customisation, Enabling technologies, Interdisciplinary fields, Machine shops, Model method, Modeling process, Multiple dimensions, Shopfloors},
	pages = {24--31},
	annote = {Cited by: 1},
}

@article{dalal_cross_2025,
	title = {A {Cross} {Attention} {Approach} to {Diagnostic} {Explainability} {Using} {Clinical} {Practice} {Guidelines} for {Depression}},
	volume = {29},
	issn = {2168-2208},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207401106&doi=10.1109%2FJBHI.2024.3483577&partnerID=40&md5=02c883078df94d7533d7f54f516e483e},
	doi = {10.1109/JBHI.2024.3483577},
	abstract = {The lack of explainability in using relevant clinical knowledge hinders the adoption of artificial intelligence-powered analysis of unstructured clinical dialogue. A wealth of relevant, untapped Mental Health (MH) data is available in online communities, providing the opportunity to address the explainability problem with substantial potential impact as a screening tool for both online and offline applications. Inspired by how clinicians rely on their expertise when interacting with patients, we leverage relevant clinical knowledge to classify and explain depression-related data, reducing manual review time and engendering trust. We developed a method to enhance attention in contemporary transformer models and generate explanations for classifications that are understandable by mental health practitioners (MHPs) by incorporating external clinical knowledge. We propose a domain-general architecture called ProcesS knowledge-infused cross ATtention (PSAT) that incorporates clinical practice guidelines (CPG) when computing attention. We transform a CPG resource focused on depression, such as the Patient Health Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable ontology using SNOMED-CT. With this resource, PSAT enhances the ability of models like GPT-3.5 to generate application-relevant explanations. Evaluation of four expert-curated datasets related to depression demonstrates PSAT’s application-relevant explanations. PSAT surpasses the performance of twelve baseline models and can provide explanations where other baselines fall short.},
	number = {2},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Dalal, Sumit and Tilwani, Deepa and Gaur, Manas and Jain, Sarika and Shalin, Valerie L. and Sheth, Amit P.},
	month = feb,
	year = {2025},
	note = {Type: Article},
	keywords = {Ontologies, Language model, artificial intelligence, language models, Diagnosis, Electronic health record, Depression, Clinical knowledge, Knowledge based systems, Unified modeling language, Clinical practice guidelines, knowledge, mental health, Health data, Explainable, Mental health, Systematized Nomenclature of Medicine, depression, language model, human, convolutional neural network, attention, Manuals, Computer architecture, Closed box, Cross attention, explainable, Guidelines, Medical treatment, PHQ-9, Article, scoring system, diagnostic test accuracy study, receiver operating characteristic, long short term memory network, clinical practice guideline, columbia suicide risk severity scale, cross attention approach, health practitioner, human experiment, On-line communities, Patient Health Questionnaire 9, suicidal ideation, suicide, suicide attempt},
	pages = {1333--1342},
	annote = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{yang_rdguru_2025,
	title = {{RDguru}: {A} {Conversational} {Intelligent} {Agent} for {Rare} {Diseases}},
	volume = {29},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2024.3464555},
	abstract = {Large language models (LLMs) hold significant promise in clinical practice, yet their real-world adoption is constrained by their propensity to produce erroneous and occasionally harmful outputs, particularly in the intricate domain of rare diseases (RDs). This study introduces RDguru, a conversational intelligent agent leveraging the LangChain framework and powered by GPT-3.5-turbo. RDguru offers a comprehensive suite of functionalities, encompassing evidence-traceable knowledge Q\&A and professional medical consultations for differential diagnosis (DDX), integrating authoritative knowledge sources and reliable tools. A novel multi-source fusion diagnostic model, rooted in deep Q-network, amalgamates three diagnostic recommendation strategies (GPT-4, PheLR, and phenotype matching) to enhance diagnostic recall during medical consultations. Through tailored tools and advanced algorithms for retrieval-augmented generation, RDguru excels in knowledge Q\&A, automated phenotype annotation, and RD DDX. A multi-aspect Q\&A analysis demonstrates RDguru outperforms ChatGPT in generating descriptions aligned with authoritative knowledge, quantified by ROUGE scores, GPT-4-based automatic rating, and RAGAs evaluation metrics. Testing on 238 published RD cases reveals that RDguru's top 5 multi-source fusion diagnoses recapture 63.87\% of actual diagnoses, marking a 5.47\% improvement over the state-of-the-art diagnostic method PheLR. Furthermore, RDguru's consultation strategy proves effective in eliciting diagnostically beneficial phenotypes and refining the prioritization of genuine diagnoses through multi-round phenotype-orient questioning. Evaluations against established benchmarks and real-world patient data demonstrate RDguru's efficacy and reliability, highlighting its potential to enhance clinical decision-making in the realm of RDs.},
	number = {9},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Yang, Jian and Shu, Liqi and Duan, Huilong and Li, Haomin},
	month = sep,
	year = {2025},
	keywords = {large language model, rare diseases, Bioinformatics, Knowledge engineering, Cognition, Phenotypes, Intelligent agents, Conversational AI, Medical diagnostic imaging, deep Q-network, Diseases, knowledge Q\&A, medical consultation},
	pages = {6366--6378},
}

@article{rezayi_exploring_2025,
	title = {Exploring {New} {Frontiers} in {Agricultural} {NLP}: {Investigating} the {Potential} of {Large} {Language} {Models} for {Food} {Applications}},
	volume = {11},
	issn = {2332-7790},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201463745&doi=10.1109%2FTBDATA.2024.3442542&partnerID=40&md5=5b2de44b44642ac5f1b62bf16a120c7f},
	doi = {10.1109/TBDATA.2024.3442542},
	abstract = {This paper explores new frontiers in agricultural natural language processing (NLP) by investigating the effectiveness of food-related text corpora for pretraining transformer-based language models. Specifically, we focus on semantic matching, establishing mappings between food descriptions and nutrition data through fine-tuning AgriBERT with the FoodOn ontology. Our work introduces an expanded comparison with state-of-the-art language models such as GPT-4, Mistral-large, Claude 3 Sonnet, and Gemini 1.0 Ultra. This exploratory investigation, rather than a direct comparison, aims to understand how AgriBERT, a domain-specific, fine-tuned, open-source model, complements the broad knowledge and generative abilities of these advanced LLMs in addressing the unique challenges of the agricultural sector. We also experiment with other applications, such as cuisine prediction from ingredients, expanding our research to include various NLP tasks beyond semantic matching. Overall, this paper underscores the potential of integrating domain-specific models like AgriBERT with advanced LLMs to enhance the performance and applicability of agricultural NLP applications.},
	number = {3},
	journal = {IEEE Transactions on Big Data},
	author = {Rezayi, Saed and Liu, Zhengliang and Wu, Zihao and Dhakal, Chandra and Ge, Bao and Dai, Haixing and Mai, Gengchen and Liu, Ninghao and Zhen, Chen and Liu, Tianming and Li, Sheng},
	month = jun,
	year = {2025},
	note = {Type: Article},
	keywords = {ChatGPT, Natural language processing, Language model, Semantics, natural language processing, Modeling languages, language models, Context modeling, Data models, Training, Semantic matching, semantic matching, Language processing, Natural languages, Biological system modeling, food applications, Task analysis, Context models, Food applications},
	pages = {1235--1246},
	annote = {Cited by: 11},
}

@article{gratius_parameter_2025,
	title = {A {Parameter} {Ontology} for {Simulation} {Models} of {Deep} {Space} {Habitats}},
	volume = {61},
	issn = {1557-9603},
	doi = {10.1109/TAES.2024.3440967},
	abstract = {Deep space habitat operations will depend less on ground support than in low Earth orbit because of increasing communication delays. For the crew to be more autonomous, some of the resources dedicated to anomaly response mechanisms must be transferred from the ground to the habitat. Simulation models replicating the behavior of the system are examples of such resources as they can inform decisions to select mitigation strategies. Multiple models are typically developed for a single spacecraft as many domains of expertise and stakeholders are involved. Enabling simulation onboard a habitat will therefore require integration efforts to provide system-wide capabilities. Specifically, model parameters are model components that must be regularly calibrated to accurately represent the system state during operation. Depending on the type of simulation, these can differ greatly in their nature, thereby making difficult the tasks of creating, reading, updating, and deleting parameters. There is no standard specifying how to represent simulation parameters in a multimodel setting. Model ontologies have been shown to support integration, but their representation of model parameters is usually limited, i.e., the required type of parameter attributes and their hierarchical relationships are unknown. For example, quantifying the uncertainties in parameter values is essential in deep space operations but it is unclear how such information should be formally represented. In this article, we propose a parameter ontology extending existing standards with attributes enabling probabilistic parameter identification. This article is validated by comparing related work with the responses of the proposed ontology to competency questions and by evaluating its consistency, conciseness, completeness, and expandability.},
	number = {1},
	journal = {IEEE Transactions on Aerospace and Electronic Systems},
	author = {Gratius, Nicolas and Bergés, Mario and Akinci, Burcu},
	month = feb,
	year = {2025},
	keywords = {Ontologies, ontology, Autonomous system, vocabulary, Unified modeling language, knowledge base, Computational modeling, Buildings, Task analysis, calibration, Calibration, parameter, simulation models, space habitat, Space technology},
	pages = {61--75},
}

@article{ji_ontology_2025,
	title = {Ontology {Versioning} for {Managing} {Inconsistencies} in {Engineering} {Models} {Arising} {From} {Model} {Changes} in the {Design} of {Intralogistics} {Systems}},
	volume = {22},
	issn = {1558-3783},
	doi = {10.1109/TASE.2024.3362599},
	abstract = {The interdisciplinary design of intralogistics systems (ILS) involves engineers from various disciplines, resulting in the generation of discipline-specific model files with overlapping information. For instance, a conveyor system can be represented from various perspectives, such as 3D-CAD models that capture its geometric information and discrete-event simulation models that depict the system’s dynamic material flow performance. The growing demands for flexible reconfigurability and adaptability in intralogistics systems necessitate frequent updates to engineering models. However, these updates often result in potential model inconsistencies due to insufficient stakeholder communication. Detecting the impact of model changes and related inconsistencies is challenging in practice due to data heterogeneity and complex inter-model relations. To address these challenges, we propose an ontology-versioning approach that automates the identification of inconsistencies resulting from model changes. Our approach facilitates the integration of heterogeneous model data, enables database versioning, detects inconsistencies caused by model updates, and provides traceability for identified issues. The concept is evaluated utilizing models from a prototypical implementation on a lab-sized demonstrator. Note to Practitioners—In the industry, the current development of intralogistics systems often lacks automated synchronization of overlapping model information and consistent model interfaces, frequently leading to contradictions among the models. This has been identified as a significant source of errors in the design of both industrial and academic intralogistics systems, as revealed by a study involving intralogistics experts from different technical disciplines. Effectively managing model inconsistencies is crucial for project success, particularly when frequent model changes occur. A promising approach to tackle this issue is to systematically link model data from different disciplines, through which model inconsistencies caused by inadequate communication among engineers can be identified and prevented. However, in many cases, changes in different model versions and their resulting inconsistencies are not adequately considered. To address this issue, we propose a concept based on ontology versioning that allows for the generation, comparison, and analysis of different versions of an ontological model database. This concept automatically identifies model changes, assesses their impacts on other models, and provides information to assist engineers in problem-solving. The effectiveness of our approach is assessed through an evaluation of three representative change scenarios, simplified from real-world use cases. In future research, we plan to extend the approach to general production systems and incorporate industrial-scale models from the broad range of disciplines involved in the design process.},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Ji, Fan and Vogel-Heuser, Birgit and Schypula, Rafael and Wünnenberg, Maximilian and Goedicke, Michael and Fottner, Johannes},
	year = {2025},
	keywords = {Ontologies, Modeling, Unified modeling language, Data models, Adaptation models, Analytical models, Solid modeling, inconsistency management, Intralogistics, model change management},
	pages = {1249--1261},
}

@article{fu_kg4nh_2025,
	title = {{KG4NH}: {A} {Comprehensive} {Knowledge} {Graph} for {Question} {Answering} in {Dietary} {Nutrition} and {Human} {Health}},
	volume = {29},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2023.3338356},
	abstract = {It is commonly known that food nutrition is closely related to human health. The complex interactions between food nutrients and diseases, influenced by gut microbial metabolism, present challenges in systematizing and practically applying knowledge. To address this, we propose a method for extracting triples from a vast amount of literature, which is used to construct a comprehensive knowledge graph on nutrition and human health. Concurrently, we develop a query-based question answering system over our knowledge graph, proficiently addressing three types of questions. The results show that our proposed model outperforms other state-of-art methods, achieving a precision of 0.92, a recall of 0.81, and an F1 score of 0.86 in the nutrition and disease relation extraction task. Meanwhile, our question answering system achieves an accuracy of 0.68 and an F1 score of 0.61 on our benchmark dataset, showcasing competitiveness in practical scenarios. Furthermore, we design five independent experiments to assess the quality of the data structure in the knowledge graph, ensuring results characterized by high accuracy and interpretability. In conclusion, the construction of our knowledge graph shows significant promise in facilitating diet recommendations, enhancing patient care applications, and informing decision-making in clinical research.},
	number = {3},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Fu, Chengcheng and Pan, Xueli and Wu, Jieyu and Cai, Junkai and Huang, Zhisheng and van Harmelen, Frank and Zhao, Weizhong and Jiang, Xingpeng and He, Tingting},
	month = mar,
	year = {2025},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Semantics, Text mining, text mining, nutrition, question answering, Question answering (information retrieval), Diseases, Task analysis, human diseases},
	pages = {1793--1804},
}

@article{gao_reusability_2025,
	title = {Reusability, {Reconfigurability} and {Efficiency} {Optimization} of {Satellite} {Network} {Modeling} and {Simulation}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3250426},
	abstract = {Model-based system engineering (MBSE) with reusable mechanisms can serve as an effective way for complex system architecture design. Stakeholder needs should be satisfied while product and architecture design need to be consistent with user requirements in all stages during the whole product lifecycle. In this paper, satellite network as an example of complex system is modeled in a reusable, reconfigurable and efficient manner using the system modeling language (SysML) together with pattern viewpoints and simulation constructs. Based upon abstract syntax described using metamodels and a set of profiles, concept reusability is established for the specific domain. Additionally a reusable modeling framework is developed with tailored design patterns and multiple viewpoints. Analysis metamodel, profile and interface are further presented to preserve reusability during iterations among multiple optimization rounds. A novel satellite network simulation model is formulated and multi-objective optimization is solved by transformation under practical application scenarios. A set of metrics are designed to assess and validate the models. Results show that the proposed reusable model has viewpoint coverage of more than 80 percent compared to a half for the baseline OOSEM model. The proposed model thus covers the pattern viewpoints and ontologies in a wider and more frequent way and is more efficient. Design choices made based on the model can be incorporated into this mechanism which is extensible along the system lifespan.},
	journal = {IEEE Access},
	author = {Gao, Su and Cao, Yue and Li, Yinqiao and Chen, Yujun and Jin, Song and Xie, Shikun and Liu, Jihong},
	year = {2025},
	keywords = {Modeling, simulation, Software, Optimization, Unified modeling language, Reusability, Domain specific languages, Computer architecture, Object oriented modeling, domain specific model, multi-objective optimization, pattern viewpoint, satellite network, Satellites},
	pages = {122035--122058},
}

@inproceedings{shah-mohammadi_comparative_2024,
	title = {Comparative {Analysis} of {Rule}-{Based} and {Large} {Language} {Model}-{Based} {Approaches} in {Addressing} {Variability} in {Clinical} {Outcome} {Reporting}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013686114&doi=10.1109%2FIEMCON62851.2024.11093511&partnerID=40&md5=12b4c2a2fd16fd5c87a7c19a6b4dd54d},
	doi = {10.1109/IEMCON62851.2024.11093511},
	abstract = {In clinical trials, varied terminologies and definitions often obscure the clarity and consistency needed to interpret results effectively. The ability to standardize clinical outcome reports and align semantically similar outcomes is crucial in healthcare and research, as inconsistencies can impede the comparability of trial results, complicating metaanalyses and informed decision-making. This research focuses on minimizing variability in the reporting of outcome measures through a comparative analysis of rule-based and advanced language modeling techniques. The rule-based method employs established ontologies, while the language model-based approach utilizes large language models. Findings indicate a low linkage of outcomes to traditional rule-based ontology, particularly for three-word outcomes, and underscore large language models’ efficacy in recognizing semantically similar outcomes across varying word counts. This supports the critical role of large language models in harmonizing outcome data, reducing redundancies, and improving data interoperability in clinical research contexts.},
	booktitle = {2024 {IEEE} 15th {Annual} {Information} {Technology}, {Electronics} and {Mobile} {Communication} {Conference} ({IEMCON})},
	author = {Shah-Mohammadi, Fatemeh and Finkelstein, Joseph},
	month = oct,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Large language model, Interoperability, Ontology, Terminology, Language model, Large language models, GPT, Semantics, Modeling languages, Decision making, Redundancy, Clinical trials, SBERT, Computational linguistics, Clinical trial, Pipelines, Analytical models, Mobile communication, Outcome alignment, Semantic variability, Model based approach, Clinical research, Rule based, Medical applications, Comparative analyzes},
	pages = {039--045},
	annote = {Cited by: 0},
}

@inproceedings{racharak_automated_2024,
	title = {An {Automated} {Medical} {Rdf} {Knowledge} {Graph} {Construction} {From} {Text} {Using} in-{Context} {Learning}},
	doi = {10.1109/KSE63888.2024.11063495},
	abstract = {The parameterized knowledge within large language models (LLMs), like ChatGPT, offers a significant opportunity for modelling domain knowledge base from text. However, LLMs' context sensitivity can hinder obtaining precise and taskaligned outcomes, thus requiring a suitable design for leveraging prompt engineering. This study explores the efficacy of different prompting methods for RDF knowledge graph construction from medical documents as our preliminary investigation, aiming to develop an efficient pipeline for a large-scale automatic knowledge graph construction according to semantic web standards and technologies. The results show that leveraging in-context learning within LLMs is capable of extracting an array of precise RDF triples from text. We perform a qualitative analysis of the extracted triples with different prompt templates, giving insights that could guide potential development in the research field.},
	booktitle = {2024 16th {International} {Conference} on {Knowledge} and {System} {Engineering} ({KSE})},
	author = {Racharak, Teeradaj and Wang, Tongyu and Jearanaiwongkul, Watanee},
	month = nov,
	year = {2024},
	note = {ISSN: 2694-4804},
	keywords = {Ontologies, Knowledge graphs, Large language models, Resource description framework, Knowledge engineering, Prompt engineering, Prompting, Standards, OpenAI, Few-shot, Sensitivity, Resource Description Framework, Pipelines, Systems engineering and theory, Generative Knowledge Graph Extraction},
	pages = {465--471},
}

@inproceedings{haw_od-sif_2024,
	title = {{OD}-{SIF}: {An} {Ontology}-{Driven} {Schema} {Integration} {Framework} for e-{Commerce} {Platform}},
	doi = {10.1109/CEII65291.2024.00046},
	abstract = {Automated data mapping is crucial in modern e-commerce ensuring seamless integration of diverse and heterogeneous datasets when migration from legacy systems to advanced platforms without any data loss or corruption. OD-SIF addresses these challenges by leveraging ontologies to unify data semantics, ensuring accurate and consistent schema integration. This framework excellently enhances the e-commerce operation by harmonizing product attributes such as brand, category, and specifications with a high precision of 93\% and recall of 88\%, completely surpassing baseline methods by 15-20\% in the Fl-score. Due to OD-SIF's semantic matching capabilities, it can easily manage noisy and incomplete data with less dependency on manual intervention. On the other hand, special ontologies may further refine the limits in handling niche domains. While improving accuracy, OD-SIF allows real-time data enrichment and ensures cross-platform interoperability that supports core functionalities like inventory management, customer data processing, and transaction across diverse systems. All these advantages make OD-SIF a key enabler for digital transformation in e-commerce bridging various platforms with payment processors, logistics providers, and marketing tools into a unified efficient ecosystem.},
	booktitle = {2024 7th {Asia} {Conference} on {Cognitive} {Engineering} and {Intelligent} lnteraction ({CEII})},
	author = {Haw, Su-Cheng and Ng, Kok-Why and J, Jayapradha and Naveen, Palanichamy},
	month = dec,
	year = {2024},
	keywords = {Ontologies, Semantics, ontology, mapping, Electronic commerce, Accuracy, Manuals, Real-time systems, automated data mapping, e-Commerce, Inventory management, Logistics, Noise measurement, Program processors, schema-based},
	pages = {196--201},
}

@inproceedings{wang_research_2024,
	title = {Research on the {Construction} of {Knowledge} {Graph} for {Emergency} {Management} of {Social} {Security} {Events}},
	doi = {10.1109/ICISE-IE64355.2024.11025395},
	abstract = {Constructing a knowledge graph for social security event emergency management promotes the development of social security event emergency management towards an intelligent model that relies on a vast amount of knowledge and data. Relevant content is collected from multiple information sources, and the UIE model, fine-tuned with a small amount of annotated data, is utilized to achieve joint entity-relation extraction. The results are stored in a Neo4j graph database, laying the foundation for application and analysis. The research concludes that utilizing LLM-KG fine-tuning provides a feasible means for constructing large-scale knowledge graphs in vertical domains with low resources, The knowledge graph obtained from this research can satisfy application scenarios in many situations.},
	booktitle = {2024 5th {International} {Conference} on {Information} {Science} and {Education} ({ICISE}-{IE})},
	author = {Wang, Xuefeng and Hu, Xiaoqing and Li, Dongsheng and Luo, Xuan},
	month = dec,
	year = {2024},
	keywords = {Knowledge graphs, Semantics, knowledge graph, Security, Data models, Organizations, Information science, Planning, Standards organizations, emergency management, Emergency services, Public security, social security events},
	pages = {616--619},
}

@inproceedings{xie_ontology_2024,
	title = {Ontology {Embeddings} for {Subsumption} {Prediction} {Based} on {Graph} {Language} {Model}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007428875&doi=10.1109%2FDSInS64146.2024.10992171&partnerID=40&md5=2b1f28e91fc4db4db48fd3da725f8872},
	doi = {10.1109/DSInS64146.2024.10992171},
	abstract = {With the growing importance of knowledge graphs in artificial intelligence, accurately modeling hierarchical relationships in ontologies has become a critical issue in knowledge representation learning. To address this, this paper proposes an ontology embedding framework based on graph language models, named GLMSubs, aimed at enhancing the prediction of subclass relationships. The GLMSubs framework adopts a two-stage strategy of “multi-semantic view partitioning” and “advanced training of graph language models”. Initially, it deconstructs the ontology's concepts, attributes, and instance information into five types of semantic views, such as class hierarchy view and class-attribute relationship view, through a multi-view partitioning mechanism, comprehensively capturing information from different semantic dimensions. Subsequently, the framework employs graph language models for joint training on the multi-view data to obtain embeddings that integrate both semantic and structural information. Experiments on datasets such as FoodOn and GO validate the effectiveness of GLMSubs, demonstrating that its performance in class hierarchy relationship prediction tasks significantly surpasses existing methods.},
	booktitle = {2024 4th {International} {Conference} on {Digital} {Society} and {Intelligent} {Systems} ({DSInS})},
	author = {Xie, Jiangcun and Li, Ren and Yang, Jianxi and Xiao, Qiao},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Language model, Semantics, OWL ontology, OWL, Resource description framework, knowledge graph, Embeddings, Knowledge representation learning, Training, Proteins, Biological system modeling, Predictive models, Medical services, graph language models, knowledge representation learning, Ontology's, Knowledge-representation, Image representation, Graph languages, OWL ontologies, Graph language model, Semantic views},
	pages = {133--137},
	annote = {Cited by: 0},
}

@inproceedings{zaeifi_deeper_2024,
	title = {Deeper and {Deeper}: {A} {Lightweight} {Semi}-{Supervised} {Deep} {Reinforcement} {Adaptive} {Learning}-{Based} {Ontology} {Alignment}},
	doi = {10.1109/AIxDKE63520.2024.00012},
	abstract = {Ontology alignment, also known as ontology matching, is pivotal for addressing semantic heterogeneity on the Semantic Web. Essentially, it entails linking entities across different ontologies or knowledge graphs in order to resolve ambiguity and enhance the interoperability of data. While various techniques exist, many still rely on rule-based or logic-based approaches, often requiring human intervention and domain specificity. Despite these challenges, ontology alignment remains crucial for seamlessly integrating disparate knowledge sources and facilitating effective data integration. In this paper, we tackle the limitations of current ontology alignment models by introducing a novel, lightweight, semi-supervised deep reinforcement learning model called Deep Reinforcement Adaptive Learning for Ontology Alignment (DRAL-OA). The DRAL-OA method incorporates both syntactic and structural information into the training phase. In addition, this approach is semi-supervised, utilizing a portion of the training data and automatically generating the rest, which reduces the need for human intervention. Moreover, DRAL-OA uses non-domain-specific language models to ensure broad applicability and reduce the need for extensive domain expertise. We evaluate our proposed approach using two datasets from the Ontology Alignment Evaluation Initiative (OAEI). In our experiments, we have shown that the proposed model can achieve high-quality alignments with F-measures on par with other state-of-the-art systems, all while maintaining a very short runtime and a compact model size.},
	booktitle = {2024 {International} {Conference} on {AI} x {Data} and {Knowledge} {Engineering} ({AIxDKE})},
	author = {Zaeifi, Mehrnoosh and Mosallanezhad, Ahmadreza and Bansal, Srividya},
	month = dec,
	year = {2024},
	note = {ISSN: 2831-7203},
	keywords = {Ontologies, Knowledge graphs, Semantic Web, Semantics, Knowledge engineering, semantic web, knowledge graph, Training, ontology alignment, reinforcement learning, Training data, Syntactics, Adaptation models, Runtime},
	pages = {28--35},
}

@inproceedings{soularidis_llm-assisted_2024,
	title = {{LLM}-{Assisted} {Generation} of {SWRL} {Rules} from {Natural} {Language}},
	doi = {10.1109/AIxDKE63520.2024.00008},
	abstract = {Recently, Large Language Models (LLMs) have attracted great attention due to their remarkable performance in human-like text generation and reasoning skills (although their memory and hallucination problems still remain key issues to tackle more efficiently). LLMs have been applied to various application domains, including Knowledge Graph (KG) generation, question and answering over KGs and text-to-SPARQL translation. In this work, we investigate the capabilities of LLMs in text-to-SWRL translation, i.e., translation of Natural Language (NL) rules into Semantic Web Rule Language (SWRL) rules, put in the context of an industrial Ontology Engineering (OE) environment called GLUON, presenting our first experimental results. The aim of this work is to identify the level of automation that is adequate for the LLM to generate well-formed SWRL rules, towards the development of an LLM-based framework, as a plugin to the GLUON OE environment. In this direction we leverage and combine the reasoning capabilities of GPT-4o model, the Retrieval-Augmented Generation (RAG) technology, and prompt engineering. We employ quantitative and qualitative metrics to evaluate the generated SWRL rules, focusing on the correct syntax and the level of human intervention.},
	booktitle = {2024 {International} {Conference} on {AI} x {Data} and {Knowledge} {Engineering} ({AIxDKE})},
	author = {Soularidis, Andreas and Kotis, Konstantinos and Lamolle, Myriam and Mejdoul, Zakaria and Lortal, Gaëlle and Vouros, George},
	month = dec,
	year = {2024},
	note = {ISSN: 2831-7203},
	keywords = {Ontologies, Automation, Large language models, Semantic Web, Retrieval augmented generation, SWRL, Ontology Engineering, Prompt engineering, Cognition, Retrieval-Augmented Generation (RAG), Translation, Syntactics, Large Language Models (LLM), Memory management},
	pages = {7--12},
}

@inproceedings{n_enhanced_2024,
	title = {Enhanced {Named} {Entity} {Recognition} in {Medical} {Texts} {Using} {Transformer}-{Based} {Models}},
	doi = {10.1109/SCOPES64467.2024.10990753},
	abstract = {The increase of digital medical text data which includes electronic health records (EHRs), clinical notes, and medical literature, gives an invaluable resource for advancing healthcare. As the data is unstructured, it is challenging to extract valuable insights from the data. To extract these insights there are models such as Rule-based and dictionary-based. These existing models face the drawback of handling ambiguity of words, out-of-vocabulary words. The proposed approach leverages advanced Natural Language Processing techniques, specifically state-of-the-art transformer-based models like BioBERT and ClinicalBERT, to perform Named Entity Recognition in the medical domain. By integrating additional domain-specific resources, including comprehensive medical terminologies and ontologies, we enhance the performance of entity recognition. The objective is to accurately identify and classify key medical entities from diverse medical text sources. The system has been evaluated using the metric accuracy, precision, recall and F1 score. The achieved F1 score is 91.5. The resulting structured information can be utilized in numerous applications like clinical decision support, patient data management and medical research.},
	booktitle = {2024 2nd {International} {Conference} on {Signal} {Processing}, {Communication}, {Power} and {Embedded} {System} ({SCOPES})},
	author = {N, Sushma Rani and CH, Dhawaleswar Rao and P, Srinivasa Rao},
	month = dec,
	year = {2024},
	keywords = {Ontologies, Terminology, Named entity recognition, Transformers, transformers, Named Entity Recognition, medical records, Measurement, Object recognition, Face recognition, Biological system modeling, Medical services, Signal processing},
	pages = {1--5},
}

@inproceedings{balaadich_toward_2024,
	title = {Toward an {Advanced} {Rural} {Tourism} {Ontology} for {Enhancing} {Visitor} {Experiences} in {Morocco}’s {Draa}-{Tafilalet} {Region}},
	doi = {10.1109/ICCTA64612.2024.10974777},
	abstract = {Integrating ontologies in tourism applications represents a transformative step toward achieving a structured digital transformation to enhance the competitive edge of tourism destinations. In Morocco’s tourism sector, particularly in the culturally and naturally rich Draa-Tafilalet region, implementing an ontology is essential to organize diverse tourism-related data and provide a more personalized and accessible visitor experience. Despite Morocco’s significant efforts, the tourism industry continues to face persistent challenges in delivering customized and accessible information aligned with tourists' preferences and needs. To address this issue, this study develops comprehensive ontology using a semi-automated methodology that combines natural language processing, language models, and expert validation. The ontology encapsulates attractions, services, and visitor preferences specific to the region. The main objective is to modernize and enhance the informational structure of the tourism sector, facilitating better navigation for tourists and supporting the strategic promotion of the region’s assets.},
	booktitle = {2024 34th {International} {Conference} on {Computer} {Theory} and {Applications} ({ICCTA})},
	author = {Balaadich, Youness and Jakimi, Abdeslam},
	month = dec,
	year = {2024},
	note = {ISSN: 2770-6575},
	keywords = {Ontologies, Ontology, Large language models, Machine learning, Digital transformation, Mobile applications, Stakeholders, Chatbots, Digital Transformation, Faces, Navigation, Personalized Experiences, Rural Tourism, Tourism industry},
	pages = {74--79},
}

@inproceedings{baidya_toward_2024,
	title = {Toward {Fine}-{Tuning} {Large} {Language} {Models} in {Ontology} of {Microbial} {Phenotypes} {Construction}},
	doi = {10.1109/BIBM62325.2024.10947604},
	abstract = {Ontologies are crucial for organizing domainspecific knowledge in biomedical fields, but their manual construction is time-consuming. This study explores the automation of ontology learning using large language models (LLMs) like BERT, RoBERTa, and DistilBERT, focusing on the Ontology of Microbial Phenotypes (OMP). We investigate three key tasks: (1) entity extraction, (2) relation extraction between entities, and (3) ontology verification. These tasks align with broader applications in biomedical annotation and named entity recognition (NER) by enabling the identification and structuring of key terms and relationships within microbial phenotypes. We evaluate LLMs in two scenarios: baseline performance using pre-trained models and fine-tuned performance after training on OMP-specific data. Our approach integrates spaCy for entity extraction, Llama 2 for relation identification, and LLMs for ontology verification. Experiments reveal that fine-tuned models significantly improve accuracy, precision, recall, and F1 scores, particularly for ontology verification. This research highlights the potential of LLMs to enhance ontology learning and support related biomedical applications like biofilm analysis, annotation, and NER, while emphasizing the value of expert curation.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Baidya, Anushuya and Do, Tuyen and Gnimpieba, Etienne Z.},
	month = dec,
	year = {2024},
	note = {ISSN: 2156-1133},
	keywords = {Ontologies, Large Language Models, Named entity recognition, Large language models, Annotations, Fine-tuning, Ontology Learning, Data models, Training, Phenotypes, Manuals, Focusing, Biological system modeling, Microbial Phenotypes, Ontology Verification},
	pages = {6913--6920},
}

@inproceedings{ge_research_2024,
	title = {Research and {Application} of {Electronic} {Data} {Retrieval} in {Material} {Supply} {Chain} {Enhanced} by {Large} {Language} {Models} and {Knowledge} {Graph}},
	doi = {10.1109/ITCEM65710.2024.00037},
	abstract = {In response to the new goals of building green and modern smart supply chains, the electric power equipment supply chain is experiencing a shift toward digital intelligence and low-carbon, environmentally friendly development [1]. However, traditional search platforms based on relational databases face challenges in handling vast amounts of multimodal electronic data. These platforms often suffer from low search accuracy, limited cross-dimensional correlation analysis capabilities, and inefficiencies, making them inadequate for constructing comprehensive big data platforms that integrate and share information across the entire green, modern, smart supply chain. This paper introduces an innovative electronic data retrieval method designed to meet the data retrieval needs of material supply chains. By integrating large language models with knowledge graph technology, it proposes an electronic data retrieval system that leverages vector database technology for text embedding of multimodal data. Additionally, artificial intelligence is used to enable knowledge retrieval and augmented generation, significantly enhancing data retrieval capabilities within the specialized domain of material supply chains.},
	booktitle = {2024 {International} {Conference} on {Information} {Technology}, {Comunication} {Ecosystem} and {Management} ({ITCEM})},
	author = {Ge, Xing and Liu, Yafei and Yang, Pei and Sun, Xin and Qiao, Junfeng and Qu, Luyao and Qiu, Jingyi},
	month = dec,
	year = {2024},
	keywords = {large language model, Knowledge graphs, Large language models, knowledge graph, Vectors, Supply chains, Information technology, Faces, data retrieval, Data retrieval, Green buildings, material supply chain, Power systems, Relational databases},
	pages = {156--160},
}

@inproceedings{ying_masked_2024,
	title = {Masked {Theme}-{Specific} {Named} {Entity} {Recognition} {Assisted} with {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002040147&doi=10.1109%2FICOCO62848.2024.10928264&partnerID=40&md5=77736378fcbefcbc8d49f4bd57f5128a},
	doi = {10.1109/ICOCO62848.2024.10928264},
	abstract = {Existing Named Entity Recognition (NER) methods are required to label relevant samples and train the concrete NER models. Due to the specification of theme-specific documents, these NER models are considerably hard to identify potential theme-specific entities. To address this challenge, we propose an effective two-stage approach of masked theme-specific NER associated with Large Language Models (LLMs), which uses the unsupervised mechanism rather than the supervised one. The approach involves theme-specific entity ontology construction and masked NER in heterogeneous documents. The first stage is associated with LLMs and Wikipedia category pages, and the second one is implemented with the masked NER based on the created ontology in the first stage. Extensive experimental results suggest that the proposed masked NER can precisely locate the known entities in the theme-specific entity ontology while improving the accuracy of NER in the remaining text. Compared to the mainstream NER frameworks such as spaCy 3, the masked NER can identify more valid entities in the input Markdown text and continuously use the newly detected unknown entities to update the created ontology.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Computing} ({ICOCO})},
	author = {Ying, Zhao and Weiyu, Chen and Longlong, Liao and Jie, Liu},
	month = dec,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Large language model, Ontology, Natural language processing, Language model, Named entity recognition, Large language models, Modeling languages, Natural Language Processing, Large Language Model, Named Entity Recognition, Computational modeling, Accuracy, Internet, Language processing, Natural languages, Libraries, Encyclopedias, Filtering, Heterogeneous Documents, Online services, Theme-specific Entity Ontology, Ontology's, Natural language processing systems, Recognition models, Heterogeneous documents, Theme-specific entity ontology},
	pages = {457--462},
	annote = {Cited by: 0},
}

@inproceedings{hier_high-throughput_2024,
	title = {High-{Throughput} {Phenotyping} of {Clinical} {Text} {Using} {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001381121&doi=10.1109%2FBHI62660.2024.10913712&partnerID=40&md5=1014a8c6a7e3962918cf865392f856d4},
	doi = {10.1109/BHI62660.2024.10913712},
	abstract = {High-throughput phenotyping automates the mapping of patient signs to standardized concepts, such as those in Human Phenotype Ontology (HPO), a process critical to precision medicine. We evaluated the automated phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using a large language model. Various APIs were used to automate text retrieval, sign identification, categorization, and normalization. GPT-4 outperformed GPT-3.5Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to concordance between manual annotators. While GPT-4 demonstrates high accuracy in sign identification and categorization, limitations remain in sign normalization, particularly in retrieving the correct HPO ID for a normalized term. Methods such as retrieval-augmented generation, changes in pre-training, and additional fine-tuning may help address these limitations. The combination of APIs with large language models presents a promising approach for high-throughput phenotyping of free text.},
	booktitle = {2024 {IEEE} {EMBS} {International} {Conference} on {Biomedical} and {Health} {Informatics} ({BHI})},
	author = {Hier, Daniel B. and Munzir, S. Ilyas and Stahlfeld, Anne and Obafemi-Ajayi, Tayo and Carrithers, Michael D.},
	month = nov,
	year = {2024},
	note = {ISSN: 2641-3604},
	keywords = {large language model, Ontologies, Large language model, Ontology, Natural language processing, Language model, Large language models, Bioinformatics, Retrieval augmented generation, Human phenotype ontology, Precision medicine, natural language processing, Modeling languages, phenotype, Phenotype, Databases, GPT-4, neurology, Phenotypes, Neurology, Brain mapping, Language processing, Manuals, Natural languages, high-throughput, HPO, OMIM, Ontology's, Natural language processing systems, High-throughput, Online mendelian inheritance in man},
	pages = {1--8},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{zhou_leveraging_2024,
	title = {Leveraging {Knowledge} {Distillation} for {Improved} {Event} {Extraction} in {QA} {Models}},
	doi = {10.1109/ICFTIC64248.2024.10913373},
	abstract = {Event extraction is an important task in natural language processing, and it is widely utilized in intelligence domains such as business and military for information extraction. Recently, many works have successfully transformed document-level event extraction into Question-answering (QA) tasks with remarkable results. This approach embeds event argument information within the questions, introducing prior knowledge into the process. Jin et al. highlighted the importance of high-quality QA pairs for practical QA tasks, emphasizing that constructing these pairs remains a key challenge[1]. In this study, we propose a Prompt-based question generation method to automatically generate questions containing event arguments, which converts event extraction into a QA task. We introduce an event ontology-based information retrieval module to enhance answer accuracy and select the most relevant document segments as input text. Additionally, we employ knowledge distillation to build the QA model, transferring knowledge from a large pre-trained model to a more compact and efficient one, improving the student model's performance. Experiments show that our model performs strongly in mainstream benchmarks, with 4.8 improvements on WikiEvents.},
	booktitle = {2024 6th {International} {Conference} on {Frontier} {Technologies} of {Information} and {Computer} ({ICFTIC})},
	author = {Zhou, Peiyao and Hu, Zhikui and Zi, Kangli and Zhang, Dawei},
	month = dec,
	year = {2024},
	keywords = {Information retrieval, Question generation, Data mining, component, Question Answering, Accuracy, Business, Question answering (information retrieval), Benchmark testing, document-level event extraction, knowledge distillation},
	pages = {703--708},
}

@inproceedings{sharma_performance_2024,
	title = {Performance {Metrics} {Analysis} for {Deep} {Learning} {Models}},
	volume = {1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001342750&doi=10.1109%2FICAICCIT64383.2024.10912181&partnerID=40&md5=392d5ba6a475465d31ae6185e73a804b},
	doi = {10.1109/ICAICCIT64383.2024.10912181},
	abstract = {Evaluating an Information Retrieval (IR) model is a multi-faceted process that requires selecting the right quantitative metrics to assess performance. The selection of evaluation metrics in IR depends on specific tasks, relevant standards, and desired characteristics. To thoroughly assess an IR system’s performance, a combination of metrics is necessary. This study explores the effectiveness of deep learning (DL) models in semantic and personalized information retrieval (SIR), focusing on BERT and other large language models (LLMs) that provide context-sensitive embeddings and advanced language comprehension capabilities. Through a detailed analysis, this paper demonstrates how DL models can significantly enhance accuracy and relevance in IR, using evaluation metrics critical for assessing model performance. Metrics like recall, precision, and F1-score are key in capturing model accuracy and coverage; for example, a recall of {\textbackslash}mathbf1. 0 indicates complete retrieval of relevant instances, while a precision of 0.6 reflects a {\textbackslash}mathbf6 0 \% accuracy in positive predictions, balancing these measures at an F1-score of 46.15\%. Ranking and prioritization metrics, such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG), evaluated at 91.67\% and 95.1\%, respectively highlight the models’ capabilities in prioritizing relevant results effectively. In personalized and semantic search, selecting the right evaluation metrics is essential to maximize DL model potential and improve the user experience. Recent LLM advancements, like GPT-4, are instrumental in capturing nuanced meanings and understanding complex user queries, which enhances the accuracy of search engines. Continuous optimization of these models through tailored metrics can improve IR systems’ contextual relevance and accuracy, fostering greater user satisfaction.},
	booktitle = {2024 2nd {International} {Conference} on {Advances} in {Computation}, {Communication} and {Information} {Technology} ({ICAICCIT})},
	author = {Sharma, Simple and Panda, Supriya P.},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontology, Semantics, BERT, Deep learning, Semantic search, Information retrieval, Transformers, Evaluation, Metrics, Standards, Context modeling, Federated learning, Deep reinforcement learning, Computational modeling, Measurement, Accuracy, Adversarial machine learning, User experience, Semantic information retrieval, Transformer Models, User Profiling, Deep learning model, Analytical models, Deep Learning Models, Personalized IR, Semantic IR, Ontology's, Contrastive Learning, Transformer modeling, Data accuracy, Learning models, Metric, Personalized information retrieval, User's profiling},
	pages = {970--976},
	annote = {Cited by: 0},
}

@inproceedings{kalaiarasi_enhancing_2024,
	title = {Enhancing {E}-{Commerce} {Product} {Recommendations} {Using} {LLMs} and {Transformer}-{Based} {Deep} {Learning} {Architectures}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001377718&doi=10.1109%2FICSES63760.2024.10910646&partnerID=40&md5=d40f3671da28d737f32400eebfa79f26},
	doi = {10.1109/ICSES63760.2024.10910646},
	abstract = {The integration of large language models with deep learning architectures brought an evolutionary revolution in the context of product recommendation systems because of several limitations of traditional methods, such as collaborative and content-based filtering. In this paper, a novel framework for product recommendation is proposed by integrating large language models jointly with deep learning. The contribution of large language models is that it adds semantic understanding capability to the predictive power provided through neural networks. Domain ontologies will be used in this hybrid model to enhance the accuracy and personalization of recommendations, considering complex user preferences and product attributes in e-commerce platforms. It takes a state-of-the-art pre-trained LLM, such as Llama-3, as input and generates personalized embeddings of users based on history, item descriptions, and contextual information. In this work, the Transformer architecture has been used in re-fining and ranking the products for relevance using attention mechanisms that select the most important features in each recommendation task. Besides, knowledge distillation will be used to conduct the small and efficient student model training process. The distilled model receives soft predictions that involve the teacher LLM, which greatly reduces computational overhead but preserves high recommendation accuracy. Eventually, the framework will be evaluated on a real-world e-commerce dataset to explore how it increases the click-through rate, purchase rate, and user's engagement compared to the traditional systems.},
	booktitle = {2024 {International} {Conference} on {Innovative} {Computing}, {Intelligent} {Communication} and {Smart} {Electrical} {Systems} ({ICSES})},
	author = {Kalaiarasi, S. Jenny and Nimala, K.},
	month = dec,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Language model, Large language models, Semantics, Deep learning, Domain ontologies, Transformers, Collaboration, Training, Electronic commerce, Recommender systems, Computational modeling, Collaborative filtering, Accuracy, Pre-trained LLM, Computer architecture, Filtering, collaborative and content-based filtering, Llama-3, Contrastive Learning, E- commerces, Personnel training, Collaborative-based filtering, Content based filtering, Learning architectures, Product recommendation, Product recommendation system, Wiener filtering},
	pages = {1--8},
	annote = {Cited by: 1},
}

@inproceedings{huynh_practical_2024,
	title = {A {Practical} {Approach} {Applying} {Deep} {Learning} and {Ontology} to {Identify} {Aspects} in {Opinions}},
	doi = {10.1109/ATC63255.2024.10908257},
	abstract = {Aspect-based sentiment analysis is a very interesting problem in opinion mining. Accurately determining the evaluated aspect in an opinion contributes to improving the performance of the sentiment analysis problem. This study proposes an approach to identify aspects that is not based on keywords but on the semantics of opinions. To determine the evaluated aspects in opinions, the proposed approach uses the method of embedding knowledge from the ontology into the corpus to train deep learning algorithms. The structure of the ontology used in this study is based on the relationship between aspect words and emotion words in the field of car evaluation. The corpus is labeled with aspects not only based on keywords indicating aspects but also based on the semantics of the sentence. The high accuracy test results show the prominent application of the proposed approach.},
	booktitle = {2024 {International} {Conference} on {Advanced} {Technologies} for {Communications} ({ATC})},
	author = {Huynh, Trung-Tru and Nguyen, The-Bao and Ho-Dac, Hung},
	month = oct,
	year = {2024},
	note = {ISSN: 2162-1039},
	keywords = {Ontologies, Ontology, Semantics, Deep learning, Deep Learning, Sentiment analysis, Data models, Accuracy, Corpus, Automobiles, Aspect},
	pages = {969--973},
}

@inproceedings{barron_domain-specific_2024,
	title = {Domain-{Specific} {Retrieval}-{Augmented} {Generation} {Using} {Vector} {Stores}, {Knowledge} {Graphs}, and {Tensor} {Factorization}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000862118&doi=10.1109%2FICMLA61862.2024.00258&partnerID=40&md5=023e5b34ed49a05e38f7a8cf11d66825},
	doi = {10.1109/ICMLA61862.2024.00258},
	abstract = {Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.},
	booktitle = {2024 {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Barron, Ryan C. and Grantcharov, Vesselin and Wanna, Selma and Eren, Maksim E. and Bhattarai, Manish and Solovyev, Nicholas and Tompkins, George and Nicholas, Charles and Rasmussen, Kim Ø. and Matuszek, Cynthia and Alexandrov, Boian S.},
	month = dec,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Ontology, Natural language processing, Retrieval augmented generation, Modeling languages, Natural Language Processing, Knowledge Graph, Question answering, Artificial Intelligence, Reliability, Malware, Anomaly detection, Agents, Non-negative matrix factorization, Tensor factorization, Accuracy, Language processing, Retrieval Augmented Generation, Topic Modeling, Natural languages, Tuning, Question answering (information retrieval), Vectors, Tensors, Non-Negative Tensor Factorization, Domain Knowledge, Natural language processing systems, Photomapping, Problem oriented languages, Domain specific, Non negatives, Non-negative tensor factorization},
	pages = {1669--1676},
	annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{r_drug_2024,
	title = {Drug {Pills} {Identification} {System} using {Google} {Gemini} {LLM}: {A} {Generative} {AI} approach},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000646321&doi=10.1109%2FICERCS63125.2024.10895371&partnerID=40&md5=4660581d49f6bd296c77a2902b2be5b0},
	doi = {10.1109/ICERCS63125.2024.10895371},
	abstract = {Generative AI is emerging as a disruptive force in the healthcare industry, bringing novel solutions ranging from drug development and clinical decision support to personalized patient care. This study is focused on drug discovery using the Generative AI model. In this paper, a system is proposed for providing drug descriptions from drug pill images. The system is implemented by utilizing Large Language Models (LLMs) in combination with computer vision to detect and provide detailed information about drugs from pill images. In the proposed system, the identification process begins by taking the medicinal drug pills and their cover images. Then, the image is converted into binary values using a standard built-in function. In addition, the target language for providing audio descriptions about the drugs is also used. Then, the Google Gemini LLM model is customized by using binary values of the image, target language, and ontology-based prompt engineering. As a result, the LLM model provides drug descriptions in text. Then, the textual description of the drug is converted into the target language audio format by using the Google Text to Speech Converter. The system is experimented by using 807 medicinal drug images which are collected from web resources. The performance of the system is measured by using accuracy. The system achieved an accuracy of 95.04\% which is a little higher when compared with the current state-of-the-art model.},
	booktitle = {2024 {International} {Conference} on {Emerging} {Research} in {Computational} {Science} ({ICERCS})},
	author = {R, Menaha and R, Abilaash and N, Mohanram P and Unnikrishnan, Akash and S, Sukumar},
	month = dec,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Large Language Models, Large language model, Language model, Generative AI, Multi-modal learning, Multimodal Learning, Computational modeling, Drugs, Text analysis, Accuracy, Internet, Medical services, Biomedical imaging, Drug Pill Identification, Medical Text Analysis, Pharmaceutical Image Analysis, Streaming media, System performance, Text to speech, Image analyze, Image-analysis, Drug pill identification, Medical text analyze, Pharmaceutical image analyze},
	pages = {1--5},
	annote = {Cited by: 0},
}

@inproceedings{couder_incorporating_2024,
	title = {Incorporating {AI} in the {Teaching} of {Requirements} {Tracing} {Within} {Software} {Engineering}},
	doi = {10.1109/FIE61694.2024.10892858},
	abstract = {During the Software Development Lifecycle (SDLC), the first stage entails the Requirement Engineering phase. In this phase, engineers gather, analyze, and specify the requirements for a software system. Requirements playa crucial role in the SDLC as they establish the foundation for the entire system by defining the expected behaviors of the software system to be built. The resulting specifications are captured in a Software Requirement Specification (SRS) document. As part of the validation process, requirement specifications are traced. Requirement tracing involves linking the requirement to the artifacts where the customer requested the high-level requirement. Teaching proper requirements tracing can be challenging in a traditional classroom setting. It is essential to educate future software engineers on the proper process of developing an SRS document and of tracing requirements back to the originating artifact, which is also challenging due to the complexity and large scope of applying the complete requirements engineering process. Understanding how changes in customer needs can impact requirements is an imperative learning opportunity. In this work, we aim to incorporate the use of AI in the teaching of requirements tracing using Large Language Models. In this experiment, both GPT -3.5 and GPT -4 are provided the transcript of an interview between the customer and the engineering team, as well as the subsequent requirements elicited from that meeting and other customer provided artifacts. The GPTs are then instructed to determine which requirements can be traced back to the interview transcript. At the same time, the students (the requirements engineering team) conduct their own effort to trace requirements back to the original interview. The experiment was taken one step further to assess students' and the GPTs abilities to address requirements modifications. After another interview with the customer, where some needs were changed, some requirements were modified, and students, and GPTs were asked to trace the modified requirements to the new interview. The results proved that students are better than both GPT versions at tracing modified requirements, yet GPTs again identified requirements that students didn't trace back. The findings, illustrate that AI can help in the teaching of requirement tracing; these results suggest that while no AI model is currently capable of replacing real requirement engineers as they don't outperform students, it can be used as a tool to test the completeness of the requirement tracing process. We posit that GPT can be a tool for students to self-assess the degree to which their own requirements tracing is exhaustive.},
	booktitle = {2024 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	author = {Couder, Juan Ortiz and Pate, William C. and Machado, Daniel A. and Ochoa, Omar},
	month = oct,
	year = {2024},
	note = {ISSN: 2377-634X},
	keywords = {Large Language Models, Artificial intelligence, AI, Software engineering, Education, Requirements engineering, Training, Visualization, Software systems, Interviews, Prototypes, Atmospheric modeling, Software development management, Requirement Tracing, Software Requirement Specification},
	pages = {1--8},
}

@inproceedings{hou_protein_2024,
	title = {Protein {Function} {Prediction} {Based} on the {Pretrained} {Language} {Model} {ESM2} and {Graph} {Convolutional} {Networks}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000150716&doi=10.1109%2FISPA63168.2024.00242&partnerID=40&md5=7b023aac5b19e341e1b0ae98ba0e6cf7},
	doi = {10.1109/ISPA63168.2024.00242},
	abstract = {Understanding protein function is crucial for comprehending life at the molecular level. Currently, less than 0.1\% of proteins having experimental GO annotations. Traditional experimental methods are time-consuming and expensive. To narrow this gap, employing accurate and efficient computational methods can fill the void in automated protein function prediction (AFP). We have developed a new method for predicting protein function using sequence and predicted structural information. We use the large-scale pretrained language model ESM2 to pretrain protein sequences and an encoder to capture contextual information. Using the 3D structural data of proteins generated by AlphaFold2, combined with the sequence, as input for the Graph Convolutional Neural Network, to infer the probabilities of Gene Ontology (GO) annotations for the proteins. Compared to earlier methods, our model achieves better performance. Evaluations on the human dataset show AUPR improvements of 9\%, 9.4\%, and 20.7\% in the BP, MF, and CC branches, respectively, demonstrating that our model is an effective tool for predicting protein function.},
	booktitle = {2024 {IEEE} {International} {Symposium} on {Parallel} and {Distributed} {Processing} with {Applications} ({ISPA})},
	author = {Hou, Lijuan and Qin, Hanyan and Zhang, Xiankun and Zhang, Yiying},
	month = oct,
	year = {2024},
	note = {ISSN: 2158-9208},
	keywords = {Ontologies, Language model, Gene Ontology, Annotations, Gene ontology, Feature extraction, Context modeling, Data models, Graph neural networks, Protein function prediction, Convolutional neural networks, Accuracy, Predictive models, Protein sequence, Three-dimensional displays, AlphaFold2, ESM2, graph pooling, Logic gates, protein function prediction, Prediction models, Protein functions, Alphafold2, Convolutional networks, Gene ontology annotations, Graph pooling, Prediction-based},
	pages = {1776--1781},
	annote = {Cited by: 0},
}

@inproceedings{lee_beyond_2024,
	title = {Beyond {Ontology} in {Dialogue} {State} {Tracking} for {Goal}-{Oriented} {Chatbot}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000199329&doi=10.1109%2FICKG63256.2024.00030&partnerID=40&md5=e836e730709478b16616e45c21443428},
	doi = {10.1109/ICKG63256.2024.00030},
	abstract = {Goal-oriented chatbots are essential for automating user tasks, such as booking flights or making restaurant reservations. A key component of these systems is Dialogue State Tracking (DST), which interprets user intent and maintains the dialogue state. However, existing DST methods often rely on fixed ontologies and manually compiled slot values, limiting their adaptability to open-domain dialogues. We propose a novel approach that leverages instruction tuning and advanced prompt strategies to enhance DST performance, without relying on any predefined ontologies. Our method enables Large Language Model (LLM) to infer dialogue states through carefully designed prompts and includes an anti-hallucination mechanism to ensure accurate tracking in diverse conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder (VGAE) to model and predict subsequent user intent. Our approach achieved state-of-the-art with a JGA of 42.57\% outperforming existing ontologyless DST models, and performed well in open-domain real-world conversations. This work presents a significant advancement in creating more adaptive and accurate goal-oriented chatbots. 1},
	booktitle = {2024 {IEEE} {International} {Conference} on {Knowledge} {Graph} ({ICKG})},
	author = {Lee, Sejin and Kim, Dongha and Song, Min},
	month = dec,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Ontology, Large language models, Neural networks, Graph Neural Network, Prompt engineering, Graph neural networks, Dialog state tracking, Chatbot, Dialogue State Tracking, Accuracy, Chatbots, Limiting, Tuning, Adaptation models, Predictive models, Oral communication, Goal-oriented Dialogue, Ontology's, Bot (Internet), Goal-oriented, Goal-oriented dialog, State tracking, Tracking method, Tracking performance},
	pages = {177--185},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{chen_multi-agent_2024,
	title = {A {Multi}-{Agent} {Collaborative} {Framework} for {Constructing} {Knowledge} {Graphs} from {Text}},
	doi = {10.1109/ICKG63256.2024.00010},
	abstract = {Recent advancements in large language models (LLMs) have significantly improved natural language understanding and generation, making them valuable tools for knowledge graph construction. However, a single LLM often struggles with the complexity of this task, leading to suboptimal results. To address this challenge, we propose a robust multi-agent collaborative framework for constructing knowledge graphs from text. This framework leverages dynamic interactions among specialized agents, including knowledge graph experts, knowledge extraction experts, data processing experts, and domain-specific experts, to effectively build accurate knowledge graphs from text. Additionally, we introduce a novel prompt construction method tailored for knowledge extraction and a revision mechanism to revise preliminary knowledge graphs. These innovations address common issues in knowledge extraction and enhance the quality of model-generated content. Experimental results on four datasets across two tasks (Named Entity Recognition and Relation Extraction) demonstrate that our approach achieves superior performance in the F1 score compared to baseline methods, highlighting its effectiveness and robustness.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Knowledge} {Graph} ({ICKG})},
	author = {Chen, Gui and Liu, Xianhui},
	month = dec,
	year = {2024},
	keywords = {large language model, Ontologies, Knowledge graphs, Named entity recognition, Large language models, Knowledge engineering, Data mining, multi-agent, prompt engineering, Collaboration, Robustness, Technological innovation, knowledge extraction, knowledge graph construction, Data processing},
	pages = {9--16},
}

@inproceedings{zhou_construction_2024,
	title = {Construction of a {Multimodal} {Knowledge} {Graph} for {Power} {Grid} {Construction} {Safety} {Based} on {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000264602&doi=10.1109%2FNPSPE62515.2024.00013&partnerID=40&md5=40fdb4c39be07596baf7f192abb6fa10},
	doi = {10.1109/NPSPE62515.2024.00013},
	abstract = {In response to the difficulties of safety management and the complexity of information at power grid construction sites, a multimodal knowledge graph construction method based on large language models is proposed. Data from the construction site is collected and filtered, and an ontology for safety management at the construction site is constructed. The ontology is then used as retrieval augmented generation(RAG) for assistance, enabling multimodal large model image extraction, resulting in structured data in the power grid safety field. Finally, the extracted results are displayed using a graph database, completing the construction of the multimodal knowledge graph. The constructed knowledge graph includes multimodal data from the construction site, allowing for quick querying of on-site safety incidents, providing safety managers with a valuable tool for site management.},
	booktitle = {2024 {International} {Conference} on {New} {Power} {System} and {Power} {Electronics} ({NPSPE})},
	author = {Zhou, Xiaofa and Shi, Jianyong and Dong, Lei and Zhang, You and Pan, Jin and Huang, Hao},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Large language model, Ontology, Language model, Large language models, Project management, Information management, Knowledge extraction, Data mining, Data models, Safety management, Accuracy, Multi-modal, Graph Databases, Information filters, Power electronics, Power grids, Structured Query Language, Ontology's, Construction sites, Grid constructions},
	pages = {21--28},
	annote = {Cited by: 0},
}

@inproceedings{li_insider_2024,
	title = {Insider {Threat} {Detection} based on {Knowledge} {Graph} and {Large} {Language} {Model}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000251547&doi=10.1109%2FDSIT61374.2024.10881842&partnerID=40&md5=71973844c9a5df7acdc955c09a1119af},
	doi = {10.1109/DSIT61374.2024.10881842},
	abstract = {This article proposes an insider threat detection method based on a combination of knowledge graph and large language model (LLM); first, the internal systems, users, IPs, access behaviors, etc. are modeled through the knowledge graph ontology; then, a few-shot learning information extraction method based on LLMs is used to extract knowledge from the behavior logs to complete the threat detection knowledge graph. Finally, the representation learning method based on the knowledge graph and the embedding based on the LLM are used to extract feature vectors, which are used as input of the insider threat detection model training based on Deep SVDD. The experimental results show that this method can automatically detect abnormal threat behaviors from massive logs at high accuracy, and has the ability to detect deeply hidden abnormal threat behaviors.},
	booktitle = {2024 7th {International} {Conference} on {Data} {Science} and {Information} {Technology} ({DSIT})},
	author = {Li, Yingna and Ding, Zhiguo and Yan, Zheng and Li, Zhuahua and Shao, Hang},
	month = dec,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {large language model, Knowledge graphs, Knowledge graph, Large language model, Language model, Large language models, Semantics, knowledge graph, Feature extraction, Representation learning, Cognition, Data models, Training, Graph embeddings, Zero-shot learning, Vectors, deep-SVDD, insider threat detection, Threat assessment, Ontology's, Contrastive Learning, Deep-SVDD, Detection methods, Information extraction methods, Insider threat detections, Internal systems},
	pages = {1--4},
	annote = {Cited by: 0},
}

@inproceedings{mejia_clinicalgraph_2024,
	title = {{ClinicalGraph}: {An} {Applied} {Approach} in {Clinical} {EHR} {Knowledge} {Graph} {Generation} for {Optimized} {Clinical} {Decision} {Support} {System}},
	doi = {10.1109/HealthCom60970.2024.10880799},
	abstract = {Electronic Health Records (EHR) are well-known for their extensive capabilities in storing pertinent patient information across various specializations. However, EHRs are part of a broader ecosystem of applications that support the medical treatment process. This ecosystem, although somewhat centralized, does not have the interoperability that many assume. The main issue is that current triage models are not personalized to each patient. Context matters in a medical emergency situation especially when resources are low. In this research, we propose a possible solution by applying state of the art techniques in order to develop a clinical knowledge graph that contains relevant data used for triage optimization. We utilize a fine-tuned named entity recognition model (NER) to extract 41 label entity categories from previous medical records. Additionally, we employed prompt engineering utilizing a large language text generation model with medical knowledge to generate relationships. This resulted in a sum of 1,429 relationship type categories and approximately 999 entity nodes were created with 2,387 relationships.},
	booktitle = {2024 {IEEE} {International} {Conference} on {E}-health {Networking}, {Application} \& {Services} ({HealthCom})},
	author = {Mejia, Jose M. Ruiz and Rawat, Danda B.},
	month = nov,
	year = {2024},
	keywords = {Knowledge graphs, Large language models, Retrieval augmented generation, Decision support systems, Knowledge Graph, Prompt engineering, Reviews, Generative Artificial Intelligence, Medical diagnostic imaging, Biological system modeling, Ecosystems, Medical treatment, Clinical Knowledge Graph, E-triage, Knowledge Graph Generation, Patient Centered Nodes, Rapid Triage},
	pages = {1--6},
}

@inproceedings{mohsenzadegan_towards_2024,
	title = {Towards {Seamless} {Data} {Translation} {Based} on {Data} {Models}: {A} {Hybrid} {AI} {Framework} for {Smart} {Transportation} and {Manufacturing}},
	doi = {10.1109/SMAP63474.2024.00022},
	abstract = {Interoperability between different data standards is essential for advancing digital technologies in smart manufacturing and transportation. This paper presents a hybrid AI framework that integrates Ontology Learning (OL), Knowledge Graphs (KGs), and Large Language Models (LLMs) to improve data translation across these standards. Focusing on IEEE 1451, ISO 15926, and IEC 61499, we address the challenges posed by these data models' unique structures and semantics. Our comparative analysis evaluates the strengths and limitations of OL, KGs, and LLMs across key metrics like accuracy, scalability, efficiency, robustness, and flexibility. The proposed framework leverages OL for systematic structuring, KGs for relational modeling, and LLMs for linguistic processing, enhancing translation accuracy and adaptability. However, integrating these approaches introduces scalability and processing efficiency trade-offs, particularly in resource-constrained environments. This study contributes to developing more sophisticated and scalable data translation models tailored for heterogeneous data environments, with practical implications for smart manufacturing and transportation.},
	booktitle = {2024 19th {International} {Workshop} on {Semantic} and {Social} {Media} {Adaptation} \& {Personalization} ({SMAP})},
	author = {Mohsenzadegan, Kabeh and Tavakkoli, Vahid and Kambale, Witesyavwirwa Vianney and Kyamakya, Kyandoghere},
	month = nov,
	year = {2024},
	keywords = {Semantics, Large Language Models (LLMs), Smart manufacturing, Standards, Data models, Scalability, Translation, Semantic Mapping, Smart Manufacturing, Accuracy, Data Interoperability, Smart transportation, Adaptation models, Systematics, AI in Transportation, Cross-Standard Data Integration, Data Model Translation, Hybrid AI Framework, Knowledge Graphs (KGs), Ontology Learning (OL)},
	pages = {68--73},
}

@inproceedings{fu_llm_2024,
	title = {{LLM} \& {Bagging} for 1-shot {Joint} {IE}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218462496&doi=10.1109%2FDSC63484.2024.00034&partnerID=40&md5=6b62b663ed4b5931142ef282d5f12949},
	doi = {10.1109/DSC63484.2024.00034},
	abstract = {Domain-specific few-shot information extraction (IE) has always been the difficulty of domain knowledge graph construction, and there is a new solution in this direction after the emergence of large language models (LLM). In this paper, based on previous research, we propose LLM-based 1-shot relation-entity joint IE scheme, and the bagging enhance LLM IE method is proposed to take advantage of the randomness of the LLM output. Against the background of the concept of Internet of Things (IoT) which has received wide attention globally, we selects the IoT interconnective communication as a domain-specific example, crawls the text of the device pages of L3Harris, RockwellCollins as our corpus, selects large language models that differ in the number of parameters and invocation methods to test the proposed joint IE method in relations given by the IoT interconnective communication ontology. The bagging method is tested based on the IE results of GPT-4 Turbo, and there is an improvement of 1-3\%, which shows the effectiveness of traditional machine learning methods in LLM. Finally, the results and shortcomings of this study are analyzed.},
	booktitle = {2024 {IEEE} 9th {International} {Conference} on {Data} {Science} in {Cyberspace} ({DSC})},
	author = {Fu, Yibin and Ding, Zhaoyun and Xu, Xiaojie},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Information extraction, Knowledge graph, Large language model, Ontology, Language model, Large language models, Large Language Model (LLM), Information retrieval, Internet of Things, Communication, Standards, Domain knowledge, Bagging, Data science, Training, Zero-shot learning, Information Extraction (IE), 1-shot, bagging, Random forests, Domain specific, Information extraction methods, Joint information},
	pages = {204--208},
	annote = {Cited by: 0},
}

@inproceedings{mou_radlink_2024,
	title = {{RadLink}: {Linking} {Clinical} {Entities} from {Radiology} {Reports}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218350606&doi=10.1109%2FFLLM63129.2024.10852450&partnerID=40&md5=0e228a5ae02d0021073a8c1aa9f6050b},
	doi = {10.1109/FLLM63129.2024.10852450},
	abstract = {Radiology reports are a critical source of information for patient diagnosis and treatment in the medical domain. However, the vast amount of data contained in these reports is often unstructured, making it challenging to extract and normalize relevant clinical entities. Named Entity Normalization (NEN) is essential for mapping these entities to a standard ontology, facilitating better data integration, retrieval, and analysis. In this paper, we introduce RadLink, a benchmark for NEN in radiology. RadLink builds upon 425 expert-annotated radiology reports from the RadGraph dataset, extending it for NEN by mapping entities to the Unified Medical Language System (UMLS) ontology. We employ a combination of morphological and semantic matching approaches to generate normalization annotations, followed by human review for validation. We aim to set a standard with our benchmark for evaluating NEN methods in the radiology domain, that facilitate interoperability across healthcare systems and accelerate medical research by providing structured, standardized data.},
	booktitle = {2024 2nd {International} {Conference} on {Foundation} and {Large} {Language} {Models} ({FLLM})},
	author = {Mou, Yongli and Chen, Hanbin and Lode, Gwendolyn Isabella and Truhn, Daniel and Sowe, Sulayman and Decker, Stefan},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Large language model, Ontology, Language model, Large language models, Semantics, large language models, Diagnosis, Benchmarking, Standards, Unified modeling language, Radiology, Reviews, named entity normalization, Accuracy, Radiology reports, Medical diagnostic imaging, Benchmark testing, radiology reports, Ontology's, Medical information systems, Clinical research, Medical domains, Patient treatment, System ontology, Named entity normalizations, Patient diagnosis, Sources of informations, Unified medical language systems},
	pages = {443--449},
	annote = {Cited by: 0},
}

@inproceedings{sorokoletova_towards_2024,
	title = {Towards a scalable {AI}-driven framework for data-independent {Cyber} {Threat} {Intelligence} {Information} {Extraction}},
	doi = {10.1109/FLLM63129.2024.10852465},
	abstract = {Cyber Threat Intelligence (CTI) is critical for mitigating threats to organizations, governments, and institutions, yet the necessary data are often dispersed across diverse formats. AI-driven solutions for CTI Information Extraction (IE) typically depend on high-quality, annotated data, which are not always available. This paper introduces 0-CTI, a scalable AI-based framework designed for efficient CTI Information Extraction. Leveraging advanced Natural Language Processing (NLP) techniques, particularly Transformer-based architectures, the proposed system processes complete text sequences of CTI reports to extract a cyber ontology of named entities and their relationships.Our contribution is the development of 0-CTI, the first modular framework for CTI Information Extraction that supports both supervised and zero-shot learning. Unlike existing state-of-the-art models that rely heavily on annotated datasets, our system enables fully dataless operation through zero-shot methods for both Entity and Relation Extraction, making it adaptable to various data availability scenarios. Additionally, our supervised Entity Extractor surpasses current state-of-the-art performance in cyber Entity Extraction, highlighting the dual strength of the framework in both low-resource and data-rich environments.By aligning the system’s outputs with the Structured Threat Information Expression (STIX) format, a standard for information exchange in the cybersecurity domain, 0-CTI standardizes extracted knowledge, enhancing communication and collaboration in cybersecurity operations.},
	booktitle = {2024 2nd {International} {Conference} on {Foundation} and {Large} {Language} {Models} ({FLLM})},
	author = {Sorokoletova, Olga and Antonioni, Emanuele and Colò, Giordano},
	month = nov,
	year = {2024},
	keywords = {Ontologies, Natural language processing, Large language models, Information retrieval, Natural Language Processing, Transformers, Data mining, Cyber threat intelligence, Named Entity Recognition, Cyber Threat Intelligence, Relation Extraction, Computer security, Standards organizations, Zero shot learning, Structured Threat Information Expression},
	pages = {398--406},
}

@inproceedings{adjei-frempah_kb2bench_2024,
	title = {{KB2Bench}: {Toward} a {Benchmark} {Framework} for {Large} {Language} {Models} on {Medical} {Knowledge}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217405922&doi=10.1109%2FICTAI62512.2024.00075&partnerID=40&md5=377fc09faaa1bd5fb481d0e449248ca8},
	doi = {10.1109/ICTAI62512.2024.00075},
	abstract = {While Large Language Models (LLMs) have trans-formed question answering tasks, their propensity for hallucinations continues to drive an area of active research. Efforts toward creating benchmarks to test LLMs' performance on queries, in particular for the field of medicine, have led to a few reputable benchmarks, but these are limited in scope because of the amount of human annotation required. Our framework addresses this issue by leveraging existing, large knowledge bases for medicine to generate vast query and answer sets dynamically, which are less likely to be memorized by LLMs. The framework rests on designing a few key knowledge patterns, which can then generate millions (potentially billions) of queries. This offers a more efficient, cost-effective, and scalable alternative to human-curated annotations used in medical question-and-answer benchmarks. Applying our framework to a small sample of five drug related ontologies, we are already capable of more than 100,000 unique drug related queries, which is 10 to 1000 times larger than existing various human annotation efforts. This paper introduces the KB2Bench framework.},
	booktitle = {2024 {IEEE} 36th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {Adjei-Frempah, Douglas and Chen, Lisa and LePendu, Paea},
	month = oct,
	year = {2024},
	note = {ISSN: 2375-0197},
	keywords = {Ontologies, Large language model, Ontology, Language model, Large language models, large language models, ontologies, Knowledge representation, Knowledge representation and reasoning, Annotations, Knowledge base, Vocabulary, Medical informatics, Benchmarking, knowledge representation and reasoning, Knowledge based systems, knowledge bases, biomedical informatics, Drugs, Medical knowledge, vocabularies, Biomedical informatics, benchmarking, Question answering (information retrieval), Benchmark testing, Structured Query Language, Ontology's, Query languages, Human annotations, Question Answering Task},
	pages = {485--493},
	annote = {Cited by: 0},
}

@inproceedings{armary_identifying_2024,
	title = {Identifying {Logical} {Patterns} in {Text} for {Reasoning}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217432359&doi=10.1109%2FICTAI62512.2024.00122&partnerID=40&md5=57088e55fc285fbc3412ea1f7c228efb},
	doi = {10.1109/ICTAI62512.2024.00122},
	abstract = {Translating unstructured text into logical format is a key challenge for building ontologies automatically and addressing deductive inference. Most of the approaches have tackled the identification of concepts and relations in text, but few of them have addressed the most complex axioms like class expression subsumption. This work proposes DeLIR, a neuro-symbolic approach to identify complex logical patterns in text by combining a grammatical translation of dependency parsing trees and a fine-tuned Large language Model (LLM). DeLIR combines the strength of the parsing accuracy provided by a grammatical approach and pattern flexibility provided by a finetuned LLM. We evaluated our approach on FOLIO dataset for both translation capacity and inference capability. Our grammatical approach has a perfect parsing accuracy and combining the grammatical approach with LLMs improves the LLMS translation capacity: tinyLlama, T5-small-text2logic, Llama-7B and Mistral-7B. We also evaluate the inference capacity of the different LLMs. Mistral-7B, while being smaller than the state-of-the-art approach using GPT-4, presents similar results to predict the correct inference labels.},
	booktitle = {2024 {IEEE} 36th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {Armary, Pauline and El-Vaigh, Cheikh-Brahim and Spicher, Antoine and Narsis, Ouassila Labbani and Nicolle, Christophe},
	month = oct,
	year = {2024},
	note = {ISSN: 2375-0197},
	keywords = {Ontologies, Ontology, Language model, Ontology learning, Large language models, Cognition, Ontology Learning, Natural language inference, Translation, Natural Language Inference, Accuracy, Language inference, Natural languages, Syntactics, Buildings, Zero shot learning, Hands, Translation to Logic, Ontology's, Contrastive Learning, Computer aided language translation, Unstructured texts, Computer circuits, Dependency parsing, State-of-the-art approach, Translation to logic},
	pages = {837--844},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{nedelchev_supporting_2024,
	title = {Supporting {Digitization} of a {Cultural} and {Historical} {Heritage} {Platform}},
	doi = {10.1109/ICAI63388.2024.10851643},
	abstract = {The article introduces a platform designed for the storage and retrieval of digitized cultural and historical objects from Bulgaria. To enhance the platform’s accessibility for tourists, a dedicated tourist guide has been created and implemented as a personalized assistant. The architecture, along with its distinct components, is thoroughly elucidated. The article provides an overview of the current state of the platform, details the experiments conducted with the realized prototype, and outlines its future development, with a focus on integrating cultural and historical object ontologies. Additionally, a brief background of the platform is provided for context.},
	booktitle = {2024 {International} {Conference} {Automatics} and {Informatics} ({ICAI})},
	author = {Nedelchev, Iliya and Tabakova-Komsalova, Veneta and Stoyanov, Ivan and Stoyanov, Stanimir and Ivanova, Vanya and Kazashka, Tsvetomira},
	month = oct,
	year = {2024},
	keywords = {Ontologies, ontologies, Informatics, Prototypes, Cultural differences, Electronic learning, cultural-historical heritage platform, digitalization of cultural-historical objects, personal assistants, tourist guide},
	pages = {490--493},
}

@inproceedings{saini_ontology_2024,
	title = {An {Ontology} for {Conversations} with {Virtual} {Research} {Assistants}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217375666&doi=10.1109%2FICTAI62512.2024.00034&partnerID=40&md5=14f6195a6fb341b084b05f06ba9ba51c},
	doi = {10.1109/ICTAI62512.2024.00034},
	abstract = {Conversational artificial intelligence has expanded rapidly in recent years, especially with the growth of large language models (LLMs). Its incorporation in scientific research in the form of research assistants has also become more common-place but remains limited in some capacities, such as in the realm of polymer science. The limitations of LLMs, especially in terms of domain knowledge, warrant the need for other tools, such as knowledge graphs (KGs), to better guide conversations. While such conversational models have been developed in the past, they are generally restricted to particular domains and lack the ability to integrate semantics from various kinds of conversations. Thus, we make progress toward the construction of a universal conversational model that has a focus on the materials domain by combining aspects of existing models. We aim to implement it in such a way that renders it amenable to modifications and usable in a variety of situations. We posit that this model will be adopted and extended by others seeking to accomplish a similar goal in the future.},
	booktitle = {2024 {IEEE} 36th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {Saini, Anmol and Ethier, Jeffrey G. and Shimizu, Cogan},
	month = oct,
	year = {2024},
	note = {ISSN: 2375-0197},
	keywords = {large language model, Ontologies, Knowledge graphs, Knowledge graph, Large language model, Ontology, Language model, Large language models, Semantics, artificial intelligence, Ontology design, ontology, Ontology design pattern, knowledge graph, Conversational artificial intelligence, conversational model, Design Patterns, ontology design pattern, Limiting, Natural languages, Adaptation models, Oral communication, polymer science, Polymers, Ontology's, Conversational model, Polymer science, Virtual research},
	pages = {181--186},
	annote = {Cited by: 0},
}

@inproceedings{li_fish-bone_2024,
	title = {Fish-{Bone} {Diagram} of {Research} {Issue}: {Gain} a {Bird}'s-{Eye} {View} on a {Specific} {Research} {Topic}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217833147&doi=10.1109%2FSMC54092.2024.10830995&partnerID=40&md5=7b09adfbb70ce24c77bc4685f35e780a},
	doi = {10.1109/SMC54092.2024.10830995},
	abstract = {Novice researchers often face difficulties in understanding a multitude of academic papers and grasping the fundamentals of a new research field. To solve such problems, the knowledge graph supporting research survey is gradually being developed. Existing keyword-based knowledge graphs make it difficult for researchers to deeply understand abstract concepts. Meanwhile, novice researchers may find it difficult to use ChatGPT effectively for research surveys due to their limited understanding of the research field. Without the ability to ask proficient questions that align with key concepts, obtaining desired and accurate answers from this large language model (LLM) could be inefficient. This study aims to help novice researchers by providing a fish-bone diagram that includes causal relationships, offering an overview of the research topic. The diagram is constructed using the issue ontology from academic papers, and it offers a broad, highly generalized perspective of the research field, based on relevance and logical factors. Furthermore, we evaluate the strengths and improvable points of the fish-bone diagram derived from this study's development pattern, emphasizing its potential as a viable tool for supporting research survey.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Li, Jinghong and Phan, Huy and Gu, Wen and Ota, Koichi and Hasegawa, Shinobu},
	month = oct,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Machine learning, Information retrieval, Sustainable development, Prompt engineering, Reviews, Training, Fishbone diagrams, Surveys, User interfaces, Abstract concept, Research fields, Academic paper, Bird's eye view, Keyword-based, Research issues, Research survey, Research topics},
	pages = {4936--4941},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{mankari_enhancing_2024,
	title = {Enhancing {Vector} based {Retrieval} {Augmented} {Generation} with {Contextual} {Knowledge} {Graph} {Construction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217854156&doi=10.1109%2FIDICAIEI61867.2024.10842699&partnerID=40&md5=640d200493915690a5a654ada13c4c03},
	doi = {10.1109/IDICAIEI61867.2024.10842699},
	abstract = {The proliferation of unstructured text data necessitates efficient information retrieval systems. Traditional vector-based Retrieval Augmented Generation (RAG) models often fail to capture complex relationships and contextual nuances, limiting effectiveness in knowledge-intensive tasks. We introduce Contextual Knowledge Graph Construction (CKGC), a novel approach enhancing vector-based RAG by dynamically building a knowledge graph that reflects inherent data structures and connections.CKGC leverages text chunking, large language models (LLMs), and ontology mapping. By segmenting text and using LLMs to identify key entities and relationships, CKGC constructs a contextualized knowledge graph enriching information representation. This bridges the gap between semantic similarity and deeper contextual understanding, enabling more accurate and nuanced retrieval.Experiments on 2,000 lease agreements demonstrate that CKGC significantly improves vector-based RAG in information retrieval and question answering tasks, with substantial gains in Mean Reciprocal Rank (MRR) and Top-k Accuracy. CKGC’s adaptability across domains positions it as a valuable tool for enhancing performance and understanding of complex textual data. Our findings underscore CKGC’s transformative potential in unlocking insights from vast text corpora, paving the way for more intelligent and context-aware information retrieval systems.},
	booktitle = {2024 2nd {DMIHER} {International} {Conference} on {Artificial} {Intelligence} in {Healthcare}, {Education} and {Industry} ({IDICAIEI})},
	author = {Mankari, Sagar and Sanghavi, Abhishek},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Ontology, Language model, Large language models, Ontology mapping, Semantics, Retrieval augmented generation, Information retrieval, Knowledge graph construction, Ontology Mapping, Contextual knowledge, Mapping, Organizations, Knowledge Graph Construction, Information Retrieval, Accuracy, Retrieval Augmented Generation, Graph construction, Question answering (information retrieval), Vectors, Time factors, Content based retrieval, Text data, Unstructured texts, Information-retrieval systems},
	pages = {1--6},
	annote = {Cited by: 0},
}

@inproceedings{okuhara_automatic_2024,
	title = {Automatic {Question} {Generation} with {Knowledge} {Graph} for {Panoramic} {Learning}},
	doi = {10.1109/ITHET61869.2024.10837665},
	abstract = {In recent years, the global social landscape has become increasingly complex, requiring the ability to think from a wide range of diverse perspectives for effective problem-solving. In the field of education, panoramic learning, which implements interdisciplinary and comprehensive education, has become essential. Also, there has been recent research on various aspects of automatic question generation (AGQ), with some studies focusing on generating panoramic questions, which provide a comprehensive understanding, across different genres using knowledge graph (KG). KG is a knowledge base that uses a graph-structured data model and consists of entities and relationships between entities. On the other hand, research on generating panoramic questions for specific subjects with educational purposes has been limited, and this study aims to address that. In this work, we specifically targeted the field of history for question generation and used complemented entities to enhance the inclusion of panoramic knowledge in the field of history. The approach involves enhancing subgraphs with link prediction, which complements missing relationships in KGs, particularly in historical contexts requiring temporal and spatial insights. Through evaluation, it was validated that the proposed method could generate questions containing more panoramic knowledge compared to existing methods.},
	booktitle = {2024 21st {International} {Conference} on {Information} {Technology} {Based} {Higher} {Education} and {Training} ({ITHET})},
	author = {Okuhara, Fumika and Egami, Shusaku and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
	month = nov,
	year = {2024},
	note = {ISSN: 2473-2060},
	keywords = {Knowledge graphs, Problem-solving, Knowledge Graph, Question generation, Knowledge based systems, Linked Data, Training, Accuracy, History, Focusing, Information technology, Hands, Automatic Question Gen-eration, Panoramic Learning},
	pages = {1--7},
}

@inproceedings{li_aligning_2024,
	title = {Aligning {Knowledge} {Graphs} {Provided} by {Humans} and {Generated} by {Neural} {Networks}},
	doi = {10.1109/BigData62323.2024.10825070},
	abstract = {In this paper, an approach that extracts knowledge graphs (KGs) from neural networks (NNs) and aligns the generated KGs with human-provided ones is proposed for network optimization or transparency enhancement, which is achieved by leveraging Vector Symbolic Architectures (VSAs). The approach identifies entities and relations of NN’s knowledge along with the training process, which makes it a plug-and-play solution. Experiments on synthetic data showed that the matching method works on middle and small-size KGs, and tests on MNIST demonstrated that the aligned NN-generated KG could be very close to the human-provided ones. Further tests on Text2KGBench showed that the method could produce KGs from embedding generated by backbone large language models (LLM) that aligned well with human-provided labels as well.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Li, Tangrui and Zhou, Jun and Wang, Hongzheng},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Ontologies, Knowledge graphs, Large language models, Knowledge Graph, Optimization, Training, Artificial neural networks, Codes, Vectors, Software development management, Knowledge Graph Alignment, Synthetic data, Vector Symbolic Architecture},
	pages = {3441--3447},
}

@inproceedings{berger_advancing_2024,
	title = {Advancing {Personalized} {Medicine}: {A} {Scalable} {LLM}-based {Recommender} {System} for {Patient} {Matching}},
	doi = {10.1109/BigData62323.2024.10825567},
	abstract = {This study explores efficient algorithms to enhance user matching in Unrare.me, a novel social networking platform designed to connect individuals affected by rare diseases. Our primary objective is to develop a recommender system that identifies and suggests users with similar medical conditions, facilitating meaningful connections within these unique communities. Utilizing textual user profile data, we train sentence embedder models to generate similar embeddings for users that have rated each other high. We investigate various fine-tuning strategies, as well as a hybrid approach between a dense embedder and sparse SPLADE embeddings. Furthermore, we investigate the efficacy of various clustering algorithms, such as TopicBERT for thematic analysis, K-Means for centroid-based grouping, and Latent Dirichlet Allocation (LDA) for probabilistic topic modeling, to reduce the matching complexity and enable better scalability of the platform.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Berger, Armin and Berghaus, David and Bashir, Ali Hamza and Grigull, Lorenz and Fendrich, Lara and Lagones, Tom Anglim and Högl, Henriette and Ernst, Gundula and Schmidt, Ralf and Bascom, David and Deußer, Tobias and Bell, Thiago and Lübbering, Max and Sifa, Rafet},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Large Language Models, Data models, Recommender systems, Recommender Systems, Computational modeling, Measurement, Rare Diseases, Performance gain, Biological system modeling, Social networking (online), Clustering algorithms, Diseases, Sparse approximation, Text Embeddings, Text Matching},
	pages = {5876--5883},
}

@inproceedings{ho_leveraging_2024,
	title = {Leveraging {Multi}-{Agent} {Systems} and {Large} {Language} {Models} for {Diabetes} {Knowledge} {Graphs}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218046486&doi=10.1109%2FBigData62323.2024.10825608&partnerID=40&md5=c77248afa42583f64d40767d76a92524},
	doi = {10.1109/BigData62323.2024.10825608},
	abstract = {This paper presents a novel framework for constructing a diabetes-specific knowledge graph (KG) using a streamlined multi-agent system powered by Gemini-based Large Language Models (LLMs). Leveraging insights from the 2016 National Diabetes Survey (NNDS) conducted by the National Diabetes Education Program (NDEP), the framework extracts critical variables related to diagnosis, risk perception, medical advice, and self-management practices across diverse U.S. populations. By processing data from the NNDS’s extensive 94-question survey, the methodology performs adaptive ontology mapping using APIs for six major medical standards (e.g., SNOMED CT, ICD-11), ensuring semantic interoperability. Relationships between variables are identified and structured using RDF, RDFS, and OWL standards. The integration of LLMs with ontology tools like Protégé enhances automation and scalability. Results demonstrate the framework’s effectiveness in generating contextually rich and clinically relevant knowledge graphs, providing a robust foundation for advancing healthcare informatics and personalized diabetes management.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Ho, Duy H. and Das, Udiptaman and Ho, Regina and Lee, Yugyung},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Ontologies, Knowledge graphs, Large Language Models, Knowledge graph, Large language model, Ontology, Language model, Ontology mapping, AI, OWL, RDF, Resource description framework, Medical education, Knowledge Graph, Multi-agent systems, Diagnosis, Standards, Ontology Mapping, Multi-Agent System, Scalability, Healthcare Informatics, Surveys, Diabetes, Medical diagnostic imaging, Medical services, Diseases, Health care informatics, Multiagent systems (MASs), Specific knowledge, Education programmes},
	pages = {3401--3410},
	annote = {Cited by: 1},
}

@inproceedings{kim_structured_2024,
	title = {Structured {Extraction} of {Real} {World} {Medical} {Knowledge} using {LLMs} for {Summarization} and {Search}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218046365&doi=10.1109%2FBigData62323.2024.10825160&partnerID=40&md5=1e4a8a263e1891acf94e028d2cdd5621},
	doi = {10.1109/BigData62323.2024.10825160},
	abstract = {Creation and curation of knowledge graphs at scale can be used to exponentially accelerate the discovery, matching, and analysis of diseases in real-world data. While disease ontologies are useful for annotation, integration, and analysis of biological data, codified disease and procedure categories e.g. SNOMED-CT, ICD10, CPT, etc. rarely capture all of the nuances in a patient condition or, in the case of rare disease, may not even exist. Furthermore, there are multiple disease definitions used in data sources and publications, each having its own structure and hierarchy. Mapping between ontologies, finding disease clusters, and building a representation of the chosen disease area are resource-intensive, often requiring significant human capital. We propose the creation and curation of a patient knowledge graph utilizing large language model extraction techniques. In order to expand in volume and scale, knowledge graphs with generalized language capability allow for data to be extracted using natural language rather than being constrained by the exact terminology or hierarchy of existing ontologies. We develop a method of mapping back to existing ontologies such as MeSH, SNOMED-CT, RxNORM, HPO, etc. to ground the extracted entities to known entities in the medical community.We have access to one of the largest ambulatory care EHR databases in the country. To demonstrate the effectiveness of our method, we benchmark our extraction in a test set with over 33.6M unique patients, in the area of patient search. In this case study, we perform a patient search for a rare disease: Dravet syndrome. Dravet syndrome was codified as an ICD10 recognizable disease in October 2020. In the following research, we describe our method of the construction of patient-specific knowledge graphs and subsequent searches for patients who exhibit symptoms of a particular disease. Using patients with confirmed ICD10 codes for Dravet syndrome as our ground truth, we utilize our LLM-based entity extraction techniques and formalize an algorithmic way of characterizing patients in a grounded ontology to assist in mapping patients to specific diseases. Finally, we present the results of a real-world discovery method on Beta-propeller protein-associated neurodegeneration (BPAN), identifying patients with a rare disease, where no ground truth currently exists.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Kim, Edward and Shrestha, Manil and Foty, Richard and DeLay, Tom and Seyfert-Margolis, Vicki},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Ontologies, Knowledge graphs, Large Language Models, Knowledge graph, Large language model, Ontology, Terminology, Language model, Large language models, Ontology mapping, Knowledge Graphs, Dravet Syndrome, Annotations, Data mining, Benchmarking, Ontology Mapping, Translation, Neurodegenerative diseases, Data curation, Proteins, Neurodegeneration, Benchmark testing, Diseases, Beta-propeller protein-associated neurodegeneration (BPAN), Structured Extraction, Decision trees, Ontology's, Photomapping, Dravet syndrome, Steganography, Real-world, Arthroplasty, Beta-propeller protein-associated neurodegeneration, Disease control, Flow visualization, Fracture fixation, Propellers, Structured extraction},
	pages = {3421--3430},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{oranekwu_automated_2024,
	title = {Automated {Knowledge} {Framework} for {IoT} {Cybersecurity} {Compliance}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218064800&doi=10.1109%2FBigData62323.2024.10825755&partnerID=40&md5=a3faa0751cd49dee16c06424344db196},
	doi = {10.1109/BigData62323.2024.10825755},
	abstract = {Rapid expansion in the manufacture and use of Internet of Things (IoT) devices has introduced significant challenges in ensuring compliance with cybersecurity standards. To protect user data and privacy, all organizations providing IoT devices must adhere to complex guidelines such as the National Institute of Standards and Technology Inter agency Report (NIST IR) 8259, which defines essential cybersecurity guidelines for IoT manufacturers. However, interpreting and applying these rules from these guidelines and the privacy policies remains a significant challenge for companies. Thus, this project presents a novel approach to extract knowledge from NIST 8259 for creating semantically rich ontology mappings. Our ontology captures key compliance rules, which are stored in a knowledge graph (KG) that allows organizations to crosscheck and update privacy policy documents with ease. The KG also enables real-time querying using SPARQL and offers a transparent view of regulatory adherence for IoT manufacturers and users. By automating the process of verifying cybersecurity compliance, the framework ensures that companies remain aligned with NIST standards, eliminating manual checks and reducing the risk of non-compliance. We also demonstrate that compared to the baseline Large Language Models (LLMs), our proposed framework has more compliance accuracy, and is more efficient and scalable.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Oranekwu, Ikechukwu and Elluri, Lavanya and Batra, Gunjan},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Large language model, Ontology, Language model, SPARQL, Cyber security, Cybersecurity, LLMs, Internet of Things, IoT, KG, Privacy policies, Privacy, Regulatory compliance, regulatory compliance, privacy policies, Manuals, Real-time systems, Computer security, Companies, Guidelines, automated compliance, KGs, NIST, NIST 8259 standards, NIST Standards, Automated compliance, Knowledge frameworks, NIST 8259 standard},
	pages = {6336--6345},
	annote = {Cited by: 0},
}

@inproceedings{linxen_ontology-driven_2024,
	title = {Ontology-driven knowledge base for digital humanities: {Restructuring} knowledge organization at the library of the {Folkwang} {University} of the {Arts}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217992027&doi=10.1109%2FBigData62323.2024.10825984&partnerID=40&md5=e53d737ab95fdc6a61b8464689e3814b},
	doi = {10.1109/BigData62323.2024.10825984},
	abstract = {Academic libraries are increasingly challenged by the need to efficiently manage and analyse vast collections of data and knowledge. The divers formats and organisation methods of these collections, ranging from traditional print media to digital archives and multimedia assets, can hinder researchers’ ability to easily access and retrieve relevant information. This paper introduces an ontology-driven knowledge base to address this issue by enabling the efficient access to knowledge in the application domain and enhancing the semantic search capabilities in the field of Digital Humanities. Our approach focuses on the development of an ontology-drive knowledge base for semantic search in academic libraries by the example of the library of the Folkwang University of Arts that captures the knowledge concepts present in the library’s archival collections. The resulting ontology framework provides a structured representation of domain knowledge, facilitating the integration of diverse data sources, including structured, semi-structured, and unstructured data from the application domain into a triple store knowledge base. By leveraging SPARQL queries generated from Large Language Model (LLM) prompts, we aim to facilitate more intuitive and effective knowledge retrieval. This approach allows users to express their information needs in a more natural and flexible way, leading to more accurate and relevant search results. We evaluate the proposed ontology-driven knowledge base in terms of its integrity, consistency, flexibility, relevance, and scalability. Our evaluation methodology includes a combination of verification and validation techniques, including automated reasoners and query results based on competence questions. Our findings demonstrate the potential of ontology engineering to enhance complex information retrieval in academic libraries. However, we also identify limitations related to processing speed for complex queries and the quality of search results. This research contributes to the field of computational archival science by providing a novel approach to semantic search in academic libraries. By enabling more precise and efficient access to knowledge, our ontology-driven knowledge base has the potential to enrich the academic and Digital Humanities landscape, empowering researchers to delve deeper into the vast resources available within these institutions.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Linxen, Andrea and Schmidt, Vera-Maria and Klinke, Harald and Beecks, Christian},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Ontologies, Ontology, Semantics, Semantic search, Information retrieval, Knowledge engineering, Ontology framework, Knowledge base, Knowledge organization, Digital humanities, knowledge engineering, Knowledge based systems, Organizations, Scalability, knowledge base, semantic search, Soft sensors, digital humanities, Libraries, Media, Art, ontology framework, Structured Query Language, Domain Knowledge, Ontology's, Query languages, Applications domains, Academic libraries, Digital multimedia, Humanities computing, Print media},
	pages = {2449--2455},
	annote = {Cited by: 0},
}

@inproceedings{berger_tackling_2024,
	title = {Tackling {Data} {Sparsity} and {Combinatorial} {Challenges} in {Rare} {Disease} {Matching} with {Medical} {Informed} {Machine} {Learning}},
	doi = {10.1109/BigData62323.2024.10825404},
	abstract = {With over 7,000 known rare diseases and a prevalence of less than one in a thousand, rare diseases pose substantial challenges to advanced medical support networks. This study investigates the efficacy of Unrare.me, a novel social networking platform designed for individuals affected by rare diseases, including patients, their family members, and medical professionals, addressing data sparsity and combinatorial complexities in user matching. We demonstrate that simple matching heuristics already serve as a decent basis for collecting user feedback on match quality. Leveraging over 10,000 user matching feedback scores from more than 2,000 active users, we evaluate algorithms including collaborative filtering and user embedding similarity with state-of-the-art Large Language Models (LLMs). With a top-10 and top-5 hit-rate of 55\% and 37\%, respectively, we show that a combination of medical data augmentation and embeddings significantly enhances performance beyond the initial heuristic baseline.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Berger, Armin and Lagones, Tom Anglim and Grigull, Lorenz and Fendrich, Lara and Bell, Thiago and Högl, Henriette and Ernst, Gundula and Schmidt, Ralf and Bascom, David and Sifa, Rafet and Lübbering, Max},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Large Language Models, Large language models, Machine learning, Big Data, Data augmentation, Data models, Recommender Systems, Collaborative filtering, Rare Diseases, Complexity theory, Social networking (online), Diseases, Text Matching},
	pages = {6430--6438},
}

@inproceedings{purohit_graphaide_2024,
	title = {{GraphAide}: {Advanced} {Graph}-{Assisted} {Query} and {Reasoning} {System}},
	doi = {10.1109/BigData62323.2024.10825705},
	abstract = {Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of such digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Purohit, Sumit and Chin, George and Mackey, Patrick S and Cottam, Joseph A},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Knowledge graphs, Large language models, Semantic Web, Semantics, Retrieval augmented generation, Cognition, Usability, Pattern matching, Scalability, Accuracy},
	pages = {3485--3493},
}

@inproceedings{okuhara_enhancing_2024,
	title = {Enhancing {Panoramic} {Competency} {Through} {Link} {Prediction} in {Question} {Knowledge} {Graphs} using a {Language} {Representation} {Model}},
	doi = {10.1109/CCET62233.2024.10837958},
	abstract = {Recently, panoramic knowledge has been required. On the other hand, multiple choice questions are suitable for efficient self-learning. Therefore, the purpose of this study is to create multiple choice questions that can reinforce learners' panoramic knowledge. Specifically, we proposed a method for automatically generating multiple choice questions that use Linked Data to present relevant information to give respondents an overall picture of relevant knowledge. There is some research on the methods that generated questions by extracting small subgraphs from the knowledge graphs consisting of entities(words) and relations(links) between the entities and hiding target words (correct answer words). In this study, our goal is to enhance the panoramic of the subgraphs of a specified size by using the link prediction method to complement edges and represent relationships not present in the knowledge graph when generating questions targeted at specific fields. The method of complementing edges involves first inputting two words as subject and object in Knowledge Graph to calculate the cosine similarity using a pretrained language model based on Wikipedia and Wikidata, then predicting the links as a predicate that should be complemented, and finally generating subgraphs by using the Graph Database added the complemented edges. For this study, we generated questions in the field of history, and since history requires temporal and spatial panoramic knowledge, words related to these aspects were focused on and complemented the relationships between them. As a result, 2,746 relationships were complemented by the proposed method in the subgraphs, and the subgraphs contained more words to learn (words found in textbooks that need to be learned) in a specific field compared to those generated using existing methods.},
	booktitle = {2024 {IEEE} 7th {International} {Conference} on {Computer} and {Communication} {Engineering} {Technology} ({CCET})},
	author = {Okuhara, Fumika and Egami, Shusaku and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
	month = aug,
	year = {2024},
	note = {ISSN: 2836-5992},
	keywords = {Knowledge graphs, Linked data, Knowledge Graph, Linked Data, Databases, Computational modeling, History, Predictive models, Hands, Encyclopedias, Online services, Automatic Generate Question, Educational Application, Panoramic Knowledge},
	pages = {267--272},
}

@inproceedings{yang_llm_2024,
	title = {An {LLM} supported approach to ontology and knowledge graph construction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280910&doi=10.1109%2FBIBM62325.2024.10822222&partnerID=40&md5=5fb25a53e38880819cbef3caab99f1a6},
	doi = {10.1109/BIBM62325.2024.10822222},
	abstract = {The continuous development in the medical field faces multiple challenges in managing a large amount of literature and research results using traditional ontology and knowledge graph construction methods. These challenges include high labor costs, limited coverage, and poor dynamism of traditional ontology and knowledge graph construction methods. Large language models (LLMs) can solve various natural language processing tasks and can understand and generate human-like natural language, which makes automated construction of ontology expansion and knowledge graphs (KGs) possible. This paper proposes an ontology expansion method based on LLMs, using LLMs to formulate competency questions (CQs) to extend the initial ontology, and then constructing the knowledge graph based on the extended ontology. We demonstrated the feasibility of the method by creating a knowledge graph for breast cancer treatment. The combination of LLMs-based medical ontology and knowledge graph can achieve more efficient medical knowledge management and application, promoting the informatization and intelligent development of the medical field.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
	month = dec,
	year = {2024},
	note = {ISSN: 2156-1133},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Large language model, Ontology, LLM, Natural language processing, Language model, Large language models, Semantic Web, Reliability, Usability, Medical knowledge, Iterative methods, Natural languages, Refining, Medical services, Breast cancer treatment, Ontology's, Natural language processing systems, Ontology graphs, Graphitization, Graph-construction method, Medical fields, Wages},
	pages = {5240--5246},
	annote = {Cited by: 0},
}

@inproceedings{chen_genesum_2024,
	title = {{GeneSum}: {Large} {Language} {Model}-based {Gene} {Summary} {Extraction}},
	doi = {10.1109/BIBM62325.2024.10822279},
	abstract = {Emerging topics in biomedical research are continuously expanding, providing a wealth of information about genes and their function. This rapid proliferation of knowledge presents unprecedented opportunities for scientific discovery and formidable challenges for researchers striving to keep abreast of the latest advancements. One significant challenge is navigating the vast corpus of literature to extract vital gene-related information, a time-consuming and cumbersome task. To enhance the efficiency of this process, it is crucial to address several key challenges: (1) the overwhelming volume of literature, (2) the complexity of gene functions, and (3) the automated integration and generation. In response, we propose GeneSum, a two-stage automated gene summary extractor utilizing a large language model (LLM). Our approach retrieves and eliminates redundancy of target gene literature and then fine-tunes the LLM to refine and streamline the summarization process. We conducted extensive experiments to validate the efficacy of our proposed framework. The results demonstrate that LLM significantly enhances the integration of gene-specific information, allowing more efficient decision-making in ongoing research.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Chen, Zhijian and Hu, Chuan and Wu, Min and Long, Qingqing and Wang, Xuezhi and Zhou, Yuanchun and Xiao, Meng},
	month = dec,
	year = {2024},
	note = {ISSN: 2156-1133},
	keywords = {large language model, Large language models, Bioinformatics, Data mining, Decision making, Redundancy, prompt learning, Complexity theory, Biological system modeling, Navigation, Gene summary},
	pages = {1438--1443},
}

@inproceedings{chen_extracting_2024,
	title = {Extracting {Structure} {Information} from {Narrative} {Medical} {Reports} based on {LLMs}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280093&doi=10.1109%2FBIBM62325.2024.10822688&partnerID=40&md5=2ef76c9f7b1b72efc12493a0c6955074},
	doi = {10.1109/BIBM62325.2024.10822688},
	abstract = {Extracting structured information and key details from medical report narratives is crucial to support healthcare data management, analysis and decision-making. However, the specialized nature of the reports, the complexity of the contents, and the high accuracy requirements of the results pose significant challenges to the structuring task. In this paper, we develop an LLM-based method to extract structure information from medical report narratives. Defining the structuring problem as mapping the narrative reports to the domain ontology, we design a framework to develop specialized LLMs that automatically learn and establish the mappings. At the core of this framework are report partitioning and interactive training data generation modules are. By separating complete reports into logically independent segments and training the LLMs on these segments independently, the trained LLMs can accurately capture the semantic relationships within each segment. Additionally, we explore different LLMs and formulate a simplistic scoring method to compare their accuracy, enabling us to select the best-performing model. Experimental evaluation on a real-world breast ultrasound report dataset demonstrates that our method achieves high accuracy with a small training dataset (400 samples). Specifically, the accuracy of structural information extraction and the attribute-value matching accuracy both exceed 96\%.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Chen, Dehua and Shen, Zijian and Wang, Mei and Dong, Na and Pan, Qiao and Su, Jianwen},
	month = dec,
	year = {2024},
	note = {ISSN: 2156-1133},
	keywords = {Ontologies, Large language model, Ontology, Language model, Large language models, Semantics, Information retrieval, Data mining, Standards, Training, Accuracy, Training data, Medical services, Medical examination reports, Report structuring, Ultrasonic imaging, Data accuracy, High-accuracy, Structured information, Data decision, Management analysis, Management decisions, Medical examination report, Structure information},
	pages = {5616--5623},
	annote = {Cited by: 1},
}

@inproceedings{qi_improved_2024,
	title = {An {Improved} {Method} for {Phenotype} {Concept} {Recognition} {Using} {Rich} {HPO} {Information}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280863&doi=10.1109%2FBIBM62325.2024.10822556&partnerID=40&md5=1a5a2eaea86f6b840c3b2833d8c1f091},
	doi = {10.1109/BIBM62325.2024.10822556},
	abstract = {Automatically identifying human phenotype ontology (HPO) concepts from text is important for disease analysis. Existing ontology-driven methods for phenotype concept recognition mainly rely on concept names and synonym information from the ontology, without fully exploiting the rich ontology information. In this paper, we present an improved phenotype concept recognition method by incorporating rich HPO information. We first design prompts with HPO information and use a cutting-edge large language model GPT-4 to generate synonym augmentation for expanding distant supervised training data. We then propose an ontology vector-enhanced phenotype concept classification model to efficiently integrate the taxonomic hierarchical structure of HPO. Additionally, we employ noisy data augmentation to improve the model’s recognition ability in noisy texts and implement a negation detection function. Experimental results on three standard corpora and two typo corpora show our method compares favorably to previous methods and achieves a significant improvement in noisy texts. The source code and data are freely available at https://github.com/DUTIR-BioNLP/PhenoTagger-Updates.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Qi, Jiewei and Luo, Ling and Yang, Zhihao and Wang, Jian and Zhou, Huiwei and Lin, Hongfei},
	month = dec,
	year = {2024},
	note = {ISSN: 2156-1133},
	keywords = {Ontologies, Large language models, Human phenotype ontology, Human Phenotype Ontology, Concept recognition, Reliability, Standards, Data augmentation, Phenotypes, Disease analysis, Training data, Source coding, Diseases, Noise measurement, Ontology Information Enhancement, Phenotype Concept Recognition, Ontology's, Ontology concepts, First designs, Ontology information enhancement, Phenotype concept recognition, Recognition methods},
	pages = {1135--1140},
	annote = {Cited by: 0},
}

@inproceedings{sugioka_bert_2024,
	title = {{BERT} {Pre}-training for {Cooking} {Time} {Prediction} from {Cooking} {Recipes}},
	doi = {10.1109/CANDARW64572.2024.00032},
	abstract = {Recently, websites that allow ordinary users to share and search for recipes have become popular. Typically, each recipe contains various information such as a title, a list of ingredients, and descriptions of the cooking process through text and photos. The estimated cooking time is another valuable piece of information when selecting a recipe. However, it can be difficult to accurately describe cooking time because it depends on the cooking environment and conditions, such as heat level and quantity. Therefore, some recipes do not include cooking time information. In this study, we consider the prediction of cooking time in general cases from the list of ingredients and the textual description of the cooking process of each recipe by BERT, a natural language processing model. For this purpose, we propose an additional pre-training method that weights words related to cooking time from a cooking ontology. Our experimental results show that our methods outperform a fine-tuned BERT model with additional pre-training by a standard method.},
	booktitle = {2024 {Twelfth} {International} {Symposium} on {Computing} and {Networking} {Workshops} ({CANDARW})},
	author = {Sugioka, Koki and Kamei, Sayaka and Morimoto, Yasuhiko},
	month = nov,
	year = {2024},
	note = {ISSN: 2832-1324},
	keywords = {Ontologies, Natural language processing, BERT, Standards, Computational modeling, Conferences, Predictive models, Cooking recipes, domain-adaptive pre-training, Heating systems},
	pages = {157--162},
}

@inproceedings{anaguchi_reasoning_2024,
	title = {Reasoning and {Justification} {System} for {Domestic} {Hazardous} {Behaviors} {Based} on {Knowledge} {Graph} of {Daily} {Activities} and {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/CANDAR64496.2024.00010},
	abstract = {Accidents among people over 65 years of age predominantly occur within residential settings, making the maintenance of a safe home environment a crucial social issue. To address this issue, previous research has developed systems that construct Knowledge Graphs (KG) based on simulations of daily household activities, and studies have been conducted on detecting hazardous behaviors using such KG analysis. In this current study, we propose a system capable of presenting the reason and justification for the detected domestic hazardous behaviors. Our system will first generates the reason for the detected behavior using a Large Language Model (LLM). To ensure the accuracy, reliability and reproducibility of the LLM output, the system will provides reliable sources to support the output. We employed Retrieval-Augmented Generation (RAG) to search for sentences similar to the reason generated by the LLM within reliable, authoritative documents describing domestic accident cases and their causes and these will be presented as the evidence alongside the search engine results to the users. Consequently, a knowledge graph (KG) of domestic hazardous behavior is developed based on evidence ontology. Finally, to evaluate the ability of our proposed system in appropriately generating reasons for domestic hazardous behaviors and the adequacy of the justifications provided, the output was rated using LLMs and human volunteers. The rating results showed a significant correlation between LLMs and human evaluation, indicating that the proposed system can provide sufficient reasons and justifications for domestic hazardous behaviors at residential setting.},
	booktitle = {2024 {Twelfth} {International} {Symposium} on {Computing} and {Networking} ({CANDAR})},
	author = {Anaguchi, Fumikatsu and Chakraborty, Sudesna and Morita, Takeshi and Egami, Shusaku and Ugai, Takanori and Fukuda, Ken},
	month = nov,
	year = {2024},
	note = {ISSN: 2379-1896},
	keywords = {Ontologies, Knowledge graphs, Retrieval augmented generation, Search engines, Knowledge Graph, Large Language Model, Retrieval-Augmented Generation, Sensors, Optimization, Explainable AI, Correlation, Accidents, Behavioral sciences, Reproducibility of results},
	pages = {11--20},
}

@inproceedings{sadlek_hierarchical_2024,
	title = {Hierarchical {Modeling} of {Cyber} {Assets} in {Kill} {Chain} {Attack} {Graphs}},
	doi = {10.23919/CNSM62983.2024.10814501},
	abstract = {Cyber threat modeling is a proactive method for identifying possible cyber attacks on network infrastructure that has a wide range of applications in security assessment, risk analysis, and threat exposure management. Popular modeling methods are kill chains and attack graphs. Kill chains divide attacks into phases, and attack graphs depict attack paths. A difficult issue is how to hierarchically model categories of cyber assets that should be used in threat models due to the variety of cyber systems in the current networks. This task should be addressed to provide automation of realistic threat modeling and interoperability with public knowledge bases, such as MITRE ATT\&CK. In this paper, we propose a hierarchical modeling methodology for representing cyber assets in kill chain attack graphs. We illustrate its practical application on MITRE D3FEND’s Digital Artifact Ontology. Moreover, we define how cyber assets with related attack techniques should be transformed into logical facts and attack rules. We implemented proof-of-concept software modules that can process data obtained from network and host-based monitoring together with attack rules to generate attack graphs. We evaluated the approach with data from a cyber exercise captured in a network of a digital twin organization. The results show that the approach is applicable in real-world networks and can reveal ground-truth attacks.},
	booktitle = {2024 20th {International} {Conference} on {Network} and {Service} {Management} ({CNSM})},
	author = {Sadlek, Lukáš and Husák, Martin and Čeleda, Pavel},
	month = oct,
	year = {2024},
	note = {ISSN: 2165-963X},
	keywords = {Ontologies, Automation, Large language models, Security, Software, Knowledge based systems, Risk analysis, Organizations, Monitoring, Threat modeling, attack graph, cyber threat scenario, kill chain, MITRE ATT\&CK, MITRE D3FEND},
	pages = {1--5},
}

@inproceedings{sophaken_leveraging_2024,
	title = {Leveraging {Graph}-{RAG} for {Enhanced} {Diagnostic} and {Treatment} {Strategies} in {Dentistry}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216814042&doi=10.1109%2FInCIT63192.2024.10810521&partnerID=40&md5=01a0bd3872977dfe4e639c5983f17ebc},
	doi = {10.1109/InCIT63192.2024.10810521},
	abstract = {This paper presents a method for extracting and interpreting information from diverse, unstructured dental literature using advanced AI techniques. By integrating information extraction, ontologies, and knowledge graphs, the approach enhances the efficiency and accuracy of dental data analysis. Named Entity Recognition (NER) and a Large Language Model (LLM) are employed to extract relevant entities and relationships, which are then structured into triples and integrated with a dental ontology to ensure contextual relevance. This enriched ontology supports Retrieval-Augmented Generation (RAG) applications, enabling advanced querying and analysis. The methodology improves the identification and categorization of dental conditions, treatments, and anatomical terms, providing a structured representation of dental knowledge. Knowledge graphs facilitate the representation and analysis of relationships between entities, fostering insightful interpretations and supporting hypothesis generation, thereby enhancing the accessibility and usability of dental knowledge. Experimental results demonstrate the effectiveness of this approach in managing complex dental information, showcasing the benefits of combining Knowledge Representation (KR) with Machine Learning (ML). This research contributes to dental studies by offering a robust framework for extracting and utilizing knowledge from diverse and extensive datasets.},
	booktitle = {2024 8th {International} {Conference} on {Information} {Technology} ({InCIT})},
	author = {Sophaken, Chotanansub and Vongpanich, Kantapong and Intaphan, Wachirawit and Utasri, Tharathon and Deepho, Chutamas and Takhom, Akkharawoot},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Large Language Models, Information extraction, Knowledge graph, Large language model, Ontology, Language model, Named entity recognition, Large language models, Retrieval augmented generation, Knowledge Graphs, Machine learning, Diagnosis, Data mining, Usability, Information Extraction, Dentistry, Information technology, Dental Literature, Oral Health, Structured Query Language, Ontology's, AI techniques, Dental literature, Extraction ontologies, Integrating information, Oral healths},
	pages = {606--611},
	annote = {Cited by: 0},
}

@inproceedings{cornelio_recover_2024,
	title = {Recover: {A} {Neuro}-{Symbolic} {Framework} for {Failure} {Detection} and {Recovery}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216491450&doi=10.1109%2FIROS58592.2024.10801853&partnerID=40&md5=49e070f22026e9cc1ce47e917b811109},
	doi = {10.1109/IROS58592.2024.10801853},
	abstract = {Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor’s logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs. Supplementary material, including the OntoThor ontology, is available at: https://recover-ontothor.github.io.},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Cornelio, Cristina and Diab, Mohammed},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Ontologies, Ontology, Language model, Large language models, Robotics, Costs, Intelligent robots, Ontology's, Traditional approaches, Traditional approachs, Failure detection and recoveries, High costs, Logical rules, Offline, Recovery procedure, Task executions},
	pages = {12435--12442},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{nakajima_combining_2024,
	title = {Combining {Ontological} {Knowledge} and {Large} {Language} {Model} for {User}-{Friendly} {Service} {Robots}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216487892&doi=10.1109%2FIROS58592.2024.10802273&partnerID=40&md5=ab838891e345c0c78ded5553dc4f308d},
	doi = {10.1109/IROS58592.2024.10802273},
	abstract = {Lifestyle support through robotics is an increasingly promising field, with expectations for robots to take over or assist with chores like floor cleaning, table setting and clearing, and fetching items. The growth of AI, particularly foundation models, such as large language models (LLMs) and visual language models (VLMs), is significantly shaping this sector. LLMs, by facilitating natural interactions and providing vast general knowledge, are proving invaluable for robotic tasks. This paper focuses on the benefits of LLMs for "bring-me" tasks, where robots fetch specific items for users, often based on ambiguous instructions. Our previous efforts utilized an ontology extended to handle environmental data to resolve such ambiguities, but faced limitations when unresolvable ambiguities required user intervention for clarity. Here, we enhance our approach by integrating LLMs for providing additional commonsense knowledge, pairing it with ontological data to mitigate the issue of hallucinations and reduce the need for user queries, thus improving system usability. We present a system that merges these knowledge bases and assess its efficacy on "bring-me" tasks, aiming to provide a more seamless and efficient robotic assistance experience.},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Nakajima, Haru and Miura, Jun},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Ontologies, Ontology, Language model, Large language models, Modeling languages, Knowledge based systems, Service robots, Foundation models, Usability, Commonsense reasoning, Visualization, Visual language model, Intelligent robots, Floors, Ontology's, Visual languages, Microrobots, Floor cleaning, General knowledge, Natural interactions, Robotic tasks, User friendly},
	pages = {4755--4762},
	annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{li_tcmrd-kg_2024,
	title = {{TCMRD}-{KG}: {Design} and {Development} of a {Rheumatism} {Knowledge} {Graph} {Based} on {Ancient} {Chinese} {Literature}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216679297&doi=10.1109%2FMedAI62885.2024.00083&partnerID=40&md5=2403f6bb0f32ac49a26147694d9bc8bb},
	doi = {10.1109/MedAI62885.2024.00083},
	abstract = {The use of Traditional Chinese medicineTCM in rheumatic diseases dates back to thousands of years ago. Compared with standardized treatment, TCM has the advantages of low cost, low side effects, and flexible medication. Ancient books of traditional Chinese medicine play an important role in clinical and scientific research. This study takes the content related to rheumatism in ancient books of traditional Chinese medicine as the research object, integrates the ontology theory and technology in the knowledge graph, realizes the reconstruction of traditional Chinese medicine information knowledge, and provides basic data structure for data mining and knowledge discovery. This study is the first rheumatism-specific knowledge graph constructed based on ancient books of traditional Chinese medicine; it has tried the construction method of knowledge graph of ancient books of traditional Chinese medicine by combining automatic labeling of mainstream large language models with manual review; and according to the knowledge characteristics of ancient books of traditional Chinese medicine, the existing word segmentation technology is difficult to accurately reproduce the accurate meaning of the original text of ancient books, a new type of entity extraction method is given.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Medical} {Artificial} {Intelligence} ({MedAI})},
	author = {Li, Haotian and Xia, Congmin and Hou, Youjuan and Hu, Sile and Quan, Jiang and Liu, Yanjun},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Knowledge discovery, Large language models, Knowledge Graph, Data mining, Reviews, Traditional Chinese Medicine, Data structures, Manuals, Labeling, Diseases, Rheumatic Diseases, Graph-based, Clinical research, Design and Development, Chinese literature, Low-costs, Rheumatic disease, Scientific researches, Side effect},
	pages = {588--593},
	annote = {Cited by: 0},
}

@inproceedings{chow_semantic_2024,
	title = {Semantic {Search} {Using} {LLM}-{Aided} {Topic} {Generation} on {Knowledge} {Graphs} for {Paper} {Discovery}},
	doi = {10.1109/ISCSLP63861.2024.10800417},
	abstract = {The exponential growth of academic papers presents a huge challenge for researchers, exacerbating the already tedious literature review process. Current tools like Google Scholar and Connected Papers offer solutions for text-based and citation-based searches but fail to address the need for finding semantically similar yet terminologically different papers efficiently. This paper proposes an innovative approach to paper discovery using semantic search to create a knowledge graph of topics and papers. By generating a tree of topics using ChatGPT 4o and calculating semantic similarity with SciBERT, this method aims to uncover relevant papers overlooked by traditional citation-based searches. The solution, validated through quantitative evaluation, demonstrates the potential to improve the efficiency and comprehensiveness of paper discovery.},
	booktitle = {2024 {IEEE} 14th {International} {Symposium} on {Chinese} {Spoken} {Language} {Processing} ({ISCSLP})},
	author = {Chow, Sabrina and Guo, Lilian and Chow, Jonathan and Chia, Chelsea and Li, Sarah and Huang, Dong-Yan},
	month = nov,
	year = {2024},
	keywords = {Knowledge graphs, Knowledge Graphs, Semantic search, Natural Language Processing (NLP), Semantic Search, Chatbots, Internet, Literature Review, Focusing, Rendering (computer graphics), Navigation, Bibliographies, SciBERT},
	pages = {353--357},
}

@inproceedings{wang_research_2024-1,
	title = {Research on {Knowledge} {Graph} {Extraction} {Methods} for {Chinese} {STEM} {Curriculum}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215598070&doi=10.1109%2FMLNLP63328.2024.10800180&partnerID=40&md5=6e0a74fed87abe5a73acc1ed74985a8b},
	doi = {10.1109/MLNLP63328.2024.10800180},
	abstract = {STEM education, as an innovative teaching model, has gained widespread attention in recent years. However, the lack of relevant textbooks and learning resources has made its implementation challenging. Developing interdisciplinary knowledge graphs tailored for STEM education has become an urgent issue. To address this, a knowledge extraction framework named Llms4edu is proposed, which utilizes a series of effective prompts to guide large language models in knowledge extraction. Specifically, the knowledge extraction task is transformed into multiple rounds of question-and-answer interactions with the LLM, gradually identifying entity-relation triplets from subject data. Through experiments, an F1-score of 89.4\% was achieved on the named entity recognition task in the chemistry subject, and an F1-score of 66.7\% on the relation extraction task. Finally, a subject ontology model was built for subject text, and a subject data set was constructed using Llms4edu, which includes three subjects of junior high school mathematics, physics, and chemistry, a total of 2,511 entities, 2,010 relationship triples, and cross-disciplinary knowledge is linked to construct a cross-disciplinary knowledge graph.},
	booktitle = {2024 7th {International} {Conference} on {Machine} {Learning} and {Natural} {Language} {Processing} ({MLNLP})},
	author = {Wang, Changlong and Sang, Xiujuan and Wang, Xijie and Gao, Yuan and Liu, Yi},
	month = oct,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {large language model, Ontologies, Knowledge graphs, Knowledge graph, Large language model, Language model, Named entity recognition, Large language models, Knowledge extraction, Knowledge engineering, Annotations, Data mining, Prompt engineering, prompt engineering, Training, Federated learning, STEM education, Adversarial machine learning, Chemistry, interdisciplinary knowledge graph, Physics, Contrastive Learning, F1 scores, Graph extractions, Cross-disciplinary, Interdisciplinary knowledge graph},
	pages = {1--8},
	annote = {Cited by: 1},
}

@inproceedings{lee_cao_2024,
	title = {Cao {Robot} for {Taiwanese}/{English} {Knowledge} {Graph} {Application}},
	doi = {10.1109/O-COCOSDA64382.2024.10800729},
	abstract = {This paper proposes a Content Attention Ontology (CAO) robot for constructing Taiwanese/English Knowledge Graphs (KGs) by prompting audio or texts to Large Language Models (LLMs), including TAIDE, Zephyr, and Llama 3.1. The collected data includes lecture videos from the IEEE WCCI 2024 in Japan and the 2024 National Language Development Forum in Taiwan, along with students' learning data from the 2024 Summer School on Taiwanese/English Human and Robot Co-Learning at Rende Elementary School (RDES). In addition, the fundamental concepts of Computational Intelligence (CI) and Quantum CI (QCI) learning were incorporated into the study. The generative KGs highlight important concepts, relations, and communities within the collected teaching and learning data. Additionally, we utilized data from subjects wearing braincomputer interface (BCI) devices while speaking Taiwanese/English to generate KGs. We also compared the differences in these KGs and analyzed the similarities between the transcribed texts of lectures and learners. In the future, we plan to expand the CAO robot to more validation fields across Taiwan, aiming to engage young students in speaking Taiwanese while concurrently enhancing their English language skills through interaction with the robot.},
	booktitle = {2024 27th {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	author = {Lee, Chang-Shing and Wang, Mei-Hui and Tseng, Guan-Ying and Yue, Chao-Cyuan and Hsieh, Hao-Chun and Reformat, Marek},
	month = oct,
	year = {2024},
	note = {ISSN: 2472-7695},
	keywords = {Ontologies, Knowledge graphs, Large language models, Knowledge Graph, Large Language Model, Measurement, Quantum computing, Videos, Robots, CAO Robot, Llama 3.1, Physiology, Speech enhancement, Statistical analysis, TAIDE, Taiwanese/English Language Co-learning},
	pages = {1--6},
}

@inproceedings{krouwel_revising_2024,
	title = {Revising the {DEMO} method: modelling wait links},
	doi = {10.1109/CBI62504.2024.00030},
	abstract = {The DEMO method and modelling language for enterprises has evolved over the past 30 years. Extensive work has been done to specify the modelling language to create DEMO based modelling tools and code generators. However, many issues have been identified regarding the adoption, readability, and completeness of DEMO specification.In this paper, we focus on the Process Model that has several issues regarding its relation with the Action Model and wait links. The proposed solution encompasses adjusting the visualisation of the fact kinds in the Process Model so that they can directly be linked to the event part of the Action Rules Specifications in the Action Model. Some other suggestions for improving the comprehensibility of the Process Model and the impact on DEMO are included. The proposed solution is illustrated with three cases and evaluated against the expected benefits. Preliminary results show an improved readability and easier creation of the Action Model that comply with the Process Model. More research is needed to validate the proposed solution with experts and to solve other issues regarding DEMO.},
	booktitle = {2024 26th {International} {Conference} on {Business} {Informatics} ({CBI})},
	author = {Krouwel, Marien R. and Mulder, Mark A.T.},
	month = sep,
	year = {2024},
	note = {ISSN: 2378-1971},
	keywords = {Informatics, Visualization, DEMO, Business, Codes, Symbols, Generators, Action Model, Enterprise Design, Enterprise Implementation, Enterprise Ontology, Process Model},
	pages = {188--197},
}

@inproceedings{ye_predicting_2024,
	title = {Predicting {Functional} {Surface} {Topographies} {Combining} {Topological} {Data} {Analysis} and {Deep} {Learning} {Across} the {Human} {Protein} {Universe}},
	doi = {10.1109/EMBC53108.2024.10782681},
	abstract = {Characterizing geometric and topological properties of protein structures encompassing surface pockets, interior cavities, and cross channels is important for understanding their functions. Our knowledge of protein structures has been greatly advanced by AI-powered structure prediction tools, with AlphaFold2 (AF2) providing accurate 3D structure predictions for most protein sequences. Nonetheless, there is a substantial lack of function annotations and corresponding functional surface topographical information. We develop a method to predict functional pockets, along with their associated Gene Ontology (GO) terms and Enzyme Commission (EC) numbers, for a set of 65,013 AF2-predicted human non-singleton representative structures, which can be mapped to 186,095 "non-fragment" AF2-predicted human protein structures. The identification of functional pockets, along with their respective GO terms and EC numbers, is achieved by combining topological data analysis and the deep learning method of DeepFRI. All predicted functional pockets for these 65,013 AF2-predicted human representative structures are accessible at: https://cfold.bme.uic.edu/castpfold.},
	booktitle = {2024 46th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Ye, Bowei and Liang, Jie},
	month = jul,
	year = {2024},
	note = {ISSN: 2694-0604},
	keywords = {Ontologies, Deep learning, Knowledge engineering, Annotations, Data analysis, Databases, Proteins, Three-dimensional displays, Enzymes, Surface topography},
	pages = {1--4},
}

@inproceedings{munzir_high_2024,
	title = {High {Throughput} {Phenotyping} of {Physician} {Notes} with {Large} {Language} and {Hybrid} {NLP} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214971917&doi=10.1109%2FEMBC53108.2024.10782119&partnerID=40&md5=75f9d263408bc17def9755cb2816a48a},
	doi = {10.1109/EMBC53108.2024.10782119},
	abstract = {Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past 30 years, progress toward making high-throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping physician notes.Clinical relevance: Large language models will likely emerge as the dominant method for the high throughput phenotyping of signs and symptoms in physician notes},
	booktitle = {2024 46th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Munzir, Syed I. and Hier, Daniel B. and Carrithers, Michael D.},
	month = jul,
	year = {2024},
	note = {ISSN: 2694-0604},
	keywords = {Ontologies, Language model, Large language models, machine learning, natural language processing, Machine learning, Natural Language Processing, Phenotyping, Machine Learning, phenotype, Electronic health record, electronic health record, Phenotype, physician, Electronic Health Records, Accuracy, Electronic medical records, human, Biological system modeling, Vectors, Medical services, Throughput, Assistive technologies, Humans, Ontology's, Machine-learning, Electronic health, Health records, High-throughput method, High-throughput phenotyping, Learning classifiers, Physicians, Word vectors},
	pages = {1--5},
	annote = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{piazza_large_2024,
	title = {Large {Language} {Models} for {Automatic} {Standardization} of {Cyber} {Deception} {Plans} based on the {Adversary} {Engagement} {Ontology}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214561637&doi=10.1109%2FMILCOM61039.2024.10773797&partnerID=40&md5=6c47ba88a0ce6b2a790968dfdddcf22a},
	doi = {10.1109/MILCOM61039.2024.10773797},
	abstract = {Adversary Engagement Ontology (AEO) is a candidate ontology for the Unified Cyber Ontology (UCO), a community effort aimed at ontological standardization of cyber domain concepts and objects under a unifying framework. It forms a part of the Cyber Domain Ontology (CDO). In the past, community efforts and development have always been labor-intensive with regards to changes in ontology, example generation for adopters, and documentation generation. Large Language Models (LLMs), such as Claude-3.5-Sonnet and GPT4, have been proven capable of automating many tasks and aiding in human expert decision-making. Additionally, LLMs have been used in code interpretation, generation, and evaluation with efficiency and accuracy comparable to that of humans. This emergent capability of LLMs has led to the advantage of using LLMs to streamline the process of ontology development. Motivated by the aforementioned-approaches, we aim to demonstrate how these foundational LLMs can assist in ontology example generation and development, as well as be utilized to automate structured, albeit tedious tasks.},
	booktitle = {{MILCOM} 2024 - 2024 {IEEE} {Military} {Communications} {Conference} ({MILCOM})},
	author = {Piazza, Nancirose and Upadhayay, Bibek and Scarpa, Ronald and Behzadan, Vahid},
	month = oct,
	year = {2024},
	note = {ISSN: 2155-7586},
	keywords = {Ontologies, Ontology, Natural language processing, Language model, Large language models, Domain ontologies, Ontology development, Standardization, Decision making, Accuracy, Documentation, Codes, Military communication, Adversary Engagement, Ontology's, Decisions makings, Human expert, Adversary engagement, Domain concepts, Labour-intensive, Plan-based},
	pages = {1--5},
	annote = {Cited by: 0},
}

@inproceedings{mohsenzadegan_hybrid_2024,
	title = {A {Hybrid} {AI} {Framework} {Integrating} {Ontology} {Learning}, {Knowledge} {Graphs}, and {Large} {Language} {Models} for {Improved} {Data} {Model} {Translation} in {Smart} {Manufacturing} and {Transportation}},
	doi = {10.1109/SDF63218.2024.10773919},
	abstract = {Interoperability among diverse data standards is crucial for advancing digital technologies in smart manufacturing and transportation. This paper studies and presents a hybrid AI framework that integrates Ontology Learning (OL), Knowledge Graphs (KGs), and Large Language Models (LLMs) to enhance data translation across different standards. Focusing on IEEE 1451, ISO 15926, and IEC 61499, which exemplify the challenges of translating between distinct data models, we evaluate the performance of OL, KGs, and LLMs in terms of accuracy, scalability, efficiency, robustness, and flexibility. The findings indicate that the hybrid framework effectively leverages OL for semantic structuring, KGs for relational modeling, and LLMs for linguistic and contextual processing. This integration significantly improves the accuracy and adaptability of data translations, offering a comprehensive solution tailored to the complex environments of smart manufacturing and transportation, thereby advancing cross-standard data interoperability.},
	booktitle = {2024 {Sensor} {Data} {Fusion}: {Trends}, {Solutions}, {Applications} ({SDF})},
	author = {Mohsenzadegan, Kabeh and Tavakkoli, Vahid and Kambale, Witesyavwirwa Vianney and Kyamakya, Kyandoghere},
	month = nov,
	year = {2024},
	note = {ISSN: 2473-7666},
	keywords = {Ontologies, Knowledge graphs, Interoperability, Large language models, Large Language Models (LLMs), Data integration, Smart manufacturing, Standards, Transportation, Data models, Semantic Mapping, Smart Manufacturing, Accuracy, Data Interoperability, AI in Transportation, Cross-Standard Data Integration, Data Model Translation, Hybrid AI Framework, Knowledge Graphs (KGs), Ontology Learning (OL)},
	pages = {1--6},
}

@inproceedings{li_large_2024,
	title = {A {Large} {Language} {Model} {Based} {Knowledge} {Mining} {Method} for {Improving} the {Reliability} of {Fire} {Water} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215273182&doi=10.1109%2FSRSE63568.2024.10772514&partnerID=40&md5=0b65baf1820c442bba07118662a1fbd9},
	doi = {10.1109/SRSE63568.2024.10772514},
	abstract = {The fire water system plays a critical role in protecting both infrastructure and human lives. An essential aspect of enhancing the reliability of this system is fault diagnosis. However, the current fault diagnosis methods primarily rely on data-driven approaches, which often result in a high threshold for application due to their lack of interpretability. To tackle this challenge, this paper introduces a novel approach based on large language models for knowledge mining from textual data to extract fault information related to the fire water system, thereby enhancing the interpretability of data-driven fault diagnosis methods. The methodology followed in this paper consists of two main steps: firstly, analyzing the characteristics and principles of fire water system faults to develop a fault ontology, and secondly, creating a knowledge mining model using a large language model guided by the established fault ontology. Experimental findings indicate that the proposed model achieves an F1 score of 0.944, meeting the necessary criteria for effective knowledge mining in fire water system fault analysis. Furthermore, a comparative experiment was conducted to evaluate the performance of various encoder models, including GRU, BiGRU, LSTM, BiLSTM, and pre-trained large language model BERT. The results revealed a significant improvement in performance with the BERT encoder, showing increases in F1 scores of 22.12 \%, 2.27 \%, 17.41 \%, and 3.16 \% compared to the other models, respectively. This study provides valuable interpretative insights that can enhance the engineering applicability and reliability of data-driven fault diagnosis methods in fire water system.},
	booktitle = {2024 6th {International} {Conference} on {System} {Reliability} and {Safety} {Engineering} ({SRSE})},
	author = {Li, Yi and Tian, Liwei and Yi, Chengyi and Li, Jingjing and Qin, Xiaodong and He, Yuxuan and Su, Huai},
	month = oct,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {large language model, Ontologies, Large language model, Language model, Large language models, Fault diagnosis, Data mining, Knowledge based systems, safety engineering, Intelligent systems, Interpretability, Knowledge mining, Water, Accuracy, Encoding, Bidirectional control, Analytical models, Reliability engineering, fire water system, knowledge mining, system reliability, Systems analysis, Data-driven fault diagnosis, Fault diagnosis method, Fire protection, Fire water system, Mine fires, System faults, System reliability, Water system},
	pages = {410--413},
	annote = {Cited by: 0},
}

@inproceedings{chusova_hybrid_2024,
	title = {A {Hybrid} {Approach} to {Extraction} of {Knowledge} {From} {Scientific} {Texts} {Based} on {Large} {Language} {Models} and {Domain} {Dictionaries}},
	doi = {10.1109/SIBIRCON63777.2024.10758538},
	abstract = {The vast information landscape of the Internet constitute a significant challenge for extracting valuable content. The lack of standardized data models and structures necessitates ad hoc solutions, often requiring expert knowledge that developers may lack. While Large Language Models (LLMs) hold promise for addressing this challenge, their susceptibility to AI hallucinations and inaccuracies necessitates ongoing research. This paper introduces a hybrid approach for extracting information on computational methods that combines domain-specific dictionaries with LLMs in order to improve the accuracy of method categorization. Our system incorporates explainability, allowing users to understand the reasoning behind method assignments. Furthermore, user-driven training is facilitated by allowing users to select theories and highlight relevant keywords, enhancing learning capabilities of the system. Its implementation demonstrates the effectiveness of this approach, achieving an impressive Fl-score of up to 90.3 \%. This research contributes to the ongoing effort to develop robust and accurate knowledge extraction systems for navigating the ever-expanding landscape of online information.},
	booktitle = {2024 {IEEE} {International} {Multi}-{Conference} on {Engineering}, {Computer} and {Information} {Sciences} ({SIBIRCON})},
	author = {Chusova, Alina and Artemieva, Irina and Chusov, Andrey},
	month = sep,
	year = {2024},
	note = {ISSN: 2995-0996},
	keywords = {Ontologies, Large language models, Large Language Model, Data mining, Software, Dictionary, Training, Computational modeling, Accuracy, Dictionaries, Acoustics, Navigation, Acoustic models, Classilication},
	pages = {266--271},
}

@inproceedings{govindharajan_framework_2024,
	title = {A {Framework} for automated selective {Fine}-{Tuning} of {Domain}-{Specific} {Large} {Language} {Models} {Using} {Graph}-{Based} {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/UEMCON62879.2024.10754778},
	abstract = {Graph based retrieval augmented generation technique in Large Language Model (LLM) brings in major advantages by providing deep context to LLMs through relational knowledge graph for text generation, classification, question and answering use cases. However, maintaining vast data volume of domain specific data in a knowledge graph with complex relationships and querying from it every time a prompt is being posted to LLM, is a time consuming and expensive process. This paper presents a novel framework for selectively fine-tuning domain-specific large language models (LLMs) using a multi-stage Knowledge Graph (KG) based Retrieval Augmented Generation (RAG) pipeline and an Automated Incremental Fine-tuning System (AIFS). The proposed system aims to enhance the accuracy and relevance of LLM responses for text generation and Question Answering use cases by finetuning the LLM incrementally based on highly sought and highly relevant information in knowledge graph identified by leveraging page rank algorithm in KG. The framework comprises three major subsystems: Knowledge Graph Generation, Automated Incremental fine-tuning system (AIFS), and Domain Based Information Retrieval (DBIR). The effectiveness of the system is demonstrated through its ability to incrementally fine-tune LLMs based on selected highly relevant nodes within the KG, thereby improving the model’s domain-specific knowledge, response accuracy by 90\% and reduce cost by 71.8\%.},
	booktitle = {2024 {IEEE} 15th {Annual} {Ubiquitous} {Computing}, {Electronics} \& {Mobile} {Communication} {Conference} ({UEMCON})},
	author = {Govindharajan, Hariharan and Vijayakumar, Senthilkumar},
	month = oct,
	year = {2024},
	keywords = {Knowledge graphs, Large language models, Information retrieval, Accuracy, Question answering (information retrieval), Pipelines, Costs, Mobile communication, Artificial Intelligence(AI), Contextual Information Extraction \&Retrieval Systems, Knowledge Graph(KG), Large Language Models(LLM), LLM Fine-tuning},
	pages = {431--439},
}

@inproceedings{casalicchio_ai-cras_2024,
	title = {{AI}-{CRAS}: {AI}-driven {Cloud} {Service} {Requirement} {Analysis} and {Specification}},
	doi = {10.1109/IC2E61754.2024.00009},
	abstract = {Automated analysis and specification of software requirements expressed in natural language is a challenge addressed by the research community and is becoming a reality thanks to the advances in Artificial Intelligence (AI) and Natural Language Processing (NLP) techniques. While the research community focuses mainly on generic software requirements or specialized solutions for security requirements, we find a gap in the automation of analysis and specification for requirements in the cloud computing domain and the automatic mapping of requirements on actual products offered in the cloud service market. In this research work, we propose AI-CRAS an AI-driven cloud service requirement analysis and specification methodology. The proposed method, which leverages state-of-the-art transformer-based large language model, has been implemented and validated in a real case. Experimental results demonstrate that the model performed well in binary and multi-label classification of requirements (achieving recall/F1-score of 0.96 / 0.92 and 0.86 / 0.76, respectively) and mapping requirements into actual cloud services.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Casalicchio, Emiliano and Cotumaccio, Alberto},
	month = sep,
	year = {2024},
	note = {ISSN: 2694-0825},
	keywords = {Natural language processing, artificial intelligence, natural language processing, Transformers, Feature extraction, Software, Cloud computing, Stakeholders, Training, Accuracy, Testing, requirement engineering, cloud computing, cloud services, Vectors, cloud migration, cloud service broker},
	pages = {11--21},
}

@inproceedings{zhang_application_2024,
	title = {The {Application} of {Fine}-{Tuning} on {Pretrained} {Language} {Model} in {Information} {Extraction} for {Fault} {Knowledge} {Graphs}},
	doi = {10.1109/ICSP62122.2024.10743881},
	abstract = {Constructing fault knowledge graphs holds significant importance for achieving intelligent maintenance and diagnosis in high-end equipment manufacturing. Effective information extraction and knowledge graph construction have proven challenging due to the lack of standardized representation of semantically complex unstructured text in the industrial domain. Therefore, in this study, we performed fine-tuning on the pre-trained language model (ChatGLM2-6B) with specific prompts to achieve information extraction from fault-related texts, ultimately leading to the construction of a fault knowledge graph. Experimental results demonstrate that the proposed method not only supports fine-tuning with limited data but also exhibits enhanced capability in understanding complex semantics related to fault symptoms and causes.},
	booktitle = {2024 9th {International} {Conference} on {Intelligent} {Computing} and {Signal} {Processing} ({ICSP})},
	author = {Zhang, Kaiwen and Su, Feiyu and Huang, Yixiang and Li, Yanming and Wu, Fengqi and Mao, Yuhan},
	month = apr,
	year = {2024},
	keywords = {Ontologies, Knowledge graphs, Information extraction, Semantics, Manufacturing, Maintenance, Information retrieval, Pretrained language model, Data models, Parameter-efficient fine-tuning, Computational modeling, Stability analysis, Signal processing, Fault knowledge graph},
	pages = {469--473},
}

@inproceedings{batten_pyhdl-eval_2024,
	title = {{PyHDL}-{Eval}: {An} {LLM} {Evaluation} {Framework} for {Hardware} {Design} {Using} {Python}-{Embedded} {DSLs}},
	doi = {10.1109/MLCAD62225.2024.10740201},
	abstract = {Embedding hardware design frameworks within Python is a promising technique to improve the productivity of hardware engineers. At the same time, there is significant interest in using large-language models (LLMs) to improve key chip design tasks. This paper describes PyHDL-Eval, a new framework for evaluating LLMs on specification-to-RTL tasks in the context of Python-embedded domainspecific languages (DSLs). The framework includes 168 problems, Verilog reference solutions, Verilog test benches, Python test scripts, and workflow orchestration scripts. We use the framework to conduct a detailed case study comparing five LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo) targeting Verilog and five Python-embedded DSLs (PyMTL3, PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate the promise of in-context learning when applied to smaller models (e.g.,pass rate for CodeGemma 7B improves from 14.9\% to 32.7\% on Verilog) and Python-embedded DSLs (e.g., pass rate for LLama3 70B improves from 0.6\% to 33.0\% on PyMTL3). We find LLMs perform better when targeting Verilog as compared to Python-embedded DSLs (e.g., pass rate for GPT4 Turbo is 72.2\% on Verilog and 29.8–62.0\% on the Python-embedded DSLs) despite using a popular general-purpose host language. PyHDL-Eval will serve as a useful framework for future research at the intersection of Python-embedded DSLs and LLMs. CCS Concepts • Hardware → Hardware description languages and compilation; • Computing methodologies → Machine learning.},
	booktitle = {2024 {ACM}/{IEEE} 6th {Symposium} on {Machine} {Learning} for {CAD} ({MLCAD})},
	author = {Batten, Christopher and Pinckney, Nathaniel and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
	month = sep,
	year = {2024},
	keywords = {large language models, Machine learning, DSL, hardware description languages, Python-embedded domain-specific languages, Solid modeling, Chip scale packaging, Hardware, Hardware design languages, Productivity},
	pages = {1--17},
}

@inproceedings{grassi_enhancing_2024,
	title = {Enhancing {LLM}-{Based} {Human}-{Robot} {Interaction} with {Nuances} for {Diversity} {Awareness}},
	doi = {10.1109/RO-MAN60168.2024.10731381},
	abstract = {This paper presents a system for diversity-aware autonomous conversation leveraging the capabilities of large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. The conversation flow is guided by the structure of the system’s pre-established knowledge base, while LLMs are tasked with various functions, including generating diversity-aware sentences. Achieving diversity-awareness involves providing carefully crafted prompts to the models, incorporating comprehensive information about users, conversation history, contextual details, and specific guidelines. To assess the system’s performance, we conducted both controlled and real-world experiments, measuring a wide range of performance indicators.},
	booktitle = {2024 33rd {IEEE} {International} {Conference} on {Robot} and {Human} {Interactive} {Communication} ({ROMAN})},
	author = {Grassi, Lucrezia and Recchiuto, Carmine Tommaso and Sgorbissa, Antonio},
	month = aug,
	year = {2024},
	note = {ISSN: 1944-9437},
	keywords = {Ontologies, Large language models, Knowledge based systems, History, Human-robot interaction, Robots, Oral communication, Noise measurement, Time factors, Hybrid power systems},
	pages = {2287--2294},
}

@inproceedings{wu_domain_2024,
	title = {Domain {Knowledge} {Graph} {Construction} {Methods} of {Construction} {Schedule} in {Steel} {Structure} {Projects}},
	doi = {10.1109/IHMSC62065.2024.00017},
	abstract = {In the domain of engineering construction, steel structure engineering construction has accumulated a large amount of data, and the development of knowledge graph construction technology to structure this data can provide effective support for high-quality construction organization. This paper analyzes the knowledge sources of construction schedule for steel structure projects, constructs a domain ontology of construction schedule for assembled steel structure projects by using a seven-step method and points out the current problems of knowledge extraction for construction organization design for assembled steel structure projects. The semi-structured construction schedule data is subject to event extraction, and the unstructured knowledge of construction schedule is discussed from the perspective of few-shot methods and large language modeling for the knowledge extraction of construction schedule for steel structure projects.},
	booktitle = {2024 16th {International} {Conference} on {Intelligent} {Human}-{Machine} {Systems} and {Cybernetics} ({IHMSC})},
	author = {Wu, Shanglin and Yao, Xiaodong and Liu, Shu and Liang, Haitao and Liu, Zhenyuan},
	month = aug,
	year = {2024},
	note = {ISSN: 2157-8982},
	keywords = {Ontologies, Knowledge graphs, Data mining, Data models, Organizations, Cybernetics, event extraction, ontology construction, construction schedule, domain knowledge graphs, Human-machine systems, Schedules, Steel},
	pages = {39--44},
}

@article{xu_chattf_2024,
	title = {{ChatTf}: {A} {Knowledge} {Graph}-{Enhanced} {Intelligent} {Q}\&{A} {System} for {Mitigating} {Factuality} {Hallucinations} in {Traditional} {Folklore}},
	volume = {12},
	issn = {2169-3536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209217304&doi=10.1109%2FACCESS.2024.3485877&partnerID=40&md5=1750490cfa14cde7c31225b029ca7fe3},
	doi = {10.1109/ACCESS.2024.3485877},
	abstract = {Large language models are rapidly advancing the field of artificial intelligence, with current research focusing primarily on traditional natural language understanding tasks, such as question answering and information extraction. However, in knowledge-intensive domains, such as intangible cultural heritage, hallucination problems due to insufficient domain knowledge persist. To address this, we present ChatTf, a knowledge graph-enhanced intelligent Q\&A system, exemplified by Chinese traditional folklore, aimed at reducing factuality hallucinations in this domain. Specifically, we constructed the Traditional Folklore Ontology (TFOnto) and proposed the Zero-shot Traditional Folklore Triplet Extraction (ZFTE) framework. Driven by TFOnto, ZFTE builds a Traditional Folklore Knowledge Graph (TFKG). We then proposed a dual-stage Retrieval-Augmented Generation framework (TFKG-RAG) based on TFKG to provide traditional folklore knowledge to large language models, mitigating factuality hallucinations in folklore Q\&A tasks. In the experimental phase, ChatTf achieved an accuracy of 96.7\% on a self-built TFCQD test set, outperforming several state-of-the-art baseline methods. This demonstrates the accuracy and reliability of folklore domain question answering.},
	journal = {IEEE Access},
	author = {Xu, Jun and Zhang, Hao and Zhang, Haijing and Lu, Jiawei and Xiao, Gang},
	year = {2024},
	note = {Type: Article},
	keywords = {large language model, Ontologies, Knowledge graphs, Knowledge graph, Large language model, Language model, Large language models, Semantics, Knowledge engineering, Retrieval-augmented generation, Question answering, Reliability, Cognition, retrieval-augmented generation, Training, Natural language understanding, Question Answering, Accuracy, question answering, Cultural differences, traditional folklore, Domain Knowledge, Ontology's, 'current, Intangible cultural heritages, Traditional folklore},
	pages = {162638--162650},
	annote = {Cited by: 4; All Open Access; Gold Open Access},
}

@inproceedings{haque_utilizing_2024,
	title = {Utilizing {Structural} {Metrics} from {Knowledge} {Graphs} to {Enhance} the {Robustness} {Quantification} of {Large} {Language} {Models} ({Extended} {Abstract})},
	doi = {10.1109/DSAA61799.2024.10722791},
	abstract = {The goal of this study is to determine whether large language models (LLMs) like CodeLlama, Mistral, and Vicuna can be used to build knowledge graphs (KGs) from textual data. We create class descriptions for well-known KGs such as DBpedia, YAGO, and Google Knowledge Graph, from which we extract RDF triples and enhance these graphs using different preprocessing methods. Six structural quality measures are used in the study to compare the constructed and existing KGs. Our results demonstrate how important LLMs are to improving KG construction and provide insightful information for KG construction researchers. Moreover, an in-depth analysis of popular open-source LLM models enables researchers to identify the most efficient model for various tasks, ensuring optimal performance in specific applications.},
	booktitle = {2024 {IEEE} 11th {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Haque, Mohd Ariful and Kamal, Marufa and George, Roy and Gupta, Kishor Datta},
	month = oct,
	year = {2024},
	note = {ISSN: 2766-4112},
	keywords = {Ontologies, Knowledge graphs, Large language models, Resource description framework, Data science, Robustness, Measurement, Internet, Analytical models},
	pages = {1--2},
}

@inproceedings{pan_mining_2024,
	title = {Mining {User} {Requirement} {Scenarios} and {Generating} {Design} {Solutions} for {Rehabilitation} {Aids} {Based} on {Large} {Language} {Models}},
	doi = {10.1109/CASE59546.2024.10711793},
	abstract = {The development of the rehabilitation aids industry for the disabled has been pivotal in recent years, particularly in the personalized design of lower limb rehabilitation aids. Facing challenges in meeting individualized demands in design practice and the information gap between medical professionals and users, we propose a design Knowledge Graph (KG) method based on the Function-Behavior-Structure (FBS) model. This approach utilizes open-source large language models (LLMs) and fine-tunes them with instruction data generated by self-instructions to improve the accuracy of user requirements mining. The method aims to enhance the personalization and innovation of rehabilitation aids design through the integration of KG and LLM, effectively narrowing the cognitive gap between service providers and users. The anticipated results of the study are expected to promote efficient innovation in rehabilitation aids design, better meeting the needs of the disabled community.},
	booktitle = {2024 {IEEE} 20th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Pan, Xinyu and Gong, Jie and Wen, Sijie and Zhuang, Weibin and Li, Xinyu},
	month = aug,
	year = {2024},
	note = {ISSN: 2161-8089},
	keywords = {Knowledge graphs, Automation, Large language models, Data mining, Technological innovation, Accuracy, Industries, Conferences, Computer aided software engineering},
	pages = {3155--3161},
}

@inproceedings{hofgen_enhancing_2024,
	title = {Enhancing {Model}-{Based} {System} {Architecting} {Through} {Formalized} {Decision} {Management}},
	doi = {10.1109/CASE59546.2024.10711329},
	abstract = {System architecture decisions are typically informally captured in design documents. This practice leads to a loss of knowledge that impedes later activities like design changes, impact analysis, and reuse. Model-Based Systems Engineering (MBSE) frameworks support the development of increasingly complex systems but must address the problem of capitalization on architectural knowledge. To this end the "Decision Ontology for System Architectures (DOSA)" is developed to provide a formalized data model to capture system architecture decisions. DOSA is developed through a synthesis of decisions observed while developing an architecture model for a preliminary study of a novel satellite navigation system at Airbus Defence and Space. The approach is integrated into an MBSE framework enabling engineers to capture decisions that influence the architecture’s characteristics while developing the system model and imminently trace decision to artifacts of the system architecture. Subsequent visual inspection and formal querying of the decision graph facilitates the analysis of made decisions, and their interrelations.},
	booktitle = {2024 {IEEE} 20th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Höfgen, Josua and Vogel-Heuser, Birgit and Vicaria, Alejandra and Pouzolz, François and Kurzhals, Christian},
	month = aug,
	year = {2024},
	note = {ISSN: 2161-8089},
	keywords = {Ontologies, Ontology, Knowledge engineering, Inspection, MBSE, Data models, Visualization, Complex systems, Atmospheric modeling, Systems architecture, Computer aided software engineering, Decision Management, Satellite navigation systems, System Architecture},
	pages = {1053--1060},
}

@inproceedings{reif_chatbot-based_2024,
	title = {Chatbot-{Based} {Ontology} {Interaction} {Using} {Large} {Language} {Models} and {Domain}-{Specific} {Standards}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207845394&doi=10.1109%2FETFA61755.2024.10711065&partnerID=40&md5=6e55dc21dbf2c13a30448996fcb6cb63},
	doi = {10.1109/ETFA61755.2024.10711065},
	abstract = {The following contribution introduces a concept that employs Large Language Models (LLMs) and a chatbot interface to enhance SPARQL query generation for ontologies, thereby facilitating intuitive access to formalized knowledge. Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology, effectively preventing misinformation or fabrication by the LLM. To enhance the quality and precision of outcomes, additional textual information from established domain-specific standards is integrated into the ontology for precise descriptions of its concepts and relationships. An experimental study assesses the accuracy of generated SPARQL queries, revealing significant benefits of using LLMs for querying ontologies and highlighting areas for future research.},
	booktitle = {2024 {IEEE} 29th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Reif, Jonathan and Jeleniewski, Tom and Gill, Milapji Singh and Gehlhoff, Felix and Fay, Alexander},
	month = sep,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Ontologies, Large Language Models, Large language model, Ontology, Language model, Large language models, Semantic Web, Semantics, Industry 4.0, Standards, Cyber-physical systems, Accuracy, Chatbots, Cyber-Physical Systems, Fake news, Fabrication, Natural languages, Manufacturing automation, Structured Query Language, Ontology's, Query languages, Cybe-physical systems, Semantic-Web, Domain specific, Query generation},
	pages = {1--4},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{vieira_da_silva_use_2024,
	title = {On the {Use} of {Large} {Language} {Models} to {Generate} {Capability} {Ontologies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197107836&doi=10.1109%2FETFA61755.2024.10710775&partnerID=40&md5=544ef7ed4c7c396646d5da759dd60db8},
	doi = {10.1109/ETFA61755.2024.10710775},
	abstract = {Capability ontologies are increasingly used to model functionalities of systems or machines. The creation of such onto-logical models with all properties and constraints of capabilities is very complex and can only be done by ontology experts. However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts. Therefore, this paper investigates how LLMs can be used to create capability ontologies. We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs. Errors in the generated ontologies are recorded and compared. To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used. The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors.},
	booktitle = {2024 {IEEE} 29th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Vieira da Silva, Luis Miguel and Kocher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
	month = sep,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Ontologies, Large Language Models, Large language model, Ontology, Language model, Large language models, Semantic Web, Semantics, OWL, Resource description framework, LLMs, Cognition, Model generation, Skill, Computational linguistics, Skills, Complexity theory, Testing, Logical models, Natural languages, Shape, Syntactics, Capabilities, Model-Generation, Ontology's, Natural language processing systems, Semantic-Web, Capability},
	pages = {1--8},
	annote = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{knollmeyer_document_2024,
	title = {Document {Knowledge} {Graph} to {Enhance} {Question} {Answering} with {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/ETFA61755.2024.10711054},
	abstract = {Reusing and managing existing knowledge from available documents is crucial for success in the factory planning domain. By leveraging Artificial Intelligence (AI) and Question Answering (QA) systems, users can query a document corpus through a chat-based application and receive precise answers. The recent advancements in Large Language Models (LLMs) and their linguistic capabilities present new opportunities for such applications. Utilizing the methodology of Retrieval Augmented Generation (RAG), document sections are provided to the LLM based on user queries. However, existing RAG implementations that use vector databases as document repositories face limitations when answering questions that extend beyond the text content of the documents. To address this issue, this paper proposes a concept to enhance RAG systems by integrating a Knowledge Graph (KG) constructed from the document structures.},
	booktitle = {2024 {IEEE} 29th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Knollmeyer, Simon and Akmal, Muhammad Uzair and Koval, Leonid and Asif, Saara and Mathias, Selvine G. and Groβmann, Daniel},
	month = sep,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Knowledge graphs, Large Language Models, Large language models, Information management, Knowledge Graph, Databases, Linguistics, Planning, Retrieval Augmented Generation, Question answering (information retrieval), Vectors, Faces, Manufacturing automation, Production facilities},
	pages = {1--4},
}

@inproceedings{vieira_da_silva_toward_2024,
	title = {Toward a {Method} to {Generate} {Capability} {Ontologies} from {Natural} {Language} {Descriptions}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207822437&doi=10.1109%2FETFA61755.2024.10710783&partnerID=40&md5=102ee20eccf408c633442c86eed6632e},
	doi = {10.1109/ETFA61755.2024.10710783},
	abstract = {To achieve a flexible and adaptable system, capabil-ity ontologies are increasingly leveraged to describe functions in a machine-interpretable way. However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise. This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks. Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique. After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements. Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process.},
	booktitle = {2024 {IEEE} 29th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Vieira da Silva, Luis Miguel and Kocher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
	month = sep,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Ontologies, Large Language Models, Large language model, Ontology, Language model, Large language models, Semantic Web, Semantics, Modeling languages, LLMs, Model generation, Reviews, Skill, Skills, Testing, Manuals, Natural languages, Syntactics, Adaptation models, Costs, Manufacturing automation, Capabilities, Model-Generation, Ontology's, Natural language processing systems, Semantic-Web, Capability},
	pages = {1--4},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{meyer_potentials_2024,
	title = {Potentials of {Large} {Language} {Models} for {Generating} {Assembly} {Instructions}},
	doi = {10.1109/ETFA61755.2024.10710806},
	abstract = {With the increasing complexity in manual assembly and a demographic decline in skilled workforce, the importance of well-documented processes through assembly instructions has grown. Creating these instructions is a time-consuming and knowledge-intensive task that typically relies on experienced employees. Although various automation solutions have been proposed to assist in generating assembly instructions, they often fall short in providing detailed textual guidance. With the rise of generative artificial intelligence (AI), new potentials arise in this domain. Therefore, this paper explores these potentials by employing various large language models (LLMs), prompting techniques and input data in an experimental setup for generating detailed assembly instructions, including the planning of assembly sequences as well as textual guidance on tools, assembly activities, and quality assurance measures. The findings reveal promising opportunities in leveraging LLMs but also substantial challenges, particularly in assembly sequence planning. To improve the reliability of generating assembly instructions, we propose a multi-agent concept that decomposes the complex task into simpler subtasks, each managed by specialized agents.},
	booktitle = {2024 {IEEE} 29th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Meyer, Frederic and Freitag, Lennart and Hinrichsen, Sven and Niggemann, Oliver},
	month = sep,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {large language model, LLM, Large language models, GPT, Generative AI, Quality assurance, Reliability, Decision making, Assembly, Complexity theory, Planning, agent, Manuals, Manufacturing automation, assembly instruction, experiment, prompt},
	pages = {1--8},
}

@inproceedings{schoch_engineering_2024,
	title = {Engineering {Data} {Funnel} ({WIP}) – {An} {Ontology}-{Enhanced} {LLM}-{Based} {Agent} and {MoE} {System} for {Engineering} {Data} {Processing}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207839628&doi=10.1109%2FETFA61755.2024.10710789&partnerID=40&md5=77445eddb39fc5dcb5176c5c97d69efc},
	doi = {10.1109/ETFA61755.2024.10710789},
	abstract = {Automation Engineering of a process automation system is still a very manual effort due to limited support for the interpretation and processing of process design specification documents. Even though standards for digital data exchange between process and automation engineering do exist, those formats are rarely used and consequently the immense automation potential in automation engineering cannot be lifted. This contribution presents an AI -based approach and prototype - using an ontology-enhanced LLM -based agent and a mixture-of-experts system - to structure and formalize multimodal unstructured process design information as in PDF, Excel, and Word formats and make it available for state-of-the-art engineering tools for the long-known “Automation of Automation”.},
	booktitle = {2024 {IEEE} 29th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Schoch, Nicolai and Hoernicke, Mario and Strem, Nika and Stark, Katharina},
	month = sep,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Ontology, Artificial intelligence, Automation, Engineering design, Standards, Process design, Manuals, Prototypes, Data processing, Manufacturing automation, automation of automation, engineering data processing, engineering design specification, LLM-based agent, mixture of experts, ontology-driven information processing, Portable document format, Ontology's, Network security, Specifications, Automation of automation, Design specification, Engineering data, Engineering data processing, Engineering design specification, Mixture of experts, Ontology-driven information processing},
	pages = {1--5},
	annote = {Cited by: 0},
}

@inproceedings{incitti_leveraging_2024,
	title = {Leveraging {LLMs} for {Knowledge} {Engineering} from {Technical} {Manuals}: {A} {Case} {Study} in the {Medical} {Prosthesis} {Manufacturing} {Domain}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207690564&doi=10.23919%2FFUSION59988.2024.10706469&partnerID=40&md5=a81498f29b7325e537a3070f58f46227},
	doi = {10.23919/FUSION59988.2024.10706469},
	abstract = {Ontologies are nowadays widely used to organize information across specific domains, being effective due to their hierarchical structure and the ability to explicitly represent relationships between concepts. Knowledge engineering, like compiling companies’ vast bodies of knowledge into these structures, however, still represents a time-consuming, largely manually performed process, esp. with significant amounts of knowledge often only recorded within unstructured text documents. Since the recently introduced Large Language Models (LLMs) excel on text summarization, this raises the question whether these could be exploited within dedicated knowledge fusion architectures to assist human knowledge engineers by automatically suggesting relevant classes, instances and relations extracted from textual corpora. We therefore propose a novel approach that leverages the taxonomic structure of a partially defined ontology to prompt LLMs for hierarchical knowledge organization. Unlike conventional methods that rely solely on static ontologies, our methodology dynamically generates prompts based on the ontology’s existing class taxonomy, prompting the LLM to generate responses that extract supplementary information from unstructured documents. It thus introduces the concept of using ontologies as scaffolds for guiding LLMs, in order to realize a mutual interplay between structured ontological knowledge and the soft fusion capabilities of LLMs. We evaluate our proposed algorithm on a real-world case study, performing a knowledge fusion task on heterogeneous technical documentation from a medical prosthesis manufacturer.},
	booktitle = {2024 27th {International} {Conference} on {Information} {Fusion} ({FUSION})},
	author = {Incitti, Francesca and Salfinger, Andrea and Snidaro, Lauro and Challapalli, Sri},
	month = jul,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Large Language Models, Large language model, Ontology, Ontology Population, Natural language processing, Language model, Large language models, Manufacturing, Knowledge engineering, Natural Language Processing, Taxonomies, Taxonomy, Knowledge Engineering, Organizations, Text summarization, Language processing, Documentation, Prosthetics, Manuals, Natural languages, Soft Fusion, Ontology's, Natural language processing systems, Case-studies, Scaffolds, Medical prosthesis, Scaffolds (biology), Soft fusions},
	pages = {1--8},
	annote = {Cited by: 1},
}

@article{lee_improving_2024,
	title = {Improving {Commonsense} {Bias} {Classification} by {Mitigating} the {Influence} of {Demographic} {Terms}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3477599},
	abstract = {Understanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP). However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models. This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms. Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods(IHTA). The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms. To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model’s predictions when these terms are masked versus unmasked. This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT. The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization. The experiments show that the first method increases the accuracy over the baseline by 2.33\%, and the second one by 0.96\% over standard augmentation methods. The IHTA techniques yielded an 8.82\% and 9.96\% higher accuracy than threshold-based and standard augmentation methods, respectively.},
	journal = {IEEE Access},
	author = {Lee, Jinkyu and Kim, Jihie},
	year = {2024},
	keywords = {Ontologies, Natural language processing, Semantics, Standards, Commonsense reasoning, Accuracy, Chatbots, Training data, Prevention and mitigation, Predictive models, Systematics, bias mitigation, Classification algorithms, Commonsense bias, demographic term, Demography, hierarchical generalization, threshold-based augmentation},
	pages = {161480--161489},
}

@inproceedings{baghdadi_tocbert_2024,
	title = {{TocBERT}: {Medical} {Document} {Structure} {Extraction} {Using} {Bidirectional} {Transformers}},
	doi = {10.1109/IS61756.2024.10705189},
	abstract = {Text segmentation holds paramount importance in the field of Natural Language Processing (NLP). It plays an important role in several NLP downstream tasks like information retrieval and document summarization. In this work, we propose a new solution, namely TocBERT, for segmenting texts using bidirectional transformers. TocBERT represents a supervised solution trained on the detection of titles and sub-titles from their semantic representations. This task was formulated as a named entity recognition (NER) problem. The solution has been applied on a medical text segmentation use-case where the Bio-ClinicalBERT model is fine-tuned to segment discharge summaries of the MIMIC-III dataset. The performance of TocBERT has been evaluated on a human-labeled ground truth corpus of 250 notes. It achieved an F1-score of 84.6\% when evaluated on a linear text segmentation problem and 72.8 \% on a hierarchical text segmentation problem. It outperformed a carefully designed rule-based solution, particularly in distinguishing titles from subtitles.},
	booktitle = {2024 {IEEE} 12th {International} {Conference} on {Intelligent} {Systems} ({IS})},
	author = {Baghdadi, Sarra and Saleh, Majd and Paquelet, Stéphane},
	month = aug,
	year = {2024},
	note = {ISSN: 2767-9802},
	keywords = {Ontologies, Named entity recognition, Semantics, BERT, NLP, Information retrieval, information retrieval, language models, Transformers, Intelligent systems, transformers, Biological system modeling, Cleaning, medical text cleaning, MIMICs, text segmentation, Title detection},
	pages = {1--6},
}

@inproceedings{baddour_phenotypes_2024,
	title = {Phenotypes {Extraction} from {Text}: {Analysis} and {Perspective} in the {LLM} {Era}},
	doi = {10.1109/IS61756.2024.10705235},
	abstract = {Collecting the relevant list of patient phenotypes, known as deep phenotyping, can significantly improve the final diagnosis. As textual clinical reports are the richest source of phenotypes information, their automatic extraction is a critical task. The main challenges of this Information Extraction (IE) task are to identify precisely the text spans related to a phenotype and to link them unequivocally to referenced entities from a source such as the Human Phenotype Ontology (HPO). Recently, Language Models (LMs) have been the most suc-cessful approach for extracting phenotypes from clinical reports. Solutions such as PhenoBERT, relying on BERT or GPT, have shown promising results when applied to datasets built on the hypothesis that most phenotypes are explicitly mentioned in the text. However, this assumption is not always true in medical genetics. Hence, although the LMs carry powerful semantic abilities, their contributions are not clear compared to syntactic string-matching steps that are used within the current pipelines. The goal of this study is to improve phenotype extraction from clinical notes related to genetic diseases. Our contributions are threefold: First, we provide a clear definition of the phenotype extraction task from free text, along with a high-level overview of the involved functions. Second, we conduct an in-depth analysis of PhenoBERT, one of the best existing solutions, to evaluate the proportion of phenotypes predicted with simple string-matching. Third, we demonstrate how utilizing and incorporating large language models (LLMs) for span detection step can improve performance especially with implicit phenotypes. In addition, this experiment revealed that the annotations of existing dataset are not exhaustive, and that LLM can identify relevant spans missed by human labelers.},
	booktitle = {2024 {IEEE} 12th {International} {Conference} on {Intelligent} {Systems} ({IS})},
	author = {Baddour, Moussa and Paquelet, Stéphane and Rollier, Paul and De Tayrac, Marie and Dameron, Olivier and Labbe, Thomas},
	month = aug,
	year = {2024},
	note = {ISSN: 2767-9802},
	keywords = {Ontologies, LLM, Large language models, Semantics, Annotations, phenotype, Data mining, Intelligent systems, entity linking, Phenotypes, Syntactics, Medical diagnostic imaging, Detectors, embed dings, genetic, phenoBERT},
	pages = {1--8},
}

@inproceedings{kumar_reidlm_2024,
	title = {{ReidLM}: {Fine}-{Tuning} {LLaMA3} using {Evol}-{Instruct} for {Enhanced} {Contextual} {Accuracy} in {Rare} {Disease} {Research}},
	doi = {10.1109/NMITCON62075.2024.10698993},
	abstract = {This study introduces ReidLM, a fine-tuned large language model (LLM) optimized for the rare disease domain. By generating a diverse and complex question-and-answer dataset using the EvolInstruct methodology, Meta’s LLaMA-3-8B-Instruct model was enhanced to deliver better performance across multiple evaluation metrics. ReidLM demonstrates significant improvements in generating contextually accurate responses for rare diseases, highlighting the potential of Evol-Instruct in specialized medical applications. Specifically, ReidLM achieved the highest ROUGE-1 (0.3281) and GEval (0.87) scores among the evaluated models, along with strong performances in METEOR (0.3662) and BERTScore (0.8782), indicating its effectiveness in producing semantically sound and relevant responses. These results offer promising advancements in medical research and patient care, with future work aimed at expanding datasets and validating clinical utility.},
	booktitle = {2024 {Second} {International} {Conference} on {Networks}, {Multimedia} and {Information} {Technology} ({NMITCON})},
	author = {Kumar, Amala Rashmi and Kumari, S. Meena and Rao, Tanvi and Shetty, Tavishi S},
	month = aug,
	year = {2024},
	keywords = {rare diseases, Terminology, Large language models, Measurement, fine-tuning, Accuracy, Medical diagnostic imaging, Medical services, Information technology, Diseases, evol-instruct, LLaMA 3, Meteors, Multimedia systems},
	pages = {1--6},
}

@inproceedings{luo_research_2024,
	title = {Research on knowledge graph construction method for mine hoist fault field},
	doi = {10.1109/CISAT62382.2024.10695412},
	abstract = {Mine hoists are integral to mining hoisting systems, with their safe and reliable operation being critical for ensuring the safety of mining operations. The consequences of hoist failure are severe, particularly when the root cause of the malfunction is not promptly identified and addressed, potentially compromising the overall safety of mining activities. The complexity of mine hoist systems stems from the interdependent and restrictive relationships among their components, each of which generates unique operational state information. This information, when aggregated and processed, can be distilled into various fault characteristic parameters. This paper introduces a novel approach to fault diagnosis within mine hoist systems by constructing a fault knowledge graph based on ontological principles. The proposed method harnesses the power of knowledge graphs to systematically represent and analyze the complex interplay of components within the hoist system. By doing so, it enhances the diagnostic capabilities and the preemptive identification of potential faults. The research focuses on the mine hoist as the subject of study and proposes the development of an ontologically-based fault knowledge graph. This approach is not only of significant importance to the coal mining industry but also offers innovative insights for knowledge graph construction across various domains. The implications of this study extend beyond the mining sector, providing a foundation for more robust and intelligent fault diagnosis systems in complex mechanical systems.},
	booktitle = {2024 7th {International} {Conference} on {Computer} {Information} {Science} and {Application} {Technology} ({CISAT})},
	author = {Luo, Junjun and Zhu, Zhongyan and Zhu, Haijiang and Dong, Xiaohui},
	month = jul,
	year = {2024},
	keywords = {Knowledge graphs, Named entity recognition, artificial intelligence, Information retrieval, Fault diagnosis, Safety, relation extraction, Information science, Complexity theory, entity recognition, fault knowledge graph, Instruments, Lifting equipment, Mechanical systems, mine hoist},
	pages = {342--346},
}

@inproceedings{kougioumtzidou_end--end_2024,
	title = {An {End}-to-{End} {Framework} for {Cybersecurity} {Taxonomy} and {Ontology} {Generation} and {Updating}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206170969&doi=10.1109%2FCSR61664.2024.10679346&partnerID=40&md5=6b9b5f05f1f9120cb0652ff6fa09d2ad},
	doi = {10.1109/CSR61664.2024.10679346},
	abstract = {Effective cyber-defense practices often require the use of structured knowledge representations, such as taxonomies and ontologies, to organise vast amounts of data and facili-tate knowledge representation and reasoning. To this end, we present an Artificial Intelligence (AI)-assisted framework for the construction and update of cybersecurity taxonomies and ontologies. The proposed framework can be divided into three main phases: Taxonomy Construction, Ontology Construction, and Taxonomy/Ontology Update, each phase consisting of both information extraction and semantic knowledge representation components. For information extraction, we employ a variety of techniques originating from Natural Language Processing (NLP), particularly Transformer Neural Networks. For constructing ontologies, we propose a conceptual ontology schema based on the STIX 2.1 standard for modeling information related to attacks, threats, and vulnerabilities, and use the Owlready2 Python library. Overall, our framework effectively builds cybersecurity taxonomies and ontologies and updates existing knowledge of both the generated and open-source taxonomies and ontologies.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Cyber} {Security} and {Resilience} ({CSR})},
	author = {Kougioumtzidou, Anna and Papoutsis, Angelos and Kavallieros, Dimitrios and Mavropoulos, Thanassis and Tsikrika, Theodora and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
	month = sep,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Large language model, Natural language processing, Language model, Semantics, artificial intelligence, large language models, natural language processing, ontologies, Cyber security, Knowledge representation, Semantic search, Transformers, Taxonomy, Dynamic update, cybersecurity, Vulnerability, Attack, Language processing, taxonomies, Natural languages, Smart grids, Filtering, attacks, dynamic update, vulnerabilities, Ontology's, Cyber attacks, Phishing},
	pages = {247--254},
	annote = {Cited by: 2},
}

@inproceedings{karalka_towards_2024,
	title = {Towards a {Generic} {Knowledge} {Graph} {Construction} {Framework} for {Privacy} {Awareness}},
	doi = {10.1109/CSR61664.2024.10679399},
	abstract = {Knowledge graphs (KGs) organize data from multi-ple sources, capturing information about entities of interest in a given domain or task, such as people, places, or events, and forge connections between them. In this paper, we introduce a generic framework for building knowledge graphs designed to enhance data privacy through semantic interpretation. We demonstrate the effectiveness of our framework by applying it to the healthcare sector, where it helps organize and analyze com-plex information, support data analysis, improve decision-making processes, and uncover hidden relationships between entities. Our approach leverages domain-specific ontologies like SNOMED CT and integrates vector databases to assess and mitigate privacy risks. By using semantic techniques, we enhance the robustness of data against reidentification attacks and suggest appropriate de-identification methods. The integration of SNOMED with vector databases allows for efficient storage, retrieval, and analysis of high-dimensional healthcare data, facilitating advanced data an-alytics and knowledge discovery while maintaining data privacy. Through this framework, we aim to provide sufficient insights for identifying privacy vulnerabilities and ensuring the security and usability of sensitive health information.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Cyber} {Security} and {Resilience} ({CSR})},
	author = {Karalka, Christina and Meditskos, Georgios and Papoutsoglou, Maria and Bassiliades, Nick},
	month = sep,
	year = {2024},
	keywords = {Knowledge graphs, Semantics, Data privacy, Databases, Privacy, Vectors, Medical services},
	pages = {700--705},
}

@inproceedings{saini_methodological_2024,
	title = {Methodological {Insights} {Into} {Protein} {Clustering} {Using} {BERT} \& {RoBERTa}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205832131&doi=10.1109%2FCONECCT62155.2024.10677287&partnerID=40&md5=6656d8b16e169ba15a9b81849b3d8247},
	doi = {10.1109/CONECCT62155.2024.10677287},
	abstract = {Proteins are present in all living organisms, and understanding their processes is vital. Protein databases such as SWISS-PROT include curated information on only 570,000 protein sequences, representing a fraction of the 250 million known evidential and predicted sequences; it becomes crucial to cluster proteins into similar groups. This research explores the application of two transformer architectures, BERT and RoBERTa in clustering proteins in the supervised prediction of Gene Ontology (GO) annotations. The detailed methodology for both the pre-training and fine-tuning processes, as well as results that showcase RoBERTa outperforming BERT in the context of protein clustering, on performance metrics of accuracy and loss. Operating under constrained computational resources, the deployed model exhibits strong performance and highlight the robustness of methodology in protein clustering within resource constraints. This study not only contributes to the understanding of protein clustering but also signifies the potential of transformer models to handle biological data.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Electronics}, {Computing} and {Communication} {Technologies} ({CONECCT})},
	author = {Saini, Shashwat and Vrindavanam, Jayavrinda and Mondal, Subhash},
	month = jul,
	year = {2024},
	note = {ISSN: 2766-2101},
	keywords = {Ontologies, Natural language processing, Language model, BERT, Modeling languages, Gene Ontology, Natural Language Processing, Annotations, Transformers, Training, Transformer, Computational modeling, RoBERTa, Proteins, Language processing, Natural languages, Biological system modeling, Analytical models, Masked Language Modelling, Protein Clustering, Natural language processing systems, Clusterings, Invertebrates, Biotic, Masked language modeling, Protein clustering},
	pages = {1--6},
	annote = {Cited by: 0},
}

@inproceedings{gupta_echo_2024,
	title = {{ECHO}: {Environmental} {Sound} {Classification} with {Hierarchical} {Ontology}-guided {Semi}-{Supervised} {Learning}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205825848&doi=10.1109%2FCONECCT62155.2024.10677303&partnerID=40&md5=dd9414b66e627f0744370228215bd482},
	doi = {10.1109/CONECCT62155.2024.10677303},
	abstract = {Environment Sound Classification has been a well-studied research problem in the field of signal processing and till now more focus has been laid on fully supervised approaches. Recently, the focus has moved towards semi-supervised methods which concentrate on utilizing unlabeled data, and self-supervised methods which learn the intermediate representation through pretext tasks or contrastive learning. However, both approaches require a vast amount of unlabelled data to improve performance. In this work, we propose a novel framework called Environmental Sound Classification with Hierarchical Ontology-guided semi-supervised Learning (ECHO) that utilizes label ontology-based hierarchy to learn semantic representation by defining a novel pretext task. The model tries to predict coarse labels represented by the Large Language Model (LLM) based on ground truth label ontology, then further fine-tuned in a supervised way to predict the actual task. ECHO achieves a 1\% to 8\% accuracy improvement over baseline systems across UrbanSound8K, ESC-10, and ESC-50 datasets.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Electronics}, {Computing} and {Communication} {Technologies} ({CONECCT})},
	author = {Gupta, Pranav and Sharma, Raunak and Kumari, Rashmi and Aditya, Sri Krishna and Choudhary, Shwetank and Kumar, Sumit and M, Kanchana and R, Thilagavathy},
	month = jul,
	year = {2024},
	note = {ISSN: 2766-2101},
	keywords = {Ontologies, Ontology, Large language models, Semantics, semi-supervised learning, Contrastive learning, Federated learning, Self-supervised learning, Semi-supervised learning, Accuracy, Adversarial machine learning, Signal processing, Environment Sound Classification, Label ontology, Semisupervised learning, Ontology's, Contrastive Learning, Learn+, Environment sound classification, Environmental sound classifications, Research problems, Signal-processing, Sound classification, Unlabeled data},
	pages = {1--5},
	annote = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{quan_clustering_2024,
	title = {Clustering for {Protein} {Representation} {Learning}},
	doi = {10.1109/CVPR52733.2024.00038},
	abstract = {Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of the protein. We evaluate on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance.},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Quan, Ruijie and Wang, Wenguan and Ma, Fan and Fan, Hehe and Yang, Yi},
	month = jun,
	year = {2024},
	note = {ISSN: 2575-7075},
	keywords = {Ontologies, Representation learning, Computer vision, Clustering, Proteins, Three-dimensional displays, Enzymes, Amino acids, Protein Representation Learning},
	pages = {319--329},
}

@inproceedings{ma_mode_2024,
	title = {{MoDE}: {CLIP} {Data} {Experts} via {Clustering}},
	doi = {10.1109/CVPR52733.2024.02489},
	abstract = {The success of contrastive language-image pretraining (CLIP) relies on the supervision from the pairing between images and captions, which tends to be noisy in web- crawled data. We present Mixture of Data Experts (MoDE) and learn a system of CLIP data experts via clustering. Each data expert is trained on one data cluster, being less sensitive to false negative noises in other clusters. At inference time, we ensemble their outputs by applying weights determined through the correlation between task metadata and cluster conditions. To estimate the correlation pre-cisely, the samples in one cluster should be semantically similar, but the number of data experts should still be rea-sonable for training and inference. As such, we consider the ontology in human language and propose to use fine- grained cluster centers to represent each data expert at a coarse-grained level. Experimental studies show that four CLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and OpenCLIP on zero-shot image classification but with less ({\textless}35\%) training cost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly include new data experts. The code is available here.},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ma, Jiawei and Huang, Po-Yao and Xie, Saining and Li, Shang-Wen and Zettlemoyer, Luke and Chang, Shih-Fu and Yih, Wen-Tau and Xu, Hu},
	month = jun,
	year = {2024},
	note = {ISSN: 2575-7075},
	keywords = {Semantics, Training, Computational modeling, Correlation, Noise, Adaptation models, Costs, Data Clustering, Data Expert, Multi-Modal},
	pages = {26344--26353},
}

@inproceedings{liu_prompt-enhanced_2024,
	title = {Prompt-{Enhanced} {Prototype} {Framework} for {Few}-shot {Event} {Detection}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204998092&doi=10.1109%2FIJCNN60899.2024.10651359&partnerID=40&md5=0bed9e0efc27aa6edceae2346f8eaa30},
	doi = {10.1109/IJCNN60899.2024.10651359},
	abstract = {Few-shot event detection (ED) aims at identifying and typing event mentions from text with limited annotations. Most existing methods for few-shot ED use event ontology and related knowledge to construct prototypes and fail to fully leverage the rich knowledge of pre-trained language models (PLMs) which could help improve the representation of prototypes. Motivated by this, we propose an prompt-enhanced prototype framework which combines prototype and prompt for few-shot ED. Considering the scarcity of labeled data, we also introduce contrastive learning to enrich prototypes. Specifically, we use heuristic rules to align FrameNet with annotated data to get corresponding prompts for each event and convert them into prompt prototype. We then leverage contrastive learning to aggregate event mentions into prototypes and maintain these prototypes for few-shot ED. Furthermore, We explore diverse prompt formats for representing prompt prototypes and introduce a more comprehensive lexical prompt which improves the performance of few-shot ED. We conduct extensive experiments on the MAVEN corpus to reveal the effectiveness of the proposed framework compared to state-of-the-art methods.},
	booktitle = {2024 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Liu, Xu and Chen, Xinming and Zhu, Yangfu and Wu, Bin},
	month = jun,
	year = {2024},
	note = {ISSN: 2161-4407},
	keywords = {Language model, Semantics, Knowledge representation, Event ontology, Annotations, Neural networks, Event detection, Contrastive learning, Performance, Zero-shot learning, Prototypes, Aggregates, FrameNet, Labeled data, Contrastive Learning, Events detection, Heuristic rules, State-of-the-art methods},
	pages = {1--7},
	annote = {Cited by: 1},
}

@inproceedings{jian_distantly_2024,
	title = {Distantly {Supervised} {Relation} {Extraction} based on {Non}-taxonomic {Relation} and {Self}-{Optimization}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205022179&doi=10.1109%2FIJCNN60899.2024.10650745&partnerID=40&md5=d2d77fa81dbf65e1b6c6a51250f1793d},
	doi = {10.1109/IJCNN60899.2024.10650745},
	abstract = {Distantly supervised relation extraction (DS-RE) leverages existing knowledge bases to generate annotated data for relation extraction (RE), addressing the issue of scarce labeled data. However, distant supervision (DS) is often limited by coarse annotations and insufficient contextual awareness, leading to relational ambiguity and introducing noise in the labeled results. Moreover, although one can optimize the classifiers in DS-RE models through weight updates, the static nature of the guiding rules for such adjustments often falls short when addressing the challenges posed by diverse non-taxonomic relations and complex noise patterns in datasets. In this paper, we propose a DS-RE framework that capitalizes on non-taxonomic relations and a self-optimizing mechanism. We define a set of consistent DS relation candidates and combine DS with a LLM to enhance the perception of entities’ contextual states during the DS process. Then, we design a Self-Optimizing Ontology-Enhanced Non-taxonomic Relation Extraction Model (SO-NRE). The model incorporates additional entity-relation knowledge to enhance the semantic depth of Non-taxonomic relation ontologies and uses an adaptive dynamic scheduling mechanism to refine the classification strategy through iterations informed by self-perception outcomes. The experimental results show that the improved DS annotation workflow has enhanced accuracy, and SO-NRE outperforms mainstream baselines in RE performance.},
	booktitle = {2024 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Jian, Zhaorui and Liu, Shengquan and Gao, Wei and Cheng, Jianming},
	month = jun,
	year = {2024},
	note = {ISSN: 2161-4407},
	keywords = {Ontology, LLM, Relation extraction, Large language models, Semantics, Annotations, Neural networks, Self-supervised learning, Accuracy, Noise, Adaptation models, Distantly Supervised Relation Extraction, Non-taxonomic Relation, Self-Optimization, Ontology's, Labeled data, Extraction modeling, Distantly supervised relation extraction, Non-taxonomic relation, Self-optimization, Self-optimizing, Weight update},
	pages = {1--9},
	annote = {Cited by: 2},
}

@article{achintalwar_alignment_2024,
	title = {Alignment {Studio}: {Aligning} {Large} {Language} {Models} to {Particular} {Contextual} {Regulations}},
	volume = {28},
	issn = {1941-0131},
	doi = {10.1109/MIC.2024.3453671},
	abstract = {The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. By contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws, and other regulations and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors, which work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company’s internal-facing enterprise chatbot to its business conduct guidelines.},
	number = {5},
	journal = {IEEE Internet Computing},
	author = {Achintalwar, Swapnaja and Baldini, Ioana and Bouneffouf, Djallel and Byamugisha, Joan and Chang, Maria and Dognin, Pierre and Farchi, Eitan and Makondo, Ndivhuwo and Mojsilović, Aleksandra and Nagireddy, Manish and Natesan Ramamurthy, Karthikeyan and Padhi, Inkit and Raz, Orna and Rios, Jesus and Sattigeri, Prasanna and Singh, Moninder and Thwala, Siphiwe A. and Uceda-Sosa, Rosario A. and Varshney, Kush R.},
	month = sep,
	year = {2024},
	keywords = {Large language models, Context modeling, Data models, Chatbots, Internet, Guidelines, Synthetic data, Context-aware services, Regulation},
	pages = {28--36},
}

@inproceedings{yang_interlinking_2024,
	title = {Interlinking {Clinical} {Guidelines} via {Mining} {Medical} {Literature} {Knowledge} for {Multi}-{Morbidity} {Decision}-{Making}},
	doi = {10.1109/COMPSAC61105.2024.00165},
	abstract = {Independently developed clinical guidelines present a systematic challenge in managing patients with multi-morbidity in a consistent and integrated manner. Existing approaches mainly focus on combining multiple guidelines and lack approaches that combine with additional medical resources. The correlations and conflicts between treatment plans in the management of multi-morbidity are well-documented in medical literature but are less explored in the Clinical Decision Support line of research. In this paper, we propose a literature-based guideline interlinking method to address these challenges through the integration of clinical guidelines and the harmonization of conflicting recommendations, thereby providing a more holistic and efficient way to manage patients with multi-morbidity conditions. This method employs an ontology model and knowledge graph technology to represent and analyze the complexity and interrelations of diseases, with the aim of transcending the limitations of traditional single disease guidelines and providing a holistic and integrated framework for multi-morbidity management. The objective is to construct a multi-morbidity knowledge graph by correlating medical literature with clinical guidelines and to provide optimal decision support for patients with multi-morbidity complications in a clinical decision support system (CDSS).},
	booktitle = {2024 {IEEE} 48th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
	month = jul,
	year = {2024},
	note = {ISSN: 2836-3795},
	keywords = {Ontologies, Knowledge graphs, ontology model, knowledge graph, Databases, Computational modeling, Accuracy, Biological system modeling, Systematics, clinical guidelines, multi-morbidity management},
	pages = {1250--1255},
}

@inproceedings{hendawi_breaking_2024,
	title = {Breaking {Down} {Barriers}: {Empowering} {Diabetes} {Patients} with {User}-{Friendly} {Medical} {Explanations}},
	doi = {10.1109/ICICS63486.2024.10638283},
	abstract = {Effective management of diabetes is contingent upon patients' understanding of their medical conditions and treatments. However, medical documents often contain complex jargon and technical details that can be challenging for patients, especially those with limited health literacy. This paper presents DiaKnow, an innovative tool that simplifies medical documents and customizes explanations to suit individual health literacy levels. Employing a robust self-attention transformer model and a comprehensive diabetes-focused knowledge graph, DiaKnow enhances patient comprehension by providing contextually relevant, simplified medical information. This study assesses DiaKnow’s efficacy in real-world clinical settings through a structured use case evaluation method. We tested the tool’s ability to accurately identify, link, and simplify crucial medical terms using a diverse set of medical documents. Our findings confirm that DiaKnow not only improves the readability of medical documents but also ensures that explanations are medically accurate, clear, and comprehensive.},
	booktitle = {2024 15th {International} {Conference} on {Information} and {Communication} {Systems} ({ICICS})},
	author = {Hendawi, Rasha and Alian, Shadi and Li, Juan},
	month = aug,
	year = {2024},
	note = {ISSN: 2573-3346},
	keywords = {Knowledge graphs, ontology, knowledge graph, Transformers, Context modeling, entity linking, Accuracy, health literacy, medical entity recognition, Diabetes, Transforms, Communication systems, self-attention transformers},
	pages = {1--6},
}

@inproceedings{liu_using_2024,
	title = {Using {Generative} {Large} {Language} {Models} for {Hierarchical} {Relationship} {Prediction} in {Medical} {Ontologies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203690166&doi=10.1109%2FICHI61247.2024.00040&partnerID=40&md5=44d6691ccccf632afe6aa2f13128c9c9},
	doi = {10.1109/ICHI61247.2024.00040},
	abstract = {This study extends the exploration of ontology enrichment by evaluating the performance of various open-sourced Large Language Models (LLMs) on the task of predicting hierarchical relationships (IS-A) in medical ontologies including SNOMED CT Clinical Finding and Procedure hierarchies and the human Disease Ontology. With the previous finetuned BERT models for hierarchical relationship prediction as the baseline, we assessed eight open-source generative LLMs for the same task. We observed only three models, without finetuning, demonstrated comparable or superior performance compared to the baseline BERT -based models. The best performance model OpenChat achieved a macro average F1 score of 0.96 (0.95) on SNOMED CT Clinical Finding (Procedure) hierarchy, an increase over 7\% from the baseline 0.89 (0.85). On human Disease Ontology, OpenChat excels with an F1 score of 0.91, outperforming the second-best performance model Vicuna (0.84). Notably, some LLMs prove unsuitable for hierarchical relationship prediction tasks or appliable for concept placement of medical ontologies. We also explored various prompt templates and ensemble techniques to uncover potential confounding factors in applying LLMs for IS-A relation predictions for medical ontologies.},
	booktitle = {2024 {IEEE} 12th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {Liu, Hao and Zhou, Shuxin and Chen, Zhehuan and Perl, Yehoshua and Wang, Jiayin},
	month = jun,
	year = {2024},
	note = {ISSN: 2575-2634},
	keywords = {Ontologies, Large Language Models, Large language model, Language model, Large language models, Medical ontology, Human disease, SNOMED CT, Informatics, Medical Ontology, Performance, SNOMED-CT, Accuracy, Predictive models, Medical services, Task analysis, Hieratical Relation Prediction, Prompts Design, Prediction models, Generative adversarial networks, Hieratical relation prediction, Performance Modeling, Prompt design},
	pages = {248--256},
	annote = {Cited by: 1},
}

@inproceedings{lian_reqcompletion_2024,
	title = {{ReqCompletion}: {Domain}-{Enhanced} {Automatic} {Completion} for {Software} {Requirements}},
	doi = {10.1109/RE59067.2024.00023},
	abstract = {Software requirements are the driving force behind software development. As the cornerstone of the entire software lifecycle, the efficiency of crafting requirement specifications and the quality of these requirements significantly influence the duration of software development. Despite massive research on requirements elicitation, the reality is that requirements are often painstakingly crafted manually, word by word. This manual process is not only time-consuming but also prone to issues such as the misuse of terminology. To address these challenges, we introduce ReqCompletion, an approach designed to recommend the next token in real-time for given prefix of requirements description. ReqCompletion comprises two primary components. First, we have devised and integrated a knowledge-injection module into GPT-2—which stands as the largest available GPT model that allows for fine-tuning on specialized downstream tasks. This injection imbues GPT-2 with richer domain-specific knowledge, thus improving the relevance of the suggested tokens. Additionally, we employ a pointer network to optimize the recommendation quality by utilizing completed requirements as contextual support. Empirical evaluations using two public datasets demonstrate that ReqCompletion surpasses all baselines in performance (Recall@7 gains up to 65.87\% than the second-best model). Furthermore, the effectiveness of its two pivotal design elements has been substantiated through rigorous ablation studies. The utility of our work has been evaluated preliminarily through a small user study.},
	booktitle = {2024 {IEEE} 32nd {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Lian, Xiaoli and Ma, Jieping and Lv, Heyang and Zhang, Li},
	month = jun,
	year = {2024},
	note = {ISSN: 2332-6441},
	keywords = {Terminology, Software, Software Requirements, Manuals, Computer architecture, Real-time systems, Benchmark testing, Automatic Text Completion, Force, Knowledge Injection},
	pages = {142--154},
}

@inproceedings{fieblinger_actionable_2024,
	title = {Actionable {Cyber} {Threat} {Intelligence} {Using} {Knowledge} {Graphs} and {Large} {Language} {Models}},
	doi = {10.1109/EuroSPW61312.2024.00018},
	abstract = {Cyber threats are constantly evolving. Extracting actionable insights from unstructured Cyber Threat Intelligence (CTI) data is essential to guide cybersecurity decisions. Increasingly, organizations like Microsoft, Trend Micro, and CrowdS trike are using generative AI to facilitate CTI extraction. This paper addresses the challenge of automating the extraction of actionable CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs (KGs). We explore the application of state-of-the-art open-source LLMs, including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting meaningful triples from CTI texts. Our methodology evaluates techniques such as prompt engineering, the guidance framework, and fine-tuning to optimize information extraction and structuring. The extracted data is then utilized to construct a KG, offering a structured and queryable representation of threat intelligence. Experimental results demonstrate the effectiveness of our approach in extracting relevant information, with guidance and fine-tuning showing superior performance over prompt engineering. However, while our methods prove effective in small-scale tests, applying LLMs to large-scale data for KG construction and Link Prediction presents ongoing challenges.},
	booktitle = {2024 {IEEE} {European} {Symposium} on {Security} and {Privacy} {Workshops} ({EuroS}\&{PW})},
	author = {Fieblinger, Romy and Alam, Md Tanvirul and Rastogi, Nidhi},
	month = jul,
	year = {2024},
	note = {ISSN: 2768-0657},
	keywords = {Ontologies, Knowledge graphs, Large Language Models, Large language models, Knowledge Graphs, Cyber threat intelligence, Organizations, Cyber Threat Intelligence, Refining, Predictive models, Threat Prediction},
	pages = {100--111},
}

@article{woods_semantic_2024,
	title = {Semantic {Quality} {Assurance} of {Industrial} {Maintenance} {Procedures}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3441757},
	abstract = {Maintenance technicians in industry follow procedures that guide them through inspection, repair, and service tasks. Organisations seek to convert procedure documentation to machine-readable formats as their digital capabilities improve and regulatory requirements tighten. In this paper, we consider the opportunity for semantic quality assurance of digital procedures. We demonstrate a configurable and repeatable workflow containing three modules. The completeness module makes implicit information in procedures explicit using OpenAI’s Generative Pre-trained Transformer (GPT) model. The consistency module creates Resource Description Framework (RDF) triples that are aligned with, and checked against, the axioms of the open-source Ontology for Maintenance Procedure Documentation (OMPD). Finally, the correctness module performs closed-world checks on the RDF triples using the Shapes Constraints Language (SHACL). Each module can be used in isolation, or together, to realise an end-to-end semi-automated quality assurance workflow. Pre-processing of the raw maintenance procedure documents to extract entities (tools, materials and activities) and relations is achieved in a novel manner using prompt engineering with OpenAI’s GPT-3.5 Turbo model and few-shot learning. This end-to-end workflow enables organisations to perform quality assurance such as assessing the correct order for task sequences, and checking that all maintenance procedures have at least one maintenance task. We demonstrate this workflow on six procedures from the iFixit repository. The outputs of this workflow support maintenance technicians, planners and engineers by realising high-quality procedure documentation and automated procedure management update processes. The code and data used in this work is publicly available at https://github.com/equonto/quokka/.},
	journal = {IEEE Access},
	author = {Woods, Caitlin and Hodkiewicz, Melinda and French, Tim},
	year = {2024},
	keywords = {Ontologies, OWL, Maintenance, Resource description framework, SHACL, Data models, Documentation, Task analysis, Industrial ontology, ontology templates, OpenAI GPT, OTTR, technical language processing},
	pages = {122029--122046},
}

@inproceedings{motevallian_semantic_2024,
	title = {Semantic {Modeling} of {Waste} {Dataflow} for {Automating} {Circular} {Economy} {Systems}},
	doi = {10.1109/DCOSS-IoT61029.2024.00105},
	abstract = {Circular Economy (CE) is a model with a concrete action plan covering the whole life cycle of a product, from production and consumption to waste management (WM). Information technologies considerably contribute to the transition towards CE, e.g., waste tracking using Internet of Things (IoT). This will cause the businesses and organizations to confront a large diversity of data (i.e. waste amount, types, locations, etc.). The generated data is often stored and processed through manual or semi-manual methods by each business or organization. However, an automated method which can also interpret and integrate the diverse data in WM fields across different organizations is still in its infancy. Often, such data is not organized and falls short of reaching its full potential in facilitating coordinated management and enabling Circular Economy initiatives. In this paper, we aim to address this need through automated interpretation and integration of municipal waste data by applying semantic data modeling. Our approach proposes to capture the semantical description of entities in the WM process and their relations, which can appear between waste producers, authorities and consumers. Then, the obtained semantic model will facilitate and automate the required interpretation and integration of waste data, both for intra- and inter-organization scenarios. We realize intelligent semantic-based searching using natural language processing and large language models.},
	booktitle = {2024 20th {International} {Conference} on {Distributed} {Computing} in {Smart} {Systems} and the {Internet} of {Things} ({DCOSS}-{IoT})},
	author = {Motevallian, Mahsa and Esfar-E-Alam, AM and Taherkordi, Amir and Abbasi, Golnoush},
	month = apr,
	year = {2024},
	note = {ISSN: 2325-2944},
	keywords = {Large Language Models, Large language models, Semantics, NLP, Circular Economy, Organizations, Computational modeling, Production, Manuals, Neural Search, Semantic Data, Waste Data Modeling, Waste management},
	pages = {677--684},
}

@inproceedings{lee_quantum_2024,
	title = {Quantum {Computational} {Intelligence} with {Generative} {AI} {Image} for {Human}-{Machine} {Interaction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191275657&doi=10.1109%2FFUZZ-IEEE60900.2024.10611970&partnerID=40&md5=a71e06505cbc4d920a6032f50764c4f5},
	doi = {10.1109/FUZZ-IEEE60900.2024.10611970},
	abstract = {This paper introduces a Quantum Computational Intelligence (QCI) agent equipped with a content attention ontology model, specifically designed to enhance human-machine interaction based on a Generative Artificial Intelligence (GAI) image generation agent for Taiwanese/English learning and experience. Its diverse primary applications include social media analysis on Facebook groups and YouTube learning videos related to the 2023 IEEE CIS Education Portal (EP) Subcommittee, as well as in the areas of Taiwanese/English language learning and dialogue experience with GAI image generation. To establish the knowledge and inference models for the QCI agent, we initially developed a Taiwanese/English learning and experience ontology, including a content attention ontology, and an image attention ontology. The QCI agent utilizes metrics such as the number of views, posts, and comments to predict the fuzzy number of reactions. In addition, the GAI image agent generates Taiwanese speech-based/English text-based images and evaluates the fuzzy similarity score between Taiwanese/English and the attention ontology together with the Sentence BERT (SBERT) agent. This Taiwanese/English fuzzy similarity score is further validated through human assessments, with these evaluations subsequently serving as an additional metric for comparative analysis of Human-Machine Interaction (HMI). Furthermore, the GAI image agent is designed to create images and Chinese/English texts from text/speech translated by the Meta AI Universal Speech Translator (UST) Taiwanese/English agent. A Particle Swarm Optimization (PSO)-based machine learning mechanism is employed to train the QCI model for assessing learners' performance and predicting the performance of others. The National University of Tainan (NUTN) Taiwan-Large Language Model (NUTN.TW-LLM) agent has been further enhanced to support interactive learning experiences for HMI. An SBERT-based assessment agent is used to calculate fuzzy similarities between questions and answers in Taiwanese/English experiences and dialogues. Experimental results demonstrate the feasibility and efficacy of the proposed QCI model, equipped with QCI\&AI-FML (Artificial Intelligence-Fuzzy Markup Language) and machine learning capabilities, for social media and language learning applications on HMI. In the future, we will extend the QCI model to various HMI applications for student learning around the world.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Fuzzy} {Systems} ({FUZZ}-{IEEE})},
	author = {Lee, Chang-Shing and Wang, Mei-Hui and Chiang, Jun-Kui and Kubota, Naoyuki and Sato-Shimokawara, Eri and Nojima, Yusuke and Acampora, Giovanni and Wu, Pei-Yu and Chiu, Szu-Chi and Yang, Sheng-Chi and Siow, Chyan-Zheng},
	month = jun,
	year = {2024},
	note = {ISSN: 1558-4739},
	keywords = {ChatGPT, Semantics, Generative AI, Machine learning, Neural networks, Intelligent systems, Human computer interaction, Computational linguistics, Computational modeling, Fuzzy inference, Measurement, Students, Quantum computing, Sentence BERT, Syntactics, Social networking (online), Speech enhancement, Content Attention Ontology, Fuzzy Markup Language, Generative AI Image Agent, IEEE CIS Education Portal, Image synthesis, NUTN.TW-LLM, Quantum CI Agent, Ontology's, Gene transfer, Problem oriented languages, Computer aided language translation, Curricula, Personnel training, CI-Agent, Content attention ontology, Education portals, Fuzzy markup languages, Fuzzy rules, Generative AI image agent, IEEE CIS education portal, Motion estimation, National university of tainan., Quantum CI agent, SGML, Swarm intelligence, TW-LLM},
	pages = {1--8},
	annote = {Cited by: 2},
}

@inproceedings{safronov_using_2024,
	title = {Using {Neural} {Networks} in {Building} an {Ontology} of {Educational} {Subjects} for {Solving} {Educational} {Tasks}},
	doi = {10.1109/℡E62556.2024.10605700},
	abstract = {In the modern world, information technologies have become an integral part of a human life, including the professional level. The rapid development of technologies based on artificial intelligence over the past few years has opened up new opportunities for their application in solving various educational tasks. One of the relevant topics for study is the investigation of ontological constructs in texts, identifying the terminology of concepts and determining the relationships between them. This article is dedicated to studying artificial intelligence systems as a tool for solving educational process tasks: it proposes the use of chatbots based on AI systems in conjunction with various digital tools in researching ontological constructs in educational texts. As part of the research, an example is provided with the processing an educational text on mathematics. Methodological characteristics are considered and a model for researching educational text using the ChatGPT is briefly described. Conclusions are drawn about the existing possibilities and difficulties in implementing the aforementioned model.},
	booktitle = {2024 4th {International} {Conference} on {Technology} {Enhanced} {Learning} in {Higher} {Education} (℡{E})},
	author = {Safronov, Artyom A.},
	month = jun,
	year = {2024},
	keywords = {Terminology, ontology, Neural networks, chatbot, Thesauri, Mathematical models, thesaurus, Chatbots, Systematics, artificial intelligence systems, digital tools, Educational courses, educational texts},
	pages = {189--191},
}

@inproceedings{schoch_nl2ibe_2024,
	title = {{NL2IBE} – {Ontology}-controlled {Transformation} of {Natural} {Language} into {Formalized} {Engineering} {Artefacts}},
	doi = {10.1109/CAI59869.2024.00182},
	abstract = {Looking at Process and Automation Engineering (P\&AE) today, for the technically adept engineer, there are many different tools available to support the engineering work from translation of engineering intentions into module and plant descriptions, to definition and parametrization of entire process plant setups, for export to a control system. However, still today, in the very early engineering phases, engineering intentions either need to be entered already in a structured and controlled expert language or require a human expert’s manual efforts for translation from unstructured language into formalized representations, in order for thereon-based consistent further processing in the existing tools. This process is time-consuming, fuzzy, and error-prone due to potential misconceptions and ambiguities, even for domain experts. In this work, we therefore present our NL2IBE Tool, which makes use of modern Natural Language Processing in combination with Ontology Mining, and which, based on and controlled by an underlying ontology, allows for the deterministic transformation of natural language intentions into structured and consistent engineering artefacts. We describe the overall tool architecture as well as crucial functionalities and implementation features, followed by an evaluation by the example of a hydrogen generation and CCSU use case. We conclude with a discussion of the proposed tool and give an outlook on future research. (Abstract)},
	booktitle = {2024 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Schoch, Nicolai and Hoernicke, Mario},
	month = jun,
	year = {2024},
	keywords = {Ontologies, Natural language processing, Automation, natural language processing, NLP, generative AI, Process control, Manuals, automation engineering, Control systems, Hydrogen, intend-based engineering, ontological domain representation, process \&Amp},
	pages = {997--1004},
}

@inproceedings{arrotta_contextgpt_2024,
	title = {{ContextGPT}: {Infusing} {LLMs} {Knowledge} into {Neuro}-{Symbolic} {Activity} {Recognition} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199931500&doi=10.1109%2FSMARTCOMP61445.2024.00029&partnerID=40&md5=309392cfa6fca5f257e6a14d9246c2da},
	doi = {10.1109/SMARTCOMP61445.2024.00029},
	abstract = {Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities. In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed. Unlike ontologies, ContextGPT requires limited human effort and expertise, while sharing similar privacy concerns if the reasoning is performed in the cloud. An extensive evaluation using two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Arrotta, Luca and Bettini, Claudio and Civitarese, Gabriele and Fiori, Michele},
	month = jun,
	year = {2024},
	note = {ISSN: 2693-8340},
	keywords = {Ontologies, Large language model, Ontology, Language model, large language models, Deep learning, Knowledge engineering, Activity recognition, Training, Privacy, Computational linguistics, Computational modeling, context-awareness, Human activity recognition, Commonsense knowledge, human activity recognition, Learning systems, Pattern recognition, Ontology's, Context-Aware, Professional aspects, Context- awareness, Human activities, Model knowledge},
	pages = {55--62},
	annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{helali_kglids_2024,
	title = {{KGLiDS}: {A} {Platform} for {Semantic} {Abstraction}, {Linking}, and {Automation} of {Data} {Science}},
	doi = {10.1109/ICDE60146.2024.00021},
	abstract = {In recent years, we have witnessed the growing interest from academia and industry in applying data science technologies to analyze large amounts of data. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.) are created. However, there has been no systematic attempt to holistically collect and exploit all the knowledge and experiences that are implicitly contained in those artifacts. Instead, data scientists recover information and expertise from colleagues or learn via trial and error. Hence, this paper presents a scalable platform, KGLiDS, that employs machine learning and knowledge graph technologies to abstract and capture the semantics of data science artifacts and their connections. Based on this information, KGLiDS enables various downstream applications, such as data discovery and pipeline automation. Our comprehensive evaluation covers use cases in data discovery, data cleaning, transformation, and AutoML. It shows that KGLiDS is significantly faster with a lower memory footprint than the state-of-the-art systems while achieving comparable or better accuracy.},
	booktitle = {2024 {IEEE} 40th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Helali, Mossad and Monjazeb, Niki and Vashisth, Shubham and Carrier, Philippe and Helal, Ahmed and Cavalcante, Antonio and Ammar, Khaled and Hose, Katja and Mansour, Essam},
	month = may,
	year = {2024},
	note = {ISSN: 2375-026X},
	keywords = {Knowledge graphs, Automation, Semantics, Knowledge Graphs, Machine learning, Data Integration, Accuracy, Data Discovery, Pipelines, Systematics, Graph Neural Networks, Linked Data Science},
	pages = {179--192},
}

@inproceedings{cavalleri_construction_2024,
	title = {Construction and {Enhancement} of an {RNA}-{Based} {Knowledge} {Graph} for {Discovering} {New} {RNA} {Drugs}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200506979&doi=10.1109%2FICDE60146.2024.00453&partnerID=40&md5=6f0b57b06f9d152f71c1d5910f4067e6},
	doi = {10.1109/ICDE60146.2024.00453},
	abstract = {Cutting-edge technologies in RNA biology are pushing the study of fundamental biological processes and human diseases and accelerate the development of new drugs tailored to the patient's biomolecular characteristics. Even if many structured and unstructured data sources report the interaction among different RNA molecules and some other biomedical entities (e.g., drugs, diseases, genes), we still lack a comprehensive and well-described RNA-centered Knowledge Graph (KG) that contains such information and sophisticated services that support the user in its creation, maintenance, and enhancement. This PhD project aims to create a biomedical KG (named RNA-KG) to represent, and eventually infer, biological, experimentally validated interactions between different RNA molecules. We also wish to enhance the KG content and develop sophisticated services designed ad-hoc to support the user in predicting uncovered relationships and identifying new RNA-based drugs. Services will rely on deep learning methods that consider the heterogeneity of the graph and the presence of an ontology that describes the possible relationships existing among the involved entities. Moreover, we will consider Large Language Models (LLMs) in combination with RNA-KG for interacting with the user with the ground truth information contained in our KG for extracting relationships from unstructured data sources.},
	booktitle = {2024 {IEEE} 40th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Cavalleri, Emanuele and Mesiti, Marco},
	month = may,
	year = {2024},
	note = {ISSN: 2375-026X},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, Language model, Large language models, Bioinformatics, Deep learning, Knowledge engineering, LLMs, Biomedical knowledge graphs, Unstructured data, Graph representation, Graph representation learning, Drugs, RNA, Drug interactions, Soft sensors, Learning systems, RNA therapeutics, Data-source, RNA molecules, Biomedical knowledge graph},
	pages = {5639--5643},
	annote = {Cited by: 1},
}

@inproceedings{peng_construction_2024,
	title = {Construction of {Rail} {Transit} {Network} {Fault} {Knowledge} {Graph} {Based} on {Pseudo}-{Dynamic} {Relationship} {Ontology} {Architecture}},
	doi = {10.1109/CCDC62350.2024.10588309},
	abstract = {In this paper, an approach to construction of rail transit network fault knowledge graph based on pseudo-dynamic relationship ontology architecture is proposed. Firstly, an ontology architecture that includes pseudo-dynamic relationships is employed to completely separate fault patterns from fault entities without losing semantics. Secondly, a named entity recognition method based on the BERT-BiGRU-CRF model is adopted, which performs well in short entity extraction tasks. Thirdly, a pseudo-dynamic relationship extraction method based on the BERT-MEA(multi entities attention) model is adopted to extract static simple relationships, followed by treating triples as head and tail entities to determine pseudo-dynamic relationships. The implementation of rail transit network fault knowledge graph demonstrates the effectiveness of the proposed approach.},
	booktitle = {2024 36th {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	author = {Peng, Tao and Rao, Taiwen and Xu, Yansong and Yang, Chao and Xie, Xiaotian and Yang, Chunhua},
	month = may,
	year = {2024},
	note = {ISSN: 1948-9447},
	keywords = {Ontologies, Knowledge graphs, Named entity recognition, Semantics, Databases, Fault knowledge graph, Ontology architecture, Pseudo-dynamic relationship, Rails, Short entity, Tail},
	pages = {4582--4587},
}

@inproceedings{vanitha_synergy_2024,
	title = {Synergy of {Human} {Language} {Processing} and {Artificial} {Intelligence}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199428543&doi=10.1109%2FAMATHE61652.2024.10582050&partnerID=40&md5=30f7be15a8a305a3ce83a24268596b45},
	doi = {10.1109/AMATHE61652.2024.10582050},
	abstract = {This study explores the intriguing harmony between human language processing and artificial intelligence (AI). We delve into the intricate process of translating mental concepts into linguistic expressions, akin to the capabilities of AI language models. Through a survey of existing research, we uncover the parallelism between the ontological taxonomy guiding human cognition and AI's language understanding mechanisms. Moreover, we investigate how AI's multilingual proficiency mirrors the cognitive multilingualism found in individuals. This convergence holds implications for cross-lingual communication and AI-human collaboration. Our analysis anticipates a symbiotic future where the interplay of cognitive insights and AI advancements amplifies the potential of both realms.},
	booktitle = {2024 {International} {Conference} on {Advances} in {Modern} {Age} {Technologies} for {Health} and {Engineering} {Science} ({AMATHE})},
	author = {Vanitha, V. and Antony Rai, A. Stephan and Vinodhini, D. and Gnanaprasanambikai, L. and Kumar, L. Senthil and Renukadevi, S.},
	month = may,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Language model, Artificial intelligence, Semantics, Taxonomies, Artificial Intelligence, Taxonomy, Mathematical models, Multilingualism, Language understanding, Computational modeling, Surveys, Linguistics, Cross-Lingual Communication, Language Cognition, On tological Taxonomy, Symbiosis, Natural language processing systems, Linguistic expressions, Cross-lingual communication, Human cognition, Human language processing, Language cognition, Mental concepts, On tological taxonomy},
	pages = {1--7},
	annote = {Cited by: 0},
}

@article{sun_semanticformer_2024,
	title = {{SemanticFormer}: {Holistic} and {Semantic} {Traffic} {Scene} {Representation} for {Trajectory} {Prediction} {Using} {Knowledge} {Graphs}},
	volume = {9},
	issn = {2377-3766},
	doi = {10.1109/LRA.2024.3426386},
	abstract = {Trajectory prediction in autonomous driving relies on accurate representation of all relevant contexts of the driving scene, including traffic participants, road topology, traffic signs, as well as their semantic relations to each other. Despite increased attention to this issue, most approaches in trajectory prediction do not consider all of these factors sufficiently. We present SemanticFormer, an approach for predicting multimodal trajectories by reasoning over a semantic traffic scene graph using a hybrid approach. It utilizes high-level information in the form of meta-paths, i.e. trajectories on which an agent is allowed to drive from a knowledge graph which is then processed by a novel pipeline based on multiple attention mechanisms to predict accurate trajectories. SemanticFormer comprises a hierarchical heterogeneous graph encoder to capture spatio-temporal and relational information across agents as well as between agents and road elements. Further, it includes a predictor to fuse different encodings and decode trajectories with probabilities. Finally, a refinement module assesses permitted meta-paths of trajectories and speed profiles to obtain final predicted trajectories. Evaluation of the nuScenes benchmark demonstrates improved performance compared to several SOTA methods. In addition, we demonstrate that our knowledge graph can be easily added to two graph-based existing SOTA methods, namely VectorNet and LaFormer, replacing their original homogeneous graphs. The evaluation results suggest that by adding our knowledge graph the performance of the original methods is enhanced by 5\% and 4\%, respectively.},
	number = {9},
	journal = {IEEE Robotics and Automation Letters},
	author = {Sun, Zhigang and Wang, Zixu and Halilaj, Lavdim and Luettin, Juergen},
	month = sep,
	year = {2024},
	keywords = {Ontologies, Knowledge graphs, Semantics, Transformers, Trajectory, intelligent transportation systems, Encoding, Predictive models, autonomous agents, Semantic scene understanding},
	pages = {7381--7388},
}

@inproceedings{lehmann_extracting_2024,
	title = {Extracting {Metadata} from {Learning} {Videos} for {Ontology}-{Based} {Recommender} {Systems} {Using} {Whisper} \& {GPT}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199069048&doi=10.1109%2FEDUCON60312.2024.10578858&partnerID=40&md5=eecad3788d4b65826c2465c2715e139f},
	doi = {10.1109/EDUCON60312.2024.10578858},
	abstract = {In modern education, individualized learning environments play a vital role by allowing learners to tailor their learning paths based on personal needs, interests, and abilities. Achieving effective individualization relies on dynamic adaptation of the learning path, typically facilitated by recommender systems. These systems offer personalized suggestions, commonly employing content-based or collaborative filtering approaches. However, traditional recommender systems often lack consideration of the semantics of learning elements. To address this limitation, ontology-based recommender systems integrate semantic modeling, establishing additional connections within a domain to enhance precision and context in recommendations. Notably, these systems mitigate the cold start problem and are particularly advantageous in learning environments with limited data. While videos are prevalent in learning platforms, their unstructured nature poses challenges for processing. This paper introduces an innovative approach, leveraging Large Language Models, specifically GPT, to extract metadata from learning videos. The proposed method intelligently augments videos and links them to a domain ontology, enabling the integration of videos into ontology-based recommender systems. The application of this approach is demonstrated through a case study in software engineering education, showcasing its potential to enhance individualized learning experiences in specific domains. The presented method offers an automated alternative to manual video processing, aligning with the evolving landscape of education technology.},
	booktitle = {2024 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
	author = {Lehmann, Alexander and Landes, Dieter},
	month = may,
	year = {2024},
	note = {ISSN: 2165-9567},
	keywords = {Ontologies, Large language model, Ontology, Language model, Large language models, Semantics, Generative AI, large language models, Metadata, Ontology-based, generative AI, Recommender systems, Computational linguistics, learning analytics, Collaborative filtering, Engineering education, Manuals, Learning systems, adaptive learning environments, learning videos, ontology-based recommender systems, Computer aided instruction, Learning analytic, Adaptive learning environment, Individualized learning, Learning environments, Learning video, Ontology-based recommende system, Video signal processing},
	pages = {1--8},
	annote = {Cited by: 0},
}

@inproceedings{melzer_sustainable_2024,
	title = {Sustainable {Development} of {Information} {Systems} {Using} {SysML}, {FAS} and {DOL}},
	doi = {10.1109/SysCon61195.2024.10553629},
	abstract = {The use of product families can improve the efficiency of product development as opposed to develop a single-product by reusing existing artefacts and optimizing variability, which leads to a saving of resources and is therefore a sustainable approach. To extend new variants of an already modelled product via the approach of a product family the challenge is to merge the product models, so that the same and varying parts in the model are semantically correct identified and mapped to each other. This paper proposes a comprehensive approach for sustainable development of product families using the Distributed Ontology Language (DOL), Functional Architectures for Systems (FAS), and Systems Modeling Language (SysML). The proposed approach integrates the idea of DOL to represent an ontology of sustainability criteria and to map them to the functional architecture of a product family. FAS is used to model the functional architecture of the product family, while the FAS ontology is used to formalize the functional architecture and provide a standardized vocabulary and set of rules for modelling the functional aspects of the product family. SysML is used to model the product family. The proposed method is demonstrated through a case study of developing a sustainable product family of vacuum cleaner robots. The results show that the proposed method can help to identify opportunities for reducing environ-mental impact and improving social responsibility in the product family design while ensuring that functional requirements and design constraints are met semantically correct.},
	booktitle = {2024 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Melzer, Sylvia and Weilkiens, Tim and Muggeo, Christian and Berres, Axel},
	month = apr,
	year = {2024},
	note = {ISSN: 2472-9647},
	keywords = {Ontologies, Product design, Sustainable development, sustainability, Product development, Vocabulary, SysML, Systems Modeling Language, Biological system modeling, Distributed Ontology Language, functional architectures for systems, product family},
	pages = {1--8},
}

@inproceedings{chis_knowledge_2024,
	title = {A {Knowledge} {Graph} {Approach} to {Cyber} {Threat} {Mitigation} {Derived} from {Data} {Flow} {Diagrams}},
	doi = {10.1109/AQTR61889.2024.10554074},
	abstract = {Data Flow Diagrams (DFD) have proven effective in designing and analyzing the flow of data in enterprise systems. They serve as indispensable tools for enterprises that are undergoing transition to cloud services. DFDs aid in understanding the current processes, identifying interfaces and integration points that require security measures. This paper reports a Design Science project to mitigate the cyber security threats at the design phase of a system and to perform auditing of an existing system through knowledge graphs. The proposal leverages knowledge gathered from various sources in a knowledge graph to identify semantic relationships and patterns, enabling automated inference, analysis and detection of vulnerability patterns. Furthermore, LLM-based (large language models) capabilities transform data management details captured as Data Flow Diagrams (DFD) into knowledge graphs for semantic querying and improved decision support.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Automation}, {Quality} and {Testing}, {Robotics} ({AQTR})},
	author = {Chiş, Andrei and Stoica, Oliviu Ionuţ and Ghiran, Ana-Maria and Buchmann, Robert Andrei},
	month = may,
	year = {2024},
	note = {ISSN: 1844-7872},
	keywords = {Knowledge graphs, Semantics, knowledge graphs, LLMs, Security, Data models, security, threat modeling, privacy, Transforms, Current measurement, data flow diagrams, Proposals},
	pages = {1--6},
}

@article{li_fine-grained_2024,
	title = {Fine-{Grained} {Task} {Planning} for {Service} {Robots} {Based} on {Object} {Ontology} {Knowledge} via {Large} {Language} {Models}},
	volume = {9},
	issn = {2377-3766},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196082646&doi=10.1109%2FLRA.2024.3412593&partnerID=40&md5=75ddbef911fd12968cb418b84b3dc2d0},
	doi = {10.1109/LRA.2024.3412593},
	abstract = {In domestic environment, the successful execution of service tasks heavily relies on the robot's capability to identify and understand objects within its surrounding. This crucial process predominantly takes place during task planning, prior to the actual performance of service tasks. Therefore, it is vital that the robot is capable of formulating object-specific action sequences through task planning. In this letter, we propose the Fine-Grained Task Planning (FGTP) framework, an innovative method that combines object ontology knowledge with Large Language Models (LLMs) to create detailed action sequences. The FGTP framework is uniquely designed to process both text descriptions of service tasks and images of relevant objects, enabling a thorough comprehension of object attributes essential for task execution. Moreover, we have developed a set of rules based on these attributes to assist in the robot's decision-making process. In scenarios where service tasks fail because the object is in an unsuitable state, our framework deploys a logic-based reasoning method, concentrating on object attributes to identify suitable substitutes. This process leverages a pre-established semantic map to locate these alternatives, thus enabling a transition back to standard task planning. Our evaluations, conducted in both the VirtualHome simulation environment and with the TIAGo real robot, demonstrate the efficacy of our approach. This confirms our framework's capability to generate practical and implementable plans for various service tasks.},
	number = {8},
	journal = {IEEE Robotics and Automation Letters},
	author = {Li, Xiaodong and Tian, Guohui and Cui, Yongcheng},
	month = aug,
	year = {2024},
	note = {Type: Article},
	keywords = {Ontologies, Ontology, Language model, Semantics, Task planning, Decision making, Service robots, Computational linguistics, Object recognition, Planning, Task analysis, Robot kinematics, service robotics, Ontology's, Fine grained, Robot programming, Job analysis, Object ontology, Objects recognition, Service robotics},
	pages = {6872--6879},
	annote = {Cited by: 3},
}

@article{larhrib_ontological_2024,
	title = {An {Ontological} {Behavioral} {Modeling} {Approach} {With} {SHACL}, {SPARQL}, and {RDF} {Applied} to {Smart} {Grids}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3412656},
	abstract = {Every engineering process, especially software, involves two complementary aspects: structural and behavioral. Behavior is, in essence, transforming the structure associated with the system. As a language for the object-oriented paradigm, Unified Modeling Language (UML) offers constructs for both aspects, for example, class diagrams for the structural aspect and activity diagrams for the behavioral aspect. However, without obtaining directly executable models, in glass-box terms, or reasoning support, on the other hand, when software engineering is approached with ontologies, only constructs for structural aspects are provided to develop a directly executable model, thanks to their reasoning capability. However, there are no constructs or approaches for this paradigm’s specification or definition of behavior. This lack appears mainly in the early stages of the software engineering process, where there are no constructs similar to, e.g., the activity diagram in the object-oriented domain. Object Management Group (OMG) already addressed the transformation between the two paradigms in structural terms throughout Ontology Definition Metamodel (ODM) from UML to Resource Description Framework (RDF) and Web Ontology Language (OWL). However, there is no transformation of the object-oriented behavioral constructs into ontologies because they are not defined in the ontological paradigm. This paper addresses the definition of behavior in the ontology paradigm and the transformation of behavioral constructs between the two paradigms. The foundation of behavior specification is the flow concept, and the basis of this is the transformation of the structural model in an evaluative sense. Therefore, once the behavior has been defined in the ontology domain, the artifacts obtained throughout the life cycle are directly executable, and their validation and testing are automatic. With this approach, the life cycle is reduced to a modeling process. Thus, the resulting software engineering process improves features such as agility, simplicity, productivity, and formalism. The target audience for this work is the software engineering community, especially in the Model-Driven Engineering (MDE) paradigm approached from object-oriented and ontology perspectives. The evaluation of the proposed approach has been performed in the electric utilities, solving the problem of the validation flow for the interoperability process specified by the Common Grid Model Exchange Standard (CGMES) standard.},
	journal = {IEEE Access},
	author = {Larhrib, Mohamed and Escribano, Miguel and Cerrada, Carlos and Escribano, Juan Jose},
	year = {2024},
	keywords = {Ontologies, OWL, Software engineering, Resource description framework, Cognition, Unified modeling language, Object oriented modeling, Behavioral sciences, Behavioral modeling, CIM for ENTSO-E (CGMES), directly executable, ontology RDF/RDFS/OWL/SHACL},
	pages = {82041--82056},
}

@inproceedings{dequan_deep_2024,
	title = {Deep {Learning}-{Based} {Fault} {Knowledge} {Graph} {Construction} for {Power} {Communication} {Networks}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196653424&doi=10.1109%2FAEEES61147.2024.10544941&partnerID=40&md5=c83a4d58497ffee841d54e82d70995b7},
	doi = {10.1109/AEEES61147.2024.10544941},
	abstract = {Power communication network is a crucial infrastructure in the model power system, and its maintenance capability are crucial to ensuring the stable operation of power grid business. As an organized semantic knowledge base, the knowledge graph effectively organizes power communication network fault documentation and expert experience to enhance intelligent maintenance. This paper outlines a top-down approach to systematically construct a fault knowledge graph in the domain of power communication networks. The approach utilizes a seven-step method to establish a domain ontology model and integrates deep learning algorithms, including pre-trained language models, bidirectional long short time memory networks, convolutional neural networks and attention mechanisms. These algorithms process unstructured text to extract key entities and relationships. The effectiveness of the approach is verified through experiments using a product device document as a test case. Extracted knowledge is then visualized and stored using Neo4j database. Finally, this paper proposes a knowledge service model centered on fault knowledge graph and explores its application in fault diagnosis.},
	booktitle = {2024 6th {Asia} {Energy} and {Electrical} {Engineering} {Symposium} ({AEEES})},
	author = {Dequan, Gao and Pengyu, Zhu and Sheng, Wang and Ziyan, Zhao},
	month = mar,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Semantics, Deep learning, deep learning, Failure analysis, fault diagnosis, Fault detection, Convolutional neural networks, Patents, Graph construction, Real-time systems, Power grids, Fault knowledge graph, fault knowledge graph, power communication networks, Learning algorithms, Faults diagnosis, Grid business, Model power systems, Power communication networks, Stable operation},
	pages = {1088--1093},
	annote = {Cited by: 0},
}

@inproceedings{libro_integrating_2024,
	title = {Integrating {Modeling} {Languages} with {Ontologies} in the {Context} of {Industry} 4.0},
	doi = {10.1109/ICIT58233.2024.10540801},
	abstract = {The evolving landscape of manufacturing systems and the increasing complexity of production lines necessitate innovative approaches for efficient information management and process modeling. The System Modeling Language (SysML) provides a powerful language to express such information. However, the expressiveness comes at a cost: on the one hand, the modeling phase requires a deep understanding of the domain; on the other, SysML lacks rigorous semantics. This work introduces a novel methodology that enriches the SysML with ontology reasoning in the context of manufacturing systems. The approach uses ontologies as a comprehensive knowledge base that encapsulates essential details about the machinery, their provided functions, and the associated constraints. The approach offers a reliable and efficient way to verify the consistency and correctness of production recipes: it ensures recipes' practical applicability in the manufacturing process while reducing errors that can occur in the modeling phase. The proposed methodology has been validated through its application to a fully-fledged manufacturing line, showing its applicability in real-world scenarios.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Industrial} {Technology} ({ICIT})},
	author = {Libro, Mario and Gaiardelli, Sebastiano and Lora, Michele and Fummi, Franco},
	month = mar,
	year = {2024},
	note = {ISSN: 2643-2978},
	keywords = {Ontologies, knowledge representation, Reliability, Cognition, Knowledge based systems, Process modeling, Systems Modeling Language, Computer-aided manufacturing, Manufacturing processes, process modeling},
	pages = {1--7},
}

@inproceedings{timperley_mapping_2024,
	title = {Mapping the {MBSE} {Environment} and {Complementary} {Design} {Space} {Exploration} {Techniques}},
	doi = {10.1109/AERO58975.2024.10521188},
	abstract = {Today’s MBSE tools and environments are highly varied and therefore present a challenge for organizations looking to implement MBSE. Furthermore, while MBSE environments are highly capable of supporting the description of design baselines, the current capabilities within these environments could be further refined for exploring alternative designs. As a result it is important to gain an understanding of the limitations of current MBSE tooling in performing the valuable activity of design space exploration, and identify a set of candidate techniques to combat these. This paper reviews the various options available to MBSE practitioners by comparing some of the most common MBSE languages, tools and methods. The possible issues that can be encountered when exploring different designs have been identified and assigned a severity rating. A set of design space exploration techniques are presented, and where possible these have been sourced from existing literature. A knowledge graph has been constructed to collect all this data into a structured format, containing all the MBSE languages, tools, methods, design space exploration-related issues and techniques, as well as the relationships between each of these. This knowledge graph, implemented as a Neo4j graph database, allowed deeper insights to be drawn from the collected information. By defining a selected MBSE environment, including language, tool and method, the knowledge graph could be used to identify the least troublesome sequence (with minimum number of related issues) to arrive at a desired design artifact, for example a set of optimized system parameters. Beside this, the knowledge graph could be used to display the relationships and clusters of MBSE languages, tools and methods, to assist organizations with selecting suitable MBSE environment elements. Future work will bring greater depth to the analysis available with the knowledge graph, for instance, differentiation between different types of design space exploration issues and techniques.},
	booktitle = {2024 {IEEE} {Aerospace} {Conference}},
	author = {Timperley, Louis and Berthoud, Lucy and Snider, Chris and Tryfonas, Theo},
	month = mar,
	year = {2024},
	note = {ISSN: 1095-323X},
	keywords = {Knowledge graphs, Knowledge based systems, Reviews, Design methodology, Organizations, Databases, Codes},
	pages = {1--20},
}

@inproceedings{peer_nlp4ref_2024,
	title = {{NLP4ReF}: {Requirements} {Classification} and {Forecasting}: {From} {Model}-{Based} {Design} to {Large} {Language} {Models}},
	doi = {10.1109/AERO58975.2024.10521022},
	abstract = {We introduce Natural Language Processing for Requirement Forecasting (NLP4ReF), a model-based machine learning and natural language processing solution for enhancing the Requirements Engineering (RE) process. RE continues to face significant challenges and demands innovative approaches for process efficiency. Traditional RE methods relying on natural language struggle with incomplete, hidden, forgotten, and evolving requirements during and after the critical design review, risking project failures and setbacks. NLP4ReF tackles several key challenges: a) distinguishing between functional and non-functional requirements, b) classification of requirements by their respective system classes, and c) generation of unanticipated requirements to enhance project success. NLP4ReF employs a common natural language toolkit (NLTK) package and the recently-trending Chat-GPT. We tested NLP4ReF on PROMISE\_exp, a pre-existing dataset with 1000 software requirements, and PROMISE\_IoT, an enhanced dataset with 2000 software and IoT requirements. We validated NLP4ReF on a genuine IoT project. NLP4ReF swiftly generated dozens of new requirements, verified by a team of systems engineers, of which over 70\% were crucial for project success. We found that GPT is superior in authentic requirement generation, while NLTK excels at requirement classification. NLP4ReF offers significant time saving, effort reduction, and improved future-proofing. Our model-based design approach provides a foundation for enhanced RE practices and future research in this domain.},
	booktitle = {2024 {IEEE} {Aerospace} {Conference}},
	author = {Peer, Jordan and Mordecai, Yaniv and Reich, Yoram},
	month = mar,
	year = {2024},
	note = {ISSN: 1095-323X},
	keywords = {Natural language processing, Natural Language Processing, Machine Learning, Internet of Things, Requirements engineering, Software, Data models, Model-Based Systems Engineering, Training data, Software algorithms, Classification algorithms, Requirements Engineering Requirement Forecasting},
	pages = {1--16},
}

@article{yang_stn4dst_2024,
	title = {{STN4DST}: {A} {Scalable} {Dialogue} {State} {Tracking} {Based} on {Slot} {Tagging} {Navigation}},
	volume = {32},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2024.3393733},
	abstract = {Dialogue state tracking plays a key role in tracking user intentions in task-oriented dialogue systems. Traditional dialogue state tracking methods usually rely on selecting slot values from a fixed ontology to represent the dialogue state. In recent years, more flexible open vocabulary based approaches have become the mainstream focus which are mainly divided into two categories: generative methods and span extraction methods. Among them, the span extraction method is favored for its outstanding ability to predict unknown slot values. However, the span extraction method only focuses on the predicted slot values, but ignores other potential slot values in the utterance, which leads to insufficient semantic understanding of the utterance and difficulty in dealing with complex utterance scenarios, such as more or longer unknown slot values. To tackle the above drawbacks, in this paper, we propose a novel scalable dialogue state tracking method, which employs slot tagging to locate all potential slot values in the utterances and jointly learns slot pointers to select the predicted slot value from them. Specifically, our STN4DST (Slot Tagging Navigation for Dialogue State Tracking) model not only adopts the above joint learning strategy, which we call slot tagging navigation, to extract slot values from utterances, but also uses previous dialogue states as dialogue contexts to track the change of slot values, and introduces appendix slot values to predict special slot values that cannot be extracted. Extensive experiments show that in the open vocabulary setting, STN4DST achieves the state-of-the-art joint goal accuracy of 85.4\% and 96.5\% on Sim-M and Sim-R datasets with a large number of unknown slot values, and is also comparable to other state-of-the-art models in the absence of token-level slot annotations for all potential slot values.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Yang, Puhai and Huang, Heyan and Shi, Shumin and Mao, Xian-Ling},
	year = {2024},
	keywords = {Ontologies, Semantics, Vocabulary, Task-oriented dialogue system, dialogue state tracking, Tagging, Predictive models, Navigation, scalable DST, Speech processing, unknown slot value},
	pages = {2494--2507},
}

@inproceedings{zhu_joint_2024,
	title = {Joint {Extraction} of {Entity} {Relationships} in {Walnut} {Disease} and {Pest} {Based} on {Chinese} {NLP} {Models}},
	doi = {10.1109/EEBDA60612.2024.10485759},
	abstract = {This study addresses the limited application of deep learning techniques in the field of walnut disease and pest and the challenges posed by complex relationships and diverse entity types in this domain. We propose a deep learning-based method for constructing a knowledge graph in the walnut disease and pest domain, incorporating ontologies to establish a conceptual model for the disease and pest knowledge graph. To overcome issues such as relationship overlap (e.g., one-to-many, many-to-many) and loss of relationship chains, we introduce a novel labeling scheme called “based on ontology binding BIESO (Begin-Inside-End-Single-Other)” that directly models triplets. By employing a label matching algorithm, we obtain triplet data. We train and predict on the dataset using an end-to-end model consisting of Bidirectional Encoder Representations from Transformers (BERT), Bi-directional Gate Recurrent Unit (BiGRU), and Conditional Random Field (CRF). Experimental results show an F1 score of 75.79\%, outperforming models such as BERT-BiLSTM-CRF and word2vec-BiGRU-CRF. We semiautomatically extract unstructured knowledge and store the extracted triplets in a Neo4j graph database, enabling visualization of the knowledge. The research methodology of this knowledge graph can serve as a reference for constructing knowledge graphs in walnut agriculture and developing intelligent question-answering systems for walnut disease and pest.},
	booktitle = {2024 {IEEE} 3rd {International} {Conference} on {Electrical} {Engineering}, {Big} {Data} and {Algorithms} ({EEBDA})},
	author = {Zhu, Ruiliang and Song, Xiangshuai and Zhang, Hao and Cai, Xuli},
	month = feb,
	year = {2024},
	keywords = {Ontologies, Knowledge graphs, ontology, deep learning, knowledge graph, Transformers, Bidirectional control, Predictive models, Logic gates, BERTBiGRU-CRF, Prediction algorithms, Walnut disease and pest},
	pages = {1027--1035},
}

@article{strader_indoor_2024,
	title = {Indoor and {Outdoor} {3D} {Scene} {Graph} {Generation} {Via} {Language}-{Enabled} {Spatial} {Ontologies}},
	volume = {9},
	issn = {2377-3766},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189615964&doi=10.1109%2FLRA.2024.3384084&partnerID=40&md5=86b59d43a6f2399478a764bbb05e2117},
	doi = {10.1109/LRA.2024.3384084},
	abstract = {This letter proposes an approach to build 3D scene graphs in arbitrary indoor and outdoor environments. Such extension is challenging; the hierarchy of concepts that describe an outdoor environment is more complex than for indoors, and manually defining such hierarchy is time-consuming and does not scale. Furthermore, the lack of training data prevents the straightforward application of learning-based tools used in indoor settings. To address these challenges, we propose two novel extensions. First, we develop methods to build a spatial ontology defining concepts and relations relevant for indoor and outdoor robot operation. In particular, we use a Large Language Model (LLM) to build such an ontology, thus largely reducing the amount of manual effort required. Second, we leverage the spatial ontology for 3D scene graph construction using Logic Tensor Networks (LTN) to add logical rules, or axioms (e.g., “a beach contains sand”), which provide additional supervisory signals at training time thus reducing the need for labelled data, providing better predictions, and even allowing predicting concepts unseen at training time. We test our approach in a variety of datasets, including indoor, rural, and coastal environments, and show that it leads to a significant increase in the quality of the 3D scene graph generation with sparsely annotated data.},
	number = {6},
	journal = {IEEE Robotics and Automation Letters},
	author = {Strader, Jared and Hughes, Nathan and Chen, William and Speranzon, Alberto and Carlone, Luca},
	month = jun,
	year = {2024},
	note = {Type: Article},
	keywords = {Ontologies, Ontology, Artificial intelligence, Semantics, Modeling languages, Scene understanding, Indoor environment, Training data, Solid modeling, Three-dimensional displays, Semantic scene understanding, 3D scene graphs, AI-based methods, Image analysis, semantic scene understanding, spatial ontologies, Spatial resolution, Ontology's, Three dimensional computer graphics, Personnel training, Spatial ontologies, 3d scene graph, 3D scenes, AI-based method, Scene-graphs, Solid modelling, Three dimensional displays, Three-dimensional display},
	pages = {4886--4893},
	annote = {Cited by: 6; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{vizcarra_representing_2024,
	title = {Representing the {Interaction} between {Users} and {Products} via {LLM}-assisted {Knowledge} {Graph} {Construction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192226431&doi=10.1109%2FICSC59802.2024.00043&partnerID=40&md5=1e6b06e16cb0ebab11075d09212e1302},
	doi = {10.1109/ICSC59802.2024.00043},
	abstract = {To understand user behavior, representing the semantic knowledge of user-product interaction is essential. In this paper, we represent the interaction between user and product via large language model (LLM)-assisted knowledge graph construction. We capture users’ behavioral actions and static properties of the products from raw text data of “user review” and “product catalog”. Moreover, the information needed for updating the knowledge graph is captured by raw texts of “news related to the products”. The proposed methodology integrates them as a single knowledge graph to provide causal reasoning on user-product interaction. To alleviate the situation where a small quantity of annotated text exists in these data, we use LLM as a data annotator and augmentor.},
	booktitle = {2024 {IEEE} 18th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Vizcarra, Julio and Haruta, Shuichiro and Kurokawa, Mori},
	month = feb,
	year = {2024},
	note = {ISSN: 2472-9671},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, LLM, Language model, Semantics, Text mining, text mining, ontology, Annotations, Causality, Cognition, Reviews, Data augmentation, Text-mining, causality, Graph construction, user-product interaction, Ontology's, Behavioral research, Product interaction, User behaviors, User-product interaction},
	pages = {231--232},
	annote = {Cited by: 8},
}

@inproceedings{luo_boosting_2024,
	title = {Boosting {LLMS} with {Ontology}-{Aware} {Prompt} for {Ner} {Data} {Augmentation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195380625&doi=10.1109%2FICASSP48485.2024.10446860&partnerID=40&md5=fe90a9ba5179ce13cc89bd5fa6b4ea7c},
	doi = {10.1109/ICASSP48485.2024.10446860},
	abstract = {Named Entity Recognition (NER) data augmentation (DA) aims to improve the performance and generalization capabilities of NER models by generating scalable training data. The key challenge lies in ensuring the generated samples maintain contextual diversity while preserving label consistency. However, existing dominant methods fail to simultaneously satisfy both criteria. Inspired by the extensive generative capabilities of large language models (LLMs), we propose ANGEL, a frAmework integrating the oNtoloGy structure and instructivE prompting within LLMs. Specifically, the hierarchical ontology structure guides prompt ranking, while instructive prompting enhances LLMs’ mastery of domain knowledge, empowering synthetic sample generation and annotation. Experiments show ANGEL surpasses state-of-the-art (SOTA) baselines, conferring absolute F1 increases of 2.86\% and 0.93\% on two benchmark datasets, respectively.},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Luo, Zhizhao and Wang, Youchen and Ke, Wenjun and Qi, Rui and Guo, Yikai and Wang, Peng},
	month = apr,
	year = {2024},
	note = {ISSN: 2379-190X},
	keywords = {Boosting, Data augmentation, Data Augmentation, Knowledge Graph, Large language Model, Named Entity Recognition, Ontologies, Signal processing, Speech recognition, Syntactics, Training data},
	pages = {12361--12365},
	annote = {Cited by: 6},
}

@inproceedings{liu_semantic_2024,
	title = {Semantic {Proximity} {Alignment}: {Towards} {Human} {Perception}-{Consistent} {Audio} {Tagging} by {Aligning} with {Label} {Text} {Description}},
	doi = {10.1109/ICASSP48485.2024.10446928},
	abstract = {Most audio tagging models are trained with one-hot labels as supervised information. However, one-hot labels treat all sound events equally, ignoring the semantic hierarchy and proximity relationships between sound events. In contrast, the event descriptions contains richer information, describing the distance between different sound events with semantic proximity. In this paper, we explore the impact of training audio tagging models with auxiliary text descriptions of sound events. By aligning the audio features with the text features of corresponding labels, we inject the hierarchy and proximity information of sound events into audio encoders, improving the performance while making the prediction more consistent with human perception. We refer to this approach as Semantic Proximity Alignment (SPA). We use Ontology-aware mean Average Precision (OmAP) as the main evaluation metric for the models. OmAP reweights the false positives based on Audioset ontology distance and is more consistent with human perception compared to mAP. Experimental results show that the audio tagging models trained with SPA achieve higher OmAP compared to models trained with one-hot labels solely (+1.8 OmAP). Human evaluations also demonstrate that the predictions of SPA models are more consistent with human perception.},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Liu, Wuyang and Ren, Yanzhen},
	month = apr,
	year = {2024},
	note = {ISSN: 2379-190X},
	keywords = {Semantics, Training, Transformer, Measurement, Natural languages, Tagging, Predictive models, Signal processing, Audio Classification, Audio-text Pretraining, Multi-modality, Sound Event Detection},
	pages = {541--545},
}

@inproceedings{xu_birgat_2024,
	title = {A {Birgat} {Model} for {Multi}-{Intent} {Spoken} {Language} {Understanding} with {Hierarchical} {Semantic} {Frames}},
	doi = {10.1109/ICASSP48485.2024.10446325},
	abstract = {Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we firstly propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin. Ablation study in transfer learning settings further uncovers the poor generalizability of current models in multi-intent cases.},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Xu, Hongshen and Cao, Ruisheng and Zhu, Su and Jiang, Sheng and Zhang, Hanchong and Chen, Lu and Yu, Kai},
	month = apr,
	year = {2024},
	note = {ISSN: 2379-190X},
	keywords = {Ontologies, Semantics, Decoding, Transfer learning, Spoken Language Understanding, Labeling, Acoustics, Signal processing, hierarchical semantic frame, relational graph attention network},
	pages = {12251--12255},
}

@article{ma_collappi_2024,
	title = {{CollaPPI}: {A} {Collaborative} {Learning} {Framework} for {Predicting} {Protein}-{Protein} {Interactions}},
	volume = {28},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2024.3375621},
	abstract = {Exploring protein-protein interaction (PPI) is of paramount importance for elucidating the intrinsic mechanism of various biological processes. Nevertheless, experimental determination of PPI can be both time-consuming and expensive, motivating the exploration of data-driven deep learning technologies as a viable, efficient, and accurate alternative. Nonetheless, most current deep learning-based methods regarded a pair of proteins to be predicted for possible interaction as two separate entities when extracting PPI features, thus neglecting the knowledge sharing among the collaborative protein and the target protein. Aiming at the above issue, a collaborative learning framework CollaPPI was proposed in this study, where two kinds of collaboration, i.e., protein-level collaboration and task-level collaboration, were incorporated to achieve not only the knowledge-sharing between a pair of proteins, but also the complementation of such shared knowledge between biological domains closely related to PPI (i.e., protein function, and subcellular location). Evaluation results demonstrated that CollaPPI obtained superior performance compared to state-of-the-art methods on two PPI benchmarks. Besides, evaluation results of CollaPPI on the additional PPI type prediction task further proved its excellent generalization ability.},
	number = {5},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Ma, Wenjian and Bi, Xiangpeng and Jiang, Huasen and Zhang, Shugang and Wei, Zhiqiang},
	month = may,
	year = {2024},
	keywords = {Deep learning, Graph neural network, Feature extraction, Collaboration, protein-protein interaction, Proteins, multi-task learning, Vectors, Protein engineering, Task analysis, protein representation learning},
	pages = {3167--3177},
}

@article{hu_transferability-based_2024,
	title = {A {Transferability}-{Based} {Method} for {Evaluating} the {Protein} {Representation} {Learning}},
	volume = {28},
	issn = {2168-2208},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186999797&doi=10.1109%2FJBHI.2024.3370680&partnerID=40&md5=67be716debc6229a1cf0c9bc6cd01e13},
	doi = {10.1109/JBHI.2024.3370680},
	abstract = {Self-supervised pre-trained language models have recently risen as a powerful approach in learning protein representations, showing exceptional effectiveness in various biological tasks, such as drug discovery. Amidst the evolving trend in protein language model development, there is an observable shift towards employing large-scale multimodal and multitask models. However, the predominant reliance on empirical assessments using specific benchmark datasets for evaluating these models raises concerns about the comprehensiveness and efficiency of current evaluation methods. Addressing this gap, our study introduces a novel quantitative approach for estimating the performance of transferring multi-task pre-trained protein representations to downstream tasks. This transferability-based method is designed to quantify the similarities in latent space distributions between pre-trained features and those fine-tuned for downstream tasks. It encompasses a broad spectrum, covering multiple domains and a variety of heterogeneous tasks. To validate this method, we constructed a diverse set of protein-specific pre-training tasks. The resulting protein representations were then evaluated across several downstream biological tasks. Our experimental results demonstrate a robust correlation between the transferability scores obtained using our method and the actual transfer performance observed. This significant correlation highlights the potential of our method as a more comprehensive and efficient tool for evaluating protein representation learning.},
	number = {5},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Hu, Fan and Zhang, Weihong and Huang, Huazhen and Li, Wang and Li, Yang and Yin, Peng},
	month = may,
	year = {2024},
	note = {Type: Article},
	keywords = {machine learning, natural language processing, Machine Learning, Information theory, Algorithms, bioinformatics, Protein representation learning, Databases, Computational linguistics, learning, Computational modeling, Transferability, drug development, Protein, Proteins, accuracy, language model, protein language model, protein structure, algorithm, Biological systems, human, protein, optimal transport, protein function, gene ontology, Biological system modeling, Predictive models, Learning systems, Protein engineering, Computational Biology, Biological information theory, Task analysis, protein representation learning, procedures, Humans, amino acid sequence, Article, probability, computer model, mathematical model, Computational modelling, protein database, sequence homology, Down-stream, Job analysis, Biological information theories, Optimal transport},
	pages = {3158--3166},
	annote = {Cited by: 1},
}

@article{chen_smart_2024,
	title = {Smart {Mining} {With} {Autonomous} {Driving} in {Industry} 5.0: {Architectures}, {Platforms}, {Operating} {Systems}, {Foundation} {Models}, and {Applications}},
	volume = {9},
	issn = {2379-8904},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186065598&doi=10.1109%2FTIV.2024.3365997&partnerID=40&md5=eab7b1a313e27297809660f660a410d7},
	doi = {10.1109/TIV.2024.3365997},
	abstract = {The increasing importance of mineral resources in contemporary society is becoming more prominent, playing an indispensable and crucial role in the global economy. These resources not only provide essential raw materials for the global economic system but also play an irreplaceable role in supporting the development of modern industry, technology, and infrastructure. With the rapid development of intelligent technologies such as Industry 5.0 and advanced Large Language Models (LLMs), the mining industry is facing unprecedented opportunities and challenges. The development of smart mines has become a crucial direction for industry progress. This article aims to explore the strategic requirements for the development of smart mines by combining advanced products or technologies such as Chat-GPT (one of the successful applications of LLMs), digital twins, and scenario engineering. We propose a comprehensive architecture consisting of three different levels, the mining industrial Internet of Things (IoT) platform, mining operating systems, and foundation models. The systems and models empower the mining equipment for transportation. The architecture delivers a comprehensive solution that aligns perfectly with the demands of Industry 5.0. The application and validation outcomes of this intelligent solution showcase a noteworthy enhancement in mining efficiency and a reduction in safety risks, thereby laying a sturdy groundwork for the advent of Mining 5.0.},
	number = {3},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Chen, Long and Li, Yuchen and Silamu, Wushour and Li, Qingquan and Ge, Shirong and Wang, Fei-Yue},
	month = mar,
	year = {2024},
	note = {Type: Article},
	keywords = {Ontologies, Large language models, Digital twins, Internet of things, Autonomous vehicles, Architecture, Industry 5.0, Autonomous driving, industry 5.0, Mining, autonomous driving, Computer architecture, Biological system modeling, Task analysis, architectures, Fifth Industrial Revolution, Mining 5.0, Mining industry, mining transportation trucks, Network architecture, smart mining, Ontology's, Mineral resources, Job analysis, Fifth industrial revolution, Industrial revolutions, Mining transportation truck, Mining transportations, Smart mining},
	pages = {4383--4393},
	annote = {Cited by: 27},
}

@article{zhao_predicting_2024,
	title = {Predicting {Protein} {Functions} {Based} on {Heterogeneous} {Graph} {Attention} {Technique}},
	volume = {28},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2024.3357834},
	abstract = {In bioinformatics, protein function prediction stands as a fundamental area of research and plays a crucial role in addressing various biological challenges, such as the identification of potential targets for drug discovery and the elucidation of disease mechanisms. However, known functional annotation databases usually provide positive experimental annotations that proteins carry out a given function, and rarely record negative experimental annotations that proteins do not carry out a given function. Therefore, existing computational methods based on deep learning models focus on these positive annotations for prediction and ignore these scarce but informative negative annotations, leading to an underestimation of precision. To address this issue, we introduce a deep learning method that utilizes a heterogeneous graph attention technique. The method first constructs a heterogeneous graph that covers the protein-protein interaction network, ontology structure, and positive and negative annotation information. Then, it learns embedding representations of proteins and ontology terms by using the heterogeneous graph attention technique. Finally, it leverages these learned representations to reconstruct the positive protein-term associations and score unobserved functional annotations. It can enhance the predictive performance by incorporating these known limited negative annotations into the constructed heterogeneous graph. Experimental results on three species (i.e., Human, Mouse, and Arabidopsis) demonstrate that our method can achieve better performance in predicting new protein annotations than state-of-the-art methods.},
	number = {4},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Zhao, Yingwen and Yang, Zhihao and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
	month = apr,
	year = {2024},
	keywords = {Deep learning, Annotations, Feature extraction, Protein function prediction, Proteins, Predictive models, Protein engineering, Amino acids, constructed heterogeneous graph, heterogeneous graph attention, positive and negative annotations},
	pages = {2408--2415},
}

@article{sewunetie_exploring_2024,
	title = {Exploring {Sentence} {Parsing}: {OpenAI} {API}-{Based} and {Hybrid} {Parser}-{Based} {Approaches}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3360480},
	abstract = {This study focuses on the fundamental process of parsing sentences to create semantic graphs from textual documents. It introduces novel techniques for parsing phrases within semantic graph-based induction, employing both ChatGPT-based and Hybrid parser-based approaches. Through a thorough analysis, the study evaluates the performance of these methods in generating semantic networks from text, particularly in capturing detailed event descriptions and relationships. Results indicate a slight advantage in accuracy for the Hybrid parser-based approach (87\%) compared to ChatGPT (85\%) in sentence parsing tasks. Furthermore, efficiency analysis reveals that ChatGPT’s response quality varies with prompt sizes, while the Hybrid parser-based method consistently maintains excellent response quality.},
	journal = {IEEE Access},
	author = {Sewunetie, Walelign Tewabe and Kovács, László},
	year = {2024},
	keywords = {ChatGPT, Knowledge graphs, Natural language processing, Semantics, natural language processing, Context modeling, Training, Chatbots, semantic graph, Adaptation models, Predictive models, Task analysis, adverb prediction, Application of sentence parsing, hybrid parser, sentence parsing},
	pages = {38801--38815},
}

@article{quevedo_legal_2024,
	title = {Legal {Natural} {Language} {Processing} {From} 2015 to 2022: {A} {Comprehensive} {Systematic} {Mapping} {Study} of {Advances} and {Applications}},
	volume = {12},
	issn = {2169-3536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178053115&doi=10.1109%2FACCESS.2023.3333946&partnerID=40&md5=9f41c71b213c8fe94892597b1be43fad},
	doi = {10.1109/ACCESS.2023.3333946},
	abstract = {The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal NLP). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models (LLMs), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study (SMS) to bridge this gap. We aim to provide a descriptive statistical analysis of the Legal NLP research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal NLP field. We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing (NLP), understanding the capabilities and limitations of Legal NLP becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal NLP researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns.},
	journal = {IEEE Access},
	author = {Quevedo, Ernesto and Cerny, Tomas and Rodriguez, Alejandro and Rivas, Pablo and Yero, Jorge and Sooksatra, Korn and Zhakubayev, Alibek and Taibi, Davide},
	year = {2024},
	note = {Type: Article},
	keywords = {Natural language processing, Deep learning, Information retrieval, deep learning, Mapping, Law, Surveys, Language processing, Natural languages, Search problems, Systematics, Task analysis, legal-NLP, Systematic-mapping-study, Natural language processing systems, Ethical technology, Job analysis, Legal-natural language processing, Search problem, Systematic, Systematic mapping studies},
	pages = {145286--145317},
	annote = {Cited by: 8; All Open Access; Gold Open Access},
}

@article{zhang_gnngo3d_2024,
	title = {{GNNGO3D}: {Protein} {Function} {Prediction} {Based} on {3D} {Structure} and {Functional} {Hierarchy} {Learning}},
	volume = {36},
	issn = {1558-2191},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177076809&doi=10.1109%2FTKDE.2023.3331005&partnerID=40&md5=4ed50ffe473b7ceacec0ea7af8f594d2},
	doi = {10.1109/TKDE.2023.3331005},
	abstract = {Protein sequences accumulate in large quantities, and the traditional method of annotating protein function by experiment has been unable to bridge the gap between annotated proteins and unannotated proteins. Machine learning-based protein function prediction is an effective approach to solve this problem. Most of the existing methods only use the protein sequence but ignore the three-dimensional structure which is closely related to the protein function. And the hierarchy of protein functions is not adequately considered. To solve this problem, we propose a graph neural network (GNNGO3D) that combines the three-dimensional structure and functional hierarchy learning. GNNGO3D simultaneously uses three kinds of information: protein sequence, tertiary structure, and hierarchical relationship of protein function to predict protein function. The novelty of GNNGO3D lies in that it integrates the learning of functional level information into the method of predicting protein function by using tertiary structure information, fully learning the relationship between protein functions, and helping to better predict protein function. Experimental results show that our method is superior to existing methods for predicting protein function based on sequence and structure.},
	number = {8},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Liyuan and Jiang, Yongquan and Yang, Yan},
	month = aug,
	year = {2024},
	note = {Type: Article},
	keywords = {Ontologies, Language model, machine learning, Gene Ontology, Gene ontology, Feature extraction, Forecasting, Graph neural networks, Protein function prediction, Convolutional neural network, Convolutional neural networks, Proteins, language model, gene ontology, Learning systems, Protein sequence, Three-dimensional displays, Task analysis, protein function prediction, Protein sequences, Ontology's, Machine-learning, Features extraction, Job analysis, Three dimensional displays, Three-dimensional display},
	pages = {3867--3878},
	annote = {Cited by: 5},
}

@inproceedings{wang_architecture_2023,
	title = {An {Architecture} {Modeling} {Framework} for {Distributed} {Automation} {Systems} {Using} {SysML} and {Semantic} {Web} {Technologies}},
	doi = {10.1109/ICMRA59796.2023.10708094},
	abstract = {The rising interdisciplinarity and complexity of the Distributed Automation Systems (DASs) require the systems to be modeled in an unambiguous and high-level abstract way for cross-discipline/stage communication and interoperability in the Model-Driven Development (MDD) lifecycle. The concept of the System Architecture Model in systems engineering has been adopted for this challenge. To support the creation and analysis of this model, a modeling framework with a modeling methodology, modeling language, knowledge base, and related toolkit is established based on SysML and Semantic Web Technologies. A modeling methodology which is the core of the framework for modeling the architecture of DASs is formally defined with domain-specificity, comprehensiveness, discipline-neutrality, and platform-independency. Based on it, the SysML-DAS modeling language is extended from SysML, and the System Architecture Ontology is built with the help of the Knowledge Extraction Tool. This ontology works as the knowledge base not only to provide a unified and unambiguous view of the system but also to support the automated accomplishment of tasks in the MDD process. As a typical task, the semantic correctness and integrity of the system architecture model can be assessed by the Knowledge Analysis Tool in this framework.},
	booktitle = {2023 6th {International} {Conference} on {Mechatronics}, {Robotics} and {Automation} ({ICMRA})(},
	author = {Wang, Dan and Li, Xiaofeng and Gu, Bin and Cao, Yue and Liu, Yusheng},
	month = nov,
	year = {2023},
	note = {ISSN: 2996-380X},
	keywords = {Ontologies, Automation, Semantic Web, Semantics, Knowledge based systems, Semantic Web technologies, Service-oriented architecture, SysML, Model-driven development, Analytical models, Systems architecture, distributed automation system, Mechatronics, System Architecture Model},
	pages = {191--200},
}

@inproceedings{tona_q-story_2023,
	title = {Q-{Story}: {An} {Ontology}-{Based} on {Quality} of {User} {Stories} in {Scrum}. {A} {Quantitative} {Assessment}},
	doi = {10.1109/CONISOFT58849.2023.00017},
	abstract = {Q-Story ontology was created utilizing the Methontology approach and further represented through the Meta Object Facility (MOF) and Unified Modeling Language (UML). Because aspects such as their structure, level of granularity, and comprehensibility hold considerable signifi-cance in ensuring a favorable project execution. That is, the quality of user stories significantly impacts the outcome of a software project, influencing its success or failure. Therefore, we performed a quantitative evaluation using the OntoQA method, resulting in a relationship richness value of 0.95, an attribute richness of 4.00, and an inheritance richness of 1.26. The outcome of this work will contribute to developing an ontology that can effectively create user stories with quality. Furthermore, it will serve as a valuable guide for development teams, aiding them in the creation, analysis, and development processes of user stories.},
	booktitle = {2023 11th {International} {Conference} in {Software} {Engineering} {Research} and {Innovation} ({CONISOFT})},
	author = {Tona, Claudia and Juárez-Ramírez, Reyes and Jiménez, Samantha and Murillo-Muñoz, Fernanda},
	month = nov,
	year = {2023},
	keywords = {Ontologies, Ontology, Software engineering, Software, Unified modeling language, Software Engineering, Technological innovation, User Story, Ontology Quality Evaluation, Quality Met-rics, Quantitative Assessment},
	pages = {55--64},
}

@inproceedings{mateiu_ontology_2023,
	title = {Ontology engineering with {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193849216&doi=10.1109%2FSYNASC61333.2023.00038&partnerID=40&md5=82842416ae615f60e5776b5c7fd1ef98},
	doi = {10.1109/SYNASC61333.2023.00038},
	abstract = {We tackle the task of enriching ontologies by automatically translating natural language (NL) into Description Logic (DL). Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert NL into OWL Functional Syntax. For fine-tuning, we designed pairs of sentences in NL and the corresponding translations. This training pairs cover various aspects from ontology engineering: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, or cardinality restrictions. The resulted axioms are used to enrich an ontology, in a human supervised manner. The developed tool is publicly provided as a Protégé plugin.},
	booktitle = {2023 25th {International} {Symposium} on {Symbolic} and {Numeric} {Algorithms} for {Scientific} {Computing} ({SYNASC})},
	author = {Mateiu, Patricia and Groza, Adrian},
	month = sep,
	year = {2023},
	note = {ISSN: 2470-881X},
	keywords = {Ontology engineering, Large language model, Ontology, Language model, large language models, OWL, Protege, Description logic, ontology engineering, Training, Computational linguistics, fine-tuning, Natural languages, Syntactics, Task analysis, Protege plugin, Scientific computing, Ontology's, Plug-ins, Fine tuning, Translation (languages), Data description},
	pages = {226--229},
	annote = {Cited by: 19; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{ferchichi_artificial_2023,
	title = {Artificial {Intelligence} {Based} {SysML} {Block} {Diagram} {Extension} and {Evolution} for {Product} {Lines}},
	doi = {10.1109/AICCSA59173.2023.10479303},
	abstract = {SysML is a standard language that permits to model systems of any type such as plane, ships and software intensive systems. Software Product Line large scale reuse approach has demonstrated its success. The industry provides benefits in term of cost savings and acceleration of time to maket. The available literature indicates that there have been efforts to enhance the capability of SysML in handling product families. However, these attempts are not yet fully systematic, and there remains a significant amount of work to be undertaken in this area. In this present paper, we deal with the SysML Block Diagram in order to investigate to what extent it permits variability representation and how it can evolve during the system evolution or when agility is needed. We want to capitalize on the knowledge necessary for block diagram extention and evolution and take advantage of knowledge from Product Line domain engineering and application engineering. So, we decide to use an ontology which is an articifial intelligence artifact. An ontology is a powerful mean to represent knowledge and reason about it. Here, we use the ontology to help decision making for Block diagram evolution as well.},
	booktitle = {2023 20th {ACS}/{IEEE} {International} {Conference} on {Computer} {Systems} and {Applications} ({AICCSA})},
	author = {Ferchichi, Olfa and Beltaifa, Raoudha and Labed Jilani, Lamia},
	month = dec,
	year = {2023},
	note = {ISSN: 2161-5330},
	keywords = {Ontologies, Ontology, Description logic, Evolution, Software, Variability, Industries, Systematics, Systems architecture, Artificial Intelligence Artifact, Mobile handsets, Product Line Engineering, SysML block diagram},
	pages = {1--8},
}

@inproceedings{du_ontology-based_2023,
	title = {An {Ontology}-based {Method} for {Heterogeneous} {Data} {Governance} with {MFI} and {MDR}},
	doi = {10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00158},
	abstract = {Currently, data governance and integration in the fields are hindered by semantic ambiguity and syntactic inconsistencies among different sub domain data and models, making it difficult to use these valuable data for further application and research jointly. Therefore, it is necessary to unify and integrate existing metadata and meta-models more effectively for information resource sharing and interoperability in the field. This paper proposes an ontology-based approach with a hybrid ISO/IEC 11179 (MDR) and ISO/IEC 19763 (MFI) framework for data governance and integration. This framework takes a Global Ontology Model (GOM) constructed for the global domain as a bridge and basis for integrating and aligning heterogeneous sub domains. It extends MDR by adding ontology registration items to implement the mapping between the GOM and multiple subdomains, thereby promoting the semantic sharing of metadata between subdomains. In addition, the MFI-12 and MFI-10 are used to solve the model interoperability between different subdomains. A detailed case study is provided to illustrate the concrete registration process and demonstrate the validity of our method.},
	booktitle = {2023 {IEEE} {International} {Conference} on {High} {Performance} {Computing} \& {Communications}, {Data} {Science} \& {Systems}, {Smart} {City} \& {Dependability} in {Sensor}, {Cloud} \& {Big} {Data} {Systems} \& {Application} ({HPCC}/{DSS}/{SmartCity}/{DependSys})},
	author = {Du, Zhihong and Xu, Duo and Huang, Danruo and Hu, Yuren and He, Keqing and Wang, Chong and Wang, Jian and Zhang, Hong-Yu and Mayer, Wolfgang and Duan, Yucong and Wang, Ying and Feng, Zaiwen},
	month = dec,
	year = {2023},
	keywords = {Ontologies, Ontology, Semantics, Metadata, Standardization, Data Standardization, MDR, Syntactics, ISO Standards, Data Governance, Information resources, MFI},
	pages = {1106--1113},
}

@inproceedings{qin_effective_2023,
	title = {An {Effective} {Knowledge} {Mining} {Method} for {Compressor} {Fault} {Text} {Data} {Based} on {Large} {Language} {Model}},
	doi = {10.1109/CSAT61646.2023.00024},
	abstract = {The fault diagnosis method of compressors determines the reliability of the gas transmission pipeline station. Existing compressor fault diagnosis methods mostly relies on data-driven, which leads to a high application threshold from the mechanism. To address this issue, this paper introduces the knowledge graph into the compressor fault diagnosis for the first time and proposes a compressor fault text data knowledge mining method based on large language model. Firstly, the characteristics and principles of compressor faults are analyzed. Then, a text data knowledge mining model called CFRTE for compressors is constructed. Experimental results show that the Fl score of the CFRTE model can reach 0.98, meeting the requirements of compressor fault knowledge mining. Finally, combined with the results of knowledge mining and the graph database, a new system for the storage and indexing of the compressor fault knowledge graph is proposed. To further verify the role of the large language model in compressor fault knowledge mining, this paper conducts a comparative experiment of CFRTE models based on RNN encoder and BERT encoder. Experimental results show that compared with GRU, BiGRU, LSTM, and BiLSTM as the encoder layer, the Fl score of the CFRTE model with BERT as the encoder layer has increased by 26.78\%, 6.18\%, 21.89\%, and 5.49\% respectively. This work provides a systematic feasible scheme for introducing knowledge graphs into compressor fault diagnosis, which can be used for reference in the fault diagnosis of related equipment.},
	booktitle = {2023 {International} {Conference} on {Computer} {Science} and {Automation} {Technology} ({CSAT})},
	author = {Qin, Xiaodong and He, Yuxuan and Ma, Jie and Peng, Weiyuan and Zio, Enrico and Su, Huai},
	month = oct,
	year = {2023},
	keywords = {large language model, Ontologies, Knowledge graphs, knowledge graph, Fault diagnosis, Computational modeling, Pipelines, Systematics, knowledge mining, compressor, compressor station, Compressors},
	pages = {44--48},
}

@inproceedings{do_knowledge_2023,
	title = {A {Knowledge} {Representation} {Model} for {Designing} the {Knowledge} {Querying} {System} in {Programming} {Language} {C}/{C}++},
	doi = {10.1109/RIVF60135.2023.10471842},
	abstract = {Knowledge querying support systems need to assist users in querying the knowledge, relationships, or combination of multiple requirements. A proper knowledge representation model and the well-structured query language play important roles in the developing the knowledge querying systems. There are knowledge representation models and systems that support the querying or searching on the knowledge-based, but they have not supported well for various query requirements on the knowledge. Specially, the structured query sentences combine the multiple requirements. The paper will propose a knowledge representation model for the programming language C/C++ knowledge domain. Moreover, the paper will present structured query sentences that meet various requirements from users. Especially, the combination of multiple requirements based on the operators AND, OR, and NOT. Results of the research will be applied to design the knowledge querying system in the programming language C/C++ knowledge domain. The system is useful for first and second-year students in the field of technology information.},
	booktitle = {2023 {RIVF} {International} {Conference} on {Computing} and {Communication} {Technologies} ({RIVF})},
	author = {Do, Nhon V. and Mai, Thanh T.},
	month = dec,
	year = {2023},
	note = {ISSN: 2473-0130},
	keywords = {Knowledge representation, ontology, knowledge representation, Knowledge based systems, Computational modeling, programming language, Communications technology, Database languages, knowledge querying system, structured query sentences},
	pages = {366--371},
}

@inproceedings{ollier_ontological_2023,
	title = {An {Ontological} {Approach} for the {Dependability} {Analysis} of {Automated} {Systems}},
	doi = {10.1109/DSD60849.2023.00087},
	abstract = {This paper presents the Ontology Language for the Dependability of Automated Systems (OLDAS), a modeling language based on Unified Modeling Language (UML) that aims to support dependability assessment for Automated Systems (ASs), i.e., systems intended to perform a function with minimal or no human intervention. OLDAS extends the Unified Foundational Ontology (UFO) and embeds validation rules to prevent constraint violations in ASs analysis. Specifically, the paper presents how OLDAS can support different activities during the design of ASs, from the definition of the Operational Design Domain to scenario-based analysis. OLDAS is available as a plugin of the open-source Papyrus for Robotics framework.},
	booktitle = {2023 26th {Euromicro} {Conference} on {Digital} {System} {Design} ({DSD})},
	author = {Ollier, Guillaume and Adedjouma, Morayo and Gerasimou, Simos and Mraidha, Chokri},
	month = sep,
	year = {2023},
	note = {ISSN: 2771-2508},
	keywords = {Ontologies, Artificial Intelligence, Unified modeling language, Hazards, Redundancy, Autonomous Systems, Probabilistic logic, Analytical models, Runtime, Automated Driving Systems, ML-based Systems, ODD, Safety Engineering},
	pages = {593--601},
}

@inproceedings{sazzed_comprehending_2023,
	title = {Comprehending {Lexical} and {Affective} {Ontologies} in the {Demographically} {Diverse} {Spatial} {Social} {Media} {Discourse}},
	doi = {10.1109/ICMLA58977.2023.00339},
	abstract = {This study aims to comprehend linguistic and sociodemographic features, encompassing English language styles, conveyed sentiments, and lexical diversity within spatial online social media review data. To this end, we undertake a case study that scrutinizes reviews composed by two distinct and demographically diverse groups. Our analysis entails the extraction and examination of various statistical, grammatical, and sentimental features from these two groups. Subsequently, we leverage these features with machine learning (ML) classifiers to discern their potential in effectively differentiating between the groups. Our investigation unveils substantial disparities in certain linguistic attributes between the two groups. When integrated into ML classifiers, these attributes exhibit a marked efficacy in distinguishing the groups, yielding a macro F1 score of approximately 0.85. Furthermore, we conduct a comparative evaluation of these linguistic features with word n-gram-based lexical features in discerning demographically diverse review data. As expected, the n-gram lexical features, coupled with finetuned transformer-based models, show superior performance, attaining accuracies surpassing 95\% and macro F1 scores exceeding 0.96. Our meticulous analysis and comprehensive evaluations substantiate the efficacy of linguistic and sentimental features in effectively discerning demographically diverse review data. The findings of this study provide valuable guidelines for future research endeavors concerning the analysis of demographic patterns in textual content across various social media platforms.},
	booktitle = {2023 {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Sazzed, Salim},
	month = dec,
	year = {2023},
	note = {ISSN: 1946-0759},
	keywords = {Ontologies, Machine learning, Transformers, Feature extraction, Reviews, Linguistics, Social networking (online), n/a},
	pages = {2247--2252},
}

@inproceedings{albokae_hybrid_2023,
	title = {Hybrid {Method} for {ICD} {Prediction} {Using} {Word} {Embedding} and {Natural} {Language} {Processing}},
	doi = {10.1109/ACIT58888.2023.10453813},
	abstract = {The international classification of diseases is a standard in medical coding, and it is contains all information and description of diseases in heroical structure, and finding the International Classification of Diseases (ICD) code for diseases is important and essential thing in medical sector, the coding process takes a lot of time and money to find the correct and the exact code of the patient disease, researchers in artificial intelligence and in natural language processing and in machine learning make a huge efforts to build and develop automatic systems and algorithms for automatic ICD encoding, in this paper we propose a hybrid method for automatic ICD encoding from patient claims, the proposed method contains two main parts first for ICD chapter, ICD group classification, and the second one for find the most relevant ICD code based on patient claim diagnosis description, the first step was implemented by using natural language processing techniques, that it include stemming (Porter Stemmer was used for stemming), stop word removing, and the implementation of the second step was done by using PubMed BERT model for embedding for the ICD codes the embedding done based on the descriptions, and also the embedding done for the patient claim diagnosis description, we have tested the developed algorithm on medical dataset The results of our tests indicate that the proposed method is highly efficient, with a precision rate of 87\%.},
	booktitle = {2023 24th {International} {Arab} {Conference} on {Information} {Technology} ({ACIT})},
	author = {Albokae, Nazeer and AlKhtib, Bassel and Omar, Khaled},
	month = dec,
	year = {2023},
	note = {ISSN: 2831-4948},
	keywords = {Natural language processing, Encoding, Codes, Medical diagnostic imaging, Diseases, Classification algorithms, Prediction algorithms, automatic ICD coding, ICD ontology, PubMed BERT},
	pages = {1--5},
}

@inproceedings{pal_recommendation_2023,
	title = {Recommendation {System} for {Clinical} {Concept} {Mapping}},
	doi = {10.1109/ICCINS58907.2023.10450023},
	abstract = {In the past decade, the healthcare industry has shifted from paper-based document storage to Electronic Health Records (EHR), enabling quick, safe access to patient data. A key role is played by Semantic Interoperability (SI) which enables seamless data exchange between diverse care settings and clinical software. SI necessitates the linking of bio-medical data (aka. clinical events) with shared, standardized, and controlled vocabularies like SNOMED, LOINC, etc. However, the healthcare data across various client domains are filled with ambiguous textual representations of clinical events that may be present in the form of synonyms, acronyms, and abbreviations. To make interoperability work, Healthcare IT service providers must map related clinical events with the appropriate standard concepts, which requires additional time and resources. Natural Language Processing (NLP) plays a vital role in addressing the challenges of SI by learning effective representations of text words in the bio-medical domain thereby capturing their semantic meaning. Our method utilizes various pre-trained word embeddings trained on the bio-medical corpus like BioWordVec fastText and SapBERT that captures fine-grained semantic relationships. In this study, we have developed a recommendation system that provides recommendations of Top ‘N’ clinical events for mapping to a standard concept. The recommendation system showed good performance with a sensitivity of above 99 \% using both the pre-trained word embedding. Further, this product can be integrated into the mapping workflow to help make accurate automated suggestions that minimize manual effort.},
	booktitle = {2023 {International} {Conference} on {Computational} {Intelligence}, {Networks} and {Security} ({ICCINS})},
	author = {Pal, Suman and Gaur, Monica and Chaudhuri, Rupanjali and Benny Anto, Oshin and R, Kalaivanan and KV, Chetan and Pradhan, Pragnya},
	month = dec,
	year = {2023},
	keywords = {Interoperability, Natural language processing, Semantics, natural language processing, ontology, semantic interoperability, mapping, Recommender systems, word embeddings, Sensitivity, Manuals, Medical services, SapBERT},
	pages = {1--6},
}

@inproceedings{omar_measurement_2023,
	title = {Measurement of {ChatGPT} {Performance} in {Mapping} {Natural} {Language} {Speficaction} into an {Entity} {Relationship} {Diagram}},
	doi = {10.1109/ICSC58660.2023.10449869},
	abstract = {This paper explores the entity relationship diagram, a popular conceptual model used to depict entities, attributes, and relationships graphically. To help with this, we use ChatGPT, a sophisticated language model based on the GPT architecture, which can translate natural language text into an entity relationship diagram. The paper details the process of evaluating how well ChatGPT can perform compared to other state-of-the-art approaches for entity and relationship extraction. Our experimental findings demonstrate the strong ability of ChatGPT to translate natural language text into entity relationship diagrams, which has potential applications for knowledge graph building, data integration, and database schema design. Moreover, it can aid in automating the extraction and organization of information from unstructured text data, thereby simplifying the study of complex systems.},
	booktitle = {2023 {IEEE} 11th {International} {Conference} on {Systems} and {Control} ({ICSC})},
	author = {Omar, Mussa A.},
	month = dec,
	year = {2023},
	note = {ISSN: 2379-0067},
	keywords = {ChatGPT, natural language processing, Machine learning, Software engineering, Chatbots, Natural languages, Adaptation models, Companies, Task analysis, entity relationship diagram},
	pages = {530--535},
}

@inproceedings{wang_study_2023,
	title = {Study on the {Construction} and {Application} of {Combat} {Simulation} {Models} {Knowledge} {Graph}},
	doi = {10.1109/BigDIA60676.2023.10429736},
	abstract = {Exploiting the knowledge graph effectively enables the integration, the management, the illustration, the retrieval, the mining, and the reasoning of the knowledge. As for the combat simulation experiments, the combat simulation knowledge graph shall enhance the efficiency and quality and reduce the complexity and cost for developing the models, as well as support the agile construction of the simulation applications. Thus, this paper focused on the requirements analysis, creation idea, construction methods and typical applications of the combat simulation models knowledge graph, which provides some guiding significance for using knowledge graphs to boost the combat simulations.},
	booktitle = {2023 9th {International} {Conference} on {Big} {Data} and {Information} {Analytics} ({BigDIA})},
	author = {Wang, Yanfeng and Zhang, Liangji and Peng, Chao and Wang, Junhui and Zhu, Yingying},
	month = dec,
	year = {2023},
	note = {ISSN: 2771-6902},
	keywords = {Ontologies, Knowledge graphs, Deep learning, Semantic search, Knowledge graph construction, Research and development, Costs, Analytical models, Combat simulation experiments, Combat simulation models, Knowledge graph application},
	pages = {442--449},
}

@inproceedings{tang_domain_2023,
	title = {Domain {Knowledge} {Distillation} from {Large} {Language} {Model}: {An} {Empirical} {Study} in the {Autonomous} {Driving} {Domain}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186526218&doi=10.1109%2FITSC57777.2023.10422308&partnerID=40&md5=eb8968513a3c5a2be5a84e95c070417e},
	doi = {10.1109/ITSC57777.2023.10422308},
	abstract = {Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by “chatting” with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our findings and tools could inspire future research toward revolutionizing the engineering of knowledge-based systems across application domains.},
	booktitle = {2023 {IEEE} 26th {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Tang, Yun and Da Costa, Antonio A. Bruto and Zhang, Xizhe and Patrick, Irvine and Khastgir, Siddartha and Jennings, Paul},
	month = sep,
	year = {2023},
	note = {ISSN: 2153-0017},
	keywords = {large language model, Ontologies, Large language model, Ontology, Language model, Domain ontologies, Knowledge engineering, Autonomous vehicles, Intelligent transportation systems, Domain knowledge, Autonomous driving, Empirical studies, Computational linguistics, Cross-domain, Engineering knowledge, Chatbots, autonomous driving, Manuals, Runtime, domain ontology distillation, Domain Knowledge, Distillation, Domain ontology distillation, Knowledge experts},
	pages = {3893--3900},
	annote = {Cited by: 11; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{vijayakumar_revolutionizing_2023,
	title = {Revolutionizing {Staffing} and {Recruiting} with {Contextual} {Knowledge} {Graphs} and {QNLP}: {An} {End}-to-{End} {Quantum} {Training} {Paradigm}},
	doi = {10.1109/ICKG59574.2023.00011},
	abstract = {The staffing and recruiting industry is continuously evolving, and recent advancements in Knowledge Graphs (KG) and Quantum Natural Language Processing (QNLP) has garnered considerable attention. The integration of these state-of-the-art technologies is fueled by the necessity to improve language models' capacity to comprehend context and make precise decisions. This research paper presents a novel approach to revolutionize the staffing and recruiting industry by integrating Knowledge Graph (KG) and Quantum Natural Language Processing (QNLP) to formulate an end-to-end QNLP training pipeline. The proposed solution consists of three interdependent subsystems that work in unison to construct contextual KG and train language models. The Information Extraction subsystem extracts semantic relationships and connections between entities from large and complex recruitment data to construct domain specific contextual KG. The QNLP model training pipeline subsystem, which is fed with domain-rich KG data, runs on Quantum Circuits, accelerates the training process by effectively incorporating high-dimensional features to the deep layers of language models. Finally, the Information Retrieval subsystem is based on semantic data taxonomy, retrieving contextual data from the KG for the trained language models to be implemented on various distinctive use cases in the staffing and recruiting industry. This solution provides a faster and more contextual approach to analyze recruitment data, empowering recruiters to concentrate on strategic tasks such as candidate engagement and client relationship building, ultimately leading to better business decision-making capabilities.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Knowledge} {Graph} ({ICKG})},
	author = {Vijayakumar, Senthilkumar and Louis, Filious},
	month = dec,
	year = {2023},
	keywords = {Knowledge graphs, Natural language processing, Context modeling, Data models, Training, Artificial Intelligence (AI), Knowledge Graph (KG), Industries, Large Language Models (LLM), Contextual Information Extraction \& Retrieval Systems, Integrated circuit modeling, Quantum Natural Language Processing (QNLP)},
	pages = {45--51},
}

@inproceedings{liem_gradtod_2023,
	title = {{GradTOD} - {A} {Unified} {Dialogue} {State} {Tracking} {Model} for {Task}-{Oriented} and {Open} {Domain} {Dialogues}},
	doi = {10.1109/CICN59264.2023.10402219},
	abstract = {The task-oriented dialogue domain system requires classifying intent and replying to a specific goal domain. In the sub-module of Task-oriented, the Dialogue State Tracker (DST) is well-known as a variety processing tracker. However, existing DST models often specialize in only task-oriented domains (ToD), leading to limited performance when applied to scenarios. In this paper, we propose GradTOD, a unified DST model that predicts both two task types, task-oriented dialogue (TOD) and open-domain dialogue (ODD). Our model leverages the recent advances in prompt engineering and conditional generation to perform zero-shot learning. After experiments, GradTOD has achieved an 88.6\% and 82.5\% score on Joint Goal Accuracy metrics when evaluating the Scheme-Guided Dialogue (SGD) and FusedChat test sets correspondingly, demonstrating the adaption ability for multi-domains.},
	booktitle = {2023 {IEEE} 15th {International} {Conference} on {Computational} {Intelligence} and {Communication} {Networks} ({CICN})},
	author = {Liem, Truc Nguyen and Cao Hoai, Sinh Nguyen and Quoc, Hung Nguyen and Van, Tien Nguyen and Pham Trung, Hieu and Quoc, Trung Nguyen and Hoang, Vinh Truong},
	month = dec,
	year = {2023},
	note = {ISSN: 2472-7555},
	keywords = {component, Computational modeling, Zero-shot learning, Measurement, Computational intelligence, Adaptation models, Predictive models, Task analysis, formatting, insert, style, styling},
	pages = {711--719},
}

@inproceedings{kosten_spider4sparql_2023,
	title = {{Spider4SPARQL}: {A} {Complex} {Benchmark} for {Evaluating} {Knowledge} {Graph} {Question} {Answering} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184975243&doi=10.1109%2FBigData59044.2023.10386182&partnerID=40&md5=e9f9603deb6910323726ccf90e5ce308},
	doi = {10.1109/BigData59044.2023.10386182},
	abstract = {With the recent spike in the number and availability of Large Language Models (LLMs), it has become increasingly important to provide large and realistic benchmarks for evaluating Knowledge Graph Question Answering (KGQA) systems. So far the majority of benchmarks rely on pattern-based SPARQL query generation approaches. The subsequent natural language (NL) question generation is conducted through crowdsourcing or other automated methods, such as rule-based paraphrasing or NL question templates. Although some of these datasets are of considerable size, their pitfall lies in their pattern-based generation approaches, which do not always generalize well to the vague and linguistically diverse questions asked by humans in real-world contexts. In this paper, we introduce Spider4SPARQL -a new SPARQL benchmark dataset featuring 9,693 previously existing manually generated NL questions and 4,721 unique, novel, and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs and ontologies, which cover 138 different domains. Our complex benchmark enables novel ways of evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the system with state-of-the-art KGQA systems as well as LLMs, which achieve only up to 45\% execution accuracy, demonstrating that Spider4SPARQL is a challenging benchmark for future research.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Kosten, Catherine and Cudré-Mauroux, Philippe and Stockinger, Kurt},
	month = dec,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Language model, Crowdsourcing, Benchmarking, Performance Evaluation, Computational linguistics, Question Answering, Measurement, Language Models, Automated methods, Natural languages, Question answering (information retrieval), Benchmark testing, Benchmark for Question Answering over Knowledge Graphs, Natural language processing systems, Question answering systems, Natural language questions, Query generation, Benchmark for question answering over knowledge graph, Complex benchmark, Performances evaluation},
	pages = {5272--5281},
	annote = {Cited by: 10; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access},
}

@inproceedings{liu_constructing_2023,
	title = {Constructing {Knowledge} {Graph} from {Cyber} {Threat} {Intelligence} {Using} {Large} {Language} {Model}},
	doi = {10.1109/BigData59044.2023.10386611},
	abstract = {Cyber Threat Intelligence (CTI) reports are valuable resources in various applications but manually extracting information from them is time-consuming. Existing approaches for automating extraction require specialized models trained on a substantial corpus. In this paper, we present an efficient methodology for constructing knowledge graphs from CTI by leveraging the Large Language Model (LLM), using ChatGPT for instance. Our approach automatically extracts attack-related entities and their relationships, organizing them within a CTI knowledge graph. We evaluate our approach on 13 CTIs, demonstrating better performance compared to AttacKG and REBEL while requiring less manual intervention and computational resources. This proves the feasibility and suitability of our method in low-resource scenarios, specifically within the domain of cyber threat intelligence.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Liu, Jiehui and Zhan, Jieyu},
	month = dec,
	year = {2023},
	keywords = {ChatGPT, large language model, Ontologies, Knowledge graphs, Information retrieval, knowledge graph, Cognition, threat intelligence, Data models, Computational modeling, Manuals},
	pages = {516--521},
}

@inproceedings{sharma_query_2023,
	title = {Query {Expansion} {Using} {Word} {Embedding}, {Ontology} and {Natural} {Language} {Processing}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184797811&doi=10.1109%2FSmartTechCon57526.2023.10391425&partnerID=40&md5=6c6f2d2806dbc26dc82d48bbb2557cf0},
	doi = {10.1109/SmartTechCon57526.2023.10391425},
	abstract = {Query Expansion (QE) is the art of reconstructing specific queries to expand validation presentation, especially in the data mining process in a requirement understanding environment. Expanding requirements is one of the techniques involved in finding information. In the search engine environment, the query extension includes the evaluation of the value of the construction and the extension of search queries to match new documents. In natural language processing (NLP), word embedding is a term used in textbook parsing, usually as a real-valued vector that encodes the meaning of adjacent words in the vector. It is assumed that the space will be analogous in meaning. Word embedding can be achieved using a set of language models and point literacy methods where vocabulary words or expressions are mapped to vectors of real numbers. For query expansion, one method used is natural language processing through word embedding. Other approaches are ontology, machine learning, and deep learning for automatic query expansion. This paper proposes a hybrid approach for query expansion by combining NLP and ontology through word embedding.},
	booktitle = {2023 {Second} {International} {Conference} {On} {Smart} {Technologies} {For} {Smart} {Nation} ({SmartTechCon})},
	author = {Sharma, Hemendra Shanker and Sharma, Ashish},
	month = aug,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontologies, Ontology, Natural language processing, natural language processing, Deep learning, Search engines, information retrieval, Embeddings, Vocabulary, Data mining, Query processing, Word embedding, word embedding, Query expansion, Language processing, Natural languages, Art, Ontology's, Natural language processing systems, Ontology language, Data mining process, Search queries},
	pages = {410--414},
	annote = {Cited by: 2},
}

@inproceedings{guo_isoform_2023,
	title = {Isoform {Function} {Prediction} {Based} on {Heterogeneous} {Graph} {Attention} {Networks}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184931633&doi=10.1109%2FBIBM58861.2023.10386048&partnerID=40&md5=fecc6275159961520398218e9f2701bb},
	doi = {10.1109/BIBM58861.2023.10386048},
	abstract = {Isoforms refer to different mRNA molecules transcribed from the same gene, which can be translated into proteins with varying structures and functions. Predicting the functions of isoforms is an essential topic in bioinformatics as it can provide valuable insights into the intricate mechanisms of gene regulation and biological processes. Conventionally, gene function labels are standardized in Gene Ontology (GO) terms. However, traditional methods for predicting isoform function are largely limited by the absence of isoform-specific labels, sparse annotations, and the vast number of GO terms. To address these issues, we propose HANIso, a deep learning-based method for isoform function prediction. HANIso leverages a pretrained protein language model to extract features from protein sequences. It also integrates heterogeneous information, such as isoform sequence features, GO annotations, and isoform interaction data, using a Heterogeneous Graph Attention Network (HAN). This allows the model to learn the importance of different sources of information and their semantic relationships through the attention mechanism. Our method can predict function labels at both the gene level and isoform level. We conduct experiments on two species datasets, and the results demonstrate that our method outperforms existing methods on both AUROC and AUPRC. HANIso has the potential to overcome the limitations of traditional methods and provide a more accurate and comprehensive understanding of isoform function.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Guo, Kuo and Li, Yifan and Chen, Hao and Shen, Hong-Bin and Yang, Yang},
	month = dec,
	year = {2023},
	note = {ISSN: 2156-1133},
	keywords = {Ontologies, Language model, Semantics, Bioinformatics, Deep learning, Gene Ontology, Annotations, Gene ontology, Feature extraction, Forecasting, Protein language model, Computational linguistics, Proteins, protein language model, Function prediction, alternative splicing, Alternative splicing, Genes, gene ontology, Biological system modeling, Predictive models, heterogeneous graph attention network, isoform function prediction, Gene ontology terms, Heterogeneous graph, Heterogeneous graph attention network, Isoform function prediction, Isoforms},
	pages = {522--527},
	annote = {Cited by: 0},
}

@inproceedings{wang_mulaxialgo_2023,
	title = {{MulAxialGO}: {Multi}-{Modal} {Feature}-{Enhanced} {Deep} {Learning} {Model} for {Protein} {Function} {Prediction}},
	doi = {10.1109/BIBM58861.2023.10385754},
	abstract = {Predicting protein function from sequences through machine learning can improve the understanding of novel proteins and biological mechanisms. Existing methods mainly rely on one-dimensional convolution or natural language processing (NLP) techniques to extract features from sequences, but they suffer from limited predictive performance. To address this challenge, we propose MulAxialGO, a new method that leverages multi-modal feature fusion to improve prediction accuracy. MulAxialGO integrates the prior features of a large-scale pre-trained protein language model and the posterior features of dynamic embedding coding and sequence homology. In addition, MulAxialGO employs a comprehensive image feature encoder to extract features from sequences, providing a novel perspective for protein function prediction. MulAxialGO is tested on two benchmark datasets and achieves state-of-the-art results. On the 2016 dataset, MulAxialGO significantly outperforms DeepGOPlus, improving molecular function by 4.5 points, biological process by 2.4 points and cellular component by 1.6 points for the AUPR metric. Similarly, on the NetGO dataset, MulAxialGO outperforms the state-of-the-art NetGO2.0, improving Fmax by 1.1 points for biological process and 2.3 points for cellular component.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Wang, Xun and Qu, Peng and Meng, Xiangyu and Yang, Qing and Qiao, Lian and Zhang, Chaogang and Xie, Xianjin},
	month = dec,
	year = {2023},
	note = {ISSN: 2156-1133},
	keywords = {Bioinformatics, Deep learning, Attention mechanism, Protein function prediction, Measurement, Proteins, Biological processes, Sequence analysis, Predictive models, Protein engineering, Convolution},
	pages = {132--137},
}

@inproceedings{shuai_protein_2023,
	title = {Protein function prediction using graph neural network with multi-type biological knowledge},
	doi = {10.1109/BIBM58861.2023.10385760},
	abstract = {Proteins play crucial roles in diverse biological functions, and accurately annotating their functions is essential for understanding cellular mechanisms and developing therapies for complex diseases. Computational methods have been proposed as alternatives to laborious experimental approaches. However, existing network-based methods focus on the protein-protein interaction (PPI) networks, while the proteins without interactions are ignored. To address this limitation, we propose a novel deep learning framework for protein function prediction, named PFP-GMB, which incorporates multi-type biological knowledge to consider the proteins not present in the PPI networks. PFP-GMB leverages a pre-trained protein language model to extract sequence representations. Moreover, PPIs and orthology relationships are used to generate functional related features via graph neural networks and attention mechanisms. Finally, these multi-type features are fused for protein function prediction. Compared to eight state-of-the-art methods, PFP-GMB outperforms all of them in terms of F-max and AUPR. The ablation studies further confirm the relevance and significance of the multi-type biological knowledge incorporated into PFP-GMB for protein function prediction.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Shuai, Yunyan and Wang, Wenkang and Li, Yiming and Zeng, Min and Li, Min},
	month = dec,
	year = {2023},
	note = {ISSN: 2156-1133},
	keywords = {Knowledge engineering, Feature extraction, Graph neural networks, Proteins, PPI network, protein function, graph neural network, protein sequence, Protein engineering, Medical treatment, Diseases, orthology network},
	pages = {30--35},
}

@inproceedings{fu_multimodal_2023,
	title = {Multimodal reasoning for nutrition and human health via knowledge graph embedding},
	doi = {10.1109/BIBM58861.2023.10385745},
	abstract = {The established links between nutrition and human health are widely acknowledged. Dietary nutrients play a crucial role in regulating gut microbial communities, influencing various human diseases. With a growing number of related studies, there’s a need to systematically organize these associations for coherent knowledge reasoning. However, due to the diverse and extensive nature of the knowledge landscape, significant challenges persist. To address this, we propose an approach using multimodal data and knowledge embeddings for effective knowledge reasoning in nutrition and human health. We create a comprehensive knowledge graph, KG4NH, covering dietary nutrition, gut microbiota, and human diseases. To ensure efficient knowledge representation, we employ knowledge embedding techniques to develop modality-specific encoders for structure, category, and description. Additionally, we introduce a mul-timodal fusion method to capture shared information across modalities. Our experimental results demonstrate the superiority of our approach over other state-of-the-art methods.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Fu, Chengcheng and Yao, Yanan and Wu, Jieyu and Zhao, Weizhong and He, Tingting and Jiang, Xingpeng},
	month = dec,
	year = {2023},
	note = {ISSN: 2156-1133},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Nutrition, Bioinformatics, Knowledge reasoning, Feature extraction, Cognition, Diseases, Human health, Multimodal embedding},
	pages = {1901--1904},
}

@inproceedings{jesus_feasibility_2023,
	title = {Feasibility of {Structured}, {Machine}-{Readable} {Privacy} {Notices}},
	doi = {10.1109/BESC59560.2023.10386763},
	abstract = {This paper offers a novel approach to the long standing problem of the interface of humans and online privacy notices. As literature and practice, and even art, for more than a decade have identified, privacy notices are nearly always ignored and "accepted" with little thought, mostly because it is not practical nor user-friendly to depend on reading a long text simply to access, e.g., a news website. Nevertheless, privacy notices are a central element, often mandated by law.We approach the problem by (partially) relieving the human from the task of inspecting such documents. Because they are documents written in natural language, often legal language, we assess the feasibility of representing privacy notices in a machine-readable format. Should this be feasible, automated processing of notices that still respect individual choices could be enabled. To this end, we manually inspected privacy notices under EU/UK's GDPR from common websites, and designed a JSON schema that captures their structure.},
	booktitle = {2023 10th {International} {Conference} on {Behavioural} and {Social} {Computing} ({BESC})},
	author = {Jesus, Vitor and Patel, Asma and Kumar, Deepak},
	month = oct,
	year = {2023},
	keywords = {Law, Privacy, Social computing, Natural languages, Task analysis, Art},
	pages = {1--8},
}

@inproceedings{taye_ontology_2023,
	title = {An {Ontology} {Learning} {Framework} for unstructured {Arabic} {Text}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808063&doi=10.1109%2FISAS60782.2023.10391548&partnerID=40&md5=383c7dfc97308ff44961706ab31047f2},
	doi = {10.1109/ISAS60782.2023.10391548},
	abstract = {Ontologies are widely regarded as valuable sources of semantics and interoperability in all artificially intelligent systems. Due to the rapid growth of unstructured data on the web, studying how to automatically get ontology from unstructured text is important. Therefore, ontology learning (OL) is an important process in the business world. It involves finding and extracting concepts from the text so that these concepts can be used for things such as information retrieval. Unfortunately, learning ontology is not easy for some reasons, and there has not been much research on how to automatically learn a domain-specific ontology from data.Ontology Studying Arabic text is not as developed as learning Latin text. There is almost no automated support for using Arabic literary knowledge in semantically enabled systems. Machine learning (ML) has proven beneficial in numerous fields, including text mining. By employing neural language models such as AraBERT, it is possible to obtain word embeddings as distributed word representations from textual input using machine learning. However, the application of machine learning to aid the development of Arabic ontology is largely unexplored. This research examines the performance of AraBERT for ontology learning tasks in Arabic. Early performance results as an application of Arabic ontology learning are promising. In this research, we provide a method for populating an existing ontology with instance information extracted from the input natural language text. This prototype has achieved an information extraction accuracy of 91\%.},
	booktitle = {2023 7th {International} {Symposium} on {Innovative} {Approaches} in {Smart} {Technologies} ({ISAS})},
	author = {Taye, Mohammad Mustafa and Abulail, Rawan and Al-Oudat, Mohammad},
	month = nov,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontologies, Interoperability, Ontology, Natural language processing, Ontology learning, Semantic Web, Semantics, Text mining, Semantic representation, Machine learning, Information retrieval, Arabic ontology, Data mining, Intelligent systems, Arabic Ontology, Language processing, Natural languages, Prototypes, Learning systems, Ontology Learning (OL), Natural language Processing (NLP), semantic representation, Ontology's, Natural language processing systems, Learning algorithms, Semantic-Web},
	pages = {1--12},
	annote = {Cited by: 5},
}

@inproceedings{leventi-peetz_biotechnology_2023,
	title = {Biotechnology {Machine} {Learning} {Techniques} for {Natural} {Language} {Processing}},
	doi = {10.1109/TransAI60598.2023.00029},
	abstract = {The possibility to transfer machine learning techniques from biotechnology to natural language processing models to increase training efficiency will be generally discussed. The motivation and reasoning behind the idea will be briefly outlined.},
	booktitle = {2023 {Fifth} {International} {Conference} on {Transdisciplinary} {AI} ({TransAI})},
	author = {Leventi-Peetz, Anastasia-Maria and Raber, Frederic and Rüll, Annika and Weber, Kai},
	month = sep,
	year = {2023},
	keywords = {Natural language processing, natural language processing, Machine learning, Sustainable development, Cognition, Training, Biotechnology, protein function, gene ontology, Biological system modeling, model sustainability},
	pages = {118--119},
}

@inproceedings{procko_gpt-4_2023,
	title = {{GPT}-4: {A} {Stochastic} {Parrot} or {Ontological} {Craftsman}? {Discovering} {Implicit} {Knowledge} {Structures} in {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808375&doi=10.1109%2FTransAI60598.2023.00043&partnerID=40&md5=1508a82f1a0c8b3376895ff708ffdb11},
	doi = {10.1109/TransAI60598.2023.00043},
	abstract = {Ontologies are representational artifacts that purport to accurately portray the aspect of reality under the purview of the ontologists laboring upon them. Ontologies exist in a spectrum of formality, from lexical thesauri to knowledge graphs, to collections of statements of first-order logic. The recent proliferation of Large Language Models (LLMs) has brought to bear interactive “knowledge bases” with general awareness of most things. As ontologists create ontologies from their understanding of reality; and as LLMs, presumably, possess some “understanding” of reality, embedded in their vector matrices corresponding to lexical terms from massive quantities of learned texts, a question is posed: what form of ontology can an LLM create when prompted about some novel facet of reality, without explicitly asking it for an ontology? I.e., will an LLM categorize things into bins, or a subsumption hierarchy, or perhaps something else? LLMs, as they are understood, respond when prompted with the most likely response, because they are predictors of next tokens, i.e., they are stochastic parrots. In any case, it is posited that, if prompted without any explicit request for an ontology, an LLM can produce an ontology of novel form, effectively granting insight into the “understanding” an LLM has of the world, as all humans possess an understanding of the world that ontologies are based upon. This paper explores the use of the flagship LLM, GPT-4, in forming an ontology of a novel domain.},
	booktitle = {2023 {Fifth} {International} {Conference} on {Transdisciplinary} {AI} ({TransAI})},
	author = {Procko, Tyler Thomas and Elvira, Timothy and Ochoa, Omar},
	month = sep,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Large language model, Ontology, Language model, GPT, large language models, ontology, OWL, taxonomy, Taxonomy, Formal logic, Organizations, Computational linguistics, Visualization, Supervised learning, First order logic, Natural languages, Stochastic processes, Ontology's, Birds, Implicit knowledge, Stochastic systems, Knowledge structures, Spectra's, Stochastic models, Stochastics},
	pages = {147--154},
	annote = {Cited by: 1},
}

@inproceedings{procko_automatic_2023,
	title = {Automatic {Generation} of {BFO}-{Compliant} {Aristotelian} {Definitions} in {OWL} {Ontologies} with {GPT}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184805224&doi=10.1109%2FTransAI60598.2023.00042&partnerID=40&md5=7f05736a0f125cf5bfd0cf0230dd81c3},
	doi = {10.1109/TransAI60598.2023.00042},
	abstract = {Ontologies are representational artifacts that purport to accurately describe some aspect of reality, including the entities and the relations that hold between them. In computer science, ontologies are software artifacts containing the schematic structure for machine-readable knowledge, typically formed as a graph of subject-predicate-object triples, constrained through Description Logics. These resources and their relations are self-defining, i.e., some resource may be defined by considering all its stated relations. Resources are often attended with natural language annotations, that humans may read and interpret, such as labels and definitions. Many long-standing ontologies have useless lexical definitions that define resources cyclically, e.g., a FOAF: Person is simply defined as “A person”. In Aristotelian terms, the definition of a thing should be reducible, by using terms simpler than itself, such that every definition can be unpacked up to the most general thing, which can only be defined by stating examples and use cases. This paper presents an innovative technique that leverages the Generative Pre-trained Transformer (GPT) large language model, GPT -4, for automatically generating Aristotelian definition annotations for OWL classes that engenders compliance with the Basic Formal Ontology standard.},
	booktitle = {2023 {Fifth} {International} {Conference} on {Transdisciplinary} {AI} ({TransAI})},
	author = {Procko, Tyler Thomas and Ochoa, Omar and Elvira, Timothy},
	month = sep,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontology, GPT, Linked data, ontology, OWL, Annotations, Description logic, Transformers, Software, Epistemology, Generative pre-trained transformer, Linked Data, BFO, Regulatory compliance, epistemology, Automatic Generation, Natural languages, Maintenance engineering, Ontology's, Birds, Linked datum, OWL ontologies, Data description, Schematic structures, Software artefacts},
	pages = {141--146},
	annote = {Cited by: 1},
}

@inproceedings{reynolds_ontology_2023,
	title = {An {Ontology} and {Management} {System} for {Learning} {Outcomes} and {Student} {Mastery}},
	doi = {10.1109/FIE58773.2023.10343171},
	abstract = {Universities, faculty, and students use Learning Outcomes (LO) to create a shared understanding of the content provided in an individual course, known as Outcome-Based Education (OBE). One area of interest in OBE is evaluating whether the instructor and individual student performance have met the LO, which is integral to ensuring all invested parties are on the same page about class content and student performance. This work proposes a system for the management and evaluation of LO. Primarily, this work defines an ontology to support the management and evaluation of LO via Knowledge Graphs (KG). The KG links individual LO with individual assessment items. Two state-of-the-art Natural Language Processing models, BERT and ChatGPT, are evaluated in respect to their effectiveness in automating this linking. This data allows the educational professional to reflect on how well their assessments match the course's LO. The second part of this system harnesses student data to measure performance in relation to LO. In this Work-in-Progress paper, the system is prototyped and tested on the midterm results of a course in the Software Engineering curriculum. Student performance is documented in relation to each assessment question on the exams to measure student mastery of course material. Through this approach, courses can be evaluated and improved to deliver better quality education to all students. This includes improvements at the course level and possibilities for early intervention to ensure student success. This paper details the development of this system and through its implementation shows how it benefits engineering educators and their students.},
	booktitle = {2023 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	author = {Reynolds, Sarah and Pate, William C. and Ochoa, Omar},
	month = oct,
	year = {2023},
	note = {ISSN: 2377-634X},
	keywords = {Ontologies, Knowledge graphs, BERT, ontology, Knowledge engineering, assessment, knowledge graph, Taxonomy, Chatbots, Market research, Software measurement, Bloom's taxonomy, Learning outcomes},
	pages = {1--5},
}

@inproceedings{guizzardi_ontological_2023,
	title = {An {Ontological} {View} on {Types}},
	doi = {10.1109/MODELS-C59198.2023.00103},
	abstract = {Types are fundamental for modeling, being an essential construct in all major modeling languages. These include traditional conceptual modeling languages - such as Entity-Relationship models, UML class diagrams, or Object-Role-Modeling (ORM) specifications, and knowledge representation languages alike (e.g., the Web Ontology Language - OWL).},
	booktitle = {2023 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Guizzardi, Giancarlo},
	month = oct,
	year = {2023},
	keywords = {Knowledge representation, OWL, Unified modeling language, Model driven engineering, Multi-Level Modeling, Ontological Foundations for Modeling, Types and Taxonomic Structures},
	pages = {634--634},
}

@inproceedings{henzgen_model-driven_2023,
	title = {Model-{Driven} {Approach} for {Automatic} {Model} {Information} {Aggregation} in {Structured} {Documents}},
	doi = {10.1109/MODELS-C59198.2023.00072},
	abstract = {While models are widely used in software development projects originating from industry and academic research, their documentation can be a time-intensive process. This paper focuses on providing a Proof of Concept for the automatic aggregation of various model data in two different document types conforming to ISO/IEC/IEEE 42010 architecture descriptions or instructional information documents according to ISO/IEC/IEEE 26514. Therefore, this work leverages a model-driven mapping approach of model information to the required document structure, dynamic templating algorithms to transform model data into text and a prototypical implementation that executes the defined mapping and transformation logic in practice. The generation results show that most of the documentation standard requirements can be fulfilled automatically and therefore, reduce the manual processing effort while enhancing consistency.},
	booktitle = {2023 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Henzgen, Arne and Strey, Lukas},
	month = oct,
	year = {2023},
	keywords = {UML, BPMN, Data models, Model-Driven Engineering, GSN, Documentation, Transforms, Computer architecture, Industries, ISO Standards, Architecture Description, Heuristic algorithms, Instructional Information, Model-to-Document},
	pages = {403--413},
}

@inproceedings{chen_prompting_2023,
	title = {Prompting or {Fine}-tuning? {A} {Comparative} {Study} of {Large} {Language} {Models} for {Taxonomy} {Construction}},
	doi = {10.1109/MODELS-C59198.2023.00097},
	abstract = {Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.},
	booktitle = {2023 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Chen, Boqi and Yi, Fandi and Varró, Dániel},
	month = oct,
	year = {2023},
	keywords = {Ontologies, large language models, Taxonomy, Software, Training, Computer science, few-shot learning, Computational modeling, fine-tuning, Systematics, domain-specific constraints, taxonomy construction},
	pages = {588--596},
}

@inproceedings{majumder_domain-driven_2023,
	title = {A {Domain}-{Driven} {Model} {Generation} {Framework} for {Cyber}-{Physical} {Production} {Systems}},
	doi = {10.1109/MODELS-C59198.2023.00044},
	abstract = {The growing influence of Information Technologies in the manufacturing domain has led to the fourth industrial revolution (Industry 4.0). Cyber-Physical Production System (CPPS) is one of the fundamental concepts of Industry 4.0 that aims to develop an intelligent manufacturing environment by leveraging concepts like the Internet of Things (IoT), cloud computing, virtualization, and Artificial Intelligence (AI). However, the challenges originating from the technological heterogeneity in the manufacturing domain remain primary obstacles towards realising a fully automated CPPS. Among them, semantic heterogeneity in manufacturing information is the most crucial which can be attributed to technology and vendor-specific information modelling mechanisms. A CPPS requires seamless machine-to-machine communication which could be hindered due to the non-interoperability among machine data on a semantic level. Therefore, the primary focus of this thesis work is to understand the semantic interoperability challenges of CPPS and propose solutions to address those challenges. The proposed solution revolves around the development of semantic domain models using the modelling philosophies of Domain-Driven Design (DDD).},
	booktitle = {2023 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Majumder, Mainak},
	month = oct,
	year = {2023},
	keywords = {Semantics, Industry 4.0, Internet of Things, Information Model, Production systems, CPPS, Domain-Driven Design (DDD), Model driven engineering, Philosophical considerations, Fourth Industrial Revolution, Machine-to-machine communications},
	pages = {172--178},
}

@inproceedings{dhaouadi_towards_2023,
	title = {Towards {Understanding} and {Analyzing} {Rationale} in {Commit} {Messages} {Using} a {Knowledge} {Graph} {Approach}},
	doi = {10.1109/MODELS-C59198.2023.00101},
	abstract = {Extracting rationale information from commit messages allows developers to better understand a system and its past development. Here we present our ongoing work on the Kantara end-to-end rationale reconstruction pipeline to a) structure rationale information in an ontologically-based knowledge graph, b) extract and classify this information from commits, and c) produce analysis reports and visualizations for developers. We also present our work on creating a labelled dataset for our running example of the Out-of-Memory component of the Linux kernel. This dataset is used as ground truth for our evaluation of NLP classification techniques which show promising results, especially the multi-classification technique XGBoost.},
	booktitle = {2023 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Dhaouadi, Mouna and Oakes, Bentley James and Famelis, Michalis},
	month = oct,
	year = {2023},
	keywords = {Knowledge graphs, ontology, Natural Language Processing, Data mining, Visualization, dataset, Pipelines, Model driven engineering, Analytical models, Linux, openCAESAR, rationale extraction, rationale structuring},
	pages = {622--630},
}

@inproceedings{lange_modeling_2023,
	title = {Modeling in {LML} with {DOCL}: {A} {Contribution} to the {MULTI} {Warehouse} {Challenge}},
	doi = {10.1109/MODELS-C59198.2023.00106},
	abstract = {This paper responds to the “Warehouse” challenge that was posed to the community of multi-level modeling researchers for the MULTI 2023 workshop. Given the many flavors of multi-level modeling approaches, the purpose of this and other similar challenges defined by the MULTI workshop community is to clarify the trade-offs entailed by the design choices underpinning the different approaches. This challenge revolves around product copies, product specifications, and product type specifications and how to guarantee certain properties at the product instance level. After first providing an overview of our modeling approach, and summarising the requirements laid out in the challenge, we present our solution using the LML and DOCL languages. We then discuss how well the solution fulfills the requirements laid out in the challenge.},
	booktitle = {2023 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Lange, Arne and Atkinson, Colin},
	month = oct,
	year = {2023},
	keywords = {Semantics, Safety, Multi-level modeling, Complexity theory, Syntactics, Conferences, Model driven engineering, Currencies, DOCL, LML},
	pages = {649--658},
}

@inproceedings{elaasar_opencaesar_2023,
	title = {{openCAESAR}: {Balancing} {Agility} and {Rigor} in {Model}-{Based} {Systems} {Engineering}},
	doi = {10.1109/MODELS-C59198.2023.00051},
	abstract = {Model-Based System Engineering (MBSE) employs models and formal languages to support development of complex (systems-of-) systems. NASA Jet Propulsion Laboratory (JPL) sees MBSE as a key approach to managing the complexity of system development. However, balancing agility and rigor in MBSE has been reported as a challenging task not yet addressed by modeling tools and frameworks. This is because existing MBSE approaches may enable agility but compromise rigor, or enhance rigor but impede agility. We discuss the challenges of balancing agility and rigor in MBSE across seven systems engineering architectural functions defined by the JPL Integrated Model-Centric Engineering (IMCE) initiative. We demonstrate how openCAESAR, an open-source MBSE methodology and framework created at JPL, can strike a balance between agility and rigor through a case study of the Kepler16b project and discussion of lessons learned from past projects.},
	booktitle = {2023 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Elaasar, Maged and Rouquette, Nicolas and Wagner, David and Oakes, Bentley James and Hamou-Lhadj, Abdelwahab and Hamdaqa, Mohammad},
	month = oct,
	year = {2023},
	keywords = {Modeling, Formal languages, Systems Engineering, Model-Based Systems Engineering, Complexity theory, Propulsion, Model driven engineering, Task analysis, openCAESAR, NASA, OML, Ontology-based Modeling},
	pages = {221--230},
}

@article{fatemi_evaluating_2023,
	title = {Evaluating the {Effectiveness} of {GPT} {Large} {Language} {Model} for {News} {Classification} in the {IPTC} {News} {Ontology}},
	volume = {11},
	issn = {2169-3536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181543202&doi=10.1109%2FACCESS.2023.3345414&partnerID=40&md5=013b0eaa020e69a49f3f6724cea13afc},
	doi = {10.1109/ACCESS.2023.3345414},
	abstract = {News classification plays a vital role in newsrooms, as it involves the time-consuming task of categorizing news articles and requires domain knowledge. Effective news classification is essential for categorizing and organizing a constant flow of information, serving as the foundation for subsequent tasks, such as news aggregation, monitoring, filtering, and organization. The automation of this process can significantly benefit newsrooms by saving time and resources. In this study, we explore the potential of the GPT large language model in a zero-shot setting for multi-class classification of news articles within the widely accepted International Press Telecommunications Council (IPTC) news ontology. The IPTC news ontology provides a structured framework for categorizing news, facilitating the efficient organization and retrieval of news content. By investigating the effectiveness of the GPT language model in this classification task, we aimed to understand its capabilities and potential applications in the news domain. This study was conducted as part of our ongoing research in the field of automated journalism.},
	journal = {IEEE Access},
	author = {Fatemi, Bahareh and Rabbi, Fazle and Opdahl, Andreas L.},
	year = {2023},
	note = {Type: Article},
	keywords = {Ontologies, Large language model, Ontology, Language model, large language models, Annotations, Annotation, Computational linguistics, Zero-shot learning, Journalism, journalism, Adaptation models, Task analysis, IPTC media topics, news classification, Sports, Support vector machines, Tag clouds, Ontology's, Classification (of information), Job analysis, News classification, Information filtering, International press telecommunication council medium topic, International press telecommunications councils, Support vectors machine},
	pages = {145386--145394},
	annote = {Cited by: 9; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{palagin_digital_2023,
	title = {Digital {Health} {Systems}: {Ontology}-{Based} {Universal} {Dialog} {Service} for {Hybrid} {E}-{Rehabilitation} {Activities} {Support}},
	volume = {1},
	doi = {10.1109/IDAACS58523.2023.10348639},
	abstract = {The medical rehabilitation system in Ukraine encountered a set of crucial challenges that demanded immediate attention and action. The primary objective revolves around rehabilitating patients with Combat stress reaction. Ukraine possesses a network of medical and preventive institutions that cater to the psycho-physiological rehabilitation needs of military personnel. These institutions employ contemporary rehabilitation technologies. Nonetheless, not all individuals have access to long-term rehabilitation within these centers. Hence, the integration of telerehabilitation technology becomes crucial for patients dealing with post-traumatic stress disorder and related conditions. This integration, combined with objective monitoring of the functional state, holds significant importance. Remote patient-centered rehabilitation emerges as one of the most effective approaches within the realm of medical rehabilitation assistance. Moreover, there is a need for efficient methods that support the “Physical therapist - Patient - Multidisciplinary team” system in the field of rehabilitation. Hence, in this paper, we not only explore conventional rehabilitation techniques but also present and elucidate the following advancements: a revised and comprehensive understanding of the hybrid e-rehabilitation concept and its underlying principles, an enhanced formalization notion of the Smart-system for remote support in hybrid e-rehabilitation services and activities, and the conceptual framework and software implementation of the ontology-based universal dialog service within the Smart-system.},
	booktitle = {2023 {IEEE} 12th {International} {Conference} on {Intelligent} {Data} {Acquisition} and {Advanced} {Computing} {Systems}: {Technology} and {Applications} ({IDAACS})},
	author = {Palagin, Oleksandr and Kaverinsky, Vladislav and Petrenko, Mykola and Malakhov, Kyrylo},
	month = sep,
	year = {2023},
	note = {ISSN: 2770-4254},
	keywords = {Ontologies, Ontology engineering, Software, Telerehabilitation, Computational linguistics, Stress, Data acquisition, Personnel, Electronic healthcare, hybrid e-rehabilitation, IEEE activities, Transdisciplinary research, Universal dialog service},
	pages = {84--89},
}

@inproceedings{sevastjanova_visual_2023,
	title = {Visual {Comparison} of {Text} {Sequences} {Generated} by {Large} {Language} {Models}},
	doi = {10.1109/VDS60365.2023.00007},
	abstract = {Causal language models have emerged as the leading technology for automating text generation tasks. Although these models tend to produce outputs that resemble human writing, they still suffer from quality issues (e.g., social biases). Researchers typically use automatic analysis methods to evaluate the model limitations, such as statistics on stereotypical words. Since different types of issues are embedded in the model parameters, the development of automated methods that capture all relevant aspects remains a challenge. To tackle this challenge, we propose a visual analytics approach that supports the exploratory analysis of text sequences generated by causal language models. Our approach enables users to specify starting prompts and effectively groups the resulting text sequences. To this end, we leverage a unified, ontology-driven embedding space, serving as a shared foundation for the thematic concepts present in the generated text sequences. Visual summaries provide insights into various levels of granularity within the generated data. Among others, we propose a novel comparison visualization that slices the embedding space and represents the differences between two prompt outputs in a radial layout. We demonstrate the effectiveness of our approach through case studies, showcasing its potential to reveal model biases and other quality issues.},
	booktitle = {2023 {IEEE} {Visualization} in {Data} {Science} ({VDS})},
	author = {Sevastjanova, Rita and Vogelbacher, Simon and Spitz, Andreas and Keim, Daniel and El-Assady, Mennatallah},
	month = oct,
	year = {2023},
	keywords = {Semantics, Data visualization, Visual analytics, Linguistics, Writing, Layout, Analytical models, Causal Language Models, Prompt Output Comparison, Text Generation},
	pages = {11--20},
}

@inproceedings{stoyanov_using_2023,
	title = {Using {LLMs} in {Cyber}-{Physical} {Systems} for {Agriculture} - {ZEMELA}},
	doi = {10.1109/BdKCSE59280.2023.10339738},
	abstract = {This paper presents the idea of developing an advisory service using the capabilities of generative artificial intelligence and in particular of Large Language Model. The service will assess the risks for farmers when preparing projects under different programs, taking into account the Bulgarian legislation related to agriculture, as well as the requirements of the relevant program. The results of a feasibility analysis are summarized in the article. Furthermore, two architectural approaches are discussed. The service will be integrated in the platform for smart agriculture named ZEMELA. A brief overview of this platform is also given in the article.},
	booktitle = {2023 {International} {Conference} on {Big} {Data}, {Knowledge} and {Control} {Systems} {Engineering} ({BdKCSE})},
	author = {Stoyanov, Stanimir and Kumurdjieva, Milena and Tabakova-Komsalova, Veneta and Doukovska, Lyubka},
	month = nov,
	year = {2023},
	keywords = {large language model, Smart agriculture, Knowledge engineering, Big Data, Cyber-physical systems, generative artificial intelligence, Legislation, Prototypes, Control systems, advisory service, smart agriculture},
	pages = {1--6},
}

@inproceedings{samardzhiev_application_2023,
	title = {Application of {Machine} {Learning} and {Natural} {Language} {Technologies} in {Building} {Semantic} {Search} {Systems}: {Case} {Study} of a {Virtual} {Legal} {Assistant}},
	doi = {10.1109/BdKCSE59280.2023.10339730},
	abstract = {Semantic search is a type of advanced information search that is based on the searcher's intent as well as on the meaning of the searched terms and phrases in the relevant context, rather than relying only on their individual dictionary meanings. Classical approaches to the design and implementation of semantic search systems are primarily associated with the appropriate use of different types of ontologies or knowledge graphs, but recently these approaches are increasingly enriched or replaced by the utilization of modern language technologies and machine learning techniques. The paper discusses a methodology for application of specific machine learning methods and language technologies and information retrieval techniques in the development of a type of semantic search systems and presents its application in the creation of a virtual legal assistant.},
	booktitle = {2023 {International} {Conference} on {Big} {Data}, {Knowledge} and {Control} {Systems} {Engineering} ({BdKCSE})},
	author = {Samardzhiev, Georgi and Nisheva-Pavlova, Maria},
	month = nov,
	year = {2023},
	keywords = {machine learning, Machine learning, Semantic search, information retrieval, Neural networks, Law, semantic search, language technology, Natural languages, User interfaces, Heuristic algorithms, virtual assistant},
	pages = {1--7},
}

@inproceedings{labbe_chatgpt_2023,
	title = {{ChatGPT} for phenotypes extraction: one model to rule them all?},
	doi = {10.1109/EMBC40787.2023.10340611},
	abstract = {Information Extraction (IE) is a core task in Natural Language Processing (NLP) where the objective is to identify factual knowledge in textual documents (often unstructured), and feed downstream use cases with the resulting output. In genomic medicine for instance, being able to extract the most precise list of phenotypes associated to a patient allows to improve genetic disease diagnostic, which represents a vital step in the modern deep phenotyping approach. As most of the phenotypic information lies in clinical reports, the challenge is to build an IE pipeline to automatically recognize phenotype concepts from free-text notes. A new machine learning paradigm around large language models (LLM) has given rise of an increasing number of academic works on this topic lately, where sophisticated combinations of different technics have been employed to improve the phenotypes extraction accuracy. Even more recently released, the ChatGPT1 application nevertheless raises the question of the relevance of these approches compared to this new generic one based on an instruction-oriented LLM. In this paper, we propose a rigorous evaluation of ChatGPT and the current state-of-the-art solutions on this specific task, and discuss the possible impacts and the technical evolutions to consider in the medical domain.Clinical relevance— Deep phenotyping on electronic health records has proven its ability to improve genetic diagnosis by clinical exomes [10]. Thus, comparing state-of-the-art solutions in order to derive insights and improving research paths is essential.},
	booktitle = {2023 45th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} \& {Biology} {Society} ({EMBC})},
	author = {Labbé, Thomas and Castel, Pierre and Sanner, Jean-Michel and Saleh, Majd},
	month = jul,
	year = {2023},
	note = {ISSN: 2694-0604},
	keywords = {Ontologies, Machine learning, Information retrieval, Chatbots, Pipelines, Statistical distributions, Temperature distribution},
	pages = {1--4},
}

@inproceedings{tothfalusi_ml-based_2023,
	title = {{ML}-{Based} {Translation} {Methods} for {Protocols} and {Data} {Formats}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180009347&doi=10.23919%2FCNSM59352.2023.10327850&partnerID=40&md5=d5e743da68565427a9eb2070f266c8b0},
	doi = {10.23919/CNSM59352.2023.10327850},
	abstract = {In order to exchange information between systems, the information must get encoded into a predefined data format, and it must be transferred in a protocol that the communicating parties have agreed upon. This works well if all parties follow the same protocol standard and use the same data description schemes. If systems use different data formats or protocols, then some sort of translation is required. Protocol and data format translation has been attempted previously through rule-based approaches, ontologies, and also by using machine learning (ML) techniques. Due to the current advances related to AI/ML methods, tools, and infrastructure, the accuracy and feasibility of “translation” with ML-approaches improved significantly. This paper introduces a generic approach and methodology for translating data formats and protocols with ML-based methods and presents our initial results through JSON-XML and JSON-SenML translation.},
	booktitle = {2023 19th {International} {Conference} on {Network} and {Service} {Management} ({CNSM})},
	author = {Tothfalusi, Tamas and Varga, Eszter and Csiszar, Zoltan and Varga, Pal},
	month = oct,
	year = {2023},
	note = {ISSN: 2165-963X},
	keywords = {Ontologies, LLM, Natural language processing, machine learning, natural language processing, Machine learning, Standards, Rule-based approach, Computational linguistics, Language processing, neural machine translation, Natural languages, protocol translation, Protocols, Natural language processing systems, Machine-learning, Learning algorithms, Translation (languages), Description schemes, Protocol translations, System use, Translation method},
	pages = {1--5},
	annote = {Cited by: 4},
}

@article{wang_semantic_2023,
	title = {Semantic {Information} {Modeling} and {Implementation} {Method} for {Water} {Conservancy} {Equipment}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3336817},
	abstract = {Water conservancy equipment (WCE) has a large amount of information, structural heterogeneity and complex relationship leads to the difficulty of semantic interoperability in smart water conservancy. To overcome this issue, we propose the WCE information interaction dimension theory, modeling process and instancing method. First, we analyze the smart water conservancy ontology and information factor, and propose semantic information interaction dimension structure of water conservancy Ontology. Second, we construct the network information model structure of water conservancy, through the relationship degree, a tree model which can realize semantic expression and interoperability is formed through the dimensionality reduction of the model. Third, the component attribute set hierarchical relationship architecture water conservancy information model is established, which use XML language to describe this model. Moreover, the three types of instancing methods are proposed. Through OPC unified architecture (OPC UA) technology, water conservancy information model can implement semantic interoperability. The experimental show that the proposed method of semantic information modeling and semantic interoperability of WCE is feasible, and obvious advantages of complete semantic interoperability than in the model architecture, semantic structure and technical implementation.},
	journal = {IEEE Access},
	author = {Wang, Songsong and Xu, Ouguan},
	year = {2023},
	keywords = {Ontologies, Interoperability, Semantics, semantics, Information model, Data models, OPC UA, Optical wavelength conversion, smart water conservancy, water conservancy equipment, Water conservation, Water resources},
	pages = {133879--133890},
}

@inproceedings{ermakov_approach_2023,
	title = {Approach to the {Development} of {Ontology}-{Driven} {Language} {Toolkits} {Based} on {Metamodeling}},
	doi = {10.1109/AICT59525.2023.10313152},
	abstract = {The information systems are to be conforming to the requirements defined by domain experts. These requirements are formalized as models created with modeling tools. Applying these tools is complicated for domain experts. Domain specific modeling (DSM) with domain specific languages (DSL) reduces the semantic gap. However, the system development complication shifts to the creation of languages and tools for transforming models and code generation. An approach to automating DSL creation and facilitating code generation based on using multifaceted ontology is proposed. The generalized description of the multifaceted ontology is given. Tools of automating generation of new DSL metamodels based on mapping the corresponding domain ontology onto the metamodels of the selected base languages are described. Metamodels of the visual languages, grammars of the target text languages and transformation rules are also included into the ontology. The proposed approach is implemented as a research prototype of the language toolkits. Examples of metamodels and rules described in the ontology, as well as the results of their application are shown. The results of experiments confirmed practical significance of the approach to the ontology-driven language toolkits development.},
	booktitle = {2023 {IEEE} 17th {International} {Conference} on {Application} of {Information} and {Communication} {Technologies} ({AICT})},
	author = {Ermakov, Ivan and Lanin, Viacheslav and Lyadova, Lyudmila and Proskuryakov, Kirill},
	month = oct,
	year = {2023},
	note = {ISSN: 2472-8586},
	keywords = {Ontologies, Metamodeling, Visualization, DSL, metamodeling, domain-specific languages, Prototypes, Codes, Grammar, domain-specific modeling, metamodel generation, model transformation rules, multifaceted ontology},
	pages = {1--6},
}

@inproceedings{nanwani_instance-level_2023,
	title = {Instance-{Level} {Semantic} {Maps} for {Vision} {Language} {Navigation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187015327&doi=10.1109%2FRO-MAN57019.2023.10309534&partnerID=40&md5=7fc3a96f9a17fdcf211b87ad0e648a08},
	doi = {10.1109/RO-MAN57019.2023.10309534},
	abstract = {Humans have a natural ability to perform semantic associations with the surrounding objects in the environment. This allows them to create a mental map of the environment, allowing them to navigate on-demand when given linguistic instructions. A natural goal in Vision Language Navigation (VLN) research is to impart autonomous agents with similar capabilities. Recent works take a step towards this goal by creating a semantic spatial map representation of the environment without any labeled data. However, their representations are limited for practical applicability as they do not distinguish between different instances of the same object. In this work, we address this limitation by integrating instance-level information into spatial map representation using a community detection algorithm and utilizing word ontology learned by large language models (LLMs) to perform open-set semantic associations in the mapping representation. The resulting map representation improves the navigation performance by two-fold (233\%) on realistic language commands with instance-specific descriptions compared to the baseline. We validate the practicality and effectiveness of our approach through extensive qualitative and quantitative experiments.},
	booktitle = {2023 32nd {IEEE} {International} {Conference} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	author = {Nanwani, Laksh and Agarwal, Anmol and Jain, Kanishk and Prabhakar, Raghav and Monis, Aaron and Mathur, Aditya and Jatavallabhula, Krishna Murthy and Abdul Hafez, A. H. and Gandhi, Vineet and Krishna, K. Madhava},
	month = aug,
	year = {2023},
	note = {ISSN: 1944-9437},
	keywords = {Ontologies, Language model, Semantics, Visualization, Measurement, Linguistics, Autonomous agents, Three-dimensional displays, Navigation, Ontology's, Labeled data, Community detection algorithms, Map representations, Mental maps, On demands, Semantic associations, Semantic map, Spatial maps},
	pages = {507--512},
	annote = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{wilcock_err_2023,
	title = {To {Err} {Is} {Robotic}; to {Earn} {Trust}, {Divine}: {Comparing} {ChatGPT} and {Knowledge} {Graphs} for {HRI}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186995135&doi=10.1109%2FRO-MAN57019.2023.10309510&partnerID=40&md5=8870798640e40252a6abbd670835e31e},
	doi = {10.1109/RO-MAN57019.2023.10309510},
	abstract = {The paper discusses two current approaches to conversational AI, using large language models and knowledge graphs, and compares types of errors that occur in human-robot interactions based on these approaches. It provides example dialogues and describes solutions to several error types including false implications, ontological errors, theory of mind errors, and handling of speech recognition errors. The paper addresses issues of particular concern for earning user trust.},
	booktitle = {2023 32nd {IEEE} {International} {Conference} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	author = {Wilcock, Graham and Jokinen, Kristiina},
	month = aug,
	year = {2023},
	note = {ISSN: 1944-9437},
	keywords = {Knowledge graphs, Knowledge graph, Terminology, Language model, Semantics, Visualization, Speech recognition, Human-robot interaction, Oral communication, 'current, Human robot interaction, Humans-robot interactions, Errors, Error theory, Error types, Recognition error, Theory of minds},
	pages = {1396--1401},
	annote = {Cited by: 6},
}

@inproceedings{elkodssi_toward_2023,
	title = {Toward {Semantic} {Framework} for {Internet} of {Things}-{Aware} {Business} {Process} {Discovery}},
	doi = {10.1109/ICDATA58816.2023.00012},
	abstract = {The Internet of Things (IoT) is often considered a disruptive technology [1]. By using smart devices, it has the potential to change everyone's daily life. With large sets of advanced sensors and actuators, it can create opportunities for commercial organizations to establish new business models. A fundamental barrier to automatic business process sensing is the lack of modeling concepts that explicitly express Internet elements as components of a business process model. Thus, there is a clear need to model these processes associated with IoT elements in a formal and unambiguous manner. However, in the context of business processes, there is a lack of formalized and explicit descriptions of IoT elements, which hinders their effective modeling and management. This article proposes a semantic formalization of the business process management perspective in an IoT environment by proposing Extended BPMNO for IoT and Domain Ontology. It uses standard semantic technologies to give a semantic representation that allows us to describe concepts relating to the IoT and the elements of an executable business process described in BPMN.},
	booktitle = {2023 {International} {Conference} on {Digital} {Age} \& {Technological} {Advances} for {Sustainable} {Development} ({ICDATA})},
	author = {Elkodssi, Iman and Sbai, Hanae},
	month = may,
	year = {2023},
	keywords = {Ontologies, Ontology, Semantics, Annotations, Sustainable development, Internet of Things, Data models, Business process management, Business Process Management Notation (BPMN), IoT element, The IoT-aware BP},
	pages = {12--16},
}

@inproceedings{ayad_towards_2023,
	title = {Towards a {Meta}-{Modeling} {Approach} for {Business} {Process} {Models} {Improvement} {Based} on {Ontological} {Analysis}},
	doi = {10.1109/CoDIT58514.2023.10284477},
	abstract = {Business process modeling enable smoother and more efficient decision making in the organizations as it achieve consistency and standardization of their operations, facilitate communication and collaboration between different stakeholders. Therefore, it is important to be able to model real world aspects. To this end, we have carried out a thorough review of the relevant literature which focuses on real-world aspects that Business process modeling languages BPMLs are not able to model. These aspects are based on ontological analysis and characterization of process modeling constructs. Our research aims to propose an approach for business process model modeling improvement by defining Object Constraint Language OCL rules written at the meta-model level making them independent from specific notations. We exploited IS domain knowledge, defined a meta-model, and added semantics to the meta-model by the mean of OCL constraints. As formalism we use the (OCL) with Ecore from the Eclipse Modeling Framework (EMF).},
	booktitle = {2023 9th {International} {Conference} on {Control}, {Decision} and {Information} {Technologies} ({CoDIT})},
	author = {Ayad, Sarah},
	month = jul,
	year = {2023},
	note = {ISSN: 2576-3555},
	keywords = {Semantics, Standardization, Process modeling, Metamodeling, Transforms, Standards organizations, Analytical models},
	pages = {1021--1026},
}

@inproceedings{sadirmekova_constructing_2023,
	title = {Constructing the {Terminological} {Core} of {NLP} {Ontology}},
	doi = {10.1109/UBMK59864.2023.10286646},
	abstract = {The basis of any intellectual resource is a knowledge base, which, based on the basic terms of the field under consideration, builds relationships between them. Therefore, the first task to building multilingual information system using Natural language processing (NLP)in scientific and educational activities will be the development of a multilingual dictionary on modern NLP methods, including terms in Kazakh, English and Russian. For its construction, linguistic models for semantic dictionaries and thesauruses will be used, as well as methods of automatic extraction of terms from the corpus of texts of a given subject area. In this paper, a system of concepts of the NLP domain will be formalized, which will form the terminological core of the NLP ontology. To systematize information and provide support for multilingualism and accessibility, we plan to apply ontological engineering methods to systematize information and build the upper levels of the NLP ontology (its terminological core) using the dictionary of terms obtained at the previous stage. The ontology developed by us can later become the conceptual basis for a multilingual information system used in scientific and educational activities using NLP. This system will provide systematization of all information, convenient navigation on it, integration into a single information space, as well as access to it.},
	booktitle = {2023 8th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Sadirmekova, Zhanna and Sambetbayeva, Madina and Daiyrbayeva, Elmira and Yerimbetova, Aigerim and Altynbekova, Zhanar and Murzakhmetov, Aslanbek},
	month = sep,
	year = {2023},
	note = {ISSN: 2521-1641},
	keywords = {Ontologies, Natural language processing, Semantics, ontology, Knowledge based systems, conceptual model, Linguistics, Dictionaries, Navigation, ontological design patterns, scientific and educational information system},
	pages = {81--85},
}

@article{mohammadat_model_2023,
	title = {A {Model} of {Design} for {Computing} {Systems}: {A} {Categorical} {Approach}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3325349},
	abstract = {This paper introduces the model of design (MoD), a framework that leverages category theory to study the design and development of computer-driven systems, to the academic and engineering communities dealing with computer systems. The model of design aims to offer a minimal framework for modelling the design and development of embedded computation across domains and abstractions, focusing on functional and extra-functional aspects as well as overarching concerns for automaticity, correctness and reuse. This nuanced approach provides insights into the theory and practice of computer systems design.},
	journal = {IEEE Access},
	author = {Mohammadat, Tage},
	year = {2023},
	keywords = {Ontologies, Semantics, Context modeling, Design automation, model-driven engineering, Computational modeling, Grammar, Symbols, Solid modeling, architectural design, computer-aided design (CAD), Computing systems, domain-specific modelling languages, electronic design automation (EDA), embedded system design, hardware/software co-design, model of computation, system-level design, System-level design},
	pages = {116304--116347},
}

@inproceedings{dhouib_papyrus4manufacturing_2023,
	title = {{Papyrus4Manufacturing}: {A} {Model}-{Based} {Systems} {Engineering} approach to {AAS} {Digital} {Twins}},
	doi = {10.1109/ETFA54631.2023.10275523},
	abstract = {As digital twins gain momentum in their usage in diverse domains, the concept of Asset Administration Shells (AAS) has become very relevant for achieving the digital twin approach, where Administration Shells are the digital representation of physical assets. Being a relatively new concept in the Industrial Internet of Things (IIoT) domain, the tools and approaches for creating and deploying AASs are likewise in infancy. This paper introduces an open-source tool, Papyrus4Manufacturing, which provides a model-based systems engineering approach to the AAS. This toolset supports the creation of AAS digital twins from modeling to automatic deployment and connection to assets using the OPC UA protocol. This paper also includes an evaluation of its usability, as it is put to test with an academic use case.},
	booktitle = {2023 {IEEE} 28th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Dhouib, Saadia and Huang, Yining and Smaoui, Asma and Bhanja, Tapanta and Gezer, Volkan},
	month = sep,
	year = {2023},
	note = {ISSN: 1946-0759},
	keywords = {Digital twins, Asset Administration Shell, Modeling, Digital Twins, Software, Databases, OPC UA, Memory, Model-Based System Engineering, Protocols, BaSyx, Eclipse Papyrus, Generative Software Engineering, Servers, UML Profiles, Unified Modelling Language},
	pages = {1--8},
}

@inproceedings{knorr_towards_2023,
	title = {Towards a {Uniform} {Exchange} {Format} for {Home} and {Building} {Automation} using {VDI} 3814},
	doi = {10.1109/ETFA54631.2023.10275701},
	abstract = {Exchanging technical documents in the building automation domain is a complicated process. Files are distributed either as drawings, spreadsheets, or text documents. Each stakeholder has to re-enter the data into their own system, and changes are revised manually, often even without revision control. This paper presents a uniform exchange format based on the ’graphical’ standard VDI 3814. To increase acceptance, the industry standards XSD and XML were chosen. As a result of this work, a model is provided that covers the concepts and exchange files provided in the VDI 3814 standard. Given a supporting tool, data can be entered, revised, and exchanged automatically. Based on this unified representation, it is subsequently possible to transfer the data into one of the already existing ontologies in this domain by using model transformations. Some of these ontologies are also referred to in this paper.},
	booktitle = {2023 {IEEE} 28th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Knorr, Felix and Kastner, Wolfgang},
	month = sep,
	year = {2023},
	note = {ISSN: 1946-0759},
	keywords = {Ontologies, Standards, Data models, Stakeholders, XML, Buildings, Industries, Building Automation, Uniform Format, VDI 3813, VDI 3814},
	pages = {1--4},
}

@inproceedings{zeng_research_2023,
	title = {Research on {Key} {Technologies} of {Automated} {Instructional} {Design} for {Engineering} {Education} {Courses}},
	doi = {10.1109/CSTE59648.2023.00022},
	abstract = {An Automated Instructional Design (AID) solution for engineering education courses instructional design is proposed in this research. By limiting the Domain of AID to engineering education courses, and limiting the course objectives to the graduate attributes and professional competence of engineering education programme, as well as limiting the instructional methods to task-and-activity-based methods, and applying the evaluation vocabulary regularly used in the field of engineering education, a complete set of instructional design vocabulary related to the instructional design domain can be abstracted. Using the Unified Modeling Language (UML) Profile mechanism, the complete set of instructional design vocabulary can be represented by a designed visual model language, which provide a complete instructional design description language, named Instructional Design Visual Model Language (IDVML), dedicated to the field of engineering education for AID tool implementation. Then the existed Model-Driven Architecture (MDA) tools can be applied to design IDVML-related Platform Independent Model (PIM) and various Platform Specific Model (PSM) meta-models, and the conversion templates from PIM to various PSM, as well as various conversion templates from PSM to executable code, database table, Web page, course Ontology, etc., so as to realize the conversion from IDVML to program code directly. Finally, using the Meta-model Object Facility (MOF), an independent AID can be implemented. This AID can help the teacher to design the teaching objectives, content, tasks and activities, evaluation of engineering education related courses which meet the requirements of engineering education accreditation. At the same time, the course Ontology can be generated to provide Ontology basis for the subsequent construction of individualized learning system.},
	booktitle = {2023 5th {International} {Conference} on {Computer} {Science} and {Technologies} in {Education} ({CSTE})},
	author = {Zeng, Ling and Liang, Zaoqing and Liang, Yun and Huang, Peijie},
	month = apr,
	year = {2023},
	keywords = {Ontologies, Vocabulary, Unified modeling language, engineering education, Visualization, Computer architecture, Limiting, Accreditation, automated instructional design, instructional design meta-model, instructional design visulized modleing language},
	pages = {86--92},
}

@inproceedings{ji_research_2023,
	title = {Research on {Domain} {Knowledge} {Representation} {Techniques}},
	doi = {10.1109/SNPD-Winter57765.2023.10223881},
	abstract = {Knowledge is the fruit of human's knowledge of the objective world in practice and the crystallization of wisdom. A knowledge base makes a knowledge-based system (or expert system) intelligent by structuring and sorting out knowledge in a specific field and storing, organizing, managing and using it in a computer using a scientific knowledge representation. Based on the research of knowledge base construction in securities industry, this paper first summarizes and explains several representative knowledge representation models. Then, it summarizes the application scenarios of common knowledge representation techniques in the fields of product design, robot control, and natural language processing. In addition, based on the investigation of the knowledge characteristics of the securities industry, a knowledge representation model of the securities industry based on 5W1lH is proposed to organize and manage the multimodal information resources and provide high-value information for user needs, while the knowledge representation technology of the mechanical industry based on hypergraph embedding is examined and the specific processes and application scenarios are summarized.},
	booktitle = {2023 26th {ACIS} {International} {Winter} {Conference} on {Software} {Engineering}, {Artificial} {Intelligence}, {Networking} and {Parallel}/{Distributed} {Computing} ({SNPD}-{Winter})},
	author = {Ji, Wei and Cao, Qinghong and Shi, Jin and Zhu, Enyao and Xu, Tianyi and He, Hao},
	month = jul,
	year = {2023},
	keywords = {Natural language processing, Knowledge representation, knowledge representation, Product design, knowledge graph, Security, Service robots, Industries, industry knowledge base, Robot control},
	pages = {150--157},
}

@inproceedings{jousselme_uncertain_2023,
	title = {Uncertain about {ChatGPT}: enabling the uncertainty evaluation of large language models},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171522315&doi=10.23919%2FFUSION52260.2023.10224086&partnerID=40&md5=cb834d8d98c6e7b7e557e1debb154586},
	doi = {10.23919/FUSION52260.2023.10224086},
	abstract = {ChatGPT, OpenAI’s chatbot, has gained consider-able attention since its launch in November 2022, owing to its ability to formulate articulated responses to text queries and comments relating to seemingly any conceivable subject. As impressive as the majority of interactions with ChatGPT are, this large language model has a number of acknowledged shortcomings, which in several cases, may be directly related to how ChatGPT handles uncertainty. The objective of this paper is to pave the way to formal analysis of ChatGPT uncertainty handling. To this end, the ability of the Uncertainty Representation and Reasoning Framework (URREF) ontology is assessed, to support such analysis. Elements of structured experiments for reproducible results are identified. The dataset built varies Information Criteria of Correctness, Non-specificity, Self-confidence, Relevance and Inconsistency, and the Source Criteria of Reliability, Competency and Type. ChatGPT’s answers are analyzed along Information Criteria of Correctness, Non-specificity and Self-confidence. Both generic and singular information are sequentially provided. The outcome of this preliminary study is twofold: Firstly, we validate that the experimental setup is efficient in capturing aspects of ChatGPT uncertainty handling. Secondly, we identify possible modifications to the URREF ontology that will be discussed and eventually implemented in URREF ontology Version 4.0 under development.},
	booktitle = {2023 26th {International} {Conference} on {Information} {Fusion} ({FUSION})},
	author = {Jousselme, A-L. and de Villiers, J.P. and de Freitas, A. and Blasch, E. and Dragos, V. and Pavlin, G. and Costa, P. C. and Laskey, K. B. and Laudy, C.},
	month = jun,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontologies, Large Language Models, Large language model, Ontology, Language model, NLP, Quality control, Uncertainty, Reliability, Cognition, Computational linguistics, Uncertainty reasoning, Chatbots, Information quality, Media, Disruptive technologies, Source quality, Uncertainty evaluation, Ontology's, Reasoning framework, Uncertainty analysis, Uncertainty handling, Uncertainty representation},
	pages = {1--8},
	annote = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{srinivasan_curriculum_2023,
	title = {Curriculum {Learning} for {Data}-{Efficient} {Vision}-{Language} {Alignment}},
	doi = {10.1109/CVPRW59228.2023.00595},
	abstract = {Aligning image and text encoders from scratch using contrastive learning requires large amounts of paired image-text data. We alleviate this need by aligning individually pre-trained language and vision representation models using a much smaller amount of paired data with a curriculum learning algorithm to learn fine-grained vision-language alignments. TOnICS (Training with Ontology-Informed Contrastive Sampling) initially samples minibatches whose image-text pairs contain a wide variety of objects to learn object-level vision-language alignment, and progressively samples minibatches where all image-text pairs contain the same object to learn finer-grained contextual alignment. Aligning pre-trained BERT and VinVL-OD models to each other using TOnICS outperforms CLIP on downstream zero-shot image retrieval using {\textless} 1\% as much training data.},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Srinivasan, Tejas and Ren, Xiang and Thomason, Jesse},
	month = jun,
	year = {2023},
	note = {ISSN: 2160-7516},
	keywords = {Computer vision, Image retrieval, Training, Computational modeling, Training data, Conferences, Bit error rate},
	pages = {5619--5624},
}

@inproceedings{nadhila_mapping_2023,
	title = {Mapping {Personality} {Traits} to {Customer} {Complaints}: {Framework} for {Personalized} {Customer} {Service}},
	doi = {10.1109/IAICT59002.2023.10205809},
	abstract = {The study establishes utilizing the Big Five Personality framework and a Personality Measurement Platform (PMP) for personality analysis. Moreover, Customer Complaint Ontology (CCOntology) framework implements a Naive Bayes machine learning methodology to evaluate and scrutinize customer complaints. The algorithm works by calculating the probability of each complaint category. This association is measured in percentages, enabling the identification of specific personality traits related to customer complaints through identifying complaint characteristics and areas of concern. The study has found that individuals with neurotic personality traits who encounter customer complaints are often associated with problem categories such as Non-Contract, Privacy, and Contract and are more likely to express strong emotional dissatisfaction with a product or service. Linking customer complaints with their corresponding personalities can be an incredibly effective and innovative strategy for personalized customer service businesses in anticipating their needs and providing tailored recommendations that can improve the likelihood of customers making purchases. This approach involves educating employees on the importance of actively listening to customers, asking relevant questions, and anticipating their needs, ensuring that businesses can enhance customer satisfaction while building a loyal customer base.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Industry} 4.0, {Artificial} {Intelligence}, and {Communications} {Technology} ({IAICT})},
	author = {Nadhila, Fadiah and Alamsyah, Andry},
	month = jul,
	year = {2023},
	note = {ISSN: 2834-8249},
	keywords = {Ontologies, Big Data, Technological innovation, Privacy, Oral communication, Social networking (online), Big Five Personality, Customer Complaint Ontology (CCOntology), Customer services, Naive Bayes, Personality Measurement Platform, Personalized Customer Service},
	pages = {96--101},
}

@inproceedings{okazaki_framework_2023,
	title = {A {Framework} to {Support} {Failure} {Cause} {Identification} in {Manufacturing} {Systems} through {Generalization} of {Past} {FMEAs}},
	doi = {10.1109/AIM46323.2023.10196264},
	abstract = {This study proposes a framework for inferring the causes of failures occurring in manufacturing systems from past Failure Mode and Effect Analyses (FMEAs) conducted on other systems to assist in inspecting and maintaining the systems. Among various manufacturing systems, a framework to search past FMEAs and the corresponding causes of the failure requires solving the following problems. First, the difference in products, equipment, and wording to represent them make it difficult to search the similar failure phenomenon from FMEAs. Secondly, the causes of failure highly depend on the process flow of the system until the failure occurs. Therefore, it is also hard to find appropriate failure causes from FMEAs without reflecting on the process. The framework solves the first issue by generalizing descriptions in past FMEAs based on structured concepts of manufacturing systems in an ontology before inference of causes to address. Furthermore, the framework analyzes the correspondence of the process flows between the target manufacturing system and past FMEAs using a process order model generated by SysML diagrams to solve the second issue. The comparison between the causes inferred by the proposed framework and by skilled experts for three typical failures in the manufacturing system and the interview with them about the plausibility of the inference results showed that more than 73 \% of them were valid.},
	booktitle = {2023 {IEEE}/{ASME} {International} {Conference} on {Advanced} {Intelligent} {Mechatronics} ({AIM})},
	author = {Okazaki, Sho and Shirafuji, Shouhei and Yasui, Toshinori and Ota, Jun},
	month = jun,
	year = {2023},
	note = {ISSN: 2159-6255},
	keywords = {Ontologies, Cognition, Data models, Search problems, Analytical models, Mechatronics, Maintenance engineering},
	pages = {858--865},
}

@article{jaradeh_aremotive_2023,
	title = {{ArEmotive} {Bridging} the {Gap}: {Automatic} {Ontology} {Augmentation} {Using} {Zero}-{Shot} {Classification} for {Fine}-{Grained} {Sentiment} {Analysis} of {Arabic} {Text}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3300737},
	abstract = {Human-computer interaction remains one of the final frontiers to conquer while held in perspective with the rapid developments and technology growth over recent years. It is an arduous task to convey the true human intent to the machine in order to generate a computerized relevant decision in a certain field. In recent years, focus has shifted to cover fields of study that relate to Sentiment Analysis (SA) to improve and ease the tasks of our daily lives. We Propose ArEmotive (Arabic Emotive), a fine-grained sentiment analysis system that is human-independent which can automatically grow its source of information allowing for more precision and a greater dataset each time it is used through ontology augmentation and classification. Our proposed architecture relies on multiple data sources running through certain pipelines to generate a central online repository utilized by any mobile system to access this info-base. This system is important because many researchers in the field of automated ontology alignment and ontology mapping achieved a semi-automated approach to map new ontologies out of old ones or to extend already existing ontologies with data from new ones. ArEmotive identifies fine-grained emotions in text based on a dynamic ontology enriched through ontology alignment, mapping and machine learning assisted classification, resulting in a structure that contributes in: a centralized dataset ever growing to fit the need of the users, a sustainable structure able to allocate new data sources without the need to modify the system, ability to generate appropriate information even with the absence of “parent” sources.},
	journal = {IEEE Access},
	author = {Jaradeh, Amer and Kurdy, Mohamad-Bassam},
	year = {2023},
	keywords = {Ontologies, Sentiment analysis, Emotion recognition, Arabic NLP, Bridges, Soft sensors, Blogs, Social networking (online), Task analysis, fine-grained emotions, ontology augmentation},
	pages = {81318--81330},
}

@inproceedings{belani_iot_2023,
	title = {{IoT} {Ontology} {Development} {Process} for {Well}-{Being}, {Aging} and {Health}: {Challenges} and {Opportunities}},
	doi = {10.23919/SpliTech58164.2023.10193435},
	abstract = {Ontology development processes are not trivial, given the inherently complex nature of knowledge capturing and management, as well as the need to provide structural and methodical approach on the process methodology itself in order for it to be adopted and usable. If aiming to develop an ontology for multidimensional concepts, such as well-being, aging and health, it is certain that knowledge from multiple domains have to be included, which only extends the time needed for ontology engineering. If such environments aim to be supported by the Internet of Things, than challenges rise even more. This paper provides a scoping analysis of existing well-known ontology development methodologies, with a note on the extent of their adoption and readiness to be used in a multi-domain circumstances. The approach to IoT ontology development process tailoring has been presented and elaborated, as well as the challenges specific to IoT ontology development for well-being, aging and health. Finally, research opportunities have been presented and future directions given on providing more comprehensive, more tailored and more usable ontology development methodologies.},
	booktitle = {2023 8th {International} {Conference} on {Smart} and {Sustainable} {Technologies} ({SpliTech})},
	author = {Belani, Hrvoje and Šolić, Petar and Perković, Toni and Pleština, Vladimir},
	month = jun,
	year = {2023},
	keywords = {Ontologies, Semantics, ontology, Knowledge engineering, Internet of Things, Reliability, Aging, development process, e-health, well-being},
	pages = {1--6},
}

@inproceedings{geng_relational_2023,
	title = {Relational {Message} {Passing} for {Fully} {Inductive} {Knowledge} {Graph} {Completion}},
	doi = {10.1109/ICDE55515.2023.00098},
	abstract = {In knowledge graph completion (KGC), predicting triples involving emerging entities and/or relations, which are unseen when the KG embeddings are learned, has become a critical challenge. Subgraph reasoning with message passing is a promising and popular solution. Some recent methods have achieved good performance, but they (i) usually can only predict triples involving unseen entities alone, failing to address more realistic fully inductive situations with both unseen entities and unseen relations, and (ii) often conduct message passing over the entities with the relation patterns not fully utilized. In this study, we propose a new method named RMPI which uses a novel Relational Message Passing network for fully Inductive KGC. It passes messages directly between relations to make full use of the relation patterns for subgraph reasoning with new techniques on graph transformation, graph pruning, relation-aware neighborhood attention, addressing empty subgraphs, etc., and can utilize the relation semantics defined in the KG’s ontological schema. Extensive evaluation on multiple benchmarks has shown the effectiveness of RMPI’s techniques and its better performance compared with the existing methods that support fully inductive KGC. RMPI is also comparable to the state-of-the-art partially inductive KGC methods with very promising results achieved. Our codes, data and some supplementary experiment results are available at https://github.com/zjukg/RMPI.},
	booktitle = {2023 {IEEE} 39th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Geng, Yuxia and Chen, Jiaoyan and Pan, Jeff Z. and Chen, Mingyang and Jiang, Song and Zhang, Wen and Chen, Huajun},
	month = apr,
	year = {2023},
	note = {ISSN: 2375-026X},
	keywords = {Knowledge graphs, Ontology, Semantics, Knowledge engineering, Knowledge Graph, Link Prediction, Data engineering, Codes, Benchmark testing, Inductive Knowledge Graph Completion, Message passing, Message Passing},
	pages = {1221--1233},
}

@inproceedings{weigand_how_2023,
	title = {How to {Identify} your {Design} {Science} {Research} {Artifact}},
	doi = {10.1109/CBI58679.2023.10187511},
	abstract = {Design Science Research (DSR) is about the development and investigation of artifacts in context. However, in many articles that subscribe to a DSR approach, the artifact is not clearly classified and identified. Very little attention has been given in the DSR literature on this topic, so guidelines are lacking. Based on artifact ontology, this paper proposes guidelines for design researchers in the IS domain on how to specify both the artifact and the research objective. For validation, the guidelines have been applied to a range of DSR papers. Our results show that the artifact definition guidelines can add to the precision of the research object specification.},
	booktitle = {2023 {IEEE} 25th {Conference} on {Business} {Informatics} ({CBI})},
	author = {Weigand, Hans and Johannesson, Paul},
	month = jun,
	year = {2023},
	note = {ISSN: 2378-1971},
	keywords = {Ontologies, Data science, Informatics, Data models, Design methodology, Philosophical considerations, Bibliographies, artifact ontology, Design Science, research methodology},
	pages = {1--10},
}

@article{razzaq_evorecipes_2023,
	title = {{EvoRecipes}: {A} {Generative} {Approach} for {Evolving} {Context}-{Aware} {Recipes}},
	volume = {11},
	issn = {2169-3536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165259066&doi=10.1109%2FACCESS.2023.3296144&partnerID=40&md5=4d706f9f3d01b30905522e9ecd973483},
	doi = {10.1109/ACCESS.2023.3296144},
	abstract = {Generative AI e.g. Large Language Models (LLMs) can be used to generate new recipes. However, LLMs struggle with more complex aspects like recipe semantics and process comprehension. Furthermore, LLMs have limited ability to account for user preferences since they are based on statistical patterns. As a result, these recipes may be invalid. Evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. These algorithms can generate large number of solutions from the set of possible solution space. Moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. In this paper, we propose the EvoRecipes framework to generate novel recipes. The EvoRecipes framework utilizes both Genetic Algorithm and generative AI in addition to RecipeOn ontology, and RecipeKG knowledge graph. Genetic Algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while LLMs are used to generate recipe text from encoded recipe solutions. EvoRecipes uses a population of context-aware recipe solutions from the RecipeKG knowledge graph. RecipeKG encodes recipes in RDF format using classes and properties as defined in the RecipeOn ontology. Moreover, to evaluate the alignment of EvoRecipe generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. Additionally, to evaluate the quality of the EvoRecipe generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). Results show that EvoRecipes generated recipes are novel, valid and incorporate user preferences.},
	journal = {IEEE Access},
	author = {Razzaq, Muhammad Saad and Maqbool, Fahad and Ilyas, Muhammad and Jabeen, Hajira},
	year = {2023},
	note = {Type: Article},
	keywords = {Ontologies, Knowledge graphs, Knowledge graph, Ontology, Language model, Semantics, ontology, Resource description framework, Creativity, Quality control, Resource Description Framework (RDF), Food products, computational creativity, User interfaces, food, Genetic algorithms, recipe, recipe evolution, Sociology, Ontology's, Computational creativities, Resources description frameworks, Function evaluation, Stochastic systems, Recipe, Recipe evolution},
	pages = {74148--74164},
	annote = {Cited by: 11; All Open Access; Gold Open Access},
}

@inproceedings{bandara_ontology_2023,
	title = {Ontology {Based} {Restaurant} {Recommendation} {Approach}},
	doi = {10.1109/ICARC57651.2023.10145722},
	abstract = {As the world moves forward, the restaurant industry is rapidly expanding. Customers may never physically evaluate a restaurant based on its services until that customer has practical experience with it. A better recommendation mechanism can always direct the customer to the correct location, resulting in a positive outcome. The paper discusses an approach of a context rich chatbot that can identify the customer’s mode of thinking using a restaurant ontology that suggests relevant restaurants and foods. Most importantly the defined methodology will use a hybrid version of knowledge bases along with a two-way bind to the primary knowledge base. The chatbot will proceed to find relationships in the ontology by tracing concept definitions and properties while feeding information from the database. The main components related to the proposed system are Natural Language Understanding (NLU) pipeline, dialog management module, action server, knowledge query module, and data repository (MongoDB). This mechanism was evaluated through information retrieval measures.},
	booktitle = {2023 3rd {International} {Conference} on {Advanced} {Research} in {Computing} ({ICARC})},
	author = {Bandara, H. M. R. L. and Ranathunga, L.},
	month = feb,
	year = {2023},
	keywords = {Ontologies, Ontology, Information retrieval, Database, Knowledge based systems, Databases, Chatbots, Pipelines, Industries, Action Server, Dialog Management Module, Knowledge Query Module, Rasa Framework},
	pages = {78--83},
}

@inproceedings{dunbar_use_2023,
	title = {Use of {Natural} {Language} {Processing} in {Digital} {Engineering} {Context} to {Aid} {Tagging} of {Model}},
	doi = {10.1109/SysCon53073.2023.10131050},
	abstract = {This paper uses Natural Language Processing to provide augmented intelligence assistance to the resource intensive task of aligning systems engineering artifacts, namely text requirements and system models, with ontologies. Ontologies are a key enabling technology for digital, multidisciplinary interoperability. The approach presented in this paper combines the efficiency of statistical based natural language processing to process large sets of data with expert verification of output to enable accurate alignment to ontologies in a time efficient manner. It applies this approach to an example from the telecommunications domain to demonstrate the workflows and highlight key points in the process. Enabling easier, faster alignment of systems engineering artifacts with ontologies allows for a holistic view of a system under design and enables interoperability between tools and domains.},
	booktitle = {2023 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Dunbar, Daniel and Vierlboeck, Maximilian and Blackburn, Mark},
	month = apr,
	year = {2023},
	note = {ISSN: 2472-9647},
	keywords = {Ontologies, Natural language processing, natural language processing, ontology, semantic web, Requirements engineering, Measurement, digital engineering, Tagging, Task analysis, augmented intelligence, authoritative source of truth, Telecommunications},
	pages = {1--8},
}

@inproceedings{chen_service-oriented_2023,
	title = {A {Service}-oriented {Approach} {Supporting} {Model} {Integration} in {Model}-based {Systems} {Engineering}},
	doi = {10.1109/SysCon53073.2023.10131078},
	abstract = {When using Model-Based Systems Engineering (MBSE) to develop complex systems, models using different syntax and semantics are typically implemented in a heterogeneous environment which leads to difficulties to realize data integrations across the entire lifecycle. Specifically, seamless exchanges between models of different modeling tools are needed to support system lifecycle activities such as requirement analysis, function analysis, verification and validation. This article illustrates a service-oriented approach to support model integration for model-based systems engineering, especially between architecture design and system verification. First, a set of semantic mapping rules between architecture models and simulation models based on Open Service of Lifecycle Collaboration (OSLC) are proposed to support the formalization of technical resources (models, data, APIs). Then OSLC adapters are developed to transform models, data and APIs into web-based services. The services are deployed by a service discovering plug-in within a specific modeling tool for model information exchange. The approach is illustrated by a case study on KARMA architecture model and Modelica simulation model for a six-degree-of-freedom robot (RobotR3) system. We evaluate the availability and efficiency of this method from both qualitative and quantitative perspectives. The results show that our approach is effective in model and data integration.},
	booktitle = {2023 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Chen, Rui and Wang, Guoxin and Wu, Shouxuan and Lu, Jinzhi and Yan, Yan},
	month = apr,
	year = {2023},
	note = {ISSN: 2472-9647},
	keywords = {Semantics, Data integration, Model-Based Systems Engineering, System verification, Model integration, Transforms, Syntactics, Adaptation models, Analytical models, Modelica, Open Service for Lifecycle Collaboration},
	pages = {1--7},
}

@article{zhao_protein_2023,
	title = {Protein {Function} {Prediction} {With} {Functional} and {Topological} {Knowledge} of {Gene} {Ontology}},
	volume = {22},
	issn = {1558-2639},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160248186&doi=10.1109%2FTNB.2023.3278033&partnerID=40&md5=10e5af9770b802276bd9466926faf7d6},
	doi = {10.1109/TNB.2023.3278033},
	abstract = {Gene Ontology (GO) is a widely used bioinformatics resource for describing biological processes, molecular functions, and cellular components of proteins. It covers more than 5000 terms hierarchically organized into a directed acyclic graph and known functional annotations. Automatically annotating protein functions by using GO-based computational models has been an area of active research for a long time. However, due to the limited functional annotation information and complex topological structures of GO, existing models cannot effectively capture the knowledge representation of GO. To solve this issue, we present a method that fuses the functional and topological knowledge of GO to guide protein function prediction. This method employs a multi-view GCN model to extract a variety of GO representations from functional information, topological structure, and their combinations. To dynamically learn the significance weights of these representations, it adopts an attention mechanism to learn the final knowledge representation of GO. Furthermore, it uses a pre-trained language model (i.e., ESM-1b) to efficiently learn biological features for each protein sequence. Finally, it obtains all predicted scores by calculating the dot product of sequence features and GO representation. Our method outperforms other state-of-the-art methods, as demonstrated by the experimental results on datasets from three different species, namely Yeast, Human and Arabidopsis. Our proposed method’s code can be accessed at: https://github.com/Candyperfect/Master.},
	number = {4},
	journal = {IEEE Transactions on NanoBioscience},
	author = {Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Yang, Yumeng and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
	month = oct,
	year = {2023},
	note = {Type: Article},
	keywords = {Language model, Pre-trained language model, Semantics, Modeling languages, Gene Ontology, semantics, Annotations, Gene ontology, Feature extraction, bioinformatics, Forecasting, Annotation, Graph neural networks, Computational linguistics, Protein function prediction, Convolutional neural networks, genetics, Proteins, pre-trained language model, metabolism, Biological systems, human, Genes, protein, gene ontology, Biological system modeling, Predictive models, Protein sequence, Computational Biology, Amino acids, multi-view GCN, Arabidopsis, molecular genetics, Molecular Sequence Annotation, procedures, Humans, Protein sequences, Directed graphs, Features extraction, Amino-acids, Multi-view GCN, Multi-views},
	pages = {755--762},
	annote = {Cited by: 9},
}

@article{zelina_extraction_2023,
	title = {Extraction, {Labeling}, {Clustering}, and {Semantic} {Mapping} of {Segments} {From} {Clinical} {Notes}},
	volume = {22},
	issn = {1558-2639},
	doi = {10.1109/TNB.2023.3275195},
	abstract = {This work is motivated by the scarcity of tools for accurate, unsupervised information extraction from unstructured clinical notes in computationally underrepresented languages, such as Czech. We introduce a stepping stone to a broad array of downstream tasks such as summarisation or integration of individual patient records, extraction of structured information for national cancer registry reporting or building of semi-structured semantic patient representations that can be used for computing patient embeddings. More specifically, we present a method for unsupervised extraction of semantically-labeled textual segments from clinical notes and test it out on a dataset of Czech breast cancer patients, provided by Masaryk Memorial Cancer Institute (the largest Czech hospital specialising exclusively in oncology). Our goal was to extract, classify (i.e. label) and cluster segments of the free-text notes that correspond to specific clinical features (e.g., family background, comorbidities or toxicities). Finally, we propose a tool for computer-assisted semantic mapping of segment types to pre-defined ontologies and validate it on a downstream task of category-specific patient similarity. The presented results demonstrate the practical relevance of the proposed approach for building more sophisticated extraction and analytical pipelines deployed on Czech clinical notes.},
	number = {4},
	journal = {IEEE Transactions on NanoBioscience},
	author = {Zelina, Petr and Halámková, Jana and Nováček, Vít},
	month = oct,
	year = {2023},
	keywords = {Ontologies, Semantics, NLP, Information retrieval, EHR, Text categorization, Feature extraction, clinical notes, text classification, Measurement, information extraction, Task analysis, Clinical diagnosis, Nanobioscience},
	pages = {781--788},
}

@inproceedings{abrougui_abstract_2023,
	title = {Abstract {Representation} for {Multi}-{Intent} {Spoken} {Language} {Understanding}},
	doi = {10.1109/ICASSP49357.2023.10095062},
	abstract = {Current sequence tagging models based on Deep Neural Network models with pretrained language models achieve almost perfect results on many SLU benchmarks with a flat semantic annotation at the token level such as ATIS or SNIPS. When dealing with more complex human-machine interactions (multi-domain, multi-intent, dialog context), relational semantic structures are needed in order to encode the links between slots and intents within an utterance and through dialog history. We propose in this study a new way to project annotation in an abstract structure with more compositional expressive power and a model to directly generate this abstract structure. We evaluate it on the MultiWoz dataset in a contextual SLU experimental setup. We show that this projection can be used to extend the existing flat annotations towards graph-based structures.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Abrougui, Rim and Damnati, Géraldine and Heinecke, Johannes and Béchet, Frédéric},
	month = jun,
	year = {2023},
	note = {ISSN: 2379-190X},
	keywords = {Semantics, Deep learning, Annotations, Neural networks, Human computer interaction, Natural Language Understanding, Spoken Language Understanding, Natural languages, Tagging, sequence tagging, sequence-to-sequence models},
	pages = {1--5},
}

@inproceedings{su_choice_2023,
	title = {Choice {Fusion} {As} {Knowledge} {For} {Zero}-{Shot} {Dialogue} {State} {Tracking}},
	doi = {10.1109/ICASSP49357.2023.10096669},
	abstract = {With the demanding need for deploying dialogue systems in new domains with less cost, zero-shot dialogue state tracking (DST), which tracks user’s requirements in task-oriented dialogues without training on desired domains, draws attention increasingly. Although prior works have leveraged question-answering (QA) data to reduce the need for in-domain training in DST, they fail to explicitly model knowledge transfer and fusion for tracking dialogue states. To address this issue, we propose CoFunDST, which is trained on domain-agnostic QA datasets and directly uses candidate choices of slot-values as knowledge for zero-shot dialogue-state generation, based on a T5 pre-trained language model. Specifically, CoFunDST selects highly-relevant choices to the reference context and fuses them to initialize the decoder to constrain the model outputs. Our experimental results show that our proposed model achieves outperformed joint goal accuracy compared to existing zero-shot DST approaches in most domains on the MultiWOZ 2.1. Extensive analyses demonstrate the effectiveness of our proposed approach for improving zero-shot DST learning from QA.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Su, Ruolin and Yang, Jingfeng and Wu, Ting-Wei and Juang, Biing-Hwang},
	month = jun,
	year = {2023},
	note = {ISSN: 2379-190X},
	keywords = {Decoding, Data models, Training, Dialogue state tracking, pre-trained language model, knowledge fusion, question answering, zero-shot, Costs, Task analysis, Signal processing, Fuses},
	pages = {1--5},
}

@inproceedings{saraswat_ontology_2023,
	title = {Ontology {Based} {Agriculture} {Data} {Mining} using {IWO} and {RNN}},
	doi = {10.1109/ISCON57294.2023.10112187},
	abstract = {An ontology is a machine-interpretable formal description of domain knowledge. In current years, ontologies have risen to prominence as a key tool for demonstrating domain knowledge and a key element of several knowledge management systems, decision-support systems (DSS) and other intelligent systems including in agriculture. However, a study of the current literature on agricultural ontologies suggests that the majority of research that suggest agricultural ontologies lack a clear assessment mechanism. This is unwanted because this is impossible to assess the value of ontologies in research and practise without well-structured assessment mechanisms. Furthermore, relying on such ontologies and sharing them on the Semantic Web or amongst semantic-aware apps is problematic. This paper presents a framework for selecting appropriate assessment techniques for Ontology Based Agriculture Data Mining utilizing Invasive Weed Optimization (IWO) and Re-current Neural Network (RNN) that appears to be absent from most recent agricultural ontology research. The framework facilitates the selection of relevant evaluation techniques for a particular ontology based on its intended user.},
	booktitle = {2023 6th {International} {Conference} on {Information} {Systems} and {Computer} {Networks} ({ISCON})},
	author = {Saraswat, Deepak},
	month = mar,
	year = {2023},
	note = {ISSN: 2832-143X},
	keywords = {Ontologies, Ontology, Semantic Web, Decision support systems, Knowledge engineering, Agriculture, Data Mining, Recurrent neural networks, Agriculture 4.0, RNN, Prototypes, Agriculture 5.0, IWO},
	pages = {1--8},
}

@article{li_enhancing_2023,
	title = {Enhancing {Semantic} {Relation} {Classification} {With} {Shortest} {Dependency} {Path} {Reasoning}},
	volume = {31},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2023.3265205},
	abstract = {Relation Classification (RC) is a basic and essential task of Natural Language Processing. Existing RC methods can be classified into two categories: sequence-based methods and dependency-based methods. Sequence-based methods identify the target relation based on the overall semantics of the whole sentence, which will inevitably introduce noisy features. Dependency-based methods extract indicative word-level features from the Shortest Dependency Path (SDP) between given entities and attempt to establish a statistical association between the words and the target relations. This pattern relatively eliminates the influence of noisy features and achieves a robust performance on long sentences. Nevertheless, we observe that majority of relation classification processes involve complex semantic reasoning which is hard to be achieved based on the word-level statistical association. To solve this problem, we categorize all relations into atomic relations and composed-relations. The atomic relations are the basic relations that can be identified based on the word-level features, while the composed-relation requires to be deducted from multiple atomic relations. Correspondingly, we propose the Atomic Relation Encoding and Reasoning Model (ATERM). In the atomic relation encoding stage, ATERM groups the word-level features and encodes multiple atomic relations in parallel. In the atomic relation reasoning stage, ATERM establishes the atomic relation chain where relation-level features are extracted to identify composed-relations. Experiments show that our method achieves state-of-the-art results on the three most popular relation classification datasets – TACRED, TACRED-Revisit, and SemEval 2010 task 8 with significant improvements.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Li, Jijie and Shuang, Kai and Guo, Jinyu and Shi, Zengyi and Wang, Hongman},
	year = {2023},
	keywords = {Ontologies, Information extraction, Natural language processing, Semantics, Feature extraction, semantic reasoning, Cognition, Knowledge based systems, Encoding, graph convolution, shortest dependency path},
	pages = {1550--1560},
}

@article{huang_event_2023,
	title = {Event {Extraction} {With} {Dynamic} {Prefix} {Tuning} and {Relevance} {Retrieval}},
	volume = {35},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2023.3266495},
	abstract = {We consider event extraction in a generative manner with template-based conditional generation. Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have several significant challenges, including using suboptimal prompts, static event type information, and the overwhelming number of irrelevant event types. In this article, we propose a generative template-based method with dynamic prefixes and a relevance retrieval framework for event extraction (GREE) by first integrating context information with type-specific prefixes to learn a context-specific prefix for each context, and then retrieving the relevant event types with an adaptive threshold. Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE. Additionally, our model is proven to be portable to new types of events effectively.},
	number = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Huang, Heyan and Liu, Xiao and Shi, Ge and Liu, Qian},
	month = oct,
	year = {2023},
	keywords = {Ontologies, Decoding, Feature extraction, Data mining, prompt tuning, event extraction, Tuning, Adaptation models, Task analysis, Conditional generation, dense retrieval},
	pages = {9946--9958},
}

@inproceedings{tang_construction_2023,
	title = {Construction and {Accurate} {Retrieval} {Method} of {Knowledge} {Graph} of {Automobile} {Engine} {Fault}},
	doi = {10.1109/EEBDA56825.2023.10090855},
	abstract = {In order to improve the efficiency and accuracy of automobile engine fault maintenance, an accurate retrieval method of automobile engine fault driven by knowledge graph was proposed. Firstly, the definition and framework of knowledge graph are discussed. The entity extraction of engine fault features was carried out by multi-source neural network, and the disambiguation of fault entities was carried out by integrating entity link technologies; Secondly, fault knowledge reasoning is carried out to eliminate the wrong knowledge in the knowledge base and infer new knowledge to form a complete knowledge graph.. On this basis, the retrieval subgraph of engine fault semantics is designed. Combined with the influence of physical distance and proximity, the retrieval result evaluation model is established, and the subgraph matching was carried out based on the similarity calculation of graph structure and semantic information. Finally, four knowledge graphs including entity equipment graph, ontology graph, maintenance rule graph and history graph were constructed by selecting some automobile engine fault cases from 2017 to 2020. Finally, the process architecture of engine fault search and analysis is constructed and the effectiveness of the proposed method was verified by precision rate and recall rata, which provides a new idea for accurate and efficient engine maintenance.},
	booktitle = {2023 {IEEE} 2nd {International} {Conference} on {Electrical} {Engineering}, {Big} {Data} and {Algorithms} ({EEBDA})},
	author = {Tang, Jin and Xu, Chengxian and Zhang, Wanda},
	month = feb,
	year = {2023},
	keywords = {Knowledge graphs, Semantic search, Knowledge engineering, knowledge graph, Feature extraction, Decision making, Subgraph matching, Visualization, semantic search, Maintenance engineering, engine fault, entity link},
	pages = {336--345},
}

@article{tu_interactive_2023,
	title = {An {Interactive} {Knowledge} and {Learning} {Environment} in {Smart} {Foodsheds}},
	volume = {43},
	issn = {1558-1756},
	doi = {10.1109/MCG.2023.3263960},
	abstract = {The Internet of Food (IoF) is an emerging field in smart foodsheds, involving the creation of a knowledge graph (KG) about the environment, agriculture, food, diet, and health. However, the heterogeneity and size of the KG present challenges for downstream tasks, such as information retrieval and interactive exploration. To address those challenges, we propose an interactive knowledge and learning environment (IKLE) that integrates three programming and modeling languages to support multiple downstream tasks in the analysis pipeline. To make IKLE easier to use, we have developed algorithms to automate the generation of each language. In addition, we collaborated with domain experts to design and develop a dataflow visualization system, which embeds the automatic language generations into components and allows users to build their analysis pipeline by dragging and connecting components of interest. We have demonstrated the effectiveness of IKLE through three real-world case studies in smart foodsheds.},
	number = {3},
	journal = {IEEE Computer Graphics and Applications},
	author = {Tu, Yamei and Wang, Xiaoqi and Qiu, Rui and Shen, Han-Wei and Miller, Michelle and Rao, Jinmeng and Gao, Song and Huber, Patrick R. and Hollander, Allan D. and Lange, Matthew and Garcia, Christian R. and Stubbs, Joe},
	month = may,
	year = {2023},
	keywords = {Ontologies, Knowledge graphs, Smart agriculture, Internet of Things, Data visualization, Data models, Food products},
	pages = {36--47},
}

@article{kim_identifying_2023,
	title = {Identifying {Alcohol}-{Related} {Information} {From} {Unstructured} {Bilingual} {Clinical} {Notes} {With} {Multilingual} {Transformers}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3245523},
	abstract = {As a key modifiable risk factor, alcohol consumption is clinically crucial information that allows medical professionals to further understand their patients’ medical conditions and suggest appropriate lifestyle modifying interventions. However, identifying alcohol-related information from unstructured free-text clinical notes is often challenging. Not only are the formats of the notes inconsistent, but they also include a massive amount of non-alcohol-related information. Furthermore, for medical institutions outside of English-speaking countries, these clinical notes contain both a mixture of English and local languages, inducing additional difficulty in the extraction. Thanks to the increasing availability of electronic medical record (EMR), several previous works explored the idea of using natural language processing (NLP) to train machine learning models that automatically identify alcohol-related information from unstructured clinical notes. However, all these previous works are limited to English clinical notes, thereby able to leverage various large-scale external ontologies during the text preprocessing. Furthermore, they rely on simple NLP techniques such as the bag-of-words models that suffer from high dimensionality and out-of-vocabulary issues. Addressing these issues, we adopt fine-tuning multilingual transformers. By leveraging their linguistically rich contextual information learned during their pre-training, we are able to extract alcohol-related information from unstructured clinical notes without preprocessing the clinical notes on any external ontologies. Furthermore, our work is the first to explore the use of transformers in bilingual clinical notes to extract alcohol-related information. Even with minimal text preprocessing, we achieve extraction accuracy of 84.70\% in terms of macro F-1 score.},
	journal = {IEEE Access},
	author = {Kim, Han Kyul and Park, Yujin and Park, Yeju and Choi, Eunji and Kim, Sodam and You, Hahyun and Bae, Ye Seul},
	year = {2023},
	keywords = {Ontologies, Terminology, Natural language processing, Bioinformatics, natural language processing, Transformers, Data mining, Informatics, Clinical informatics, Symbols, alcohol information extraction, Hospitals, multilingual transformers, Clinical diagnosis, Alcoholic beverages, information extraction from clinical notes},
	pages = {16066--16075},
}

@article{liu_dysr_2023,
	title = {{DySR}: {A} {Dynamic} {Graph} {Neural} {Network} {Based} {Service} {Bundle} {Recommendation} {Model} for {Mashup} {Creation}},
	volume = {16},
	issn = {1939-1374},
	doi = {10.1109/TSC.2023.3234293},
	abstract = {An increasing number and diversity of services are available, which results in significant challenges to effectively reuse service during mashup creation. Many works have modeled the mashup creation problem as a service recommendation task and have achieved remarkable results. However, the performance of these methods can be further improved. The main problems affecting these methods include the constraints among recommended services, the evolution of services, and the semantic gap existing in services and mashups. In this article, we model the mashup creation problem as a service bundle recommendation task that is formally defined to address the constraints among recommended services. And then, a dynamic graph neural network based model called DySR is proposed to tackle the evolution of service and the semantic gap between services and mashups. In order to quantitatively measure how significant the semantic gap between mashups and services is, a measurement method of a semantic gap is given. With it, experiments show that to what extent DySR can reduce the semantic gap in the context of mashup creation. In addition, new evaluation metrics are introduced to overcome the preference for popular services in traditional service recommendations. Extensive experiments conducted on a real-world dataset from ProgrammableWeb, and the experiment results show that DySR outperforms existing state-of-the-art methods.},
	number = {4},
	journal = {IEEE Transactions on Services Computing},
	author = {Liu, Mingyi and Tu, Zhiying and Xu, Hanchuan and Xu, Xiaofei and Wang, Zhongjie},
	month = jul,
	year = {2023},
	keywords = {Ontologies, Semantics, Graph neural networks, Computational modeling, Task analysis, Dynamic graph neural networks, evolving service, mashup creation, Mashups, Quality of service, semantic gap, service bundle recommendation},
	pages = {2592--2605},
}

@article{benarab_global_2023,
	title = {Global {Ontology} {Entities} {Embeddings}},
	volume = {35},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2023.3235779},
	abstract = {Ontologies are among the most widely used types of knowledge representation formalisms. The application of deep learning techniques in the field of ontology engineering has reinforced the need to learn and generate representations of ontological data. This allows ontologies to be exploited by such models, and thus automate various ontology engineering tasks, where most of the existing tools and machine learning approaches require a numerical feature vectors associated with each concept. This paper outlines a novel approach for learning global ontology entities embeddings by exploiting the structure and the various taxonomic and semantic relationships present in ontologies, taking into account all the information present in the ontological graph and carried by the OWL/RDF triples. Thus, producing global ontology entities embeddings capturing the global ontological graph semantics and similarities enclosed in the source ontology. Three different neural network models have been proposed based on two architectures: multi-input and multi-output, trained using the contrastive estimation technique. The evaluation on OWL/RDF ontologies and word semantic similarity tasks using various graph and WordNet based similarity measures, show that our approach yields competitive results outperforming the state-of-the-art ontology and word embedding models.},
	number = {11},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Benarab, Achref and Sun, Jianguo and Rafique, Fahad and Refoufi, Allaoua},
	month = nov,
	year = {2023},
	keywords = {Ontologies, Semantics, Deep learning, Neural networks, neural networks, feature representation, Adaptation models, Predictive models, Task analysis, Concept embeddings, ontology embeddings, ontology entities vector representations},
	pages = {11449--11460},
}

@article{zhao_improving_2023,
	title = {Improving {Protein} {Function} {Prediction} by {Adaptively} {Fusing} {Information} {From} {Protein} {Sequences} and {Biomedical} {Literature}},
	volume = {27},
	issn = {2168-2208},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142828087&doi=10.1109%2FJBHI.2022.3221988&partnerID=40&md5=ec8195343d0849d89fd38e4dd432e354},
	doi = {10.1109/JBHI.2022.3221988},
	abstract = {Proteins are the main undertakers of life activities, and accurately predicting their biological functions can help human better understand life mechanism and promote the development of themselves. With the rapid development of high-throughput technologies, an abundance of proteins are discovered. However, the gap between proteins and function annotations is still huge. To accelerate the process of protein function prediction, some computational methods taking advantage of multiple data have been proposed. Among these methods, the deep-learning-based methods are currently the most popular for their capability of learning information automatically from raw data. However, due to the diversity and scale difference between data, it is challenging for existing deep learning methods to capture related information from different data effectively. In this paper, we introduce a deep learning method that can adaptively learn information from protein sequences and biomedical literature, namely DeepAF. DeepAF first extracts the two kinds of information by using different extractors, which are built based on pre-trained language models and can capture rudimentary biological knowledge. Then, to integrate those information, it performs an adaptive fusion layer based on a Cross-attention mechanism that considers the knowledge of mutual interactions between two information. Finally, based on the mixed information, DeepAF utilizes logistic regression to obtain prediction scores. The experimental results on the datasets of two species (i.e., Human and Yeast) show that DeepAF outperforms other state-of-the-art approaches.},
	number = {2},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
	month = feb,
	year = {2023},
	note = {Type: Article},
	keywords = {Language model, Pre-trained language model, Semantics, Deep learning, Modeling languages, deep learning, Data mining, Forecasting, pre-trained language models, Protein function prediction, Proteins, accuracy, metabolism, Biological systems, human, biomedicine, Attention mechanisms, protein, protein function, gene ontology, Biological system modeling, Predictive models, Protein engineering, Amino acids, cross-attention mechanism, multiple data, molecular genetics, Humans, amino acid sequence, Article, controlled study, nonhuman, diagnostic test accuracy study, computer model, area under the curve, receiver operating characteristic, Amino Acid Sequence, yeast, Saccharomyces cerevisiae, high throughput analysis, sequence homology, ablation therapy, HTTP, Amino-acids, adaptive fusion module, area under the precision recall curve, Cross-attention mechanism, f max numeric value, human versus nonhuman data, logistic regression analysis, Multiple data, statistical concepts},
	pages = {1140--1148},
	annote = {Cited by: 3},
}

@article{sun_revisiting_2023,
	title = {Revisiting {Embedding}-{Based} {Entity} {Alignment}: {A} {Robust} and {Adaptive} {Method}},
	volume = {35},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2022.3200981},
	abstract = {Entity alignment—the discovery of identical entities across different knowledge graphs (KGs)—is a critical task in data fusion. In this paper, we revisit existing entity alignment methods in practical and challenging scenarios. Our empirical studies show that current work has a low level of robustness to long-tail entities and the lack of entity names or relation triples. We aim to develop a robust and adaptive entity alignment method, and the availability of relations, attributes, or names is not required. Our method consists of an attribute encoder and a relation encoder, representing an entity by aggregating its attributes or relational neighbors using the attention mechanisms that can highlight the useful attributes and relations in end-to-end learning. To let the encoders complement each other and produce a coherent representation space, we propose adaptive embedding fusion via a gating mechanism. We consider four evaluation settings, i.e., the conventional setting with both relation and attribute triples, as well as three challenging settings without attributes, without relations, without both relations and names, respectively. Results show that our method can achieve state-of-the-art performance. Even in the most challenging setting without relations and names, our method can still achieve promising results while existing methods fail.},
	number = {8},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Sun, Zequn and Hu, Wei and Wang, Chengming and Wang, Yuxin and Qu, Yuzhong},
	month = aug,
	year = {2023},
	keywords = {Ontologies, Robustness, Knowledge graph embedding, entity alignment, Manuals, Task analysis, Logic gates, Convolution, adaptive embedding fusion, Sun},
	pages = {8461--8475},
}

@inproceedings{zhang_conversational_2022,
	title = {Conversational {System} for {Clinical} {Communication} {Training} {Supporting} {User}-defined {Tasks}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163868207&doi=10.1109%2FTALE54877.2022.00071&partnerID=40&md5=8e0d6e8e834a1f7349304e355cbee063},
	doi = {10.1109/TALE54877.2022.00071},
	abstract = {Effective clinical communication is essential for delivering safe and high-quality patient care, especially in emergent cases. Standard communication protocols have been developed to improve communication accuracy and efficiency. However, traditional training and evaluation require substantial manpower and time, which can be infeasible during public crises when training is most needed. This research aims to facilitate autonomous, low-cost, adaptive clinical communication training via artificial intelligence (AI)-powered techniques. We propose a conversational system for clinical communication training supporting user-defined tasks. Two data augmentation (DA) methods, term replacement and context expansion, are proposed to allow non-professional users to create Al models with a small number of samples. Equipped with biomedical ontology and pre-trained language models, our system is able to simulate clinical communication scenarios, provide timely evaluation, and adapt to new tasks with minimal editing. Various experiments demonstrate that our proposed algorithms can achieve satisfactory performance using a small amount of training data. Real-world practice in local hospitals shows that our system can provide expert-level evaluation and deliver effective clinical communication training.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Teaching}, {Assessment} and {Learning} for {Engineering} ({TALE})},
	author = {Zhang, Xiang and Yu, Bruce X.B. and Liu, Yan and Chen, Gong and Ng, George Wing-Yiu and Chia, Nam-Hung and So, Eric Hang-Kwong and So, Sze-Sze and Cheung, Victor Kai-Lam},
	month = dec,
	year = {2022},
	note = {ISSN: 2470-6698},
	keywords = {Ontologies, Data models, Human computer interaction, Training, Conversational systems, human-computer interaction, Training data, Adaptation models, Biological system modeling, Protocols, User interfaces, autonomous communication training, clinical communication, conversational system, Clinical research, High quality, Low-costs, Autonomous communication training, Autonomous communications, Clinical communications, Communications protocols, Communications training, Patient care, Public crisis},
	pages = {396--403},
	annote = {Cited by: 2},
}

@inproceedings{chen_enhancing_2022,
	title = {Enhancing {Cross}-lingual {Medical} {Concept} {Alignment} by {Leveraging} {Synonyms} and {Translations} of the {Unified} {Medical} {Language} {System}},
	doi = {10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00309},
	abstract = {Well-developed medical terminology systems like the Unified Medical Language System (UMLS) improve the ability of language models to handle medical entity linking tasks. However, such magnificent terminology systems are only available for few languages, such as English. For Chinese, both simplified and traditional, the lack of well-developed terminology systems remains a big challenge to unify Chinese medical terminologies by linking medical entities as concepts. In this study, we purpose a translation enhanced contrastive learning scheme which leverages translations and synonyms of UMLS to infuse knowledge into the language model, and present a cross-lingual pre-trained language model called TeaBERT that aligns cross-lingual Chinese and English medical synonyms well at semantic level. Comparing with former cross-lingual language models, TeaBERT significantly outperforms on evaluation datasets, with 93.21\%, 89.89\% and 76.45\% of Top 5 accuracy on ICDI0-CN, CHPO and RealWorld dataset respectively, and achieves new state-of-theart performance without task specific fine-tuning. Our contrastive learning scheme can not only be used for enhancing Chinese-English medical concepts alignment, but also be applied to other languages facing the same challenges.},
	booktitle = {2022 {IEEE} 24th {Int} {Conf} on {High} {Performance} {Computing} \& {Communications}; 8th {Int} {Conf} on {Data} {Science} \& {Systems}; 20th {Int} {Conf} on {Smart} {City}; 8th {Int} {Conf} on {Dependability} in {Sensor}, {Cloud} \& {Big} {Data} {Systems} \& {Application} ({HPCC}/{DSS}/{SmartCity}/{DependSys})},
	author = {Chen, Luming and Qi, Yifan and Wu, Aiping and Deng, Lizong and Jiang, Taijiao},
	month = dec,
	year = {2022},
	keywords = {Terminology, Semantics, Knowledge representation, NLP, Unified modeling language, UMLS, pre-trained language model, Biological system modeling, Task analysis, cross-lingual medical entity linking},
	pages = {2078--2083},
}

@inproceedings{taru_building_2022,
	title = {Building {Ontology} for {Toxic} words},
	doi = {10.1109/MLCSS57186.2022.00052},
	abstract = {Many online social media platforms have particular community guidelines for comment sections. The platforms that maintain commentary sections in various posts, videos, and blogs need to adhere to these guidelines. These comment sections may have specific comments that fail to satisfy the rules and regulations to maintain societal norms of communication. These comments are classified as toxic comments. Google's Perspective API defines toxic comments as comments that are rude, offensive, and likely to make someone leave the conversation. In this paper, we have built a toxic words ontology, which is as per our knowledge, first Ontology built on toxic words. This Ontology consists of toxic words and their antonyms and synonyms in increasing order of their toxicity levels. Traversing this ontology, we can find the best-suited word with less toxicity and similar meaning. This is a dynamic ontology and new words can be added easily. Thus letting us convey messages in a civil manner. We propose to reduce toxicity in the most straightforward way. After studying several papers, we found out that the toxicity mainly occurs because of use of toxic words. We also observed that use of less toxic synonyms or no toxic synonyms has huge effects on toxicity score given by the Perspective API, and results section proves that.},
	booktitle = {2022 {International} {Conference} on {Machine} {Learning}, {Computer} {Systems} and {Security} ({MLCSS})},
	author = {Taru, Uma and Patil, Archana},
	month = aug,
	year = {2022},
	keywords = {Ontologies, Machine learning, ontology, Toxicology, Internet, Oral communication, Social networking (online), Regulation, antonyms, similarity, synonyms, toxicity},
	pages = {241--246},
}

@inproceedings{mendil_non-intrusive_2022,
	title = {Non-{Intrusive} {Annotation}-{Based} {Domain}-{Specific} {Analysis} to {Certify} {Event}-{B} {Models} {Behaviours}},
	doi = {10.1109/APSEC57359.2022.00025},
	abstract = {System engineering advocates a thorough under-standing of the engineering domain or certification standards (aeronautics, railway, medical, etc.) associated to the system under design. In this context, engineering domain knowledge plays a predominant role in system design and/or certification. Furthermore, it is a prerequisite to achieve the effectiveness and performance of the designed system. This article proposes a formal method for describing and setting up domain-specific behavioural analyses. It defines a formal verification technique for dynamic properties entailed by engineering domain knowledge where Event-B formal models are annotated and analysed in a non-intrusive way, i.e. without destructive alteration. This method is based on the formalisation of behavioural properties analyses relying on domain knowledge as an ontology on the one hand and a meta-theory for Event-B on the other hand. The proposed method is illustrated using a critical interactive system.},
	booktitle = {2022 29th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Mendil, Ismail and Rivière, Peter and Ait-Ameur, Yamine and Singh, Neeraj Kumar and Méry, Dominique and Palanque, Philippe},
	month = dec,
	year = {2022},
	note = {ISSN: 2640-0715},
	keywords = {Ontologies, ontology, Certification, Knowledge engineering, Domain knowledge, Event-B, formal methods, refinement, Analytical models, Proposals, behavioural analyses, Interactive systems, proof, Rail transportation},
	pages = {129--138},
}

@inproceedings{mijalcheva_learning_2022,
	title = {Learning {Robust} {Food} {Ontology} {Alignment}},
	doi = {10.1109/BigData55660.2022.10020417},
	abstract = {In today’s knowledge society, large number of information systems use many different individual schemes to represent data. Ontologies are a promising approach for formal knowledge representation and their number is growing rapidly. The semantic linking of these ontologies is a necessary prerequisite for establishing interoperability between the large number of services that structure the data with these ontologies. Consequently, the alignment of ontologies becomes a central issue when building a worldwide Semantic Web. There is a need to develop automatic or at least semi-automatic techniques to reduce the burden of manually creating and maintaining alignments. Ontologies are seen as a solution to data heterogeneity on the Web. However, the available ontologies are themselves a source of heterogeneity. On the Web, there are multiple ontologies that refer to the same domain, and with that comes the challenge of a given graph-based system using multiple ontologies whose taxonomy is different, but the semantics are the same. This can be overcome by aligning the ontologies or by finding the correspondence between their components.In this paper, we propose a method for indexing ontologies as a support to a solution for ontology alignment based on a neural network. In this process, for each semantic resource we combine the graph based representations from the RDF2vec model, together with the text representation from the BERT model in order to capture the semantic and structural features. This methodology is evaluated using the FoodOn and OntoFood ontologies, based on the Food Onto Map alignment dataset, which contains 155 unique and validly aligned resources. Using these limited resources, we managed to obtain accuracy of 74\% and F1 score of 75\% on the test set, which is a promising result that can be further improved in future. Furthermore, the methodology presented in this paper is both robust and ontology-agnostic. It can be applied to any ontology, regardless of the domain.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Mijalcheva, Viktorija and Davcheva, Ana and Gramatikov, Sasho and Jovanovik, Milos and Trajanov, Dimitar and Stojanov, Riste},
	month = dec,
	year = {2022},
	keywords = {Ontologies, Natural language processing, Semantic Web, Semantics, Big Data, Neural networks, Embeddings, Taxonomy, Ontology Alignment, Training, Data linking, Data normalization, Text representation},
	pages = {4097--4104},
}

@inproceedings{hsiao_using_2022,
	title = {Using {Contextual} {Text} {Mining} and {Ontology} {Methods} to {Establish} a {Novel} {Technology} {Trend} and {Associative} {Analysis} {Framework} for {Sustainable} {Energy} {Development} in {Taiwan}},
	doi = {10.1109/BigData55660.2022.10020882},
	abstract = {In 2015, the United Nations proposed 17 Sustainable Development Goals, SDGs, as the guidelines for all countries in the world to promote sustainable development before 2030. Government Research Bulletin (GRB), the research projects and technical reports sponsored by government, which has long-term, numerous, complete research method, technology development and policy analysis information in Taiwan. Therefore, it is an important and effective way to explore SDG-related information from a large amount of GRB text. In this paper, a novel technologies trend and associative analysis framework which uses contextual text mining and ontology methods is proposed and applied to SDG 7, which "Affordable and Clean Energy". First, we integrate dictionary-based method and semantic textual similarity analysis algorithm to obtain a SDG 7 classifier which can exactly and quickly classify a large amount number of GRB text to SDG 7. Then, two major SDG 7 analysis procedures based on the classification results are implemented. One is using contextual text mining algorithm to obtain energy technologies trend information. The other is adopting ontology method to establish energy technologies associative analysis concept map. According to the analysis results mentioned above, we are able to efficiently incorporate the energy technology with long-term trend, energy technology associative information, and the most influential authors on the specify energy technology in order to generate a global strategy for continuous improvement in Taiwan.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Hsiao, Yi-Hao and Chuang, Chia-Yi and Huang, Megn-Chi and Yang, Chia-Lee and Wu, Jyh-Horng},
	month = dec,
	year = {2022},
	keywords = {Ontologies, Ontology, Semantics, Text mining, Big Data, Writing, Market research, Contextual text mining, Government, SBERTs, Sustainable Development Goals (SDGs)},
	pages = {4491--4494},
}

@inproceedings{wullschleger_auto-regressive_2022,
	title = {Auto-{Regressive} {Self}-{Attention} {Models} for {Diagnosis} {Prediction} on {Electronic} {Health} {Records}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147928928&doi=10.1109%2FBigData55660.2022.10020339&partnerID=40&md5=cbff75806de935dcc3174e3308a2f471},
	doi = {10.1109/BigData55660.2022.10020339},
	abstract = {Insufficient data and data lacking the diversity to represent the general public is a common challenge when modelling diagnosis prediction. We consider a much larger and more diverse database of commercial Electronic Health Records than what is prevalent in the literature. We formulate a simplified version of diagnosis prediction that focuses on major developments in medical histories of patients. To this end, we leverage Auto-Regressive Self-Attention models that have seen promising applications in language modelling and extend them to incorporate ontological representations of medical codes. Additionally, we include time-intervals between diagnoses into the attention calculation. We evaluate models and baselines at different levels of diagnostic granularity and our results suggest that using very detailed clinical classifications does not significantly degrade performance, possibly allowing their use in practice. Our model outperforms all baselines and we suggest that leveraging the ontology for generating diagnosis representations is mostly helpful for rare diagnoses.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Wullschleger, Pascal and Lionetti, Simone and Daly, Donnacha and Volpe, Francesca and Caro, Grégoire},
	month = dec,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Ontologies, Ontology, Modeling languages, Big Data, Diagnosis, Electronic health record, eHealth, Forecasting, Data models, Transformer, Databases, History, Attention model, Codes, Predictive models, Diagnosis Prediction, Electronic Health Record, Ontological Representation, Medical computing, Electronic health, Health records, Records management, Auto-regressive, Diagnose prediction, General publics, Ontological representation, Public IS},
	pages = {1950--1956},
	annote = {Cited by: 1},
}

@inproceedings{chen_dl-bert_2022,
	title = {{DL}-{BERT}: a time-aware double-level {BERT}-style model with pre-training for disease prediction},
	doi = {10.1109/BigData55660.2022.10020513},
	abstract = {Disease prediction based on the Electronic Health Record (EHR) is an important task in healthcare. EHR records patients’ every visit by time, and there are many kinds of medical codes within a visit, therefore, EHR has characteristics of temporal irregularity and hierarchical structure. Some recent works employ BERT-style models to process EHR data for disease prediction. However, few of these models can give consideration to capture both the interaction between medical codes and the impact of temporal irregularity. To solve this problem, we propose the Double-Level BERT-style model (DL-BERT). Considering EHR’s hierarchical structure, the model contains a code-level and a visit-level representation layer which can learn the relationship between medical codes and temporal influence respectively. In the code-level representation layer, the model achieves the representation power by employing external medical ontologies to provide multi-resolution information of medical codes and the Transformer to embed medical codes. Besides, the model adopts two pre-training tasks to enhance the ability to capture the link between different kinds of codes. In the visit-level representation layer, DL-BERT utilizes a special time-aware Transformer to model temporal information. And the model adopts a visit-level pre-training task for better learning context information. Experiments are conducted on two real-world healthcare datasets and show that our model outperforms all baselines demonstrating the effectiveness of our model.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Chen, Xianlai and Lin, Jiamiao and An, Ying},
	month = dec,
	year = {2022},
	keywords = {Ontologies, BERT, Big Data, Transformers, Data models, Transformer, medical ontology, Codes, Predictive models, Medical services, Electronic Health Record, pre-train},
	pages = {1801--1808},
}

@inproceedings{parolin_confli-t5_2022,
	title = {Confli-{T5}: {An} {AutoPrompt} {Pipeline} for {Conflict} {Related} {Text} {Augmentation}},
	doi = {10.1109/BigData55660.2022.10020509},
	abstract = {Recent advances in natural language processing (NLP) and Big Data technologies have been crucial for scientists to analyze political unrest and violence, prevent harm, and promote global conflict management. Government agencies and public security organizations have invested heavily in deep learning-based applications to study global conflicts and political violence. However, such applications involving text classification, information extraction, and other NLP-related tasks require extensive human efforts in annotating/labeling texts. While limited labeled data may drastically hurt the models’ performance (over-fitting), large demands on annotation tasks may turn real-world applications impracticable. To address this problem, we propose Confli-T5, a prompt-based method that leverages the domain knowledge from existing political science ontology to generate synthetic but realistic labeled text samples in the conflict and mediation domain. Our model allows generating textual data from the ground up and employs our novel Double Random Sampling mechanism to improve the quality (coherency and consistency) of the generated samples. We conduct experiments over six standard datasets relevant to political science studies to show the superiority of Confli-T5. Our codes are publicly available 1.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio, Javier and D’Orazio, Vito},
	month = dec,
	year = {2022},
	keywords = {Natural language processing, natural language processing, classification, Big Data, Text categorization, Safety, Data models, Pipelines, Standards organizations, CAMEO, coding event data, conflict, generation, text augmentation},
	pages = {1906--1913},
}

@inproceedings{kulagin_ontology-based_2022,
	title = {Ontology-{Based} {Development} of {Domain}-{Specific} {Languages} via {Customizing} {Base} {Language}},
	doi = {10.1109/AICT55583.2022.10013619},
	abstract = {The quality of the systems depends on compliance to the domain requirements. High quality is achieved only with involving experts in the relevant fields to the system design as experts. Modern design methods are based on using professional tools and modeling languages. Using these tools are difficult for domain experts. Domain-Specific Languages (DSLs) can be considered as "user interfaces" for experts because they bridge the gap between the domain experts and the software development tools via customizing modeling languages. Usability of DSLs by domain experts is a key factor for their successful adoption. But DSL creation is challenging task. An approach to DSL customization based on using multifaceted ontology is proposed. General scheme of DSL metamodel generation based on multifaceted ontology is described. Examples of created DSLs and models illustrating the applicability of the proposed method are shown. The DSL metamodels were developed and tested in several domains. The results of experiments confirmed practical significance of the ontology-based approach to DSL creation.},
	booktitle = {2022 {IEEE} 16th {International} {Conference} on {Application} of {Information} and {Communication} {Technologies} ({AICT})},
	author = {Kulagin, Grigory and Ermakov, Ivan and Lyadova, Lyudmila},
	month = oct,
	year = {2022},
	note = {ISSN: 2472-8586},
	keywords = {Ontologies, Software, domain-specific language, DSM, Visualization, DSL, Prototypes, User interfaces, Task analysis, domain-specific modeling, metamodel generation, multifaceted ontology, algorithm description language, GalileoSky, language customization},
	pages = {1--6},
}

@inproceedings{zindel_building_2022,
	title = {Building a {Semantic} {Layer} for {Early} {Design} {Trade} {Studies} in the {Development} of {Commercial} {Aircraft}},
	doi = {10.1109/ISSE54508.2022.10005324},
	abstract = {To improve the adoption of Model-based Systems Engineering (MBSE), data that is distributed across engineering disciplines needs to be made available in an open and descriptive way. This paper describes a new approach to implementing a semantic layer that allows integrating and publishing MBSE data stored in heterogeneous models in a uniform way by means of Semantic Web technologies. The tool-independent views on engineering data provided by the semantic layer enable the implementation of services for accessing, classifying, checking and reuse of federated information. We report on the creation of a common vocabulary in the Ontology Modeling Language (OML) that can be automatically instantiated from distributed models into a knowledge graph. We demonstrate the benefits of our approach using a Systems Modeling Language (SysML) based early design trade study in the aeronautics domain.},
	booktitle = {2022 {IEEE} {International} {Symposium} on {Systems} {Engineering} ({ISSE})},
	author = {Zindel, Andreas and Feo-Arenis, Sergio and Helle, Philipp and Schramm, Gerrit and Elaasar, Maged},
	month = oct,
	year = {2022},
	note = {ISSN: 2687-8828},
	keywords = {Ontologies, Semantic Web, Semantics, Vocabulary, Unified modeling language, Data models, Atmospheric modeling},
	pages = {1--8},
}

@inproceedings{choi_gcl-go_2022,
	title = {{GCL}-{GO}: {A} novel sequence-based hierarchy-aware method for protein function prediction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146642218&doi=10.1109%2FBIBM55620.2022.9994917&partnerID=40&md5=3c3103760b6bccffd29c52ac32885d60},
	doi = {10.1109/BIBM55620.2022.9994917},
	abstract = {Experimental protein functional annotation does not cover rapidly-expanding protein sequences. Sequence-based methods, one of the computational methods, have been developed for extending functional annotations to fast-growing sequence databases. We propose a novel sequence-based hierarchy-aware method, namely GCL-GO. GCL-GO applies a protein language model to represent sequences, applies graph contrastive learning to represent GO terms, and then predicts protein functions by combining these two features. By contrasting the GO graph and semantic features of GO terms, GCL-GO has generalizability and scalability by accurately embedding the features of GO terms while relying less on training data. We also suggest GCL-GO+, which combines a sequence similarity-based method with GCLGO, to improve performance. GCL-GO+ outperforms sequence-based competing methods on both the CAFA3 and the TALE datasets. Furthermore, GCL-GO and GCL-GO+ demonstrate functional generalization and scalability potential by having the best performance on new GO terms or on GO terms annotated infrequently in the training dataset. Our code is available in https://github.com/kch38896/GCL-GO},
	booktitle = {2022 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Choi, Kyudam and Lee, Yurim and Kim, Cheongwon},
	month = dec,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Semantics, Bioinformatics, Gene Ontology, Annotations, Gene ontology, Functional annotation, Training, Databases, Protein language model, Scalability, Computational linguistics, Protein function prediction, Proteins, protein language model, gene ontology, Learning systems, Protein engineering, protein function prediction, graph constructive learning, Protein sequences, Constructive learning, GO terms, Graph constructive learning, Sequence database},
	pages = {51--56},
	annote = {Cited by: 1},
}

@inproceedings{wang_deepdga_2022,
	title = {{deepDGA}: {Biomedical} {Heterogeneous} {Network}-based {Deep} {Learning} {Framework} for {Disease}-{Gene} {Association} {Predictions}},
	doi = {10.1109/BIBM55620.2022.9995651},
	abstract = {Accurate prediction of disease-gene associations is a crucial tissue in the treatment of diseases. Currently, deep learning-based methods have been proposed to determine the associations between diseases and genes. However, previous network-based models do not consider the semantic characteristics of various biomedical entities and suffer from the problems of cold-start. To this end, this study proposes a heterogeneous network-based deep learning framework (termed deepDGA) to predict disease-gene associations. First, a heterogeneous network with four kinds of biological nodes and eight kinds of edges is constructed. Second, we develop a meta path-driven deep Transformer encoder to learn node representations which contains semantic characteristics of nodes in the heterogeneous network. Finally, the inductive matrix completion algorithm that can solve problem of cold-start, is used for disease-gene association prediction. The results of 5-flod cross-validation and top-ranked predictions suggest that deepDGA is superior to other methods. In addition, we further observe that deepDGA performs the highest predictive ability for specific diseases via the literature verification, KEGG human pathway analyses, and GO enrichment analyses. In summary, deepDGA is an effective framework for predicting the diseases-gene associations.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Wang, Hong and Wang, Xiaoqi and Liu, Wenjuan and Xie, Xiaolan and Peng, Shaoliang},
	month = dec,
	year = {2022},
	keywords = {Semantics, Deep learning, Transformers, Pipelines, Biological system modeling, Predictive models, Prediction algorithms, deep Transformer encoder, disease-gene association, heterogeneous network, inductive matrix completion},
	pages = {601--606},
}

@inproceedings{zhao_adaptive_2022,
	title = {Adaptive {Multi}-view {Graph} {Convolutional} {Network} for {Gene} {Ontology} {Annotations} of {Proteins}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146713546&doi=10.1109%2FBIBM55620.2022.9995517&partnerID=40&md5=ac2039ebebc24d2aaae96a61c5280a42},
	doi = {10.1109/BIBM55620.2022.9995517},
	abstract = {Gene Ontology (GO) containing a set of standard concepts (or terms) is launched to unify the functional descriptions of proteins. Developing computational models based on GO to automatically annotate protein functions has been a longstanding active research area. In this paper, we propose a novel method to adaptively fuse functional and topological information between GO Terms. Our method is composed of a pre-trained language model for encoding protein sequences and an adaptive multi-view graph convolutional network (Multi-view GCN) for representing GO terms. Particularly, the Multi-view GCN considers multiple views from functional information, topological structures, and their combinations, and extracts multiple corresponding representations of GO terms. Then, an attention mechanism is applied to adaptively learn the importance weights of these representations. Finally, the predicted scores are calculated by using a dot product between protein sequence features and GO term representations. Experimental results on the datasets of two species (i.e., Human and Yeast) show that our method outperforms other state-of-the-art methods. The code of our proposed method is available at: https://github.com/Candyperfect/Master.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
	month = dec,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Ontologies, Bioinformatics, Deep learning, Gene Ontology, deep learning, Gene ontology, Feature extraction, Protein function prediction, Convolutional neural networks, Computational modeling, Adaptive systems, RNA, Proteins, Topology, Genes, Adaptation models, protein function prediction, Convolution, adaptive multi-view graph convolutional network, Convolutional codes, gene ontology terms, Protein sequences, Gene ontology terms, Convolutional networks, Gene ontology annotations, Multi-views, Adaptive multi-view graph convolutional network, Functional information},
	pages = {90--93},
	annote = {Cited by: 1},
}

@inproceedings{huang_deepfusiongo_2022,
	title = {{DeepFusionGO}: {Protein} function prediction by fusing heterogeneous features through deep learning},
	doi = {10.1109/BIBM55620.2022.9994899},
	abstract = {Exploring the functions of proteins is crucial for explaining cellular mechanisms, treating diseases, and developing new drugs. Due to experimental limitations, large-scale identification of protein function remains a challenging task in cell biology. Here we propose DeepFusionGo, a novel protein function prediction method that adopts a graph representation learning approach (GraphSAGE) to extract features from heterogeneous data sources. First, we generate embeddings from protein sequences using the pre-trained protein language model and InterPro domains with scaling gradient. Then we integrate these two embeddings with adaptive feature weights to the PPI graph and use GraphSAGE to generate the representation vector. Finally, we build the classification model to predict protein function based on the concatenated feature vector. The experimental results show that DeepFusionGO outperforms existing state-of-the-art methods, including sequence-based DeepGOPLUS, and PPI-based DeepGraphGO. DeepFusionGO also performs well in difficult protein function prediction. We demonstrate that selecting an appropriate protein features fusion method can improve the prediction performance, and using the PPI network and the protein representation vector obtained from the protein language model through the GraphSAGE algorithm is an effective way to mine potential functional clues. The source code and data sets are available at: https://github.com/Hhhzj-7/DeepFusionGO.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Huang, Zhijian and Zheng, Rongtao and Deng, Lei},
	month = dec,
	year = {2022},
	keywords = {Representation learning, Protein function prediction, Proteins, Soft sensors, feature fusion, Adaptation models, Biological system modeling, Predictive models, Source coding, graph representation learning, GraphSAGE},
	pages = {12--17},
}

@inproceedings{japa_question_2022,
	title = {Question {Answering} over {Knowledge} {Base} with {Variational} {Auto}-{Encoder}},
	doi = {10.1109/BigMM55396.2022.00012},
	abstract = {Knowledge Base is a semantic data repos-itory made available over the web. Knowledge bases are multi-relational graphs consisting of entities and relations representing facts about the world. These facts follow a controlled vocabulary that drives the knowledge base, often called ontology. Entities are the graph nodes, while relations are the edges connecting the nodes. While knowledge base attention prolifer-ates, extracting information from these resources is challenging. One of the promising approaches is Question Answering systems. The Question Answering over Knowledge Base(KB-QA) system aims to provide a natural language interface to pose a query to the KB. QA systems use embeddings to map the posed Question to the facts stored in the knowledge bases. QA systems can be effectively improved using large transformer models. These transformer models are computationally expensive, which prevents their use in real-world applications. To solve this, we started our study with the use-case of Knowledge Base Question Answering systems. In KB-QA systems, representing the entities and relationships play a pivotal role in extracting the answer to the posed query. Traditional KB embedding techniques represent entities/relations as vectors, mapping them in different semantic spaces and ignoring the subtle cor-relation. A pre-training language model for encoding the KB facts will preserve the semantic and contextual correlation. While learning linguistic knowledge, these models also store relational knowledge in the training data. These Language Models are often trained on a massive corpus addressing the Out-Of- Vocabulary problem. In this paper, we incorporate a co-embedding model for knowledge base embedding, which learns low-dimensional representations of entities and relations in the same semantic space. We propose a Variational Auto-Encoder, an efficient transformer architecture that represents knowledge base representations as Gaussian distributions to address the issue of neglecting uncertainty. In addition, our method has high quality and interpretability advantages compared with previous methods. Our experimental results show the effectiveness and the superiority of the VAE approach for question-answering systems on knowledge bases over other well-known pre-trained embedding methods.},
	booktitle = {2022 {IEEE} {Eighth} {International} {Conference} on {Multimedia} {Big} {Data} ({BigMM})},
	author = {Japa, Sai Sharath and Green, Sarah},
	month = dec,
	year = {2022},
	keywords = {Semantics, Uncertainty, Transformers, Vocabulary, Knowledge based systems, Language Model, Computational modeling, Bert, Training data, Encoder, KBQA, knowledge base question answering, Multi-Head Attention, VAE},
	pages = {29--36},
}

@inproceedings{chen_selection_2022,
	title = {Selection {Method} of {Fuzzy} {Semantics} in {Machine} {Translation} and the {Integration} of {LBP} {Algorithm}},
	doi = {10.1109/I-SMAC55078.2022.9987258},
	abstract = {This paper studies the accuracy and rationality of machine English translation based on the LBP algorithm, and proposes a machine English translation method based on the selection of the optimal solution of fuzzy semantics. Construct an information extraction model for machine English translation, establish a fuzzy semantic topic word attribute table for machine English translation, and use phrases as the basic granularity to produce paraphrase results that are semantically consistent with the translation hypothesis set. Extract phrase paraphrase resources by using massively parallel corpus. Experimental test results show that using this method for machine English translation improves the recall performance of semantic information by 6.7\%, and the feature matching degree of topic words is higher.},
	booktitle = {2022 {Sixth} {International} {Conference} on {I}-{SMAC} ({IoT} in {Social}, {Mobile}, {Analytics} and {Cloud}) ({I}-{SMAC})},
	author = {Chen, Jun},
	month = nov,
	year = {2022},
	note = {ISSN: 2768-0673},
	keywords = {Ontologies, Semantics, Information retrieval, Feature extraction, Machine translation, Machine Translation, Fuzzy Semantics, Analytical models, Coherence, LBP Algorithm, Selection Method},
	pages = {963--966},
}

@inproceedings{vasantharajan_medbert_2022,
	title = {{MedBERT}: {A} {Pre}-trained {Language} {Model} for {Biomedical} {Named} {Entity} {Recognition}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146303653&doi=10.23919%2FAPSIPAASC55919.2022.9980157&partnerID=40&md5=f6cceacbfce834cc156367a72b5018ce},
	doi = {10.23919/APSIPAASC55919.2022.9980157},
	abstract = {This paper introduces MedBERT, a new pre-trained transformer-based model for biomedical named entity recognition. MedBERT is trained with 57.46M tokens collected from biomedical-related data sources, i.e. datasets acquired from N2C2, BioNLP, CRAFT challenges, and biomedical-related articles crawled from Wikipedia. We validate the effectiveness of MedBERT by comparing it with four publicly available pre-trained models on ten biomedical datasets from BioNLP and CRAFT shared tasks. Our experimental results show that models fine-tuned on MedBERT achieve state-of-the-art performance in nine datasets that predict Protein, Gene, Chemical, Cellular/Component, Gene Ontology, and Taxonomy entities. Specifically, the model achieved an average of 84.04\% F1-micro score on ten test sets from BioNLP and CRAFT challenges with an improvement of 3.7\% and 7.83\% as compared to models that were fine-tuned on BioBERT and Bio\_ClinicalBERT, respectively.},
	booktitle = {2022 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Vasantharajan, Charangan and Tun, Kyaw Zin and Thi-Nga, Ho and Jain, Sparsh and Rong, Tong and Siong, Chng Eng},
	month = nov,
	year = {2022},
	note = {ISSN: 2640-0103},
	keywords = {Ontologies, Language model, Gene Ontology, Gene ontology, Transformers, Taxonomy, Wikipedia, Biomedical named entity recognition, Proteins, Genes, Biological system modeling, Predictive models, Protein engineering, Test sets, Data-source, Protein genes, Cellular components, State-of-the-art performance},
	pages = {1482--1488},
	annote = {Cited by: 16; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{filgueira_frances_2022,
	title = {frances: {A} {Deep} {Learning} {NLP} and {Text} {Mining} {Web} {Tool} to {Unlock} {Historical} {Digital} {Collections}: {A} {Case} {Study} on the {Encyclopaedia} {Britannica}},
	doi = {10.1109/eScience55777.2022.00038},
	abstract = {This work presents frances, an integrated text mining tool that combines information extraction, knowledge graphs, NLP, deep learning, parallel processing and Semantic Web techniques to unlock the full value of historical digital textual collections, offering new capabilities for researchers to use powerful analysis methods without being distracted by the technology and middleware details. To demonstrate these capabilities, we use the first eight editions of the Encyclopaedia Britannica offered by the National Library of Scotland (NLS) as an example digital collection to mine and analyse. We have developed novel parallel heuristics to extract terms from the original collection (alongside metadata), which provides a mix of unstructured and semi-structured input data, and populated a new knowledge graph with this information. Our Natural Language Processing models enable frances to perform advanced analyses that go significantly beyond simple search using the information stored in the knowledge graph. Furthermore, frances also allows for creating and running complex text mining analyses at scale. Our results show that the novel computational techniques developed within frances provide a vehicle for researchers to formalize and connect findings and insights derived from the analysis of large-scale digital corpora such as the Encyclopaedia Britannica.},
	booktitle = {2022 {IEEE} 18th {International} {Conference} on e-{Science} (e-{Science})},
	author = {Filgueira, Rosa},
	month = oct,
	year = {2022},
	keywords = {Semantic Web, Text mining, knowledge graphs, natural language processing, text mining, Deep learning, Information retrieval, Knowledge engineering, semantic web, Metadata, information extraction, digital tools, deep transfer learning, historical digital textual collections, parallel computing, Parallel processing, web tools},
	pages = {246--255},
}

@inproceedings{cahyaningsih_commonkads_2022,
	title = {{COMMONKADS} for {Knowledge} {Based} {System} {Development}: {A} {Literature} {Study}},
	doi = {10.1109/ICITSI56531.2022.9970920},
	abstract = {COMMONKADS is a method for developing knowledge-based system. This method describes foundation, technique, modeling language and document structure for develop the knowledge-based system. COMMONKADS is people-oriented system development methodology, and this methodology is often used for developing organizational knowledge management system. COMMONKADS approach is divided based on context (organizational model, task model, agent model), concept (knowledge model) and artifact (design model). COMMONKADS have been used widely for knowledge-based system in several fields, such as COMMONKADS that integrated in tourism knowledge-based system, COMMONKADS for irrigation expert system, expertise model using COMMONKADS in manufactured company, COMMONKADS in energy management system and many more. Generally, there are eight strengths of COMMONKADS methodology for develop knowledge-based system. Its strength is flexible to use in any scope, represent knowledge (organizational, domain, task and inference knowledge), complete (representation, model, and form), powerful, accurate, comprehensive, represent KM process, systematic and effective. While the weakness of COMMONKADS methodology only three, there are don't have validation process and difficult to acquisition knowledge and use semi formal language, large data storage. Nevertheless, COMMONKADS is recommended methodology for develop knowledge-based system.},
	booktitle = {2022 {International} {Conference} on {Information} {Technology} {Systems} and {Innovation} ({ICITSI})},
	author = {Cahyaningsih, Elin and Silalahi, Natascha Lestari Eunike and Rohajawati, Siti and Avianti, Yuliza Maulina},
	month = nov,
	year = {2022},
	keywords = {Software, Knowledge based systems, Technological innovation, methodology, knowledge-based engineering, Memory, Systematics, Regulation, Government, COMMONKADS, knowledge-based system, strength of COMMONKADS, weakness of COMMONKADS},
	pages = {213--218},
}

@inproceedings{li_petroleum_2022,
	title = {Petroleum {Exploration} and {Development} {Text} {Triplet} {Extraction} {Based} on {Deep} {Learning}},
	doi = {10.1109/IALP57159.2022.9961280},
	abstract = {The petroleum exploration and development industry is moving from digital to intelligent. Under the guidance of AI, machine reading and automatic extraction of petroleum exploration and development knowledge are needed for unstructured datas. There are a large number of long entities and complex nested entities in petroleum exploration and development text, which increase the challenge of petroleum exploration and development triplet extraction task. To solve the above problems, (1) The ALBERT-BiLSTM-Attention-CRF method based on large-scale pre-trained Chinese language model is used to extract text triples for Petroleum exploration and development. (2) The ALBERT-BiGRU-Attention method is used to carry out text dichotomies to judge whether triplet extraction is effective. By collecting the datas of Petroleum exploration and development, the corpus of Petroleum exploration and development is established. Secondly, knowledge composition is analyzed and corpus is annotated under the guidance of Petroleum exploration and development theory. Finally, the deep learning training of Petroleum exploration and development descriptive corpus knowledge extraction is carried out. The experimental results show that the accuracy of SPO triplet named entity recognition is 85.49\%. The recognition accuracy of extracted triples is 95.90\%, which can achieve good recognition effect in small-scale Petroleum exploration and development corpus.},
	booktitle = {2022 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	author = {Li, Jianli and Yilahun, Hankiz and Hamdulla, Askar},
	month = oct,
	year = {2022},
	keywords = {Ontologies, Semantics, Deep learning, Geology, Training, Triplet extraction, Text recognition, Industries, ALBERT-BiGRU-Attention, ALBERT-BiLSTM-Attention-CRF, Petroleum Exploration and Development},
	pages = {225--230},
}

@article{borrego_completing_2022,
	title = {Completing {Scientific} {Facts} in {Knowledge} {Graphs} of {Research} {Concepts}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3220241},
	abstract = {In the last few years, we have witnessed the emergence of several knowledge graphs that explicitly describe research knowledge with the aim of enabling intelligent systems for supporting and accelerating the scientific process. These resources typically characterize a set of entities in this space (e.g., tasks, methods, evaluation techniques, proteins, chemicals), their relations, and the relevant actors (e.g., researchers, organizations) and documents (e.g., articles, books). However, they are usually very partial representations of the actual research knowledge and may miss several relevant facts. In this paper, we introduce SciCheck, a new triple classification approach for completing scientific statements in knowledge graphs. SciCheck was evaluated against other state-of-the-art approaches on seven benchmarks, yielding excellent results. Finally, we provide a real-world use case and applied SciCheck to the Artificial Intelligence Knowledge Graph (AI-KG), a large-scale automatically-generated open knowledge graph including 1.2M statements extracted from the 333K most cited articles in the field of Artificial Intelligence, and generated a new version of this knowledge graph with 300K additional triples.},
	journal = {IEEE Access},
	author = {Borrego, Agustín and Dessì, Danilo and Hernández, Inma and Osborne, Francesco and Reforgiato Recupero, Diego and Ruiz, David and Buscaldi, Davide and Motta, Enrico},
	year = {2022},
	keywords = {Knowledge graphs, Semantic Web, machine learning, Machine learning, semantic web, Feature extraction, Knowledge based systems, Context modeling, Computational modeling, knowledge graph completion, science of science, Benchmark testing, Task analysis, triple classification},
	pages = {125867--125880},
}

@inproceedings{ji_identifying_2022,
	title = {Identifying {Inconsistencies} in the {Design} of {Large}-scale {Casting} {Systems} – {An} {Ontology}-based {Approach}},
	doi = {10.1109/CASE49997.2022.9926710},
	abstract = {The development of modern automated production systems requires the close cooperation of engineers from different domains. Due to the large amount of domain-specific documents and heterogeneous data they create during the multidisciplinary engineering activities, ensuring the consistency of information is always challenging. Since most of these documents are texted-based and lack a standardized structure, extracting required information from these files is oftentimes problematic. This issue is particularly critical in the development of large-scale production plants due to the high complexity of the systems and the diversity of disciplines involved. To help engineers efficiently utilize unstructured data sources as well as identify potential information contradictions, we propose an ontology-based inconsistency management approach for large-scale production systems that generates the knowledge base from unstructured engineering data and (semi-) automatically detects multiple types of inconsistencies. In addition, the presented framework also supports the tracking of information changes during the system design process.},
	booktitle = {2022 {IEEE} 18th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Ji, Fan and Ocker, Felix and Zou, Minjie and Vogel-Heuser, Birgit and Oligschläger, Marius},
	month = aug,
	year = {2022},
	note = {ISSN: 2161-8089},
	keywords = {Automation, Knowledge based systems, Production systems, Complexity theory, Soft sensors, Computer aided software engineering, Casting},
	pages = {319--325},
}

@inproceedings{kocher_modeling_2022,
	title = {Modeling and {Executing} {Production} {Processes} with {Capabilities} and {Skills} using {Ontologies} and {BPMN}},
	doi = {10.1109/ETFA52439.2022.9921564},
	abstract = {Current challenges of the manufacturing industry require modular and changeable manufacturing systems that can be adapted to variable conditions with little effort. At the same time, production recipes typically represent important company know-how that should not be directly tied to changing plant configurations. Thus, there is a need to model general production recipes independent of specific plant layouts. For execution of such a recipe however, a binding to then available production resources needs to be made. In this contribution, we select a suitable modeling language to model and execute such recipes. Furthermore, we present an approach to solve the issue of recipe modeling and execution in modular plants using semantically modeled capabilities and skills as well as BPMN. We make use of BPMN to model production recipes using capability processes, i.e. production processes referencing abstract descriptions of resource functions. These capability processes are not bound to a certain plant layout, as there can be multiple resources fulfilling the same capability. For execution, every capability in a capability process is replaced by a skill realizing it, effectively creating a skill process consisting of various skill invocations. The presented solution is capable of orchestrating and executing complex processes that integrate production steps with typical IT functionalities such as error handling, user interactions and notifications. Benefits of the approach are demonstrated using a flexible manufacturing system.},
	booktitle = {2022 {IEEE} 27th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Köcher, Aljosha and Da Silva, Luis Miguel Vieira and Fay, Alexander},
	month = sep,
	year = {2022},
	keywords = {Ontologies, Semantic Web, BPMN, Skills, Production, Adaptation models, Layout, Companies, Capabilities, Flexible manufacturing systems, Manufacturing industries, Orchestration, Skill-Based Production},
	pages = {1--8},
}

@inproceedings{rybinski_beyond_2022,
	title = {Beyond {Low}-{Code} {Development}: {Marrying} {Requirements} {Models} and {Knowledge} {Representations}},
	doi = {10.15439/2022F129},
	abstract = {Typical Low-Code Development platforms enable model-driven generation of web applications from high-level visual notations. They normally express the UI and the application logic, which allows generating the frontend and basic CRUD operations. However, more complex domain logic (data processing) operations still necessitate the use of traditional programming. This paper presents a visual language, called RSL-DL, to represent domain knowledge with complex domain rules aligned with requirements models. The language synthesises and extends approaches found in knowledge representation (ontologies) and software modelling language engineering. Its purpose is to enable a fully automatic generation of domain logic code by reasoning over and reusing domain knowledge. The language’s abstract syntax is defined using a meta-model expressed in MOF. Its semantics is expressed with several translational rules that map RSL-DL models onto typical programming language constructs. The rules are explained informally in natural language and formalised using a graphical transformation notation. It is also supported by introducing an inference engine that enables processing queries to domain models and selecting appropriate invocations to generated code. The presented language was implemented by building a dedicated model editor and transformation engine. It was also initially validated through usability studies. Based on these results, we conclude that declarative knowledge representations can be successfully used to produce imperative back-end code with non-trivial logic.},
	booktitle = {2022 17th {Conference} on {Computer} {Science} and {Intelligence} {Systems} ({FedCSIS})},
	author = {Rybiński, Kamil and Śmiałek, Michał},
	month = sep,
	year = {2022},
	keywords = {Ontologies, Semantics, Visualization, Computational modeling, Programming, Codes, Syntactics},
	pages = {919--928},
}

@inproceedings{pradeepani_adding_2022,
	title = {Adding {Commonsense} to {Robotic} {Application} {Using} {Ontology}-{Based} {Model} {Retraining}},
	volume = {5},
	doi = {10.1109/SCSE56529.2022.9905090},
	abstract = {In terms of the level of technological capability in the world today, the use of automated robotics is common in various fields. There are large projects going on in many industries that collaborate between robots and other robots, as well as humans and robots. In hospital environments, care for people with medical needs and their needs and used to make appropriate suggestions to their problems. Robots can also be found in certain areas that can respond quickly as an emergency rescue agent. Furthermore, robots, which can be seen in the hotel industry as waiters and as farm assistants in agriculture, have a great tendency to be used as multi-tasking agents in many fields. In each of these areas, robots must co-operate with humans. In that situation, the importance of the exchange of mutual knowledge between robots-robots and between humans-robots comes into the picture. What matters here is not only the quantitative vastness of knowledge but also the ability to understand each other in the same medium. Although the common sense that people need in their day-to-day work is completely obvious to humans, the commonsense knowledge domain needs to be implanted in robots. Whatever concept is defined for adding commonsense to robotics, it should be a consistent concept that can be logically constructed so that it can be understood by a machine. As will be discussed later in the paper, different methods have been used in various related works to add a different kind of domain knowledge to robotics. The objective of this paper is to provide an improved retrained model for robotics in order to give them the ability to act more human-like when performing tasks. By using the proposed model robots are able to answer the incomplete command or inquiries related to a given context. One of the objectives of this work is to use the ontology-based, commonsense-support existing knowledge base as a mechanism to retrain and build a new model.},
	booktitle = {2022 {International} {Research} {Conference} on {Smart} {Computing} and {Systems} {Engineering} ({SCSE})},
	author = {Pradeepani, M. K. T. and Jayawardena, C. and Rajapaksha, U. U. S.},
	month = sep,
	year = {2022},
	note = {ISSN: 2613-8662},
	keywords = {BERT, Sensors, Knowledge based systems, Service robots, Training, robotics, Robot sensing systems, transfer learning, Adaptation models, Hardware, commonsense},
	pages = {157--164},
}

@inproceedings{lin_trans-sblgcn_2022,
	title = {Trans-{SBLGCN}: {A} {Transfer} {Learning} {Model} for {Event} {Logic} {Knowledge} {Graph} {Construction} of {Fault} {Diagnosis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140756470&doi=10.1109%2FIJCNN55064.2022.9892075&partnerID=40&md5=2a5f27922df5f74a673f6496463c1200},
	doi = {10.1109/IJCNN55064.2022.9892075},
	abstract = {Taking fault diagnosis corpus as the research object, an event logic knowledge graph construction method is proposed in this paper. Firstly, we propose a data labeling strategy based on a constructed event logic ontology model, then collect large-scale robot transmission system fault diagnosis corpus, and label part of the data according to the strategy. Secondly, we propose a transfer learning model called Trans-SBLGCN for event argument entity and event argument relation joint extraction. A language model is trained based on large-scale unlabeled fault diagnosis corpus and transferred to a model based on stacked bidirectional long short term memory (BiLSTM) and bidirectional graph convolutional network (BiGCN). Experimental results show that the method is superior to other methods. Finally, an event logic knowledge graph of robot transmission system fault diagnosis is constructed to provide decision support for autonomous robot transmission system fault diagnosis.},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Lin, RuiMing and Cheng, LiangLun and Wang, Tao and Deng, Jianfeng},
	month = jul,
	year = {2022},
	note = {ISSN: 2161-4407},
	keywords = {Ontologies, Knowledge graphs, Decision support systems, Knowledge engineering, Fault Diagnosis, Neural networks, Fault diagnosis, Entropy, Failure analysis, Data models, Transfer learning, Extraction, Fault detection, Event logic knowledge graph, Robots, Labeling, Learning systems, BiGCN, Event Logic Knowledge Graph, Knowledge Joint Extraction, Faults diagnosis, Computer circuits, System faults, Convolutional networks, Bidirectional graph convolutional network, Event logic, Knowledge joint extraction, Transmission systems},
	pages = {1--8},
	annote = {Cited by: 2},
}

@inproceedings{rogachev_automation_2022,
	title = {Automation of {Architecture} {Justification} and {Parameters} {Selection} of {Artificial} {Neural} {Networks} for {Intelligent} {Detection} of {Cyber}-{Physical} {Threats}},
	doi = {10.1109/RusAutoCon54946.2022.9896311},
	abstract = {The problems of improving the quality of training of deep artificial neural networks (ANN) for various applied tasks require automatization of the selection of hyperparameters of neural networks. The KerasTuner software toolkit can be used to automate the search for optimal values of ANN hyperparameters. It includes random search methods, Bayesian optimization, etc. The formation of training text samples for neural network identification of cyber-physical threats is a separate scientific and methodological task. The complexity of the problem is due to the diversity of the ontology of the key terms of the cyberphysical thesaurus, the variety of styles of lexicological content, as well as the partial intersection of the content of previously identified ontological categories. In the process of experimental study of hyperparameters of deep ANNs being developed, models of “embedding”, “bag of words” and dense vector representation in Python were compared. On the basis of a systematic approach, an information-morphological matrix of thematic blocks is constructed. In the conducted experiments, the values of parameters such as the number of convolutional blocks, the number of their filters, the type of activation functions, the parameters of the “dropout” layers, etc. were changed. The studied tools provided optimization of hyperparameters of the convolutional network, while the calculation time on the Colaboratory platform for the studied ANN architectures using GPU graphics accelerators was 5…9 o’clock. The developed modified algorithm for computer detection of cyberphysical threats in electronic resources allowed to substantiate alternative architectures and optimize the main hyperparameters of ANN.},
	booktitle = {2022 {International} {Russian} {Automation} {Conference} ({RusAutoCon})},
	author = {Rogachev, Aleksey and Melikhova, Elena},
	month = sep,
	year = {2022},
	keywords = {Ontologies, Automation, Cyber-physical systems, Training, Artificial neural networks, Computer architecture, artificial neural network, cyber-physical threat, Graphics processing units, hyperparameter, intelligent detection},
	pages = {908--912},
}

@inproceedings{dudija_identification_2022,
	title = {Identification of {Extraversion} and {Neuroticism} {Personality} {Dimensions} {Using} {IndoBERT}’s {Deep} {Learning} {Model}},
	doi = {10.1109/IAICT55358.2022.9887476},
	abstract = {Human resources are essential for the business organization to adapt to change. Identifying the personality dimensions of new talent could help recruiters conduct the selection process of matching skilled talent to the organization’s needs. The objective of this study is to identify the personality dimensions corresponding to the job need, which correlates with extraversion and neuroticism. The legacy methodology to determine personality dimensions is through interviews or questionnaire surveys, but this process is costly and takes longer time to complete. This paper proposes a work on a person personality identification based on social media text as a complementary methodology. We utilize the textual data to support identifying new talent personality dimensions. In this study, we use IndoBERT model to capture person personality dimension based on their post on Twitter social media. As a result, our model achieves 96\% accuracy in identifying extraversion and neuroticism personality dimensions. We also compare our result with the previous work based on the ontology model.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Industry} 4.0, {Artificial} {Intelligence}, and {Communications} {Technology} ({IAICT})},
	author = {Dudija, Nidya and Natalia, Lezia and Alamsyah, Andry and Romadhony, Ade},
	month = jul,
	year = {2022},
	keywords = {Ontologies, Deep learning, Deep Learning, Blogs, Social networking (online), Costs, Communications technology, Fourth Industrial Revolution, Human Resource, IndoBERT, Personality Identification Dimension, Talent Selection},
	pages = {155--159},
}

@inproceedings{faramarzi_combining_2022,
	title = {Combining {Attention}-based {Models} with the {MeSH} {Ontology} for {Semantic} {Textual} {Similarity} in {Clinical} {Notes}},
	doi = {10.1109/ICHI54592.2022.00023},
	abstract = {In this study, we present several transformer-based models as well as traditional machine learning methods to detect semantic textual similarity (STS) in clinical notes. We investigate transformer models pretrained on general English as well as clinical notes, and use generic English STS datasets as a supplemental corpus to clinical notes data. Our work is based on the 2019 National NLP Clinical Challenge (n2c2). We identify and annotate six types of sentences in the clinical notes corpus, and report an ensemble method that combines attention-based contextualized embeddings with a similarity score based on the MeSH ontology obtained by computing least common ancestors of clinical terms. Our approach does not need additional clinical data for model training, while still achieving comparable Pearson's correlation coefficient of 0.901.},
	booktitle = {2022 {IEEE} 10th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {Faramarzi, Noushin Salek and Dara, Akanksha and Banerjee, Ritwik},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-2634},
	keywords = {Ontologies, Semantics, Machine learning, Natural Language Processing, Transformers, Vocabulary, Training, Electronic Health Records, Computational modeling, Drugs, MeSH, Clinical Semantic Textual Similarity},
	pages = {74--83},
}

@inproceedings{zheng_graph-augmented_2022,
	title = {Graph-{Augmented} {Cyclic} {Learning} {Framework} for {Similarity} {Estimation} of {Medical} {Clinical} {Notes}},
	doi = {10.1109/ICHI54592.2022.00026},
	abstract = {Semantic textual similarity (STS) in the clinical domain helps improve diagnostic efficiency and produce concise texts for downstream data mining tasks. However, given the high degree of domain knowledge involved in clinic text, it remains challenging for general language models to infer implicit medical relationships behind clinical sentences and output similarities correctly. In this paper, we present a graph-augmented cyclic learning framework for similarity estimation in the clinical domain. The framework can be conveniently implemented on a state-of-art backbone language model, and improve its performance by leveraging domain knowledge through co-training with an auxiliary graph convolution network (GCN) based network. We report the success of introducing domain knowledge in GCN and the co-training framework by improving the Bio-clinical BERT baseline by 16.3\% and 27.9\%, respectively.},
	booktitle = {2022 {IEEE} 10th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {Zheng, Can and Wang, Yanshan and Jia, Xiaowei},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-2634},
	keywords = {Ontologies, Semantics, BERT, Knowledge engineering, clinical notes, Estimation, graph neural networks, Manuals, Biological system modeling, Medical services},
	pages = {97--103},
}

@inproceedings{dordiuk_natural_2022,
	title = {Natural language processing for clusterization of genes according to their functions},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138478040&doi=10.1109%2FCSGB56354.2022.9865330&partnerID=40&md5=6c4eac4e8e9a5b8004ace8541bc5ae8b},
	doi = {10.1109/CSGB56354.2022.9865330},
	abstract = {There are hundreds of methods for analysis of data obtained in mRNA-sequencing. The most of them are focused on small number of genes. In this study, we propose an approach that reduces the analysis of several thousand genes to analysis of several clusters. The list of genes is enriched with information from open databases. Then, the descriptions are encoded as vectors using the pretrained language model (BERT) and some text processing approaches. The encoded gene function pass through the dimensionality reduction and clusterization. Aiming to find the most efficient pipeline, 180 cases of pipeline with different methods in the major pipeline steps were analyzed. The performance was evaluated with clusterization indexes and expert review of the results.},
	booktitle = {2022 {Ural}-{Siberian} {Conference} on {Computational} {Technologies} in {Cognitive} {Science}, {Genomics} and {Biomedicine} ({CSGB})},
	author = {Dordiuk, Vladislav and Demicheva, Ekaterina and Espino, Fernando Polanco and Ushenin, Konstantin},
	month = jul,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Natural language processing, Semantics, BERT, natural language processing, Genomics, Gene Ontology, Gene ontology, Semantic analysis, Databases, Dimensionality reduction, Gene expression, gene expression, Language processing, Cognitive science, gene ontology, semantic analysis, Natural languages, Pipelines, Text processing, Bit error rate, clusterization, differential gene expression analysis, Natural language processing systems, Gene expression analysis, Clusterization, Differential gene expression analyse, Differential gene expressions, Genes expression},
	pages = {1--4},
	annote = {Cited by: 1},
}

@inproceedings{ataei_predicting_2022,
	title = {Predicting the specific substrate for transmembrane transport proteins using {BERT} language model},
	doi = {10.1109/CIBCB55180.2022.9863051},
	abstract = {Transmembrane transport proteins play a vital role in cells' metabolism by the selective passage of substrates through the cell membrane. Metabolic network reconstruction requires transport reactions that describe the specific substrate transported as well as the metabolic reactions of enzyme catalysis. In this paper, we apply BERT (Bidirectional Encoder Representations from Transformers) language model for protein sequences to predict one of 12 specific substrates. Our UniProt-ICAT-100 dataset is automatically constructed from UniProt using the ChEBI and GO ontologies to identify 4,112 proteins transporting 12 inorganic anion or cation substrates. We classified this dataset using three different models including Logistic Regression with an MCC of 0.81 and accuracy of 97.5\%; Feed-forward Neural Networks classifier with an MCC of 0.88 and accuracy of 98.5\%. Our third model utilizes a Fine-tuned BERT language model to predict the specific substrate with an MCC of 0.95 and accuracy of 99.3\% on an independent test set.},
	booktitle = {2022 {IEEE} {Conference} on {Computational} {Intelligence} in {Bioinformatics} and {Computational} {Biology} ({CIBCB})},
	author = {Ataei, Sima and Butler, Gregory},
	month = aug,
	year = {2022},
	keywords = {Gene Ontology, BERT model, Neural networks, Classification, Computational modeling, Proteins, Biological system modeling, Predictive models, Bit error rate, Cells (biology), ChEBI ontology, Specific substrate Prediction, Transport protein},
	pages = {1--8},
}

@article{silega_transformation_2022,
	title = {Transformation {From} {CIM} to {PIM}: a {Systematic} {Mapping}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3201556},
	abstract = {Model Driven Architecture (MDA) is the most prominent and accepted methodology based on the Model Driven Development (MDD) principles. MDA includes three abstraction levels: Computer Independent Models (CIM), Platform Independent models (PIM) and Platform specific models (PSM). MDA encourages the automatic transformation of models as a means to increase the speed of the software development process and to prevent human errors. There are plenty of solutions to transform PIMs to PSMs, however the CIM to PIM transformation does not receive a similar attention. In that sense, this paper aims to describe a systematic mapping to analyze the main characteristics of the approaches that deal with the CIM to PIM transformation as well as to discuss research directions stemming out from our analysis. The results of this mapping study could be a valuable information source for the scientific community in order to know the real advances in this topic and to avoid unnecessary effort dealing with problems that have already been addressed. For example, this study yielded the models at the CIM level that have already been transformed into models at the PIM level. Hence, with this information, the researchers could focus their attention on finding solutions to transform those models at CIM level that have not been transformed into models at PIM level. Likewise, this mapping study provides information regarding the technological support of this type of transformation. This information could be useful for those software projects interested to adopt MDA.},
	journal = {IEEE Access},
	author = {Silega, Nemury and Noguera, Manuel and Rogozov, Yuri I. and Lapshin, Vyacheslav S. and González, Tahumara},
	year = {2022},
	keywords = {Software engineering, Software, Mathematical models, Computational modeling, Internet, Business, systematic mapping, Systematics, computer independent models (CIM), Model driven architecture (MDA), platform independent models (PIM)},
	pages = {90857--90872},
}

@inproceedings{morine_comprehensive_2022,
	title = {A {Comprehensive} and {Holistic} {Health} {Database}},
	doi = {10.1109/ICDH55609.2022.00039},
	abstract = {Health and the initiation, progression, and outcome of disease are the result of multiple environmental factors interacting with individual genetic makeups. Collectively, results from primary clinical research on health and disease represent the most compendious and reliable source of actionable knowledge on strategies to optimize health. However, the dispersal of this information as unstructured data, distributed across millions of documents, is a substantial challenge in bridging the gap between primary research and concrete recommendations for improving health. Described here is the development and implementation of a machine reading pipeline that builds a knowledge graph of causal relationships between a broad range of predictive/modifiable diet and lifestyle factors and health outcomes, extracted from the vast biomedical corpus in the National Library of Medicine.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Digital} {Health} ({ICDH})},
	author = {Morine, Melissa J. and Priami, Corrado and Coronado, Edith and Haber, Juliana and Kaput, Jim},
	month = jul,
	year = {2022},
	keywords = {Semantics, Text mining, knowledge graphs, natural language processing, Genetics, Pipelines, Systematics, Libraries, Environmental factors, Healthware},
	pages = {202--207},
}

@inproceedings{rudwan_ontology_2022,
	title = {Ontology {Reuse}: {Neural} {Network}-{Based} {Measurement} of {Concepts} {Representations} and {Similarities} in {Ontology} {Corpus}},
	doi = {10.1109/icABCD54961.2022.9856059},
	abstract = {Ontologies are the heart of the semantic web. They are designed to be reused in web applications. This paper aims to discover a given concept's representation against existing ontologies in a corpus, and if the concept is represented, other similar concepts and terms to it are extracted. A corpus formed of several ontologies in the agricultural domain was constructed. SPARQL queries were used to extract the required data from existing ontologies. And a machine learning technique, the Word2Vec, was employed for ontology reuse process to measure concepts similarity against the existing ontologies. The experimental results showed that the proposed methodology successfully detected previously seen vocabularies during the training on the data in the ontology corpus, and retrieved other similar concepts from the ontologies as well as their degree of similarity (Cosin similarity). Furthermore, the proposed model could process over two million terms in around one minute, reflecting its effectiveness in this context. The proposed method would be useful to ontology and knowledge engineers to conduct a preliminary investigation about which existing ontologies are suitable for reuse in the process of developing new ontologies. Other applications of the proposed method may include ontology alignment to measure the degree of similarity between existing ontologies.},
	booktitle = {2022 {International} {Conference} on {Artificial} {Intelligence}, {Big} {Data}, {Computing} and {Data} {Communication} {Systems} ({icABCD})},
	author = {Rudwan, Mohammed Suleiman Mohammed and Fonou-Dombeu, Jean Vincent},
	month = aug,
	year = {2022},
	keywords = {Ontologies, Semantic Web, Machine learning, NLP, Knowledge engineering, Vocabulary, Training, artificial neural networks, Heart, Word2Vec, automated ontology reuse},
	pages = {1--8},
}

@inproceedings{yiming_research_2022,
	title = {Research on the {Construction} of {Maritime} {Legal} {Knowledge} {Graph}},
	doi = {10.1109/ICCCS55155.2022.9845845},
	abstract = {As the marine industry booms, the maritime legal documents are of great importance to the maneuver on the sea. However, the traditional way of consulting the text can not meet the demand of maritime operation nowadays. This paper aims to explore a way to extract and strengthen data from maritime legal texts to better support legal question answering. To mine knowledge from unstructured maritime laws and regulations, this paper proposes a method to build the maritime legal knowledge graph. To extract information from unstructured texts, BERT+BiLSTM+CRF is used for named entity recognition. DeepKE toolkit is used for relation extraction. And to strengthen the logics between entities, heterogeneous nodes are introduced to enhance the semantic associations in the maritime legal knowledge graph. The document-enhanced knowledge graph expanded in scale, so it can better support subsequent intelligent applications.},
	booktitle = {2022 7th {International} {Conference} on {Computer} and {Communication} {Systems} ({ICCCS})},
	author = {Yiming, Liu and Li, Duan},
	month = apr,
	year = {2022},
	keywords = {Ontologies, Semantics, Information retrieval, knowledge graph, named entity recognition, Law, Text recognition, Pipelines, Regulation, heterogeneous entities, maritime law},
	pages = {903--908},
}

@inproceedings{mordecai_applying_2022,
	title = {Applying {Model}-{Based} {Ontology} {Coverage} {Analysis} to {Mission} {Architectures}},
	doi = {10.1109/AERO53065.2022.9843341},
	abstract = {This paper introduces a method for Model-based Ontology Coverage Analysis (MOCA) and applies it to SysML models of mission architectures. An ontology is a set of concepts that constitute a common language, standard terminology, and consistent pattern reference across multiple models within an organization, industry, or domain. The purpose of MOCA is to assess the overlap between a system architecture model and a given ontology, and thereby the architecture model's compliance with the ontology and the ontology's utilization by the architecture. We demonstrate MOCA on a SysML model of a humanitarian airlift mission, using a conceptual mission architecting SysML profile model that serves as the ontology. MOCA automates and simplifies reasoning over models, and creates digital model-based artifacts that support stakeholders in concept validation, decision making, and system/mission design. Thus, MOCA enhances digital systems engineering.},
	booktitle = {2022 {IEEE} {Aerospace} {Conference} ({AERO})},
	author = {Mordecai, Yaniv and Markina–Khusid, Aleksandra and Quinn, Greg and Crawley, Edward F.},
	month = mar,
	year = {2022},
	note = {ISSN: 1095-323X},
	keywords = {Ontology, Semantics, MBSE, Vocabulary, Unified modeling language, Digital Engineering, Model-Based Systems Engineering, Visualization, Atmospheric modeling, Analytical models, Digital systems, Mission Architecture, Mission Engineering, Ontological Analysis},
	pages = {01--18},
}

@inproceedings{andreadis_sparql_2022,
	title = {{SPARQL} querying for validating the usage of automatically georeferenced social media data as human sensors for air quality},
	doi = {10.1109/IVMSP54334.2022.9816191},
	abstract = {The problem of air pollution is one of the countless topics discussed on social media on an everyday basis. This rich, crowdsourced information can be exploited to assess the air quality of urban areas, using humans as sensors. Nevertheless, the majority of social media data are falsely geotagged or completely lack geoinformation, which is an essential attribute, while the reliability of the air pollution events reported by online citizens has to be proven. The scope of this work is to present a framework that collects Twitter messages in German that refer to the atmosphere, automatically georeferences them, and finally validates them through semantic representation and SPARQL queries in order to associate them with real measurements of air quality sensors. The georeferencing models are evaluated against state-of-the-art works and the proposed framework is validated in a near-six-month scenario in Germany.},
	booktitle = {2022 {IEEE} 14th {Image}, {Video}, and {Multidimensional} {Signal} {Processing} {Workshop} ({IVMSP})},
	author = {Andreadis, Stelios and Elias, Mirette and Mavropoulos, Thanassis and Papadopoulos, Charis and Pantelidis, Nick and Gialampoukidis, Ilias and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
	month = jun,
	year = {2022},
	keywords = {Semantics, social media, SPARQL querying, Atmospheric modeling, Social networking (online), Sensor phenomena and characterization, semantic representation, air quality, Atmospheric measurements, georeferencing, Multidimensional signal processing, Urban areas},
	pages = {1--5},
}

@inproceedings{ma_profiling_2022,
	title = {A {Profiling} and {Query} {Platform} for {Research} {Management} {Based} on {Knowledge} {Graph}},
	doi = {10.1109/CSCWD54268.2022.9776306},
	abstract = {Researching is a process whereby a large amount of new and unstructured knowledge is created and accumulated. In this context, the capture of complex knowledge about detailed work and decision-making issues throughout a research project is very challenging for modern researchers. Knowledge graph technology can help machines better understand complicated relationships between entities, has great potential for helping researchers with organizing and automating such kinds of repetitive works, and even uncovering and providing new insights into related topics.This paper introduces a way to construct a research management platform by providing a profiling and query system visualized as a knowledge graph. With the scope of this platform being restricted to the research field, typical ontologies are proposed on different levels. For better and more meaningful visualization, specific modifications and improvements to the traditional knowledge graph structure are discussed. A prototype system that is under construction is then described based on the above work, with the extensive applications discussed.},
	booktitle = {2022 {IEEE} 25th {International} {Conference} on {Computer} {Supported} {Cooperative} {Work} in {Design} ({CSCWD})},
	author = {Ma, Ke and Qin, Bo and Wang, Hongwei},
	month = may,
	year = {2022},
	keywords = {Ontologies, ontology, knowledge graph, Decision making, Visualization, Prototypes, Conferences, Collaborative work, research management},
	pages = {822--827},
}

@article{hsu_xiao-shih_2022,
	title = {Xiao-{Shih}: {A} {Self}-{Enriched} {Question} {Answering} {Bot} {With} {Machine} {Learning} on {Chinese}-{Based} {MOOCs}},
	volume = {15},
	issn = {1939-1382},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127504665&doi=10.1109%2FTLT.2022.3162572&partnerID=40&md5=c40a12601b7bac535006b76426c54dc6},
	doi = {10.1109/TLT.2022.3162572},
	abstract = {This article introduces Xiao-Shih, the first intelligent question answering bot on Chinese-based massive open online courses (MOOCs). Question answering is critical for solving individual problems. However, instructors on MOOCs must respond to many questions, and learners must wait a long time for answers. To address this issue, Xiao-Shih integrates many novel natural language processing and machine learning approaches to achieve state-of-the-art performance. Furthermore, Xiao-Shih has a built-in self-enriched mechanism for expanding the knowledge base through open community-based question answering. This article proposes a novel approach, known as spreading question similarity (SQS), which iterates similar keywords on our keyword networks to find duplicate questions. Compared with BERT, an advanced neural language model, the results showed that SQS outperforms BERT on recall and accuracy above a prediction probability threshold of 0.8. After training, Xiao-Shih achieved a perfect correct rate. Furthermore, Xiao-Shih outperforms Jill Watson 1.0, which is a noted question answering bot, on answer rate with the self-enriched mechanism.},
	number = {2},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Hsu, Hao-Hsuan and Huang, Nen-Fu},
	month = apr,
	year = {2022},
	note = {Type: Article},
	keywords = {ontologies, Machine learning, E-learning, Knowledge based systems, Standards, Data science, natural language processing (NLP), Question Answering, Chatbots, machine learning (ML), Electronic learning, Learning systems, Bit error rate, Answer selection, Computer aided instruction, massive open online courses (MOOCs), question answering bot, question retrieval, Ontology's, Natural language processing systems, Machine-learning, Learning algorithms, Machine learning approaches, Bit-error rate, Massive open online course, Question answering bot, Question retrieval},
	pages = {223--237},
	annote = {Cited by: 19},
}

@inproceedings{lim_knowledge_2022,
	title = {Knowledge {Management} {Approach} for {Memory} {Components} {Based} on {User}-friendly {Conversational} {System}},
	doi = {10.1109/BigComp54360.2022.00091},
	abstract = {Due to the recent technological development and the growth of computing resources, there are various studies that apply large-scale language models in the field of natural language processing such as conversational systems. Also, there are researches that attempt to maintain a conversation flow and naturally lead a dialogue by treating the contextual information exchanged with users from the perspective of knowledge. In this paper, we propose a method for managing various contextual information such as chat history, situations, and preferred topics based on a knowledge base and generating a conversation customized for the specific user. We design a schema of memory components to deal with the user's contextual information so that implement a Web-based conversational system, which is friendly come to those users. It is expected that these systems using the user-specific memory components will be helpful in such domains like education or customer consultation.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} and {Smart} {Computing} ({BigComp})},
	author = {Lim, Chae-Gyun and Lee, Dongkun and Lee, Young-Jun and Choi, Ho-Jin},
	month = jan,
	year = {2022},
	note = {ISSN: 2375-9356},
	keywords = {Education, Big Data, Knowledge based systems, Computational modeling, Conferences, Memory management, conversational system, conversation history, Lead, memory component, ontology-based approach},
	pages = {401--403},
}

@inproceedings{burgdorf_docsemmap_2022,
	title = {{DocSemMap}: {Leveraging} {Textual} {Data} {Documentations} for {Mapping} {Structured} {Data} {Sets} into {Knowledge} {Graphs}},
	doi = {10.1109/ICSC52841.2022.00042},
	abstract = {Today, knowledge graphs have been proven to enable the efficient integration of heterogeneous data sets. An important step in creating such knowledge graphs is the mapping of the attributes of a data set to the knowledge graph's ontology. So far, numerous methods have been developed to support this mapping process by using both the schema information as well as the actual data values from a data set in conjunction with external knowledge bases or machine learning approaches. A third source of information, namely textual data documentations, has not yet been considered. In this paper, we present DocSemMap, a novel approach that utilizes textual data documentations of data sets as an additional source for the creation of semantic mappings. We train custom embeddings on the textual data documentations. Further, we utilize pre-trained embeddings that allow us to identify similarities between excerpts of the textual data documentations and descriptions of ontological concepts. Based on this, we build candidate sets of the best suitable concepts for mapping and finally use weighted similarity scores to identify the best fitting concept for each attribute of a data set. The evaluation of our approach outperforms existing approaches for semantic mapping but still has potential for improvement.},
	booktitle = {2022 {IEEE} 16th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Burgdorf, Andreas and Paulus, Alexander and Pomp, André and Meisen, Tobias},
	month = jan,
	year = {2022},
	note = {ISSN: 2325-6516},
	keywords = {Semantics, natural language processing, Machine learning, Knowledge based systems, knowledge graph construction, semantic mapping, Documentation, Natural languages, Syntactics, Fitting, textual data documentation},
	pages = {209--216},
}

@inproceedings{sajid_resume_2022,
	title = {Resume {Parsing} {Framework} for {E}-recruitment},
	doi = {10.1109/IMCOM53663.2022.9721762},
	abstract = {Modern approaches to improve networking and communication have given ways to the advancement of recruitment process through the development of e-recruitment recommender systems. The increasing expansion of internet- based recruiting has resulted in a large number of resumes being stored in recruitment systems. Most resumes are prepared in a variety of styles to attract the attention of recruiters, including different font sizes, font colors, and table formats. However, data mining operations such as resume information extraction, automatic profile matching, and applicant ranking are immensely affected by the variety of formats. Rule-based methods, supervised methods and semantics-based methods have been introduced to extract facts from resume accurately, however, these methods heavily depend on large amounts of annotated data that is usually difficult to collect Furthermore, these techniques are time-intensive and bear knowledge incompleteness that strongly affect the accuracy of resume parser. In this paper, we present a resume parsing framework that handles the limitations faced in the previous techniques. At first, the raw text is extracted from resumes and blocks are separated using text block classification. Furthermore, the entities are extracted using named entity recognition and enriched using ontology. The proposed resume parser accurately extracts information from resumes that directly contributes towards the selection of best candidate.},
	booktitle = {2022 16th {International} {Conference} on {Ubiquitous} {Information} {Management} and {Communication} ({IMCOM})},
	author = {Sajid, Hira and Kanwal, Javeria and Bhatti, Saeed Ur Rehman and Qureshi, Saad Ali and Basharat, Amna and Hussain, Shujaat and Khan, Kifayat Ullah},
	month = jan,
	year = {2022},
	keywords = {Ontologies, Ontology, Information management, Information retrieval, Feature extraction, Training, Resume parsing, Layout, Text Extraction, Boolean Naive Bayes, Data Enrichment, Resumes},
	pages = {1--8},
}

@article{marschall_design_2022,
	title = {Design and {Implementation} of a {Smart}, {Product}-{Led} {Production} {Control} {Using} {Industrial} {Agents}},
	volume = {3},
	issn = {2687-9743},
	doi = {10.1109/JESTIE.2021.3117121},
	abstract = {In theory, the design of modern production systems in the form of a cyber–physical production system (CPPS) allows more flexibility, simple expandability, quick adaptability, and intelligent production control by the product. Multiagent systems (MASs) are thereby recommended as control solution because of their autonomy and dynamic decentralized architecture. Although their potential use and technical excellence have been proven, the costs of implementation and maintenance still outweigh their supposed advantages. This results in low acceptance and usage of operational MAS in the industry. This article describes topics that need to be considered when designing and implementing an MAS as production control for a CPPS in the context of customized mass production. Generally developed approaches are presented, which were implemented in a commercially developed agent framework and validated on the basis of an industrial use case for a product-led filling process in lot size one. All implemented concepts aim to be reusable in comparable applications across industries. In combination with the MAS-internal testing approach also presented, this should contribute to faster, more cost-effective implementation of reliable MAS solutions and ultimately increase their technical maturity.},
	number = {1},
	journal = {IEEE Journal of Emerging and Selected Topics in Industrial Electronics},
	author = {Marschall, Benedikt and Ochsenkuehn, Daniel and Voigt, Tobias},
	month = jan,
	year = {2022},
	keywords = {XML, Production systems, Testing, Companies, Costs, cyber–physical product ion system (CPPS), Industrial agents (IAs), multiagent system (MAS), Production control, Radiofrequency identification},
	pages = {48--56},
}

@article{lu_design_2022,
	title = {Design {Ontology} {Supporting} {Model}-{Based} {Systems} {Engineering} {Formalisms}},
	volume = {16},
	issn = {1937-9234},
	doi = {10.1109/JSYST.2021.3106195},
	abstract = {Model-based systems engineering (MBSE) provides an important capability for managing the complexities of system development. MBSE empowers the formalism of system architectures for supporting model-based requirement elicitation, specification, design, development, testing, fielding, etc. However, the modeling languages and techniques are heterogeneous, even within the same enterprise system, which leads to difficulties for data interoperability. The discrepancies among data structures and language syntaxes make information exchange among MBSE models more difficult, resulting in considerable information deviations when connecting data flows across the enterprise. Therefore, this article presents an ontology based upon graphs, objects, points, properties, roles, and relationships with extensions (GOPPRRE), providing metamodels that support the various MBSE formalisms across lifecycle stages. In particular, knowledge graph models are developed to support unified model representations to further implement ontological data integration based on GOPPRRE throughout the entire lifecycle. The applicability of the MBSE formalism is verified using quantitative and qualitative approaches. Moreover, the GOPPRRE ontologies are used to create the MBSE formalisms in a domain-specific modeling tool, MetaGraph, for evaluating its availability. The results demonstrate that the proposed ontology supports the formal structures and descriptive logic of the systems engineering lifecycle.},
	number = {4},
	journal = {IEEE Systems Journal},
	author = {Lu, Jinzhi and Ma, Junda and Zheng, Xiaochen and Wang, Guoxin and Li, Han and Kiritsis, Dimitris},
	month = dec,
	year = {2022},
	keywords = {Ontologies, Semantics, ontology, knowledge graph, Modeling, interoperability, Tools, Unified modeling language, Data models, model-based systems engineering, Systems engineering and theory, Formalism},
	pages = {5465--5476},
}

@article{nourani_tripletprot_2022,
	title = {{TripletProt}: {Deep} {Representation} {Learning} of {Proteins} {Based} {On} {Siamese} {Networks}},
	volume = {19},
	issn = {1557-9964},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144974462&doi=10.1109%2FTCBB.2021.3108718&partnerID=40&md5=3826a8f3a9386e049cd885e29680fba4},
	doi = {10.1109/TCBB.2021.3108718},
	abstract = {Pretrained representations have recently gained attention in various machine learning applications. Nonetheless, the high computational costs associated with training these models have motivated alternative approaches for representation learning. Herein we introduce TripletProt, a new approach for protein representation learning based on the Siamese neural networks. Representation learning of biological entities which capture essential features can alleviate many of the challenges associated with supervised learning in bioinformatics. The most important distinction of our proposed method is relying on the protein-protein interaction (PPI) network. The computational cost of the generated representations for any potential application is significantly lower than comparable methods since the length of the representations is significantly smaller than that in other approaches. TripletProt offers great potentials for the protein informatics tasks and can be widely applied to similar tasks. We evaluate TripletProt comprehensively in protein functional annotation tasks including sub-cellular localization (14 categories) and gene ontology prediction (more than 2000 classes), which are both challenging multi-class, multi-label classification machine learning problems. We compare the performance of TripletProt with the state-of-the-art approaches including a recurrent language model-based approach (i.e., UniRep), as well as a protein-protein interaction (PPI) network and sequence-based method (i.e., DeepGO). Our TripletProt showed an overall improvement of F1 score in the above mentioned comprehensive functional annotation tasks, solely relying on the PPI network. Availability: The source code and datasets are available at https://github.com/EsmaeilNourani/TripletProt.},
	number = {6},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Nourani, Esmaeil and Asgari, Ehsaneddin and McHardy, Alice C. and Mofrad, Mohammad R.K.},
	month = nov,
	year = {2022},
	note = {Type: Article},
	keywords = {Bioinformatics, Deep learning, Gene Ontology, Functional annotation, Feature extraction, Language, Software, Neural Networks, Protein representation learning, Training, software, Computational modeling, Proteins, metabolism, Computational efficiency, language, protein, Siamese network, Protein engineering, Task analysis, siamese networks, triplet loss, Computer, Protein Interaction Maps, Classification (of information), Neural-networks, New approaches, Biological entities, Computational costs, Machine learning applications, protein analysis, Protein-protein interaction networks, Triplet loss},
	pages = {3744--3753},
	annote = {Cited by: 11; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{eckhart_automated_2022,
	title = {Automated {Security} {Risk} {Identification} {Using} {AutomationML}-{Based} {Engineering} {Data}},
	volume = {19},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2020.3033150},
	abstract = {Systems integrators and vendors of industrial components need to establish a security-by-design approach, which includes the assessment and subsequent treatment of security risks. However, conducting security risk assessments along the engineering process is a costly and labor-intensive endeavor due to the complexity of the system(s) under consideration and the lack of automated methods. This, in turn, hampers the ability of security analysts to assess risks pertaining to cyber-physical systems (CPSs) in an efficient manner. In this work, we propose a method that automatically identifies security risks based on the CPS's data representation, which exists within engineering artifacts. To lay the foundation for our method, we present security-focused semantics for the engineering data exchange format AutomationML (AML). These semantics enable the reuse of security-relevant know-how in AML artifacts by means of a formal knowledge representation, modeled with a security-enriched ontology. Our method is capable of automating the identification of security risk sources and potential consequences in order to construct cyber-physical attack graphs that capture the paths adversaries may take. We demonstrate the benefits of the proposed method through a case study and an open-source prototypical implementation. Finally, we prove that our solution is scalable by conducting a rigorous performance evaluation.},
	number = {3},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Eckhart, Matthias and Ekelhart, Andreas and Weippl, Edgar},
	month = may,
	year = {2022},
	keywords = {Semantics, Knowledge engineering, Risk management, Security, Cyber-physical systems, Data models, information security, AutomationML, Topology, IEC Standards, IEC 62443, industrial control systems, security modeling, security risk assessment},
	pages = {1655--1672},
}

@inproceedings{yang_test_2021,
	title = {Test {Case} {Reuse} {Based} on {Software} {Testing} {Knowledge} {Graph} and {Collaborative} {Filtering} {Recommendation} {Algorithm}},
	doi = {10.1109/QRS-C55045.2021.00020},
	abstract = {As an important role of software test, the reuse of test cases is essential in terms of finding software defects and locating the causes of them. However, the existing related approaches are insufficient to establish an internal relationship between test cases and defects and their abilities to find or diagnose errors are limited. In this paper, an ontology model based on the software testing process is applied to establish a software testing knowledge graph, which serves as the foundation to build an recommendation system. Specifically, the recommendation system takes the functions of software under test as the “user”, and the defect-occurrence-chain which establishes the correlation between test cases and defects in the knowledge graph as the “item”. Both of them provide the evidence to build collaborative filtering recommendation algorithm based on the user-item scoring matrix. It aims to assist testers in recommending reusable test cases to identify software errors effectively. Against this background, the BERT+Bi-LSTM-CRF model is selected to extract the latent test requirements of the software under test, and an overt variable factorization model is built so as to iteratively optimize the user-item scoring matrix. Further, an empirical study has been conducted, and it is found that the recommended test cases can significantly help testers find software defects faster in a more efficient way, and locate defects more accurately.},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Yang, Wansheng and Deng, Fei and Ma, Siyou and Wu, Linbo and Sun, Zhe and Hu, Chi},
	month = dec,
	year = {2021},
	note = {ISSN: 2693-9371},
	keywords = {Ontologies, Semantics, Software, Software quality, Collaborative filtering, Software testing, collaborative filtering, Software algorithms, BERT+Bi-LSTM-CRF, defect-occurrence-chain, overt variable factorization model, software testing knowledge graph},
	pages = {67--76},
}

@inproceedings{kim_how_2021,
	title = {“{How} {Robust} {R} {U}?”: {Evaluating} {Task}-{Oriented} {Dialogue} {Systems} on {Spoken} {Conversations}},
	doi = {10.1109/ASRU51503.2021.9688274},
	abstract = {Most prior work in dialogue modeling has been on written conversations mostly because of existing data sets. However, written dialogues are not sufficient to fully capture the nature of spoken conversations as well as the potential speech recognition errors in practical spoken dialogue systems. This work presents a new benchmark on spoken task-oriented conversations, which is intended to study multi-domain dialogue state tracking and knowledge-grounded dialogue modeling. We report that the existing state-of-the-art models trained on written conversations are not performing well on our spoken data, as expected. Furthermore, we observe improvements in task performances when leveraging n-best speech recognition hypotheses such as by combining predictions based on individual hypotheses. Our data set enables speech-based benchmarking of task-oriented dialogue systems.},
	booktitle = {2021 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	author = {Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Hakkani-Tür, Dilek},
	month = dec,
	year = {2021},
	keywords = {Automatic speech recognition, Data models, Robustness, dialogue state tracking, Benchmark testing, Conferences, Task analysis, knowledge-grounded dialogue generation, spoken dialogue systems},
	pages = {1147--1154},
}

@inproceedings{palchunov_semantic_2021,
	title = {Semantic methods of intelligent assistant developing},
	doi = {10.1109/KNOTH54462.2021.9686335},
	abstract = {Human-computer interaction with people whose visual perception is limited is possible only with tactile and voice interfaces, the latter are being used more and more recently. The aim of the work is to create an intelligent assistant for an indoor navigation system designed for blind and visually impaired people. The development of an intelligent assistant is based on a semantic user model and a four-level ontological model of the subject domain. To build a dialogue between an intelligent assistant and a user, we use the theory of speech acts, argumentation theory and case-based reasoning. The developed software system is aimed at identifying the desires and user needs and proposing possible user actions aimed at achieving them. The system allows for the decomposition of user tasks and the formation of a sequence of their execution based on semantic models of the user and the subject domain.},
	booktitle = {2021 {International} {Symposium} on {Knowledge}, {Ontology}, and {Theory} ({KNOTH})},
	author = {Palchunov, Dmitry and Tregubov, A.S.},
	month = dec,
	year = {2021},
	keywords = {Ontologies, Semantics, machine learning, natural language processing, ontology, Knowledge engineering, Uncertainty, Web services, case-based reasoning, Speech recognition, Software systems, intent recognition, argumentation theory, intelligent assistant},
	pages = {30--35},
}

@inproceedings{sakhrani_transformer-based_2021,
	title = {Transformer-based {Hierarchical} {Encoder} for {Document} {Classification}},
	doi = {10.1109/ICDMW53433.2021.00109},
	abstract = {Document Classification has a wide range of applications in various domains like Ontology Mapping, Sentiment Analysis, Topic Categorization and Document Clustering, to mention a few. Unlike Text Classification, Document Classification works with longer sequences that typically contain multiple paragraphs. Previous approaches for this task have achieved promising results, but have often relied on complex recurrence mechanisms that are expensive and time-consuming in nature. Recently, self-attention based models like Transformers and BERT have achieved state-of-the-art performance on several Natural Language Understanding (NLU) tasks, but owing to the quadratic computational complexity of the self-attention mechanism with respect to the input sequence length, these approaches are generally applied to shorter text sequences. In this paper, we address this issue, by proposing a new Transformer-based Hierarchical Encoder approach for the Document Classification task. The hierarchical framework we adopt helps us extend the self-attention mechanism to long-form text modelling thereby reducing the complexity considerably. We use the Bidirectional Transformer Encoder (BTE) at the sentence-level to generate a fixed-size sentence embedding for each sentence in the document. A document-level Transformer Encoder is then used to model the global document context and learn the inter-sentence dependencies. We also carry out experiments with the BTE in a feature-extraction and a fine-tuning setup, allowing us to evaluate the trade-off between computation power and accuracy. Furthermore, we also conduct ablation experiments, and evaluate the impact of different pre-training strategies on the overall performance. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on two standard benchmark datasets.},
	booktitle = {2021 {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	author = {Sakhrani, Harsh and Parekh, Saloni and Ratadiya, Pratik},
	month = dec,
	year = {2021},
	note = {ISSN: 2375-9259},
	keywords = {Ontologies, Text categorization, Sentiment analysis, Training, Transfer learning, Transformer, Computational modeling, Document Classification, Natural languages, Self-attention},
	pages = {852--858},
}

@inproceedings{amini_twenty_2021,
	title = {Twenty {Years} of {Configuration} {Knowledge} {Modeling} {Research}. {Main} {Works}, {What} {To} {Do} {Next}?},
	doi = {10.1109/IEEM50564.2021.9673037},
	abstract = {A configuration software (configurator) associates a knowledge base (KB) with a knowledge processing unit (PU). The KB describes all possible combinations of components while the PU overlays this knowledge with the customer requirements. Our work deals with the KB and the approaches, models, or tools for modeling configuration knowledge. Our goal is to present a small quantitative literature survey highlighting two work streams: the first one gathers modeling works dealing with constraint-based approaches while the second deals with ontologies, description logic, or object-oriented modeling approach. We will also consider hybrid approaches. We will present a quantitative analysis of published materials in Web of science over the last twenty years. The keywords occurrence versus time will also be studied in detail to identify tendencies in configuration knowledge modeling.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Industrial} {Engineering} and {Engineering} {Management} ({IEEM})},
	author = {Amini, M. Mohammad and Aldanondo, M. and Vareilles, E. and Coudert, T.},
	month = dec,
	year = {2021},
	keywords = {Ontologies, ontology, OWL, Knowledge engineering, UML, Knowledge based systems, rules, Engineering management, Object oriented modeling, Statistical analysis, Maintenance engineering, Configuration knowledge modeling, constraints satisfaction problem},
	pages = {1328--1332},
}

@inproceedings{an_main_2021,
	title = {{MAIN}: {Multimodal} {Attention}-based {Fusion} {Networks} for {Diagnosis} {Prediction}},
	doi = {10.1109/BIBM52615.2021.9669634},
	abstract = {Predicting the future diagnoses from patients’ historical Electronic Health Records (EHR) is a significant task in healthcare. EHR consist of multiple modal data, each modality has different features and contains a wealth of information of patients. However, most of the existing EHR-based prediction methods either only use unimodal data, or fail to fully explore the correlation between different modalities when fusing multimodal data. To address these challenges, we propose a Multimodal Attention-based fusIon Networks (MAIN) for diagnosis prediction. In this model, we first design different feature extraction modules for each modality. Then, an inter-modal correlation module which contains two layers is applied to capture the intermodal correlation. Finally, a multimodal fusion module based on weighted averaging is utilized to integrate the representations derived from different modalities and their correlation to obtain the patient representation for diagnosis prediction. We evaluate our proposed model on two medical datasets, and the experimental results demonstrate the effectiveness of MAIN.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {An, Ying and Zhang, Haojia and Sheng, Yu and Wang, Jianxin and Chen, Xianlai},
	month = dec,
	year = {2021},
	keywords = {Ontologies, Feature extraction, Electronic Health Records, Correlation, attention mechanism, Conferences, Medical services, Task analysis, diagnosis prediction, multimodal fusion, Prediction methods},
	pages = {809--816},
}

@inproceedings{parolin_come-ke_2021,
	title = {{CoMe}-{KE}: {A} {New} {Transformers} {Based} {Approach} for {Knowledge} {Extraction} in {Conflict} and {Mediation} {Domain}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125338805&doi=10.1109%2FBigData52589.2021.9672080&partnerID=40&md5=e4f62b651a33a919e3bbd5fdbfd41b9c},
	doi = {10.1109/BigData52589.2021.9672080},
	abstract = {Knowledge discovery and extraction approaches attract special attention across industries and areas moving toward the 5V Era. In the political and social sciences, scholars and governments dedicate considerable resources to develop intelligent systems for monitoring, analyzing and predicting conflicts and affairs involving political entities across the globe. Such systems rely on background knowledge from external knowledge bases, that conflict experts commonly maintain manually. The high costs and extensive human efforts associated with updating and extending these repositories often compromise their correctness of. Here we introduce CoMe-KE (Conflict and Mediation Knowledge Extractor) to extend automatically knowledge bases about conflict and mediation events. We explore state-of-the-art natural language models to discover new political entities, their roles and status from news. We propose a distant supervised method and propose an innovative zero-shot approach based on a dynamic hypothesis procedure. Our methods leverage pre-trained models through transfer learning techniques to obtain excellent results with no need for a labeled data. Finally, we demonstrate the superiority of our method through a comprehensive set of experiments involving two study cases in the social sciences domain. CoMe-KE significantly outperforms the existing baseline, with (on average) double of the performance retrieving new political entities.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Osorio, Javier and Brandt, Patrick T. and D’Orazio, Vito},
	month = dec,
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Knowledge discovery, Semantics, natural language processing, ontologies, Knowledge extraction, Big Data, Transformers, Data mining, Knowledge based systems, Intelligent systems, Transfer learning, Extraction, knowledge extraction, Graph mining, Social sciences, Natural languages, Learning systems, CAMEO, knowledge base construction, link and graph mining, semantic-based data mining, transfer-learning, web search and mining, Ontology's, Natural language processing systems, Behavioral research, Web searches, Knowledge-base construction, Link mining, Semantic-based data mining, Web Mining},
	pages = {1449--1459},
	annote = {Cited by: 5},
}

@inproceedings{zhao_knowledge_2021,
	title = {Knowledge {Graph}-{Empowered} {Materials} {Discovery}},
	doi = {10.1109/BigData52589.2021.9671503},
	abstract = {In this position paper, we describe research on knowledge graph-empowered materials science prediction and discovery. The research consists of several key components including ontology mapping, materials data annotation, and information extraction from unstructured scholarly articles. We argue that although big data generated by simulations and experiments have motivated and accelerated the data-driven science, the distribution and heterogeneity of materials science-related big data hinders major advancements in the field. Knowledge graphs, as semantic hubs, integrate disparate data and provide a feasible solution to addressing this challenge. We design a knowledge-graph based approach for data discovery, extraction, and integration in materials science.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Zhao, Xintong and Greenberg, Jane and McClellan, Scott and Hu, Yong-Jie and Lopez, Steven and Saikin, Semion K. and Hu, Xiaohua and An, Yuan},
	month = dec,
	year = {2021},
	keywords = {Ontology, Semantics, Natural Language Processing, Big Data, Knowledge Graph, Vocabulary, Information Extraction, Technological innovation, Transforms, Prototypes, Materials science and technology, Materials Discovery},
	pages = {4628--4632},
}

@inproceedings{eggleston_woolery_2021,
	title = {Woolery: {Extending} {Frame} {Semantics} to {Structured} {Documents}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125339948&doi=10.1109%2FBigData52589.2021.9671788&partnerID=40&md5=c4091df453aa66f2878efe5e01e91714},
	doi = {10.1109/BigData52589.2021.9671788},
	abstract = {This paper presents Woolery, a system for semantic annotation and mapping of structured documents (such as JSON key-value pairs) to FrameNet. Implemented as a graphical interface, Woolery provides an annotator with a guided means to map keys in a JSON document to FrameNet elements, without the need for extensive knowledge of FrameNet's semantic structures. Candidate frame elements are identified via a search across FrameNet's internal representations, or via mapping keys to their potential WordNet synsets. Final element selection is automated via a pretrained language model. Initial results are promising, with the model giving an overall accuracy of 77.8\% when labeling frames across a diverse corpus of JSON document schemas.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Eggleston, Chloe and Abramson, Jeremy},
	month = dec,
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Ontology, Semantics, natural language processing, Ontology alignment, Annotations, Big Data, Mapping, Computational semantics, Annotation, annotation, JSON, Semantic annotations, ontology alignment, Lexical database, Labeling, Conferences, computational semantics, FrameNet, lexical databases, Natural language processing systems, Semantics mappings, Frame semantics, Structured document},
	pages = {5597--5601},
	annote = {Cited by: 0},
}

@inproceedings{jeusfeld_unifying_2021,
	title = {Unifying multi-level modeling: {A} position paper},
	doi = {10.1109/MODELS-C53483.2021.00083},
	abstract = {Multi-level modeling (MLM) as part of object-oriented modeling aims at fully utilizing the expressive power of multiple abstraction levels. While these levels where initially used to define domain-specific modeling languages, i.e. for linguistic purposes, the MLM community has long argued that there is much more to gain by tapping into ontological abstraction levels. While MLM is a rather specialized research field, there are now quite a number of different proposals. There is thus an opportunity to develop a uniform core of MLM that then possibly can become part of a standard and be taken up by the larger modeling community.},
	booktitle = {2021 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Jeusfeld, Manfred A. and Frank, Ulrich},
	month = oct,
	year = {2021},
	keywords = {Education, Linguistics, conceptual modeling, multi-level modeling, Limiting, Object oriented modeling, Model driven engineering, Reflection, research agenda, Solids},
	pages = {536--540},
}

@inproceedings{k_c_topic_2021,
	title = {Topic {Recognition} and {Correlation} {Analysis} of {Articles} in {Computer} {Science}},
	doi = {10.1109/I-SMAC52330.2021.9641021},
	abstract = {Topic identification and similarity detection are two related essential task in data mining, information retrieval, and bibliometric data analysis, which aims to identify significant topics and to find similarity between text collections.It is an essential activity to identify research papers according to their research topics to enhance their retrievability, help create smart analytics, and promote a range of approaches to evaluating the research environment and making sense of it.The proposed frame work deals with three main steps: text extraction, topic identification, and similarity detection.The PyPDF2 module is used to extract text from pdf file. CSO classifier is used for topic identification and similarity between documents is calculated using different models, such as Tf-Idf, Bert, Glove, Word2vec, and Doc2vec.and compared these models with respect to cosine similarity and Eucleadian distance obtained from these models.},
	booktitle = {2021 {Fifth} {International} {Conference} on {I}-{SMAC} ({IoT} in {Social}, {Mobile}, {Analytics} and {Cloud}) ({I}-{SMAC})},
	author = {K C, Hitha and V K, Kiran},
	month = nov,
	year = {2021},
	note = {ISSN: 2768-0673},
	keywords = {Ontologies, Semantic search, Computational modeling, Bert, Doc2Vec, Word2Vec, Manuals, Syntactics, Portable document format, CSO Classifier, Euclidean distance, Glove, PyPDF2 module, Tf-Idf},
	pages = {1115--1118},
}

@inproceedings{lyadova_ontology-based_2021,
	title = {An {Ontology}-{Based} {Approach} to the {Domain} {Specific} {Languages} {Design}},
	doi = {10.1109/AICT52784.2021.9620493},
	abstract = {Developing software systems for various domains is a complex task. The quality of the system, corresponding to the domain requirements, can only be achieved via involving the model development of experts in the relevant fields. Traditional design methods based on the using professional tools and modeling languages are difficult for subject matter experts. Using Domain Specific Languages (DSL) have been increasingly gaining attention of developers because DSLs are created to cope with specific domain particularities. However, DSL development consists of several steps to be performed can be hard. Identifying the correct set of elements and constructions of DSL, defining their constraints can be very error-prone. Automation of the new DSLs development is relevant task. The designing of new DSLs should be based on the knowledge of experts, which can be represented using an ontology. An approach to DSM platform development based on using multifaceted ontology to DSL design is proposed. Examples of DSLs and models illustrating the applicability of the proposed methodology are described.},
	booktitle = {2021 {IEEE} 15th {International} {Conference} on {Application} of {Information} and {Communication} {Technologies} ({AICT})},
	author = {Lyadova, Lyudmila N. and Sukhov, Alexander O. and Nureev, Marsel R.},
	month = oct,
	year = {2021},
	note = {ISSN: 2472-8586},
	keywords = {Ontologies, Tools, DSM, Metamodeling, DSL, Software systems, metamodeling, domain specific language, Natural languages, Prototypes, metamodel generation, multifaceted ontology, domain specific modeling, DSM platform, language toolkits, visual language},
	pages = {1--6},
}

@article{loubach_classification_2021,
	title = {Classification and {Mapping} of {Model} {Elements} for {Designing} {Runtime} {Reconfigurable} {Systems}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3129899},
	abstract = {Embedded systems are ubiquitous and control many critical functions in society. A fairly new type of embedded system has emerged with the advent of partial reconfiguration, i.e. runtime reconfigurable systems. They are attracting interest in many different applications. Such a system is capable of reconfiguring itself at the hardware level and without the need to halt the application’s execution. While modeling and implementing these systems is far from a trivial task, there is currently a lack of systematic approaches to tackle this issue. In other words, there is no unanimously agreed upon modeling paradigm that can capture adaptive behaviors at the highest level of abstraction, especially when regarding the design entry, namely, the initial high-level application and platform models. Given this, our paper proposes two domain ontologies for application and virtual platform models used to derive a classification system and to provide a set of rules on how the different model elements are allowed to be composed together. The application behavior is captured through a formal model of computation which dictates the semantics of execution, concurrency, and synchronization. The main contribution of this paper is to combine suitable formal models of computation, a functional modeling language, and two domain ontologies to create a systematic design flow from an abstract executable application model into a virtual implementation model based on a runtime reconfigurable architecture (virtual platform model) using well-defined mapping rules. We demonstrate the applicability, generality, and potential of the proposed model element classification system and mapping rules by applying them to representative and complete examples: an encoder/decoder system and an avionics attitude estimation system. Both cases yield a virtual implementation model from an abstract application model.},
	journal = {IEEE Access},
	author = {Loubach, Denis S. and Bonna, Ricardo and Ungureanu, George and Sander, Ingo and Söderquist, Ingemar},
	year = {2021},
	keywords = {Ontologies, domain ontology, Unified modeling language, Embedded systems, Computational modeling, Adaptation models, Runtime, Hardware, mapping rules, models of computation (MoC), runtime reconfiguration},
	pages = {156337--156360},
}

@inproceedings{kiv_towards_2021,
	title = {Towards a {Systematic} {Socio}-{Intentional} {Framework} for {Agile} {Methods} {Tailoring}},
	volume = {02},
	doi = {10.1109/CBI52690.2021.10065},
	abstract = {Agile has become one of the most popular software development approaches thanks to its flexible and evolutive features. To find further suitable practices, teams start to follow the tailoring approach by choosing only the fragments of different methods that fit their needs and context. Many tailoring approaches have been proposed by orienting different aspects such as process, resource and goal. While the interaction between team members is very important in agile methods, none of these approaches focuses on the socio-intentional aspect. In the literature, we can find many case studies that link socio-intentional aspects to the tailoring of agile practices. Even though it is helpful to know it, locating relevant information can be effort and time-consuming. This research proposes a socio-intentional framework that can analyze agile practices and indicate how to tailor them with the help of an evidence-based tool and a modeling language. This framework will allow practitioners to identify the right practices to achieve their goals and analyze their suitability and vulnerability. It will also indicate how to successfully implement them in the software development process.},
	booktitle = {2021 {IEEE} 23rd {Conference} on {Business} {Informatics} ({CBI})},
	author = {Kiv, Soreangsey and Heng, Samedi and Wautelet, Yves and Kolp, Manuel},
	month = sep,
	year = {2021},
	note = {ISSN: 2378-1971},
	keywords = {Ontology, Software, Tools, Visualization, Planning, Focusing, Systematics, Analytical models, Evidence-based System, Socio-intentional Modeling, Tailoring Agile},
	pages = {143--152},
}

@inproceedings{odukoya_architectural_2021,
	title = {An {Architectural} {Description} {For} {The} {Application} {Of} {Mbse} {In} {Complex} {Systems}},
	doi = {10.1109/ISSE51541.2021.9582510},
	abstract = {The design of a complex warship is a multidisciplinary effort which often encounters major challenges, particularly with respect to integration across interfaces in the System of Systems (SoS). In principle, the goal of Model Based Systems Engineering (MBSE) with respect to system design is to provide a means of capturing and communicating the system design in a structured, consistent, and coherent fashion; that can be easily assessed by engineering teams and quickly analysed using queries and toolsets. The focus of this paper is to investigate the potential to achieve a consistent description, identify a viable methodology that minimises mismatch in requirements and to avoid an extended design lifecycle. This study highlights the need to develop a generic Architectural Description (AD) that is based on a common ontology which would clearly define the fundamental tenets of applying state-of-the-art Architectural Frameworks (AFs) in naval ship design. An investigation on the effectiveness and accuracy of a graph-based approach is needed to assess whether it is possible to create a ‘Rosetta stone’ for AFs, which links any two or more different model viewpoints in different AF’s using the approach.},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Systems} {Engineering} ({ISSE})},
	author = {Odukoya, Kofoworola Adebowale and Whitfield, Robert Ian and Hay, Laura and Harrison, Neil and Robb, Malcolm},
	month = sep,
	year = {2021},
	note = {ISSN: 2687-8828},
	keywords = {Ontologies, Stakeholders, Complex systems, System analysis and design, Analytical models, Systems architecture, Complex systems1, Marine vehicles, System of systems},
	pages = {1--8},
}

@inproceedings{mandel_model-based_2021,
	title = {A {Model}-{Based} {Systems} {Engineering} {Approach} to {Support} {Continuous} {Validation} in {PGE} - {Product} {Generation} {Engineering}},
	doi = {10.1109/ISSE51541.2021.9582475},
	abstract = {Increasing customer demands, especially regarding functionality, safety and environmental sustainability, are major drivers of nowadays product development processes. Those increasing demands as well as today’s product development context of distributed interdisciplinary development teams lead to an increasing complexity of systems as well as their respective development processes. MBSE - Model-Based Systems Engineering is regarded as a promising approach to cope with this complexity. MBSE aims at supporting during the whole product lifecycle in system analysis, requirements management, design as well as verification and validation. Especially validation plays a central role in product development as it is the only activity that can ensure customer satisfaction and thus a successful product on the market. However, comprehensive MBSE-approaches to support validation in product development seem to be missing. This paper describes such a MBSE approach to support validation in product development. The approach includes an ontology of terms and their interrelations in the context of validation. The ontology is used to construct viewpoints, views and a modeling framework to structure a system model in the understanding of MBSE. In addition, a modeling method interacting with the constructed views is developed and presented. The presented approach aims at enabling a continuous validation concept, starting in the early phase of PGE - Product Generation Engineering and continuing throughout the entire lifecycle. Furthermore, the approach should support in integrating the development of products and appropriate validation systems, creating a consistent traceability of information throughout the created models. Finally, a specific focus of the approach lies on usability in order to guarantee individual and organizational acceptance. This acceptance is of particular importance to realize a human centered development as it is envisioned in approaches such as ASE - Advanced Systems Engineering.},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Systems} {Engineering} ({ISSE})},
	author = {Mandel, Constantin and Böning, Jannis and Behrendt, Matthias and Albers, Albert},
	month = sep,
	year = {2021},
	note = {ISSN: 2687-8828},
	keywords = {Ontologies, Product development, MBSE, Product Generation Engineering, Safety, Validation, Complexity theory, Green products, Continuous Validation Concept, Customer satisfaction, Modeling Framework, PGE, Requirements management},
	pages = {1--8},
}

@inproceedings{ahmad_unsupervised_2021,
	title = {Unsupervised {Approach} for {Knowledge}-{Graph} {Creation} from {Conversation}: {The} {Use} of {Intent} {Supervision} for {Slot} {Filling}},
	doi = {10.1109/IJCNN52387.2021.9534398},
	abstract = {In this paper, we propose an unsupervised approach for knowledge graph (KG) creation from conversational data. We make use of intent classification and slot-filling, the two important components of any dialogue agent, exploit their interconnectedness, and finally construct a KG. We build a supervised intent classifier to extract the intent classes, and then on top of this we run our occlusion based slot-information extraction algorithm. Our algorithm is able to make use of supervised training of intent classifiers for extracting the relevant slot-information in an unsupervised way. To test the effectiveness of our system, we perform both automatic and manual evaluation of our intent-classifier and slot-filling system on three dialog datasets. Finally, we construct a knowledge graph from the dialogue conversation using an algorithm that makes use of our occlusion based slot-information extraction module. Empirical evaluation shows that our occlusion based method is able to successfully extract slot information from conversations, resulting in a high-quality KG.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Ahmad, Zishan and Ekbal, Asif and Sengupta, Shubhashis and Maitra, Anutosh and Ramnani, Roshni and Bhattacharyya, Pushpak},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {Information retrieval, Neural networks, Training, Text generation, Deep-learning, Manuals, Codes, Bit error rate, Filling, Style-transfer, Unsupervised},
	pages = {1--8},
}

@inproceedings{choudhary_comparative_2021,
	title = {A {Comparative} {Study} of {Methods} for {Visualizable} {Semantic} {Embedding} of {Small} {Text} {Corpora}},
	volume = {2021-July},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116441915&doi=10.1109%2FIJCNN52387.2021.9534250&partnerID=40&md5=076e82432db0576c5c7c6b98d7138f7b},
	doi = {10.1109/IJCNN52387.2021.9534250},
	abstract = {Text embedding has recently emerged as a very useful and successful method for semantic representation. Following initial word-level embedding methods such as Latent Semantic Analysis (LSA) and topic-based bag-of-words approaches like Latent Dirichlet Allocation (LDA), the focus has turned to language models and text encoders implemented as neural networks - ranging from word-level models to those embedding whole documents. The distinctive feature of these models is their ability to infer semantic spaces at all levels based purely on data, with no need for complexities such as syntactic analysis or ontology building. Many of these models are available pre-trained on enormous amounts of data, providing downstream applications with general-purpose semantic spaces. In particular, embedding models at the sentence level or higher are most useful in applications because the meaning of text only becomes clear at that level. Most text embedding methods produce text embeddings in high-dimensional spaces, with a dimensionality ranging from a few hundred to thousands. However, it is often useful to visualize semantic spaces in very low dimension, which requires the use of dimensionality reduction methods. It is not clear what language models and what method of dimensionality reduction would work well in these cases. In this paper, we compare four text embedding methods in combination with three methods of dimensionality reduction to map three related real-world datasets comprising textual descriptions of items in a particular domain (sports) to a 2-dimensional semantic visualization space. The results provide several insights into the utility of these methods for data of this type.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Choudhary, Rishabh and Doboli, Simona and Minai, Ali A.},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {Language model, Semantics, Information retrieval, Neural networks, language models, Embeddings, Semantic embedding, Text embedding, Training, Dimensionality reduction, Computational linguistics, Visualization, Statistics, Syntactics, Analytical models, Bit error rate, semantic spaces, semantic visualization, text embedding, Natural language processing systems, Embedding method, Comparatives studies, Semantic Space, Semantic visualization, Word level},
	pages = {1--8},
	annote = {Cited by: 6},
}

@inproceedings{weerakoon_question_2021,
	title = {Question {Classification} for the {Travel} {Domain} using {Deep} {Contextualized} {Word} {Embedding} {Models}},
	doi = {10.1109/MERCon52712.2021.9525789},
	abstract = {Question answering can be considered as a key area in Natural Language Processing and Information Retrieval, where users construct queries in natural language and receive suitable answers in return. In the travel domain, most questions are “content questions”, where the expected answer is not the equivalent of “yes” or “no”, but rather factual information. Replying to a free-form factual question based on a large collection of text is challenging. Previous research has shown that the accuracy of question answering systems can be improved by adding a classification phase based on the expected answer type. This paper focuses on implementing a multi-level, multi-class question classification system focusing on the travel domain. Existing research for the travel domain is conducted using language-specific features and traditional Machine Learning models. In contrast, this research employs transformer-based state-of-the-art deep contextualized word embedding models for question classification. The proposed method improves the coarse class Micro F1-Score by 5.43\% compared to the baseline. Fine-grain Micro F1-Score has also improved by 3.8\%. We also present an empirical analysis of the effectiveness of different transformer-based deep contextualized word embedding models for multi-level multi-class classification.},
	booktitle = {2021 {Moratuwa} {Engineering} {Research} {Conference} ({MERCon})},
	author = {Weerakoon, Charmy and Ranathunga, Surangika},
	month = jul,
	year = {2021},
	note = {ISSN: 2691-364X},
	keywords = {Knowledge discovery, Natural language processing, Machine learning, Information retrieval, Transformers, ontology learning, transformers, RoBERTa, question classification, Focusing, Analytical models, expected answer type},
	pages = {573--578},
}

@inproceedings{neji_hir_2021,
	title = {{HIR}: {A} {Hybrid} {IR} {Ranking} {Model}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115711808&doi=10.1109%2FCOMPSAC51774.2021.00256&partnerID=40&md5=394c57fba9408bd0aaaff2e781f17ef6},
	doi = {10.1109/COMPSAC51774.2021.00256},
	abstract = {The aim of information retrieval (IR) process is to respond effectively to user queries by retrieving information that are better meets their expectations. A gap could exist between user information needs and his/her defined context as the level of the user's expertise in the search domain is directly influencing the query richness. The information retrieval system (IRS) must be intelligent enough to identify information needs and respond effectively to meet the expected needs, regardless the level of user's expertise level. This process is difficult and it remains a major and open challenge in the domain of IR. IR models integrate many sources to achieve an effective retrieval system. Semantic IR is an environment in which semantic techniques were applied to sort the documents according to their degree of relevance to the query. The present work proposes a hybrid model to rank documents. The proposed model is based on a query likelihood language model and the semantic similarity between concepts to assess the relevance between query-document pairs. Concepts were extracted by projection on WordNet ontology then word sense disambiguation was conducted. A semantic index was built to validate the proposed model. The conducted empirical experiments show that the proposed model is outperformed the compared benchmarks in the measured IR metrics.},
	booktitle = {2021 {IEEE} 45th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Neji, Sameh and Chenaina, Tarek and Shoeb, Abdullah M. and Ben Ayed, Leila},
	month = jul,
	year = {2021},
	note = {ISSN: 0730-3157},
	keywords = {Ontologies, Ontology, Semantics, Search engines, Semantic search, Information retrieval, Word Sense Disambiguation, Semantic similarity, conceptual model, semantic similarity, Learning to rank, Computational modeling, Measurement, language model, semantic information retrieval, Benchmark testing, query-document relevance, Natural language processing systems, Retrieval systems, Application programs, Degree of relevance, Empirical experiments, Query documents, Semantic techniques, User information need},
	pages = {1717--1722},
	annote = {Cited by: 1},
}

@article{cho_re-ranking_2021,
	title = {Re-{Ranking} {System} with {BERT} for {Biomedical} {Concept} {Normalization}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3108445},
	abstract = {In recent years, various neural network architectures have been successfully applied to natural language processing (NLP) tasks such as named entity normalization. Named entity normalization is a fundamental task for extracting information in free text, which aims to map entity mentions in a text to gold standard entities in a given domain-specific ontology; however, the normalization task in the biomedical domain is still challenging because of multiple synonyms, various acronyms, and numerous lexical variations. In this study, we regard the task of biomedical entity normalization as a ranking problem and propose an approach to rank normalized concepts. We additionally employ two factors that can notably affect the performance of normalization, such as task-specific pre-training (Task-PT) and calibration approach. Among five different biomedical benchmark corpora, our experimental results show that our proposed model achieved significant improvements over the previous methods and advanced the state-of-the-art performance for biomedical entity normalization, with up to 0.5\% increase in accuracy and 1.2\% increase in F-score.},
	journal = {IEEE Access},
	author = {Cho, Hyejin and Choi, Dongha and Lee, Hyunju},
	year = {2021},
	keywords = {natural language processing, text mining, Unified modeling language, Context modeling, Dictionaries, Biological system modeling, Conferences, Task analysis, Bit error rate, Named entity normalization, text recognition},
	pages = {121253--121262},
}

@inproceedings{vlasenko_usage_2021,
	title = {Usage {Features} of {Semantic} {Query} and {Rule} {Languages} of {Semantic} {Web} in the {Intelligent} {Systems}, {Based} on {Conceptual} {Graphs} {Technologies}},
	doi = {10.1109/SCM52931.2021.9507178},
	abstract = {The article considers some problems related to applying languages of semantic query and rules, oriented towards their application within Semantic Web, in the information systems, based on conceptual graphs techniques and knowledge models of the appropriate type. At this, special attention is paid to the analysis of given languages interpretation correctness, as well as technological aspects of organizing for processing chosen-class knowledge models.},
	booktitle = {2021 {XXIV} {International} {Conference} on {Soft} {Computing} and {Measurements} ({SCM})},
	author = {Vlasenko, Sergey V.},
	month = may,
	year = {2021},
	keywords = {Semantic Web, Semantics, Software, Tools, knowledge models, ISO Standards, Regulation, conceptual graphs, intelligent systems, W3C},
	pages = {120--123},
}

@inproceedings{wu_cognitive_2021,
	title = {Cognitive {Thread} {Supports} {System} of {Systems} for {Complex} {System} {Development}},
	doi = {10.1109/SOSE52739.2021.9497473},
	abstract = {Model-based Systems Engineering (MBSE) has been widely used in the development of complex systems. The system architectures, organizations, research and development processes using MBSE to design complex systems can be seen as a System of Systems (SoS), which has high complexity and hard to manage. The concept of digital thread is proposed to integrate all the models and data in the SoS. However, lack of cognition ability makes it hard to connect the models and data with human, processes and things in the SoS, which reduces the efficiency of complex system development. In this paper, a new concept named Cognitive Thread is first proposed as digital thread with augmented semantic capabilities for identifying the information of the SoS. Then a cognitive thread construction approach based on Open Services for Lifecycle Collaboration (OSLC) specification and knowledge graphs is proposed to support decision-making and management in the SoS. Finally, the feasibility of the proposed approach is verified through a case study of the advanced driver-assistance system development.},
	booktitle = {2021 16th {International} {Conference} of {System} of {Systems} {Engineering} ({SoSE})},
	author = {Wu, Shouxuan and Lu, Jinzhi and Hu, Zhenchao and Yang, Pengfei and Wang, Guoxin and Kiritsis, Dimitris},
	month = jun,
	year = {2021},
	keywords = {Natural language processing, Semantics, Cognition, Digital Thread, Data models, Organizations, Systems architecture, Cognitive Thread, Instruction sets, Model-based Systems Engineering, OSLC specification, System of Systems},
	pages = {82--87},
}

@inproceedings{alisa_method_2021,
	title = {Method for {Biomedical} {Information} {Extraction} of {Immunosuppressive} {Cell} {Properties}},
	doi = {10.1109/CSGB53040.2021.9496030},
	abstract = {Automated extraction of cell populations' immunosuppressive properties from research articles is a basic problem, which requires specialized methods and tools for meta-analysis of publications. It is necessary to extract information about specific types of cells, their roles in the text, detect the immunosuppressive properties of cell populations, and certain types of relationships. It is also crucial to filter out those texts, which describe immunosuppressive features of different chemical compounds or drugs. Typically, efficient automatic information extraction requires a relatively large set of samples or marked-up texts. The paper presents a novel information extraction method that can be useful for such an analysis. This method uses external linguistic resources and can be trained on limited corpora. Namely, the method combines medical ontologies, rich unsupervised lexis representations (Fasttext word embeddings) with rule-based entity and relationship extraction, and supervised machine learning-based post-filtering. The developed method allows one to extract information about the target cells (for “in vitro” experiments), effector cells, diseases ("in vivo”), and arbitrary descriptions of immune suppression. In the paper, we also present a manually labeled corpus to train immunosuppressive information extraction methods. That corpus contains texts of 330 PubMed Central abstracts. The experiments on that corpus show the method has relatively high evaluation scores on the labeled dataset. Therefore, the proposed method makes it possible to identify descriptions of the immunosuppressive properties of cell populations in biomedical texts with sufficiently high quality. In the future, the method can be applied to perform an automatic meta-analysis of research in immunosuppressive cell therapy.},
	booktitle = {2021 {IEEE} {Ural}-{Siberian} {Conference} on {Computational} {Technologies} in {Cognitive} {Science}, {Genomics} and {Biomedicine} ({CSGB})},
	author = {Alisa, Gisina and Dmitry, Devyatkin and Anton, Lukin and Alexey, Lupatov and Alexey, Molodchenkov and Irina, Kholodenko},
	month = may,
	year = {2021},
	keywords = {Ontologies, Genomics, Tools, Linguistics, Medical treatment, Information filters, Sociology, biomedical information extraction, properties of immunosuppressive cells, relational-situational analysis},
	pages = {210--213},
}

@inproceedings{giachetti_requirements_2021,
	title = {Requirements for a {System} {Model} in the {Context} of {Digital} {Engineering}},
	doi = {10.1109/SysCon48628.2021.9447088},
	abstract = {The vision of achieving digital engineering in the US Department of Defense has instigated work on defining the information content and structure of the system model. However, few seem to have asked what are the requirements for the system model? In this paper, we use a requirements process to elicit and define the requirements for the system model. The system model is a digital artifact containing descriptions of all the essential objects, their properties, and the relationships between them for the system-of-interest (SoI). The paper describes the context of the system model in relationships to the other components of model-based systems engineering (MBSE) consisting of a modeling language, schema, model-based process, presentation framework, MBSE tools, and knowledgeable workforce. The paper describes how these components interact to provide effective MBSE. Requirements are stated for each component. The paper additionally derives information requirements for the system model according to the systems engineering process’s information needs by examining the inputs and outputs of each activity in the systems engineering process. Lastly, the paper derives the quality characteristics for the system model from the literature on ontologies, modeling languages, and semiotics. The result is a set of requirements for the system model to support MBSE and the digital thread.},
	booktitle = {2021 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Giachetti, Ronald E. and Vaneman, Warren},
	month = apr,
	year = {2021},
	note = {ISSN: 2472-9647},
	keywords = {Ontologies, Project management, Context, Tools, Data models, Conferences, IEEE Standards},
	pages = {1--7},
}

@inproceedings{sarsembayeva_problem_2021,
	title = {The {Problem} of {Named} {Entities} {Unification} based on {Geographical} {Ontologies}},
	doi = {10.1109/AIEEE51419.2021.9435777},
	abstract = {The subject of this research is to develop a system for extracting knowledge from both semi-structured and unstructured data and filling with this system a knowledge base that would provide support for decision-making on any problematic issues. The article deals with the problem of unification of named entities based on geographical ontologies.},
	booktitle = {2020 {IEEE} 8th {Workshop} on {Advances} in {Information}, {Electronic} and {Electrical} {Engineering} ({AIEEE})},
	author = {Sarsembayeva, Talshyn and Mansurova, Madina and Chikibayeva, Darya and Karymsakova, Dariya},
	month = apr,
	year = {2021},
	note = {ISSN: 2689-7342},
	keywords = {Ontologies, machine learning, Machine learning, ontology, Information retrieval, Knowledge engineering, Decision making, Knowledge based systems, Data models, decision support system, knowledge base, knowledge extraction, thesaurus, information extraction, named entities, semi-structured and unstructured data},
	pages = {1--6},
}

@article{zeng_semantic_2021,
	title = {Semantic {Service} {Clustering} {With} {Lightweight} {BERT}-{Based} {Service} {Embedding} {Using} {Invocation} {Sequences}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3069509},
	abstract = {Service clustering is an efficient method for facilitating service discovery and composition. Traditional approaches based on the self-description documents for services usually utilize service signatures. In Web service composition, service clustering can also be performed by the invocation relationship between services. Therefore, based on the successful development of several embedding techniques for words in several contexts, a novel deep learning-based service embedding using invocation sequences is devised for service clustering. Moreover, many microservices are being created because of the rapid development of the Internet of Things (IoT), and edge, and fog computing. Following these developments, Web service composition based on these environments has emerged in abundance. More efficient lightweight approaches to analyze large numbers of services are necessary for service clustering. Consequently, a lightweight deep learning-based approach for the semantic clustering of service composition is presented to address these requirements. In this paper, we first propose the concept of service embedding to capture semantic information from invocation sequences. Second, we suggest using state-of-the-art neural language sequence models for service embedding and develop a corresponding lightweight Bidirectional Encoder Representations of Transformers (BERT)-based model. Next, combined with K-means clustering, the semantic clustering of service composition is evaluated. Finally, the experimental results show that the clustering process can be effectively performed by the lightweight BERT-based model.},
	journal = {IEEE Access},
	author = {Zeng, Kungan and Paik, Incheon},
	year = {2021},
	keywords = {Ontologies, Semantics, Deep learning, Feature extraction, Data mining, Web services, Computational modeling, composition, lightweight BERT, Semantic service clustering, service embedding},
	pages = {54298--54309},
}

@article{morales-garzon_word_2021,
	title = {A {Word} {Embedding}-{Based} {Method} for {Unsupervised} {Adaptation} of {Cooking} {Recipes}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3058559},
	abstract = {Studying food recipes is indispensable to understand the science of cooking. An essential problem in food computing is the adaptation of recipes to user needs and preferences. The main difficulty when adapting recipes is in determining ingredients relations, which are compound and hard to interpret. Word embedding models can catch the semantics of food items in a recipe, helping to understand how ingredients are combined and substituted. In this work, we propose an unsupervised method for adapting ingredient recipes to user preferences. To learn food representations and relations, we create and apply a specific-domain word embedding model. In contrast to previous works, we not only use the list of ingredients to train the model but also the cooking instructions. We enrich the ingredient data by mapping them to a nutrition database to guide the adaptation and find ingredient substitutes. We performed three different kinds of recipe adaptation based on nutrition preferences, adapting to similar ingredients, and vegetarian and vegan diet restrictions. With a 95\% of confidence, our method can obtain quality adapted recipes without a previous knowledge extraction on the recipe adaptation domain. Our results confirm the potential of using a specific-domain semantic model to tackle the recipe adaptation task.},
	journal = {IEEE Access},
	author = {Morales-Garzón, Andrea and Gómez-Romero, Juan and Martin-Bautista, Maria J.},
	year = {2021},
	keywords = {Ontologies, natural language processing, Vocabulary, Data models, Databases, Computational modeling, word embedding, Data mapping, food computing, Adaptation models, Task analysis, recipe adaptation},
	pages = {27389--27404},
}

@article{amato_model_2021,
	title = {A {Model} for {Verification} and {Validation} of {Law} {Compliance} of {Smart} {Contracts} in {IoT} {Environment}},
	volume = {17},
	issn = {1941-0050},
	doi = {10.1109/TII.2021.3057595},
	abstract = {The interest of Industry 4.0 in smart contracts and blockchain technologies is growing up day by day. Smart contracts have enabled new kinds of interactions whereby contractors can even fully automate processes they agree on. This technology is really appealing in Internet of Things (IoT) domain because smart devices generate events for software agents involved in a smart contract execution, making full automation possible. However, smart contracts have to comply with national and international laws and accountability of participant's actions. Soundness of a smart contract has to be verified in terms of law compliance. Here, we propose a model for verification and validation of law compliance of smart contracts in IoT environments. The main goal of this article is to propose a formal model (based on multiagent logic and ontological description of contracts) for validating law compliance of smart contracts and to determine potential responsibilities of failures.},
	number = {11},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Amato, Flora and Cozzolino, Giovanni and Moscato, Francesco and Moscato, Vincenzo and Xhafa, Fatos},
	month = nov,
	year = {2021},
	keywords = {Blockchain, Internet of Things, Unified modeling language, Internet of Things (IoT), industry 4.0, Law, Smart contracts, smart contracts, Automobiles, Analytical models, Insurance, multiagent systems},
	pages = {7752--7759},
}

@article{balaraman_domain-aware_2021,
	title = {Domain-{Aware} {Dialogue} {State} {Tracker} for {Multi}-{Domain} {Dialogue} {Systems}},
	volume = {29},
	issn = {2329-9304},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100500450&doi=10.1109%2FTASLP.2021.3054309&partnerID=40&md5=12198f262ed6a6e21de55d25965957df},
	doi = {10.1109/TASLP.2021.3054309},
	abstract = {In task-oriented dialogue systems the dialogue state tracker component (DST) is responsible for predicting the current state of the dialogue based on the dialogue history and the user utterance. Current DST approaches rely on a predefined domain ontology, a fact that limits their effective usage for large scale conversational agents, where the DST constantly needs to be interfaced with ever-increasing services and APIs. Focused towards overcoming this drawback, we propose a domain-aware dialogue state tracker, that is completely data-driven and it is modeled to predict for dynamic service schemas, including zero-shot domains. Unlike approaches that propose separate models for prediction of intents, requested slots, slot status, categorical slots and non-categorical slots, we propose a single model in an end-to-end architecture. The proposed model utilizes domain and slot information to extract both domain and slot specific representations from a given dialogue, and then uses such representations to predict the values of the corresponding slot in a given domain. Integrating this mechanism with pretrained language models, our approach can effectively learn semantic relations and effectively perform transfer learning between domains or zero-shot tracking for domains not present in training.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Balaraman, Vevake and Magnini, Bernardo},
	year = {2021},
	note = {Type: Article},
	keywords = {Ontologies, Language model, Semantics, Domain ontologies, Forecasting, Transfer learning, Conversational agents, Dialogue state tracking, Semantic relations, Dialogue systems, Speech recognition, end-to-end, Training data, Predictive models, Task analysis, Speech processing, Bit error rate, multi-domain dialogue systems, Virtual assistants, zero-shot tracking, Task-oriented, Dynamic services, Single models},
	pages = {866--873},
	annote = {Cited by: 16; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{stojanov_toward_2020,
	title = {Toward {Robust} {Food} {Ontology} {Mapping}},
	doi = {10.1109/BigData50022.2020.9378066},
	abstract = {Data normalization methodologies are extremely welcome to link extracted information from textual data to different semantic resources. These methodologies have been previously well researched especially in the biomedical domain, where health concepts were normalized and described using semantic tags. Recently, a methodology for normalizing food concepts has been proposed, based on Named-Entity Recognition methods resulting in the FoodOntoMap semantic resource. In this paper, we propose and evaluate a new architecture for linking phrases (i.e. textual name for foods) to concepts from semantic resources in the Food and Nutrition domain. We represent the food phrases (i.e. their textual name) in continuous vector space using state-of-the-art Natural Language Processing (NLP) embedding algorithms, and evaluate their proximity with respect to the annotated semantic food concepts. Additionally, indexing was incorporated to improve efficiency.The GloVe embedding with mean pooling provided best evaluation results, with maximum recall of 74\% for the Snomed CT semantic dataset, which is promising result, but also opens a space for future improvement of the phrase representations, and their incorporation in this system.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Stojanov, Riste and Kocev, Ilija and Gramatikov, Sasho and Popovski, Gorjan and Koroušić Seljak, Barbara and Eftimov, Tome},
	month = dec,
	year = {2020},
	keywords = {Ontologies, Natural language processing, Semantics, Natural Language Processing, Big Data, Embeddings, Taxonomy, Indexing, Syntactics, Text representation, Data normalization and linking},
	pages = {3596--3601},
}

@inproceedings{jiang_understanding_2020,
	title = {Understanding {Contexts} {Inside} {Robot} and {Human} {Manipulation} {Tasks} through {Vision}-{Language} {Model} and {Ontology} {System} in {Video} {Streams}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094109055&doi=10.1109%2FIROS45743.2020.9340905&partnerID=40&md5=8f2b1e4a2ac15acd484c559e19070193},
	doi = {10.1109/IROS45743.2020.9340905},
	abstract = {Manipulation tasks in daily life, such as pouring water, unfold through human intentions. Being able to process contextual knowledge from these Activities of Daily Living (ADLs) over time can help us understand manipulation intentions, which are essential for an intelligent robot to transition smoothly between various manipulation actions. In this paper, to model the intended concepts of manipulation, we present a vision dataset under a strictly constrained knowledge domain for both robot and human manipulations, where manipulation concepts and relations are stored by an ontology system in a taxonomic manner. Furthermore, we propose a scheme to generate a combination of visual attentions and an evolving knowledge graph filled with commonsense knowledge. Our scheme works with real-world camera streams and fuses an attention-based Vision-Language model with the ontology system. The experimental results demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans. The proposed scheme allows the robot to mimic human-like intentional behaviors by watching real-time videos. We aim to develop this scheme further for real-world robot intelligence in Human-Robot Interaction.},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Jiang, Chen and Dehghan, Masood and Jagersand, Martin},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Ontologies, Ontology, Knowledge representation, Contextual knowledge, Social robots, Computational linguistics, Visualization, Commonsense knowledge, Videos, Real-time systems, Task analysis, Intelligent robots, Fuses, Behavioral research, Knowledge domains, Activities of daily living (ADLs), Agricultural robots, Computer hardware description languages, Human manipulation, Manipulation task, Object manipulation, Robot intelligences, Video streaming},
	pages = {8366--8372},
	annote = {Cited by: 9},
}

@inproceedings{batista_conception_2020,
	title = {The conception of a large-scale {Systems} {Engineering} environment},
	doi = {10.1109/DASC50938.2020.9256709},
	abstract = {With the rise of artificial intelligence, it is time to shape the systems engineering tooling environment for the future. In the last decade, we have seen several emerging technologies that will potentially have a great impact in complex systems. These new technologies are expected to cause a disruptive impact not only in the products but also in to the tools used across the whole product life cycle. For this reason, is imperative to perform a critical review of the current systems engineering tooling ecosystem. This assessment should also map the open research problems that could prevent the complete integration of the new technologies into the systems engineering framework. This paper proposes a new architecture for a system engineering environment to operate in large scale projects. The objective of this research is twofold: it will first identify the capabilities for the next generation platform, and secondly, it will evaluate how artificial intelligence applications can be integrated in compliance with DO-330. The concept developed by this research will drive tool design recommendations enabling the use of artificial intelligence driven applications in a systems engineering tooling ecosystem.},
	booktitle = {2020 {AIAA}/{IEEE} 39th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
	author = {Batista, Leandro and Monsuez, Bruno},
	month = oct,
	year = {2020},
	note = {ISSN: 2155-7209},
	keywords = {Artificial Intelligence, MBSE, Modeling, Vocabulary, Standards, Tools, Systems Engineering, Complex systems, Systems engineering and theory, Ecosystems, DO-330},
	pages = {1--7},
}

@inproceedings{helmke_machine_2020,
	title = {Machine {Learning} of {Air} {Traffic} {Controller} {Command} {Extraction} {Models} for {Speech} {Recognition} {Applications}},
	doi = {10.1109/DASC50938.2020.9256484},
	abstract = {Increasing digitization and automation is a widely accepted method to cope with the challenges of constantly increasing air traffic. The analogue communication of air traffic controllers (ATCo) to pilots has been excluded so far from the digitization process. However, the content of this communication is of decisive importance for various automation systems. Although Assistant Based Speech Recognition (ABSR) has recently significantly improved the recognition performance and, therefore, enables the digitization of ATCo-pilot-communication, its adaptation to other airports is a critical and costly process, This is even more important, if ATCos tend to deviate from the published ICAO phraseology: “start reducing to two fifty” instead of “reduce two five zero knots” is just an example. User acceptance requires that these deviations are also correctly recognized. Therefore, this paper presents an approach, which automatically learns a so-called Command Extraction Model from labelled controller utterances. The initial Command Extraction Model without learning only covers 60\% of the commands, whereas the automatically learned Command Extraction Model covers more than 98\%. With just six hours of training data we could achieve 94\%.},
	booktitle = {2020 {AIAA}/{IEEE} 39th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
	author = {Helmke, Hartmut and Kleinert, Matthias and Ohneiser, Oliver and Ehr, Heiko and Shetty, Shruthi},
	month = oct,
	year = {2020},
	note = {ISSN: 2155-7209},
	keywords = {Ontologies, Ontology, Annotations, Machine Learning, Annotation, Automatic Speech Recognition, Speech recognition, Engines, Adaptation models, Atmospheric modeling, Airports, Controller Command Extraction Model},
	pages = {1--9},
}

@inproceedings{novacek_lemons_2020,
	title = {Lemons: {Leveraging} {Model}-{Based} {Techniques} to {Enable} {Non}-{Intrusive} {Semantic} {Enrichment} in {Wireless} {Sensor} {Networks}},
	doi = {10.1109/SEAA51224.2020.00092},
	abstract = {The paper presents an efficient approach to the semantic enrichment of measured sensor data in Wireless Sensor Networks (WSNs), by bridging techniques from Model-driven Software Development (MDSD) and Semantic Web Technology (SWT). Our approach reinforces data interoperability, fostering data sharing and reuse, by utilizing SWT. Model-based and type-agnostic configuration reduces the overall effort for WSN setup and maintenance, which are traditionally complex and time-consuming tasks. The presented approach addresses the problem of large-scale WSN management through the application of SWT in WSN configuration and management without requiring expert knowledge. Additionally, we present a generic architecture and an implementation which is also supplemented by hands-on descriptions of an illustrative use case. Our experimental results demonstrate that our model-based approach provides non-intrusive semantic enrichment with sub-millisecond computational overhead, as well as partially automated configuration of WSNs.},
	booktitle = {2020 46th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	author = {Novacek, Jan and Kühlwein, Arthur and Reiter, Sebastian and Viehl, Alexander and Bringmann, Oliver and Rosenstiel, Wolfgang},
	month = aug,
	year = {2020},
	keywords = {Ontologies, Semantics, OWL, Internet of Things, Software, Data models, Semantic Web of Things, Robot sensing systems, DevOps, Wireless sensor networks, Knowledge-based Engineering, Model-Driven Software Development, Wireless Sensor Networks},
	pages = {561--568},
}

@inproceedings{krungklang_analysis_2020,
	title = {An {Analysis} of {Natural} {Language} {Text} {Relating} to {Thai} {Criminal} {Law}},
	doi = {10.1109/ECAI50035.2020.9223143},
	abstract = {This paper analyses Thailand's criminal law enforcement in chapter 1, Offenses causing death section category section 288 and 289 of title 10 offenses affecting life and body under the Thai Criminal Code. The first part of this paper is using criminal law domain knowledge and supreme court judgment results, to be the initial domain information and result is the rules that humans can understand. The second part of this paper is bringing training data set from the final judgment to train with deep learning methods. Due to the training set which have severe imbalances, the Synthetic Minority Over-Sampling TEchnique (SMOTE) [1] is used to solve this problem. Models are trained on the training set using unidirectional Long Short-Term Memory (LSTM) [2] networks and bidirectional Long Short-Term Memory (BiLSTM) [3] are type of Recurrent Neural Networks (RNN) [2]. The word embeddings of the dataset can be learned while training a deep neural network. BiLSTM average F1 score is higher than LSTM. Pre-trained word embeddings are then used to make the average F1 score higher than before. Finally, using models to predict online crime news, the highest average probability of each model is selected by using Soft Voting as input to the rules. The test results compared with the predictions of our methods with the opinion of the lawyer, corresponding 76\%.},
	booktitle = {2020 12th {International} {Conference} on {Electronics}, {Computers} and {Artificial} {Intelligence} ({ECAI})},
	author = {Krungklang, Weerayut and Sinthupinyo, Sukree},
	month = jun,
	year = {2020},
	keywords = {Ontologies, Criminal law, Decision tree, LSTM, Feature extraction, Word embedding, Data models, Training, BiLSTM, SMOTE, Word2Vec, Classification algorithms, Decision trees, Deepcut, Pre-trained word embeddings, Soft Voting, Thai Supreme Court},
	pages = {1--6},
}

@inproceedings{parvizimosaed_towards_2020,
	title = {Towards the {Specification} and {Verification} of {Legal} {Contracts}},
	doi = {10.1109/RE48521.2020.00066},
	abstract = {A contract is a legally binding agreement that expresses high-level requirements of parties in terms of obligations, powers and constraints. Parties' actions influence the status of a contract and shall comply with its clauses. Manual contract monitoring is very laborious in real markets, such as transactive energy, where plenty of complex contracts are running concurrently. Furthermore, liability, right and performance transition through run-time operations such as subcontracting, assignment and substitution complicate contract interpretation. Automation is needed to ensure that contracts respect desirable properties and to support monitoring of compliance and handling of violations. In this thesis research, I propose an innovative ontology that defines fundamental contractual notions (such as the ones mentioned above) and their relationships, on which is built a specification language, called Symboleo, that provides syntax and axiomatic semantics of contracts via first-order logic. Symboleo enables the development of advanced automation tools such as a compliance checker that monitors contracts at runtime, and a model checking verification method that analyzes liveness and safety properties of contracts. This paper reports on the problem domain, research method, current status, expected contributions, and main foreseen challenges.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Parvizimosaed, Alireza},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	keywords = {Ontologies, Ontology, Tools, Law, Smart Contract, Monitoring, Model Checking, Contracts, Legal Contract, Specification Language},
	pages = {445--450},
}

@inproceedings{jiang_bridging_2020,
	title = {Bridging {Visual} {Perception} with {Contextual} {Semantics} for {Understanding} {Robot} {Manipulation} {Tasks}},
	volume = {2020-August},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094164796&doi=10.1109%2FCASE48305.2020.9216770&partnerID=40&md5=ecdffea342d89820ff110175eca432ec},
	doi = {10.1109/CASE48305.2020.9216770},
	abstract = {Understanding manipulation scenarios allows intelligent robots to plan for appropriate actions to complete a manipulation task successfully. It is essential for intelligent robots to semantically interpret manipulation knowledge by describing entities, relations and attributes in a structural manner. In this paper, we propose an implementing framework to generate high-level conceptual dynamic knowledge graphs from video clips. A combination of a Vision-Language model and an ontology system, in correspondence with visual perception and contextual semantics, is used to represent robot manipulation knowledge with Entity-Relation-Entity (E-R-E) and Entity-Attribute-Value (E-A-V) tuples. The proposed method is flexible and well-versed. Using the framework, we present a case study where robot performs manipulation actions in a kitchen environment, bridging visual perception with contextual semantics using the generated dynamic knowledge graphs.},
	booktitle = {2020 {IEEE} 16th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Jiang, Chen and Jagersand, Martin},
	month = aug,
	year = {2020},
	note = {ISSN: 2161-8089},
	keywords = {Ontologies, Knowledge graphs, Language model, Semantics, Knowledge representation, Robot manipulation, Vision, Task analysis, Intelligent robots, Robot kinematics, Manipulator dynamics, Visual perception, Visual languages, Manipulation task, Attribute values, Contextual semantics, Ontology system},
	pages = {1447--1452},
	annote = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{sai_sharath_question_2020,
	title = {Question {Answering} over {Knowledge} {Base} using {Language} {Model} {Embeddings}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093833070&doi=10.1109%2FIJCNN48605.2020.9206698&partnerID=40&md5=b38f01ccb83edf9fb6b1001f74af82d3},
	doi = {10.1109/IJCNN48605.2020.9206698},
	abstract = {Knowledge Base, represents facts about the world, often in some form of subsumption ontology, rather than implicitly, embedded in procedural code, the way a conventional computer program does. While there is a rapid growth in knowledge bases, it poses a challenge of retrieving information from them. Knowledge Base Question Answering is one of the promising approaches for extracting substantial knowledge from Knowledge Bases. Unlike web search, Question Answering over a knowledge base gives accurate and concise results, provided that natural language questions can be understood and mapped precisely to an answer in the knowledge base. However, some of the existing embedding-based methods for knowledge base question answering systems ignore the subtle correlation between the question and the Knowledge Base (e.g., entity types, relation paths, and context) and suffer from the Out Of Vocabulary problem. In this paper, we focused on using a pre-trained language model for the Knowledge Base Question Answering task. Firstly, we used Bert base uncased for the initial experiments. We further fine-tuned these embeddings with a two way attention mechanism from the knowledge base to the asked question and from the asked question to the knowledge base answer aspects. Our method is based on a simple Convolutional Neural Network architecture with a Multi-Head Attention mechanism to represent the asked question dynamically in multiple aspects. Our experimental results show the effectiveness and the superiority of the Bert pre-trained language model embeddings for question answering systems on knowledge bases over other well-known embedding methods.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Sai Sharath, Japa and Banafsheh, Rekabdar},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Knowledge discovery, Semantics, BERT, Information retrieval, Embeddings, Knowledge based systems, Context modeling, Computational linguistics, Language Model, Convolutional neural networks, Question Answering, Attention mechanisms, Natural languages, Task analysis, Network architecture, Bit error rate, KBQA, knowledge base question answering, Multi-Head Attention, Natural language processing systems, Question answering systems, Natural language questions, Embedding method, Question Answering Task, Conventional computers, Procedural codes},
	pages = {1--8},
	annote = {Cited by: 10; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{larhrib_converting_2020,
	title = {Converting {OCL} and {CGMES} {Rules} to {SHACL} in {Smart} {Grids}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3026941},
	abstract = {Models are first-class elements in Model-Driven Engineering (MDE). In this paradigm, the most widespread approaches adopted by the development community are Object-Oriented and ontological, formalized using Unified Modeling Language (UML) and Resource Description Framework (RDF), respectively. However, Object Management Group (OMG) does not provide a specific standard language for validating UML models against Object Constraints Language (OCL) constraints; meanwhile, World Wide Web Consortium (W3C) has defined Shapes Constraint Language (SHACL) as a standard validation language. Although the transformation between UML and RDF can be performed at the structural level, no effort has been made to transform OCL to SHACL. This paper addresses the transformation of OCL and text-based constraints to SHACL shapes in the context of Common Grid Model Exchange Standard (CGMES), a UML-based standard for electric utilities in Europe. This paper presents several contributions to the software engineering community. First, solving the validation problem in a standardized way. Second, facilitating European Network of Transmission System Operators for Electricity (ENTSO-E) the construction of an ontology associated with the CGMES standard. Third, allowing developers to integrate the two complementary approaches. Finally, Promoting the adoption and integration of the ontological approach in the software community.},
	journal = {IEEE Access},
	author = {Larhrib, Mohamed and Escribano, Miguel and Cerrada, Carlos and Escribano, Juan Jose},
	year = {2020},
	keywords = {Ontologies, ontology, Resource description framework, Unified modeling language, Object oriented modeling, IEC Standards, CIM for ENTSO-E (CGMES), Common Information Model (electricity), OCL rules, RDF/RDFS, SHACL standard},
	pages = {177255--177266},
}

@inproceedings{mordecai_object-process_2020,
	title = {Object-{Process} {Model}-{Based} {Operational} {Viewpoint} {Specification} for {Aerospace} {Architectures}},
	doi = {10.1109/AERO47225.2020.9172685},
	abstract = {Remote-controlled or autonomous multi-rotor air vehicles, or drones, have become common and commercially available even to individual consumers, mostly for imaging purposes. Drones appeal to mission architects looking to extend the toolbox provided to operators performing challenging missions such as public safety operations. However, careful analysis of the operational context and concept of operations must take place before major acquisitions. The purpose of this paper is to propose a model-based operational architecture definition framework, which is based on the Department of Defense Architecture Framework (DoDAF) ontology and uses Object Process Methodology (OPM) as its underlying modeling language. Through careful mapping of DoDAF Operational Viewpoint (OV) ontology to OPM ontology, we were able to show that the entire OV ontology can be covered by a small set of objects, processes, relations among them, and constructs comprising them. We then show how to instantiate the ontology to create a model of an actual architecture of interest (AoI) while maintaining strong typing of the model elements to ensure validity, integrity, consistency, and continuous compliance with the OV. We demonstrate our approach on the case of using drones in public safety enterprises for the purpose of crowd management in massively attended events and locations. The proposed framework allows for capturing ConOps and OpsCon in a lightweight, yet robust and consistent manner, and improve communication and concept validation between operational stakeholders and enterprise architects.},
	booktitle = {2020 {IEEE} {Aerospace} {Conference}},
	author = {Mordecai, Yaniv and James, Nicholas K. and Crawley, Edward F.},
	month = mar,
	year = {2020},
	note = {ISSN: 1095-323X},
	keywords = {Ontologies, Safety, Stakeholders, Imaging, Conferences, Drones},
	pages = {1--15},
}

@inproceedings{alamsyah_recognizing_2020,
	title = {Recognizing {Personality} from {Social} {Media} {Linguistic} {Cues}: {A} {Case} {Study} of {Brand} {Ambassador} {Personality}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091032378&doi=10.1109%2FICoICT49345.2020.9166221&partnerID=40&md5=5bbc0e86df816c44ea4c031a13b59e60},
	doi = {10.1109/ICoICT49345.2020.9166221},
	abstract = {The burgeoning need of a brand ambassador (BA) as a company representative begin to rise in recent year. The phenomena followed by the increase of method to select the most suitable BA. The universal way of selecting one appropriate ambassador is by understanding their personality, therefore, measurement of a BA personality considered as one way to characterize a company credibility. This research proposes to design a method of measuring the BA personality from their social media data in Bahasa Indonesia. We enrich the methodology to measure human personality using the ontology modeling approach. The ontology model constructed under the ngram language model which provides a rapid and effective way of measuring a BA personality. The results of a BA personality measurement allow the utilization to portray of how an ambassador represent their brand and interact with their customer.},
	booktitle = {2020 8th {International} {Conference} on {Information} and {Communication} {Technology} ({ICoICT})},
	author = {Alamsyah, Andry and Bastikarana, Rafa Syafiq and Ramadhanti, Alya Rysda and Widiyanesti, Sri},
	month = jun,
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Ontologies, Ontology, Ontology model, Social media, Psychology, Design methodology, Linguistics, Accuracy, Social networking (online), Companies, Brand Ambassador, Information and communication technology, Personality Measurement, Time measurement, Weight measurement, Social media datum, Indonesia, N-gram language models},
	pages = {1--5},
	annote = {Cited by: 4},
}

@article{huang_combination_2020,
	title = {Combination of {ELMo} {Representation} and {CNN} {Approaches} to {Enhance} {Service} {Discovery}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3009393},
	abstract = {With the rapid growth of Web services, the demand for discovering the optimal services to satisfy the users' requirements is no longer an easy task. The critical issue in the process of service discovery is to conduct a similarity calculation. To solve such an issue, this study proposes an effective approach that combines the Embeddings from Language Models (ELMo) representation and Convolutional Neural Network (CNN) to obtain a more accurate similarity score for retrieving target Web services. More specifically, first, the study adopts the ELMo model to generate effective word representations for capturing the sufficient information from services and queries. Then, the word representations are used to compose a similarity matrix, which will be taken as the input for the CNN to learn the matching relationships. Finally, the combination of the ELMo representation and CNN is used to address the representation and interaction processes within the matching task to improve the service discovery performance. The results demonstrate the effectiveness of our proposed approach for retrieving better targeted Web services.},
	journal = {IEEE Access},
	author = {Huang, Zhao and Zhao, Wei},
	year = {2020},
	keywords = {Ontologies, Semantics, Service-oriented architecture, web service, CNN, Service discovery, Linguistics, Syntactics, Task analysis, ELMo, service similarity},
	pages = {130782--130796},
}

@inproceedings{torres_modeling_2020,
	title = {Modeling of {IoT} devices in {Business} {Processes}: {A} {Systematic} {Mapping} {Study}},
	volume = {1},
	doi = {10.1109/CBI49978.2020.00031},
	abstract = {The Internet of Things (IoT) enables to connect the physical world to digital business processes (BP). By using the IoT, a BP can, e.g.: 1) take into account real-world data to take more informed business decisions, and 2) automate and/or improve BP tasks. To achieve these benefits, the integration of IoT and BPs needs to be successful. The first step to this end is to support the modeling of IoT-enhanced BPs. Although numerous researchers have studied this subject, it is unclear what is the current state of the art in terms of current modeling solutions and gaps. In this work, we carry out a Systematic Mapping Study (SMS) to find out how current solutions are modelling IoT into business processes. After studying 600 papers, we identified and analyzed in depth a total of 36 different solutions. In addition, we report on some important issues that should be addressed in the near future, such as, for instance the lack of standardization.},
	booktitle = {2020 {IEEE} 22nd {Conference} on {Business} {Informatics} ({CBI})},
	author = {Torres, Victoria and Serral, Estefanía and Valderas, Pedro and Pelechano, Vicente and Grefen, Paul},
	month = jun,
	year = {2020},
	note = {ISSN: 2378-1971},
	keywords = {Internet of Things, Unified modeling language, Systematic mapping study, Business process modeling, Computers, Conferences, Systematics, Task analysis, IoT devices, IoT-enhanced BP},
	pages = {221--230},
}

@article{yonglin_ontological_2020,
	title = {An ontological metamodeling framework for semantic simulation model engineering},
	volume = {31},
	issn = {1004-4132},
	doi = {10.23919/JSEE.2020.000032},
	abstract = {Recently, the ontological metamodel plays an increasingly important role to specify systems in two forms: ontology and metamodel. Ontology is a descriptive model representing reality by a set of concepts, their interrelations, and constraints. On the other hand, metamodel is a more classical, but more powerful model in which concepts and relationships are represented in a prescriptive way. This study firstly clarifies the difference between the two approaches, then explains their advantages and limitations, and attempts to explore a general ontological metamodeling framework by integrating each characteristic, in order to implement semantic simulation model engineering. As a proof of concept, this paper takes the combat effectiveness simulation systems as a motivating case, uses the proposed framework to define a set of ontological composable modeling frameworks, and presents an underwater targets search scenario for running simulations and analyzing results. Finally, this paper expects that this framework will be generally used in other fields.},
	number = {3},
	journal = {Journal of Systems Engineering and Electronics},
	author = {Yonglin, Lei and Zhi, Zhu and Qun, Li},
	month = jun,
	year = {2020},
	keywords = {Ontologies, Semantics, ontology, Unified modeling language, Metamodeling, Computational modeling, metamodeling, model-driven engineering (MDE), Analytical models, semantic composability},
	pages = {527--538},
}

@inproceedings{brummett_model-driven_2020,
	title = {A {Model}-driven {Middleware} {Integration} {Approach} for {Performance}-{Sensitive} {Distributed} {Simulations}},
	doi = {10.1109/ISORC49007.2020.00019},
	abstract = {Complex simulation systems often comprise multiple distributed simulators that need to interoperate and synchronize states and events. In many cases, the simulation logics which are developed by different teams with specific expertise, need to be integrated to build a complete simulation system. Thus, supporting composability and reusability of simulation functionalities with minimal integration and performance overhead is a challenging but required capability. Middleware for game engines are promising to realize both the modular and reusable development criteria as well as the high performance requirements, while data-centric publish/subscribe middleware can support seamless integration and synchronization of the distributed artifacts. However, differences in the level of abstraction at which these middleware operate and the semantic differences in their underlying ontologies make it hard and challenging for simulation application developers and system integrators to realize a complete, operational system. To that end this paper presents a model-driven approach to blending the two middleware, wherein the modeling capabilities provide intuitive and higher-level abstractions for developers to reason about the composition and validation of the complete system, and the generative capabilities address the inherent and accidental complexities incurred in reconciling the semantic differences between the gaming and pub/sub middleware. We present a concrete implementation of our approach and illustrate its use and performance results using simple use cases.},
	booktitle = {2020 {IEEE} 23rd {International} {Symposium} on {Real}-{Time} {Distributed} {Computing} ({ISORC})},
	author = {Brummett, Travis and An, Kyoungho and Gokhale, Aniruddha and Mertens, Sanders},
	month = may,
	year = {2020},
	note = {ISSN: 2375-5261},
	keywords = {Ontologies, Automation, Semantics, Standardization, Data models, Complexity theory, Distributed Simulation, Real-time systems, Composable Simulation, Data Distribution Service, Entity Component System, Model Driven Engineering, Ports (computers)},
	pages = {65--73},
}

@inproceedings{chondamrongkul_automated_2020,
	title = {Automated {Security} {Analysis} for {Microservice} {Architecture}},
	doi = {10.1109/ICSA-C50368.2020.00024},
	abstract = {Designing a software system that applied the microservice architecture style is a challenging task, as its characteristics are vulnerable to various security attacks. Software architect, therefore, needs to pinpoint the security flaws in the design before the implementation can proceed. This task is error-prone as it requires manual analysis on the design model, to identify security threats and trace possible attack scenarios. This paper presents an automated security analysis approach for microservice architecture. Our approach can automatically identify security threats according to a collection of formally defined security characteristics and provide an insightful result that demonstrates how the attack scenarios may happen. A collection of formally defined security characteristics can be extended to support other security characteristics not addressed in this paper.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Architecture} {Companion} ({ICSA}-{C})},
	author = {Chondamrongkul, Nacha and Sun, Jing and Warren, Ian},
	month = mar,
	year = {2020},
	keywords = {Ontologies, Security, Ontology Web Language, Tools, Model Checking, Computer architecture, Analytical models, Connectors, Containers, Microservice Architecture, Security Analysis},
	pages = {79--82},
}

@inproceedings{matveev_virtual_2020,
	title = {A {Virtual} {Dialogue} {Assistant} for {Conducting} {Remote} {Exams}},
	doi = {10.23919/FRUCT48808.2020.9087557},
	abstract = {In this paper, we demonstrate issues and possible solutions to building an Artificial Intelligence Dialogue Assistant for human-machine communication. We specialize it for conducting written exams at online education platforms, talk about the main logical components of the system: knowledge base, question encoder, question generation module, question analysis module. As a knowledge base we consider text fragments representing parts of the course of text fragments representing parts of a course and is a source for a format ontology; and is also sourced for neural network generation of fact-based questions in question generation module and building dependency trees for answer evaluation in question analysis module.},
	booktitle = {2020 26th {Conference} of {Open} {Innovations} {Association} ({FRUCT})},
	author = {Matveev, Anton and Makhnytkina, Olesia and Lizunova, Inna and Vinogradova, Taisiia and Chirkovskii, Artem and Svischev, Aleksei and Mamaev, Nikita},
	month = apr,
	year = {2020},
	note = {ISSN: 2305-7254},
	keywords = {Ontologies, Neural networks, Knowledge based systems, Technological innovation, Complexity theory, Buildings, Solids},
	pages = {284--290},
}

@inproceedings{han_domain-specific_2020,
	title = {Domain-{Specific} {Image} {Caption} {Generator} with {Semantic} {Ontology}},
	doi = {10.1109/BigComp48618.2020.00-12},
	abstract = {Image captioning is the task of generating textual descriptions of a given image, requiring techniques of computer vision and natural language processing. Recent models have utilized deep learning techniques for this task to gain performance improvement. However, these models can neither fully use information included in a given image such as object and attribute, nor generate a domain-specific caption because existing methods use open dataset such as MSCOCO which include general images. To overcome these limitations, this paper proposes a domain-specific image caption generator, which generates a caption based on attention mechanism with object and attribute information, and reconstruct a generate caption using a semantic ontology to provide natural language description for given specific-domain. To show the effectiveness of the proposed model, we evaluate the image caption generator with a dataset, MSCOCO, quantitatively and qualitatively.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} and {Smart} {Computing} ({BigComp})},
	author = {Han, Seung-Ho and Choi, Ho-Jin},
	month = feb,
	year = {2020},
	note = {ISSN: 2375-9356},
	keywords = {Ontologies, Semantics, Machine learning, Feature extraction, Visualization, Generators, attention model, attribute predictionm, domain-specific ontology, image captioning, Image reconstruction},
	pages = {526--530},
}

@inproceedings{lai_simple_2020,
	title = {A {Simple} {But} {Effective} {Bert} {Model} for {Dialog} {State} {Tracking} on {Resource}-{Limited} {Systems}},
	doi = {10.1109/ICASSP40776.2020.9053975},
	abstract = {In a task-oriented dialog system, the goal of dialog state tracking (DST) is to monitor the state of the conversation from the dialog history. Recently, many deep learning based methods have been proposed for the task. Despite their impressive performance, current neural architectures for DST are typically heavily-engineered and conceptually complex, making it difficult to implement, debug, and maintain them in a production setting. In this work, we propose a simple but effective DST model based on BERT. In addition to its simplicity, our approach also has a number of other advantages: (a) the number of parameters does not grow with the ontology size (b) the model can operate in situations where the domain ontology may change dynamically. Experimental results demonstrate that our BERT-based model outperforms previous methods by a large margin, achieving new state-of-the-art results on the standard WoZ 2.0 dataset 1. Finally, to make the model small and fast enough for resource-restricted systems, we apply the knowledge distillation method to compress our model. The final compressed model achieves comparable results with the original model while being 8x smaller and 7x faster.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Lai, Tuan Manh and Hung Tran, Quan and Bui, Trung and Kihara, Daisuke},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Ontologies, BERT, Standards, Task-Oriented Dialog Systems, Production, Task analysis, Signal processing, Speech processing, Bit error rate, Dialog State Tracking, Knowledge Distillation},
	pages = {8034--8038},
}

@article{bandyszak_orthogonal_2020,
	title = {Orthogonal {Uncertainty} {Modeling} in the {Engineering} of {Cyber}-{Physical} {Systems}},
	volume = {17},
	issn = {1558-3783},
	doi = {10.1109/TASE.2020.2980726},
	abstract = {Software-intensive cyber-physical systems (CPS) perform essential tasks such as controlling automated production processes in industrial production plants. The required levels of autonomy, openness, and self-adaptation, as well as the dynamic nature of the context of such CPS, result in challenging tasks for their engineering. During operation, unexpected situations in which the system has insufficient knowledge about the current state of the system itself as well as its context may occur. Engineering CPS, e.g., for industrial production sites, must account for such uncertainties the system will have to cope with during its lifetime in a structured and systematic way. Since the development of CPS requires consideration of different system perspectives, current uncertainty modeling approaches cannot be applied right away, as they do not explicitly consider uncertainty aspects that affect different artifacts. To aid the engineering of CPS, this article presents a model-based approach to document uncertainty. We propose “Orthogonal Uncertainty Models,” which closely integrate with other engineering artifacts from different perspectives, as a means for capturing a dedicated uncertainty viewpoint. Our approach has been evaluated in the industry automation domain. The application shows that the idea of regarding uncertainty within a dedicated perspective is highly beneficial. Particularly, our approach helps to uncover and document uncertainties related to behavioral, functional, and structural properties of a system, as well as uncertainties related to business models that would otherwise possibly remain covert. Note to Practitioners-Identifying and documenting uncertainties, which may occur during operation of a system, is a common problem in engineering processes. Such uncertainties may lead to severe damage, and thus need to be mitigated appropriately. It is crucial to account for these uncertainties during engineering, especially in the early phases. Depending on the specific project characteristics, a multitude of different diagram types are used to model a system. Uncertainties thus reflect in many artifacts, which leads to: 1) redundancies in the specified uncertainty attached to diagram elements and 2) uncertainty information (e.g., about the cause or effect of uncertainty) that is spread across different diagrams. The latter makes it difficult to structure uncertainty information and trace it throughout the engineering process so that uncertainty can be systematically considered. Our approach provides a graphical modeling language that employs a dedicated perspective on uncertainty in separate diagrams that can be linked to any engineering artifact.},
	number = {3},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Bandyszak, Torsten and Daun, Marian and Tenbergen, Bastian and Kuhs, Patrick and Wolf, Stefanie and Weyer, Thorsten},
	month = jul,
	year = {2020},
	keywords = {Uncertainty, model-based engineering, Context modeling, Cyber-physical systems, Collaboration, Robot sensing systems, Production, uncertainty, Runtime, industry automation case study, orthogonal modeling, uncertainty modeling},
	pages = {1250--1265},
}

@article{noauthor_ieee_2020,
	title = {{IEEE} {Standard} for an {Architectural} {Framework} for the {Internet} of {Things} ({IoT})},
	doi = {10.1109/IEEESTD.2020.9032420},
	abstract = {An architecture framework description for the Internet of Things (IoT) which conforms to the international standard ISO/IEC/IEEE 42010:2011 is defined. The architecture framework description is motivated by concerns commonly shared by IoT system stakeholders across multiple domains (transportation, healthcare, Smart Grid, etc.). A conceptual basis for the notion of things in the IoT is provided and the shared concerns as a collection of architecture viewpoints is elaborated to form the body of the framework description.},
	journal = {IEEE Std 2413-2019},
	month = mar,
	year = {2020},
	keywords = {Internet of Things, Internet of Things (IoT), Computer architecture, IEEE Standards, architectural framework, IEEE 2413},
	pages = {1--269},
}

@inproceedings{annighoefer_challenges_2019,
	title = {Challenges and {Ways} {Forward} for {Avionics} {Platforms} and their {Development} in 2019},
	doi = {10.1109/DASC43569.2019.9081794},
	abstract = {Today's air vehicles depend on digital technology. It accounts for more than 30\% of their development costs. The number of functions, the lines of code, the degree of autonomy, and the number of vehicles rise. This is why there is a need for cutting-edge technology and development methods. There is a gap between academia's methods and industrial applications due to multi-disciplinary challenges. We summarize the state-of-the-art in avionics, namely avionics platforms, requirements engineering, model-based development, automated verification, emerging technologies, and emerging demands. Experts review the most demanding challenges, research gaps, and promising solutions. They provide recommendations for the enhancement of the cooperation between industry and academia and suggest necessary research topics. This article is an introduction for those who are new to avionics. It is an up-to-date summary, for insiders looking for most promising solutions to their current problems; and it is a guide for those advancing avionics research.},
	booktitle = {2019 {IEEE}/{AIAA} 38th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
	author = {Annighoefer, Bjoern and Halle, Martin and Schweiger, Andreas and Reich, Marina and Watkins, Christopher and VanderLeest, Steven H. and Harwarth, Stefan and Deiber, Patrick},
	month = sep,
	year = {2019},
	note = {ISSN: 2155-7209},
	keywords = {Education, Security, Requirements engineering, Standards, Transportation, Reviews, Computational modeling, requirements engineering, model-based development, Adaptation models, Aerospace electronics, automated verification, avionics platforms, multi-core, Virtualization},
	pages = {1--10},
}

@inproceedings{vijayalakshmi_information_2019,
	title = {Information {Retrieval} in {Kannada} using {Ontology}},
	doi = {10.1109/CSITSS47250.2019.9031044},
	abstract = {As Internet technology has become a part of the lifestyle of the common man, research efforts are extensively made in the fields of Natural Language Processing (NLP) and Information Retrieval. Studying regional languages for developing the system to store, retrieve, extract the information from the database has gained lots of prominence nowadays. Case studies show that Ontological Information Retrieval has many advantages over keyword-based approach. In this paper we have focused on the general architecture of ontology-based Information Retrieval used for Kannada.},
	booktitle = {2019 4th {International} {Conference} on {Computational} {Systems} and {Information} {Technology} for {Sustainable} {Solution} ({CSITSS})},
	author = {Vijayalakshmi, H C and Dixit, Bhavana S},
	month = dec,
	year = {2019},
	keywords = {Ontologies, Ontology, Natural language processing, Semantics, NLP, Information retrieval, Databases, Information Retrieval, Kannada, Structured Query Language},
	pages = {1--5},
}

@inproceedings{andryushkevich_composition_2019,
	title = {Composition and {Application} of {Power} {System} {Digital} {Twins} {Based} on {Ontological} {Modeling}},
	volume = {1},
	doi = {10.1109/INDIN41052.2019.8972267},
	abstract = {The approach to create power system digital twins is presented by the example of energy supply of a geographically localized R\&D facility. In this paper, the six-layer digital twin architecture is proposed and its prototype software implementation is described. The architecture consists of an ontological model, a digital single line diagram, electronic documentation, master data, load measurement data, and mathematical models and simulations. The paper describes problems and principles of ontological modeling of the prosumer infrastructure, including customer load, low-voltage distribution network sections, small-scale generation equipment, and electric energy storage devices. The optimal configuration of the hybrid power supply system with renewable energy sources was computed using the digital twin that was composed according to the presented approach. For machine-readable representation of the digital twin, the ontological modeling language OWL is used.},
	booktitle = {2019 {IEEE} 17th {International} {Conference} on {Industrial} {Informatics} ({INDIN})},
	author = {Andryushkevich, Sergey K. and Kovalyov, Serge P. and Nefedov, Evgeny},
	month = jul,
	year = {2019},
	note = {ISSN: 2378-363X},
	keywords = {Digital twins, modeling, ontology, digital twin, Data models, Mathematical models, Computational modeling, Computer architecture, Prototypes, Power systems, distributed energy resources, generative design, Internet of Energy, Load modeling, Low voltage, Power supplies},
	pages = {1536--1542},
}

@inproceedings{daneth_automatic_2019,
	title = {Automatic {Identifying} {Interaction} {Components} in {Collaborative} {Cyber}-{Physical} {Systems}},
	doi = {10.1109/APSEC48747.2019.00035},
	abstract = {Due to diverse set of heterogeneous computing devices communicating with one another and fusing with physical components in Cyber-Physical Systems, software engineers may use different tools and/or modeling languages to formally describe or verify the system properties. As a result, the integration of these diverse constituents poses key challenges such as task for identifying interactions of components to be synthesized for a function in the systems. Although existing studies such as ontology and integration semantic languages have been used for specifying interactions of components in a Cyber-Physical System, these are still not applicable to discover the component interactions in collaborative Cyber-Physical Systems. It is due to the fact that functionalities of Cyber-Physical Systems are generally realized through interactions among multiple systems in a collaborative environment. This paper proposes a model interaction language, CyPhyML+ which can identify component interactions of realized functions in collaborative Cyber-Physical Systems. We show the proposed approach validity and applicability via an Automatic Incident Detection System.},
	booktitle = {2019 26th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Daneth, Horn and Ali, Nazakat and Hong, Jang-Eui},
	month = dec,
	year = {2019},
	note = {ISSN: 2640-0715},
	keywords = {Safety, Software, Cyber-physical systems, Collaboration, Computational modeling, Cyber-Physical Systems, Logic gates, Component Interactions, Finite element analysis, Model Interaction Language},
	pages = {197--203},
}

@inproceedings{nardi_ontology-based_2019,
	title = {An {Ontology}-{Based} {Diagnosis} of {Mainstream} {Service} {Modeling} {Languages}},
	doi = {10.1109/EDOC.2019.00023},
	abstract = {This paper presents a diagnosis of mainstream service modeling languages (SoaML, USDL, and ArchiMate) in light of UFO-S, a reference ontology for services. UFO-S is intended as a broad ontology for service phenomena, harmonizing different perspectives on services (e.g., "service as commitment", and "service as capability"), and addressing several phases of the service lifecycle (service offering, service agreement, and service delivery). As result, UFO-S is used as an "analysis theory" to identify choices in these languages concerning their focus and coverage of service phenomena. We identify a number of possible improvements concerning the representation of service participant (roles), the description of service offerings, service agreements and service delivery.},
	booktitle = {2019 {IEEE} 23rd {International} {Enterprise} {Distributed} {Object} {Computing} {Conference} ({EDOC})},
	author = {Nardi, Julio Cesar and Almeida, João Paulo A. and da Silva, Paulo Henrique A. and Guizzardi, Giancarlo},
	month = oct,
	year = {2019},
	note = {ISSN: 2325-6362},
	keywords = {Ontologies, ArchiMate, Computational modeling, Conferences, service modeling languages, service ontology, SoaML, USDL},
	pages = {112--121},
}

@inproceedings{bikmullina_instrumentation_2019,
	title = {Instrumentation and {Control} {System} for {Test} {Bench}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078014886&doi=10.1109%2FFarEastCon.2019.8934256&partnerID=40&md5=fc9156db8da97a10d47699e51a955daa},
	doi = {10.1109/FarEastCon.2019.8934256},
	abstract = {The main problem of the article is the lack of automated structural synthesis of information systems based on the semantic relations of subject area. As presented in the article, in technology creation software system expert developing the domain ontology with help user-friendly the program interfaces. This program is a mechanism for automatically formalizing domain description and automatically synthesizing the class diagram in the Unified Modeling Language. In the article discusses the method for automatic design based on automated synthesis of Unified Modeling Language models of the application program to control the test stand. The method of its use in solving the problems of designing an instrumentation and control system for test bench of an unmanned aerial vehicle is described. Expert inputs a domain ontology, then system automatically formalizes domain description and synthesizes the class diagram in the Unified Modeling Language.},
	booktitle = {2019 {International} {Multi}-{Conference} on {Industrial} {Engineering} and {Modern} {Technologies} ({FarEastCon})},
	author = {Bikmullina, I. and Barkov, I.},
	month = oct,
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Ontologies, Ontology, Semantics, ontology, Domain ontologies, domain, UML, Unified modeling language, Information systems, Unified Modeling Language, Semantic relations, Software testing, Complexity theory, Software systems, Industrial engineering, User interfaces, Control systems, an unmanned aerial vehicle, synthesis of class diagrams, Encoding (symbols), Computer simulation languages, Unmanned aerial vehicles (UAV), Domain description, Application programs, Antennas, Automated synthesis, Class diagrams, Flight dynamics, Instrumentation and control system, Structural synthesis},
	pages = {1--6},
	annote = {Cited by: 3},
}

@inproceedings{jue_semi-formal_2019,
	title = {A {Semi}-{Formal} {Requirement} {Modeling} {Pattern} for {Designing} {Industrial} {Cyber}-{Physical} {Systems}},
	volume = {1},
	doi = {10.1109/IECON.2019.8926665},
	abstract = {Requirement engineering is a crucial part of the engineering process. The traditional methods of requirement engineering are time-consuming and human-centered. A well-established software requirement description model needs to ensure the accuracy and integrity of the transformation and is also hoped to be scalable, versatile, and efficient in transformation and transmission. This paper presents a method of requirement engineering, including constricted nature language requirement input pattern, and the formalized requirement description JSON model. This method provides convenience for requirement modification and validation that can satisfy the real-time constraints of industrial cyber-physical systems.},
	booktitle = {{IECON} 2019 - 45th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	author = {Jue, Wang and Song, Yineng and Wu, Xian and Dai, Wenbin},
	month = oct,
	year = {2019},
	note = {ISSN: 2577-1647},
	keywords = {Requirements engineering, Software, Unified modeling language, requirement engineering, formal models, Real-time systems, Analytical models, Solid modeling, requirement component, system behavior decomposition},
	pages = {2883--2888},
}

@inproceedings{le_novel_2019,
	title = {A {Novel} {Approach} to {Modeling} {Enterprise} {Services} {Leveraging} {Object} {Cloning} and {Multilevel} {Classification}},
	doi = {10.1109/EDOCW.2019.00036},
	abstract = {Object-oriented modeling is concerned with capturing common properties of objects. The dominant thinking in this realm is to classify objects that share certain properties into what is called a class, which in turn enables us to instantiate additional objects. Deep modeling takes a step further by introducing the notion of clabject that might be instantiated multiple times until its instantiation potency runs out. This initiative has gained a lot of momentum of late, primarily due to the inadequacy of the classical mechanics of two-level object instantiation. There exists a less familiar way of reasoning in object-orientation that takes its root from the prototype theory. We believe that they co-exist as two sides of the same coin. Unfortunately, prototype-based modeling still stays on the sidelines in the mainstream of conceptual modeling and related areas (e.g., enterprise modeling). In this paper, we argue that the two methods actually complement each other. We propose a hybrid modeling suite that allows for both instantiation and cloning in enterprise modeling. We formally state that a clabject not only features the so-called potency (i.e., for how many levels this clabject might further be classified) but also carries the notion of characteristics (i.e., the extent to which this clabject resembles those being represented). We demonstrate our novel ways of modeling for capturing business processes in a service-oriented enterprise architecture.},
	booktitle = {2019 {IEEE} 23rd {International} {Enterprise} {Distributed} {Object} {Computing} {Workshop} ({EDOCW})},
	author = {Lê, Lam-Son and Truong, Thai-Minh and Wegmann, Alain},
	month = oct,
	year = {2019},
	note = {ISSN: 2325-6605},
	keywords = {Cognition, Process modeling, Computational modeling, Business, Computer architecture, Prototypes, Conferences, Object oriented modeling, Business Process Modeling, Cloning, Enterprise Modeling, Multilevel Classification, Prototype Theory},
	pages = {160--167},
}

@inproceedings{rui_research_2019,
	title = {Research on {Semantic} {Service} {Technology} in {Mobile} {Geographic} {Information} {System}},
	doi = {10.1109/ICSGEA.2019.00076},
	abstract = {This paper combines the conceptual construction and classification methods of geo-ontology, explores the method of formal description language to express geo-domain ontology concepts, and defines the concept of geo-ontology in the process of geo-spatial information semantic expression. Controposing to the problem of inadequate description of spatial information services by traditional description methods, Construction of spatial information application ontology with Protege tool and description service information with OWL-S description language are adopted, to realize the semantic integration between geo-information system. It constructs spatial information application ontology and describes service information with OWL-S description language. The elements of service quality are expanded to improve the comprehensiveness of service description. To verify the feasibility of the combination of theory and practical application of geo-ontology, a mobile GIS is developed from the bottom under the support of current mobile development technology, and an example of spatial semantic retrieval is tested. The superiority of geo-ontology in semantic retrieval is verified, which has important theoretical and practical significance.},
	booktitle = {2019 {International} {Conference} on {Smart} {Grid} and {Electrical} {Automation} ({ICSGEA})},
	author = {Rui, Jiang},
	month = aug,
	year = {2019},
	keywords = {Ontologies, Semantics, ontology, OWL, Protege, Tools, GIS, Topology, Smart grids, Geographic information systems, Information services, semantic service},
	pages = {304--307},
}

@inproceedings{guizzardi_ontology-based_2019,
	title = {Ontology-{Based} {Model} {Abstraction}},
	doi = {10.1109/RCIS.2019.8876971},
	abstract = {In recent years, there has been a growth in the use of reference conceptual models to capture information about complex and critical domains. However, as the complexity of domain increases, so does the size and complexity of the models that represent them. Over the years, different techniques for complexity management in large conceptual models have been developed. In particular, several authors have proposed different techniques for model abstraction. In this paper, we leverage on the ontologically well-founded semantics of the modeling language OntoUML to propose a novel approach for model abstraction in conceptual models. We provide a precise definition for a set of Graph-Rewriting rules that can automatically produce much-reduced versions of OntoUML models that concentrate the models' information content around the ontologically essential types in that domain, i.e., the so-called Kinds. The approach has been implemented using a model-based editor and tested over a repository of OntoUML models.},
	booktitle = {2019 13th {International} {Conference} on {Research} {Challenges} in {Information} {Science} ({RCIS})},
	author = {Guizzardi, Giancarlo and Figueiredo, Guylerme and Hedblom, Maria M. and Poels, Geert},
	month = may,
	year = {2019},
	note = {ISSN: 2151-1357},
	keywords = {Ontologies, Semantics, Unified modeling language, Context modeling, Complexity theory, Object oriented modeling, Clustering methods, Complexity Management in Conceptual Modeling, Model Abstraction, Ontology-Based Conceptual Modeling},
	pages = {1--13},
}

@inproceedings{patzer_industrie_2019,
	title = {The {Industrie} 4.0 {Asset} {Administration} {Shell} as {Information} {Source} for {Security} {Analysis}},
	doi = {10.1109/ETFA.2019.8869059},
	abstract = {One of the essential concepts of the Reference Architecture Model Industrie 4.0 (RAMI4.0) is the uniform modelling of assets by means of a common meta-data model called the Asset Administration Shell (AAS). However, important practical experience with this concept is still missing, as not many use cases for the AAS have yet been implemented. Thus, practical issues within the AAS concept and respective solutions are hard to identify. In this paper, presents our experience with the implementation of an AAS use case. The AAS is used as information source to create an ontology, which is then used for security analysis. The paper discusses the use-case-specific modelling language selection and provides a practical examination of several of our implementations that use OWL and OPC UA together. Furthermore, it provides recommendations for the implementation of Asset Administration Shells for this and similar use cases.},
	booktitle = {2019 24th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Patzer, Florian and Volz, Friedrich and Usländer, Thomas and Blöcher, Immanuel and Beyerer, Jürgen},
	month = sep,
	year = {2019},
	note = {ISSN: 1946-0759},
	keywords = {Ontology, Semantic Web, Asset Administration Shell, Security, Cognition, Tools, Data models, XML, OPC UA, Security Ontology, Protocols, Analytical models, Industrial Systems},
	pages = {420--427},
}

@inproceedings{vijaya_candidate_2019,
	title = {Candidate {Generation} for {Instance} {Matching} on {Semantic} {Web}},
	doi = {10.1109/ICCIDS.2019.8862131},
	abstract = {The growth of semantic web has given rise to proliferation of data sources wherein the task of recognizing real world entities and identifying multiple references of the same real world entity becomes an essential task in order to facilitate sharing and integration of data. Due to the heterogeneous nature of data on the semantic web, entities belonging to different sources are compared by assessing the similarity of features that are common in order to identify matches. With the increasing size of data sets Candidate generation methods are generally employed to avoid quadratic time complexity that would otherwise be incurred if pairwise similarity of all entities are computed. Here we propose a novel index based approach for candidate generation and reduction. The evaluation shows that the proposed method scales well and improves recall significantly.},
	booktitle = {2019 {International} {Conference} on {Computational} {Intelligence} in {Data} {Science} ({ICCIDS})},
	author = {Vijaya, B. and Gharpure, Prachi},
	month = feb,
	year = {2019},
	keywords = {Semantic web, Semantic Web, Data science, Mathematical model, Computational intelligence, Instance matching, Record Linkage, Couplings, Indexes, Task analysis, Blocking, Candidate generation},
	pages = {1--5},
}

@inproceedings{paczona_model-driven_2019,
	title = {Model-{Driven} {Mechatronic} {System} {Development}},
	doi = {10.1109/COASE.2019.8843314},
	abstract = {This paper presents an approach for model-driven mechatronic system development. The approach starts with the definition of a suitable domain-specific modeling language and its semantic foundation in a domain ontology. Models created in this language are used to generate application-specific artefacts. We illustrate our approach with the example of the development of Electric Vehicle Testbeds (EVTs), i.e. systems for testing high-voltage electric vehicle components. Companies in the electric vehicle industry (automobile, aircraft and rail vehicle manufacturers) mainly use such systems. Like many other mechatronic systems, EVTs are typically tailor-made solutions. Our approach automates manual development steps and can thus contribute to quality improvement, development time reduction and finally cost reduction.},
	booktitle = {2019 {IEEE} 15th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Paczona, Martin and Mayr, Heinrich C.},
	month = aug,
	year = {2019},
	note = {ISSN: 2161-8089},
	keywords = {Ontologies, Software, Tools, Unified modeling language, Integrated circuit modeling, Electric vehicles},
	pages = {1730--1736},
}

@inproceedings{lee_towards_2019,
	title = {Towards {Automating} {Design} and {Development} of {Inference} {Enterprise} {Models}},
	doi = {10.1109/SYSCON.2019.8836882},
	abstract = {In this paper, a process improvement approach, which provides automated assistance in designing and developing Inference Enterprise Models (IEM), is described. An IEM forecasts and evaluates the performance of the modeled enterprise and can be used to improve the operational quality of an enterprise making mission-focused inferences. As part of developing the IEM methodology, a team of researchers built several Inference Enterprise Models and acquired a breadth of knowledge in the domain of modeling insider threat detection systems. Although there are many different ways this type of inference enterprise can be modeled, there are common elements that are reused from case to case. We propose a solution for formalizing and reusing this IEM domain knowledge by developing an IEM Process Ontology and Workflow Generator.},
	booktitle = {2019 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Lee, James D. and Matsumoto, Shou and Zaidi, Abbas K. and Laskey, Kathryn B.},
	month = apr,
	year = {2019},
	note = {ISSN: 2472-9647},
	keywords = {Ontologies, Ontology, Modeling, Unified modeling language, Statistics, Predictive models, Task analysis, Sociology, Business Process Improvement, Inference Enterprise Modeling, Knowledge Reuse, Template},
	pages = {1--6},
}

@inproceedings{oba_extraction_2019,
	title = {Extraction of {Taxonomic} {Relation} of {Complex} {Terms} by {Recurrent} {Neural} {Network}},
	doi = {10.1109/ICCC.2019.00024},
	abstract = {In recent years, while the Internet has brought various technological evolutions, a lot of ontology is required to organize and systemize knowledge, and its generation is necessary. Especially, classification of hypernym-hyponym relation which describes taxonomy of ontology has received a lot of attention. As a method to automate the generation, word embedding based method was proposed recently. Although the method enabled high accuracy classification by using semantics, it does not correspond to complex term consisting of multiple words. Based on this background, in this paper, we proposed a new model combined word embedding and Recurrent Neural Network(RNN), evaluated the classification performance with data extracted from WordNet. For the result, it is indicated that the RNN approach is more effective and general for ontology generation.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Cognitive} {Computing} ({ICCC})},
	author = {Oba, Atsushi and Paik, Incheon},
	month = jul,
	year = {2019},
	keywords = {Ontologies, Semantics, Natural Language Processing, Taxonomy, Data models, Training, Recurrent neural networks, Word Embedding, Recurrent Neural Network, Support vector machines, Ontological Classification, Recurrent Neural Network Language Model, Word2Vector},
	pages = {70--72},
}

@inproceedings{kreines_artificial_2019,
	title = {Artificial {Intelligence} {Tools} for {Business} {Applications}: {Objective} {Map} of {Science} and {Analysis} of {Texts}},
	volume = {01},
	doi = {10.1109/CBI.2019.00058},
	abstract = {Business is looking for technological and investment possibilities in research and development (R\&D). Here the basic problems are to find R\&D's results and/or teams for solving the professional tasks and for making investment. But business has no personal view on scientific problems. So business is seeking the objective tools for forecasting and evaluation of R\&D prospects and results. Experts have own interests and require a lot of funding. R\&D reflections are the texts. The modern methods of computer analysis of texts can do a lot of the experts' work for making it more objective and cheaper. The tools for search, systematization and ranking R\&D's results and teams are computer analysis of texts and the map of science. The map of science is the distribution of the collection of texts of a scientific nature by the topics. The map of science is a way to navigate through the world of scientific publications and R\&D's teams, a tool for identifying trends and assessing R\&D directions. The usual ways for the map of science formation use bibliometric/scientometric data, general probability models of the texts, expert's opinion or artificial intelligence (AI) models and methods based on the thesaurus or on the ontology of the subject domains. The interests of business are not in line with the orientation on a priori established ideas about possible topics or there number for rapidly changing scientific fields. Precisely these fields are of the greatest interest to business. On the basis of mathematical modeling of texts and large-scale text collections, an approach is proposed for the computational formation of the adaptive dynamic map of science that does not use a priori classification schemes and data of the scientific publications' citation. Topics (thematic groups), their number and the distribution of texts over the topics are determined computationally without experts' involvement. Examples of the maps of science for various collections of scientific publications are given. The original method is proposed for checking the adequacy of the text models and the map of science. The method uses the categorization of articles and their abstracts as the separate objects on the basis of computationally generated map (its topics). The results of the large-scale experiment confirmed the high efficiency of the proposed mathematical modeling of texts and text collections. The possibilities of practical use of the map of science for business applications are considered.},
	booktitle = {2019 {IEEE} 21st {Conference} on {Business} {Informatics} ({CBI})},
	author = {Kreines, Mikhail G. and Kreines, Elena M.},
	month = jul,
	year = {2019},
	note = {ISSN: 2378-1971},
	keywords = {Semantics, modeling, information retrieval, semantics, Tools, Computational modeling, text, semiotics, Research and development, Business, interpretation, Adaptation models, Analytical models, analytical support, collection, research and development},
	pages = {445--451},
}

@inproceedings{bucko_ontology_2019,
	title = {Ontology as a {Modeling} {Tool} within {Model} {Driven} {Architecture} {Abstraction}},
	doi = {10.23919/MIPRO.2019.8756968},
	abstract = {This paper is focused on automatic transformation process of top levels of Model Driven Architecture (MDA) within the information system development phase. System architects are always trying to find easier, complex and more united way of information system development. Although the Model Driven Architecture (MDA) provides a set of guidelines for the structuring of specifications it also comes with challenging tasks of transformations between the various levels of abstraction. The primary objective of this work is to design a universal automated approach within the Computer Independent Model (CIM) and Platform Independent Model (PIM) manual transformation. The manual process of the transformations within MDA could be automated using ontology model with the combination of mapping rules and Extensible Markup Process Definition Language (XPDL) and Extensible Markup Language Metadata interchange (XMI) conversion.},
	booktitle = {2019 42nd {International} {Convention} on {Information} and {Communication} {Technology}, {Electronics} and {Microelectronics} ({MIPRO})},
	author = {Bučko, B. and Zábovská, K. and Zábovský, M.},
	month = may,
	year = {2019},
	note = {ISSN: 2623-8764},
	keywords = {Ontologies, Ontology, Unified modeling language, Information systems, Data models, Model driven architecture, Computational modeling, Business, Computer architecture, Computer independent model, Information system development, Platform independent model},
	pages = {1525--1530},
}

@article{zafar_novel_2019,
	title = {A {Novel} {Framework} to {Automatically} {Generate} {Executable} {Web} {Services} {From} {BPMN} {Models}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2927785},
	abstract = {Enterprise resource planning (ERP) is a business process management system in which integrated applications are used to manage business processes in a shared data environment. ERP systems usually deal with the two types of business processes, i.e., exchange and conversion. In the exchange process, economic resource, such as product, exchanges to another economic resource, such as the sales process. In a conversion process, an enterprise consumes resources in order to produce new resources, such as the distribution process. Generally, the communication between ERP applications, based on the conversion and exchange processes, is accomplished through Web services. In this context, the implementation of Web services in ERP systems is a complex task. To manage this, the business process model and notation (BPMN) are frequently utilized to simplify the development of ERP applications. However, state-of-the-art BPMN approaches usually deal with the modeling of exchange processes without considering the conversion process. Furthermore, the model transformation solution to automatically generate Web services from the BPMN models are hard to find in the literature. Therefore, in this paper, a novel framework is proposed that supports the modeling of both exchanges as well as conversion processes through BPMN. Particularly, a modeling approach is introduced to represent the ERP processes through BPMN concepts. Subsequently, the rules are developed to convert source BPMN models into target Service-oriented architecture Modeling Language (SoaML) models. Finally, transformation rules are developed to generate fully functional executable Java Web services from SoaML models. As a part of the research, a complete open-source BPMN to Web services transformation (B2W) tool is developed to automatically generate the Web services from the high-level BPMN models. The proposed framework is validated through multiple case studies. The experimental results prove that the proposed framework accurately generates Web services from the BPMN models, which eventually helps in developing the ERP systems with simplicity.},
	journal = {IEEE Access},
	author = {Zafar, Iqra and Azam, Farooque and Anwar, Muhammad Waseem and Maqbool, Bilal and Butt, Wasi Haider and Nazir, Aiman},
	year = {2019},
	keywords = {Ontologies, Unified modeling language, Service-oriented architecture, Business, Biological system modeling, business process model and notation (BPMN), Economics, ERP systems, Model-driven engineering (MDE), service generation, service oriented architecture (SOA)},
	pages = {93653--93677},
}

@inproceedings{qie_deep_2018,
	title = {A {Deep} {Learning} {Based} {Framework} for {Textual} {Requirement} {Analysis} and {Model} {Generation}},
	doi = {10.1109/GNCC42960.2018.9018722},
	abstract = {Requirement analysis is a key part of systems engineering process. Analyzing requirements correctly and creating design model sequentially could be critical to the whole process of a product development. Nevertheless, requirement text handling and model transferring could be really time-consuming and error-prone. Thus, we proposed an artificial intelligence based framework to deal with textual requirement handling and model creation. With deep learning and natural language process skills, our approach could be able to analyze textual requirements automatically, and then create the related models. This would indeed alleviate the work of engineers and promote the efficiency and quality of product development process. With our limited knowledge, our paper is the first one to propose the deep learning and NLP based framework to automatically create requirement models.},
	booktitle = {2018 {IEEE} {CSAA} {Guidance}, {Navigation} and {Control} {Conference} ({CGNCC})},
	author = {Qie, Yongiun and Zhu, Weijie and Liu, Aishan and Zhang, Yuchen and Wang, Jun and Li, Teng and Li, Yaqing and Ge, Yufei and Wang, Yufeng},
	month = aug,
	year = {2018},
	keywords = {Natural language processing, Semantics, Machine learning, NLP, deep learning, Computational modeling, Air traffic control, Analytical models, requirement modeling},
	pages = {1--6},
}

@inproceedings{litvak_improving_2018,
	title = {Improving the {Identification} of {Conflicts} in {Collaborative} {Requirements} {Engineering}},
	doi = {10.1109/CSCI46756.2018.00173},
	abstract = {Requirements engineering has the aim of describing as accurately as possible the needs and expectations of all the stakeholders involved in the software development. The collaborative work of the stakeholders in this process allows them to improve the quality of the requirements. Nevertheless, collaborative work involves the raising of conflicts, and they must be solved in order to achieve the desired quality. This paper presents the evolution in our understanding of the process to identify and solve conflicts during the collaborative construction of the Language Extended Lexicon that captures the domain language. This process was validated with three different case studies and the usability SUS questionnaire.},
	booktitle = {2018 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Litvak, Claudia and Antonelli, Leandro and Rossi, Gustavo and Gigante, Nora},
	month = dec,
	year = {2018},
	keywords = {Ontologies, Requirements engineering, Software, Stakeholders, Collaboration, collaboration, requirements engineering, Companies, Collaborative work, conflicts, natural language models},
	pages = {872--877},
}

@inproceedings{landolfi_ontology_2018,
	title = {An {Ontology} {Based} {Semantic} {Data} {Model} {Supporting} {A} {Maas} {Digital} {Platform}},
	doi = {10.1109/IS.2018.8710519},
	abstract = {The integration of IoT infrastructures across production systems, together with the extensive digitalisation of industrial processes, are drastically impacting manufacturing value chains and the business models built on the top of them. By exploiting these capabilities companies are evolving the nature of their businesses shifting value proposition towards models relying on product servitization and share, instead of ownership. In this paper, we describe the semantic data-model developed to support a digital platform fostering the reintroduction in the loop and optimization of unused industrial capacity. Such data-model aims to establish the main propositions of the semantic representation that constitutes the essential nature of the ecosystem to depict their interactions, the flow of resources and exchange of production services. The inference reasoning on the semantic representation of the ecosystem allows to make emerge nontrivial and previously unknown opportunities. This will apply not only to the matching of demand and supply of manufacturing services, but to possible and unpredictable relations. For instance, a particular kind of waste being produced at an ecosystem node can be linked to the requirements for an input material needed in a new product being developed on the platform, or new technologies can be suggested to enhance processes under improvement. The overall architecture and individual ontologies are presented and their usefulness is motivated via the application to use cases.},
	booktitle = {2018 {International} {Conference} on {Intelligent} {Systems} ({IS})},
	author = {Landolfi, Giuseppe and Bami, Andrea and Izzo, Gabriele and Montini, Elias and Bettoni, Andrea and Vujasinovic, Marko and Gugliotta, Alessio and Soares, Antόnio Lucas and Diogo Silva, Henrique},
	month = sep,
	year = {2018},
	note = {ISSN: 1541-1672},
	keywords = {Reliability, Stakeholders, Technological innovation, knowledge discovery, Production, Ions, Manganese, manufacturing ontologies, semantic data-model, servitization, US Department of Transportation},
	pages = {896--904},
}

@inproceedings{rajasekar_e_2018,
	title = {"{E} {MARUTHUVACHI}" – {INFORMATION} {EXTRACTION} {FRAMEWORK} {FOR} {DATA} {ABOUT} {OBSTETRICS} {AND} {GYNECOLOGY} {IN} {TAMIL}},
	doi = {10.1109/I-SMAC.2018.8653655},
	abstract = {Technology is transforming the world from traditional into Artificial Intelligence. Human beings are adopting themselves into the change using Technology. India is famous for the name of unique traditional culture. The traditional culture protected people to do useful things. Especially for women, they were protected by the traditional culture to gain knowledge about maternity and gynecology. The target framework to extract the useful information from the raw documents. The extraction process extracts the NLP elements from the raw documents. The framework is developed using modified model of neural network language model (NNLM). The proposed model is evaluated with F-Test. The evaluation produces the good result for accuracy.},
	booktitle = {2018 2nd {International} {Conference} on {I}-{SMAC} ({IoT} in {Social}, {Mobile}, {Analytics} and {Cloud}) ({I}-{SMAC}){I}-{SMAC} ({IoT} in {Social}, {Mobile}, {Analytics} and {Cloud}) ({I}-{SMAC}), 2018 2nd {International} {Conference} on},
	author = {Rajasekar, M. and Udhayakumar, A.},
	month = aug,
	year = {2018},
	keywords = {Neural networks, Artificial Intelligence, Taxonomy, Data mining, Neural Networks, Information Extraction, Computational modeling, Task analysis, Gynecology, Neural Network Language Model, Obstetrics and Gynecology},
	pages = {399--407},
}

@inproceedings{blas_conceptual_2018,
	title = {A {CONCEPTUAL} {FRAMEWORK} {TO} {CLASSIFY} {THE} {EXTENSIONS} {OF} {DEVS} {FORMALISM} {AS} {VARIANTS} {AND} {SUBCLASSES}},
	doi = {10.1109/WSC.2018.8632265},
	abstract = {The Discrete Event System Specification (DEVS) is a general modeling formalism with sound semantics founded on a system theoretic basis. It can be used as a base for the development of specialized modeling formalisms. Usually, the extensions of DEVS expand the classes of systems models that can be represented in DEVS. However, with a growing number in new variants of DEVS and an increasing number of problems to be solved using discrete simulation techniques, it is necessary to define the relations among different approaches. This paper presents a conceptual modeling perspective applied to DEVS extensions that structure a framework over the traditional modeling and simulation approach. The framework provides a multilevel structure to analyze the features required for each extension type. Two main types of extensions are identified: variants and subclasses. In order to illustrate the proposed guidelines, the Routed DEVS formalism is presented as example of the subclass type.},
	booktitle = {2018 {Winter} {Simulation} {Conference} ({WSC})},
	author = {Blas, María J. and Gonnet, Silvio M. and Leone, Horacio P. and Zeigler, Bernard P.},
	month = dec,
	year = {2018},
	note = {ISSN: 1558-4305},
	keywords = {Ontologies, Semantics, Context modeling, Computational modeling, Analytical models, Guidelines, Discrete-event systems},
	pages = {560--571},
}

@inproceedings{kharlamov_towards_2018,
	title = {Towards {Semantically} {Enhanced} {Digital} {Twins}},
	doi = {10.1109/BigData.2018.8622503},
	abstract = {Digital twins (DTs) are a powerful mechanism for representing complex industrial assets such as oil platforms as digital models. These models can facilitate temporal analyses and computer simulations of assets. In order to enable this, DTs should be able to capture characteristics of an asset as specified by the manufacturer, its state during the run time, as well as how the asset interacts with other assets in a complex system. We argue that semantic technologies and in particular semantic models or ontologies is promising modelling paradigm for DTs. Semantic models allow to capture complex systems in an intuitive fashion, can be written in standardised ontology languages, and come with a wide range of off-the-shelf systems to design, maintain, query, and navigate semantic models. In this work we report our preliminary results on developing a system that would support semantic-based DTs. In particular, we plan to augment the PI System developed by OSIsoft with ontologies and show how the resulting solution can help in simplifying analytical and machine learning routines for DTs.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Kharlamov, Evgeny and Martin-Recuerda, Francisco and Perry, Brandon and Cameron, David and Fjellheim, Roar and Waaler, Arild},
	month = dec,
	year = {2018},
	keywords = {Ontologies, Semantics, Context modeling, Data models, Turbines, Computational modeling, Analytical models},
	pages = {4189--4193},
}

@inproceedings{bouzidi_alignment_2018,
	title = {Alignment of {Business} {Processes} and {Requirements} {Through} {Model} {Integration}},
	doi = {10.1109/AICCSA.2018.8612870},
	abstract = {Business process and requirement specification are crucial phases in the software development process. Yet, business and requirement modeling are often carried out separately by different languages and design teams, leading to misaligned models. The degree of misalignment grows continuously with their independent evolution. Thus, the potential of model-driven software development cannot be fully exploited. There is a considerable agreement among researchers about the integration technique roles to bridge the gap between heterogeneous models. Therefore, this approach is based on this technique to align the business world represented by BPMN and the software requirement world represented by the UML use case. We define an integrated meta-model that incorporates all BPMN and use case elements as well as new others to map traceable elements. Further, we define a new diagram that provides a means to visualize and combines their use within an integrated model. Our approach supplements CASE tools with additional information and relationships to maintain the global system consistency. To illustrate it, we implement an editor for designing the proposed diagram, and we apply it in a topical case study.},
	booktitle = {2018 {IEEE}/{ACS} 15th {International} {Conference} on {Computer} {Systems} and {Applications} ({AICCSA})},
	author = {Bouzidi, Aljia and Haddar, Nahla and Ben Abdallah, Mounira and Haddar, Kais},
	month = oct,
	year = {2018},
	note = {ISSN: 2161-5330},
	keywords = {Ontologies, Semantics, Software, Unified modeling language, BPMN, Alignment, Computational modeling, Bridges, Business, business process models, meta-model integration, requirements models, UML use case},
	pages = {1--8},
}

@inproceedings{lohar_control_2018,
	title = {Control and {Management} of {Optical} {Networks} {Using} {Optical} {Network} {Description} {Language}},
	doi = {10.1109/NCC.2018.8600201},
	abstract = {Management and configuration of optical networks, implementing new policies to keep up with ever-changing network etc. have always been tedious tasks. Software-defined networking (SDN) has provided many network solutions in the electrical counterpart. SDN for optical networks can provide new opportunities to make the above mentioned tasks easier and faster. As a first step towards this goal, we develop an optical network description language (ONDL). We use it to describe various network components, and their configuration and run-time states, such as modulation schemes, wavelength and spectral-width of a transponder, switching matrix of an optical switch etc. The language is based on resource description framework (RDF). Furthermore, we develop a controller which understands and sends instructions in this language to different network devices to provide/change their states. We show the applicability of ONDL by simulating a network, controlling and managing its nodes using ONDL and developed controller.},
	booktitle = {2018 {Twenty} {Fourth} {National} {Conference} on {Communications} ({NCC})},
	author = {Lohar, Nitin K. and Kar, Subrat},
	month = feb,
	year = {2018},
	keywords = {Resource description framework, Modulation, Optical fiber networks, Optical switches},
	pages = {1--5},
}

@inproceedings{tueno_fotso_back_2018,
	title = {Back {Propagating} {B} {System} {Updates} on {SysML}/{KAOS} {Domain} {Models}},
	doi = {10.1109/ICECCS2018.2018.00025},
	abstract = {Nowadays, the usefulness of the formal verification and validation of system specifications is well established, at least for critical systems. However, one of the main obstacles to their adoption lies in obtaining the formal specification of the system, and, in the case of refinement-based formal methods such as B System or Event-B, in obtaining the most abstract specification that heads the development of the system. The SysML/KAOS requirements engineering method is proposed to overcome this difficulty. It includes a goal modeling language to model requirements from stakeholders needs. Translation rules from a goal model to a B System specification have already been defined. They allow to obtain a skeleton of the system specification. To complete it, a language has been defined to express the domain model associated to the goal model. Its translation gives the structural part of the B System specification. However, it very often appears that new elements must be added in the B System specification obtained from SysML/KAOS models, discovered for instance when specifying the body of events and/or by using formal validation and/or verification tools. We have therefore defined a set of rules allowing the back propagation, within domain models, of every newly added element. This paper describes these rules and how they are specified in Event-B. Their consistency is proved using the Rodin tool. We show that they are structure preserving: two related elements within the B System specification remain related within the domain model. This is done by proving various isomorphisms between the B System specification and the domain models.},
	booktitle = {2018 23rd {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems} ({ICECCS})},
	author = {Tueno Fotso, Steve Jeffrey and Frappier, Marc and Laleau, Régine and Mammar, Amel},
	month = dec,
	year = {2018},
	keywords = {Requirements engineering, Requirements Engineering, Event-B, Sensor phenomena and characterization, Actuators, B System, Backpropagation, Boilers, Domain Modeling, Optimized production technology, SysML/KAOS},
	pages = {160--169},
}

@inproceedings{singh_formal_2018,
	title = {Formal {Ontology} {Driven} {Model} {Refactoring}},
	doi = {10.1109/ICECCS2018.2018.00022},
	abstract = {Refactoring, successfully used in the field of programming, can be used in maintenance and restructuring of the large and complex models. In this paper, we present a novel approach for model refactoring and a set of modelling patterns that are applicable for refinement-based formal development. In order to carry out this study, we investigate the previously developed large and complex model and required ontology to develop a domain model and a refactored system model. Further, we use the Rodin tools to check the internal consistency with respect to the desired functional behaviour and the required safety properties. Our main contributions are: to develop a refactoring technique related to the correct by construction approach; to use the domain specific knowledge in a system model explicitly; to define a set of modelling patterns; and to define a restructuring mechanism in the formal development. Finally, this proposed approach is evaluated through a complex medical case study: ECG clinical assessment protocol.},
	booktitle = {2018 23rd {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems} ({ICECCS})},
	author = {Singh, Neeraj Kumar and Ait-Ameur, Yamine and Mery, Dominique},
	month = dec,
	year = {2018},
	keywords = {Ontologies, Semantics, ontologies, Safety, Tools, Event-B, Computational modeling, Analytical models, Electrocardiography, do main theories, Refactoring, refinement and proofs},
	pages = {136--145},
}

@inproceedings{papazoglou_metaprogramming_2018,
	title = {Metaprogramming {Environment} for {Industry} 4.0},
	doi = {10.1109/ES.2018.00008},
	abstract = {Industry 4.0 is blurring the lines between the physical, and digital spheres of global production systems. Industry 4.0 sets the foundations for a completely connected factories that are characterized by the digitization and interconnection of supply chains, production equipment and production lines, and the application of the latest advanced digital information technologies to manufacturing activities. To fully realize the promise of Industry 4.0, disparate manufacturing systems, devices, data and processes need to connect, communicate, and interoperate. This paper has dual purpose. It first introduces a model-based engineering that enables a concurrent, collaborative design process where users examine and define requirements, propose solution architectures, demonstrate and exchange ideas with stakeholders, and consider product feature tradeoffs. Subsequently, it proposes a novel programming paradigm and a flexible environment that helps developers develop design-to-production industrial automation solutions by employing structured higher-level modular software techniques.},
	booktitle = {2018 {Sixth} {International} {Conference} on {Enterprise} {Systems} ({ES})},
	author = {Papazoglou, Michael P.},
	month = oct,
	year = {2018},
	note = {ISSN: 2572-6609},
	keywords = {Digital twins, Manufacturing, Industry 4.0, Model-based development, Real-time systems, Industries, Supply chains, Solid modeling, Three-dimensional displays, Enterprise Knowledge Engineering, Information integration and interoperability, Meta-programming},
	pages = {1--8},
}

@inproceedings{kurjakovic_enterprise_2018,
	title = {Enterprise {Architecture} {Driven} and {User}-{Friendly} {SaaS} {Service} {Selection}},
	doi = {10.1109/ES.2018.00037},
	abstract = {In the last decade the number of cloud services has grown significantly, and its nature challenges the traditional process of requirements elicitation for software solutions. Most of the conducted cloud research address non-functional requirements, such as payment strategies, performance or security. Methods are required which provide an appropriate level of abstraction for describing the functional aspects of cloud services. We suggest an approach which exploits the potential of enterprise architecture modeling for cloud service selection. This study contributes to cloud service selection in two ways. First, we suggest a concept for the description of functional requirements for vertical SaaS services. Second, we provide a method that enables various kinds of stakeholders to describe their requirements considering their individual knowledge level.},
	booktitle = {2018 {Sixth} {International} {Conference} on {Enterprise} {Systems} ({ES})},
	author = {Kurjakovic, Sabrina and Hinkelmann, Knut},
	month = oct,
	year = {2018},
	note = {ISSN: 2572-6609},
	keywords = {Ontologies, Stakeholders, Software as a service, enterprise architecture, Computer architecture, Biological system modeling, business IT alignment, enterprise ontologies, SaaS service selection},
	pages = {196--203},
}

@inproceedings{wan_integrated_2018,
	title = {An {Integrated} {Design} {Method} {For} {Cyber}-{Physical} {Production} {Systems}},
	doi = {10.1109/IAEAC.2018.8577518},
	abstract = {The industrial cyber-physical production system contains a variety of heterogeneous devices and models, which increase the complexity of system design, particularly in software programming. Although the proposed model-driven engineering (MDE) is applied to industrial automation for this issue, the inherent complex dependencies and constraints between models and the availability of suitable tools for code generation limit the application of the model-driven approach in practice, especially as the system becomes more enormous. The component-based design (CBD) approach, which is a bottom-up approach, in contrast to the MDE that is top-down, is proposed to manage the complexity of software aspect based on the idea that building a system from existing components instead of the scratch. However, CBD can't provide the whole system model, so other ways should be adopted to design the system structure in advance. This paper presents an integrated design approach based on the combination of CBD and MDE to realize auto-design for cyber-physical production systems.},
	booktitle = {2018 {IEEE} 3rd {Advanced} {Information} {Technology}, {Electronic} and {Automation} {Control} {Conference} ({IAEAC})},
	author = {Wan, Guangxi and Wang, Peng and Xue, Lingling and Zeng, Peng},
	month = oct,
	year = {2018},
	note = {ISSN: 2381-0947},
	keywords = {Ontologies, Ontology, Automation, Software, Semantic technology, Model-based design, Unified modeling language, Integrated design, Industrial automation, Testing, Control systems, Component-based Design, IEC 61499},
	pages = {791--796},
}

@inproceedings{helmke_ontology_2018,
	title = {Ontology for {Transcription} of {ATC} {Speech} {Commands} of {SESAR} 2020 {Solution} {PJ}.16-04},
	doi = {10.1109/DASC.2018.8569238},
	abstract = {Nowadays Automatic Speech Recognition (ASR) applications are increasingly successful in the air traffic (ATC) domain. Paramount to achieving this is collecting enough data for speech recognition model training. Thousands of hours of ATC communication are recorded every day. However, the transcription of these data sets is resource intense, i.e. writing down the sequence of spoken words, and more importantly, interpreting the relevant semantics. Many different approaches including CPDLC (Controller Pilot Data Link Communications) currently exist in the ATC community for command transcription, a fact that e.g. complicates exchange of transcriptions. The partners of the SESAR funded solution PJ.16-04 are currently developing on a common ontology for transcription of controller-pilot communications, which will harmonize integration of ASR into controller working positions. The resulting ontology is presented in this paper.},
	booktitle = {2018 {IEEE}/{AIAA} 37th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
	author = {Helmke, Hartmut and Slotty, Michael and Poiger, Michael and Herrer, Damián Ferrer and Ohneiser, Oliver and Vink, Nathan and Cerna, Aneta and Hartikainen, Petri and Josefsson, Billy and Langr, David and Lasheras, Raquel García and Marin, Gabriela and Mevatne, Odd Georg and Moos, Sylvain and Nilsson, Mats N. and Pérez, Mario Boyero},
	month = sep,
	year = {2018},
	note = {ISSN: 2155-7209},
	keywords = {Ontologies, Ontology, Standards, Training, Speech recognition, Transcription, Aircraft, Automatic Speech Recognition (ASR), Controller Command, CWP HMI, PJ.16-04, Radar, SESAR, Strips},
	pages = {1--10},
}

@inproceedings{albab_resource-based_2018,
	title = {Resource-{Based} and {Value}-{Based} {Extension} for {Archimate}},
	doi = {10.1109/ICTSS.2018.8549977},
	abstract = {The concept of resource and value in the archimate that is too general allows for modeling and communication problems. Therefore, an ontological analysis is needed to evaluate and redesign the concept of resource and value on the archimate. In this paper, Unified foundational ontology (UFO) is used in conducting an ontological analysis on the concept of resource and value. An ontological analysis is done by considering the mapping between the concept on Archimate and ontology concept on UFO. Semantic problems in resource and value concepts are found when the ontological analysis is performed. The result of this study is Archimate extension with the improvement of resource and value concept. The resulting extension was successfully applied in case studies and was able to handle semantic problems that were identified.},
	booktitle = {2018 {International} {Conference} on {ICT} for {Smart} {Society} ({ICISS})},
	author = {Albab, Muhammad Ulil and Arman, Arry Akhmad},
	month = oct,
	year = {2018},
	keywords = {Ontologies, Semantics, Informatics, resource, Business, Information technology, Analytical models, Archimate, Electrical engineering, enterprise architecture modeling, ontology-based semantics, value},
	pages = {1--6},
}

@article{gomez_multi-domain_2018,
	title = {Multi-{Domain} {Semantic} {Information} and {Physical} {Behavior} {Modeling} of {Power} {Systems} and {Gas} {Turbines} {Expanding} the {Common} {Information} {Model}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2882311},
	abstract = {Due to the rapid increase of intermittent energy resources (IERs), there is a need to have dispatchable production available to ensure secure operation and increase opportunity for energy system flexibility. Gas turbine-based power plants offer flexible operation that is being improved with new technology advancements. Those plants provide, in general, quick start together with significant ramping capability, which can be exploited to balance IERs. Consequently, to understand the potential source of flexibility, better models for gas turbines are required for power system studies and analysis. In this paper, both the required semantic information and physical behavior models of such multi-domain systems are considered. First, UML class diagrams and RDF schemas based on the common information model standards are used to describe the semantic information of the electrical power grid. An extension that exploits the ISO 15926 standard is proposed herein to derive the multi-domain semantics required by integrated electrical power grid with detailed gas turbine dynamic models. Second, the Modelica language is employed to create the equation-based models, which represent the behavior of a multi-domain physical system. A comparative simulation analysis between the power system domain model and the multi-domain model has been performed. Some differences between the turbine dynamics representation of the commonly used GGOV1 standard model and a more detailed gas turbine model are shown.},
	journal = {IEEE Access},
	author = {Gómez, Francisco J. and Aguilera Chaves, Miguel and Vanfretti, Luigi and Olsen, Svein Harald},
	year = {2018},
	keywords = {Semantics, CIM, cyber-physical systems, Standards, Unified modeling language, Turbines, Mathematical model, dynamic simulation, Modelica, Common Information Model (electricity), equation-based modeling, IEC 61970, information modeling, ISO 15926, Power system dynamics, power systems modeling, power systems simulation},
	pages = {72663--72674},
}

@article{zhou_3-d_2018,
	title = {A 3-{D} {Security} {Modeling} {Platform} for {Social} {IoT} {Environments}},
	volume = {5},
	issn = {2329-924X},
	doi = {10.1109/TCSS.2018.2878921},
	abstract = {Social Internet-of-Things (SIoT) environment comprises not only smart devices but also the humans who interact with these IoT devices. The benefits of such system are overshadowed due to the cyber security issues. A novel approach is required to understand the security implication under such a dynamic environment while taking both the social and technical aspects into consideration. This paper addressed such challenges and proposed a 3-D security modeling platform that can capture and model the security requirements in the SIoT environment. The modeling process is graphical notation based and works as a security extension to the Business Process Model and Notation. Still, it utilizes the latest 3-D game technology; thus, the security extensions are generated through the third dimension. Consequently, the introduction of security extensions will not increase the complexity of the original SIoT scenario, while keeping all the key information on the same platform. Together with the proposed security ontology, these comprehensive security notations created a unique platform that aims at addressing the ever complicated security issues in the SIoT environment.},
	number = {4},
	journal = {IEEE Transactions on Computational Social Systems},
	author = {Zhou, Bo and Maines, Curtis and Tang, Stephen and Shi, Qi and Yang, Po and Yang, Qiang and Qi, Jun},
	month = dec,
	year = {2018},
	keywords = {Ontologies, Security, Internet of Things, Business process, Process modeling, Complexity theory, notation, security modeling, game technology, social Internet of Things (SIoT)},
	pages = {1174--1188},
}

@inproceedings{prince_sales_ontological_2018,
	title = {Ontological {Analysis} and {Redesign} of {Risk} {Modeling} in {ArchiMate}},
	doi = {10.1109/EDOC.2018.00028},
	abstract = {Risk analysis is a complex and critical activity in various contexts, ranging from strategic planning to IT systems operation. Given its complexity, several Enterprise Architecture (EA) frameworks and modeling languages have been developed to help analysts in representing and analyzing risks. Yet, the notion of risk remains overloaded and conceptually unclear in most of them. In this paper, we investigate the real-world semantics underlying risk-related constructs in one of such approaches, namely ArchiMate's Risk and Security Overlay (RSO). We perform this investigation by means of ontological analysis to reveal semantic limitations in the overlay, such as ambiguity and missing constructs. Building on the results of this analysis, we propose a well-founded redesign of the risk modeling aspects of the RSO.},
	booktitle = {2018 {IEEE} 22nd {International} {Enterprise} {Distributed} {Object} {Computing} {Conference} ({EDOC})},
	author = {Prince Sales, Tiago and Almeida, João Paulo A. and Santini, Sebastiano and Baião, Fernanda and Guizzardi, Giancarlo},
	month = oct,
	year = {2018},
	note = {ISSN: 2325-6362},
	keywords = {Ontologies, Unified Foundational Ontology, Semantics, Risk management, Security, ArchiMate, Organizations, Enterprise Architecture, Analytical models, Ontological Analysis, Risk Modeling},
	pages = {154--163},
}

@article{halawi_few_2018,
	title = {Few are as {Good} as {Many}: {An} {Ontology}-{Based} {Tweet} {Spam} {Detection} {Approach}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2877685},
	abstract = {Due to the high popularity of Twitter, spammers tend to favor its use in spreading their commercial messages. In the context of detecting twitter spams, different statistical and behavioral analysis approaches were proposed. However, these techniques suffer from many limitations due to: 1) ongoing changes to Twitter's streaming API which constrains access to a user's list of followers/followees; 2) spammer's creativity in building diverse messages; 3) use of embedded links and new accounts; and 4) need for analyzing different characteristics about users without their consent. To address the aforementioned challenges, we propose a novel ontology-based approach for spam detection over Twitter during events by analyzing the relationship between ham user tweets versus spams. Our approach relies solely on public tweet messages while performing the analysis and classification tasks. In this context, ontologies are derived and used to generate a dictionary that validates real tweet messages from random topics. Similarity ratio among the dictionary and tweets is used to reflect the legitimacy of the messages. Experiments conducted on real tweet data illustrate that message-to-message techniques achieved a low detection rate compared with our ontology-based approach which outperforms them by approximately 200\%, in addition to promising scalability for large data analysis.},
	journal = {IEEE Access},
	author = {Halawi, Bahia and Mourad, Azzam and Otrok, Hadi and Damiani, Ernesto},
	year = {2018},
	keywords = {Ontologies, ontology, Feature extraction, Twitter, Tagging, Analytical models, Electronic mail, Uniform resource locators, event spammers, meta-data, spam detection, text based analysis},
	pages = {63890--63904},
}

@inproceedings{shakeri_hossein_abad_elica_2018,
	title = {{ELICA}: {An} {Automated} {Tool} for {Dynamic} {Extraction} of {Requirements} {Relevant} {Information}},
	doi = {10.1109/AIRE.2018.00007},
	abstract = {Requirements elicitation requires extensive knowledge and deep understanding of the problem domain where the final system will be situated. However, in many software development projects, analysts are required to elicit the requirements from an unfamiliar domain, which often causes communication barriers between analysts and stakeholders. In this paper, we propose a requirements ELICitation Aid tool (ELICA) to help analysts better understand the target application domain by dynamic extraction and labeling of requirements-relevant knowledge. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural language processing tasks. In addition to the information conveyed through text, ELICA captures and processes non-linguistic information about the intention of speakers such as their confidence level, analytical tone, and emotions. The extracted information is made available to the analysts as a set of labeled snippets with highlighted relevant terms which can also be exported as an artifact of the Requirements Engineering (RE) process. The application and usefulness of ELICA are demonstrated through a case study. This study shows how pre-existing relevant information about the application domain and the information captured during an elicitation meeting, such as the conversation and stakeholders' intentions, can be captured and used to support analysts achieving their tasks.},
	booktitle = {2018 5th {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Shakeri Hossein Abad, Zahra and Gervasi, Vincenzo and Zowghi, Didar and Barker, Ken},
	month = aug,
	year = {2018},
	keywords = {Ontologies, Natural language processing, Feature extraction, Data mining, Tools, Stakeholders, Interviews, Task analysis, Dynamic information extraction, Requirements elicitation, Tool support},
	pages = {8--14},
}

@inproceedings{rabinia_fol-based_2018,
	title = {The {FOL}-{Based} {Legal}-{GRL} ({FLG}) {Framework}: {Towards} an {Automated} {Goal} {Modeling} {Approach} for {Regulations}},
	doi = {10.1109/MoDRE.2018.00014},
	abstract = {In recent years, several goal modeling approaches have been used and extended to capture the complexity of legal requirements and help modeling them in notations familiar to the requirements engineers and analysts. Legal-GRL, which is an extension of the Goal-oriented Requirements Language (GRL), is used for modeling and analyzing legal requirements. However, creating Legal-GRL models is still a manual process, which limits its effectiveness and scalability. In this paper, we propose a new goal modeling framework based on GRL to facilitate the automation of the legal requirements modeling process. Our FOL-based Legal-GRL (FLG) framework uses a legal ontology, which entails a modal theory and First-order Logic (FOL) approach, for the purpose of extraction, refinement, and representation of legal requirements. Our FLG framework consists of a database design and a set of methods for automating the modeling process. We evaluate our work by modeling several statements from HIPAA, PHIPA, the EU GDPR and EU-US Privacy Shield.},
	booktitle = {2018 {IEEE} 8th {International} {Model}-{Driven} {Requirements} {Engineering} {Workshop} ({MoDRE})},
	author = {Rabinia, Amin and Ghanavati, Sepideh},
	month = aug,
	year = {2018},
	keywords = {Ontologies, Law, Databases, Computational modeling, GRL, Complexity theory, Analytical models, First Order Logic, Goal Modeling, Legal Requirements},
	pages = {58--67},
}

@inproceedings{ali_cognitive_2018,
	title = {Cognitive {Computing} to {Optimize} {IT} {Services}},
	doi = {10.1109/ICCI-CC.2018.8482078},
	abstract = {In this paper, the challenges of maintaining a healthy IT operational environment have been addressed by proactively analyzing IT Service Desk tickets, customer satisfaction surveys and social media data. A Cognitive solution goes beyond the traditional structured data analysis solutions by deep analyses of both structured and unstructured text. The salient features of the proposed platform include language identification, translation, hierarchical extraction of the most frequently occurring topics, entities and their relationships, text summarization, sentiments and knowledge extraction from the unstructured text using Natural Language Processing techniques. Moreover, the insights from unstructured text combined with structured data allows the development of various classification, segmentation and time-series forecasting use-cases on incident, problem and change datasets. The text and predictive insights together with raw data are used for visualization and exploration of actionable insights on a rich and interactive dashboard. However, it is hard not only to find these insights using traditional Analytics solutions but it might also take very long time to discover them, especially while dealing with massive amount of unstructured data. By taking actions on these insights, organizations can benefit from significant reduction of ticket volume, reduced operational costs and increased customer satisfaction. In various experiments, on average, up to 18-25 \% of yearly ticket volume has been reduced using the proposed approach.},
	booktitle = {2018 {IEEE} 17th {International} {Conference} on {Cognitive} {Informatics} \& {Cognitive} {Computing} ({ICCI}*{CC})},
	author = {Ali, Abbas Raza},
	month = jul,
	year = {2018},
	keywords = {Semantics, Feature extraction, Data visualization, Social network services, Knowledge Extraction, Cognitive systems, Servers, Customer satisfaction, Cognitive Computing, Optimizing IT Services, Semantic Text Analytics, Service Desk, Topic Clustering},
	pages = {54--60},
}

@inproceedings{lopez_supporting_2018,
	title = {Supporting {Product} {Oriented} {Manufacturing}: a {Model} {Driven} and {Agent} based {Approach}},
	doi = {10.1109/INDIN.2018.8472069},
	abstract = {Product oriented manufacturing is aligned with one of the Industry 4.0 trends consisting of integrating all production systems. This implies shifting from a traditional view of manufacturing processes focused on production line in order to reduce costs, to a more flexible and customized product manufacturing. On the other hand, Multi Agent Systems (MAS) have been proved to be a suitable way to fulfill these requirements. However, a key aspect of the use of novel technologies is to offer methodologies and tools for supporting the implementation of such systems. In this sense, this paper uses Model Driven Engineering and MAS technology to propose an architecture that is able to launch and execute a manufacturing execution plan. It focuses on the information models managed by architecture agents that can be customized to particular manufacturing plants as well as on the definition of agent templates.},
	booktitle = {2018 {IEEE} 16th {International} {Conference} on {Industrial} {Informatics} ({INDIN})},
	author = {López, Mikel and Martín, Jon and Gangoiti, Unai and Armentia, Aintzane and Estévez, Elisabet and Marcos, Marga},
	month = jul,
	year = {2018},
	note = {ISSN: 2378-363X},
	keywords = {Ontologies, Tools, model driven engineering, Industries, Systems engineering and theory, Manufacturing processes, multi agent system, product oriented manufacturing},
	pages = {133--139},
}

@inproceedings{gand_towards_2018,
	title = {Towards {Conceptual} {Enhancements} of the {Business} {Model} {Canvas}: {The} {Case} of {Health} {Information} {Technology}},
	volume = {02},
	doi = {10.1109/CBI.2018.10047},
	abstract = {Business models (BM) describe mechanisms of value creation, delivery and capture. Business Model Representation (BMRs) in terms of conceptual models systematically and formally visualize BMs. The field of BMRs is characterized by conceptual inconsistencies such as heterogeneous notations and insufficiently described semantics. Describing and constructing structured and comparable BMs capturing concrete business cases is hampered. Even the success of the most prominent BMR approach - the business model canvas (BMC) - could only lead to slight advances. So, we aim to mature research on BMRs by enhancing the BMC with further conceptual modeling's concepts. We will focus on Health Information Technology (HIT) initiatives that are prone to fail. Analyses showed that the domain's networked nature of value creation requires to link and interrelate the BM's building blocks. Current BMR approaches, including the BMC, can only hardly provide such. For addressing BMC's conceptual weaknesses, we propose to substantiate the BMC by means of a layered concept making use of conceptual modeling's principles and considering the domain's specifics to a greater extent. Systematic and HITspecific BM(R) approaches are worthwhile as these identify and map all relevant stakeholders as well as their interrelations for value creation. That is a primary prerequisite for sustainable HIT solutions. So, we aim at bringing necessary complex information into a structured concept that allows to make profound managerial decisions. In turn, this provides more profound guidance for the implementation of HIT solutions that may lead to a higher success rates of such.},
	booktitle = {2018 {IEEE} 20th {Conference} on {Business} {Informatics} ({CBI})},
	author = {Gand, Kai},
	month = jul,
	year = {2018},
	note = {ISSN: 2378-1971},
	keywords = {Semantics, Information systems, Stakeholders, Organizations, Conceptual Modeling, Visualization, Information technology, Business Model Representations, Business Models, Health Information Technology},
	pages = {62--71},
}

@inproceedings{machado_lunardi_probabilistic_2018,
	title = {Probabilistic {Ontology} {Reasoning} in {Ambient} {Assistance}: {Predicting} {Human} {Actions}},
	doi = {10.1109/AINA.2018.00092},
	abstract = {Providing reminders to elderly people in their home environment, while they perform their daily activities, is considered as a user support activity, and thus a relevant topic in Active and Assisted Living (AAL) research and development. Determining such reminders implies decision-making, since the actions' flow (behavior) usually involves probabilistic branches. An automated system needs to decide which of the next actions is the best one for the user in a given situation. Problems of this nature involve uncertainty levels that have to be dealt with. Many approaches to this problem exploit statistical data only, thus ignoring important semantic data as, for instance, are provided by Ontologies. However, ontologies do not support reasoning over uncertainty natively. In this paper, we present a probabilistic semantic model that enables reasoning over uncertainty without losing semantic information. This model will be exemplified by an extension of the Human Behavior Monitoring and Support [HBMS] approach that provides a conceptual model for representing the user's behavior and its context in her/his living environment. The performance of this approach was evaluated using real data collected from a smart home prototype equipped with sensors. The experiments provided promising results which we will discuss regarding limits and challenges to overcome.},
	booktitle = {2018 {IEEE} 32nd {International} {Conference} on {Advanced} {Information} {Networking} and {Applications} ({AINA})},
	author = {Machado Lunardi, Gabriel and Medeiros Machado, Guilherme and Al Machot, Fadi and Maran, Vinícius and Machado, Alencar and C. Mayr, Heinrich and A. Shekhovtsov, Vladimir and Palazzo M. de Oliveira, José},
	month = may,
	year = {2018},
	note = {ISSN: 2332-5658},
	keywords = {Ontologies, Semantics, Context awareness, Uncertainty, Cognition, Context modeling, Smart home, Ambient assistance, Probabilistic ontologies, Uncertainty reasoning, Probabilistic logic, Hidden Markov models},
	pages = {593--600},
}

@inproceedings{figueiredo_breaking_2018,
	title = {Breaking into pieces: {An} ontological approach to conceptual model complexity management},
	doi = {10.1109/RCIS.2018.8406642},
	abstract = {In recent years, there has been a growth in the use of reference conceptual models, in general, and domain ontologies, in particular, to capture information about complex and critical domains. These models play a fundamental role in different types of critical semantic interoperability tasks. Therefore, it is essential that domain experts are able to understand and reason using the models' content. In other words, it is important that conceptual models are cognitively tractable. However, it is unavoidable that when the information of the represented domain grows, so does the size and complexity of the artifacts and models that represent them. For this reason, more sophisticated techniques for complexity management in ontology-driven conceptual models, need to be developed. Some approaches are based on the notion of model modularization. In this paper, we follow the work on model modularization to present an approach for view extraction for the ontology-driven conceptual modeling language OntoUML. We provide a formal definition for ontological views over OntoUML conceptual models that completely leverages on the ontologically well-grounded real-world semantics of that language. Moreover, we present a plug-in tool, particularly developed for an OntoUML model-based editor that implements this formal view structure in terms of queries defined over the OntoUML metamodel embedded in that tool.},
	booktitle = {2018 12th {International} {Conference} on {Research} {Challenges} in {Information} {Science} ({RCIS})},
	author = {Figueiredo, Guylerme and Duchardt, Amelie and Hedblom, Maria M. and Guizzardi, Giancarlo},
	month = may,
	year = {2018},
	note = {ISSN: 2151-1357},
	keywords = {Ontologies, Semantics, Tools, Unified modeling language, Computational modeling, Ontological Views, Complexity theory, Marine vehicles, Complexity Management in Conceptual Modeling, Conceptual Model Modularization, On-toUML},
	pages = {1--10},
}

@inproceedings{torsleff_developing_2018,
	title = {Developing {Ontologies} for the {Collaboration} of {Cyber}-{Physical} {Systems}: {Requirements} and {Solution} {Approach}},
	doi = {10.1109/EITEC.2018.00009},
	abstract = {Cyber-physical systems that interact with each other and form groups to achieve joint goals, are referred to as collaborative cyber-physical systems. They are exposed to highly dynamic and open contexts only partly known at design time. While model-based systems engineering already helped overcome various challenges regarding the development of conventional cyber-physical systems, challenges remain with respect to highly collaborative systems. This paper proposes an approach for modeling the context of collaborative cyber-physical systems and generating ontologies that they can use at runtime to communicate with each other and perform context-related reasoning. To account for the broad domain scope of such systems and unforeseeable future applications, the approach is not limited to a static set of prescribed ontological concepts. Instead, it facilitates the incorporation of custom concepts to meet the individual requirements of a broad spectrum of development projects concerned with collaborative cyber-physical systems.},
	booktitle = {2018 4th {International} {Workshop} on {Emerging} {Ideas} and {Trends} in the {Engineering} of {Cyber}-{Physical} {Systems} ({EITEC})},
	author = {Törsleff, Sebastian and Hildebrandt, Constantin and Daun, Marian and Brings, Jennifer and Fay, Alexander},
	month = apr,
	year = {2018},
	keywords = {Ontologies, Unified modeling language, Context modeling, Cyber-physical systems, Context Modeling, Collaboration, Model-Based Systems Engineering, DSML, Runtime, Collaborations, Context Awareness, Distributed Control, Embedded Systems},
	pages = {25--32},
}

@inproceedings{bendib_semantic_2018,
	title = {A semantic indexing approach of multimedia documents content based partial transcription},
	doi = {10.1109/ICNLSP.2018.8374390},
	abstract = {The complexity of searching and indexing spoken document retrieval depends closely on content structure and access strategies. However, Spoken Document Retrieval (SDR) approaches based on the transcription of content by LVCSR systems must treat the impact of recognition errors on the performance of the information retrieval system. Currently, several multimedia resources (spoken document) are unexploited, because the errors generated in the transcription process, decrease the performance of the systems of searches for spoken documents. Our contribution in this paper is the proposition a novel approach semantic based indexing content of multimedia documents by using the results of a partial automatic transcription to ride out research and indexing problems on these resources. In this context, the main hypothesis is developed around the following question: is it possible to identify and retrieve spoken document by using a semantic content indexing system based on his partial transcription? Also, does the partial content transcription of a spoken document is sufficient for his indexing? First, we are interested to look for a linguistic syntactic representation of the whole of the content spoken document. Then, it will be used to define the most discriminating indexing terms for his content. Nevertheless, in our approach, we have an assumption that the use of segments of a spoken document with an efficient semantic enrichment process instead of the whole spoken document content is sufficient. Also, with this strategy, we can resolve OOV problems, recognition errors and technical terms. Finally, in validation and experimentation phase, we tested the different modules proposed on the TED-LIUM corpora. The results obtained are interesting and encourage us to lead off our future perspectives.},
	booktitle = {2018 2nd {International} {Conference} on {Natural} {Language} and {Speech} {Processing} ({ICNLSP})},
	author = {Bendib, Issam and Laouar, Mohammed Ridda},
	month = apr,
	year = {2018},
	keywords = {Semantics, Indexing, Speech recognition, Syntactics, Keyword Spotting, L VCSR, Lattices, Multimedia communication, Semantic content Indexing, similarity measures, Spoken Document, Spoken Term detection, WordNet Ontology},
	pages = {1--6},
}

@inproceedings{blackburn_modeling_2018,
	title = {Modeling and cross-domain dependability analysis of cyber-physical systems},
	doi = {10.1109/SYSCON.2018.8369586},
	abstract = {This paper discusses a novel method of modeling and formal verification to support dependability analyses. The method is demonstrated in an example of a fault management capability of robots that interacts with equipment and humans. Hazard analyses produce derived requirements for fault management capabilities. These include safety critical functions for collision avoidance and temporary autonomy. Derived requirements are represented formally in models that are used to produce dependability evidence using theorem proving, model-based test vector generation, test execution with code coverage analysis, and requirement-to-test traceability. To address the challenges of heterogeneity of modeling tools and languages, Semantic Web Technologies are used for model composition and model transformation from modeling tools to formal analysis tools.},
	booktitle = {2018 {Annual} {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Blackburn, Mark R. and Austin, Mark A. and Coelho, Maria},
	month = apr,
	year = {2018},
	note = {ISSN: 2472-9647},
	keywords = {modeling, Software, Tools, component, Hazards, Mathematical model, formal methods, formal verification, Robots, Object oriented modeling, Analytical models, formatting, style, styling, cyber physical systems, dependability, fault management},
	pages = {1--8},
}

@inproceedings{weigelt_detection_2018,
	title = {Detection of {Conditionals} in {Spoken} {Utterances}},
	doi = {10.1109/ICSC.2018.00021},
	abstract = {State-of-the-art intelligent assistant systems such as Siri \& Co. struggle with conditionals. They reliably react to ordinary commands. However, their architectures are not designed to cope with complex conditional queries. We propose a system to overcome these limitations. Our approach models if-then-else constructs in spoken utterances explicitly. The model bridges the gap between linguistic and programmatic semantics. To proof our concept, we apply a rule-based approach to extract conditionals. For our prototype we use part-of-speech and chunk tags provided by NLP tools. We make use of coreference information to determine the reference frame of a condition. The explicit modeling of conditionals allows us to evaluate the accuracy of our approach independently from other language understanding tasks. The prototype works well in the domain of humanoid robotics. In a user study we achieve F1 scores of 0.783 (automatic speech recognition) up to 0.898 (manual transcripts) on unrestricted utterances.},
	booktitle = {2018 {IEEE} 12th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Weigelt, Sebastian and Hey, Tobias and Steurer, Vanessa},
	month = jan,
	year = {2018},
	keywords = {Ontologies, Semantics, Natural Language Processing, Knowledge Representation, Tools, Language Model, Linguistics, Natural Language Understanding, Spoken Language Understanding, Semantic Parsing, Robots, Task analysis, Condition Model, Conditional Semantics, Conditionals, End User Programming, Programming In Natural Language, Spoken Language Interfaces},
	pages = {85--92},
}

@article{hafeez_khan_ontology-based_2018,
	title = {Ontology-{Based} {Finite} {Satisfiability} of {UML} {Class} {Model}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2017.2786781},
	abstract = {Software models are core artifacts in model driven engineering (MDE) and processable by computer. They are automatically transformed into other models and in MDE, programming code is also produced by the models. The automatic transformation provides a systematic reuse of existing artifacts. However, sometimes models are developed with defects and the defects can implicitly shift into the code, which may be difficult to discover and repair. A promising solution to this problem is model verification. UML class model is a key ingredient of MDE. However, UML only offers graphical components without the support of reasoning, due to lack of the formal foundation. Therefore, the verification of formal properties, such as consistency and finite satisfiability is not possible in UML. This paper proposes an ontology-based optimized verification method for important correctness property “finite satisfiability”of UML class model.},
	journal = {IEEE Access},
	author = {Hafeez Khan, Abdul and Hyder Abbas Musavi, Sayed and Rehman, Aqeel-Ur and Shaikh, Asadullah},
	year = {2018},
	keywords = {Ontologies, Semantics, Software, Unified modeling language, Computational modeling, Business, model checking, Object oriented modeling, Finite satisfiability, model satisfiability, model verification, ontology-based satisfiability},
	pages = {3040--3050},
}

@article{mordecai_model-based_2018,
	title = {Model-{Based} {Interoperability} {Engineering} in {Systems}-of-{Systems} and {Civil} {Aviation}},
	volume = {48},
	issn = {2168-2232},
	doi = {10.1109/TSMC.2016.2602543},
	abstract = {Interoperability is the capability of multiple parties and systems to collaborate and exchange information and matter to obtain their objectives. Interoperability challenges call for a model-based systems engineering approach. This paper describes a conceptual modeling framework for model-based interoperability engineering (MoBIE) for systems of systems, which integrates multilayered interoperability specification, modeling, architecting, design, and testing. Treating interoperability infrastructure as a system in its own right, MoBIE facilitates interoperability among agents, processes, systems, services, and interfaces. MoBIE is founded on ISO 19450 standard-object-process methodology, a holistic paradigm for modeling and architecting complex, dynamic, and multidisciplinary systems-and allows for synergistic integration of the interoperability model with system-centric models. We also discuss the implementation of MoBIE with the unified modeling language. We discuss the importance of interoperability in the civil aviation domain, and apply MoBIE to analyze the passenger departure process in an airport terminal as a case-in-point. The resulting model enables architectural and operational decision making and analysis at the system-of-systems level and adds significant value at the interoperability engineering program level.},
	number = {4},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Mordecai, Yaniv and Orhof, Ori and Dori, Dov},
	month = apr,
	year = {2018},
	keywords = {Ontologies, Interoperability, interoperability, Unified modeling language, Business, Atmospheric modeling, Airport terminals, model-based systems engineering (MBSE), object process methodology (OPM), systems-of-systems (SoS)},
	pages = {637--648},
}

@article{alvarez_methodological_2018,
	title = {A {Methodological} {Approach} to {Model}-{Driven} {Design} and {Development} of {Automation} {Systems}},
	volume = {15},
	issn = {1558-3783},
	doi = {10.1109/TASE.2016.2574644},
	abstract = {The growing complexity of industrial automation demands the adoption of software engineering principles for improving the development process of control systems. This paper presents a methodological approach to the design and development of complex automation systems relying on model-driven engineering (MDE). A benefit of this approach is the integration of methods and techniques widespread within the automation discipline with modern MDE techniques guiding the designer through the development phases. A second advantage is to add flexibility enough to adapt the steps to the needs of the system under design. Finally, the architecture presented is prepared to be adapted to methodology extensions to cover other aspects of automation systems. The framework is based on domain models that are defined through the development phases using the terminology of the automation field. Using model transformations both documentation about system analysis and design and the skeleton of software units are automatically generated. A proof-of-concept tool has been developed that has been tested on the design of medium-complexity projects to assess the impact of its use with respect to project documentation and maintenance.Note to Practitioners—Control software development can be considered one of the challenges in automation field for achieving leadership in the future economic market. This work presents a model-driven engineering-based approach making use of both automation and software engineering methods and techniques for developing automation control systems. The framework implements the methodology for industrial automation systems ( {\textbackslash}rm MeiA\_{\textbackslash}bullet ) for guiding developers through the development phases and generates the analysis and design documentation using domain terminology, the design documentation that involves the minimal units of design, and the program organization units in one-to-one correspondence with the minimal units of design. From a practical point of view, it should be highly emphasized that developers of automation projects benefit from more structured designs, reduced number of errors, and improved project documentation.},
	number = {1},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Alvarez, María Luz and Sarachaga, Isabel and Burgos, Arantzazu and Estévez, Elisabet and Marcos, Marga},
	month = jan,
	year = {2018},
	keywords = {Automation, Software engineering, Software, Unified modeling language, Production, IEC 61131-3, industrial automation, model-driven engineering (MDE), Documentation, Control systems, Engineering frameworks, methodology for industrial automation systems ( {\textbackslash}mathrmMeiA\_{\textbackslash}bullet ), PLCopen XML},
	pages = {67--79},
}

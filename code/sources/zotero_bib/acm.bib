
@inproceedings{haghighi_ontologies_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Ontologies in {Design}: {How} {Imagining} a {Tree} {Reveals} {Possibilities} and {Assumptions} in {Large} {Language} {Models}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713633},
	doi = {10.1145/3706598.3713633},
	abstract = {Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologies—concerning what we allow ourselves to think or talk about—is a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Haghighi, Nava and Yu, Sunny and Landay, James A. and Rosner, Daniela},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Large language model, Ontology, Language model, Generative AI, large language models, ontologies, LLM agent, Ontological analysis, generative AI, Foundation models, foundation models, Ontological design, LLM agents, ontological design, Ontology's, Sociotechnical, Value-based},
	annote = {Cited by: 1},
}

@inproceedings{liu_ontotune_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{OntoTune}: {Ontology}-{Driven} {Self}-training for {Aligning} {Large} {Language} {Models}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714816},
	doi = {10.1145/3696410.3714816},
	abstract = {Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain knowledge of LLMs, leading to fragmented understanding. Inspired by how humans connect concepts and organize knowledge through mind maps, we aim to emulate this approach by using ontology with hierarchical conceptual knowledge to reorganize LLM's domain knowledge. From this perspective, we propose an ontology-driven self-training framework called OntoTune, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology. We leverage in-context learning to identify whether the LLM has acquired the specific concept's ontology knowledge, and select the entries not yet mastered by LLM as the training set to further align the LLM with ontology. Compared to existing domain LLMs based on newly collected large-scale domain-specific corpora, our OntoTune, which relies on the existing, long-term developed ontology and LLM itself, significantly reduces data maintenance costs and offers improved generalization ability. We conduct our study in the medical domain to evaluate the effectiveness of OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology source. Experimental results demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA. Moreover, compared to the latest direct ontology injection method TaxoLLaMA, our OntoTune better preserves original knowledge of LLM. The code and data are available at https://github.com/zjukg/OntoTune.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhiqiang and Gan, Chengtao and Wang, Junjie and Zhang, Yichi and Bo, Zhongpu and Sun, Mengshu and Chen, Huajun and Zhang, Wen},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language model, Large language model, Language model, Domain knowledge, Self-training, align with ontology, self-training, Ontology's, Large-scales, Align with ontology, Domain specific, Existing domains, Scale domains},
	pages = {119--133},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{shin_platform-independent_2025,
	address = {New York, NY, USA},
	series = {{SAC} '25},
	title = {A {Platform}-{Independent} {Software}-{Intensive} {Workflow} {Modeling} {Language} {And} {An} {Open}-{Source} {Visual} {Programming} {Tool}: {A} {Bottom}-{Up} {Approach} {Using} {Ontology} {Integration} {Of} {Industrial} {Workflow} {Engines}},
	isbn = {979-8-4007-0629-5},
	url = {https://doi.org/10.1145/3672608.3707840},
	doi = {10.1145/3672608.3707840},
	abstract = {Many contemporary software-intensive services are developed as workflows of collaborative and interdependent tasks. Industrial workflow platforms (i.e., engines) such as Airflow and Kubeflow automatically execute and monitor the workflow specified in platform-specific code. The code-based workflow specification becomes complex and error-prone as services grow in complexity. Furthermore, differences in platform-specific workflow specifications cause inefficiencies when porting workflows between platforms, even if the different platforms handle semantically the same workflow.In this paper, we propose a bottom-up approach for developing a platform-independent software-intensive workflow modeling language. The approach systematically extends the UML activity diagram by building platform-independent ontologies of the workflow specification from the given target industrial workflow engines. Based on the approach, we develop a platform-independent Workflow Modeling Language (WorkflowML) that covers four famous workflow engines (Airflow, Kubeflow, Argo workflow, and Metaflow). Furthermore, we implement an open-source visual programming tool for WorkflowML using the ADOxx metamodeling platform. We validate our approach by evaluating the expressiveness of WorkflowML based on modeling case studies of 42 simple workflows and two real-case workflow-based services. The evaluation results validate that WorkflowML serves as an effective common visual language for target workflow engines, supported by an open-source visual programming tool.},
	booktitle = {Proceedings of the 40th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Shin, Yong-Jun and Utz, Wilfrid},
	year = {2025},
	note = {event-place: Catania International Airport, Catania, Italy},
	keywords = {ontology, ADOxx, domain-specific modeling language, metamodeling, tool, visual programming, workflow},
	pages = {1421--1430},
}

@inproceedings{mulayim_large_2024,
	address = {New York, NY, USA},
	series = {{BuildSys} '24},
	title = {Large {Language} {Models} for the {Creation} and {Use} of {Semantic} {Ontologies} in {Buildings}: {Requirements} and {Challenges}},
	isbn = {979-8-4007-0706-3},
	url = {https://doi.org/10.1145/3671127.3698792},
	doi = {10.1145/3671127.3698792},
	abstract = {Semantic ontologies offer a formalized, machine-readable framework for representing knowledge, enabling the structured description of complex systems. In the building domain, the adoption of ontologies like the Brick schema has transformed how buildings and their systems are modeled by providing a standardized, interoperable language. However, the complexity and the steep learning curve involved in developing and querying semantic models present substantial challenges, often requiring a workforce with specialized expertise. This paper builds on our experience in investigating how Large Language Models (LLMs) can help address these challenges, focusing on their role in constructing and querying of semantic models, particularly using the Brick Schema. Our study outlines the requirements and metrics for evaluating the scalability and effectiveness of LLM-based tools, while also discussing the current challenges and limitations in developing such tools. Ultimately, this paper aims to orient research efforts as various groups experiment with diverse techniques, while enabling more effective comparison of emerging solutions and fostering collaboration across the field.},
	booktitle = {Proceedings of the 11th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {Mulayim, Ozan Baris and Paul, Lazlo and Pritoni, Marco and Prakash, Anand Krishnan and Sudarshan, Malavikha and Fierro, Gabe},
	year = {2024},
	note = {event-place: Hangzhou, China},
	keywords = {Knowledge graphs, Large Language Models, Knowledge graph, Large language model, Ontology, Language model, Semantics, Knowledge Graphs, Semantic modelling, Semantic ontology, Brick, Semantic Ontology, Latent semantic analysis, Structured Query Language, Ontology's, Query languages, In-buildings, Building requirements, Querying semantics, Steep learning curve},
	pages = {312--317},
	annote = {Cited by: 4; All Open Access; Hybrid Gold Open Access},
}

@article{meloni_exploring_2025,
	title = {Exploring {Large} {Language} {Models} for {Scientific} {Question} {Answering} via {Natural} {Language} to {SPARQL} {Translation}},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3757923},
	doi = {10.1145/3757923},
	abstract = {Translating scientific questions expressed in natural language into SPARQL queries that can be executed over knowledge graphs remains a significant challenge in the field of question answering. Recently, several prominent benchmarks, notably SciQA and DBLP-QuAD, have emerged to evaluate performance in this domain. In this paper, we provide a comprehensive analysis of the performance of language models on these benchmarks, assessing various optimization strategies. Our results indicate that the combined use of fine-tuning and prompting techniques, especially when incorporating strategic few-shot selection, produces excellent results on both benchmarks. These findings underscore an urgent need for more challenging benchmarks to better assess model capabilities. We identify key insights, common error patterns, and potential opportunities for transfer learning, and we discuss their implications for optimizing the performance of large language models in knowledge graph-based question answering tasks.},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Meloni, Antonello and Reforgiato Recupero, Diego and Osborne, Francesco and salatino, angelo and Motta, Enrico and Vahadati, Sahar and Lehmann, Jens},
	month = aug,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge graphs, Language models, Question answering, Fine-tuning, Few-shot learning},
	annote = {Just Accepted},
}

@inproceedings{komarlu_ontotype_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {{OntoType}: {Ontology}-{Guided} and {Pre}-{Trained} {Language} {Model} {Assisted} {Fine}-{Grained} {Entity} {Typing}},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671745},
	doi = {10.1145/3637528.3671745},
	abstract = {Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Komarlu, Tanay and Jiang, Minhao and Wang, Xuan and Han, Jiawei},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {Ontology, Language model, Semantics, Modeling languages, Fine-grained entity typing, Natural language understanding, natural language understanding, fine-grained entity typing, masked language model prompting, zero-shot entity typing, Natural languages, Ontology's, Natural language processing systems, Inference engines, Economic and social effects, Fine grained, Context sensitive languages, Context-sensitive, Masked language model prompting, Ontological structures, Zero-shot entity typing},
	pages = {1407--1417},
	annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{zhang_large_2024,
	address = {Richland, SC},
	series = {{AAMAS} '24},
	title = {Large {Language} {Model} {Assissted} {Multi}-{Agent} {Dialogue} for {Ontology} {Alignment}},
	isbn = {979-8-4007-0486-4},
	abstract = {Ontology alignment is critical in cross-domain integration; however, it typically necessitates the involvement of a human domain-expert, which can make the task costly. Although a variety of machine-learning approaches have been proposed that can simplify this task by learning the patterns from experts, such techniques are still susceptible to domain knowledge updates that could potentially change the patterns and lead to extra expert involvement. The use of Large Language Models (LLMs) has demonstrated a general cognitive ability, which has the potential to assist ontology alignment from the cognition level, thus obviating the need for costly expert involvement. However, the process by which the output of LLMs is generated can be opaque and thus the reliability and interpretability of such models is not always predictable. This paper proposes a dialogue model, in which multiple agents negotiate the correspondence between two knowledge sets with the support from an LLM. We demonstrate that this approach not only reduces the need for the involvement of a domain expert for ontology alignment, but that the results are interpretable despite the use of LLMs.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Zhang, Shiyao and Dong, Yuji and Zhang, Yichuan and Payne, Terry R. and Zhang, Jie},
	year = {2024},
	note = {event-place: Auckland, New Zealand},
	keywords = {large language model, multi-agent system, ontology alignment, dialogue, negotiation},
	pages = {2594--2596},
}

@inproceedings{hertling_olala_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {{OLaLa}: {Ontology} {Matching} with {Large} {Language} {Models}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627571},
	doi = {10.1145/3587259.3627571},
	abstract = {Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Hertling, Sven and Paulheim, Heiko},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, Ontology, Language model, Ontology matching, Ontology alignment, Large Language Model, Ontology Matching, Entity Resolution, Computational linguistics, Zero-shot learning, Natural languages, Ontology's, Matchings, Graph matchings, Entity resolutions},
	pages = {131--139},
	annote = {Cited by: 51; All Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{oleary_using_2025,
	title = {Using {Large} {Language} {Models} for {Armchair} {Auditors}},
	volume = {6},
	url = {https://doi.org/10.1145/3676280},
	doi = {10.1145/3676280},
	abstract = {Armchair auditors are citizens who use open data to investigate and monitor government activities, typically using analytics and other approaches. Armchair auditors provide a valuable role in holding governments and organizations accountable. This paper investigates the potential use of large language models (LLM) to support armchair auditor analyses of different governmental entities. Unfortunately, the literature, prior to the development of LLM suggested several challenges for armchair auditors. However, the analysis in this paper suggests that LLM can provide substantial data and analytic process support for armchair auditors mitigating issues such as providing guidelines for analyses, guiding users to appropriate communities, suggesting potential data availability opportunities, doing analysis, and other issues. As part of an approach to unifying armchair auditor searches, this paper also suggests a prompt library designed to support, standardize and promote best practice analyzes among armchair auditors. In addition to these issues, this paper also analyzes emerging ethical issues associated with armchair auditors and their use of open data and LLMs. Finally, this paper extends the activity theory model to account for LLMs.},
	number = {2},
	journal = {Digit. Gov.: Res. Pract.},
	author = {O'leary, Daniel E.},
	month = jun,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ChatGPT, large language models, Activity theory, armchair auditor, BARD, open data},
}

@inproceedings{jiang_retrieval_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {Retrieval {And} {Structuring} {Augmented} {Generation} with {Large} {Language} {Models}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3736557},
	doi = {10.1145/3711896.3736557},
	abstract = {Large Language Models (LLMs) have revolutionized natural language processing with their remarkable capabilities in text generation and reasoning. However, these models face critical challenges when deployed in real-world applications, including hallucination generation, outdated knowledge, and limited domain expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these limitations by integrating dynamic information retrieval with structured knowledge representations. This survey (1) examines retrieval mechanisms including sparse, dense, and hybrid approaches for accessing external knowledge; (2) explore text structuring techniques such as taxonomy construction, hierarchical classification, and information extraction that transform unstructured text into organized representations; and (3) investigate how these structured representations integrate with LLMs through prompt-based methods, reasoning frameworks, and knowledge embedding techniques. It also identifies technical challenges in retrieval efficiency, structure quality, and knowledge integration, while highlighting research opportunities in multimodal retrieval, cross-lingual structures, and interactive systems. This comprehensive overview provides researchers and practitioners with insights into RAS methods, applications, and future directions.},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Pengcheng and Ouyang, Siru and Jiao, Yizhu and Zhong, Ming and Tian, Runchu and Han, Jiawei},
	year = {2025},
	note = {event-place: Toronto ON, Canada},
	keywords = {large language models, information retrieval, knowledge representation},
	pages = {6032--6042},
}

@inproceedings{yeung_comparative_2025,
	address = {New York, NY, USA},
	series = {{LAK} '25},
	title = {A comparative study of rule-based, machine learning and large language model approaches in automated writing evaluation ({AWE})},
	isbn = {979-8-4007-0701-8},
	url = {https://doi.org/10.1145/3706468.3706566},
	doi = {10.1145/3706468.3706566},
	abstract = {Automated Writing Evaluation (AWE) tools have proved beneficial to writing development. Research on AWE methods is essential for improving tool performance and further comparative studies are needed as new methods emerge. This study examines the performance of several AWE approaches, comparing rule-based and statistical methods, machine learning (ML) models, and a large language model (LLM). These three AWE methods were applied to a representative sample of academic essays from the TOEFL11 dataset to compare their assessment performance. Results show that the selected LLM, GPT-4, outperformed the other two approaches in terms of QWK and Pearson’s correlation coefficient, while the Support Vector Machine (SVM) model in the ML approach had the highest accuracy and the lowest mean absolute error. This paper provides a detailed comparison of these three approaches and discusses implications for educational practice and future research around AWE.},
	booktitle = {Proceedings of the 15th {International} {Learning} {Analytics} and {Knowledge} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Yeung, Steven},
	year = {2025},
	keywords = {large language model, machine learning, generative AI, automated essay scoring, automated writing evaluation, Rule-based method},
	pages = {984--991},
}

@article{kreikemeyer_using_2025,
	title = {Using ({Not}-so) {Large} {Language} {Models} to {Generate} {Simulation} {Models} in a {Formal} {DSL}: {A} {Study} on {Reaction} {Networks}},
	issn = {1049-3301},
	url = {https://doi.org/10.1145/3733719},
	doi = {10.1145/3733719},
	abstract = {Formal languages are an integral part of modeling and simulation. They allow the distillation of knowledge into concise simulation models amenable to automatic execution, interpretation, and analysis. However, the arguably most humanly accessible means of expressing models is through natural language, which is not easily interpretable by computers. Here, we evaluate how a Large Language Model (LLM) might be used for formalizing natural language into simulation models. Existing studies only explored using very large LLMs, like the commercial GPT models, without fine-tuning model weights. To close this gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned to translate natural language descriptions to reaction network models in a domain-specific language, offering a self-hostable, compute-, and memory efficient alternative. To this end, we develop a synthetic data generator to serve as the basis for fine-tuning and evaluation. Our quantitative evaluation shows that our fine-tuned Mistral model can recover the ground truth simulation model in up to (84.5\% ) of cases. In addition, our small-scale user study demonstrates the model’s practical potential for one-time generation as well as interactive modeling in various domains. While promising, in its current form, the fine-tuned small LLM cannot catch up with large LLMs. We conclude that higher-quality training data are required, and expect future small and open-source LLMs to offer new opportunities.},
	journal = {ACM Trans. Model. Comput. Simul.},
	author = {Kreikemeyer, Justin Noah and Jankowski, Miłosz and Wilsdorf, Pia and Uhrmacher, Adelinde M.},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language processing, knowledge extraction, language model, constrained decoding, simulation model generation},
	annote = {Just Accepted},
}

@inproceedings{schneider_nlfoa_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {{NLFOA}: {Natural} {Language} {Focused} {Ontology} {Alignment}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627560},
	doi = {10.1145/3587259.3627560},
	abstract = {For Ontology Alignment (OA), the task is to align semantically equivalent concepts and relations from different ontologies. This task plays a crucial role in many downstream tasks and applications in academia and industry. Since manually aligning ontologies is inefficient and costly, numerous approaches exist to do this automatically. However, most approaches are tailored to specific domains, are rule-based systems or based on feature engineering, and require external knowledge. The most recent advances in the field of OA rely on the widely proven effectiveness of pre-trained language models to represent the human-generated language that describes the entities in an ontology. However, these approaches additionally require sophisticated algorithms or Graph Neural Networks to exploit an ontology’s graphical structure to achieve state-of-the-art performance. In this work, we present NLFOA, or Natural Language Focused Ontology Alignment, which purely focuses on the natural language contained in ontologies to process the ontology’s semantics as well as graphical structure. An evaluation of our approach on common OA datasets shows superior results when finetuning with only a small number of training samples. Additionally, it demonstrates strong results in a zero-shot setting which could be employed in an active learning setup to reduce human labor when manually aligning ontologies significantly.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Schneider, Florian and Dash, Sarthak and Bagchi, Sugato and Mihindukulasooriya, Nandana and Gliozzo, Alfio Massimiliano},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {Knowledge graph, Ontology, Language model, Semantics, Ontology alignment, Graph neural networks, Zero-shot learning, Ontology Alignment Sentence Transformers Zero-Shot, Natural languages, Ontology's, External knowledge, Down-stream, Feature engineerings, Graphical structures, Ontology alignment sentence transformer zero-shot, Rules based systems},
	pages = {114--121},
	annote = {Cited by: 1},
}

@article{arazzi_rag-ioe_2025,
	title = {{RAG}-{IoE}: {IoT} context-aware information retrieval with {Large} {Language} {Models} in {Industry} 5.0},
	url = {https://doi.org/10.1145/3762669},
	doi = {10.1145/3762669},
	abstract = {Human-centric design, intelligence, and seamless interconnectivity are key pillars of the Industry 5.0. A critical challenge in these scenarios is the efficient retrieval of relevant, context-aware information for workers within Internet of Everything (IoE) networks. Traditional information retrieval techniques struggle with the heterogeneous, dynamic data generated in industrial settings. To address this, we define a context-aware data model for IoE scenarios, on top of which we propose RAG-IoE, a novel Retrieval-Augmented Generation (RAG) solution to enable adaptive, scalable, and context-based information retrieval from both structured and unstructured data sources. Our approach organizes IoE data within a semantic framework, integrating hybrid retrieval methods. It combines structured search on a Knowledge Graph with unstructured data retrieval using embeddings stored in a vector database, followed by LLM-driven reasoning to refine results. This architecture enhances decision-making, reduces cognitive overload, and ensures precise guidance for industrial operators. We validate the efficiency and effectiveness of RAG-IoE using a novel dataset through both a user study and quantitative analysis, demonstrating its potential to optimize human-machine collaboration in Industry 5.0 environments.},
	journal = {ACM Trans. Internet Things},
	author = {Arazzi, Marco and Marconi Sciarroni, Monica and Nocera, Antonino and Storti, Emanuele},
	month = aug,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge Graph, Large Language Model, IoT, Context-aware, Retrieval-Augmented Generation, Industry 5.0, IoE},
	annote = {Just Accepted},
}

@article{li_large_2025,
	title = {Large {Language} {Models} for {Automated} {Web}-{Form}-{Test} {Generation}: {An} {Empirical} {Study}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3735553},
	doi = {10.1145/3735553},
	abstract = {Testing web forms is an essential activity for ensuring the quality of web applications. It typically involves evaluating the interactions between users and forms. Automated test-case generation remains a challenge for web-form testing: Due to the complex, multi-level structure of web pages, it can be difficult to automatically capture their inherent contextual information for inclusion in the tests. Large Language Models (LLMs) have shown great potential for contextual text generation. This motivated us to explore how they could generate automated tests for web forms, making use of the contextual information within form elements. To the best of our knowledge, no comparative study examining different LLMs has yet been reported for web-form-test generation. To address this gap in the literature, we conducted a comprehensive empirical study investigating the effectiveness of 11 LLMs on 146 web forms from 30 open-source Java web applications. In addition, we propose three HTML-structure-pruning methods to extract key contextual information. The experimental results show that different LLMs can achieve different testing effectiveness, with the GPT-4, GLM-4, and Baichuan2 LLMs generating the best web-form tests. Compared with GPT-4, the other LLMs had difficulty generating appropriate tests for the web forms: Their successfully-submitted rates (SSRs) — the proportions of the LLMs-generated web-form tests that could be successfully inserted into the web forms and submitted — decreased by 9.10\% to 74.15\%. Our findings also show that, for all LLMs, when the designed prompts include complete and clear contextual information about the web forms, more effective web-form tests were generated. Specifically, when using Parser-Processed HTML for Task Prompt (PH-P), the SSR averaged 70.63\%, higher than the 60.21\% for Raw HTML for Task Prompt (RH-P) and 50.27\% for LLM-Processed HTML for Task Prompt (LH-P). With RH-P, GPT-4’s SSR was 98.86\%, outperforming models like LLaMa2 (7B) with 34.47\% and GLM-4V with 0\%. Similarly, with PH-P, GPT-4 reached an SSR of 99.54\%, the highest among all models and prompt types. Finally, this paper also highlights strategies for selecting LLMs based on performance metrics, and for optimizing the prompt design to improve the quality of the web-form tests.},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Li, Tao and Cui, Chenhui and Huang, Rubing and Towey, Dave and Ma, Lei},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large Language Models (LLMs), Automated Web-Form Testing, Empirical Study, Java Web Applications, Web-Form-Test Generation},
	annote = {Just Accepted},
}

@inproceedings{mulayim_towards_2025,
	address = {New York, NY, USA},
	series = {{FMSys}},
	title = {Towards {Zero}-shot {Question} {Answering} in {CPS}-{IoT}: {Large} {Language} {Models} and {Knowledge} {Graphs}},
	isbn = {979-8-4007-1608-9},
	url = {https://doi.org/10.1145/3722565.3727197},
	doi = {10.1145/3722565.3727197},
	abstract = {Natural language provides an intuitive interface for querying data, yet its unstructured nature often makes precise retrieval of information challenging. Knowledge graphs (KGs), with their structured and relational representations, offer a powerful solution to structuring knowledge, while large language models (LLMs) are capable of interpreting user intent through language. This combination of KGs and LLMs has been explored extensively for Knowledge Graph Question Answering (KGQA), primarily for open-domain or encyclopedic knowledge. Domain-specific KGQA, instead, presents significant opportunities for Cyber-Physical Systems (CPS) and the Internet of Things (IoT), where the extraction of structured metadata is essential for automation and scalability of control and analytics applications.In this work, we evaluate and improve AutoKGQA, a domain-independent KGQA framework that utilizes LLMs to generate structured queries. Through a case study on KGs of sensor data from buildings, we assess its ability to retrieve time series identifiers, which are a requirement for extracting time series data from large sensory databases. Our results demonstrate that while AutoKGQA performs well in certain cases, its domain-agnostic approach leads to systematic failures particularly in complex queries requiring implicit knowledge. We show that domain-specific prompting significantly enhances query accuracy, allowing even smaller LLMs to perform on par with larger ones. These findings highlight the impact of domain-adapted prompting in KGQA (DA-KGQA) and suggest a path toward more efficient, scalable, and interpretable AI-driven metadata retrieval for CPS-IoT applications.},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Foundation} {Models} for {Cyber}-{Physical} {Systems} \&amp; {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Mulayim, Ozan Baris and Fierro, Gabe and Bergés, Mario and Pritoni, Marco},
	year = {2025},
	note = {event-place: Irvine, CA, USA},
	keywords = {Large Language Models, Knowledge Graphs, Time series Extraction},
	pages = {7--12},
}

@inproceedings{jadhav_leveraging_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Leveraging {Large} {Language} {Models} for {Biomedical} {Knowledge} {Graph} {Construction} and {Querying}: {An} {Advanced} {NLP} {Approach}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3717817},
	doi = {10.1145/3701716.3717817},
	abstract = {This paper introduces a novel methodology for constructing a comprehensive biomedical knowledge graph by applying advanced Natural Language Processing (NLP) techniques. By leveraging Large Language Models (LLMs) and a multifaceted prompt engineering approach, we effectively perform Named Entity Recognition (NER) and Relation Extraction (RE) on biomedical literature, targeting entities such as diseases, drugs, proteins, procedures, and symptoms. Our methodology incorporates eight distinct prompt engineering strategies for NER and a standardized approach for RE, facilitating the extraction of intricate inter-entity relationships. The resulting knowledge graph amalgamates diverse data sources into a unified framework, enabling efficient querying, visualization, and analysis of biomedical information. Furthermore, we present an innovative query processing pipeline that integrates GPT-3.5 turbo with the knowledge graph, allowing users to interact with the graph through natural language. This integrated system empowers the discovery of novel correlations, accelerating scientific research and fostering interdisciplinary collaboration. This represents a substantial contribution to the field of biomedical knowledge graph construction, offering a robust platform for accelerating scientific discovery and informing clinical decision-making.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Jadhav, Suramya and Perumal, Suki and Tadavi, Yasmin and Dash, Bikshita and Parthiban, Srinivasan},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {knowledge graphs, large language models, relationship extraction, named entity recognition, prompt engineering, query processing},
	pages = {2560--2566},
}

@inproceedings{chen_open_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Open {Local} {Knowledge} {Graph} {Construction} from {Academic} {Papers} {Using} {Generative} {Large} {Language} {Models}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3717820},
	doi = {10.1145/3701716.3717820},
	abstract = {This manuscript introduces paper2lkg, a novel Local Knowledge Graph Construction (KGC) pipeline designed to transform individual academic papers into their structured local Knowledge Graph (KG) representations. The pipeline harnesses Large Language Models (LLMs), particularly generative LLMs, to automate key Natural Language Processing (NLP) tasks in KGC. The constructed local KGs can potentially be used to enrich an existing academic KG that lacks detailed local representations of individual papers or further integrated into new academic KGs through Knowledge Graph Alignment (KGA).},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Chen, Haoting and Rodríguez Méndez, Sergio José and Omran, Pouya Ghiasnezhad},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language model, natural language processing, knowledge graph construction},
	pages = {2551--2559},
}

@inproceedings{huang_foodpuzzle_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {{FoodPuzzle}: {Toward} {Developing} {Large} {Language} {Model} {Agents} as {Autonomous} {Flavor} {Scientists}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3737384},
	doi = {10.1145/3711896.3737384},
	abstract = {Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address these challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. By leveraging their capacity to identify relevant evidence and reason within large context spaces, language model-backed agents can perform the labor-intensive tasks of flavor sourcing and understanding with enhanced efficiency and precision. To facilitate research in this area, we introduce the FoodPuzzle dataset, a challenging benchmark consisting of 978 food items and 1,766 flavor molecule profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Huang, Tenghao and Lee, Dong Hee and Sweeney, John and Shi, Jiatong and Steliotes, Emily and Lange, Matthew and May, Jonathan and Chen, Muhao},
	year = {2025},
	note = {event-place: Toronto ON, Canada},
	keywords = {large language models, retrieval-augmented generation, agent, flavor science, in-context learning},
	pages = {5493--5504},
}

@article{civitarese_large_2025,
	title = {Large {Language} {Models} {Are} {Zero}-{Shot} {Recognizers} for {Activities} of {Daily} {Living}},
	volume = {16},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3725856},
	doi = {10.1145/3725856},
	abstract = {The sensor-based recognition of Activities of Daily Living (ADLs) in smart home environments enables several applications in the areas of energy management, safety, well-being, and healthcare. ADL recognition is typically based on deep learning methods requiring large datasets to be trained. Recently, several studies proved that Large Language Models (LLMs) effectively capture common-sense knowledge about human activities. However, the effectiveness of LLMs for ADL recognition in smart home environments still deserves to be investigated. In this work, we propose ADL-LLM, a novel LLM-based ADL recognition system. ADL-LLM transforms raw sensor data into textual representations, that are processed by an LLM to perform zero-shot ADL recognition. Moreover, in the scenario where a small labeled dataset is available, ADL-LLM can also be empowered with few-shot prompting. We evaluated ADL-LLM on two public datasets, showing its effectiveness in this domain.},
	number = {4},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Civitarese, Gabriele and Fiori, Michele and Choudhary, Priyankar and Bettini, Claudio},
	month = jun,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large Language Models, Activities of Daily Living, Human Activity Recognition, Smart Home},
}

@article{vasic_knowledge_2025,
	title = {Knowledge {Graphs} vs. {Large} {Language} {Models}: {Competitors} or {Partners} in {Supporting} {Virtual} {Museums}},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3756016},
	doi = {10.1145/3756016},
	abstract = {Virtual museums are factual means for the dissemination and documentation of Cultural Heritage (CH) content. They are suitable environments for the semantic annotation of artifacts and automatic virtual guides. To this end, we identify and compare Traditional (ontology-based), Large Language Model (LLM)-extended, and LLM-pure methods for the semantic information strategies of digital CH. The traditional method is described through an application prototype, while the methods that involve LLM are tested experimentally. To investigate the integral tasks related to LLMs, our experiments include (i) semantic annotation using the CIDOC Conceptual Reference Model (CRM) and Knowledge Graph (KG) generation with LLMs for a painting sample, and (ii) painting ranking relying solely on LLMs using catalog descriptions as input. The experiments demonstrate the potential of these methods to enhance artwork interpretation, description, and refinement of the results. Based on the relevant literature on traditional semantic annotation and conducted experiments with LLMs, a combination of ontologies and LLMs may provide an optimal approach, as it offers the accuracy of structured knowledge while providing a tool that interprets these elements into natural language and vice versa. Relying solely on LLMs may be risky due to the lack of domain-specific knowledge in the training data of LLMs, whereas traditional methods demand expertise in a specific domain and are more time-consuming. Our approach shows potential in use cases such as guiding museum visitors to artifacts that match their interests, assisting museum curators with documentation, or helping CH researchers identify similarities in artifact collections.},
	journal = {J. Comput. Cult. Herit.},
	author = {Vasic, Iva and Fill, Hans-Georg and Quattrini, Ramona and Pierdicca, Roberto},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge Graphs, Large Language Model, Semantic Annotation, Cultural Heritage},
	annote = {Just Accepted},
}

@article{ampel_large_2025,
	title = {Large {Language} {Models} for {Conducting} {Advanced} {Text} {Analytics} {Information} {Systems} {Research}},
	volume = {16},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3682069},
	doi = {10.1145/3682069},
	abstract = {The exponential growth of digital content has generated massive textual datasets, necessitating the use of advanced analytical approaches. Large Language Models (LLMs) have emerged as tools that are capable of processing and extracting insights from massive unstructured textual datasets. However, how to leverage LLMs for text analytics Information Systems (IS) research is currently unclear. To assist the IS community in understanding how to operationalize LLMs, we propose a Text Analytics for Information Systems Research (TAISR) framework. Our proposed framework provides detailed recommendations grounded in IS and LLM literature on how to conduct meaningful text analytics IS research for design science, behavioral, and econometric streams. We conducted three business intelligence case studies using our TAISR framework to demonstrate its application in several IS research contexts. We also outline the potential challenges and limitations of adopting LLMs for IS. By offering a systematic approach and evidence of its utility, our TAISR framework contributes to future IS research streams looking to incorporate powerful LLMs for text analytics.},
	number = {1},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Ampel, Benjamin and Yang, Chi-Heng and Hu, James and Chen, Hsinchun},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large language models, text analytics, information systems research},
}

@inproceedings{karanikolas_large_2024,
	address = {New York, NY, USA},
	series = {{PCI} '23},
	title = {Large {Language} {Models} versus {Natural} {Language} {Understanding} and {Generation}},
	isbn = {979-8-4007-1626-3},
	url = {https://doi.org/10.1145/3635059.3635104},
	doi = {10.1145/3635059.3635104},
	abstract = {In recent years, the process humans adopt to learn a foreign language has moved from the strict "Grammar –Translation" method, which is based mainly on grammar and syntax rules, to more innovative processes, resulting to the more modern "Communicative approach". As its name states, this approach focuses on the coherent communication with native speakers and the cultivation of oral skills, without taking into consideration, at least at the first stages, the rules that govern the language. The same trend seems to have been applied to the way machinery can be "educated" to comprehend and reproduce the unfamiliar, human language. The "rule based" Natural Language Generation (NLG) and Natural Language Understanding (NLU) algorithms, on one hand, and the "text based" Large Language Models (LLMs), on the other, are two, analogous to the two human foreign language learning processes, subareas of Natural Language Processing (NLP). This paper presents these two alternative approaches, LLMs (a technology having surfaced as an influential catalyst of NLP, during last years) on the one hand and NLG/NLU on the other, highlighting their applications, their technologies, their capabilities, their differences, their strengths and weaknesses and the challenges they present, contributing to a deeper comprehension of the evolving landscape of Artificial Intelligence and human-computer communication.},
	booktitle = {Proceedings of the 27th {Pan}-{Hellenic} {Conference} on {Progress} in {Computing} and {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Karanikolas, Nikitas and Manga, Eirini and Samaridi, Nikoletta and Tousidou, Eleni and Vassilakopoulos, Michael},
	year = {2024},
	note = {event-place: Lamia, Greece},
	keywords = {Large Language Models, Natural Language Processing, Natural Language Generation, Natural Language Understanding},
	pages = {278--290},
}

@inproceedings{xiong_enhancing_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {Enhancing the {Patent} {Matching} {Capability} of {Large} {Language} {Models} via the {Memory} {Graph}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3729970},
	doi = {10.1145/3726302.3729970},
	abstract = {Intellectual Property (IP) management involves strategically protecting and utilizing intellectual assets to enhance organizational innovation, competitiveness, and value creation. Patent matching is a crucial task in intellectual property management, which facilitates the organization and utilization of patents. Existing models often rely on the emergent capabilities of Large Language Models (LLMs) and leverage them to identify related patents directly. However, these methods usually depend on matching keywords and overlook the hierarchical classification and categorical relationships of patents. In this paper, we propose MemGraph, a method that augments the patent matching capabilities of LLMs by incorporating a memory graph derived from their parametric memory. Specifically, MemGraph prompts LLMs to traverse their memory to identify relevant entities within patents, followed by attributing these entities to corresponding ontologies. After traversing the memory graph, we utilize extracted entities and ontologies to improve the capability of LLM in comprehending the semantics of patents. Experimental results on the PatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a 17.68\% performance improvement over baseline LLMs. The further analysis highlights the generalization ability of MemGraph across various LLMs, both in-domain and out-of-domain, and its capacity to enhance the internal reasoning processes of LLMs during patent matching. All data and codes are available at https://github.com/NEUIR/MemGraph.},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Xiong, Qiushi and Xu, Zhipeng and Liu, Zhenghao and Wang, Mengjia and Chen, Zulong and Sun, Yue and Gu, Yu and Li, Xiaohua and Yu, Ge},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {Large language model, Ontology, Language model, Semantics, large language models, Retrieval-augmented generation, retrieval-augmented generation, memory graph, patent matching, Ontology's, Copyrights, Intellectual assets, Intellectual property management, Matchings, Memory graph, Organizational innovation, Patent matching, Patents and inventions},
	pages = {337--347},
	annote = {Cited by: 0},
}

@inproceedings{saketos_large_2024,
	address = {New York, NY, USA},
	series = {{SETN} '24},
	title = {The {Large} {Language} {Model} {GreekLegalRoBERTa}},
	isbn = {979-8-4007-0982-1},
	url = {https://doi.org/10.1145/3688671.3688770},
	doi = {10.1145/3688671.3688770},
	abstract = {We develop four versions of GreekLegalRoBERTa, which are four large language models trained on Greek legal and nonlegal text. We show that our models surpass the performance of GreekLegalBERT, Greek- LegalBERT-v2, and GreekBERT in two tasks involving Greek legal documents: named entity recognition and multi-class legal topic classification. We view our work as a contribution to the study of domain-specific NLP tasks in low-resource languages, like Greek, using modern NLP techniques and methodologies.},
	booktitle = {Proceedings of the 13th {Hellenic} {Conference} on {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Saketos, Vasileios and Pantazi, Despina-Athanasia and Koubarakis, Manolis},
	year = {2024},
	keywords = {Natural Language Processing, Classification, Named Entity Recognition, Greek Legislation, Greek NLP Resources, Pre-trained Language Models},
}

@inproceedings{torshizi_large_2025,
	address = {Richland, SC},
	series = {{AAMAS} '25},
	title = {Large {Language} {Models} for {Virtual} {Human} {Gesture} {Selection}},
	isbn = {979-8-4007-1426-9},
	abstract = {Co-speech gestures convey a wide variety of meanings and play an important role in face-to-face human interactions. These gestures have been shown to significantly influence the addressee's engagement, recall, comprehension, and attitudes toward the speaker. Similarly, they have been shown to impact human and embodied virtual agent interaction. The process of selecting and animating meaningful gestures has thus become a key focus in designing embodied virtual agents. However, the automation of this gesture selection process poses a significant challenge. Prior gesture generation techniques have attempted to address this challenge in varied ways from fully automated, data-driven techniques – which often struggle to produce contextually meaningful gestures – to more manual approaches of crafting gesture expertise, which are time-consuming and lack generalizability. In this paper, we leverage the semantic capabilities of Large Language Models to realize a gesture selection approach that suggests meaningful, appropriate co-speech gestures. We first illustrate the information on gestures encoded into GPT4. Then we perform a study to specifically evaluate alternative prompting approaches for their ability to select meaningful, contextually relevant gestures and to align them appropriately to the co-speech utterance. Finally, we detail and demonstrate how this approach has been implemented within a virtual agent system, automating the selection and subsequent animation of the selected gestures for human-agent interactions.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Torshizi, Parisa Ghanad and Hensel, Laura B. and Shapiro, Ari and Marsella, Stacy C.},
	year = {2025},
	note = {event-place: Detroit, MI, USA},
	keywords = {large language models, gesture selection, virtual humans},
	pages = {2051--2059},
}

@article{liu_improving_2025,
	title = {Improving {Emotional} {Support} {Delivery} in {Text}-{Based} {Community} {Safety} {Reporting} {Using} {Large} {Language} {Models}},
	volume = {9},
	url = {https://doi.org/10.1145/3711012},
	doi = {10.1145/3711012},
	abstract = {Emotional support is a crucial aspect of communication between community members and police dispatchers during incident reporting. However, there is a lack of understanding about how emotional support is delivered through text-based systems, especially in various non-emergency contexts. In this study, we analyzed two years of chat logs comprising 57,114 messages across 8,239 incidents from 130 higher education institutions. Our empirical findings revealed significant variations in emotional support provided by dispatchers, influenced by the type of incident, service time, and a noticeable decline in support over time across multiple organizations. To improve the consistency and quality of emotional support, we developed and implemented a fine-tuned Large Language Model (LLM), named dispatcherLLM, designed to suggest replies through simulating human dispatchers' languages with appropriate emotional support. We evaluated dispatcherLLM by comparing its generated responses to those of human dispatchers and other off-the-shelf models using real chat messages. Additionally, we conducted a human evaluation to assess the perceived effectiveness of the support provided by dispatcherLLM. This study not only contributes new empirical understandings of emotional support in text-based dispatch systems but also demonstrates the significant potential of generative AI in improving service delivery.},
	number = {2},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Liu, Yiren and Li, Yerong and Mayfield, Ryan and Huang, Yun},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {large language models, emotion classification, event argument extraction, live chat, safety reporting, text-based reporting system},
}

@inproceedings{siddeshwar_comparative_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {A {Comparative} {Study} of {Large} {Language} {Models} for {Goal} {Model} {Extraction}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3686246},
	doi = {10.1145/3652620.3686246},
	abstract = {User stories, expressed in snippets of natural language text, are commonly used to elicit stakeholder's needs in agile software development. Requirement engineers model user stories to interpret the relations among goals and requirements. Manual transformation of goal models has challenges such as, difficulty of converting lower-abstraction user stories into higher-level goals, and extraction of goals embedded in user stories depends on the skill of requirements engineers. In this paper we introduce a technique that leverages Large Language Models (LLMs) to automatically generate goal models from user stories. The approach uses Iterative Prompt Engineering that guides LLM to extract intentional elements and generate its XML-compatible representation in Goal-oriented Requirements Language (GRL). The generated models can be visualized using jUCMNav tool. We evaluated our approach using three LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation indicates that GPT-4 or Llama can be used to assist requirements engineers in modeling as they can produce GRL goal models that are understandable. Additionally, these LLMs are capable of exposing soft goals that are not apparent to stakeholders who are new to the domain.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Siddeshwar, Vaishali and Alwidian, Sanaa and Makrehchi, Masoud},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {agile development, GPT-4, cohere, goal modeling, goal-oriented requirement language (GRL), large language models (LLMS), llama, requirements engineering, user story},
	pages = {253--263},
}

@article{zhou_aekg4apt_2025,
	title = {{AEKG4APT}: {An} {AI}-{Enhanced} {Knowledge} {Graph} for {Advanced} {Persistent} {Threats} with {Large} {Language} {Model} {Analysis}},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3735645},
	doi = {10.1145/3735645},
	abstract = {This paper introduces AEKG4APT, an APT Knowledge Graph (KG) enhanced by Large Language Models (LLMs), as a way to deal with the cybersecurity problems caused by Advanced Persistent Threats (APTs). The core of AEKG4APT lies in the combined application of LLMs, Cyber Threat Intelligence (CTI), and KG. The first part of the paper goes into great detail about how the AEKG4APT was constructed, including its ontology schema, data sources, and dataset features. There are also statistics on the AEKG4APT’s nodes, relationships, and key attributes. Secondly, it was shown how to utilize LLMs and public sandboxes for the collection and analysis of CTI Additionally, tests that compare traditional deep learning models to LLM methods show that LLM is both more efficient and more accurate at extracting information. Subsequently, the Decision Making Trial and Evaluation Laboratory - Interpretive Structural Modeling (DEMA℡-ISM) analytical method was introduced to identify and analyse the factors and their interrelationships within the AEKG4APT data, thereby revealing the key dependencies and influence paths within the data structure. Experiments were designed to demonstrate its applications in modeling, computing, and obtaining interpretable computational results on AEKG4APT. In addition, this paper also explores the dynamic expansion capabilities of AEKG4APT, including data expansion, schema expansion, and permanent maintenance strategies, to address the evolving APT threats. Finally, this paper summarizes the competitiveness and application value of AEKG4APT by comparing it with other CTI KGs and platforms in academia and industry, demonstrating its extensive application potential in the field of cybersecurity.},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Zhou, Yinghai and Wang, Ziyu and Jiang, Yunxin and Ma, Bingqi and Wang, Rui and Liu, Yuan and Zhao, Yue and Tian, Zhihong},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large Language Models, Knowledge Graph, Cyber Threat Intelligence, Advanced Persistent Threat, DEMA℡-ISM, Sandboxes},
	annote = {Just Accepted},
}

@article{bombieri_llms_2025,
	title = {Do {LLMs} {Dream} of {Ontologies}?},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3725852},
	doi = {10.1145/3725852},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across diverse natural language processing tasks, yet their ability to memorize structured knowledge remains underexplored. In this paper, we investigate the extent to which general-purpose pre-trained LLMs retain and correctly reproduce concept identifier (ID)–label associations from publicly available ontologies. We conduct a systematic evaluation across multiple ontological resources, including the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as Pythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only a small fraction of ontological concepts is accurately memorized, with GPT-4 demonstrating the highest performance. To understand why certain concepts are memorized more effectively than others, we analyze the relationship between memorization accuracy and concept popularity on the Web. Our results indicate a strong correlation between the frequency of a concept’s occurrence online and the likelihood of accurately retrieving its ID from the label. This suggests that LLMs primarily acquire such knowledge through indirect textual exposure rather than directly from structured ontological resources. Furthermore, we introduce new metrics to quantify prediction invariance, demonstrating that the stability of model responses across variations in prompt language and temperature settings can serve as a proxy for estimating memorization robustness.},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Bombieri, Marco and Fiorini, Paolo and Ponzetto, Simone Paolo and Rospocher, Marco},
	month = mar,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontologies, Large Language Models, Memorization},
	annote = {Just Accepted},
}

@inproceedings{maslaris_evaluating_2025,
	address = {New York, NY, USA},
	series = {{PCI} '24},
	title = {Evaluating {Large} {Language} {Models} in {Interaction} with {Open} {Government} {Data}},
	isbn = {979-8-4007-1317-0},
	url = {https://doi.org/10.1145/3716554.3716558},
	doi = {10.1145/3716554.3716558},
	abstract = {Large Language Models (LLMs) exhibit great abilities in understanding and generating natural language. Open Government Data (OGD) are datasets that while are available to the public, their linked structure makes it difficult to access. Large language models can significantly enhance access to linked open government data by enabling users to interact with OGD portals using natural language. This study examines the use of LLMs to interact with open government linked data effectively and efficiently. Based on the QB vocabulary, we develop a framework that formulates the task. A set of 20 questions is developed to assess the capabilities of LLMs to execute OGD-related tasks. We propose a simple system in which LLMs interact semi-automatically with OGD. Our findings indicate that smaller and quantized versions of popular LLMs are capable of effectively managing these tasks, with Llama-3.1-8B-Instruct-bnb-4bit identified as the most effective model. This paper aims to promote further interest in systems that enhance Open Government Data portals and improve public access to open data.},
	booktitle = {Proceedings of the 28th {Pan}-{Hellenic} {Conference} on {Progress} in {Computing} and {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Maslaris, Ioannis and Karamanou, Areti and Kalampokis, Evangelos and Tarabanis, Konstantinos},
	year = {2025},
	keywords = {large language models, natural language processing, linked data, open government data},
	pages = {26--33},
}

@inproceedings{zhou_ontology-semantic_2024,
	address = {New York, NY, USA},
	series = {{CSAI} '23},
	title = {Ontology-{Semantic} {Alignment} {On} {Contrastive} {Video}-{Language} {Model} for {Multimodel} {Video} {Retrieval} {Task}},
	isbn = {979-8-4007-0868-8},
	url = {https://doi.org/10.1145/3638584.3638635},
	doi = {10.1145/3638584.3638635},
	abstract = {Contrastive Learning-based models have shown impressive performance in text-image retrieval tasks. However, when applied in video retrieval, traditional contrastive learning strategies have faced challenges in achieving satisfactory results due to redundancy of video contents. We discern several potential reasons: (1)Current methodologies sometimes overlook the significant information imbalance between videos and query text, specifically neglecting the in-depth textual representation of the content within the videos. (2) Current video matching methodologies typically focus on cross-model alignment at general entity similarity level, without specific consideration for how entity pair preferences and similarity properties affect the task at hand. (3) Previous vectorized retrieval based on video content features have been somewhat flawed. They primarily focused on aligning overall features without having an video content tags feature for meaningful feature discrimination. Considering the shortcomings identified in the mentioned three aspects, we propose an ontology semantic labels augments retrieval model and introduce a method to integrate video ontology semantic labels into the contrastive learning framework. In particular, we have developed ontology semantic descriptions about entities encompassing both human figures and textual elements within the videos. Subsequently, we conducted training and testing on the CMIVQA dataset to assess the performance of our approach. The experimental results show that employing fine-grained ontology labels as sample pairs for contrastive learning leads to an increased level of precision in video retrieval tasks.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Yifan and Ding, Yizhou and Dong, Yuwu and He, Hao},
	year = {2024},
	note = {event-place: Beijing, China},
	keywords = {Ontology, Semantics, Video retrieval, Image retrieval, Performance, Alignment, Multi-modal, Multimodal alignment, Ontology description, Video content understanding, Learning systems, Ontology's, 'current, Ontology semantics, Video recording, Statistical tests, Video contents},
	pages = {408--413},
	annote = {Cited by: 0},
}

@inproceedings{kocyigit_deceptilens_2025,
	address = {New York, NY, USA},
	series = {{FAccT} '25},
	title = {{DeceptiLens}: an {Approach} supporting {Transparency} in {Deceptive} {Pattern} {Detection} based on a {Multimodal} {Large} {Language} {Model}},
	isbn = {979-8-4007-1482-5},
	url = {https://doi.org/10.1145/3715275.3732129},
	doi = {10.1145/3715275.3732129},
	abstract = {To detect deceptive design patterns on UIs, traditional artificial intelligence models, such as machine learning, have limited coverage and a lack of multimodality. In contrast, the capabilities of Multimodal Large Language Model (MM-LLM) can achieve wider coverage with superior performance in the detection, while providing reasoning behind each decision. We propose and implement an MM-LLM-based approach (DeceptiLens) that analyzes UIs and assesses the presence of deceptive design patterns. We utilize Retrieval Augmented Generation (RAG) process in our design and task the model with capturing the deceptive patterns, classifying its category, e.g., false hierarchy, confirmshaming, etc., and explaining the reasoning behind the classifications by employing recent prompt engineering techniques, such as Chain-of-Thought (CoT). We first create a dataset by collecting UI screenshots from the literature and web sources and quantify the agreement between the model’s outputs and a few experts’ opinions. We additionally ask experts to gauge the transparency of the system’s explanations for its classifications in terms of recognized metrics of clarity, correctness, completeness, and verifiability. The results indicate that our approach is capable of capturing the deceptive patterns in UIs with high accuracy while providing clear, correct, complete, and verifiable justifications for its decisions. We additionally release two curated datasets, one with expert-labeled UIs with deceptive design patterns, and one with AI-based generated explanations. Lastly, we propose recommendations for future improvement of the approach in various contexts of use.},
	booktitle = {Proceedings of the 2025 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Kocyigit, Emre and Rossi, Arianna and Sergeeva, Anastasia and Negri Ribalta, Claudia and Farjami, Ali and Lenzini, Gabriele},
	year = {2025},
	keywords = {LLMs, dark patterns, deceptive design patterns, multimodal LLMs},
	pages = {1942--1959},
}

@article{zhang_improving_2025,
	title = {Improving {Deep} {Assertion} {Generation} via {Fine}-{Tuning} {Retrieval}-{Augmented} {Pre}-{Trained} {Language} {Models}},
	volume = {34},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3721128},
	doi = {10.1145/3721128},
	abstract = {Unit testing validates the correctness of the units of the software system under test and serves as the cornerstone in improving software quality and reliability. To reduce manual efforts in writing unit tests, some techniques have been proposed to generate test assertions automatically, including Deep Learning (DL)-based, retrieval-based, and integration-based ones. Among them, recent integration-based approaches inherit from both DL-based and retrieval-based approaches and are considered state-of-the-art. Despite being promising, such integration-based approaches suffer from inherent limitations, such as retrieving assertions with lexical matching while ignoring meaningful code semantics and generating assertions with a limited training corpus.In this article, we propose a novel Retrieval-Augmented Deep Assertion Generation (RetriGen) approach based on a hybrid assertion retriever and a Pre-Trained Language Model (PLM)-based assertion generator. Given a focal-test, RetriGen first builds a hybrid assertion retriever to search for the most relevant test–assert pair from external codebases. The retrieval process takes both lexical similarity and semantical similarity into account via a token-based and an embedding-based retriever, respectively. RetriGen then treats assertion generation as a sequence-to-sequence task and designs a PLM-based assertion generator to predict a correct assertion with historical test–assert pairs and the retrieved external assertion. Although our concept is general and can be adapted to various off-the-shelf encoder–decoder PLMs, we implement RetriGen to facilitate assertion generation based on the recent CodeT5 model. We conduct extensive experiments to evaluate RetriGen against six state-of-the-art approaches across two large-scale datasets and two metrics. The experimental results demonstrate that RetriGen achieves 57.66\% and 73.24\% in terms of accuracy and CodeBLEU, outperforming all baselines with an average improvement of 50.66\% and 14.14\%, respectively. Furthermore, RetriGen generates 1,598 and 1,818 unique correct assertions that all baselines fail to produce, 3.71X and 4.58X more than the most recent approach EditAS. We also demonstrate that adopting other PLMs can provide substantial advancement, e.g., four additionally utilized PLMs outperform EditAS by 7.91\%–12.70\% accuracy improvement, indicating the generalizability of RetriGen. Overall, our study highlights the promising future of fine-tuning off-the-shelf PLMs to generate accurate assertions by incorporating external knowledge sources.},
	number = {7},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Zhang, Quanjun and Fang, Chunrong and Zheng, Yi and Zhang, Yaxin and Zhao, Yuan and Huang, Rubing and Zhou, Jianyi and Yang, Yun and Zheng, Tao and Chen, Zhenyu},
	month = aug,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Pre-trained Language Models, AI4SE, Assertion Generation, Unit Testing},
}

@inproceedings{haag_last_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {The {Last} {JITAI}? {Exploring} {Large} {Language} {Models} for {Issuing} {Just}-in-{Time} {Adaptive} {Interventions}: {Fostering} {Physical} {Activity} in a {Prospective} {Cardiac} {Rehabilitation} {Setting}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713307},
	doi = {10.1145/3706598.3713307},
	abstract = {We evaluated the viability of using Large Language Models (LLMs) to trigger and personalize content in Just-in-Time Adaptive Interventions (JITAIs) in digital health. As an interaction pattern representative of context-aware computing, JITAIs are being explored for their potential to support sustainable behavior change, adapting interventions to an individual's current context and needs. Challenging traditional JITAI implementation models, which face severe scalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs in the use case of heart-healthy activity in cardiac rehabilitation. Using three personas representing patients affected by CVD with varying severeness and five context sets per persona, we generated 450 JITAI decisions and messages. These were systematically evaluated against those created by 10 laypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated JITAIs surpassed human-generated intervention suggestions, outperforming both LayPs and HCPs across all metrics (i.e., appropriateness, engagement, effectiveness, and professionalism). These results highlight the potential of LLMs to enhance JITAI implementations in personalized health interventions, demonstrating how generative AI could revolutionize context-aware computing.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Haag, David and Kumar, Devender and Gruber, Sebastian and Hofer, Dominik P., MSc and Sareban, Mahdi and Treff, Gunnar and Niebauer, Josef and Bull, Christopher N and Schmidt, Albrecht and Smeddinck, Jan David},
	year = {2025},
	keywords = {large language models, digital health, LLMs, generative AI, adaptive interventions, context-aware computing, healthcare AI, human-AI interaction, JITAIs, just-in-time adaptive interventions},
}

@inproceedings{bouadi_synergizing_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Synergizing {Large} {Language} {Models} and {Knowledge}-{Based} {Reasoning} for {Interpretable} {Feature} {Engineering}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714720},
	doi = {10.1145/3696410.3714720},
	abstract = {Feature engineering stands as a pivotal step in enhancing the performance of machine learning (ML) models, particularly with tabular data. However, traditional feature engineering methods are often time-consuming and requires case-by-case domain knowledge. In addition, as ML systems become more common, interpretability becomes increasingly important, especially among domain experts. To this end, we propose ReaGen, an automated feature engineering (AutoFE) approach that combines knowledge graphs (KGs) with large language models (LLMs) to generate interpretable features. ReaGen begins by symbolic REAsoning over the KG to extract relevant information based on datasets description. Then, it uses an LLM to iteratively GENerate meaningful features. Finally, to overcome challenges such as hallucinations and handling long contexts typical in LLMs, our model performs logical reasoning on the KG to ensure that the generated features maintain interpretability. ReaGen provides Python code for automatic feature generation and detailed explanations of feature utility. It leverages both LLM's internal knowledge and retrieved information from KGs. Experiments on public datasets demonstrate that ReaGen significantly improves prediction accuracy while ensuring high interpretability through human-like explanations for each feature. This work highlights the potential of integrating LLMs and KGs in feature engineering, paving the way for interpretable ML models.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Bouadi, Mohamed and Alavi, Arta and Benbernou, Salima and Ouziri, Mourad},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {knowledge graphs, large language models, automated feature engineering, interpretable ML, symbolic reasoning},
	pages = {2606--2620},
}

@inproceedings{peng_refining_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {Refining {Wikidata} {Taxonomy} using {Large} {Language} {Models}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679156},
	doi = {10.1145/3627673.3679156},
	abstract = {Due to its collaborative nature, Wikidata is known to have a complex taxonomy, with recurrent issues like the ambiguity between instances and classes, the inaccuracy of some taxonomic paths, the presence of cycles, and the high level of redundancy across classes. Manual efforts to clean up this taxonomy are time-consuming and prone to errors or subjective decisions. We present WiKC, a new version of Wikidata taxonomy cleaned automatically using a combination of Large Language Models (LLMs) and graph mining techniques. Operations on the taxonomy, such as cutting links or merging classes, are performed with the help of zero-shot prompting on an open-source LLM. The quality of the refined taxonomy is evaluated from both intrinsic and extrinsic perspectives, on a task of entity typing for the latter, showing the practical interest of WiKC.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Peng, Yiwen and Bonald, Thomas and Alam, Mehwish},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {large language model, knowledge graphs, graph mining},
	pages = {5395--5399},
}

@inproceedings{barile_lp-dixit_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{LP}-{DIXIT}: {Evaluating} {Explanations} for {Link} {Predictions} on {Knowledge} {Graphs} using {Large} {Language} {Models}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714667},
	doi = {10.1145/3696410.3714667},
	abstract = {Knowledge Graphs provide a machine-readable representation of knowledge conforming to graph-based data models. Link prediction methods predict missing facts in incomplete knowledge graphs, often using scalable embedding based solutions that, however, lack comprehensibility which is crucial in many domains. Filling this gap, explanation methods identify supporting knowledge. For evaluating them, user studies are the obvious choice as users are the main recipients of explanations. However, finding domain experts is often challenging. In contrast, an automated approach is to measure the influence of explanations on the very same link prediction task, thus disregarding the perspective of users. Additionally, current evaluation methods vary across different explanation approaches. We propose LP-DIXIT, the first protocol to evaluate the utility of explanations of link predictions. LP-DIXIT is user-aware, algorithmic and unique for different explanation methods. It builds on a typical setting of user studies, but adopts Large Language Models (LLMs) to mimic users. Specifically, it measures how explanations improve the user (LLM) ability to perform predictions, which is key to trust. We experimentally proved an overall agreement between LP-DIXIT and user evaluations. Moreover, we adopted LP-DIXIT to conduct a comparative study of state-of-the-art explanation methods. The outcomes suggest that less is more: the most effective explanations are those consisting of a single fact.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Barile, Roberto and d'Amato, Claudia and Fanizzi, Nicola},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {knowledge graphs, large language models, explanation, link prediction},
	pages = {4034--4042},
}

@inproceedings{zhai_large_2024,
	address = {New York, NY, USA},
	series = {{SIGIR} '24},
	title = {Large {Language} {Models} and {Future} of {Information} {Retrieval}: {Opportunities} and {Challenges}},
	isbn = {979-8-4007-0431-4},
	url = {https://doi.org/10.1145/3626772.3657848},
	doi = {10.1145/3626772.3657848},
	abstract = {Recent years have seen great success of large language models (LLMs) in performing many natural language processing tasks with impressive performance, including tasks that directly serve users such as question answering and text summarization. They open up unprecedented opportunities for transforming information retrieval (IR) research and applications. However, concerns such as halluciation undermine their trustworthiness, limiting their actual utility when deployed in real-world applications, especially high-stake applications where trust is vital. How can we both exploit the strengths of LLMs and mitigate any risk caused by their weaknesses when applying LLMs to IR? What are the best opportunities for us to apply LLMs to IR? What are the major challenges that we will need to address in the future to fully exploit such opportunities? Given the anticipated growth of LLMs, what will future information retrieval systems look like? Will LLMs eventually replace an IR system? In this perspective paper, we examine these questions and provide provisional answers to them. We argue that LLMs will not be able to replace search engines, and future LLMs would need to learn how to use a search engine so that they can interact with a search engine on behalf of users. We conclude with a set of promising future research directions in applying LLMs to IR.},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhai, ChengXiang},
	year = {2024},
	note = {event-place: Washington DC, USA},
	keywords = {large language models, conversational information access, information retrieval models, intelligent agent, search engines},
	pages = {481--490},
}

@inproceedings{alvarado_garcia_emerging_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Emerging {Data} {Practices}: {Data} {Work} in the {Era} of {Large} {Language} {Models}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3714069},
	doi = {10.1145/3706598.3714069},
	abstract = {Data is one of the foundational aspects of making Artificial Intelligence (AI) work as intended. As large language models (LLMs) become the epicenter of AI, it is crucial to understand better how the datasets that maintain such models are created. The emergent nature of LLMs makes it critical to understand the challenges practitioners developing Gen AI technologies face to design alternatives for better responding to Gen AI’s ethical issues. In this paper, we provide such understanding by reporting on 25 interviews with practitioners who handle data in three distinct development stages of different LLMs. Our contributions are (1) empirical evidence of how uncertainty, data practices, and reliance mechanisms change across LLMs’ development cycle; (2) how the unique qualities of LLMs impact data practices and their implications for the future of Gen AI technologies; and (3) provide three opportunities for HCI researchers interested in supporting practitioners developing Gen AI technologies.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Alvarado Garcia, Adriana and Candello, Heloisa and Badillo-Urquiola, Karla and Wong-Villacres, Marisol},
	year = {2025},
	keywords = {AI, LLMs, generative AI, GenAI, synthetic data, AI practitioners, data governance, data practices, data work},
}

@article{koyuncu_exploring_2025,
	title = {Exploring {Fine}-{Grained} {Bug} {Report} {Categorization} with {Large} {Language} {Models} and {Prompt} {Engineering}: {An} {Empirical} {Study}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3736408},
	doi = {10.1145/3736408},
	abstract = {Accurate classification of issues is essential for effective project management and timely responses, as the volume of issue reports continues to grow. Manual classification is labor-intensive and error-prone, necessitating automated solutions. While large language models (LLMs) show promise in automated issue labeling, most research focuses on broad categorization (e.g., bugs, feature requests), with limited attention to fine-grained categorization. Understanding specific bug types is crucial, as different bugs require tailored resolution strategies.This study addresses this gap by evaluating LLMs and prompt engineering strategies for fine-grained bug report categorization. We analyze 221,184 fine-grained bug report category labels generated by selected LLMs using various prompt engineering strategies for 1,024 bug reports. We examine how LLMs and prompt engineering influence output characteristics, control over outputs, and categorization performance. Our findings highlight that LLMs and prompt engineering significantly impact output consistency and classification capability, with some yielding consistent results and others introducing variability. Based on these findings, we analyze the agreements and disagreements between LLM-generated labels and human annotations to assess category correctness. Our results suggest that examining label consistency and discrepancies can serve as a complementary method for validating bug report categories, identifying unclear reports, and detecting misclassifications in human annotations.},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Koyuncu, Anil},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large Language Models, Prompt Engineering, Automatic Bug Report Classification, Label correctness},
	annote = {Just Accepted},
}

@inproceedings{zhang_automated_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Automated {Mining} of {Structured} {Knowledge} from {Text} in the {Era} of {Large} {Language} {Models}},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671469},
	doi = {10.1145/3637528.3671469},
	abstract = {Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have shed light on the text mining field with their superior text understanding and instruction-following ability. There are typically two ways of utilizing LLMs: fine-tune the LLMs with human-annotated training data, which is labor intensive and hard to scale; prompt the LLMs in a zero-shot or few-shot way, which cannot take advantage of the useful information in the massive text data. Therefore, it remains a challenge on automated mining of structured knowledge from massive text data in the era of large language models. In this tutorial, we cover the recent advancements in mining structured knowledge using language models with very weak supervision. We will introduce the following topics in this tutorial: (1) introduction to large language models, which serves as the foundation for recent text mining tasks, (2) ontology construction, which automatically enriches an ontology from a massive corpus, (3) weakly-supervised text classification in flat and hierarchical label space, (4) weakly-supervised information extraction, which extracts entity and relation structures.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yunyi and Zhong, Ming and Ouyang, Siru and Jiao, Yizhu and Zhou, Sizhe and Ding, Linyi and Han, Jiawei},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {Large language model, Ontology, Language model, large language models, text mining, Self-supervised learning, Zero-shot learning, Text-mining, Scientific papers, weak supervision, Structured knowledge, Text data, Unstructured texts, Automated mining, News articles, Weak supervision},
	pages = {6644--6654},
	annote = {Cited by: 3; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{zhao_llmre_2024,
	address = {New York, NY, USA},
	series = {{EITCE} '23},
	title = {{LlmRe}: {A} zero-shot entity relation extraction method based on the large language model},
	isbn = {979-8-4007-0830-5},
	url = {https://doi.org/10.1145/3650400.3650478},
	doi = {10.1145/3650400.3650478},
	abstract = {Entity relation extraction aims to extract knowledge triples from unstructured or semi-structured text data and can be applied to various fields, including medicine, finance knowledge graph construction and intelligent question-answering. Traditional entity relation extraction requires a large amount of labeled data, consumes a lot of labor and time, and the trained model lacks generalization ability, which is difficult to migrate to other fields. Zero-shot entity relation extraction relieves the dependence on labeled data in traditional method. Based on unlabeled text data, zero-shot entity relation extraction has strong domain adaptability, which is a very challenging and practical task. Recent work on large language models shows that large models can effectively complete downstream tasks through natural language instructions and have good generalization ability. Inspired by this, we explore the use of large models for information extraction. Due to the randomness of large language model generation, we introduce in-context learning in entity relation extraction task to guide large language model to output data in a specified format to help obtain structured data. At the same time, we propose a three-stage extraction framework for decomposing entity relation extraction tasks, and each stage is conducted in the form of question and answer to reduce the complexity of extraction. We evaluated the knowledge triples extraction performance of the model on three self-built test datasets in different fields, and the experimental result showed that our proposed method achieved impressive performance in the zero-shot entity relation extraction task, surpassing the comparison model on multiple metrics, proving the effectiveness and domain adaptability of the proposed method.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Wei and Chen, Qinghui and You, Junling},
	year = {2024},
	note = {event-place: Xiamen, China},
	pages = {475--480},
}

@inproceedings{arrieta_cosmo_2024,
	address = {New York, NY, USA},
	series = {{SAC} '24},
	title = {{CoSMo}: {A} multilingual modular language for {Content} {Selection} {Modelling}},
	isbn = {979-8-4007-0243-3},
	url = {https://doi.org/10.1145/3605098.3635889},
	doi = {10.1145/3605098.3635889},
	abstract = {Representing snippets of information abstractly is a task that needs to be performed for various purposes, such as database view specification and the first stage in the natural language generation pipeline for generative AI from structured input, i.e., the content selection stage to determine what needs to be verbalised. For the Abstract Wikipedia project, requirements analysis revealed that such an abstract representation requires multilingual modelling, content selection covering declarative content and functions, and both classes and instances. There is no modelling language that meets either of the three features, let alone a combination. Following a rigorous language design process inclusive of broad stakeholder consultation, we created CoSMo, a novel Content Selection Modeling language that meets these and other requirements so that it may be useful both in Abstract Wikipedia as well as other contexts. We describe the design process, rationale and choices, the specification, and preliminary evaluation of the language.},
	booktitle = {Proceedings of the 39th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Arrieta, Kutz and Fillottrani, Pablo R and Keet, C. Maria},
	year = {2024},
	note = {event-place: Avila, Spain},
	keywords = {modeling language, multilingualism, query language, wikidata},
	pages = {706--713},
}

@article{spinner_-generaitor_2024,
	title = {-{generAItor}: {Tree}-in-the-loop {Text} {Generation} for {Language} {Model} {Explainability} and {Adaptation}},
	volume = {14},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/3652028},
	doi = {10.1145/3652028},
	abstract = {Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases.},
	number = {2},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Spinner, Thilo and Kehlbeck, Rebecca and Sevastjanova, Rita and Stähle, Tobias and Keim, Daniel A. and Deussen, Oliver and El-Assady, Mennatallah},
	month = jun,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large language models, natural language generation, beam search tree, explainability, language transformers, visual analytics},
}

@inproceedings{zeginis_applying_2025,
	address = {New York, NY, USA},
	series = {{PCI} '24},
	title = {Applying an ontology-aware zero-shot {LLM} prompting approach for information extraction in {Greek}: the case of {DIAVGEIA} gov gr},
	isbn = {979-8-4007-1317-0},
	url = {https://doi.org/10.1145/3716554.3716603},
	doi = {10.1145/3716554.3716603},
	abstract = {Large Language Models (LLMs) have attracted considerable attention, primarily due to their potential to revolutionize sectors that heavily rely on textual information. Governance is one such sector. Public administrations around the globe produce millions of documents including laws, administrative decisions and acts (e.g., travel/budget approvals) that contain valuable information in unstructured way. The documents are usually stored at document-centered repositories. As a result the actual data of the documents cannot be further searched or processed. The availability of structured metadata of the documents (e.g., who has traveled, where, when) could further enhance the searching and processing of the documents as well as enable data analytics. The construction of metadata can be done through information extraction approaches such as Named Entity Recognition (NER), Relation Extraction (RE) and Event Extraction (EE) on the documents. LLMs are recently used successfully for information extraction tasks, while ontologies are traditionally used for meaningful data modeling. The aim of the paper is to apply and evaluate an ontology-aware zero-shot LLM prompting approach for information extraction in Greek language documents available in DIAVGEIA.gov.gr - the Greek Open Government portal for administrative documents. The evaluation assesses various LLM models/sizes for various difficulties of information extraction tasks. Overall the results are very promising, since most LLM models, even smaller ones, performed very well for all tasks in Greek.},
	booktitle = {Proceedings of the 28th {Pan}-{Hellenic} {Conference} on {Progress} in {Computing} and {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Zeginis, Dimitris and Kalampokis, Evangelos and Tarabanis, Konstantinos},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Information extraction, Large language model, Ontology, LLM, Language model, Named entity recognition, Relation extraction, Modeling languages, Information retrieval, Metadata, Data mining, Public administration, Data analytics, NER, Greek, Ontology's, Information retrieval systems, Structured metadatas, Textual information},
	pages = {324--330},
	annote = {Cited by: 0},
}

@article{fang_how_2024,
	title = {How do {Large} {Language} {Models} understand {Genes} and {Cells}},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3702234},
	doi = {10.1145/3702234},
	abstract = {Researching genes and their interactions is crucial for deciphering the fundamental laws of cellular activity, advancing disease treatment, drug discovery, and more. Large language Models (LLMs), with their profound text comprehension and generation capabilities, have made significant strides across various natural science fields. However, their application in cell biology remains limited and a systematic evaluation of their performance is lacking. To address this gap, in this paper, we select seven mainstream LLMs and evaluate their performance across nine gene-related problem scenarios. Our findings indicate that LLMs possess a certain level of understanding of genes and cells, but still lag behind domain-specific models in comprehending transcriptional expression profiles. Moreover, we have improved the current method of textual representation of cells, enhancing the LLMs’ ability to tackle cell annotation tasks. We encourage cell biology researchers to leverage LLMs for problem-solving while being mindful of the associated challenges. We release our code and data at .},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Fang, Chen and Wang, Yidong and Song, Yunze and Long, Qingqing and Lu, Wang and Chen, Linghui and Feng, Guihai and Zhou, Yuanchun and Li, Xin},
	month = oct,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {large language models, cell annotation, cell biology, gene gene interaction},
	annote = {Just Accepted},
}

@article{swaileh_a_alzaidi_text-inception-based_2024,
	title = {A {Text}-{Inception}-{Based} {Natural} {Language} {Processing} {Model} for {Sentiment} {Analysis} of {Drug} {Experiences}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3678470},
	doi = {10.1145/3678470},
	abstract = {The study of sentiment in Natural Language Processing (NLP) is among the most successful research areas because of the availability of millions of user opinions online since the turn of the century. The economic, political, and medical fields are just some of the many that have benefited from studies of sentiment research. While numerous studies have examined more mainstream topics like consumer electronics, movies, and restaurants, relatively few have examined health and medical concerns. Considerable insight into where to direct efforts to improve public health might be gained by a study of how people feel about healthcare as a whole and of individual drug experiences in particular. When it comes to medicine, automatic analysis of online user evaluations paves the way for sifting through massive amounts of user feedback to find information regarding medications' efficacy and side effects that might be used to enhance pharmacovigilance programs. Simple rules-based methods have given way to more complex machine learning approaches like deep learning, which is developing as a technology for many natural language processing jobs. The opensource datasets have been analyzed with models that use word embeddings and term frequency-inverse document frequency (TF-IDF). A feature-enhanced text-inception model for sentiment classification was presented to work in tandem with this approach. The model first employed a cutting-edge text-inception module to glean useful shallow features from the text. K-MaxPooling was subsequently employed to reduce the dimensionality of its shallow and deep includes as well as enhance the generalization of characteristics, and a deep feature extraction module was formed using the bidirectional gated recurrent unit (Bi-GRU) and the capsule neural network to comprehend the text's semantic data. By combining traditional methods with cutting-edge artificial intelligence techniques, this hybrid approach can revolutionize public health initiatives, decision-making, and pharmacovigilance in the healthcare industry. This model achieved an exceptional accuracy rate of 99\%, underscoring its effectiveness in sentiment classification and demonstrating its potential to significantly contribute to advancing healthcare and medical research.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Swaileh A. Alzaidi, Muhammad and Alshammari, Alya and Almanea, Manar and Al-khawaja, Haneen A. and Al Sultan, Hanan and Alotaibi, Shoayee and Almukadi, Wafa},
	month = aug,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Deep learning, Healthcare, Natural Language Processing (NLP), Medical Sentiment, Public Health, User Opinions},
	annote = {Just Accepted},
}

@inproceedings{ali_glamor_2024,
	address = {New York, NY, USA},
	series = {{RecSys} '24},
	title = {{GLAMOR}: {Graph}-based {LAnguage} {MOdel} embedding for citation {Recommendation}},
	isbn = {979-8-4007-0505-2},
	url = {https://doi.org/10.1145/3640457.3688171},
	doi = {10.1145/3640457.3688171},
	abstract = {Digital publishing’s exponential growth has created vast scholarly collections. Guiding researchers to relevant resources is crucial, and knowledge graphs (KGs) are key tools for unlocking hidden knowledge. However, current methods focus on external links between concepts, ignoring the rich information within individual papers. Challenges like insufficient multi-relational data, name ambiguity, and cold-start issues further limit existing KG-based methods, failing to capture the intricate attributes of diverse entities. To solve these issues, we propose GLAMOR, a robust KG framework encompassing entities e.g., authors, papers, fields of study, and concepts, along with their semantic interconnections. GLAMOR uses a novel random walk-based KG text generation method and then fine-tunes the language model using the generated text. Subsequently, the acquired context-preserving embeddings facilitate superior top@k predictions. Evaluation results on two public benchmark datasets demonstrate our GLAMOR’s superiority against state-of-the-art methods especially in solving the cold-start problem.},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ali, Zafar and Qi, Guilin and Ullah, Irfan and Mohammed, Adam A. Q. and Kefalas, Pavlos and Muhammad, Khan},
	year = {2024},
	note = {event-place: Bari, Italy},
	keywords = {Large Language Model, Recommender Systems, Attributed Graph Embedding, Citation Recommendation, Cold-start, GLAMOR},
	pages = {929--933},
}

@inproceedings{kraft_knowledge-enhanced_2024,
	address = {New York, NY, USA},
	series = {{FAccT} '24},
	title = {Knowledge-{Enhanced} {Language} {Models} {Are} {Not} {Bias}-{Proof}: {Situated} {Knowledge} and {Epistemic} {Injustice} in {AI}},
	isbn = {979-8-4007-0450-5},
	url = {https://doi.org/10.1145/3630106.3658981},
	doi = {10.1145/3630106.3658981},
	abstract = {The factual inaccuracies ("hallucinations") of large language models have recently inspired more research on knowledge-enhanced language modeling approaches. These are often assumed to enhance the overall trustworthiness and objectivity of language models. Meanwhile, the issue of bias is usually only mentioned as a limitation of statistical representations. This dissociation of knowledge-enhancement and bias is in line with previous research on AI engineers’ assumptions about knowledge, which indicate that knowledge is commonly understood as objective and value-neutral by this community. We argue that claims and practices by actors of the field still reflect this underlying conception of knowledge. We contrast this assumption with literature from social and, in particular, feminist epistemology, which argues that the idea of a universal disembodied knower is blind to the reality of knowledge practices and seriously challenges claims of "objective" or "neutral" knowledge. Knowledge enhancement techniques commonly use Wikidata and Wikipedia as their sources for knowledge, due to their large scales, public accessibility, and assumed trustworthiness. In this work, they serve as a case study for the influence of the social setting and the identity of knowers on epistemic processes. Indeed, the communities behind Wikidata and Wikipedia are known to be male-dominated and many instances of hostile behavior have been reported in the past decade. In effect, the contents of these knowledge bases are highly biased. It is therefore doubtful that these knowledge bases would contribute to bias reduction. In fact, our empirical evaluations of RoBERTa, KEPLER, and CoLAKE, demonstrate that knowledge enhancement may not live up to the hopes of increased objectivity. In our study, the average probability for stereotypical associations was preserved on two out of three metrics and performance-related gender gaps on knowledge-driven task were also preserved. We build on these results and critical literature to argue that the label of "knowledge" and the commonly held beliefs about it can obscure the harm that is still done to marginalized groups. Knowledge enhancement is at risk of perpetuating epistemic injustice, and AI engineers’ understanding of knowledge as objective per se conceals this injustice. Finally, to get closer to trustworthy language models, we need to rethink knowledge in AI and aim for an agenda of diversification and scrutiny from outgroup members.},
	booktitle = {Proceedings of the 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Kraft, Angelie and Soulier, Eloïse},
	year = {2024},
	note = {event-place: Rio de Janeiro, Brazil},
	keywords = {knowledge graphs, natural language processing, language models, epistemology, representation, bias, fairness, feminism, knowledge enhancement},
	pages = {1433--1445},
}

@inproceedings{yang_knowledge-enhanced_2025,
	address = {New York, NY, USA},
	series = {{ICAICE} '24},
	title = {Knowledge-{Enhanced} {Large} {Language} {Model}-{Based} {Assistance} {Training} {System} for {Subway} {Maintenance} {Personnel}},
	isbn = {979-8-4007-1800-7},
	url = {https://doi.org/10.1145/3716895.3716900},
	doi = {10.1145/3716895.3716900},
	abstract = {To address the various challenges faced in training urban rail transit system maintenance personnel, this paper proposes a solution for developing a training system for subway maintenance personnel using knowledge graphs and a Retrieval-Augmented Generation (RAG)-enhanced large language model. The approach involves first creating a fine-tuning dataset from subway maintenance technical documents to fine-tune the large language model. This fine-tuned model then assists in constructing a subway maintenance knowledge graph. Concurrently, a vector database of subway maintenance knowledge is established. Finally, a question-answering system leveraging both the knowledge graph and the vector database as external knowledge sources is developed to support the training of subway maintenance personnel. Results demonstrate that this system can effectively enhance the learning efficiency of maintenance staff.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Da and Wang, Hongbo and Shao, Shanzhong and Liu, Shutian},
	year = {2025},
	keywords = {Retrieval-Augmented Generation (RAG), knowledge graph (KG), large language model (LLM), subway maintenance},
	pages = {25--29},
}

@inproceedings{custode_investigation_2024,
	address = {New York, NY, USA},
	series = {{GECCO} '24 {Companion}},
	title = {An investigation on the use of {Large} {Language} {Models} for hyperparameter tuning in {Evolutionary} {Algorithms}},
	isbn = {979-8-4007-0495-6},
	url = {https://doi.org/10.1145/3638530.3664163},
	doi = {10.1145/3638530.3664163},
	abstract = {Hyperparameter optimization is a crucial problem in Evolutionary Computation. In fact, the values of the hyperparameters directly impact the trajectory taken by the optimization process, and their choice requires extensive reasoning by human operators. Although a variety of self-adaptive Evolutionary Algorithms have been proposed in the literature, no definitive solution has been found. In this work, we perform a preliminary investigation to automate the reasoning process that leads to the choice of hyperparameter values. We employ two open-source Large Language Models (LLMs), namely Llama2-70b and Mixtral, to analyze the optimization logs online and provide novel real-time hyperparameter recommendations. We study our approach in the context of step-size adaptation for (1 + 1)-ES. The results suggest that LLMs can be an effective method for optimizing hyperparameters in Evolution Strategies, encouraging further research in this direction.},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Custode, Leonardo Lucio and Caraffini, Fabio and Yaman, Anil and Iacca, Giovanni},
	year = {2024},
	note = {event-place: Melbourne, VIC, Australia},
	keywords = {large language models, evolutionary algorithms, landscape analysis, parameter tuning},
	pages = {1838--1845},
}

@article{liu_magneto_2025,
	title = {Magneto: {Combining} {Small} and {Large} {Language} {Models} for {Schema} {Matching}},
	volume = {18},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3742728.3742757},
	doi = {10.14778/3742728.3742757},
	abstract = {Recent advances in language models (LMs) open new opportunities for schema matching (SM). Recent approaches have shown their potential and key limitations: while small LMs (SLMs) require costly, difficult-to-obtain training data, large LMs (LLMs) demand significant computational resources and face context window constraints. We present Magneto, a cost-effective and accurate solution for SM that combines the advantages of SLMs and LLMs to address their limitations. By structuring the SM pipeline in two phases, retrieval and reranking, Magneto can use computationally efficient SLM-based strategies to derive candidate matches which can then be reranked by LLMs, thus making it possible to reduce runtime while improving matching accuracy. We propose (1) a self-supervised approach to fine-tune SLMs which uses LLMs to generate syntactically diverse training data, and (2) prompting strategies that are effective for reranking. We also introduce a new benchmark, developed in collaboration with domain experts, which includes real biomedical datasets and presents new challenges for SM methods. Through a detailed experimental evaluation, using both our new and existing benchmarks, we show that Magneto is scalable and attains high accuracy for datasets from different domains.},
	number = {8},
	journal = {Proc. VLDB Endow.},
	author = {Liu, Yurong and Pena, Eduardo H. M. and Santos, Aécio and Wu, Eden and Freire, Juliana},
	month = sep,
	year = {2025},
	note = {Publisher: VLDB Endowment},
	pages = {2681--2694},
}

@inproceedings{zhang_multi-layer_2024,
	address = {New York, NY, USA},
	series = {{SIGIR} '24},
	title = {Multi-{Layer} {Ranking} with {Large} {Language} {Models} for {News} {Source} {Recommendation}},
	isbn = {979-8-4007-0431-4},
	url = {https://doi.org/10.1145/3626772.3657966},
	doi = {10.1145/3626772.3657966},
	abstract = {To seek reliable information sources for news events, we introduce a novel task of expert recommendation, which aims to identify trustworthy sources based on their previously quoted statements. To achieve this, we built a novel dataset, called NewsQuote, consisting of 23,571 quote-speaker pairs sourced from a collection of news articles. We formulate the recommendation task as the retrieval of experts based on their likelihood of being associated with a given query. We also propose a multi-layer ranking framework employing Large Language Models to improve the recommendation performance. Our results show that employing an in-context learning based LLM ranker and a multi-layer ranking-based filter significantly improve both the predictive quality and behavioural quality of the recommender system.},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Wenjia and Gui, Lin and Procter, Rob and He, Yulan},
	year = {2024},
	note = {event-place: Washington DC, USA},
	keywords = {large language model, recommender system, in-context learning},
	pages = {2537--2542},
}

@inproceedings{afreen_edge_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {{EDGE}: {A} {Conversational} {Interface} driven by {Large} {Language} {Models} for {Educational} {Knowledge} {Graphs} {Exploration}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679231},
	doi = {10.1145/3627673.3679231},
	abstract = {As education adopts digital platforms, the vast amount of information from various sources, such as learning management systems and learning object repositories, presents challenges in navigation and elaboration. Traditional interfaces involve a steep learning curve, limited user accessibility, and lack flexibility. Language models alone cannot address these issues as they do not have access to structured information specific to the educational organization. In this paper, we propose EDGE (EDucational knowledge Graph Explorer), a natural language interface that uses knowledge graphs to organize educational information. EDGE translates natural language requests into queries and converts the results back into natural language responses. We show EDGE's versatility using knowledge graphs built from public datasets, providing example interactions of different stakeholders. Demo video: https://u.garr.it/eYq63.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Afreen, Neda and Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Malloci, Francesca Maridina and Marras, Mirko and Martis, Andrea Giovanni},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {information retrieval, knowledge graph, graph database, language model, conversational interface, learning management},
	pages = {5159--5163},
}

@inproceedings{walker_promise_2024,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '24},
	title = {The {Promise} and {Challenge} of {Large} {Language} {Models} for {Knowledge} {Engineering}: {Insights} from a {Hackathon}},
	isbn = {979-8-4007-0331-7},
	url = {https://doi.org/10.1145/3613905.3650844},
	doi = {10.1145/3613905.3650844},
	abstract = {Knowledge engineering (KE) is the process of building, maintaining and using knowledge-based systems. This recently takes the form of knowledge graphs (KGs). The advent of new technologies like Large Language Models (LLMs) has the potential to improve automation in KE work due to the richness of their training data and their performance at solving natural language processing tasks. We conducted a multiple-methods study exploring user opinions and needs regarding the use of LLMs in KE. We used ethnographic techniques to observe KE workers using LLMs to solve KE tasks during a hackathon, followed by interviews with some of the participants. This interim study found that despite LLMs’ promising capabilities for efficient knowledge acquisition and requirements elicitation, their effective deployment requires an extended set of capabilities and training, particularly in prompting and understanding data. LLMs can be useful for simple quality assessment tasks, but in complex scenarios, the output is hard to control and evaluation may require novel approaches. With this study, we aim to evidence the interaction of KE stakeholders with LLMs, identify areas of potential, and understand the barriers to their effective use. We find copilot approaches may be valuable in developing processes where the human or a team of humans is assisted by generative AI.},
	booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Walker, Johanna and Koutsiana, Elisavet and Nwachukwu, Michelle and Meroño Peñuela, Albert and Simperl, Elena},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Large Language Models, Knowledge Graph, Knowledge Engineering, Interviews},
}

@inproceedings{xie_promptlink_2024,
	address = {New York, NY, USA},
	series = {{SIGIR} '24},
	title = {{PromptLink}: {Leveraging} {Large} {Language} {Models} for {Cross}-{Source} {Biomedical} {Concept} {Linking}},
	isbn = {979-8-4007-0431-4},
	url = {https://doi.org/10.1145/3626772.3657904},
	doi = {10.1145/3626772.3657904},
	abstract = {Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses, but it is challenging due to the discrepancies in concept naming conventions. Various strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models. However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples. Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities. However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions. In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs. Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink. Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources. The source code of this study is available at https://github.com/constantjxyz/PromptLink.},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Xie, Yuzhang and Lu, Jiaying and Ho, Joyce and Nahab, Fadi and Hu, Xiao and Yang, Carl},
	year = {2024},
	note = {event-place: Washington DC, USA},
	keywords = {biomedical concept linking, few-shot prompting, large language models for resource-constrained field, re-rank, retrieve \&amp},
	pages = {2589--2593},
}

@inproceedings{zhu_automated_2025,
	address = {New York, NY, USA},
	series = {{CECCT} '24},
	title = {Automated {Framework} for {Constructing} {Knowledge} {Graphs} {Oriented} for {Standard} {Analysis} {Using} {Large} {Language} {Models}},
	isbn = {979-8-4007-1019-3},
	url = {https://doi.org/10.1145/3705754.3705790},
	doi = {10.1145/3705754.3705790},
	abstract = {In an era characterized by rapid technological growth and digital transformation, the necessity for efficient and structured knowledge representation has grown more crucial. Standards serve as fundamental cornerstones that offer guidelines, specifications, and frameworks to ensure the quality and interoperability of products, services, as well as systems. Nonetheless, the complexity and extensive nature of standard documents present significant challenges in extraction, alignment, and organization. Traditional manual processing methods frequently prove labor-intensive and susceptible to errors, impeding the capturing of intricate relationships and hierarchies within these standards. Knowledge Graphs (KGs) provide a robust methodology for organizing information, facilitating improved functionalities for advanced search, reasoning, and analytics. Despite their potential, structuring KGs from standard documents continues to be challenging because of unstructured text, domain-specific terminology, and the intricacies in accurate information extraction. Recent advancements in Natural Language Processing (NLP), particularly the emergence of Large Language Models (LLMs) like GPT-3, have opened new avenues for automating the extraction and structuring of information from unstructured content. These models exhibit exceptional proficiency in comprehending and producing human-like text, positioning them as feasible solutions for addressing the complexities associated with standard documents. This paper presents an automated framework named StandardKG Builder, which utilizes LLMs to construct knowledge graphs tailored for standard analysis from multiple perspectives for complex information extraction. Our evaluation on a comprehensive dataset of standard documents highlights the framework’s effectiveness and scalability. By merging sophisticated knowledge representation with advanced NLP techniques, our work significantly enhances the accessibility and analysis of standard documents, paving the way for more efficient and intelligent standard management systems.},
	booktitle = {Proceedings of the 2024 2nd {International} {Conference} on {Electronics}, {Computers} and {Communication} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Sitong and Xia, Baobing and Duan, Fangwei and Zhao, Zhenyang and Zhao, Zhenxia and Xiao, Teliang and Liu, Zhicheng and Liu, Xia},
	year = {2025},
	keywords = {Large Language Models, Knowledge Graph, Standard Analysis},
	pages = {183--189},
}

@inproceedings{hu_lightllm_2025,
	address = {New York, NY, USA},
	series = {{SenSys} '25},
	title = {{LightLLM}: {A} {Versatile} {Large} {Language} {Model} for {Predictive} {Light} {Sensing}},
	isbn = {979-8-4007-1479-5},
	url = {https://doi.org/10.1145/3715014.3722067},
	doi = {10.1145/3715014.3722067},
	abstract = {We propose LightLLM, a model that fine tunes pre-trained large language models (LLMs) for light-based sensing tasks. It integrates a sensor data encoder to extract key features, a contextual prompt to provide environmental information, and a fusion layer to combine these inputs into a unified representation. This combined input is then processed by the pre-trained LLM, which remains frozen while being fine-tuned through the addition of lightweight, trainable components, allowing the model to adapt to new tasks without altering its original parameters. This approach enables flexible adaptation of LLM to specialized light sensing tasks with minimal computational overhead and retraining effort. We have implemented LightLLM for three light sensing tasks: light-based localization, outdoor solar forecasting, and indoor solar estimation. Using real-world experimental datasets, we demonstrate that LightLLM significantly outperforms state-of-the-art methods, achieving 4.4x improvement in localization accuracy and 3.4x improvement in indoor solar estimation when tested in previously unseen environments. We further demonstrate that LightLLM outperforms ChatGPT-4 with direct prompting, highlighting the advantages of LightLLM's specialized architecture for sensor data fusion with textual prompts.},
	booktitle = {Proceedings of the 23rd {ACM} {Conference} on {Embedded} {Networked} {Sensor} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Jiawei and Jia, Hong and Hassan, Mahbub and Yao, Lina and Kusy, Brano and Hu, Wen},
	year = {2025},
	note = {event-place: UC Irvine Student Center., Irvine, CA, USA},
	pages = {158--171},
}

@inproceedings{poria_understanding_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Understanding, {Leveraging}, and {Improving} {Large} {Language} {Models}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3653009},
	doi = {10.1145/3589335.3653009},
	abstract = {The emergence of Large Language Models (LLMs) has marked a substantial advancement in Natural Language Processing (NLP), contributing significantly to enhanced task performance both within and outside specific domains. However, amidst these achievements, three key questions remain unanswered: 1) The mechanism through which LLMs accomplish their tasks and their limitations, 2) Effectively harnessing the power of LLMs across diverse domains, and 3) Strategies for enhancing the performance of LLMs. This talk aims to delve into our research group's endeavors to address these pivotal questions. Firstly, I will outline our approach, which involves utilizing ontology-guided prompt perturbations to unravel the primary limitations of LLMs in solving mathematical problems. Moving on to the second question, we will explore the utilization of synthetic data generated by LLMs to bolster challenging downstream tasks, particularly focusing on structured prediction where LLMs face persistent challenges. I will elaborate on our initiatives aimed at improving LLMs by incorporating highly effective retrieval strategies, specifically addressing the prevalent challenge of hallucinations that often plagues contemporary LLMs. Finally, I will present a technique on LLM realignment to restore safety lost during fine-tuning.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Poria, Soujanya},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {keynote},
	pages = {1805},
}

@inproceedings{oelen_leveraging_2024,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '24},
	title = {Leveraging {Large} {Language} {Models} for {Realizing} {Truly} {Intelligent} {User} {Interfaces}},
	isbn = {979-8-4007-0331-7},
	url = {https://doi.org/10.1145/3613905.3650949},
	doi = {10.1145/3613905.3650949},
	abstract = {The number of published scholarly articles is growing at a significant rate, making scholarly knowledge organization increasingly important. Various approaches have been proposed to organize scholarly information, including describing scholarly knowledge semantically leveraging knowledge graphs. Transforming unstructured knowledge, presented within articles, to structured and semantically represented knowledge generally requires human intelligence and labor since natural language processing methods alone typically do not render sufficient precision and recall for many applications. With the recent developments of Large Language Models (LLMs), it becomes increasingly possible to provide truly intelligent user interfaces guiding humans in the transformation process. We present an approach to integrate non-intrusive LLMs guidance into existing user interfaces. More specifically, we integrate LLM-supported user interface components into an existing scholarly knowledge infrastructure. Additionally, we provide our experiences with LLM integration, detailing best practices and obstacles. Finally, we evaluate the approach using a small-scale user evaluation with domain experts.},
	booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Oelen, Allard and Auer, Sören},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Intelligent User Interface, LLM Interface, Scholarly Knowledge Graphs},
}

@article{bui_systematic_2024,
	title = {A {Systematic} {Comparison} of {Large} {Language} {Models} {Performance} for {Intrusion} {Detection}},
	volume = {2},
	url = {https://doi.org/10.1145/3696379},
	doi = {10.1145/3696379},
	abstract = {We explore the capabilities of Large Language Models (LLMs) to assist or substitute devices (i.e., firewalls) and humans (i.e., security experts) respectively in the detection and analysis of security incidents. We leverage transformer-based technologies, from relatively small to foundational sizes, to address the problem of correctly identifying the attack severity (and accessorily identifying and explaining the attack type). We contrast a broad range of LLM techniques (prompting, retrieval augmented generation, and fine-tuning of several models) using state-of-the-art machine learning models as a baseline. Using proprietary data from commercial deployment, our study provides an unbiased picture of the strengths and weaknesses of LLM for intrusion detection.},
	number = {CoNEXT4},
	journal = {Proc. ACM Netw.},
	author = {Bui, Minh-Thanh and Boffa, Matteo and Valentim, Rodolfo Vieira and Navarro, Jose Manuel and Chen, Fuxing and Bao, Xiaosheng and Houidi, Zied Ben and Rossi, Dario},
	month = nov,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {machine learning, natural language processing, computing methodologies, firewalls, intrusion detection systems, security and privacy},
}

@inproceedings{alam_workshop_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Workshop on {Deep} {Learning} and {Large} {Language} {Models} for {Knowledge} {Graphs} ({DL4KG})},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671491},
	doi = {10.1145/3637528.3671491},
	abstract = {The use of Knowledge Graphs (KGs) which constitute large networks of real-world entities and their interrelationships, has grown rapidly. A substantial body of research has emerged, exploring the integration of deep learning (DL) and large language models (LLMs) with KGs. This workshop aims to bring together leading researchers in the field to discuss and foster collaborations on the intersection of KG and DL/LLMs.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Alam, Mehwish and Buscaldi, Davide and Cochez, Michael and Gesese, Genet Asefa and Osborne, Francesco and Reforgiato Recupero, Diego},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {artificial intelligence, knowledge graphs, large language models, deep learning},
	pages = {6704--6705},
}

@article{jain_ontology-based_2024,
	title = {Ontology-{Based} {Natural} {Language} {Processing} for {Sentimental} {Knowledge} {Analysis} {Using} {Deep} {Learning} {Architectures}},
	volume = {23},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3624012},
	doi = {10.1145/3624012},
	abstract = {When tested with popular datasets, sentiment categorization using deep learning (DL) algorithms will produce positive results. Building a corpus on novel themes to train machine learning methods in sentiment classification with high assurance, however, will be difficult. This study proposes a way for representing efficient features of a dataset into a word embedding layer of DL methods in sentiment classification known as KPRO (knowledge processing and representation based on ontology), a procedure to embed knowledge in the ontology of opinion datasets. This research proposes novel methods in ontology-based natural language processing utilizing feature extraction as well as classification by a DL technique. Here, input text has been taken as web ontology based text and is processed for word embedding. Then the feature mapping is carried out for this processed text using least square mapping in which the sentiment-based text has been mapped for feature extraction. The feature extraction is carried out using a Markov model based auto-feature encoder (MarMod\_AuFeaEnCod). Extracted features are classified by utilizing hierarchical convolutional attention networks. Based on this classified output, the sentiment of the text obtained from web data has been analyzed. Results are carried out for Twitter and Facebook ontology-based sentimental analysis datasets in terms of accuracy, precision, recall, F-1 score, RMSE, and loss curve analysis. For the Twitter dataset, the proposed MarMod\_AuFeaEnCod\_HCAN attains an accuracy of 98\%, precision of 95\%, recall of 93\%, F-1 score of 91\%, RMSE of 88\%, and loss curve of 70.2\%. For Facebook, ontology web dataset analysis is also carried out with the same parameters in which the proposed MarMod\_AuFeaEnCod\_HCAN acquires accuracy of 96\%, precision of 92\%, recall of 94\%, F-1 score of 91\%, RMSE of 77\%, and loss curve of 68.2\%.},
	number = {1},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Jain, Deepak Kumar and Qamar, Shamimul and Sangwan, Saurabh Raj and Ding, Weiping and Kulkarni, Anand J.},
	month = jan,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology, NLP, deep learning, classification, feature extraction, KPRO},
}

@inproceedings{huang_optimizing_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {Optimizing {Numerical} {Estimation} and {Operational} {Efficiency} in the {Legal} {Domain} through {Large} {Language} {Models}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3680025},
	doi = {10.1145/3627673.3680025},
	abstract = {The legal landscape encompasses a wide array of lawsuit types, presenting lawyers with challenges in delivering timely and accurate information to clients, particularly concerning critical aspects like potential imprisonment duration or financial repercussions. Compounded by the scarcity of legal experts, there's an urgent need to enhance the efficiency of traditional legal workflows. Recent advances in deep learning, especially Large Language Models (LLMs), offer promising solutions to this challenge. Leveraging LLMs' mathematical reasoning capabilities, we propose a novel approach integrating LLM-based methodologies with specially designed prompts to address precision requirements in legal Artificial Intelligence (LegalAI) applications. The proposed work seeks to bridge the gap between traditional legal practices and modern technological advancements, paving the way for a more accessible, efficient, and equitable legal system. To validate this method, we introduce a curated dataset tailored to precision-oriented LegalAI tasks, serving as a benchmark for evaluating LLM-based approaches. Extensive experimentation confirms the efficacy of our methodology in generating accurate numerical estimates within the legal domain, emphasizing the role of LLMs in streamlining legal processes and meeting the evolving demands of LegalAI. Github: https://github.com/Jhhuangkay/Optimizing-Numerical-Estimation-and-Operational-Efficiency-in-the-Legal-Domain.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Jia-Hong and Yang, Chao-Chun and Shen, Yixian and Pacces, Alessio M. and Kanoulas, Evangelos},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {large language models, precision-oriented legal artificial intelligence, tailored prompt design},
	pages = {4554--4562},
}

@article{jin_integrating_2025,
	title = {Integrating {AI} {Planning} with {Natural} {Language} {Processing}:\&nbsp;{A}\&nbsp;{Combination} of {Explicit} and {Tacit} {Knowledge}},
	volume = {16},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3729236},
	doi = {10.1145/3729236},
	abstract = {Natural language processing (NLP) aims at investigating the interactions between agents and humans, which processes and analyzes large amounts of natural language data. Large-scale language models play an important role in current NLP. However, the challenges of explainability and complexity come along with the development of language models. One way is to introduce logical relations and rules into NLP models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to those two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and NLP effectively improves the communication between human and intelligent agents. This article outlines the commons and relations between AI planning and NLP, and it argues that each of them can effectively impact the other one in six areas: (1) planning-based text understanding, (2) planning-based NLP, (3) text-based human–robot interaction, (4) planning-based explainability, (5) evaluation metrics, and (6) applications. We also explore some potential future issues between AI planning and NLP. To the best of our knowledge, this survey is the first that addresses the deep connections between AI planning and NLP.},
	number = {4},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Jin, Kebing and Zhuo, Hankz Hankui},
	month = aug,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural language processing, Explainability, Natural language understanding, AI planning, Human-robot interaction},
}

@inproceedings{biancini_multiple-choice_2024,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '24},
	title = {Multiple-{Choice} {Question} {Generation} {Using} {Large} {Language} {Models}: {Methodology} and {Educator} {Insights}},
	isbn = {979-8-4007-0466-6},
	url = {https://doi.org/10.1145/3631700.3665233},
	doi = {10.1145/3631700.3665233},
	abstract = {Integrating Artificial Intelligence (AI) in educational settings has brought new learning approaches, transforming the practices of both students and educators. Among the various technologies driving this transformation, Large Language Models (LLMs) have emerged as powerful tools for creating educational materials and question answering, but there are still space for new applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess student knowledge, but manually generating these questions is resource-intensive and requires significant time and cognitive effort. In our opinion, LLMs offer a promising solution to these challenges. This paper presents a novel comparative analysis of three widely known LLMs - Llama 2, Mistral, and GPT-3.5 - to explore their potential for creating informative and challenging MCQs. In our approach, we do not rely on the knowledge of the LLM, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test’s source text, too. Our experiment involving 21 educators shows that GPT-3.5 generates the most effective MCQs across several known metrics. Additionally, it shows that there is still some reluctance to adopt AI in the educational field. This study sheds light on the potential of LLMs to generate MCQs and improve the educational experience, providing valuable insights for the future.},
	booktitle = {Adjunct {Proceedings} of the 32nd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Biancini, Giorgio and Ferrato, Alessio and Limongelli, Carla},
	year = {2024},
	note = {event-place: Cagliari, Italy},
	keywords = {Generative AI, LLMs, Multiple Choice Question},
	pages = {584--590},
}

@inproceedings{mao_research_2025,
	address = {New York, NY, USA},
	series = {{MLNN} '25},
	title = {Research on the {Construction} of {Machine} {Translation} {Model} of {Fuzzy} {Language} in {Ancient} {Chinese} {Medicine} {Books} for the {Transmission} of {Chinese} {Medicine} {Culture}},
	isbn = {979-8-4007-1438-2},
	url = {https://doi.org/10.1145/3747227.3747264},
	doi = {10.1145/3747227.3747264},
	abstract = {In order to improve the translation quality of fuzzy language in ancient Chinese medical texts, a hybrid translation model integrating deep learning and knowledge graph is constructed to perform context modeling and disambiguation optimization for the literary syntax, semantic ambiguities, and terminology of the ancient texts. The Bi-LSTM context modeling module is designed to enhance semantic understanding with knowledge graph, and a multi-strategy fusion translation decision mechanism is used to improve translation accuracy. The results show that the model performs well in the processing of ambiguities such as disease dynamics and treatment principles, improves terminological consistency and semantic retention, and provides effective support for cross-lingual communication of TCM culture.},
	booktitle = {Proceedings of the 2025 {International} {Conference} on {Machine} {Learning} and {Neural} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Mao, Huijuan},
	year = {2025},
	keywords = {machine translation, ancient Chinese medical books, fuzzy language, knowledge map},
	pages = {229--235},
}

@inproceedings{yang_scientometrics_2025,
	address = {New York, NY, USA},
	series = {{ICCSMT} '24},
	title = {A {Scientometrics} {Analysis} and {Visualization} of {Large} {Language} {Model} in {China}'s {Library}},
	isbn = {979-8-4007-0999-9},
	url = {https://doi.org/10.1145/3708036.3708272},
	doi = {10.1145/3708036.3708272},
	abstract = {Large Language Model has been researched in the field of library from the following aspects:space reproduction, service reform, library construction and so on. In order to clarify the current research situation of Large Language Model's application research in the field of library, and provide some reference for the further development of research fields related to Large Language Model empowering library in the future. This paper utilizes two methods of scientometrics and data visualization to analyze and study the journal papers on the application of Large Language Model in the field of Chinese libraries from the aspects of the degree of academic focus, the way of creating academic achievements and research topics of academic achievements, and puts forward the research practice of strengthening the application of Large Language Model in library from the aspects of ’Strengthen the practical research of Large Language Model empowering Chinese library’ and ‘Broaden the field of research related to Large Language Model empowering Chinese library’, in order to promote the all-round development of Large Language Model in the field of library.},
	booktitle = {Proceedings of the 2024 5th {International} {Conference} on {Computer} {Science} and {Management} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Guangyuan and Xie, Quanying and Chen, Lei},
	year = {2025},
	keywords = {Large Language Model, Data Visualization, Scientometrics, Chinese libraries, Library Service},
	pages = {1403--1407},
}

@article{comuzzi_language_2025,
	title = {A {Language} to {Model} and {Simulate} {Data} {Quality} {Issues} in {Process} {Mining}},
	volume = {17},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3743144},
	doi = {10.1145/3743144},
	abstract = {Real-life business process event logs may suffer from significant data quality problems negatively influencing process mining analysis. Over time, a range of approaches has been developed to detect and repair these quality problems. Validation of these approaches tends to be challenging due to the lack of a ground truth. Moreover, the identification and definition of event log quality problems have been tackled mainly through a pattern-based approach, with systematic and extensible methods currently lacking. In this article, we present FLAWD, a formal language for describing event log data quality issues that enables solutions addressing the shortcomings of process mining data quality research identified above. FLAWD can be used to formally describe and possibly reason over event log data quality errors, as well as to guide the development of tools for controlled and sophisticated “polluting” of event logs through which benchmark datasets may be systematically created. We present the abstract syntax grammar of FLAWD and an open-source software tool based on it that allows for the insertion of all so-called event log imperfection patterns in a stochastic manner. We show how FLAWD has been used in our research to generate benchmark datasets and how it can be used to formally describe and replicate a range of errors found in real-life event logs.},
	number = {2},
	journal = {J. Data and Information Quality},
	author = {Comuzzi, Marco and Ko, Jonghyeon and Maggi, Fabrizio},
	month = jun,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {data quality, Event log, abstract syntax grammar, error description, error injection, FLAWD},
}

@inproceedings{yang_research_2024,
	address = {New York, NY, USA},
	series = {{ISAIE} '24},
	title = {Research and {Practice} on the {Construction} of {Course} {Ideological} and {Political} {Education} {Based} on {Knowledge} {Graphs} and {Large} {Language} {Models}},
	isbn = {979-8-4007-0710-0},
	url = {https://doi.org/10.1145/3700297.3700331},
	doi = {10.1145/3700297.3700331},
	abstract = {Knowledge graphs and large language models (LLMs) have become important tools for educational innovation. This paper explores the application of these two technologies in the construction of ideological and political education in university courses. The paper begins by analyzing the importance of course-based ideological and political education and the challenges currently faced. It then introduces the role of knowledge graphs in integrating educational resources and constructing knowledge systems, as well as the potential and current status of LLMs in natural language processing and providing personalized educational content. This study presents a method that integrates the use of knowledge graphs and LLMs to construct resources and application systems for course-based ideological and political education. The results of practical case studies demonstrate that the proposed method improves the efficiency of constructing ideological and political education content, enhances the effectiveness of moral education within courses, and contributes to the innovative development of ideological and political education.},
	booktitle = {Proceedings of the 2024 {International} {Symposium} on {Artificial} {Intelligence} for {Education}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Da and Liu, Shutian and Fu, Haoyang and Shen, Jiayi},
	year = {2024},
	keywords = {Large Language Model (LLM), Knowledge Graph, Course Ideological and Political Education, Educational Innovation},
	pages = {193--198},
}

@inproceedings{huang_disambiguate_2024,
	address = {New York, NY, USA},
	series = {{GUIDE}-{AI} '24},
	title = {Disambiguate {Entity} {Matching} using {Large} {Language} {Models} through {Relation} {Discovery}},
	isbn = {979-8-4007-0694-3},
	url = {https://doi.org/10.1145/3665601.3669844},
	doi = {10.1145/3665601.3669844},
	abstract = {Entity matching is a critical problem in data integration, central to tasks like fuzzy joins for tuple enrichment. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT. However, when integrating with external databases, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a "match". This is because external databases contain tuples with varying levels of detail and granularity among entities, and an "exact match" in traditional entity matching rarely happens. As a result, understanding how entities are related and the potential nuances is critical, especially for high-stake tasks for responsible AI. In this work, we study a case problem of entity matching for ESG reporting. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the "relations" between entities for resolving ambiguities in matching, with a human-in-the-loop process to make the final decision. By pre-defining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities, and responsibly perform downstream tasks.},
	booktitle = {Proceedings of the {Conference} on {Governance}, {Understanding} and {Integration} of {Data} for {Effective} and {Responsible} {AI}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Zezhou},
	year = {2024},
	note = {event-place: Santiago, AA, Chile},
	keywords = {Large Language Models, Data Integration, Entity Matching},
	pages = {36--39},
}

@article{mountantonakis_generating_2025,
	title = {Generating {SPARQL} {Queries} over {CIDOC}-{CRM} {Using} a {Two}-{Stage} {Ontology} {Path} {Patterns} {Method} in {LLM} {Prompts}},
	volume = {18},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3708326},
	doi = {10.1145/3708326},
	abstract = {In this article, we focus on the task of exploiting the capabilities of Large Language Models (LLMs) to generate SPARQL Queries for answering natural questions over cultural Knowledge Graphs (KGs) expressed according to the ISO standard ontology CIDOC-CRM. Since CIDOC-CRM is an event-based model, usually we have to follow long paths for answering a question, thereby, the challenge is how to construct the prompt for aiding the LLM to produce the right SPARQL query. We propose and comparatively evaluate methods based on the creation of ontology path patterns of a configurable path radius (or length). Then, we construct a new dedicated benchmark that includes 100 natural questions and the corresponding SPARQL queries over two real KGs from the cultural domain describing artworks. Finally, we present comparative results about the effectiveness and efficiency over the benchmark by using ChatGPT-3.5. The most effective method follows a two-stage process that predicts and uses the most appropriate path patterns of (rleq 4) . This method achieves 3.5 (times) higher accuracy than the baseline method (0.66 versus 0.19), that includes in the prompt only the list of properties and classes of the KG.Benchmark:},
	number = {1},
	journal = {J. Comput. Cult. Herit.},
	author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, Ontology, LLM, Language model, Prompt Engineering, Query processing, Prompt engineering, Benchmarking, CIDOC-CRM, Question Answering, Cultural Heritage, CIDOC CRM, ISO Standards, Structured Query Language, Ontology's, Natural language processing systems, Query languages, Cultural heritages, Cultural knowledge, ISO standards},
	annote = {Cited by: 3},
}

@inproceedings{deng_k2_2024,
	address = {New York, NY, USA},
	series = {{WSDM} '24},
	title = {K2: {A} {Foundation} {Language} {Model} for {Geoscience} {Knowledge} {Understanding} and {Utilization}},
	isbn = {979-8-4007-0371-3},
	url = {https://doi.org/10.1145/3616855.3635772},
	doi = {10.1145/3616855.3635772},
	abstract = {Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization.We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2},
	booktitle = {Proceedings of the 17th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Deng, Cheng and Zhang, Tianhang and He, Zhongmou and Chen, Qiyuan and Shi, Yuanyuan and Xu, Yi and Fu, Luoyi and Zhang, Weinan and Wang, Xinbing and Zhou, Chenghu and Lin, Zhouhan and He, Junxian},
	year = {2024},
	note = {event-place: Merida, Mexico},
	keywords = {foundation model, geoscience knowledge mining, geoscience large language model},
	pages = {161--170},
}

@inproceedings{li_distilling_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {Distilling {Semantic} {Concept} {Embeddings} from {Contrastively} {Fine}-{Tuned} {Language} {Models}},
	isbn = {978-1-4503-9408-6},
	url = {https://doi.org/10.1145/3539618.3591667},
	doi = {10.1145/3539618.3591667},
	abstract = {Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence from the neighbourhood structure of the contextualised word embeddings. The second strategy instead relies on a distant supervision signal from ConceptNet. Our experimental results show that the resulting vectors substantially outperform existing concept embeddings in predicting the semantic properties of concepts, with the ConceptNet-based strategy achieving the best results. These findings are furthermore confirmed in a clustering task and in the downstream task of ontology completion.},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Li, Na and Kteich, Hanane and Bouraoui, Zied and Schockaert, Steven},
	year = {2023},
	note = {event-place: Taipei, Taiwan},
	keywords = {Language model, Semantics, language models, Embeddings, Word embedding, Contrastive learning, Computational linguistics, word embedding, ConceptNet, Commonsense knowledge, commonsense knowledge, contrastive learning, Vectors, Property, Down-stream, Word vectors, Semantic properties},
	pages = {216--226},
	annote = {Cited by: 5; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{zhai_recommendation_2025,
	address = {New York, NY, USA},
	series = {{BDICN} '25},
	title = {Recommendation {System} {Based} on {Heterogeneous} {Graph} {Neural} {Network} and {Large} {Language} {Model}},
	isbn = {979-8-4007-1242-5},
	url = {https://doi.org/10.1145/3727353.3727365},
	doi = {10.1145/3727353.3727365},
	abstract = {In order to break through technical problems and reduce R\&amp;D costs and risks, innovation subjects usually establish cooperative relationships based on common cooperation goals and technological similarities among them. Therefore, based on the analysis of the common cooperation goals of innovation subjects, this chapter proposes a partner identification model for emerging technological innovation, constructs a vector representation of patented technologies obtained by patent heterogeneous networks, and identifies potential partners in the technology field on the basis of technological similarity.},
	booktitle = {Proceedings of the 2025 4th {International} {Conference} on {Big} {Data}, {Information} and {Computer} {Network}},
	publisher = {Association for Computing Machinery},
	author = {Zhai, Dongsheng and Du, Ruize and Liang, Guoqiang},
	year = {2025},
	keywords = {Large Language Model, Graph Neural Network, Heterogeneous Patent Network, Potential Partner Identification},
	pages = {67--72},
}

@inproceedings{antoniou_utilizing_2025,
	address = {New York, NY, USA},
	series = {{PCI} '24},
	title = {Utilizing {LLMs} and ontologies to query educational knowledge graphs},
	isbn = {979-8-4007-1317-0},
	url = {https://doi.org/10.1145/3716554.3716598},
	doi = {10.1145/3716554.3716598},
	abstract = {Knowledge Graphs (KGs) provide knowledge and data in a structured format with content from various fields. But the access to the knowledge graphs is done by experienced users, that is, users who know the syntax of the SPARQL language and the KG vocabulary (either in RDF Schema or in OWL) in order to be able to ask questions to exploit the knowledge graphs. However, this requires a lot of time and effort for most of the users, which makes KGs inaccessible to a large number of users. Large Language Models (LLMs) that have appeared recently can provide an alternative way to query knowledge graphs without the need to learn SPARQL and/or know the schema and vocabulary of them, eliminating the time and effort that ordinary users need to spend in order to use them. In this article, we present some experiments and their results illustrating how ChatGPT can help ordinary users to generate SPARQL queries, without knowing SPARQL, to effectively use knowledge graphs and exploit their wealth of data. We experimented with ChatGPT to explore whether it can generate SPARQL queries based on user's natural language input and a given vocabulary (ontology) about an educational knowledge graph. To this end we have devised a specific prompt template. Results indicate that LLMs can indeed help in this direction, given the fact that they are prompted properly, using good English language. We have also discussed some practical lessons learned through this experiment.},
	booktitle = {Proceedings of the 28th {Pan}-{Hellenic} {Conference} on {Progress} in {Computing} and {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Antoniou, Christina and Bassiliades, Nick},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {ChatGPT, Knowledge graphs, Knowledge graph, Language model, knowledge graphs, RDF, Query processing, Computational linguistics, AI applications, AI application, large language model use cases, Structured Query Language, Graphic methods, Ontology's, Natural language processing systems, Query languages, RDF schemas, Educational knowledge, Large language model use case, Model use},
	pages = {287--295},
	annote = {Cited by: 0},
}

@inproceedings{fiori_using_2024,
	address = {New York, NY, USA},
	series = {{UbiComp} '24},
	title = {Using {Large} {Language} {Models} to {Compare} {Explainable} {Models} for {Smart} {Home} {Human} {Activity} {Recognition}},
	isbn = {979-8-4007-1058-2},
	url = {https://doi.org/10.1145/3675094.3679000},
	doi = {10.1145/3675094.3679000},
	abstract = {Recognizing daily activities with unobtrusive sensors in smart environments enables various healthcare applications. Monitoring how subjects perform activities at home and their changes over time can reveal early symptoms of health issues, such as cognitive decline. Most approaches in this field use deep learning models, which are often seen as black boxes mapping sensor data to activities. However, non-expert users like clinicians need to trust and understand these models' outputs. Thus, eXplainable AI (XAI) methods for Human Activity Recognition have emerged to provide intuitive natural language explanations from these models. Different XAI methods generate different explanations, and their effectiveness is typically evaluated through user surveys, that are often challenging in terms of costs and fairness. This paper proposes an automatic evaluation method using Large Language Models (LLMs) to identify, in a pool of candidates, the best XAI approach for non-expert users. Our preliminary results suggest that LLM evaluation aligns with user surveys.},
	booktitle = {Companion of the 2024 on {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Fiori, Michele and Civitarese, Gabriele and Bettini, Claudio},
	year = {2024},
	note = {event-place: Melbourne VIC, Australia},
	keywords = {evaluation, human activity recognition, llms, xai},
	pages = {881--884},
}

@inproceedings{afreen_explainable_2024,
	address = {New York, NY, USA},
	series = {{RecSys} '24},
	title = {Explainable and {Faithful} {Educational} {Recommendations} through {Causal} {Language} {Modelling} via {Knowledge} {Graphs}},
	isbn = {979-8-4007-0505-2},
	url = {https://doi.org/10.1145/3640457.3688022},
	doi = {10.1145/3640457.3688022},
	abstract = {The rapid expansion of digital education has significantly increased the need for recommender systems to help learners navigate the extensive variety of available learning resources. Recent advancements in these systems have notably improved the personalization of course recommendations. However, many existing systems fail to provide clear explanations for their recommendations, making it difficult for learners to understand why a particular suggestion was made. Researchers have emphasized the importance of explanations in various domains such as e-commerce, media, and entertainment, demonstrating how explanations can enhance system transparency, foster user trust, and improve decision-making processes. Despite these insights, such approaches have been rarely applied to the educational domain, and their effectiveness in practical use remains largely unexamined. My research focuses on developing explainable recommender systems for digital education. First, I aim to design knowledge graphs that can support high-quality recommendations in the educational context. Second, I will create models backed by these knowledge graphs that not only deliver accurate recommendations but also provide faithful explanations for each suggestion. Third, I will evaluate the effectiveness of these explainable recommender systems in real-world educational environments. Ultimately, this research aims to advance the development of more transparent and user-centric educational technologies.},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Afreen, Neda},
	year = {2024},
	note = {event-place: Bari, Italy},
	keywords = {Knowledge Graph, Personalization, Explainability, Recommendation, Recommender Systems, Transparency, Language Modeling},
	pages = {1358--1360},
}

@inproceedings{zhao_multi-grained_2024,
	address = {New York, NY, USA},
	series = {{MM} '24},
	title = {Multi-grained {Correspondence} {Learning} of {Audio}-language {Models} for {Few}-shot {Audio} {Recognition}},
	isbn = {979-8-4007-0686-8},
	url = {https://doi.org/10.1145/3664647.3681389},
	doi = {10.1145/3664647.3681389},
	abstract = {Large-scale pre-trained audio-language models excel in general multi-modal representation, facilitating their adaptation to downstream audio recognition tasks in a data-efficient manner. However, existing few-shot audio recognition methods based on audio-language models primarily focus on learning coarse-grained correlations, which are not sufficient to capture the intricate matching patterns between the multi-level information of audio and the diverse characteristics of category concepts. To address this gap, we propose multi-grained correspondence learning for bootstrapping audio-language models to improve audio recognition with few training samples. This approach leverages generative models to enrich multi-modal representation learning, mining the multi-level information of audio alongside the diverse characteristics of category concepts. Multi-grained matching patterns are then established through multi-grained key-value cache and multi-grained cross-modal contrast, enhancing the alignment between audio and category concepts. Additionally, we incorporate optimal transport to tackle temporal misalignment and semantic intersection issues in fine-grained correspondence learning, enabling flexible fine-grained matching. Our method achieves state-of-the-art results on multiple benchmark datasets for few-shot audio recognition, with comprehensive ablation experiments validating its effectiveness.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Shengwei and Xu, Linhai and Liu, Yuying and Du, Shaoyi},
	year = {2024},
	note = {event-place: Melbourne VIC, Australia},
	keywords = {audio-language models, few-shot audio recognition, multi-grained correspondence learning, optimal transport},
	pages = {9244--9252},
}

@article{sun_are_2024,
	title = {Are {Large} {Language} {Models} a {Good} {Replacement} of {Taxonomies}?},
	volume = {17},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3681954.3681973},
	doi = {10.14778/3681954.3681973},
	abstract = {Large language models (LLMs) demonstrate an impressive ability to internalize knowledge and answer natural language questions. Although previous studies validate that LLMs perform well on general knowledge while presenting poor performance on long-tail nuanced knowledge, the community is still doubtful about whether the traditional knowledge graphs should be replaced by LLMs. In this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies and at taxonomy levels that are common to people. Unfortunately, there lacks a comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies from common to specialized domains and at levels from root to leaf so that we can draw a confident conclusion. To narrow the research gap, we constructed a novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten representative taxonomies from common to specialized domains with in-depth experiments of different levels of entities in this taxonomy from root to leaf. Our comprehensive experiments of eighteen LLMs under three prompting settings validate that LLMs perform miserably poorly in handling specialized taxonomies and leaf-level entities. Specifically, the QA accuracy of the best LLM drops by up to 30\% as we go from common to specialized domains and from root to leaf levels of taxonomies.},
	number = {11},
	journal = {Proc. VLDB Endow.},
	author = {Sun, Yushi and Xin, Hao and Sun, Kai and Xu, Yifan Ethan and Yang, Xiao and Dong, Xin Luna and Tang, Nan and Chen, Lei},
	month = jul,
	year = {2024},
	note = {Publisher: VLDB Endowment},
	pages = {2919--2932},
}

@inproceedings{barbon_junior_are_2024,
	address = {New York, NY, USA},
	series = {{BiDEDE} '24},
	title = {Are {Large} {Language} {Models} the {New} {Interface} for {Data} {Pipelines}?},
	isbn = {979-8-4007-0679-0},
	url = {https://doi.org/10.1145/3663741.3664785},
	doi = {10.1145/3663741.3664785},
	abstract = {A Language Model is a term that encompasses various types of models designed to understand and generate human communication. Large Language Models (LLMs) have gained significant attention due to their ability to process text with human-like fluency and coherence, making them valuable for a wide range of data-related tasks fashioned as pipelines. The capabilities of LLMs in natural language understanding and generation, combined with their scalability, versatility, and state-of-the-art performance, enable innovative applications across various AI-related fields, including eXplainable Artificial Intelligence (XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG). Furthermore, we believe these models can extract valuable insights and make data-driven decisions at scale, a practice commonly referred to as Big Data Analytics (BDA). In this position paper, we provide some discussions in the direction of unlocking synergies among these technologies, which can lead to more powerful and intelligent AI solutions, driving improvements in data pipelines across a wide range of applications and domains integrating humans, computers, and knowledge.},
	booktitle = {Proceedings of the {International} {Workshop} on {Big} {Data} in {Emergent} {Distributed} {Environments}},
	publisher = {Association for Computing Machinery},
	author = {Barbon Junior, Sylvio and Ceravolo, Paolo and Groppe, Sven and Jarrar, Mustafa and Maghool, Samira and Sèdes, Florence and Sahri, Soror and Van Keulen, Maurice},
	year = {2024},
	note = {event-place: Santiago, AA, Chile},
	keywords = {Knowledge Graphs, Natural Language Understanding, Automated Machine Learning, Big Data Analytic, eXplainable Artificial Intelligence, Human-Computer Interaction},
}

@inproceedings{grigis_playwriting_2024,
	address = {New York, NY, USA},
	series = {{AVI} '24},
	title = {Playwriting with {Large} {Language} {Models}: {Perceived} {Features}, {Interaction} {Strategies} and {Outcomes}},
	isbn = {979-8-4007-1764-2},
	url = {https://doi.org/10.1145/3656650.3656688},
	doi = {10.1145/3656650.3656688},
	abstract = {Large Language Models (LLMs) are sparking debates about creativity, intellectual property, and artistic integrity. This paper focuses on creativity, defined as consensual agreement among domain experts. It presents an inductive analysis of seven semi-structured interviews with professional playwrights who engaged in a longitudinal project with the aim of writing a theatre script using commercial systems. Overall, participants regarded LLMs as unsuitable for playwrighting. However, they enjoyed the experience and identified utility for editorial tasks and brainstorming. A significant obstacle was associated with the politics embedded in LLMs. Not only did these systems avoid a language that could offend sensibilities, but they also refused to engage in taboos and conflicts, which are the core of dramaturgy. Other system features (speed, exploitation, and unpredictability) were sometimes considered conducive and sometimes detrimental to creativity. Participants experienced difficulties and tried to build common ground by trial and error. Often, this strategy evolved into role play: the playwright instructed the LLM to enact characters. The interaction provided hints of inspiration and fostered suspension of disbelief and ontological reflection. However, it often led to technology rejection. Comparing and contrasting our insights with related work, we conclude by opening new directions for research at the boundaries of HCI and AI.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Advanced} {Visual} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Grigis, Paolo and De Angeli, Antonella},
	year = {2024},
	note = {event-place: Arenzano, Genoa, Italy},
	keywords = {Language model, Creativity, Computational linguistics, Creative AI, Roleplay, Suspension of Disbelief, Theatre, Unpredictability, Creatives, Domain experts, Feature interactions, Interaction strategy, Role-plays, Suspension of disbelief},
	annote = {Cited by: 6; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{vakaj_4th_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {4th {International} {Workshop} on {Natural} {Language} {Processing} for {Knowledge} {Graph} {Construction}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3717822},
	doi = {10.1145/3701716.3717822},
	abstract = {Knowledge graphs (KG) are becoming increasingly popular, at the heart of Gartner's emerging tech impact radar, especially as a complementary theme for addressing the challenges of recent advances in natural language processing (NLP) with large language models related to responsible AI such as fairness, transparency, accountability, and explainability. Sir Tim Berners-Lee's seminal work ”Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web”, envisioned a World Wide Web where information is not only accessible but structured, allowing machines to interpret data meaningfully. This vision laid the groundwork for technologies such as RDF (Resource Description Framework) and OWL (Web Ontology Language), which serve as foundational components for modern KGs.However, the process of building domain-specific KGs from extensive text corpora is highly complex and resource-intensive, requiring careful task design for entity recognition, disambiguation, and relationship extraction, among others. These tasks are essential to ensure accuracy and relevance in knowledge representation, but they pose considerable challenges. Addressing these complexities is crucial for the continued advancement and application of KGs across domains.In this context, the 4th NLP4KGC workshop is held to create a collaborative platform for researchers, practitioners, and industry experts in NLP and KG construction. Following the success and growing community engagement in the previous three editions, this year's workshop aims to deepen collaboration and encourage innovative solutions in this rapidly evolving field. The 4th NLP4KGC will continue to bridge academia and industry, fostering the exchange of insights, tools, and methodologies at the intersection of NLP and KG development. The 4th NLP4KGC will consist of five accepted papers and three keynotes from distinguished speakers.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Vakaj, Edlira and Mihindukulasooriya, Nandana and Tiwari, Sanju and Rodriguez-Méndez, Sergio J.},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, Ontology, Language model, large language models, Construction, semantic web, knowledge graph, Web of data, Responsible AI, Natural language understanding, Resource Description Framework (RDF), natural language understanding, Language processing, responsible ai, web of data, Graph construction, Natural languages, Natural language processing systems, Human engineering, Semantic-Web, World Wide Web},
	pages = {2545--2548},
	annote = {Cited by: 0},
}

@inproceedings{hensel_large_2023,
	address = {New York, NY, USA},
	series = {{ICMI} '23},
	title = {Large language models in textual analysis for gesture selection},
	isbn = {979-8-4007-0055-2},
	url = {https://doi.org/10.1145/3577190.3614158},
	doi = {10.1145/3577190.3614158},
	abstract = {Gestures perform a variety of communicative functions that powerfully influence human face-to-face interaction. How this communicative function is achieved varies greatly between individuals and depends on the role of the speaker and the context of the interaction. Approaches to automatic gesture generation vary not only in the degree to which they rely on data-driven techniques but also the degree to which they can produce context and speaker specific gestures. However, these approaches face two major challenges: The first is obtaining sufficient training data that is appropriate for the context and the goal of the application. The second is related to designer control to realize their specific intent for the application. Here, we approach these challenges by using large language models (LLMs) to show that these powerful models of large amounts of data can be adapted for gesture analysis and generation. Specifically, we used ChatGPT as a tool for suggesting context-specific gestures that can realize designer intent based on minimal prompts. We also find that ChatGPT can suggests novel yet appropriate gestures not present in the minimal training data. The use of LLMs is a promising avenue for gesture generation that reduce the need for laborious annotations and has the potential to flexibly and quickly adapt to different designer intents.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Hensel, Laura Birka and Yongsatianchot, Nutchanon and Torshizi, Parisa and Minucci, Elena and Marsella, Stacy},
	year = {2023},
	note = {event-place: Paris, France},
	keywords = {large language models, gesture selection, gesture analysis},
	pages = {378--387},
}

@inproceedings{klimek_re-oriented_2025,
	address = {New York, NY, USA},
	series = {{FSE} {Companion} '25},
	title = {{RE}-oriented {Model} {Development} with {LLM} {Support} and {Deduction}-based {Verification}},
	isbn = {979-8-4007-1276-0},
	url = {https://doi.org/10.1145/3696630.3730562},
	doi = {10.1145/3696630.3730562},
	abstract = {The requirements engineering (RE) phase is pivotal in developing high-quality software. Integrating advanced modelling techniques with large language models (LLMs) and formal verification in a logical style can significantly enhance this process. We propose a comprehensive framework that focuses on specific Unified Modelling Language (UML) diagrams for preliminary system development. This framework offers visualisations at various modelling stages and seamlessly integrates large language models and logical reasoning engines. The behavioural models generated with the assistance of LLMs are automatically translated into formal logical specifications. Deductive formal verification ensures that logical requirements and interrelations between software artefacts are thoroughly addressed. Ultimately, the framework facilitates the automatic generation of program skeletons, streamlining the transition from design to implementation.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Klimek, Radoslaw},
	year = {2025},
	note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
	keywords = {large language models, requirements engineering, automated logical reasoning, formal IDE, temporal logic, UML modelling},
	pages = {1297--1304},
}

@inproceedings{jiang_research_2024,
	address = {New York, NY, USA},
	series = {{ICMVA} '24},
	title = {Research on {Engineering} {Management} {Question}-answering {System} in the {Communication} {Industry} {Based} on {Large} {Language} {Models} and {Knowledge} {Graphs}},
	isbn = {979-8-4007-1655-3},
	url = {https://doi.org/10.1145/3653946.3653961},
	doi = {10.1145/3653946.3653961},
	abstract = {In the engineering management of the communication industry, there are many issues, including low efficiency in information acquisition and limitations in the level of intelligence.Large language models, with their powerful text comprehension and generation capabilities, offer new perspectives for the development of this field.This study constructed a question-answering system using a combined approach of large language models and text knowledge bases. The system dynamically leverages abundant external knowledge and enhances the model's reasoning ability and interpretability through knowledge graphs. In response to five categories of issues in engineering management, experiments and in-depth analysis revealed that although large language models may lack granularity in addressing some complex problems, the question-answering system overall achieved intelligent assistance, improving the efficiency of collaborative engineering management.},
	booktitle = {Proceedings of the 2024 7th {International} {Conference} on {Machine} {Vision} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Yingdi and Yao, Jiarui and Li, Fangfei and Zhang, Yan},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {Knowledge graphs, Question-answering, Engineering management, Keywords • Large language models},
	pages = {100--105},
}

@article{zhao_softmax_2023,
	title = {From {Softmax} to {Nucleusmax}: {A} {Novel} {Sparse} {Language} {Model} for {Chinese} {Radiology} {Report} {Summarization}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3596219},
	doi = {10.1145/3596219},
	abstract = {The Chinese radiology report summarization is a crucial component in smart healthcare that employs language models to summarize key findings in radiology reports and communicate these findings to physicians. However, most language models for radiology report summarization utilize a softmax transformation in their output layer, leading to dense alignments and strictly positive output probabilities. This density is inefficient, reducing model interpretability and giving probability mass to many unrealistic outputs. To tackle this issue, we propose a novel approach named nucleusmax. Nucleusmax is able to mitigate dense outputs and improve model interpretability by truncating the unreliable tail of the probability distribution. In addition, we incorporate nucleusmax with a copy mechanism, a useful technique to avoid professional errors in the generated diagnostic opinions. To further promote the research of radiology report summarization, we also have created a Chinese radiology report summarization dataset, which is freely available. Experimental results showed via both automatic and human evaluation that the proposed approach substantially improves the sparsity and overall quality of outputs over competitive softmax models, producing radiology summaries that approach the quality of those authored by physicians. In general, our work demonstrates the feasibility and prospect of the language model to the domain of radiology and smart healthcare.},
	number = {6},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Zhao, Shuai and Li, Qing and Yang, Yuer and Wen, Jinming and Luo, Weiqi},
	month = jun,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {language model, abstractive summarization, Chinese radiology report summarization, softmax},
}

@inproceedings{thiede_talking_2024,
	address = {New York, NY, USA},
	series = {Onward! '24},
	title = {Talking to {Objects} in {Natural} {Language}: {Toward} {Semantic} {Tools} for {Exploratory} {Programming}},
	isbn = {979-8-4007-1215-9},
	url = {https://doi.org/10.1145/3689492.3690049},
	doi = {10.1145/3689492.3690049},
	abstract = {In exploratory programming, programmers often face a semantic gap between their high-level understanding and the low-level interfaces available for interacting with objects in a system. That is, technical object structure and behavior need to be interpreted as abstract domain concepts, which then increases cognitive load and thus impedes exploration progress. We propose semantic object interfaces that bridge this gap by enabling contextual, natural-language conversations with objects. Our approach leverages an exploratory programming agent powered by a large language model (LLM) to translate natural-language questions into low-level experiments and provide high-level answers. We describe a framework for integrating semantic object interfaces into existing exploratory programming systems, including a prototype implementation in Squeak/Smalltalk using GPT-4o. We showcase the potential of semantic object interfaces through case studies and discuss their feasibility, limitations, and impact on the programming experience. While challenges remain, our approach promises to reduce mental effort and empower programmers to explore and understand systems at a higher level of abstraction for a better programming experience.},
	booktitle = {Proceedings of the 2024 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {Association for Computing Machinery},
	author = {Thiede, Christoph and Taeumel, Marcel and Böhme, Lukas and Hirschfeld, Robert},
	year = {2024},
	note = {event-place: Pasadena, CA, USA},
	keywords = {ChatGPT, LLMs, generative AI, object-oriented programming, conversational agents, exploratory programming, natural-language programming, semantic tools, Smalltalk},
	pages = {68--84},
}

@article{hossain_natural_2023,
	title = {Natural {Language}–{Based} {Conceptual} {Modelling} {Frameworks}: {State} of the {Art} and {Future} {Opportunities}},
	volume = {56},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3596597},
	doi = {10.1145/3596597},
	abstract = {Identifying requirements for an information system is an important task and conceptual modelling is the first step in this process. Conceptual modelling plays a critical role in the information system design process and usually involves domain experts and knowledge engineers who brainstorm together to identify the required knowledge to build an information system. The conceptual modelling process starts with the collection of necessary information from the domain experts by the knowledge engineers. Afterwards, the knowledge engineers use traditional model driven engineering techniques to design the system based on the collected information. Natural language–based conceptual modelling frameworks or systems are used to help domain experts and knowledge engineers in eliciting requirements and building conceptual models from a natural language text. In this article, we discuss the state of the art of some recent conceptual modelling frameworks that are based on natural language. We take a closer look at how these frameworks are built, in particular at the underlying motivation, architecture, types of natural language used (e.g., restricted vs. unrestricted), types of the conceptual model generated, verification support of the requirements specifications as well as the conceptual models, and underlying knowledge representation formalism. We also discuss some future research opportunities that these frameworks offer.},
	number = {1},
	journal = {ACM Comput. Surv.},
	author = {Hossain, Bayzid Ashik and Mukta, Md. Saddam Hossain and Islam, Md Adnanul and Zaman, Akib and Schwitter, Rolf},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural language processing, knowledge representation, information extraction, conceptual modelling, semantic round-tripping},
}

@inproceedings{ding_hpc-gpt_2023,
	address = {New York, NY, USA},
	series = {{SC}-{W} '23},
	title = {{HPC}-{GPT}: {Integrating} {Large} {Language} {Model} for {High}-{Performance} {Computing}},
	isbn = {979-8-4007-0785-8},
	url = {https://doi.org/10.1145/3624062.3624172},
	doi = {10.1145/3624062.3624172},
	abstract = {Large Language Models (LLMs), including the LLaMA model, have exhibited their efficacy across various general-domain natural language processing (NLP) tasks. However, their performance in high-performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model’s responses. In response to this challenge, we propose HPC-GPT, a novel LLaMA-based model that has been supervised fine-tuning using generated QA (Question-Answer) instances for the HPC domain. To evaluate its effectiveness, we concentrate on two HPC tasks: managing AI models and datasets for HPC, and data race detection. By employing HPC-GPT, we demonstrate comparable performance with existing methods on both tasks, exemplifying its excellence in HPC-related scenarios. Our experiments on open-source benchmarks yield extensive results, underscoring HPC-GPT’s potential to bridge the performance gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way for LLMs to excel in HPC domains, simplifying the utilization of language models in complex computing applications.},
	booktitle = {Proceedings of the {SC} '23 {Workshops} of the {International} {Conference} on {High} {Performance} {Computing}, {Network}, {Storage}, and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto and Du, Wan},
	year = {2023},
	note = {event-place: Denver, CO, USA},
	keywords = {Large Language Model, Data Race Detection, High-performance Computing, Neural Network., OpenMP},
	pages = {951--960},
}

@article{varagnolo_translating_2025,
	title = {Translating {Natural} {Language} {Questions} into {CIDOC}-{CRM} {SPARQL} {Queries} to {Access} {Cultural} {Heritage} {Knowledge} {Bases}},
	volume = {18},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3715156},
	doi = {10.1145/3715156},
	abstract = {To explore information on the semantic web, SPARQL queries or DL-queries are suitable tools. However, users interested in exploring the content of such knowledge bases often find it challenging to employ formal query languages, as this requires familiarity with the target domain’s representation model. To address these challenges, a question-answering system that automatically translates natural language questions into SPARQL queries, over the Smithsonian American Art Museum CIDOC-CRM representation is presented. The proposed approach uses an ontology, named Query Ontology, defined to represent the natural language concepts and relations specific to the question’s domain. This system’s architecture uses a traditional natural language processing symbolic approach, with a pipeline of modules for the syntactic, semantic, and pragmatic analysis. An evaluation of the proposed system is presented and shows very promising results.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Varagnolo, Davide and Melo, Dora and Pimenta Rodrigues, Irene},
	month = apr,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {CIDOC-CRM representation, datasets, SAAM’s knowledge base, SPARQL queries},
}

@article{piche_comparing_2023,
	title = {Comparing {Heuristic} {Rules} and {Masked} {Language} {Models} for {Entity} {Alignment} in the {Literature} {Domain}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3606699},
	doi = {10.1145/3606699},
	abstract = {The cultural world offers a staggering amount of rich and varied metadata on cultural heritage, accumulated by governmental, academic, and commercial players. However, the variety of involved institutions means that the data are stored in as many complex and often incompatible models and standards, which limits its availability and explorability by the greater public. The adoption of Linked Open Data technologies allows a strong interlinking of these various databases as well as external connections with existing knowledge bases. However, as they often contain references to the same entities, the delicate issue of entity alignment becomes the central challenge, especially in the absence or scarcity of unique global identifiers. To tackle this issue, we explored two approaches, one based on a set of heuristic rules and one based on masked language models, or masked language models (MLMs). We compare these two approaches, as well as different variations of MLMs, including some models trained on a different language, and various levels of data cleaning and labeling. Our results show that heuristics are a solid approach but also that MLM-based entity alignment obtains better performance coupled with the fact that it is robust to the data format and does not require any form of data preprocessing, which was not the case of the heuristic approach in our experiments.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Piché, Dominique and Font, Ludovic and Zouaq, Amal and Gagnon, Michel},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Linked open data, cultural heritage, entity matching, literature, masked language models},
}

@inproceedings{rula_procedural_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {Procedural {Text} {Mining} with {Large} {Language} {Models}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627572},
	doi = {10.1145/3587259.3627572},
	abstract = {Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Rula, Anisa and D'Souza, Jennifer},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {Language model, Knowledge representation, Deep learning, knowledge representation, Computational linguistics, Zero-shot learning, Knowledge capture, Language processing, Text-mining, knowledge capture, Natural languages, Learning systems, Natural language processing systems, Knowledge-representation, Context learning, In contexts, Large-scales, Learning settings},
	pages = {9--16},
	annote = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{schlor_optimizing_2023,
	address = {New York, NY, USA},
	series = {{ICMHI} '23},
	title = {Optimizing {Medical} {Service} {Request} {Processes} through {Language} {Modeling} and {Semantic} {Search}},
	isbn = {979-8-4007-0071-2},
	url = {https://doi.org/10.1145/3608298.3608324},
	doi = {10.1145/3608298.3608324},
	abstract = {Medical service requests are a crucial part of the workflow in hospitals and healthcare organizations. However, the process of requesting medical services can be time consuming and can require physicians and medical personnel to navigate complex interfaces and enter detailed information about the requested service. In this paper, we propose a system that uses machine learning techniques such as large language models and semantic search to optimize the process of requesting medical services. Our approach enables physicians to request medical services using natural language rather than navigating complex interfaces, allowing for more efficient and flexible interactions with hospital information systems. We evaluate our approach on real-world data and discuss the implications of our work for the future of digital health care. Our results suggest that our approach has the potential to streamline the process of requesting medical services and reduce the time and manual effort required in the daily hospital routine.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Medical} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Schlör, Daniel and Pfister, Jan and Hotho, Andreas},
	year = {2023},
	note = {event-place: Kyoto, Japan},
	keywords = {semantic search, language modeling, medical service optimization},
	pages = {136--141},
}

@inproceedings{sequeda_benchmark_2024,
	address = {New York, NY, USA},
	series = {{GRADES}-{NDA} '24},
	title = {A {Benchmark} to {Understand} the {Role} of {Knowledge} {Graphs} on {Large} {Language} {Model}'s {Accuracy} for {Question} {Answering} on {Enterprise} {SQL} {Databases}},
	isbn = {979-8-4007-0653-0},
	url = {https://doi.org/10.1145/3661304.3661901},
	doi = {10.1145/3661304.3661901},
	abstract = {Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16\%. Notably, this accuracy increases to 54\% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems.},
	booktitle = {Proceedings of the 7th {Joint} {Workshop} on {Graph} {Data} {Management} {Experiences} \&amp; {Systems} ({GRADES}) and {Network} {Data} {Analytics} ({NDA})},
	publisher = {Association for Computing Machinery},
	author = {Sequeda, Juan and Allemang, Dean and Jacob, Bryon},
	year = {2024},
	note = {event-place: Santiago, AA, Chile},
	keywords = {Knowledge graphs, Knowledge graph, Language model, Computational linguistics, Database systems, Question Answering, Zero-shot learning, Question answering systems, Model-based OPC, Modeling accuracy, SQL database, Business contexts, Enterprise applications, Question database},
	annote = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{gerhold_modelling_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Modelling of {Cyber}-{Physical} {Systems} through {Domain}-{Specific} {Languages}: {Decision}, {Analysis}, {Design}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688348},
	doi = {10.1145/3652620.3688348},
	abstract = {Cyber-Physical Systems (CPS) integrate computational algorithms and physical components, requiring sophisticated modelling techniques to address complex interactions and dynamics. This paper explores the creation of Domain-Specific Languages (DSLs) tailored for CPS, focusing on the initial three critical phases: decision, analysis, design. We present four key aspects to address in the decision phase, design an ontology as a domain model for the analysis phase, and collect some advice for the design phase. By systematically addressing these phases, we provide a comprehensive framework for developing DSLs that can efficiently model CPS, facilitating improved design, verification, and deployment of these intricate systems.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gerhold, Marcus and Kouzel, Aliaksei and Mangal, Haroun and Mehmed, Selin and Zaytsev, Vadim},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {cyber-physical systems, domain-specific languages, ontological analysis},
	pages = {1170--1179},
}

@inproceedings{he_study_2024,
	address = {New York, NY, USA},
	series = {{ICBDDM} '24},
	title = {A {Study} on {Large} {Language} {Model}-{Based} {Approach} for {Construction} {Contract} {Risk} {Detection}},
	isbn = {979-8-4007-1027-8},
	url = {https://doi.org/10.1145/3696500.3696523},
	doi = {10.1145/3696500.3696523},
	abstract = {Construction projects typically involve large-scale operations and are subject to complex external conditions, making it essential to safeguard the interests of contractor enterprises through well-crafted contract clauses. However, the current reliance on expert judgment for identifying contract risks presents several challenges, including lengthy processing times, heavy workloads, and inconsistent results. To address these issues, this study introduces a Large Language Model (LLM)-based approach for automating the identification of risks in construction contracts. The proposed method was rigorously validated on 26 actual contracts, achieving an average accuracy of 76.7\% across four state-of-the-art LLMs. This research advances the application of LLMs in construction contract management, providing practical solutions to existing challenges and setting the stage for further exploration in LLM-driven contract analysis.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Big} {Data} and {Digital} {Management}},
	publisher = {Association for Computing Machinery},
	author = {He, Yudong and Tang, Yinqiu and Chen, Tianhong},
	year = {2024},
	note = {event-place: Shanghai, China},
	pages = {136--141},
}

@article{van_thin_vietnamese_2023,
	title = {Vietnamese {Sentiment} {Analysis}: {An} {Overview} and {Comparative} {Study} of {Fine}-tuning {Pretrained} {Language} {Models}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3589131},
	doi = {10.1145/3589131},
	abstract = {Sentiment Analysis (SA) is one of the most active research areas in the Natural Language Processing (NLP) field due to its potential for business and society. With the development of language representation models, numerous methods have shown promising efficiency in fine-tuning pre-trained language models in NLP downstream tasks. For Vietnamese, many available pre-trained language models were also released, including the monolingual and multilingual language models. Unfortunately, all of these models were trained on different architectures, pre-trained data, and pre-processing steps; consequently, fine-tuning these models can be expected to yield different effectiveness. In addition, there is no study focusing on evaluating the performance of these models on the same datasets for the SA task up to now. This article presents a fine-tuning approach to investigate the performance of different pre-trained language models for the Vietnamese SA task. The experimental results show the superior performance of the monolingual PhoBERT model and ViT5 model in comparison with previous studies and provide new state-of-the-art performances on five benchmark Vietnamese SA datasets. To the best of our knowledge, our study is the first attempt to investigate the performance of fine-tuning Transformer-based models on five datasets with different domains and sizes for the Vietnamese SA task.},
	number = {6},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Van Thin, Dang and Hao, Duong Ngoc and Nguyen, Ngan Luu-Thuy},
	month = jun,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {fine-tuning language models, monolingual BERT model, multilingual BERT model, T5 architecture, Vietnamese Sentiment Analysis},
}

@article{zhou_ethics_2024,
	title = {Ethics, {Governance}, and {User} {Mental} {Models} for {Large} {Language} {Models} in {Computing} {Education}},
	volume = {31},
	issn = {1528-4972},
	url = {https://doi.org/10.1145/3688089},
	doi = {10.1145/3688089},
	abstract = {Large language models like ChatGPT are disrupting many industries, including computing education. How should policy evolve to improve learning outcomes?},
	number = {1},
	journal = {XRDS},
	author = {Zhou, Kyrie Zhixuan and Kilhoffer, Zachary and Sanfilippo, Madelyn Rose and Underwood, Ted and Gumusel, Ece and Wei, Mengyi and Choudhry, Abhinav and Xiong, Jinjun},
	month = oct,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {46--51},
}

@article{zini_explainability_2022,
	title = {On the {Explainability} of {Natural} {Language} {Processing} {Deep} {Models}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3529755},
	doi = {10.1145/3529755},
	abstract = {Despite their success, deep networks are used as black-box models with outputs that are not easily explainable during the learning and the prediction phases. This lack of interpretability is significantly limiting the adoption of such models in domains where decisions are critical such as the medical and legal fields. Recently, researchers have been interested in developing methods that help explain individual decisions and decipher the hidden representations of machine learning models in general and deep networks specifically. While there has been a recent explosion of work on Explainable Artificial Intelligence (ExAI) on deep models that operate on imagery and tabular data, textual datasets present new challenges to the ExAI community. Such challenges can be attributed to the lack of input structure in textual data, the use of word embeddings that add to the opacity of the models and the difficulty of the visualization of the inner workings of deep models when they are trained on textual data.Lately, methods have been developed to address the aforementioned challenges and present satisfactory explanations on Natural Language Processing (NLP) models. However, such methods are yet to be studied in a comprehensive framework where common challenges are properly stated and rigorous evaluation practices and metrics are proposed.Motivated to democratize ExAI methods in the NLP field, we present in this work a survey that studies model-agnostic as well as model-specific explainability methods on NLP models. Such methods can either develop inherently interpretable NLP models or operate on pre-trained models in a post hoc manner. We make this distinction and we further decompose the methods into three categories according to what they explain: (1) word embeddings (input level), (2) inner workings of NLP models (processing level), and (3) models’ decisions (output level). We also detail the different evaluation approaches interpretability methods in the NLP field. Finally, we present a case-study on the well-known neural machine translation in an appendix, and we propose promising future research directions for ExAI in the NLP field.},
	number = {5},
	journal = {ACM Comput. Surv.},
	author = {Zini, Julia El and Awad, Mariette},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {NLP, language models, transformers, ExAI, explaining decisions, neural machine translation, transparent embedding models},
}

@inproceedings{mamalis_can_2024,
	address = {New York, NY, USA},
	series = {{PCI} '23},
	title = {Can {Large} {Language} {Models} {Revolutionalize} {Open} {Government} {Data} {Portals}? {A} {Case} of {Using} {ChatGPT} in statistics.gov.scot},
	isbn = {979-8-4007-1626-3},
	url = {https://doi.org/10.1145/3635059.3635068},
	doi = {10.1145/3635059.3635068},
	abstract = {Large language models possess tremendous natural language understanding and generation abilities. However, they often lack the ability to discern between fact and fiction, leading to factually incorrect responses. Open Government Data are repositories of, often times linked, information that is freely available to everyone. By combining these two technologies in a proof of concept designed application utilizing the GPT3.5 OpenAI model and the Scottish open statistics portal, we show that not only is it possible to augment the large language model’s factuality of responses, but also propose a novel way to effectively access and retrieve statistical information from the data portal just through natural language querying. We anticipate that this paper will trigger a discussion regarding the transformation of Open Government Portals through large language models.},
	booktitle = {Proceedings of the 27th {Pan}-{Hellenic} {Conference} on {Progress} in {Computing} and {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Mamalis, Marios Evangelos and Kalampokis, Evangelos and Karamanou, Areti and Brimos, Petros and Tarabanis, Konstantinos},
	year = {2024},
	note = {event-place: Lamia, Greece},
	keywords = {large language model, natural language processing, linked data, open government data, chatgpt},
	pages = {53--59},
}

@inproceedings{quintana_leveraging_2023,
	address = {New York, NY, USA},
	series = {{BCB} '23},
	title = {Leveraging {Large} {Language} {Models} for {Predicting} {Microbial} {Virulence} from {Protein} {Structure} and {Sequence}},
	isbn = {979-8-4007-0126-9},
	url = {https://doi.org/10.1145/3584371.3612953},
	doi = {10.1145/3584371.3612953},
	abstract = {In the aftermath of COVID-19, screening for pathogens has never been a more relevant problem. However, computational screening for pathogens is challenging due to a variety of factors, including (i) the complexity and role of the host, (ii) virulence factor divergence and dynamics, and (iii) population and community-level dynamics. Considering a potential pathogen's molecular interactions, specifically individual proteins and protein interactions can help pinpoint a potential protein of a given microbe to cause disease. However, existing tools for pathogen screening rely on existing annotations (KEGG, GO, etc), making the assessment of novel and unannotated proteins more challenging. Here, we present an LLM-inspired approach that considers protein sequence and structure to predict protein virulence. We present a two-stage model incorporating evolutionary features captured from the DistilProtBert language model and protein structure in a graph convolutional network. Our model performs better than sequence alone for virulence function when high-quality structures are present, thus representing a path forward for virulence prediction of novel and unannotated proteins.},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Quintana, Felix and Treangen, Todd and Kavraki, Lydia},
	year = {2023},
	note = {event-place: Houston, TX, USA},
	keywords = {large language models, graph-based models, protein function, virulence prediction},
}

@article{qiang_agent-om_2024,
	title = {Agent-{OM}: {Leveraging} {LLM} {Agents} for {Ontology} {Matching}},
	volume = {18},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3712221.3712222},
	doi = {10.14778/3712221.3712222},
	abstract = {Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.},
	number = {3},
	journal = {Proc. VLDB Endow.},
	author = {Qiang, Zhangcheng and Wang, Weiqing and Taylor, Kerry},
	month = nov,
	year = {2024},
	note = {Publisher: VLDB Endowment},
	keywords = {Interoperability, Ontology, Language model, Semantic interoperability, Semantics, Ontology matching, Search engines, Expert systems, Performance, Adversarial machine learning, Learning systems, Ontology's, Matching system, Model agents, Design paradigm, Knowledge-based expert systems, Related entities},
	pages = {516--529},
	annote = {Cited by: 0},
}

@article{quere_state_2025,
	title = {The {State} of {Large} {Language} {Models} in {HCI} {Research}: {Workshop} {Report}},
	volume = {32},
	issn = {1072-5520},
	url = {https://doi.org/10.1145/3705617},
	doi = {10.1145/3705617},
	abstract = {In this section, we feature reports from conferences, symposia, workshops, and similar events, focusing on discussions where the boundaries of HCI and UX are being challenged and where debate is lively and ongoing.},
	number = {1},
	journal = {Interactions},
	author = {Quéré, Marianne Aubin Le and Schroeder, Hope and Randazzo, Casey and Gao, Jie},
	month = jan,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {8--9},
}

@article{chen_hyperbolic_2024,
	title = {Hyperbolic {Pre}-{Trained} {Language} {Model}},
	volume = {32},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2024.3407575},
	doi = {10.1109/TASLP.2024.3407575},
	abstract = {In recent years, we have witnessed significant improvements in pre-trained language models (PLM) brought about by the scaling of parameter sizes and data amounts. However, this also brings high computational and storage costs. In this paper, we present a new direction to improve PLMs without scaling parameters and data: adopting a geometric feature space that is more suitable for encoding the intrinsic structured features of text. Although text is generally considered unstructured data, it possesses rich intrinsic structured features that signify syntactic and semantic relationships. Leveraging these structured features is vital for text understanding. Given that structured features are better encoded in hyperbolic spaces than in the Euclidean spaces used by conventional PLMs, we propose that PLMs should operate entirely within hyperbolic spaces. Our experiments demonstrate the superiority of hyperbolic PLMs over Euclidean PLMs across a wide variety of tasks, using the same parameter and data settings. This suggests that altering the geometry of model representation is a promising direction for model enhancement.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Chen, Weize and Han, Xu and Lin, Yankai and He, Kaichen and Xie, Ruobing and Zhou, Jie and Liu, Zhiyuan and Sun, Maosong},
	month = may,
	year = {2024},
	note = {Publisher: IEEE Press},
	pages = {3101--3112},
}

@inproceedings{ashby_personalized_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {Personalized {Quest} and {Dialogue} {Generation} in {Role}-{Playing} {Games}: {A} {Knowledge} {Graph}- and {Language} {Model}-based {Approach}},
	isbn = {978-1-4503-9421-5},
	url = {https://doi.org/10.1145/3544548.3581441},
	doi = {10.1145/3544548.3581441},
	abstract = {Procedural content generation (PCG) in video games offers unprecedented opportunities for customization and user engagement. Working within the specialized context of role-playing games (RPGs), we introduce a novel framework for quest and dialogue generation that places the player at the core of the generative process. Drawing on a hand-crafted knowledge base, our method grounds generated content with in-game context while simultaneously employing a large-scale language model to create fluent, unique, accompanying dialogue. Through human evaluation, we confirm that quests generated using this method can approach the performance of hand-crafted quests in terms of fluency, coherence, novelty, and creativity; demonstrate the enhancement to the player experience provided by greater dynamism; and provide a novel, automated metric for the relevance between quest and dialogue. We view our contribution as a critical step toward dynamic, co-creative narrative frameworks in which humans and AI systems jointly collaborate to create unique and user-specific playable experiences.},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ashby, Trevor and Webb, Braden K and Knapp, Gregory and Searle, Jackson and Fulda, Nancy},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {natural language processing, knowledge graph, transformers, English, language model, narrative, RPG, computational creativity, dynamic quest generation, GPT-2, human-AI co-creativity, human-computer interaction, knowledge-grounded text generation, large-scale language models, MMORPG, NPC dialogue, procedural content generation, quest, quests, text generation, video games, World of Warcraft},
}

@inproceedings{jamil_knowledge_2024,
	address = {New York, NY, USA},
	series = {{SAC} '24},
	title = {Knowledge {Synthesis} using {Large} {Language} {Models} for a {Computational} {Biology} {Workflow} {Ecosystem}},
	isbn = {979-8-4007-0243-3},
	url = {https://doi.org/10.1145/3605098.3636026},
	doi = {10.1145/3605098.3636026},
	abstract = {An understanding of the molecular basis of musculoskeletal pain is necessary for the development of therapeutics, their management, and possible personalization. One-in-three Americans use OTC pain killers, and one tenth use prescription drugs to manage pain. The CDC also estimates that about 20\% Americans suffer from chronic pain. As the experience of acute or chronic pain varies due to individual genetics and physiology, it is imperative that researchers continue to find novel therapeutics to treat or manage symptoms. In this paper, our goal is to develop a seed knowledgebase computational platform, called BioNursery, that will allow biologists to computationally hypothesize, define and test molecular mechanisms underlying pain. In our knowledge ecosystem, we accumulate curated information from users about the relationships among biological databases, analysis tools, and database contents to generate biological analyses modules, called π-graphs, or process graphs. We propose a mapping function from a natural language description of a hypothesized molecular model to a computational workflow for testing in BioNursery. We use a crowd computing feedback and curation system, called Explorer, to improve proposed computational models for molecular mechanism discovery, and growing the knowledge ecosystem. Since the pain knowledge ecosystem does not yet exist, we validate our approach over a similar application in fertility research.},
	booktitle = {Proceedings of the 39th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Jamil, Hasan and Krawetz, Stephen and Gow, Alexander},
	year = {2024},
	note = {event-place: Avila, Spain},
	keywords = {crowdsourcing, knowledge ecosystem, query reformulation},
	pages = {523--530},
}

@inproceedings{mukanova_developing_2025,
	address = {New York, NY, USA},
	series = {{ICISE} '24},
	title = {Developing an {Ontological} {Model} for {Pre}-election {Advertising}},
	isbn = {979-8-4007-1736-9},
	url = {https://doi.org/10.1145/3711954.3711961},
	doi = {10.1145/3711954.3711961},
	abstract = {The article discusses the process and methods of creating ontological modeling in the field of “Pre-election advertising”. The work includes a structural-semantic analysis of the text corpus of political discourse and the creation of an ontological model of this subject area. The document discusses key concepts, their properties and relationships necessary for building the model, as well as the use of the formal language OWL to describe axioms and relationships between concepts. The ontological model was created using the Protégé platform and includes various classes, properties, objects and individuals specific to the subject area of pre-election advertising. Methods of constructing logical rules for extracting knowledge from a database, such as production rules and logical programming, are also considered.},
	booktitle = {Proceedings of the 2024 9th {International} {Conference} on {Information} {Systems} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Mukanova, Assel and Yergesh, Banu and Yelibayeva, Gaziza and Bekmanova, Gulmira and Razakhova, Bibigul},
	year = {2025},
	keywords = {Ontology, Knowledge base, Formal model, Political discourse, Pre-election advertising},
	pages = {23--28},
}

@inproceedings{reitmaier_cultivating_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Cultivating {Spoken} {Language} {Technologies} for {Unwritten} {Languages}},
	isbn = {979-8-4007-0330-0},
	url = {https://doi.org/10.1145/3613904.3642026},
	doi = {10.1145/3613904.3642026},
	abstract = {We report on community-centered, collaborative research that weaves together HCI, natural language processing, linguistic, and design insights to develop spoken language technologies for unwritten languages. Across three visits to a Banjara farming community in India, we use participatory, technical, and creative methods to engage community members, collect spoken language photo annotations, and develop an information retrieval (IR) system. Drawing on orality theory, we interrogate assumptions and biases of current speech interfaces and create a simple application that leverages our IR system to match fluidly spoken queries with recorded annotations and surface corresponding photos. In-situ evaluations show how our novel approach returns reliable results and inspired the co-creation of media retrieval use-cases that are more appropriate in oral contexts. The very low (\&lt; 4h) spoken data requirements makes our approach adaptable to other contexts where languages are unwritten or have no digital language resources available.},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Reitmaier, Thomas and Raju, Dani Kalarikalayil and Klejch, Ondrej and Wallington, Electra and Markl, Nina and Pearson, Jennifer and Jones, Matt and Bell, Peter and Robinson, Simon},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {co-creation, field study, Speech/language, zero-resource information retrieval},
}

@inproceedings{gobel_model-based_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Model-{Based} {Trust} {Analysis} of {LLM} {Conversations}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3687809},
	doi = {10.1145/3652620.3687809},
	abstract = {LLM-based chatbots are routinely advertised as supporting the collaboration of humans and AI. We study LLM conversations from a knowledge elicitation perspective with the objective of being able to understand and assess the human's trust in knowledge elicited from the LLM and complementary sources. Our approach is supported by the DSML KEML, the Knowledge Elicitation Modeling Language, subject to abstract and visual syntax as well as a model transformation-based model semantics for trust analysis. Conversations are modeled by a combination of sequence diagrams and enhanced argumentation graphs — the latter for the purpose of relating information pieces (facts and instructions) that are extracted from messages. The analysis of the corresponding models entails trust scores for gathered information (i.e., elicited knowledge).},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Göbel, Susanne and Lämmel, Ralf},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {dsmls for AI usage, knowledge representation models, MDE for AI, model-based analysis of LLMS},
	pages = {602--610},
}

@inproceedings{ichida_bdi_2024,
	address = {Richland, SC},
	series = {{AAMAS} '24},
	title = {{BDI} {Agents} in {Natural} {Language} {Environments}},
	isbn = {979-8-4007-0486-4},
	abstract = {Developing autonomous agents to deal with real-world problems is challenging, especially when developers are not necessarily specialists in artificial intelligence. This poses two key challenges regarding the interface of the programming with the developer, and the efficiency of the resulting agents. In this paper we tackle both challenges in an efficient agent architecture that leverages recent developments in natural language processing, and the intuitive folk psychology abstraction of the beliefs, desires, intentions (BDI) architecture. The resulting architecture uses existing reinforcement learning techniques to bootstrap the agent's reasoning capabilities while allowing a developer to instruct the agent more directly using natural language as its programming interface. We empirically show the efficiency gains of natural language plans over a pure machine learning approach in the ScienceWorld environment.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Ichida, Alexandre Yukio and Meneguzzi, Felipe and Cardoso, Rafael C.},
	year = {2024},
	note = {event-place: Auckland, New Zealand},
	keywords = {large language models, natural language, bdi agents, reinforcement learning},
	pages = {880--888},
}

@inproceedings{petridis_anglekindling_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {{AngleKindling}: {Supporting} {Journalistic} {Angle} {Ideation} with {Large} {Language} {Models}},
	isbn = {978-1-4503-9421-5},
	url = {https://doi.org/10.1145/3544548.3580907},
	doi = {10.1145/3544548.3580907},
	abstract = {News media often leverage documents to find ideas for stories, while being critical of the frames and narratives present. Developing angles from a document such as a press release is a cognitively taxing process, in which journalists critically examine the implicit meaning of its claims. Informed by interviews with journalists, we developed AngleKindling, an interactive tool which employs the common sense reasoning of large language models to help journalists explore angles for reporting on a press release. In a study with 12 professional journalists, we show that participants found AngleKindling significantly more helpful and less mentally demanding to use for brainstorming ideas, compared to a prior journalistic angle ideation tool. AngleKindling helped journalists deeply engage with the press release and recognize angles that were useful for multiple types of stories. From our findings, we discuss how to help journalists customize and identify promising angles, and extending AngleKindling to other knowledge-work domains.},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Petridis, Savvas and Diakopoulos, Nicholas and Crowston, Kevin and Hansen, Mark and Henderson, Keren and Jastrzebski, Stan and Nickerson, Jeffrey V and Chilton, Lydia B},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {Large Language Models, Generative AI, Brainstorming, Ideation, Journalism},
}

@inproceedings{zhang_cladmop_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {{CLaDMoP}: {Learning} {Transferrable} {Models} from {Successful} {Clinical} {Trials} via {LLMs}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3736879},
	doi = {10.1145/3711896.3736879},
	abstract = {Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder the learning of generalizable representations, leading to more false positives/negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset (SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trials' eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a ”pair matching” proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction (TOP) benchmark. CLaDMoP achieves up to 10.5\% improvement in PR-AUC and 3.6\% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yiqing and Liu, Xiaozhong and Murai, Fabricio},
	year = {2025},
	note = {event-place: Toronto ON, Canada},
	keywords = {representation learning, llms, clinical trial outcome prediction, multi-modal data fusion, self-supervised pre-training},
	pages = {3901--3911},
}

@inproceedings{kabir_comparative_2023,
	address = {New York, NY, USA},
	series = {{BCB} '23},
	title = {A {Comparative} {Analysis} of {Transformer}-based {Protein} {Language} {Models} for {Remote} {Homology} {Prediction}},
	isbn = {979-8-4007-0126-9},
	url = {https://doi.org/10.1145/3584371.3612942},
	doi = {10.1145/3584371.3612942},
	abstract = {Protein language models based on the transformer architecture are increasingly shown to learn rich representations from protein sequences that improve performance on a variety of downstream protein prediction tasks. These tasks encompass a wide range of predictions, including prediction of secondary structure, subcellular localization, evolutionary relationships within protein families, as well as superfamily and family membership. There is recent evidence that such models also implicitly learn structural information. In this paper we put this to the test on a hallmark problem in computational biology, remote homology prediction. We employ a rigorous setting, where, by lowering sequence identity, we clarify whether the problem of remote homology prediction has been solved. Among various interesting findings, we report that current state-of-the-art, large models are still underperforming in the "twilight zone" of very low sequence identity.},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Kabir, Anowarul and Moldwin, Asher and Shehu, Amarda},
	year = {2023},
	note = {event-place: Houston, TX, USA},
	keywords = {large language model, transformer, remote homology},
}

@article{wang_arobert_2022,
	title = {{ARoBERT}: {An} {ASR} {Robust} {Pre}-{Trained} {Language} {Model} for {Spoken} {Language} {Understanding}},
	volume = {30},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2022.3153268},
	doi = {10.1109/TASLP.2022.3153268},
	abstract = {Spoken Language Understanding (SLU) aims to interpret the meanings of human speeches in order to support various human-machine interaction systems. A key technique for SLU is Automatic Speech Recognition (ASR), which transcribes speech signals into text contents. As the output texts of modern ASR systems unavoidably contain errors, mainstream SLU models either trained or tested on texts transcribed by ASR systems would not be sufficiently error robust. We present ARoBERT, an ASR Robust BERT model, which can be fine-tuned to solve a variety of SLU tasks with noisy inputs. To guarantee the robustness of ARoBERT, during pretraining, we decrease the fluctuations of language representations when some parts of the input texts are replaced by homophones or synophones. Specifically, we propose two novel self-supervised pre-training tasks for ARoBERT, namely Phonetically-aware Masked Language Modeling (PMLM) and ASR Model-adaptive Masked Language Modeling (AMMLM). The PMLM task explicitly fuses the knowledge of word phonetic similarities into the pre-training process, which forces homophones and synophones to share similar representations. In AMMLM, a data-driven algorithm is further introduced to mine typical ASR errors such that ARoBERT can tolerate ASR model errors. In the experiments, we evaluate ARoBERT over multiple datasets. The results show the superiority of ARoBERT, which consistently outperforms strong baselines. We have also shown that ARoBERT outperforms state-of-the-arts on a public benchmark. Currently, ARoBERT has been deployed in an online production system with significant improvements.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Wang, Chengyu and Dai, Suyang and Wang, Yipeng and Yang, Fei and Qiu, Minghui and Chen, Kehan and Zhou, Wei and Huang, Jun},
	month = feb,
	year = {2022},
	note = {Publisher: IEEE Press},
	pages = {1207--1218},
}

@inproceedings{yao_key_2023,
	address = {New York, NY, USA},
	series = {{EITCE} '22},
	title = {Key {Phrase} {Extraction} based on {Pre}-trained {Language} {Models}},
	isbn = {978-1-4503-9714-8},
	url = {https://doi.org/10.1145/3573428.3573598},
	doi = {10.1145/3573428.3573598},
	abstract = {With the explosion of information and a large amount of data appearing every moment, it is a meaningful task to quickly find the information people want to know in a large amount of text and to present long texts in a streamlined form. Key phrase extraction, which aims to extract from documents a collection of key phrases that express the topic and content of the document, is important for text processing tasks such as information retrieval and document classification and can provide readers with a more comprehensive overview of the topic. We use two types of pre-trained language models for key phrase extraction, namely DeBERTa and RoBERTa, which are first pre-trained on the dataset and then fine-tuned, and the experimental results of these models proved that DeBERTa-V3-Large has reached an F1 score of 0.8925, which is the best result among these models.},
	booktitle = {Proceedings of the 2022 6th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yao, Shunyu and Hu, Jie and Sun, Chuxiong and Gao, Zhiqiao and Liu, Ning},
	year = {2023},
	note = {event-place: Xiamen, China},
	keywords = {Natural Language Processing, Artificial Intelligence, Key Phrase Extraction, Neural Network},
	pages = {941--945},
}

@inproceedings{sun_auto-acd_2024,
	address = {New York, NY, USA},
	series = {{MM} '24},
	title = {Auto-{ACD}: {A} {Large}-scale {Dataset} for {Audio}-{Language} {Representation} {Learning}},
	isbn = {979-8-4007-0686-8},
	url = {https://doi.org/10.1145/3664647.3681472},
	doi = {10.1145/3664647.3681472},
	abstract = {Recently, the AI community has made significant strides in developing powerful foundation models, driven by large-scale multimodal datasets. However, for audio representation learning, existing datasets suffer from limitations in the following aspects: insufficient volume, simplistic content, and arduous collection procedures. To establish an audio dataset with high-quality captions, we propose an innovative, automatic approach leveraging multimodal inputs, such as video frames, audio streams. Specifically, we construct a large-scale, high-quality, audio-language dataset, named as Auto-ACD, comprising over 1.5M audio-text pairs. We exploit a series of pre-trained models or APIs, to determine audio-visual synchronisation, generate image captions, object detection, or audio tags for specific videos. Subsequently, we employ LLM to paraphrase a congruent caption for each audio, guided by the extracted multi-modality clues. To demonstrate the effectiveness of the proposed dataset, we train widely used models on our dataset and show performance improvement on various downstream tasks, for example, audio-language retrieval, audio captioning, zero-shot classification. In addition, we establish a novel benchmark with environmental information and provide a benchmark for audio-text tasks.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Luoyi and Xu, Xuenan and Wu, Mengyue and Xie, Weidi},
	year = {2024},
	note = {event-place: Melbourne VIC, Australia},
	keywords = {audio captioning, audio-language dataset, audio-language representation learning},
	pages = {5025--5034},
}

@inproceedings{ding_fluid_2023,
	address = {New York, NY, USA},
	series = {C\&amp;{C} '23},
	title = {Fluid {Transformers} and {Creative} {Analogies}: {Exploring} {Large} {Language} {Models}’ {Capacity} for {Augmenting} {Cross}-{Domain} {Analogical} {Creativity}},
	isbn = {979-8-4007-0180-1},
	url = {https://doi.org/10.1145/3591196.3593516},
	doi = {10.1145/3591196.3593516},
	abstract = {Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of-concept of Large language Models’ (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently (∼ 80\% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of ∼ 25\% of outputs being rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility — and risks — of LLMs for augmenting cross-domain analogical creativity.},
	booktitle = {Proceedings of the 15th {Conference} on {Creativity} and {Cognition}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Zijian and Srinivasan, Arvind and Macneil, Stephen and Chan, Joel},
	year = {2023},
	note = {event-place: Virtual Event, USA},
	keywords = {Large Language Models, Analogy, Creativity Support Tools},
	pages = {489--505},
}

@inproceedings{wang_knowledge_2024,
	address = {New York, NY, USA},
	series = {{EITCE} '23},
	title = {Knowledge {Graphs} {Enhanced} {Large} {Language} {Model} {Prompt} for {Electric} {Power} {Question} {Answering}},
	isbn = {979-8-4007-0830-5},
	url = {https://doi.org/10.1145/3650400.3650405},
	doi = {10.1145/3650400.3650405},
	abstract = {With the continuous development and digital transformation in the field of electric power, the application of large language models in the electric power industry has become a remarkable trend. The electric power industry is an information-intensive domain involving extensive data processing, predictive analysis, and decision-making. Therefore, the application of large language models in the electric power sector is of great significance. Current large language models such as GPT3.5 and GLM can perform well in tasks such as question answering dialogues. However, these models still face challenges such as answer hallucination and inaccurate responses. This paper proposes a method to enhance question answering in large language models using knowledge graphs, aiming to improve the accuracy and reliability of these models in question answering tasks in the electric power domain.The proposed method first utilizes local electric power data to extract triplets and generate a question answering dataset specific to the electric power domain using a large language model. Then, the relationships of the knowledge graph triplets are incorporated into the question prompt to enhance the quality of the model's answers. Furthermore, we fine-tune the large language model using the expanded question set derived from the triplets as knowledge enhanced data. Subsequently, we conduct experiments on both an electric power question answering dataset and a knowledge graph question answering dataset. The experimental results demonstrate that our method significantly improves various metrics of the large language model in the electric power question answering task. This research provides new insights and approaches to enhance the effectiveness of question answering systems in the electric power domain. Future studies can further explore and optimize this prompt expansion method for application in broader domains and tasks.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Chen and Hua, Min and Song, Jiale and Tang, Xue-song},
	year = {2024},
	note = {event-place: Xiamen, China},
	pages = {24--29},
}

@inproceedings{setlur_supporting_2025,
	address = {New York, NY, USA},
	series = {{SIGMOD}/{PODS} '25},
	title = {Supporting {Human}-{Centric} {Data} {Exploration} {Through} {Semantics} and {Natural} {Language} {Interaction}},
	isbn = {979-8-4007-1564-8},
	url = {https://doi.org/10.1145/3722212.3725628},
	doi = {10.1145/3722212.3725628},
	abstract = {Data science plays an increasingly central role in decision-making across domains, yet the effectiveness of these decisions hinges not only on sophisticated algorithms but also on how well systems support human interpretation, exploration, and communication of data. This tutorial explores the intersection of semantics, natural language processing (NLP), and human-computer interaction in creating human-centric data exploration tools that promote accessibility, trust, and transparency. This includes techniques for generating perceptually meaningful visual encodings, using NLP for query interpretation and ambiguity resolution, and designing conversational interfaces that align with users' intent. The tutorial will highlight research from a broad set of contributors in the SIGMOD/VLDB, HCI, and visualization communities, spanning natural language interfaces for databases, multimodal interaction systems, semantic search for data repositories, and the use of AI and large language models to augment visual and textual analysis.As part of this 1.5-hour session, we will present case studies and systems that demonstrate how human-centric design can be integrated into the data analysis pipeline, including tools that support mixed-initiative interaction, adaptive defaults, and subjective query interpretation. We will also discuss open challenges and research opportunities, such as semantic inferencing for unstructured data, retrieval-augmented generation (RAG), and ethical considerations around fairness, explainability, and user agency. Drawing from principles in perception, linguistics, AI, and HCI, this tutorial aims to equip attendees with both conceptual frameworks and practical techniques to build more inclusive, interpretable, and intelligent data systems. It is intended for a broad audience in the SIGMOD/VLDB community interested in designing the next generation of data exploration tools that are aligned with human needs.},
	booktitle = {Companion of the 2025 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Setlur, Vidya},
	year = {2025},
	note = {event-place: Berlin, Germany},
	keywords = {semantic enrichment, conversational analytics, human-centered data exploration, natural language interfaces},
	pages = {851--854},
}

@article{bergami_towards_2023,
	title = {Towards a {Generalised} {Semistructured} {Data} {Model} and {Query} {Language}},
	volume = {2023},
	issn = {1931-1745},
	url = {https://doi.org/10.1145/3609429.3609433},
	doi = {10.1145/3609429.3609433},
	abstract = {Although current efforts are all aimed at re-defining new ways to harness old data representations, possibly with new schema features, the challenges still open provide evidence of the need for a "diametrically opposite" approach: in fact, all information generated in real contexts is to be understood lacking of any form of schema, where the schema associated with such data is only determined a posteriori based on either a specific application context, or from some data's facets of interest. This solution should still enable recommendation systems to manipulate the aforementioned data semantically. After providing evidence of these limitations from current literature, we propose a new Generalized Semistructured data Model that makes possible queries expressible in any data representation through a Generalised Semistructured Query Language, both relying upon script v2.0 as a MetaModel language manipulating types as terms as well as allowing structural aggregation functions.},
	number = {Summer},
	journal = {SIGWEB Newsl.},
	author = {Bergami, Giacomo and Zegadło, Wiktor},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
}

@inproceedings{wanna_topictag_2024,
	address = {New York, NY, USA},
	series = {{DocEng} '24},
	title = {{TopicTag}: {Automatic} {Annotation} of {NMF} {Topic} {Models} {Using} {Chain} of {Thought} and {Prompt} {Tuning} with {LLMs}},
	isbn = {979-8-4007-1169-5},
	url = {https://doi.org/10.1145/3685650.3685667},
	doi = {10.1145/3685650.3685667},
	abstract = {Topic modeling is a technique for organizing and extracting themes from large collections of unstructured text. Non-negative matrix factorization (NMF) is a common unsupervised approach that decomposes a term frequency-inverse document frequency (TF-IDF) matrix to uncover latent topics and segment the dataset accordingly. While useful for highlighting patterns and clustering documents, NMF does not provide explicit topic labels, necessitating subject matter experts (SMEs) to assign labels manually. We present a methodology for automating topic labeling in documents clustered via NMF with automatic model determination (NMFk). By leveraging the output of NMFk and employing prompt engineering, we utilize large language models (LLMs) to generate accurate topic labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs demonstrates the effectiveness of our method in enhancing knowledge management and document organization.},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Document} {Engineering} 2024},
	publisher = {Association for Computing Machinery},
	author = {Wanna, Selma and Solovyev, Nicholas and Barron, Ryan and Eren, Maksim E. and Bhattarai, Manish and Rasmussen, Kim Ø. and Alexandrov, Boian S.},
	year = {2024},
	note = {event-place: San Jose, CA, USA},
	keywords = {chain of thought, llm, nmf, prompt tuning, topic labeling},
}

@article{feuer_archetype_2024,
	title = {{ArcheType}: {A} {Novel} {Framework} for {Open}-{Source} {Column} {Type} {Annotation} {Using} {Large} {Language} {Models}},
	volume = {17},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3665844.3665857},
	doi = {10.14778/3665844.3665857},
	abstract = {Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type; incur high run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve CTA problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes a new state-of-the-art performance on zero-shot CTA benchmarks (including three new domain-specific benchmarks which we release along with this paper), and when used in conjunction with classical CTA techniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB benchmark.},
	number = {9},
	journal = {Proc. VLDB Endow.},
	author = {Feuer, Benjamin and Liu, Yurong and Hegde, Chinmay and Freire, Juliana},
	month = may,
	year = {2024},
	note = {Publisher: VLDB Endowment},
	pages = {2279--2292},
}

@article{arora_language_2023,
	title = {Language {Models} {Enable} {Simple} {Systems} for {Generating} {Structured} {Views} of {Heterogeneous} {Data} {Lakes}},
	volume = {17},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3626292.3626294},
	doi = {10.14778/3626292.3626294},
	abstract = {A long standing goal in the data management community is developing systems that input documents and output queryable tables without user effort. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using the in-context learning abilities of large language models (LLMs). We propose and evaluate Evaporate, a prototype system powered by LLMs. We identify two strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than directly processing each document with the LLM. To improve quality while maintaining low cost, we propose an extended implementation, Evaporate-Code+, which achieves better quality than direct extraction. Our insight is to generate many candidate functions and ensemble their extractions using weak supervision. Evaporate-Code+ outperforms the state-of-the art systems using a sublinear pass over the documents with the LLM. This equates to a 110X reduction in the number of documents the LLM needs to process across our 16 real-world evaluation settings.},
	number = {2},
	journal = {Proc. VLDB Endow.},
	author = {Arora, Simran and Yang, Brandon and Eyuboglu, Sabri and Narayan, Avanika and Hojel, Andrew and Trummer, Immanuel and Ré, Christopher},
	month = oct,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {92--105},
}

@inproceedings{marron_programming_2024,
	address = {New York, NY, USA},
	series = {Onward! '24},
	title = {A {Programming} {Language} for {Data} and {Configuration}!},
	isbn = {979-8-4007-1215-9},
	url = {https://doi.org/10.1145/3689492.3690054},
	doi = {10.1145/3689492.3690054},
	abstract = {A day in the life of a developer often involves more time working with schemas, configurations, and data description systems than writing code and logic in a classical programming language. As more systems move into distributed worlds, e.g. cloud and microservices, and developers make increasing use of libraries and frameworks, the need to interact with a range of data formats and configuration mechanisms is only increasing. This is a treacherous world, where a misspelled property name or missing field can render an entire service inoperable, a mistake that a number in an API represents seconds instead of milli-seconds can lead to a message being set for delivery in several months instead of in an hour, misconfigured schema can lead to public exposure of sensitive data, and corrupt or erroneous results from a misunderstood data format could result in massive financial and/or reputational damage. To address these challenges this paper casts the problems of data and configuration descriptions, not as a problem of data representation, but as a type system problem, that can be addressed with well understood and highly effective programming language techniques! The novel challenge is that data representation and configuration are universal concerns in a system and, particularly in modern cloud or micro-service systems, these systems may involve many programming languages. In the past this has led to specification systems that use a least-common-denominator set of data types, often little more than strings and numbers, and then rely on conventions or (out-of-date) documentation to ensure that the data is interpreted correctly. This paper shows that, with careful design, it is possible to create a rich universal system that can be used to express data and configuration specifications in a way that is human readable/writable and that can be produced/consumed, much like JSON, by a wide range of programming languages and systems.},
	booktitle = {Proceedings of the 2024 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {Association for Computing Machinery},
	author = {Marron, Mark},
	year = {2024},
	note = {event-place: Pasadena, CA, USA},
	keywords = {Configuration, Data Specification, Programming Language},
	pages = {147--161},
}

@inproceedings{qiu_research_2025,
	address = {New York, NY, USA},
	series = {{ISBDAI} '24},
	title = {Research on the {Construction} of {Semantic} {Information} {Retrieval} {Model} {Based} on {Logistics} {Ontology}},
	isbn = {979-8-4007-1829-8},
	url = {https://doi.org/10.1145/3723366.3723405},
	doi = {10.1145/3723366.3723405},
	abstract = {Along with the popularity of the Internet and the increase in the amount of information, how to quickly and effectively retrieve the expected information in the massive information has become the focus of information retrieval concerns and research. The current retrieval system is mainly based on the search keywords of the full text matching query, the results often return a large number of useless information, in the search rate and accuracy rate can not meet the user's retrieval needs. In this paper, based on the logistics field, we study the construction method of domain ontology, semantic annotation of knowledge document information based on ontology, and semantic query expansion of user's query statement using domain ontology, and propose a framework structure of semantic information retrieval model based on domain ontology, as well as designing and realising a prototype system.},
	booktitle = {Proceedings of the 2024 4th {International} {Symposium} on {Big} {Data} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Qiu, Shunli and Jiang, Wenxia},
	year = {2025},
	keywords = {Ontology, semantic information retrieval, model building},
	pages = {247--252},
}

@inproceedings{li_knowledge_2024,
	address = {New York, NY, USA},
	series = {{EITCE} '23},
	title = {Knowledge {Graph}-{Based} {Credibility} {Evaluation} {Method} for {Electric} {Grid} {Large} {Language} {Model} {Knowledge} {Question}-{Answering}},
	isbn = {979-8-4007-0830-5},
	url = {https://doi.org/10.1145/3650400.3650526},
	doi = {10.1145/3650400.3650526},
	abstract = {In the field of electricity, specialized terminology is often intricate and complex, making it challenging for non-experts to comprehend. However, with the advancement of artificial intelligence technology, the emergence of large language models provides a new technological solution to address this issue. Large language models, based on deep learning techniques, have the capability to quickly understand and interpret specialized terminology in the electricity domain through learning from a vast corpus of professional literature and data. They can then be applied to various domains, including question-answering systems. However, existing large language models still face issues of unreliable outputs, necessitating a method to evaluate their results and improve the quality of their applications. We propose a knowledge graph-based credibility evaluation method for electric grid large language model knowledge question-answering. This method aligns the answers generated by large language models with the knowledge graph of a local knowledge base and calculates their cosine similarity and Pearson correlation coefficient. We batch-process the answers from the large language model into an electricity dataset and validate them using this method. Experimental results demonstrate that this method can accurately and efficiently reflect the relevance between texts, providing a reliable scoring basis for question-answering by large models in vertical domains. Future research can focus on exploring other embedding methods that can better extract semantic relationships between texts and validating the feasibility of this method in vertical domains other than electricity.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Li, Wenqing and Qi, Xiaoman and Zhao, Qi and Wang, Chen and Wu, Qiongyu and Tang, Xue-song},
	year = {2024},
	note = {event-place: Xiamen, China},
	pages = {754--759},
}

@inproceedings{liu_cgir_2023,
	address = {New York, NY, USA},
	series = {{ICISS} '23},
	title = {{CGIR}: a {Model} of {Cross} {Language} {Information} {Retrieval} based on {Concept} {Graph} by {Fusing} {Attention} {Mechanism}},
	isbn = {979-8-4007-0820-6},
	url = {https://doi.org/10.1145/3625156.3625157},
	doi = {10.1145/3625156.3625157},
	abstract = {Cross language information retrieval faces challenges such as language differences, data scarcity, contextual disparities, and machine translation errors. To enhance retrieval accuracy and effectiveness, this paper proposes a similarity evaluation framework called Concept Graph Information Retrieval (CGIR). The framework includes the creation of concept graphs, quantized representations of these graphs, and retrieval processes. The construction process of CGIR incorporates an attention mechanism, significantly boosting its performance and accuracy. Through this fusion, concept graphs offer a comprehensive representation of texts, capturing the essence of the entire content while minimizing the displayed information, all while preserving the original meaning of the text to the fullest extent possible. The experimental results clearly demonstrate that the generated concept graphs effectively function as semantic representations of the entire texts. In comparison to keyword-based, ontology-based, and term-based retrieval methods, CGIR exhibits a remarkable improvement in accuracy, surpassing them by over 10\%.CCS CONCEPTS • Information systems∼Information retrieval},
	booktitle = {Proceedings of the 2023 6th {International} {Conference} on {Information} {Science} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Gang and Jiang, Wenhua and Hu, Yulin and Zhan, Kai and Wang, Tongli},
	year = {2023},
	note = {event-place: Edinburgh, United Kingdom},
	keywords = {information retrieval, attention mechanism, concept graph},
	pages = {1--7},
}

@article{di_building_2023,
	title = {Building {Dialogue} {Understanding} {Models} for {Low}-resource {Language} {Indonesian} from {Scratch}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3575803},
	doi = {10.1145/3575803},
	abstract = {Using off-the-shelf resources from resource-rich languages to transfer knowledge to low-resource languages has received a lot of attention. The requirements of enabling the model to achieve the reliable performance, including the scale of required annotated data and the effective framework, are not well guided. To address the first question, we empirically investigate the cost-effectiveness of several methods for training intent classification and slot-filling models from scratch in Indonesia (ID) using English data. Confronting the second challenge, we propose a Bi-Confidence-Frequency Cross-Lingual transfer framework (BiCF), which consists of “BiCF Mixing”, “Latent Space Refinement” and “Joint Decoder”, respectively, to overcome the lack of low-resource language dialogue data. BiCF Mixing based on the word-level alignment strategy generates code-mixed data by utilizing the importance-frequency and translating-confidence. Moreover, Latent Space Refinement trains a new dialogue understanding model using code-mixed data and word embedding models. Joint Decoder based on Bidirectional LSTM (BiLSTM) and Conditional Random Field (CRF) is used to obtain experimental results of intent classification and slot-filling. We also release a large-scale fine-labeled Indonesia dialogue dataset (ID-WOZ1) and ID-BERT for experiments. BiCF achieves 93.56\% and 85.17\% (F1 score) on intent classification and slot filling, respectively. Extensive experiments demonstrate that our framework performs reliably and cost-efficiently on different scales of manually annotated Indonesian data.},
	number = {4},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Di, Donglin and Song, Xianyang and Zhang, Weinan and Zhang, Yue and Wang, Fanglin},
	month = apr,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Dialogue datasets, indonesian, intent classification, slot-filling},
}

@inproceedings{sreedharan_legata_2025,
	address = {New York, NY, USA},
	series = {{CSLAW} '25},
	title = {Legata - {A} domain language for maritime regulatory compliance},
	isbn = {979-8-4007-1421-4},
	url = {https://doi.org/10.1145/3709025.3712213},
	doi = {10.1145/3709025.3712213},
	abstract = {The paper addresses the challenge of ensuring that increasingly powerful autonomous maritime vessels operate safely and conform to regulatory standards. We presents Legata, a domain language designed to ensure regulatory compliance in autonomous maritime vessels. By leveraging large-scale simulations, Legata translates legal regulations into computable terms, enabling precise evaluation of vessel behavior across diverse scenarios. The framework quantifies risk based on regulatory violations, providing a structured method for assessing compliance. A case study on the Istanbul Strait demonstrates Legata's practical application.},
	booktitle = {Proceedings of the 2025 {Symposium} on {Computer} {Science} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Sreedharan, Sreekant and Akdağ, Melih and Ramachandran, Muthu and Røseag, Erik and Rokseth, Børge},
	year = {2025},
	note = {event-place: Munich, Germany},
	keywords = {Law, AI \&amp, Autonomous Vessels, Maritime Regulations, Risk Evaluation, Safety Assurance},
	pages = {89--107},
}

@inproceedings{yelibayeva_ontology-based_2021,
	address = {New York, NY, USA},
	series = {{DATA}'21},
	title = {Ontology-{Based} {Extraction} of {Kazakh} {Language} {Word} {Combinations} in {Natural} {Language} {Processing}},
	isbn = {978-1-4503-8838-2},
	url = {https://doi.org/10.1145/3460620.3460631},
	doi = {10.1145/3460620.3460631},
	abstract = {This article provides an ontological model of nominative word combinations in the Kazakh language. It is necessary for creation of the automated templates for search of nominative word combinations of the Kazakh language in text corpora. The presented model expands the theory of applied linguistics in the field of extracting information from the text during corpus studies. The results will be used in semantic searches, Q\&amp;A systems and in the development of software applications for obtaining knowledge, as well as for training and evaluation of knowledge on the syntax of the Kazakh language in the system of e-learning.},
	booktitle = {International {Conference} on {Data} {Science}, {E}-{Learning} and {Information} {Systems} 2021},
	publisher = {Association for Computing Machinery},
	author = {Yelibayeva, Gaziza and Sharipbay, Altynbek and Bekmanova, Gulmira and Omarbekova, Assel},
	year = {2021},
	note = {event-place: Ma'an, Jordan},
	pages = {58--59},
}

@inproceedings{moore_translating_2024,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '24},
	title = {Translating motivational interviewing for the {HPV} vaccine into a computable ontology model for automated {AI} conversational interaction},
	isbn = {979-8-4007-0331-7},
	url = {https://doi.org/10.1145/3613905.3651051},
	doi = {10.1145/3613905.3651051},
	abstract = {Human papillomavirus (HPV) vaccinations are lower than expected. To protect the onset of head and neck cancers, innovative strategies to improve the rates are needed. Artificial intelligence may offer some solutions, specifically conversational agents to perform counseling methods. We present our efforts in developing a dialogue model for automating motivational interviewing (MI) to encourage HPV vaccination. We developed a formalized dialogue model for MI using an existing ontology-based framework to manifest a computable representation using OWL2. New utterance classifications were identified along with the ontology that encodes the dialogue model. Our work is available on GitHub under the GPL v.3. We discuss how an ontology-based model of MI can help standardize/formalize MI counseling for HPV vaccine uptake. Our future steps will involve assessing MI fidelity of the ontology model, operationalization, and testing the dialogue model in a simulation with live participants.},
	booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Moore, Nicole and Amith, Muhammad and Neumann, Ana and Hamilton, Jane and Tang, Lu and Savas, Lara and Tao, Cui},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {ontology, cancer, human papillomavirus, conversational agents, chat bots, dialogue systems, oral health, patient-provider communication},
}

@inproceedings{asmawi_understanding_2024,
	address = {New York, NY, USA},
	series = {{ICDTE} '24},
	title = {Understanding the {Digital} {Epistemologies} of {Chat} {GPT}: {Towards} a {Decolonial} {Language} {Pedagogy}},
	isbn = {979-8-4007-1757-4},
	url = {https://doi.org/10.1145/3696230.3696248},
	doi = {10.1145/3696230.3696248},
	abstract = {Since its emergence, research around Chat GPT and language teaching has trended into an asymmetry of opportunities and challenges from both utopian and dystopian perspectives. Chat GPT has Western data-based inherent coloniality and thus carries invisible colonial perpetuation when used in language education. However, Chat GPT has context-awareness and personalization capacity and is open to user control. Therefore, rather than decolonizing Chat GPT itself, decolonizing by Chat GPT can be a flipped approach to materialize decolonial persuasion in language pedagogy. Grounded in Santos's epistemology of the south, this paper attempts to conceptualize Chat GPT-assisted decolonial pedagogy. Using the authors’ constructivist ideation, the study employed simulated text data generated through a series of Chat GPT-author conversations. The collected data were analyzed by applying the educational data mining (EDM) method to support the primary conceptualization of the proposed decolonial pedagogy. The findings serve as a breakthrough with a novelty discovered in Chat GPT-facilitated decolonization of language pedagogy empowering decolonially charged educators working in the global south.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Digital} {Technology} in {Education} ({ICDTE})},
	publisher = {Association for Computing Machinery},
	author = {Asmawi, Adelina and Alam, Md. Saiful},
	year = {2024},
	keywords = {AI, Chat GPT, Chat GPT Epistemology, Decolonizing Education, Decolonizing ELT},
	pages = {277--283},
}

@inproceedings{yan_innovation_2025,
	address = {New York, NY, USA},
	series = {{ICAIES} '25},
	title = {Innovation and {Development} of {Chinese} {Language} {International} {Education} in the {Digital} {Age}},
	isbn = {979-8-4007-1506-8},
	url = {https://doi.org/10.1145/3744367.3744411},
	doi = {10.1145/3744367.3744411},
	abstract = {To study the development of China's digital applications and international education, modern technological methods, innovative education models and challenges were analysed. The solution explains several examples of key technology applications such as AI language assessment, internal learning and blockchain, and shows that integrating technology into an educational innovation in China offers huge opportunities, but the supply of resources needs to be further improved. Technical adaptation and ethical issues.},
	booktitle = {Proceedings of the 2025 {International} {Conference} on {Artificial} {Intelligence} and {Educational} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yan, Chengmin},
	year = {2025},
	keywords = {Chinese language international education, Digital education, ethical risks, innovative teaching and learning, technological empowerment},
	pages = {272--277},
}

@inproceedings{jiang_killing_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Killing {Two} {Birds} with {One} {Stone}: {Cross}-modal {Reinforced} {Prompting} for {Graph} and {Language} {Tasks}},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671742},
	doi = {10.1145/3637528.3671742},
	abstract = {In recent years, Graph Neural Networks (GNNs) and Large Language Models (LLMs) have exhibited remarkable capability in addressing different graph learning and natural language tasks, respectively. Motivated by this, integrating LLMs with GNNs has been increasingly studied to acquire transferable knowledge across modalities, which leads to improved empirical performance in language and graph domains. However, existing studies mainly focused on a single-domain scenario by designing complicated integration techniques to manage multimodal data effectively. Therefore, a concise and generic learning framework for multi-domain tasks, i.e., graph and language domains, is highly desired yet remains under-exploited due to two major challenges. First, the language corpus of downstream tasks differs significantly from graph data, making it hard to bridge the knowledge gap between modalities. Second, not all knowledge demonstrates immediate benefits for downstream tasks, potentially introducing disruptive noise to context-sensitive models like LLMs. To tackle these challenges, we propose a novel plug-and-play framework for incorporating a lightweight cross-domain prompting method into both language and graph learning tasks. Specifically, we first convert the textual input into a domain-scalable prompt, which not only preserves the semantic and logical contents of the textual input, but also highlights related graph information as external knowledge for different domains. Then, we develop a reinforcement learning-based method to learn the optimal edge selection strategy for useful knowledge extraction, which profoundly sharpens the multi-domain model capabilities. In addition, we introduce a joint multi-view optimization module to regularize agent-level collaborative learning across two domains. Finally, extensive empirical justifications over 23 public and synthetic datasets demonstrate that our approach can be applied to diverse multi-domain tasks more accurately, robustly, and reasonably, and improve the performances of the state-of-the-art graph and language models in different learning paradigms.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Wenyuan and Wu, Wenwei and Zhang, Le and Yuan, Zixuan and Xiang, Jian and Zhou, Jingbo and Xiong, Hui},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {large language models, prompt learning, graph neural networks, reinforcement learning},
	pages = {1301--1312},
}

@inproceedings{singh_power_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {The {Power} of {Language}: {Resisting} {Western} {Heteropatriarchal} {Normative} {Writing} {Standards}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3714073},
	doi = {10.1145/3706598.3714073},
	abstract = {Language is more than communication; it is a form of power. Whereas science has been scrutinized for privileging Western values and norms, what has been less explored is scientific linguistic performance (e.g. writing). The enforcement of English as the “normative standard” has prioritized hegemonic values and assumptions, thereby shaping the expectations of scientific performance. HCI/CSCW is dominated by heteropatriarchal Western practices, overlooking entangled values and assumptions impacting non-Western colleagues. Our work presents a design fiction (fictitious case study) envisioning a research contribution which embodies non-Western linguistic nuances as an alternative “normative standard” for scientific communication. Through this work, not only are we championing care in developing responsible linguistic practices in HCI/CSCW, but also epistemically challenging readers with intentional confusion. We establish a call to action for acknowledging and embracing different writing practices that are more inclusive of the diverse representation of scholars in HCI/CSCW.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Singh, Divyanshu Kumar and Das, Dipto and Semaan, Bryan},
	year = {2025},
	keywords = {Justice, Language, Decolonization, Power, Feminism, Coloniality, Human-Computer Interaction, Design Fiction, People of Color},
}

@inproceedings{erekhinskaya_ten_2020,
	address = {New York, NY, USA},
	series = {{SBD} '20},
	title = {Ten ways of leveraging ontologies for natural language processing and its enterprise applications},
	isbn = {978-1-4503-7974-8},
	url = {https://doi.org/10.1145/3391274.3393639},
	doi = {10.1145/3391274.3393639},
	abstract = {In the last years, Artificial Intelligence and Deep Learning have matured from a facinating research area to real-word applications across multiple domains. Enterprises adopt data-driven approaches for various use cases. With the increased adoption, such issues as governance of the models, deployment, scalability, reusablity and maintenance are widely addressed on the engineering side, but not so much on the knowledge side. In this paper, we demonstrate 10 ways of leveraging ontology for Natural Language Processing. Specifically, we explore the usage of ontologies and related standards for labeling schema, configuration, providing lexical data, powering rule engine and automated generation of rules, as well as providing a standard output format. Additionally, we discuss three NLP-based applications: semantic search, question answering and natural language querying and show how they can benefit from ontology usage. The paper summarizes our experience of using ontology in a number of projects for medical, enterprise, financial, legal and security domains.},
	booktitle = {Proceedings of {The} {International} {Workshop} on {Semantic} {Big} {Data}},
	publisher = {Association for Computing Machinery},
	author = {Erekhinskaya, Tatiana and Strebkov, Dmitriy and Patel, Sujal and Balakrishna, Mithun and Tatu, Marta and Moldovan, Dan},
	year = {2020},
	note = {event-place: Portland, Oregon},
	keywords = {natural language processing, ontologies, domain-specific knowledge, labeling, natural language querying, semantic graph},
}

@inproceedings{negreanu_rows_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Rows from {Many} {Sources}: {Enriching} row completions from {Wikidata} with a pre-trained {Language} {Model}},
	isbn = {978-1-4503-9130-6},
	url = {https://doi.org/10.1145/3487553.3524923},
	doi = {10.1145/3487553.3524923},
	abstract = {Row completion is the task of augmenting a given table of text and numbers with additional, relevant rows. The task divides into two steps: subject suggestion, the task of populating the main column; and gap filling, the task of populating the remaining columns. We present state-of-the-art results for subject suggestion and gap filling measured on a standard benchmark (WikiTables). Our idea is to solve this task by harmoniously combining knowledge base table interpretation and free text generation. We interpret the table using the knowledge base to suggest new rows and generate metadata like headers through property linking. To improve candidate diversity, we synthesize additional rows using free text generation via GPT-3, and crucially, we exploit the metadata we interpret to produce better prompts for text generation. Finally, we verify that the additional synthesized content can be linked to the knowledge base or a trusted web source such as Wikipedia.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Negreanu, Carina and Karaoglu, Alperen and Williams, Jack and Chen, Shuang and Fabian, Daniel and Gordon, Andrew and Lin, Chin-Yew},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {language models, free text generation, knowledge base linking, natural language applications, semantic knowledge, tabular data},
	pages = {1272--1280},
}

@inproceedings{xing_fusion_2024,
	address = {New York, NY, USA},
	series = {{IoTAAI} '23},
	title = {A fusion inference method for large language models and knowledge graphs based on structured injection and causal inference},
	isbn = {979-8-4007-1648-5},
	url = {https://doi.org/10.1145/3653081.3653117},
	doi = {10.1145/3653081.3653117},
	abstract = {In this paper, we propose a large language model and knowledge graph fusion reasoning method based on structured injection and causal reasoning (LKFSC) to address the limitations of existing large language models and knowledge graphs in practical applications. The approach effectively mitigates the problems of long-distance dependency and limited contextual information, and improves the reasoning capability of the large language model. Meanwhile, by fusing the generative ability of the large language model and the inference ability of the knowledge graph, the method realizes intelligent reasoning for complex problems. The main contributions of this paper include proposing a structured injection method that introduces causality for reasoning, and constructing a fusion reasoning framework that effectively mitigates the illusory problem of large language models and provides powerful and intelligent decision support for practical applications.},
	booktitle = {Proceedings of the 2023 5th {International} {Conference} on {Internet} of {Things}, {Automation} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Xing, Xueyang and Jia, Bo and Huang, Zhicheng and Chen, Yongzhi and Wang, Junjie and Fan, Anfei and Chen, Xin and Cao, Lei},
	year = {2024},
	note = {event-place: Nanchang, China},
	pages = {208--213},
}

@inproceedings{wang_multi_2025,
	address = {New York, NY, USA},
	series = {{ICITEE} '24},
	title = {Multi {Domain} {Ontology} {Model} {Fusion} and {Interoperability} {Method} of {Digital} {Twin} in {Distribution} {Network}},
	isbn = {979-8-4007-0708-7},
	url = {https://doi.org/10.1145/3717934.3718013},
	doi = {10.1145/3717934.3718013},
	abstract = {The digital twin of distribution network covers models including topology, geography, space, production, operation, control, measurement, marketing, electric field, etc. There is no unified rule framework for multi domain ontology modeling methods, and there is an urgent need for model fusion and comprehensive utilization. This article focuses on the cross domain joint modeling problem of distribution network ontology, conducts research on the fusion method of multi domain ontology models of distribution network digital twins, proposes a multi type model resource fusion and comprehensive utilization method with equipment and facilities as the core, integrates business models and power grid models, studies multi domain ontology model fusion interoperability technology for digital twins, and realizes the comprehensive utilization of distribution network digital twin model resources.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Information} {Technologies} and {Electrical} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Gang and Wang, He and Xu, Min and Zhou, Aihua and Yu, Hai and Qian, Zhonghao},
	year = {2025},
	keywords = {Digital twin, Model fusion, Distribution network, Multi domain ontology, Operation method},
	pages = {516--522},
}

@inproceedings{frey_pots_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{POTS} - {A} {Polyparadigmatic} {Ontology} {Term} {Search} with {Fine}-{Grained} {Context} {Steering} using {Hyper}-{Level} {Vector} {Spaces}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715194},
	doi = {10.1145/3701716.3715194},
	abstract = {We present a novel microservice-based system, that facilitates a polyparadigmatic ontology term search (leveraging semantic search via vector embeddings, keyword search, and attribute filters). The search index strategy intends to preserve important semantic aspects of the ontological context of a term (selected attributes and term relationships) using structured search fields and multilevel vector spaces assembling hyper-level vector spaces. The flexible, yet simple query API allows fine-grained search requests based on a combination of fuzzy and exact filters. The architecture is based on a highly automatable and flexible Docker Compose setup strategy. While deploying the system for a local ontology is only one command away, the setup also allows ingesting a configurable subset of over 1,800 published ontologies in over 12,000 versions via DBpedia Archivo.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Frey, Johannes and Ferraz, Lucas and Hofer, Marvin},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {Ontology, Terminology, LLM, Semantic Web, Semantics, ontology, Search engines, Semantic search, OWL, Information retrieval, Embeddings, Ontology retrieval, semantic search, llms, graph retrieval augmented generation, ontology retrieval, ontology terms embedding, owl, terminology lookup service, Vectors, Ontology's, Ontology graphs, Lookup services, Ontology \& graph retrieval augmented generation, Ontology term embedding, Ontology terms, Terminology lookup service, Vector spaces},
	pages = {2831--2834},
	annote = {Cited by: 1},
}

@inproceedings{shuttleworth_narratives_2023,
	address = {Singapore, Singapore},
	series = {{WSC} '22},
	title = {From {Narratives} to {Conceptual} {Models} via {Natural} {Language} {Processing}},
	abstract = {This paper explores the use of natural language processing (NLP) towards the semi-automatic generation of conceptual models, and eventual simulation specifications, from descriptions of a phenomenon. Narratives describing the problem are transformed into a list of concepts and relationships and visualized using a network graph. The process relies on pattern-based grammatical rules and an NLP dependency parser identifying important concept types, namely actors, factors, and mechanisms. We use three conceptualizations, created by potential users, to understand how the NLP-generated model should and could be adjusted. The objective of the research is to develop potential standard approaches users can use to generate conceptual models; develop a conceptual modeling assistant that subject matter experts can use to make them participant in the simulation creation process; and to identify how narratives should be written so an NLP-based conceptual modeling assistant may provide a thorough description of a phenomenon.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Shuttleworth, David and Padilla, Jose},
	year = {2023},
	pages = {2222--2233},
}

@inproceedings{pinna_integration_2025,
	address = {New York, NY, USA},
	series = {{PETRA} '25},
	title = {Integration of {Retrieval}-{Augmented} {Generation} {Technique} for {LLM}-based {Differential} {Diagnosis} {Assistant}},
	isbn = {979-8-4007-1402-3},
	url = {https://doi.org/10.1145/3733155.3733192},
	doi = {10.1145/3733155.3733192},
	abstract = {Artificial Intelligence (AI) is increasingly transforming the medical field, offering significant potential for diagnosis, treatment, and patient care. However, its successful integration relies on healthcare professionals, such as doctors, psychologists, and nurses, trusting the technology’s reliability and accuracy. For Large Language Models (LLMs), this trust requires transparent, verifiable, and rigorously reviewed information sources. This paper presents an AI-powered tool for differential diagnosis and disease comparison, utilizing an LLM enhanced by Retrieval-Augmented Generation (RAG). RAG overcomes traditional LLM limitations by enabling access to external, domain-specific knowledge, ensuring accurate and contextually relevant responses. The system leverages PubMed, a biomedical article aggregator, to extract symptom-related information from scientific literature on various disorders. Evaluations involving psychologist-administered questionnaires demonstrate that combining a similarity score with detailed symptom descriptions provides a clear understanding of relationships between disorders. This approach may enhance diagnostic precision and build trust in AI-driven tools, encouraging their broader adoption in clinical practice.},
	booktitle = {Proceedings of the 18th {ACM} {International} {Conference} on {PErvasive} {Technologies} {Related} to {Assistive} {Environments}},
	publisher = {Association for Computing Machinery},
	author = {Pinna, Simone and Massa, Silvia Maria and Fenu, Matteo and Casti, Giulio and Riboni, Daniele},
	year = {2025},
	keywords = {Large Language Models, Retrieval-Augmented Generation, Differential diagnosis, e-Health},
	pages = {277--284},
}

@inproceedings{negm_towards_2021,
	address = {New York, NY, USA},
	series = {{ICSIE} '20},
	title = {Towards {Ontology}-based {Domain} {Specific} {Language} for {Internet} of {Things}},
	isbn = {978-1-4503-7721-8},
	url = {https://doi.org/10.1145/3436829.3436833},
	doi = {10.1145/3436829.3436833},
	abstract = {Development of Internet of Things (IoT) applications is considered as a complex task. It requires knowledge in the different software layers starting from the low level perception layer to the high level application layer. The domain expert should be involved from the start of the project to its end, to ensure that the delivered system satisfies the user needs. Such involvement results from the continuous need for the domain knowledge throughout the software development lifecycle. Such long development time along with the high cost of IoT applications, cause a slow progress in the IoT development. In this paper, a Domain Specific Language (DSL), called OntIoT, is proposed that contributes in reducing the complexity of IoT application development through providing the needed domain knowledge in an automated manner. OntIoT is an ontology-based DSL that utilizes the Semantic Sensor Network (SSN) ontology to catch the IoT domain concepts and constraints.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Software} and {Information} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Negm, Eman and Makady, Soha and Salah, Akram},
	year = {2021},
	note = {event-place: Cairo, Egypt},
	keywords = {Ontology, Internet of Things, Domain Specific Language},
	pages = {146--151},
}

@inproceedings{netz_using_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Using {Grammar} {Masking} to {Ensure} {Syntactic} {Validity} in {LLM}-based {Modeling} {Tasks}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3687805},
	doi = {10.1145/3652620.3687805},
	abstract = {Low-code development platforms (LCDPs) are becoming increasingly important in industry, which confronts us in academic teaching with the challenge of educating students in the basic principles, critical engagement, and evaluation of LCDPs. This leads us to the question, how to teach the usage of different LCDPs during an university course. The short time frame of university-level courses makes it challenging to teach more than only one LCDP. In our teaching approach, students use two different LCDPs and create a web-application with both of them. Firstly, we require the students to define a target application with common modeling languages, next they use the first LCDP, at about half the time they switch to the second LCDP and present their findings of the differences in methodology and development processes at the end. We discuss this approach, show survey results from the participants, and explain lessons learned. This concept allows students critical engagement with LCDPs and model-driven software engineering. Supervisors get an insight into the learnability of each LCDP and how novices adapt to different domain-specific languages and their notations.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Netz, Lukas and Reimer, Jan and Rumpe, Bernhard},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {education, problem-based learning, low-code development platforms, model-driven software engineering, university-level courses},
	pages = {115--122},
}

@article{jha_prediction_2023,
	title = {Prediction of {Protein}-{Protein} {Interactions} {Using} {Vision} {Transformer} and {Language} {Model}},
	volume = {20},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2023.3248797},
	doi = {10.1109/TCBB.2023.3248797},
	abstract = {The knowledge of protein-protein interaction (PPI) helps us to understand proteins’ functions, the causes and growth of several diseases, and can aid in designing new drugs. The majority of existing PPI research has relied mainly on sequence-based approaches. With the availability of multi-omics datasets (sequence, 3D structure) and advancements in deep learning techniques, it is feasible to develop a deep multi-modal framework that fuses the features learned from different sources of information to predict PPI. In this work, we propose a multi-modal approach utilizing protein sequence and 3D structure. To extract features from the 3D structure of proteins, we use a pre-trained vision transformer model that has been fine-tuned on the structural representation of proteins. The protein sequence is encoded into a feature vector using a pre-trained language model. The feature vectors extracted from the two modalities are fused and then fed to the neural network classifier to predict the protein interactions. To showcase the effectiveness of the proposed methodology, we conduct experiments on two popular PPI datasets, namely, the human dataset and the \&lt;italic\&gt;S. cerevisiae\&lt;/italic\&gt; dataset. Our approach outperforms the existing methodologies to predict PPI, including multi-modal approaches. We also evaluate the contributions of each modality by designing uni-modal baselines. We perform experiments with three modalities as well, having gene ontology as the third modality.},
	number = {5},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
	month = feb,
	year = {2023},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	keywords = {Language model, Deep learning, Gene Ontology, Transformers, Feature extraction, Neural Networks, Forecasting, Transformer, Computational linguistics, Vision transformer, protein-protein interaction, vision transformer, Proteins, chemistry, metabolism, human, protein, Protein sequence, Three-dimensional displays, Amino acids, artificial neural network, Humans, amino acid sequence, Computer, Protein sequences, Protein-protein interactions, Amino Acid Sequence, Saccharomyces cerevisiae, Features extraction, multiomics, Three dimensional displays, Three-dimensional display, Amino-acids, 3D Structure, Multiomics},
	pages = {3215--3225},
	annote = {Cited by: 7},
}

@article{mikhaylova_extending_2023,
	title = {Extending {RiC}-{O} to {Model} {Historical} {Architectural} {Archives}: {The} {ITDT} {Ontology}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3606706},
	doi = {10.1145/3606706},
	abstract = {Historical architectural archives enjoy attention from diverse audiences, acting as a primary source of information for architects, historians, public authorities, and common citizens alike. In Italy, the interest in architectural archives has grown slowly but steadily for the last 20 years. However, architectural archives do not generally follow the trend common for museums and galleries in publishing digitized materials and providing standard metadata for individual records. The information that is available online usually includes only an archival finding aid, instead of metadata about the individual records, or fully digital versions of the records. While cataloguing standards for archival descriptions of architectural records have existed at least since the 1980s, the rise of Linked Open Data as a framework for publishing cultural heritage data has allowed archivists to enhance these archival descriptions with richer contextual information and links to external knowledge bases. In this paper we present the ITDT ontology, an extension of the Records in Contexts Ontology that facilitates the representation of architectural records and of the context related to architectural projects, its process, and participating entities. We discuss the application of the ontology to the project files of Italian architect and engineer Dino Tamburini (1924–2011), and the creation of a digital archive offering multiple perspectives over the records.},
	number = {4},
	journal = {J. Comput. Cult. Herit.},
	author = {Mikhaylova, Daria and Metilli, Daniele},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, Historical archives, semantic annotations},
}

@inproceedings{deshpande_leveraging_2019,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '19},
	title = {Leveraging {Ontological} {Knowledge} for {Neural} {Language} {Models}},
	isbn = {978-1-4503-6207-8},
	url = {https://doi.org/10.1145/3297001.3297059},
	doi = {10.1145/3297001.3297059},
	abstract = {Neural Language Models such as Word2Vec and GloVe have been shown to encode semantic relatedness between words. Improvements in unearthing these embeddings can ameliorate performance in numerous downstream applications such as sentiment analysis, question answering, and dialogue generation. Lexical ontologies such as WordNet are known to supply information about semantic similarity rather than relatedness. Further, extracting word em-beddings from small corpora is daunting for data-hungry neural networks. This work shows how methods that conflate Word2Vec and Ontologies can achieve better performance, reduce training time and help adapt to domains with a minimum amount of data.},
	booktitle = {Proceedings of the {ACM} {India} {Joint} {International} {Conference} on {Data} {Science} and {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Deshpande, Ameet and Jegadeesan, Monisha},
	year = {2019},
	note = {event-place: Kolkata, India},
	keywords = {Ontology, Semantics, Knowledge management, Embeddings, Sentiment analysis, Semantic similarity, Computational linguistics, Question Answering, Hierarchy, Semantic relatedness, Domain-transfer, Word Vectors, Word vectors, Downstream applications, Dialogue generations, Domain transfers},
	pages = {350--353},
	annote = {Cited by: 0},
}

@inproceedings{yang_transbox_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{TransBox}: {EL}++-closed {Ontology} {Embedding}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714672},
	doi = {10.1145/3696410.3714672},
	abstract = {OWL (Web Ontology Language) ontologies, which are able to represent both relational and type facts as standard knowledge graphs and complex domain knowledge in Description Logic (DL) axioms, are widely adopted in domains such as healthcare and bioinformatics. Inspired by the success of knowledge graph embeddings, embedding OWL ontologies has gained significant attention in recent years. Current methods primarily focus on learning embeddings for atomic concepts and roles, enabling the evaluation based on normalized axioms through specially designed score functions. However, they often neglect the embedding of complex concepts, making it difficult to infer with more intricate axioms. This limitation reduces their effectiveness in advanced reasoning tasks, such as Ontology Learning and ontology-mediated Query Answering. In this paper, we propose EL++-closed ontology embeddings which are able to represent any logical expressions in DL EL++ via composition. Furthermore, we develop TransBox, an effective EL++ -closed ontology embedding method that can handle many-to-one, one-to-many and many-to-many relations. Our extensive experiments demonstrate that TransBox often achieves state-of-the-art performance across various real-world datasets for predicting complex axioms.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Yang, Hui and Chen, Jiaoyan and Sattler, Uli},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {web ontology language, ontology learning, description logic, ontology completion, ontology embedding},
	pages = {22--34},
}

@inproceedings{gao_exploring_2024,
	address = {New York, NY, USA},
	series = {{ISCER} '24},
	title = {Exploring {Computational} {Visual} {Interfaces} for {Artificial} {Intelligence} {Language} {Modeling} {User} {Experience} among {College} {Students}: {A} {Rooted} {Theoretical} {Approach}},
	isbn = {979-8-4007-0995-1},
	url = {https://doi.org/10.1145/3679431.3679514},
	doi = {10.1145/3679431.3679514},
	abstract = {Artificial Intelligence (AI) is an emerging technology with the aim of developing intelligent applications that have broad applications in various fields such as healthcare, education, and design. AI design research is presently in an exploratory phase, yet its influence on computer vision interfaces for subjective user experience is becoming increasingly significant. Focused on Chinese university students, the research delves into AI user experience, emphasizing NLP, HCI, and SU. Data was collected via surveys and interviews, with deep learning techniques aiding data processing. A substantial volume of user data was gathered through user surveys and in-depth interviews, with deep learning techniques employed for data preprocessing and feature extraction. Results show a preference for personalized services and data mining in interface design, while technical features and operational fluency are priorities in programming. Enhancing HCI design can improve operational efficiency and meet individual needs, bolstered by clear visual interfaces and HCI technologies, thus enhancing overall user experience quality and effectiveness.},
	booktitle = {Proceedings of the 2024 3rd {International} {Symposium} on {Control} {Engineering} and {Robotics}},
	publisher = {Association for Computing Machinery},
	author = {Gao, Xiang and Zhou, Yuanchao},
	year = {2024},
	note = {event-place: Changsha, China},
	pages = {516--522},
}

@article{bhagwat_marathi_2024,
	title = {Marathi to {Indian} {Sign} {Language} {Machine} {Translation}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3664609},
	doi = {10.1145/3664609},
	abstract = {Machine translation has been a prominent field of research, contributing significantly to human life enhancement. Sign language machine translation, a subfield, focuses on translating spoken language content into sign language and vice versa, thereby facilitating communication between the normal hearing and hard-of-hearing communities, promoting inclusivity.This study presents the development of a ‘sign language machine translation system’ converting simple Marathi sentences into Indian Sign Language (ISL) glosses and animation. Given the low-resource nature of both languages, a phrase-level rule-based approach was employed for the translation. Initial encoding of translation rules relied on basic linguistic knowledge of Marathi and ISL, with subsequent incorporation of rules to address 'simultaneous morphological' features in ISL. These rules were applied during the ‘generation phase’ of translation to dynamically adjust phonological sign parameters, resulting in improved target sentence fluency.The paper provides a detailed description of the system architecture, translation rules, and comprehensive experimentation. Rigorous evaluation efforts were undertaken, encompassing various linguistic features, and the findings are discussed herein.The web-based version of the system serves as an interpreter for brief communications and can support the teaching and learning of sign language and its grammar in schools for hard-of-hearing students.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Bhagwat, Suvarna Rajesh. and Bhavsar, R. P. and Pawar, B. V.},
	month = may,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Inclusion of specially-abled community, Indian Sign Language, Marathi, Phrase-level Translation, Rule-based Translation, Sign languages’ simultaneous morphological features},
	annote = {Just Accepted},
}

@article{huang_sequence_2023,
	title = {Sequence {Generation} {Model} {Integrating} {Domain} {Ontology} for {Mathematical} question tagging},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3593804},
	doi = {10.1145/3593804},
	abstract = {In online learning systems, tagging knowledge points for questions is a fundamental task. Automatic tagging technology uses intelligent algorithms to automatically tag knowledge points for questions to reduce manpower and time costs. However, the current knowledge point tagging technology cannot satisfy the situation that mathematics questions often involve a variable number of knowledge points, lacks the consideration of the characteristics of the mathematics field, and ignores the internal connection between knowledge points. To address the above issues, we propose a Sequence Generation Model Integrating Domain Ontology for Mathematical question tagging (SOMPT). SOMPT performs data augmentation for text and then obtains intermediate text based on domain ontology replacement to facilitate deep learning model to understand mathematical question text. SOMPT is able to obtain dynamic word vector embedding to optimize the textual representation for math questions. What’s more, our model can capture the relationship between tags to generate knowledge points more accurately in the way of sequence generation. The comparative experimental results show that our proposed model has an excellent tagging ability for mathematical questions. Moreover, the sequence generation module in SOMPT can be applied on other multi-label classification tasks and be on par with the state-of-the-art performance models.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Huang, Tao and Hu, Shengze and Lin, Keke and Yang, Huali and Zhang, Hao and Song, Houbing and Lv, Zhihan},
	month = apr,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Language models, Deep learning, Mathematical question tagging, Sequence generation},
	annote = {Just Accepted},
}

@article{wu_unveiling_2024,
	title = {Unveiling {Transformative} {Insights} via {Cross}-{Modal} {Learning} and {Natural} {Language} {Processing} for {Enhanced} {Supply} {Chain} {Intelligence}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3687306},
	doi = {10.1145/3687306},
	abstract = {This day's quickly developing business landscape, supply chains have become more globalized, intricate, and multi-covering, making them crucial for companies to navigate through disruptions and unpredictability. The major which are addressed in the supply chain process are lack of transparency and visibility of the supply chain network and that's leads to delay and inefficiency in the process. In order to overcome those drawbacks in the supply chain process, in this article an enhanced supply chain intelligence is developed which performs Unveiling Transformative Insights using the learning process like Cross-Modal Learning (CML) and Natural Language Processing (NLP). The implementation of these techniques is carried out in the software Python. This analysis consists of certain calculation called enhanced supply chain analysis, sales revenue Vs SKU analysis, various modes cost analysis, Lead time vs different supplier and location. The comparative analysis is performed among the technique like RF regression, SARIMA-LSTM-BP and BiLSTM model. The parameters which are involved in this performance analysis are MAE, MSE, RMSE and R{\textasciicircum}2.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Wu, Xiaobing},
	month = sep,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cross-Modal Learning (CML) and Natural Language Processing (NLP), Enhanced Supply Chain Intelligence, Unveiling Transformative Insights},
	annote = {Just Accepted},
}

@article{luoma_snails_2025,
	title = {{SNAILS}: {Schema} {Naming} {Assessments} for {Improved} {LLM}-{Based} {SQL} {Inference}},
	volume = {3},
	url = {https://doi.org/10.1145/3709727},
	doi = {10.1145/3709727},
	abstract = {Large Language Models (LLMs) have revolutionized Natural Language to SQL (NL-to-SQL), dominating most NL-to-SQL benchmarks. But LLMs still face limitations due to hallucinations, semantic ambiguity, and lexical mismatches between an NL query and the database schema. Naturally, a lot of work in the ML+DB intersection aims to mitigate such LLM limitations. In this work, we shine the light on a complementary data-centric question: How should DB schemas evolve in this era of LLMs to boost NL-to-SQL? The intuition is that more NL-friendly schema identifiers can help LLMs work better with DBs. We dive deeper into this seemingly obvious, but hitherto underexplored and important, connection between schema identifier ”naturalness” and the behavior of LLM-based NL-to-SQL by creating a new integrated benchmark suite we call SNAILS. SNAILS has 4 novel artifacts: (1) A collection of real-world DB schemas not present in prior NL-to-SQL benchmarks; (2) A set of labeled NL-SQL query pairs on our collection not seen before by public LLMs; (3) A notion of naturalness level for schema identifiers and a novel labeled dataset of modified identifiers; and (4) AI artifacts to automatically modify identifier naturalness. Using SNAILS, we perform a comprehensive empirical evaluation of the impact of schema naturalness on LLM-based NL-to-SQL accuracy, and present a method for improving LLM-based NL-to-SQL with natural views. Our results reveal statistically significant correlations across multiple public LLMs from OpenAI, Meta, and Google on multiple databases using both zero-shot prompting as well as more complex NL-to-SQL workflows: DIN SQL, and CodeS. We present several fine-grained insights and discuss pathways for DB practitioners to better exploit LLMs for NL-to-SQL.},
	number = {1},
	journal = {Proc. ACM Manag. Data},
	author = {Luoma, Kyle and Kumar, Arun},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {database, llm, benchmark, natural language to sql, relational database schema, schema design, schema linking, schema naturalness},
}

@article{de_lara_language_2021,
	title = {Language {Family} {Engineering} with {Product} {Lines} of {Multi}-level {Models}},
	volume = {33},
	issn = {0934-5043},
	url = {https://doi.org/10.1007/s00165-021-00554-3},
	doi = {10.1007/s00165-021-00554-3},
	abstract = {Modelling is an essential activity in software engineering. It typically involves two meta-levels: one includes meta-models that describe modelling languages, and the other contains models built by instantiating those meta-models. Multi-level modelling generalizes this approach by allowing models to span an arbitrary number of meta-levels. A scenario that profits from multi-level modelling is the definition of language families that can be specialized (e.g., for different domains) by successive refinements at subsequent meta-levels, hence promoting language reuse. This enables an open set of variability options given by all possible specializations of the language family. However, multi-level modelling lacks the ability to express closed variability regarding the availability of language primitives or the possibility to opt between alternative primitive realizations. This limits the reuse opportunities of a language family. To improve this situation, we propose a novel combination of product lines with multi-level modelling to cover both open and closed variability. Our proposal is backed by a formal theory that guarantees correctness, enables top-down and bottom-up language variability design, and is implemented atop the MetaDepth multi-level modelling tool.},
	number = {6},
	journal = {Form. Asp. Comput.},
	author = {de Lara, Juan and Guerra, Esther},
	month = dec,
	year = {2021},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Domain-specific languages, Meta-modelling, MetaDepth, Multi-level modelling, Product lines, Software language engineering},
	pages = {1173--1208},
}

@inproceedings{fitsilis_using_2023,
	address = {New York, NY, USA},
	series = {{PCI} '22},
	title = {Using {TOSCA} language to model personalized educational content: {Introducing} {eduTOSCA}},
	isbn = {978-1-4503-9854-1},
	url = {https://doi.org/10.1145/3575879.3576017},
	doi = {10.1145/3575879.3576017},
	abstract = {Students attending Higher Education Institutions (HEIs) of Vocational Educational and Training (VET) are faced with a variety of complex decisions and procedures. To provide students with more sustained and personalized advising, many HEIs/VETs use academic advising systems and tools as a way to minimize costs and streamline their advising services. Furthermore, it is quite common for educational programs to include and combine educational content from different educational providers, while they are managed and executed on different platforms. Therefore, the ability to develop conceptual models for personalized learning based on educational content produced by heterogeneous educational service providers is a pressing need to address. A similar issue is confronted when deploying applications across diverse cloud computing platforms. A solution that is provided in these situations is the development of specialized languages for defining the topology and the orchestration of applications such as TOSCA, CAMP, Open-CSA, etc. In this paper, we propose to use similar conceptual models for modelling heterogeneous educational offerings toward personalized learning, which are presented along with the overall architecture of a system, named cc-coach, able to support these concepts. Further, this paper is a proposal for the standardization efforts needed for creating a multi-vendor educational ecosystem with diverse stakeholders, able to support personalized learning at various levels.},
	booktitle = {Proceedings of the 26th {Pan}-{Hellenic} {Conference} on {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Fitsilis, Panos and Iatrellis, Omiros and Tsoutsa, Paraskevi},
	year = {2023},
	note = {event-place: Athens, Greece},
	keywords = {e-learning, curriculum modelling, eduTOSCA, orchestration, personalized learning, TOSCA},
	pages = {355--360},
}

@article{ghimire_hwrex_2025,
	title = {{HWREx}: {AI}-enabled {Hardware} {Weakness} and {Risk} {Exploration} and {Storytelling} {Framework} with {LLM}-assisted {Mitigation} {Suggestion}},
	issn = {1084-4309},
	url = {https://doi.org/10.1145/3737459},
	doi = {10.1145/3737459},
	abstract = {Abstract:The growing complexity of modern computing frameworks has led to an increase in cybersecurity vulnerabilities reported to the National Vulnerability Database (NVD). Extracting meaningful trends from this vast amount of unstructured data is challenging without proper tools and methodologies. Existing approaches lack a holistic strategy for vulnerability mitigation and prediction and effective knowledge extraction from the Common Weakness Enumeration (CWE), Common Vulnerability Exposure (CVE), and Common Attack Pattern Enumeration and Classification (CAPEC) databases. We introduce the AI-enabled Hardware Weakness and Risk Exploration and Storytelling Framework with LLM-assisted Mitigation Suggestion (HWREx), designed to address hardware vulnerabilities and IoT security. Our architecture features an Ontology-driven Storytelling capability that automates ontology updates to track vulnerability patterns and evolution over time, while offering mitigation strategies. It also clarifies the complex interrelations among CVEs, CWEs, and CAPECs through interactive visual knowledge graphs. Our framework achieved accuracy rates of 62\% for CWE-CWE, 83\% for CWE-CVE, and 77\% for CWE-CAPEC linkage predictions. These graphs are instrumental for in-depth hardware weakness analysis and enable HWREx to deliver comprehensive assessments and actionable mitigation strategies. Additionally, HWREx utilizes Generative Pre-trained Transformers (GPT) to offer tailored mitigation suggestions.},
	journal = {ACM Trans. Des. Autom. Electron. Syst.},
	author = {Ghimire, Sujan and Lin, Yu-Zheng and Mamun, Muntasir and Chowdhury, Muhtasim Alam and Alemi, Farhad and Cai, Shuyu and Guo, Jinduo and Zhu, Mingyu and Li, Honghui and Saber Latibari, Banafsheh and Rafatirad, Setareh and Satam, Pratik and Salehi, Soheil},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural Language Processing (NLP), Internet of Things (IoT), Ontology Learning, Common Attack Pattern Enumeration and Classification (CAPEC), Common Vulnerability and Exposure (CVE), Common Weakness Enumeration (CWE), Electronic Design Automation (EDA), Hardware Security, Large Langauge Model (LLM), National Vulnerability Database (NVD)},
	annote = {Just Accepted},
}

@article{dou_shennongmgs_2025,
	title = {{ShennongMGS}: {An} {LLM}-based {Chinese} {Medication} {Guidance} {System}},
	volume = {16},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3658451},
	doi = {10.1145/3658451},
	abstract = {The rapidly evolving field of Large Language Models (LLMs) holds immense promise for healthcare, particularly in medication guidance and adverse drug reaction prediction. Despite their potential, existing LLMs face challenges in dealing with complex polypharmacy scenarios and often grapple with data lag issues. To address these limitations, we introduce an LLM-based Chinese medication guidance system, called ShennongMGS, specifically tailored for robust medication guidance and adverse drug reaction predictions. Our system transforms multi-source heterogeneous medication information into a knowledge graph and employs a two-stage training strategy to construct a specialized LLM (ShennongGPT). This method enables the simulation of professional pharmacists’ decision-making processes and incorporates the capability for knowledge self-updating, thereby significantly enhancing drug safety and the overall quality of medical services. Rigorously evaluated by medical professionals and artificial intelligence experts, our method demonstrates superiority, outperforming existing general and specialized LLMs in performance.},
	number = {2},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Dou, Yutao and Huang, Yuwei and Zhao, Xiongjun and Zou, Haitao and Shang, Jiandong and Lu, Ying and Yang, Xiaolin and Xiao, Jian and Peng, Shaoliang},
	month = mar,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large language model, natural language processing, Chinese medical system, medication guidance, model fine-tuning, software system},
}

@article{jayasundara_sok_2024,
	title = {{SoK}: {Access} {Control} {Policy} {Generation} from {High}-level {Natural} {Language} {Requirements}},
	volume = {57},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3706057},
	doi = {10.1145/3706057},
	abstract = {Administrator-centered access control failures can cause data breaches, putting organizations at risk of financial loss and reputation damage. Existing graphical policy configuration tools and automated policy generation frameworks attempt to help administrators configure and generate access control policies by avoiding such failures. However, graphical policy configuration tools are prone to human errors, making them unusable. On the other hand, automated policy generation frameworks are prone to erroneous predictions, making them unreliable. Therefore, to find ways to improve their usability and reliability, we conducted a Systematic Literature Review analyzing 49 publications. The thematic analysis of the publications revealed that graphical policy configuration tools are developed to write and visualize policies manually. Moreover, automated policy generation frameworks are developed using machine learning (ML) and natural language processing (NLP) techniques to automatically generate access control policies from high-level requirement specifications. Despite their utility in the access control domain, limitations of these tools, such as the lack of flexibility, and limitations of frameworks, such as the lack of domain adaptation, negatively affect their usability and reliability, respectively. Our study offers recommendations to address these limitations through real-world applications and recent advancements in the NLP domain, paving the way for future research.},
	number = {4},
	journal = {ACM Comput. Surv.},
	author = {Jayasundara, Sakuna Harinda and Gamagedara Arachchilage, Nalin Asanka and Russello, Giovanni},
	month = dec,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Access control, frameworks, policy engineering, reliability, system administrator, usability, user interfaces},
}

@inproceedings{ali_glossapi_2025,
	address = {New York, NY, USA},
	series = {{PCI} '24},
	title = {{GlossAPI}: {Architecturing} the {Greek} {Data} {Pile} for {LLM} development},
	isbn = {979-8-4007-1317-0},
	url = {https://doi.org/10.1145/3716554.3716557},
	doi = {10.1145/3716554.3716557},
	abstract = {With the release of large language models such as ChatGPT, there has been a surge in demand for national language data sources. Greek language resources, in particular, have become widely available and are frequently discussed in the context of LLM development for tailored or custom use. The development of large language models presents several challenges, one of which is data preparation, which must be of high quality and grounded in linguistic, conceptual, and historical foundations. Creating data sets of this nature is a highly complex task. This study focuses on the collection, curation, and management of data sets for the Greek language, as well as making the data provision for use in training custom Greek LLMs smoother, not only for custom LLM development but also for fine-tuning existing LLM models. Immediately after the ChatGPT invention, the GlossAPI project initiative started in 2023. This research encourages the need for a Greek data pile creation, which is further dependent upon the data collection, acquisition, data cleaning, annotation, and classification steps. Upon cleaning the data pile, storage of the data pile and serving it to the LLMs or other users is the main concern. This concern is resolved by the use of an integrative, capable database solution that can facilitate in-database inference, including data collection, annotation, classification, and storage. For each of these stages, a special protocol requires, for instance, the use of existing LLMs or other artificial intelligence models for data preprocessing, annotation, and classification. To store data, we need a database that can seamlessly integrate with other data resources. Our study aim at the development of an architecture to unite Greek data pile and intelligent AI models at a one place using MindsDB integrative capabilities. MindsDB is an emerging database with an integration functionality to wide variety of AI models which can facilitate the data processing, annotation, classification through the integration of Supervised, unsupervised, and even though LLMs models.},
	booktitle = {Proceedings of the 28th {Pan}-{Hellenic} {Conference} on {Progress} in {Computing} and {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Ali, Mohsan and Giallousi, Nina and Melidis, Alexandros and Alexopoulos, Charalampos and Charalabidis, Yannis},
	year = {2025},
	keywords = {Large Language Models, Natural Language Processing, Generative Pre-Trained Transformers, Large Scale Language Corpora, Pre-Trained Language Models},
	pages = {16--25},
}

@inproceedings{alharbi_experiment_2024,
	address = {New York, NY, USA},
	series = {{SAC} '24},
	title = {An {Experiment} in {Retrofitting} {Competency} {Questions} for {Existing} {Ontologies}},
	isbn = {979-8-4007-0243-3},
	url = {https://doi.org/10.1145/3605098.3636053},
	doi = {10.1145/3605098.3636053},
	abstract = {Competency Questions (CQs) are a form of ontology functional requirements expressed as natural language questions. Inspecting CQs together with the axioms in an ontology provides critical insights into the intended scope and applicability of the ontology. CQs also underpin a number of tasks in the development of ontologies e.g. ontology reuse, ontology testing, requirement specification, and the definition of patterns that implement such requirements. Although CQs are integral to the majority of ontology engineering methodologies, the practice of publishing CQs alongside the ontological artefacts is not widely observed by the community.In this context, we present an experiment in retrofitting CQs from existing ontologies. We propose RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using Generative AI. In the paper we present the pipeline that facilitates the extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its application to a number of existing ontologies.},
	booktitle = {Proceedings of the 39th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Alharbi, Reham and Tamma, Valentina and Grasso, Floriana and Payne, Terry},
	year = {2024},
	note = {event-place: Avila, Spain},
	keywords = {Ontology engineering, Large language model, Ontology, Language model, large language models, Ontology reuse, Competency question, ontology engineering, Computational linguistics, competency questions, Ontology's, Natural language processing systems, Functional requirement, Natural language questions, Retrofitting, Testing requirements, Requirements specifications},
	pages = {1650--1658},
	annote = {Cited by: 7; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{kantz_onset_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {{OnSET}: {Ontology} and {Semantic} {Exploration} {Toolkit}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3730148},
	doi = {10.1145/3726302.3730148},
	abstract = {Retrieval over knowledge graphs is typically performed using specialized, complex query languages such as SPARQL. We propose a novel system, Ontology and Semantic Exploration Toolkit (OnSET), that allows novice users to quickly build queries with visual user guidance provided by topic modeling and semantic search throughout the application. OnSET enables users without prior knowledge of the ontology or networked knowledge to start exploring topics of interest over knowledge graphs, including the retrieval and detailed exploration of prototypical sub-graphs and their instances. Existing systems either focus on direct graph exploration or do not foster further exploration of the result set. We, however, provide a node-based editor that can extend these missing properties of existing systems to support search over large ontologies with sub-graph instances. Furthermore, OnSET combines efficient and open platforms to deploy the system on commodity hardware.},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Kantz, Benedikt and Innerebner, Kevin and Waldert, Peter and Lengauer, Stefan and Lex, Elisabeth and Schreck, Tobias},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {ontology, visualization, natural language, graph retrieval, user guidance},
	pages = {3980--3984},
}

@inproceedings{chowdhury_community_2023,
	address = {New York, NY, USA},
	series = {{NLPIR} '22},
	title = {Community {Asset} {Ontology} for {Modeling} {Community} {Data} using {Information} {Extraction}},
	isbn = {978-1-4503-9762-9},
	url = {https://doi.org/10.1145/3582768.3582778},
	doi = {10.1145/3582768.3582778},
	abstract = {In this paper, we analyze some data-related challenges to building resilient and sustainable communities, particularly how to computationally model the social and economical dynamic that exists within a community. To that end, we propose the Community Asset Ontology (CAO) for a knowledge graph that can encapsulate community data as modeled in existing social science literature. We utilize existing information extraction paradigms to map natural language community data to CAO and evaluate the usefulness of such an ontology-based approach compared to a baseline open information extraction approach.},
	booktitle = {Proceedings of the 2022 6th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Chowdhury, Md Towhidul Absar and Sharma, Naveen},
	year = {2023},
	note = {event-place: Bangkok, Thailand},
	keywords = {ontologies, knowledge graph, information extraction},
	pages = {195--199},
}

@inproceedings{chomatek_words_2025,
	address = {New York, NY, USA},
	series = {{FSE} {Companion} '25},
	title = {From {Words} to {Wisdom}: {LLMs} {Summarizing} {Instructional} {Content}},
	isbn = {979-8-4007-1276-0},
	url = {https://doi.org/10.1145/3696630.3728697},
	doi = {10.1145/3696630.3728697},
	abstract = {This study explores the effectiveness of large language models (LLMs) in summarizing instructional video transcriptions, a key application in educational technology. We assessed nine LLMs using two prompts—a simple base prompt and an enhanced, structured prompt—across 62 instructional videos. Two evaluating models, gpt-4o-mini and gemini-1.5-flash, scored the summaries based on seven criteria tailored to instructional content: overall structure, presence of examples, availability of sources, relevance, coherence, narration, and ACCURACY. Results showed notable performance differences, with models like Mistral Large and Claude 3.5 Sonnet performing best, especially with the enhanced prompt. However, the enhanced prompt improved narrative quality at the expense of structural clarity in some cases. Evaluator bias was also observed, with gpt-4o-mini assigning higher scores than gemini-1.5-flash, highlighting the need for multiple evaluators. These findings underscore the role of prompt design and model choice in educational LLM applications and suggest future research into optimizing prompts and standardizing evaluation methods.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chomątek, Łukasz and Słabosz, Wojciech and Poniszewska-Marańda, Aneta},
	year = {2025},
	note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
	keywords = {large language models, experience, instructional materials, summarization},
	pages = {1623--1630},
}

@inproceedings{koziolek_llm-based_2024,
	address = {New York, NY, USA},
	series = {{LLM4Code} '24},
	title = {{LLM}-based and {Retrieval}-{Augmented} {Control} {Code} {Generation}},
	isbn = {979-8-4007-0579-3},
	url = {https://doi.org/10.1145/3643795.3648384},
	doi = {10.1145/3643795.3648384},
	abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Large} {Language} {Models} for {Code}},
	publisher = {Association for Computing Machinery},
	author = {Koziolek, Heiko and Grüner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
	year = {2024},
	note = {event-place: Lisbon, Portugal},
	keywords = {ChatGPT, large language models, GPT-4, code generation, DCS, IEC 61131-3, industrial automation, PLC},
	pages = {22--29},
}

@inproceedings{hagen_class-based_2021,
	address = {New York, NY, USA},
	series = {{AIMLSystems} '21},
	title = {Class-{Based} {Order}-{Independent} {Models} of {Natural} {Language} for {Bayesian} {Auto}-{Complete} {Inference}},
	isbn = {978-1-4503-8594-7},
	url = {https://doi.org/10.1145/3486001.3486240},
	doi = {10.1145/3486001.3486240},
	abstract = {We introduce a model for auto-complete of general queries via Bayesian inference. To that end, we address three issues: First, the problem of predicting a word given previous words in a text. Usually, the context words are treated as a directional sequence. In our approach, we introduce a set-based class language model with order-independence, modeling the context words as a set of classes. Second, towards the task of predicting the next word’s class based on the classes of previous words plus an incomplete word prefix, we present a Bayesian framework that incorporates the set-based class language model in conjunction with an ontology. Third, regarding the auto-complete problem, we provide complete query suggestions via abstract class-space search which determines similar historical queries that contain the classes of previous words plus the next word’s predicted class. Subsequently, we apply the model to auto-complete inference in a system setting, in which users can access data via natural language queries.},
	booktitle = {Proceedings of the {First} {International} {Conference} on {AI}-{ML} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hagen, Morten and Arora, Piyush and Ghosh, Rahul and Thomas, Dawn and Joshi, Salil R},
	year = {2021},
	note = {event-place: Bangalore, India},
	keywords = {Language model, Modeling languages, Bayesian networks, Computational linguistics, auto-complete, Bayesian inference, Class-based language model, order-independence, Natural languages, Natural language processing systems, Inference engines, Abstracting, Auto-complete, Class-based, Context-word, Independent model, Order independents, Order-independence},
	annote = {Cited by: 1},
}

@article{banerjee_automatic_2023,
	title = {Automatic {Resource} {Augmentation} for {Machine} {Translation} in {Low} {Resource} {Language}: {EnIndic} {Corpus}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3617371},
	doi = {10.1145/3617371},
	abstract = {Parallel corpus is the primary ingredient of machine translation. It is required to train the statistical machine translation (SMT) and neural machine translation (NMT) systems. There is a lack of good quality parallel corpus for Hindi to English. Comparable corpora for a given language pair are comparatively easy to find, but this cannot be used directly in SMT or NMT systems. As a result, we generate a parallel corpus from the comparable corpus. For this purpose, the sentences (which are translations of each other) are mined from the comparable corpus to prepare the parallel corpus. The proposed algorithm uses the length of the sentence and word translation model to align sentence pairs that are translations of each other. Then, the sentence pairs that are poor translations of each other (measured by a similarity score based on IBM model 1 translation probability) are filtered out. We apply this algorithm to comparable corpora, which are crawled from speeches of the President and Vice-President of India, and mined parallel corpora out of them. The prepared parallel corpus contains good quality aligned sentences (with 96.338\% f-score). Subsequently, incorrect sentence pairs are filtered out manually to make the corpus in qualitative practical use. Finally, we gather various sentences from different sources to prepare the EnIndic corpus, which comprises 1,656,207 English-Hindi sentence pairs (miscellaneous domain). We have deployed this prepared largest English-Hindi parallel corpus at https://github.com/debajyoty/EnIndic.git and the source code at https://github.com/debajyoty/EnIndicSourceCode.git.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Banerjee, Anasua and Kumar, Vinay and Shankar, Achyut and Jhaveri, Rutvij H. and Banik, Debajyoty},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Machine Translation, Comparable Corpus, Linguistic Resources and Natural Language Processing, Parallel Corpus},
	annote = {Just Accepted},
}

@article{fafalios_sealit_2023,
	title = {The {SeaLiT} {Ontology} – {An} {Extension} of {CIDOC}-{CRM} for the {Modeling} and {Integration} of {Maritime} {History} {Information}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3586080},
	doi = {10.1145/3586080},
	abstract = {We describe the construction and use of the SeaLiT Ontology, an extension of the ISO standard CIDOC-CRM for the modelling and integration of maritime history information. The ontology has been developed gradually, following a bottom-up approach that required the analysis of large amounts of real primary data (archival material) as well as knowledge and validation by domain experts (maritime historians). We present the specification of the ontology, RDFS and OWL implementations, as well as knowledge graphs that make use of this data model for integrating information originating from a large and diverse set of archival documents, such as crew lists, sailors registers, naval ship registers, and payrolls. We also describe an application that operates over these knowledge graphs and which supports historians in exploring and quantitatively analysing the integrated data through a user-friendly interface. Finally, we discuss aspects related to the use, evolution, and sustainability of the ontology.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Fafalios, Pavlos and Kritsotaki, Athina and Doerr, Martin},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontologies, data integration, semantic interoperability, CIDOC-CRM, maritime history},
}

@inproceedings{gui_natural_2024,
	address = {New York, NY, USA},
	series = {{IoT} '23},
	title = {A {Natural} {Language} {Interface} for {IoT} {Systems} {Using} the {Web} of {Things} {Abstraction}},
	isbn = {979-8-4007-0854-1},
	url = {https://doi.org/10.1145/3627050.3630732},
	doi = {10.1145/3627050.3630732},
	abstract = {We present a demo of a Natural Language Interface (NLI) for controlling Internet of Things (IoT) devices using the Web of Things (WoT) specification as an intermediate abstraction layer. All interaction information of a device is stored in a Knowledge Graph using the thing description ontology. The central component of the NLI is a sequence-to-sequence neural network model for text to code translation. We build a data corpus based on the functionalities of a Philips Hue smart lamp and use the corpus to train the text to code model. Our demonstration illustrates how to control the power state, the light colour, and the brightness of a Philips Hue smart lamp using natural language commands. The implementation of an NLI system based on the WoT specification represents an approach towards the development of easy-to-use and interoperable IoT systems.},
	booktitle = {Proceedings of the 13th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Gui, Zhou and Freund, Michael and Harth, Andreas},
	year = {2024},
	note = {event-place: Nagoya, Japan},
	keywords = {Knowledge Graphs, Web of Things, Natural Language Understanding, Text to Code},
	pages = {186--188},
}

@inproceedings{westhauser_caim_2025,
	address = {New York, NY, USA},
	series = {{IUI} '25 {Companion}},
	title = {{CAIM}: {A} {Cognitive} {AI} {Memory} {Framework} for {Long}-term {Interaction} with {LLMs}},
	isbn = {979-8-4007-1409-2},
	url = {https://doi.org/10.1145/3708557.3716342},
	doi = {10.1145/3708557.3716342},
	abstract = {The concept of cognitive artificial intelligence (AI) intends to simulate the human thought process in a computerized model. When applied as an extension to large language models (LLMs), cognitive AI has the potential to support fostering long-term relationships by improving the contextual relevance of generated responses. In this context, we propose CAIM, a framework inspired by cognitive AI principles. CAIM aims to enhance the memory capabilities of LLMs by integrating aspects of cognitive AI, such as thoughts, memory mechanisms, and decision-making. CAIM consists of three modules: 1.) The Memory Controller as central decision unit 2.) the Memory Retrieval, which filters relevant data for an interaction upon request, and 3.) the Post-Thinking, which maintains the memory storage. In this paper, we describe the core architecture of CAIM and outline potential extensions aiming to stimulate discussions around holistic memory modeling for LLMs inspired by cognitive AI.},
	booktitle = {Companion {Proceedings} of the 30th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Westhäußer, Rebecca and Zepf, Sebastian and Minker, Wolfgang},
	year = {2025},
	keywords = {Large Language Models, Cognitive AI, Long-term Memory},
	pages = {22--25},
}

@inproceedings{wu_structure-enhanced_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {Structure-{Enhanced} {Protein} {Instruction} {Tuning}: {Towards} {General}-{Purpose} {Protein} {Understanding} with {LLMs}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3737138},
	doi = {10.1145/3711896.3737138},
	abstract = {Proteins, as essential biomolecules, play a central role in biological processes, including metabolic reactions and DNA replication. Accurate prediction of their properties and functions is crucial in biological applications. Recent development of protein language models (pLMs) with supervised fine tuning provides a promising solution to this problem. However, the fine-tuned model is tailored for particular downstream prediction task, and achieving general-purpose protein understanding remains a challenge. In this paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT) framework to bridge this gap. Our approach incorporates a novel structure-aware module into pLMs to enrich their structural knowledge, and subsequently integrates these enhanced pLMs with large language models (LLMs) to advance protein understanding. In this framework, we propose a novel instruction tuning pipeline. First, we warm up the enhanced pLMs using contrastive learning and structure denoising. Then, caption-based instructions are used to establish a basic understanding of proteins. Finally, we refine this understanding by employing a mixture of experts (MoEs) to capture more complex properties and functional information with the same number of activated parameters. Moreover, we construct the largest and most comprehensive protein instruction dataset to date, which allows us to train and evaluate the general-purpose protein understanding model. Extensive experiments on both open-ended generation and closed-set answer tasks demonstrate the superior performance of SEPIT over both closed-source general LLMs and open-source LLMs trained with protein knowledge.},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Wu, Wei and Wang, Chao and Chen, Liyi and Yin, Mingze and Zhu, Yiheng and Fu, Kun and Ye, Jieping and Xiong, Hui and Wang, Zheng},
	year = {2025},
	note = {event-place: Toronto ON, Canada},
	keywords = {large language models, protein, insturction tuning},
	pages = {3216--3227},
}

@inproceedings{bekmanova_development_2023,
	address = {New York, NY, USA},
	series = {{ICEMT} '23},
	title = {Development of an {Ontological} {Model} of {Words} in {Public} {Political} {Discourse}},
	isbn = {979-8-4007-0914-2},
	url = {https://doi.org/10.1145/3625704.3625720},
	doi = {10.1145/3625704.3625720},
	abstract = {The aim of the research is to develop methods for analyzing political discourse in social networks in the Kazakh language in order to identify official and unofficial information sources of political discourse, as well as to determine the mood of the discussion in these sources. The article presents an ontological model of the subject area of elections, a referendum.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Education} and {Multimedia} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Bekmanova, Gulmira and Omarbekova, Assel and Mukanova, Assel and Zulkhazhav, Altanbek and Zakirova, Alma and Ongarbayev, Yerkin},
	year = {2023},
	note = {event-place: Tokyo, Japan},
	keywords = {Artificial intelligence, ontology, knowledge base, discourse, formalization},
	pages = {362--367},
}

@inproceedings{jiao_retrieval_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Retrieval and {Structuring} {Augmented} {Generation} with {LLMs} for {Web} {Applications}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715870},
	doi = {10.1145/3701716.3715870},
	abstract = {Although Large Language Models (LLMs) have revolutionized natural language processing, they face significant challenges in web applications, including hallucinations, outdated knowledge, and limited specialization in niche domains. To address these issues, this tutorial explores how integrating retrieval mechanisms and structured knowledge can enhance LLM performance for web use. By leveraging Retrieval-Augmented Generation (RAG), we can ground LLM outputs with relevant external data, mitigating limitations in applications like search engines, chatbots, and recommendation systems. We delve into text structuring techniques-such as taxonomy construction, multi-level text classification, and taxonomy-guided information retrieval-that improve the effectiveness of information retrieval processes. Furthermore, we examine how structure-guided augmented generation through information extraction and knowledge graph construction can reduce hallucinations and enhance factual accuracy. By bridging the gap between unstructured language models and structured knowledge, we aim to unlock new potentials for dynamic web content generation and personalized user experiences. Finally, we highlight future directions for seamlessly integrating retrieval and generation, enhancing personalization, and incorporating multimodal data to expand the capabilities of LLMs in web applications.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Jiao, Yizhu and Ouyang, Siru and Zhong, Ming and Zhang, Yunyi and Ding, Linyi and Zhou, Sizhe and Han, Jiawei},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language models, text mining, retrieval augmented generation, text structuring},
	pages = {25--28},
}

@inproceedings{huang_enabling_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Enabling semantic interoperability of asset administration shells through an ontology-based modeling method},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561606},
	doi = {10.1145/3550356.3561606},
	abstract = {Digital twin technology establishes the future development vision for Industry 4.0, and is also an important exploration direction for the Model-Driven Engineering (MDE) paradigm. Because it builds a more flexible and communicative production system through models that spans life cycle, hierarchy and architecture. The standard proposed under the concept of Industry 4.0, the Asset Administration Shell (AAS), provides a syntactic interoperability interface for all assets involved in smart factories. However, there is still a need to fill the gap regarding semantic interoperability, in order to allow efficient interactions between Industry 4.0 components. Ontologies are a good candidate because they provide formal semantics expressed using a knowledge representation language, and in addition, there are many associated mature tools for reasoning and inference. Therefore, we propose a modeling approach that provides semantic interoperability for AAS-based digital twins using ontologies.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Yining and Dhouib, Saadia and Medinacelli, Luis Palacios and Malenfant, Jacques},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {ontology, semantic interoperability, digital twins, industry 4.0, model-driven engineering, smart manufacturing, asset administration shell},
	pages = {497--502},
}

@inproceedings{dong_ontology_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Ontology {Enrichment} from {Texts}: {A} {Biomedical} {Dataset} for {Concept} {Discovery} and {Placement}},
	isbn = {979-8-4007-0124-5},
	url = {https://doi.org/10.1145/3583780.3615126},
	doi = {10.1145/3583780.3615126},
	abstract = {Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based methods.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Horrocks, Ian},
	year = {2023},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {Ontology, Language model, text mining, Ontology enrichment, Taxonomies, language models, SNOMED CT, Biomedical ontologies, biomedical ontologies, SNOMED-CT, Computational linguistics, Entity linking, ontology enrichment, entity linking, Text-mining, concept placement, Natural language processing systems, Concept discoveries, Concept placement, Automated approach, Knowledge taxonomies, Large dataset},
	pages = {5316--5320},
	annote = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{degbelo_prolegomena_2025,
	title = {Prolegomena to a {Description} {Language} for {GenAI} {Tools} in {Cities}},
	volume = {6},
	url = {https://doi.org/10.1145/3652952},
	doi = {10.1145/3652952},
	abstract = {The potential of generative AI has been recently demonstrated through different applications. The open government and smart city initiatives can leverage this potential to produce innovations that improve government workflows and the lives of citizens. This commentary makes the case for a description language enabling the structured documentation of these upcoming innovations. The description language would facilitate the communication between governments, citizens, and innovators. The key elements of the description language are briefly sketched and its usefulness is shown by the generation of ideas for GenAI tools related to interactive maps in cities.},
	number = {1},
	journal = {Digit. Gov.: Res. Pract.},
	author = {Degbelo, Auriol},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Smart cities, human-computer interaction, GenAI tools, interactive maps, metadata generation, open government},
}

@article{xue_integrating_2023,
	title = {Integrating {Heterogeneous} {Ontologies} in {Asian} {Languages} {Through} {Compact} {Genetic} {Algorithm}\&nbsp;with {Annealing} {Re}-sample {Inheritance} {Mechanism}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3519298},
	doi = {10.1145/3519298},
	abstract = {An ontology is a state-of-the-art knowledge modeling technique in the natural language domain, which has been widely used to overcome the linguistic barriers in Asian and European countries’ intelligent applications. However, due to the different knowledge backgrounds of ontology developers, the entities in the ontologies could be defined in different ways, which hamper the communications among the intelligent applications built on them. How to find the semantic relationships among the entities that are lexicalized in different languages is called the Cross-lingual Ontology Matching problem (COM), which is a challenge problem in the ontology matching domain. To face this challenge, being inspired by the success of the Genetic Algorithm\&nbsp;(GA) in the ontology matching domain, this work proposes a Compact GA with Annealing Re-sample Inheritance mechanism (CGA-ARI) to efficiently address the COM problem. In particular, a Cross-lingual Similarity Metric (CSM) is presented to distinguish two cross-lingual entities, a discrete optimal model is built to define the COM problem, and the compact encoding mechanism and the Annealing Re-sample Inheritance mechanism (ARI) are introduced to improve CGA’s searching performance. The experiment uses Multifarm track to test CGA-ARI’s performance, which includes 45 ontology pairs in different languages. The experimental results show that CGA-ARI is able to significantly improve the performance of GA and CGA and determine better alignments than state-of-the-art ontology matching systems.},
	number = {3},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Xue, Xingsi and Liu, Wenyu},
	month = mar,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Annealing Re-sample Inheritance Mechanism, compact genetic algorithm, Cross-lingual ontology alignment},
}

@inproceedings{yang_visual_2025,
	address = {New York, NY, USA},
	series = {{ICIEAI} '24},
	title = {A {Visual} {Analysis} of the {Hotspots} of the {National} {Security} and {Language} {Studies} in the {Context} of {Big} {Data}},
	isbn = {979-8-4007-1173-2},
	url = {https://doi.org/10.1145/3724504.3724548},
	doi = {10.1145/3724504.3724548},
	abstract = {This paper explores the convergence of national security and language, a critical but under-explored area of contemporary scholarship. It adopts a visual analytical approach, using bibliometric techniques and CiteSpace software to analyse 2,039 journal articles from the Web of Science Core Collection. All of these articles have been published over the past 38 years (1986-2024) on the topic of national security and language. The aim of this paper is to map knowledge using big data mining, to identify research hotspots in the field of language and national security, and to visualize the results using knowledge mapping software. The analysis focuses on publication time, keyword co-occurrence, citation bursts and their clustering relationships. The findings reveal that: (1)there has been a significant rise in academic interest in the field of language and national security since 1986, with a notable increase after 2014, which underscores the growing awareness of the critical role of language in national security discourse and practice; (2) the research hotspots of national security and language research are divided into nine main clusters, namely, \#0 food security, \#1 European Union, \#2politics, \#3 discourse analysis, \#4 national language processing, \#5 security, \#6 national security, \#7 national identity and \#8 globalization. Future research should aim to further explore the intersection of language and national security by incorporating a wider range of sources, including non-English literature, in order to enrich the understanding of this field and to examine the potential impact of emerging technologies such as artificial intelligence and machine learning on the dynamics of language in the security contexts.},
	booktitle = {Proceedings of the 2024 2nd {International} {Conference} on {Information} {Education} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Xiaoqing and Liang, Xiaobo},
	year = {2025},
	keywords = {Language, CiteSpace, Bibliometric Analysis, National Security},
	pages = {267--272},
}

@inproceedings{yang_llm_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{LLM} as {Auto}-{Prompt} {Engineer}: {Automated} {NER} {Prompt} {Optimisation}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3717818},
	doi = {10.1145/3701716.3717818},
	abstract = {The emergence of Large Language Models (LLMs) has revolutionised natural language processing capabilities. However, despite these advances, effectively optimising prompts for knowledge extraction tasks like Named Entity Recognition (NER) remains challenging. This paper presents a zero-shot automated prompt engineering approach that decomposes the NER task into two phases: entity boundary detection and entity classification. Our method incorporates structured task analysis, automated prompt generation, test case generation, and iterative optimisation, requiring no labelled training examples. This decomposition allows for more precise entity recognition while maintaining the efficiency. Through experimentation on the CoNLL-2003 dataset using standard exact-match evaluation metrics, our approach demonstrates improvements over unified methods, achieving a 75.39\% F1 score compared to baseline approaches (72.90\%). The key contributions include: (1) A structured pipeline for zero-shot automated prompt engineering in NER tasks that addresses the challenges of prompt design and optimisation; (2) A two-phase approach to NER tasks that separates boundary detection from entity classification; and (3) Experimental results demonstrating the effectiveness of our approach compared to existing zero-shot approaches in NER tasks.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Yang, Can and Pereira Nunes, Bernardo and Rodríguez Méndez, Sergio},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language models, named entity recognition, automated prompt engineering, prompt optimisation},
	pages = {2574--2578},
}

@article{omar_dialogue_2025,
	title = {Dialogue {Benchmark} {Generation} from {Knowledge} {Graphs} with {Cost}-{Effective} {Retrieval}-{Augmented} {LLMs}},
	volume = {3},
	url = {https://doi.org/10.1145/3709681},
	doi = {10.1145/3709681},
	abstract = {Dialogue benchmarks are crucial in training and evaluating chatbots engaging in domain-specific conversations. Knowledge graphs (KGs) represent semantically rich and well-organized data spanning various domains, such as DBLP, DBpedia, and YAGO. Traditionally, dialogue benchmarks have been manually created from documents, neglecting the potential of KGs in automating this process. Some question-answering benchmarks are automatically generated using extensive preprocessing from KGs, but they do not support dialogue generation. This paper introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation platform for automatically generating high-quality dialogue benchmarks tailored to a specific domain using a KG. Chatty-Gen decomposes the generation process into manageable stages and uses assertion rules for automatic validation between stages. Our approach enables control over intermediate results to prevent time-consuming restarts due to hallucinations. It also reduces reliance on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront processing of the entire KG using efficient query-based retrieval to find representative subgraphs based on the dialogue context. Our experiments with several real and large KGs demonstrate that Chatty-Gen significantly outperforms state-of-the-art systems and ensures consistent model and system performance across multiple LLMs of diverse capabilities, such as GPT-4o, Gemini 1.5, Llama 3, and Mistral.},
	number = {1},
	journal = {Proc. ACM Manag. Data},
	author = {Omar, Reham and Mangukiya, Omij and Mansour, Essam},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {benchmarking, assertion-based validation, conversational question answering, cost-effecive inference, graph serialization, knowledge graphs (kgs), large language models (llms), retrieval-augumented generation (rag)},
}

@inproceedings{deng_more_2023,
	address = {New York, NY, USA},
	series = {{W4A} '23},
	title = {A {More} {Accessible} {Web} with {Natural} {Language} {Interface}},
	isbn = {979-8-4007-0748-3},
	url = {https://doi.org/10.1145/3587281.3587700},
	doi = {10.1145/3587281.3587700},
	abstract = {The past decade has witnessed the rapid growth and evolution of the Web. Today, people can perform a multitude of tasks through the use of a single browser. Despite the immense power and capabilities of the Web, its increasing complexity and the overwhelming amount of information pose significant challenges for users to easily access the information they need and achieve their goals. Especially for those who are less technologically proficient or have disabilities. In this work, we propose to tackle this issue by building a general natural language interface for the Web, which enables users to express their needs in natural language and have the system carry out the arduous actions. We examine the key components crucial for building the natural language interface. On top of that, we present our ongoing efforts in curating a new benchmark dataset covering a diverse range of websites and tasks, and establishing baselines to demonstrate the feasibility of building such a system.},
	booktitle = {Proceedings of the 20th {International} {Web} for {All} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Deng, Xiang},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {Natural Language Interface, Web Accessibility},
	pages = {153--155},
}

@inproceedings{hoseini_end--end_2025,
	address = {New York, NY, USA},
	series = {{DEEM} '25},
	title = {End-{To}-{End} {ML} with {LLMs} and {Semantic} {Data} {Management}: {Experiences} from {Chemistry} 4.0},
	isbn = {979-8-4007-1924-0},
	url = {https://doi.org/10.1145/3735654.3735942},
	doi = {10.1145/3735654.3735942},
	abstract = {Machine Learning (ML) in industrial chemistry is often hindered by the complexity of preprocessing heterogeneous datasets. In this proof-of-concept study, we explore the use of semantic data management to support LLM-driven automation of end-to-end ML pipelines in a real-world Chemistry 4.0 setting. A semantic model is used to capture domain knowledge and metadata in a machine-readable form, guiding LLMs through natural language prompts to generate complete data wrangling and ML modeling code. We evaluate several state-of-the-art LLMs on their ability to autonomously produce functionally correct Python code for preprocessing and Gaussian Process modeling. Our results show that, when guided by structured semantic context, larger LLMs can reliably generate accurate pipelines, significantly reducing the need for manual intervention. These findings provide an encouraging starting point for further exploration toward leveraging the semantic model to improve the robustness of code generation by systematically integrating relevant information into the generation process, rather than relying solely on the raw intelligence of the LLM.},
	booktitle = {Proceedings of the {Workshop} on {Data} {Management} for {End}-to-{End} {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Hoseini, Sayed and Herrmann, Vincent and Quix, Christoph},
	year = {2025},
	note = {event-place: Berlin, Germany},
	keywords = {LLMs, Semantic Data Management, AutoML, Data Wrangling},
}

@inproceedings{burgueno_human_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {A {Human} {Behavior} {Exploration} {Approach} {Using} {LLMs} for {Cyber}-{Physical} {Systems}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3687806},
	doi = {10.1145/3652620.3687806},
	abstract = {In the early phases of Cyber-Physical Systems (CPS) development, scoping human behavior plays a significant role, especially when interactions extend beyond expected behavior. Here, it is especially challenging to develop cases that capture the full spectrum of human behavior. Up to now, identifying such behavior of humans remains a task for domain experts. We explore how one can use Large Languages Models (LLMs) in the design phase of systems to provide additional information about human-CPS interaction. Our approach proposes a preliminary ontology describing a hierarchy of types of behavior and relevant CPS components as input for prompt templates. It uses them to generate parts of human behavior descriptions, as well as a canned prompt with one variable about behavior. For demonstration, we take a smart building with a Home Energy System as the use case.An initial user evaluation shows that the behavior descriptions generated with standard and ontology-driven prompts complement each other and are useful when assisting humans. The discovered uncommon behaviors can be used to complete interaction scenarios that eventually result in a more robust CPS implementation.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Burgueño, Lola and Keet, Maria and Kienzle, Jörg and Michael, Judith and Babur, Önder},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {Large language model, Ontology, Language model, large language models, digital twin, cyber-physical systems, Cyber-physical systems, System development, human behavior, user scenario, Ontology's, Cybe-physical systems, Behaviour descriptions, Hierarchical systems, Human behaviors, Scoping, User scenario},
	pages = {578--586},
	annote = {Cited by: 0},
}

@inproceedings{yamani_leveraging_2025,
	address = {New York, NY, USA},
	series = {{PROMISE} '25},
	title = {Leveraging {LLMs} for {User} {Stories} in {AI} {Systems}: {UStAI} {Dataset}},
	isbn = {979-8-4007-1594-5},
	url = {https://doi.org/10.1145/3727582.3728689},
	doi = {10.1145/3727582.3728689},
	abstract = {AI systems are gaining widespread adoption across various sectors and domains. Creating high-quality AI system requirements is crucial for aligning the AI system with business goals and consumer values and for social responsibility. However, with the uncertain nature of AI systems and the heavy reliance on sensitive data, more research is needed to address the elicitation and analysis of AI systems requirements. With the proprietary nature of many AI systems, there is a lack of open-source requirements artifacts and technical requirements documents for AI systems, limiting broader research and investigation. With Large Language Models (LLMs) emerging as a promising alternative to human-generated text, this paper investigates the potential use of LLMs to generate user stories for AI systems based on abstracts from scholarly papers. We conducted an empirical evaluation using three LLMs and generated 1260 user stories from 42 abstracts from 26 domains. We assess their quality using the Quality User Story (QUS) framework. Moreover, we identify relevant non-functional requirements (NFRs) and ethical principles. Our analysis demonstrates that the investigated LLMs can generate user stories inspired by the needs of various stakeholders, offering a promising approach for generating user stories for research purposes and for aiding in the early requirements elicitation phase of AI systems. We have compiled and curated a collection of stories generated by various LLMs into a dataset (UStAI), which is now publicly available for use.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Predictive} {Models} and {Data} {Analytics} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yamani, Asma and Baslyman, Malak and Ahmed, Moataz},
	year = {2025},
	note = {event-place: Trondheim, Norway},
	keywords = {large language models, User stories, quality requirements, requirements elicitation, requirements generation},
	pages = {21--30},
}

@inproceedings{zeng_cross-language_2022,
	address = {New York, NY, USA},
	series = {{WSSE} '22},
	title = {Cross-{Language} {Vocabulary} {Teaching} {Based} on {Information} {Network} {Technology}: {The} {Example} of {Loanwords}},
	isbn = {978-1-4503-9695-0},
	url = {https://doi.org/10.1145/3568364.3568388},
	doi = {10.1145/3568364.3568388},
	abstract = {Loanwords have always been a hot topic in the study and teaching of Chinese as a foreign language, but there are also many different views and debates. This article summarises and outlines the hotspots of research into and teaching of loanwords in linguistics in recent decades and discusses the current issues of language teaching and cross-linguistic education in the context of the new COVID-19 pandemic in relation to information network technology. First, the concept and scope of loanwords are defined, and the rationale of loanwords is discussed from the perspective of word formation. On this basis, the characteristics of loanwords are summarised, the online teaching of loanwords in the context of the new pandemic is discussed from the perspective of information technology and the Sinicization of loanwords is discussed from the perspective of language development.},
	booktitle = {Proceedings of the 4th {World} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zeng, Jinghan},
	year = {2022},
	note = {event-place: Xiamen, China},
	keywords = {information-based teaching, language education, loanwords, word-building rationale},
	pages = {155--160},
}

@inproceedings{ma_seek_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Seek {Inner}: {LLM}-{Enhanced} {Information} {Mining} for {Medical} {Visual} {Question} {Answering}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3717556},
	doi = {10.1145/3701716.3717556},
	abstract = {Medical visual question answering (Med-VQA) focuses on analyzing medical images to accurately respond to clinicians' specific questions. Although integrating prior knowledge can enhance VQA reasoning, current methods often struggle to extract relevant information from the vast and complex medical knowledge base, thereby limiting the models' ability to learn domain-specific features. To overcome this limitation, our study presents a novel information mining approach that leverages large language models (LLMs) to efficiently retrieve pertinent data. Specifically, we design a latent knowledge generation module that employs LLMs to separately extract and filter information from questions and answers, enhancing the model's inference capabilities. Furthermore, we propose a multi-level prompt fusion module in which an initial prompt interacts with the extracted latent knowledge to draw clinically relevant details from both unimodal and multimodal features. Experimental results demonstrate that our approach outperforms current state-of-the-art models on multiple Med-VQA benchmark datasets.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Ma, Ao and Li, Zhiyuan and Liang, Zhuonan and Gu, Tiancheng and Fan, Jianan and Long, Jieting and Müller, Henning and Cai, Weidong},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language models, knowledge injection, medical visual question answering, multimodal learning},
	pages = {2297--2305},
}

@inproceedings{zhang_align_2025,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '25},
	title = {Align with {Me}, {Not} {TO} {Me}: {How} {People} {Perceive} {Concept} {Alignment} with {LLM}-{Powered} {Conversational} {Agents}},
	isbn = {979-8-4007-1395-8},
	url = {https://doi.org/10.1145/3706599.3720126},
	doi = {10.1145/3706599.3720126},
	abstract = {Concept alignment—building a shared understanding of concepts—is essential for human and human-agent communication. While large language models (LLMs) promise human-like dialogue capabilities for conversational agents, the lack of studies to understand people’s perceptions and expectations of concept alignment hinders the design of effective LLM agents. This paper presents results from two lab studies with human-human and human-agent pairs using a concept alignment task. Quantitative and qualitative analysis reveals and contextualizes potentially (un)helpful dialogue behaviors, how people perceived and adapted to the agent, as well as their preconceptions and expectations. Through this work, we demonstrate the co-adaptive and collaborative nature of concept alignment and identify potential design factors and their trade-offs, sketching the design space of concept alignment dialogues. We conclude by calling for designerly endeavors on understanding concept alignment with LLMs in context, as well as technical efforts to combine theory-informed and LLM-driven approaches.},
	booktitle = {Proceedings of the {Extended} {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Shengchen and Guo, Weiwei and Sun, Xiaohua},
	year = {2025},
	keywords = {Large Language Models, Conversational Agents, Concept Alignment, Grounding, Human-Agent Interaction},
}

@inproceedings{ferrero_narrative-driven_2025,
	address = {New York, NY, USA},
	series = {{RecSys} '25},
	title = {Narrative-{Driven} {Itinerary} {Recommendation}: {LLM} {Integration} for {Immersive} {Urban} {Walking}},
	isbn = {979-8-4007-1364-4},
	url = {https://doi.org/10.1145/3705328.3748751},
	doi = {10.1145/3705328.3748751},
	abstract = {Sedentary behavior, dubbed the disease of the 21st century, is a ubiquitous force driving chronic illness. Yet, traditional itinerary and Point-of-Interest (POI) Recommender Systems (RSs) lack engaging elements that motivate routine urban walking. This research proposes a novel framework combining narrative-driven storytelling with location-based RSs to promote physical activity and immersive urban exploration. This approach introduces a bidirectional alignment between POI and itinerary recommendations and LLM-generated narratives, transforming routine urban walks into dynamic journeys where contextually relevant stories unfold across city locations. Unlike sequential POI recommendations, this framework embeds location suggestions within contextually relevant narratives of various genres, simultaneously promoting health benefits and deeper city exploration. The research addresses three research questions using a method that builds a structured knowledge base by extracting entities (e.g., POIs, and characters) and semantic links from narrative corpora, enabling semantic alignment between recommended physical locations and story elements. The core aspects of this work are: (i) context-aware itinerary recommendations and personalized story generation, (ii) bidirectional mapping between RSs and story generation, and (iii) systems design bridging user’s needs to promote urban walking as a health activity. Evaluation employs comparative user studies measuring quality and engagement, route-narrative semantic alignment, and narrative analysis to validate the integrated proposed approach.},
	booktitle = {Proceedings of the {Nineteenth} {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ferrero, Fabio},
	year = {2025},
	keywords = {Large Language Models, Geographically Anchored Interactive Narratives., Walking Route Recommender Systems},
	pages = {1485--1491},
}

@article{an_domain-slot_2022,
	title = {Domain-{Slot} {Relationship} {Modeling} {Using} a {Pre}-{Trained} {Language} {Encoder} for {Multi}-{Domain} {Dialogue} {State} {Tracking}},
	volume = {30},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2022.3181350},
	doi = {10.1109/TASLP.2022.3181350},
	abstract = {Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. As using pre-trained language models is the de facto standard for natural language processing tasks, many recent studies use them to encode the dialogue context for predicting the dialogue states. Model architectures that have certain inductive biases for modeling the relationship among different domain-slot pairs are also emerging. Our work is based on these research approaches on multi-domain dialogue state tracking. We propose a model architecture that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special \&lt;inline-formula\&gt;\&lt;tex-math notation="LaTeX"\&gt;[CLS]\&lt;/tex-math\&gt;\&lt;/inline-formula\&gt; token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results on the datasets MultiWOZ-2.0 and MultiWOZ-2.1 show that our model outperforms other models with the same setting. Our ablation studies incorporate three main parts. The first component shows the effectiveness of our approach exploiting the relationship modeling. The second component compares the effect of using different pre-trained language encoders. The final component involves comparing different initialization methods that could be used for the special tokens. Qualitative analysis of the attention map of the pre-trained language encoder shows that our special tokens encode relevant information through the encoding process by attending to each other.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {An, Jinwon and Cho, Sungzoon and Bang, Junseong and Kim, Misuk},
	month = jun,
	year = {2022},
	note = {Publisher: IEEE Press},
	pages = {2091--2102},
}

@inproceedings{zhao_ontology-guided_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {Ontology-{Guided} {Knowledge} {Graph} {Retrieval} for {Multi}-{Hop} and {Cross}-{Granularity} {Store} {Fulfillment} {Queries}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3731964},
	doi = {10.1145/3726302.3731964},
	abstract = {Answering complex queries in store fulfillment, such as ”What percentage of employee-assigned actions remain unresolved?” or ”How many worklists for a specific product type were completed within a timeframe at each location?” requires precise, multi-hop reasoning across datasets with varying granularities. This paper introduces an ontology-based knowledge graph (KG) approach integrated with a structured text-to-Cypher generation pipeline, enabling accurate retrieval for such queries. Benchmarking against a robust hybrid search baseline combining BM25 and semantic search, our method demonstrates superior performance in addressing multi-hop and cross-granularity questions. Leveraging a KG schema designed to capture intricate relationships (e.g. (OrderLineItem)-[:INVOLVES\_ACTION]-\&gt;(Action)-[:INVOLVES]-\&gt;(BatchProcess)-[:IS\_COMPLETED\_AT]-\&gt;(Location)), we reveal universal patterns for constructing and querying highly relational data. This work highlights the transformative potential of ontology-driven KGs to improve reasoning, data aggregation, and decision-making, with broader implications for any domain requiring structured, multi-relational data analysis.},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Mengyue and Nokleby, Matthew and Shen, Bo and Dong, Wenbo and Pachauri, Deepti and Yang, Andrew},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {large language model, Knowledge graphs, Knowledge graph, Large language model, Ontology, Natural language processing, Language model, Semantics, natural language processing, Knowledge management, Information retrieval, knowledge graph, Query processing, Decision making, Data aggregation, Computational linguistics, Language processing, hybrid search, multi-hop reasoning, ontology-driven retrieval, store fulfillment., text-to-cypher, Natural languages, Ontology's, Natural language processing systems, Human engineering, Hybrid search, Multi-hop reasoning, Multi-hops, Ontology-driven retrieval, Store fulfillment, Text-to-cipher},
	pages = {4360--4364},
	annote = {Cited by: 0},
}

@inproceedings{okgetheng_named_2024,
	address = {New York, NY, USA},
	series = {{NLPIR} '23},
	title = {Named {Entity} {Recognition} for {Setswana} {Language}: {A} conditional {Random} {Fields} ({CRF}) {Approach}},
	isbn = {979-8-4007-0922-7},
	url = {https://doi.org/10.1145/3639233.3639234},
	doi = {10.1145/3639233.3639234},
	abstract = {Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) focused on identifying entities like individuals, organizations, and locations within text. Locating these entities can present initial challenges, and subsequent classification can be equally daunting. This complexity is exemplified in Setswana, where shared naming between locations and personal names adds an extra layer of intricacy. This study introduces a Setswana NER approach, featuring a Setswana Regex Annotator (SERxA) for preliminary entity classification, followed by BRAT tool annotation. Employing the Conditional Random Fields (CRF) algorithm, we establish a supervised statistical machine learning NER model for Setswana. Evaluation using standard metrics on a held-out test set attains impressive F1-scores of 0.94 for person entities and 0.79 for location entities. Our findings underscore the viability of NER in Setswana and emphasize the necessity of nurturing NLP resources for less-resourced languages.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Okgetheng, Boago and Malema, Gabofetswe},
	year = {2024},
	note = {event-place: Seoul, Republic of Korea},
	pages = {240--244},
}

@inproceedings{ihsan_diso_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {{DISO}: {A} {Domain} {Ontology} for {Modeling} {Dislocations} in {Crystalline} {Materials}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3578739},
	doi = {10.1145/3555776.3578739},
	abstract = {Crystalline materials, such as metals and semiconductors, nearly always contain a special defect type called dislocation. This defect decisively determines many important material properties, e.g., strength, fracture toughness, or ductility. Over the past years, significant effort has been put into understanding dislocation behavior across different length scales via experimental characterization techniques and simulations. This paper introduces the dislocation ontology (DISO), which defines the concepts and relationships related to linear defects in crystalline materials. We developed DISO using a top-down approach in which we start defining the most general concepts in the dislocation domain and subsequent specialization of them. DISO is published through a persistent URL following W3C best practices for publishing Linked Data. Two potential use cases for DISO are presented to illustrate its usefulness in the dislocation dynamics domain. The evaluation of the ontology is performed in two directions, evaluating the success of the ontology in modeling a real-world domain and the richness of the ontology.},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Ihsan, Ahmad Zainul and Fathalla, Said and Sandfeld, Stefan},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {ontology, linked data, crystallographic defects, dislocation, materials science and engineering},
	pages = {1746--1753},
}

@inproceedings{priyanka_rani_systematically_2025,
	address = {New York, NY, USA},
	series = {L@{S} '25},
	title = {Systematically {Identifying}, {Defining} and {Organizing} {Knowledge} {Components} for {Data} {Science} {Problem} {Solving} through {Human}-{LLM} {Collaboration}},
	isbn = {979-8-4007-1291-3},
	url = {https://doi.org/10.1145/3698205.3733952},
	doi = {10.1145/3698205.3733952},
	abstract = {As demand grows for job-ready data science professionals, there is increasing recognition that traditional training often falls short in cultivating the higher-order reasoning and real-world problem-solving skills essential to the field. A foundational step toward addressing this gap is the identification and organization of knowledge components (KCs) that underlie data science problem solving (DSPS). KCs represent conditional knowledge-knowing about appropriate actions given particular contexts or conditions-and correspond to the critical decisions data scientists must make throughout the problem-solving process. While existing taxonomies in data science education support curriculum development, they often lack the granularity and focus needed to support the assessment and development of DSPS skills. In this paper, we present a novel framework that combines the strengths of large language models (LLMs) and human expertise to identify, define, and organize KCs specific to DSPS. We treat LLMs as ”knowledge engineering assistants” capable of generating candidate KCs by drawing on their extensive training data, which includes a vast amount of domain knowledge and diverse sets of real-world DSPS cases. Our process involves prompting multiple LLMs to generate decision points, synthesizing and refining KC definitions across models, and using sentence-embedding models to infer the underlying structure of the resulting taxonomy. Human experts then review and iteratively refine the taxonomy to ensure validity. This human-AI collaborative workflow offers a scalable and efficient proof-of-concept for LLM-assisted knowledge engineering. The resulting KC taxonomy lays the groundwork for developing fine-grained assessment tools and adaptive learning systems that support deliberate practice in DSPS. Furthermore, the framework illustrates the potential of LLMs not just as content generators but as partners in structuring domain knowledge to inform instructional design. Future work will involve extending the framework by generating a directed graph of KCs based on their input-output dependencies and validating the taxonomy through expert consensus and learner studies. This approach contributes to both the practical advancement of DSPS coaching in data science education and the broader methodological toolkit for AI-supported knowledge engineering.},
	booktitle = {Proceedings of the {Twelfth} {ACM} {Conference} on {Learning} @ {Scale}},
	publisher = {Association for Computing Machinery},
	author = {Priyanka Rani, FNU and Alomair, Maryam and Pan, Shimei and Chen, Lujie K.},
	year = {2025},
	note = {event-place: Palermo, Italy},
	keywords = {large language models, knowledge engineering, data science education, data science problem solving, domain analysis, knowledge components, problem solving},
	pages = {341--345},
}

@inproceedings{venkatakrishnan_semantic_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Semantic interlinking of {Immigration} {Data} using {LLMs} for {Knowledge} {Graph} {Construction}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3651557},
	doi = {10.1145/3589335.3651557},
	abstract = {The challenge of managing immigration data is exacerbated by its reliance on paper-based, evidence-driven records maintained by legal professionals, creating obstacles for efficient processing and analysis due to inherent trust issues with AI-based systems. This paper introduces a cutting-edge framework to surmount these hurdles by synergizing Large Language Models (LLMs) with Knowledge Graphs (KGs), revolutionizing traditional data handling methods. Our method transforms archaic, paper-based immigration records into a structured, interconnected knowledge network that intricately mirrors the legal and procedural nuances of immigration, ensuring a dynamic and trustworthy platform for data analysis. Utilizing LLMs, we extract vital entities and relationships from diverse legal documents to forge a comprehensive knowledge graph, encapsulating the complex legalities and procedural disparities in immigration processes and mapping the multifaceted interactions among stakeholders like applicants, sponsors, and legal experts. This graph not only facilitates a deep dive into the legal stipulations but also incorporates them, significantly boosting the system's reliability and precision. With the integration of Retrieval Augmented Generation (RAG) for exact, context-aware data retrieval and Augmented Knowledge Creation for developing a conversational interface via LLMs, our framework offers a scalable, adaptable solution to immigration data management. This innovative amalgamation of LLMs, KGs, and RAG techniques marks a paradigm shift towards more informed, efficient, and trustworthy decision-making in the sphere of global migration, setting a new benchmark for legal technology and data source management.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Venkatakrishnan, Radhakrishnan and Tanyildizi, Emrah and Canbaz, M. Abdullah},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {knowledge graphs, large language models, information retrieval, data restructuring, document processing, legal tech},
	pages = {605--608},
}

@article{sethi_pragmatic_2023,
	title = {A {Pragmatic} {Analysis} of {Machine} {Translation} {Techniques} for {Preserving} the {Authenticity} of the {Sanskrit} {Language}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3610582},
	doi = {10.1145/3610582},
	abstract = {Machine Translation has been a field of study for over six decades, but it has acquired substantial prominence in the last decade as processing capacity in personal computers has increased. The purpose of this paper is to discuss the usage of Sanskrit as a source, target, or supporting language in various Machine Translation systems. To investigate Machine Translation, researchers use a variety of strategies, including corpus-based, direct, and rule-based approaches. The primary goal of employing Sanskrit in Machine Translation is to evaluate its appropriateness, lexicon, and performance when proper Machine Translation methods are used. The research examines various modelling strategies for developing a machine translation system, specifically Statistical and Neural Machine Translation, in order to bridge the gap between Sanskrit and its current successor, Hindi. Interpretations are formed in Statistical Machine Translation by matching words from the source and target languages with statistical models and bilingual text corpora to learn parameters. Neural Machine Translation, on the other hand, uses an artificial neural network to predict the likelihood of a word sequence, frequently modelling entire phrases within a single integrated model. Neural Machine Translation is implemented using an encoder-decoder architecture with an attention mechanism. One of the most significant contributions of this paper is the use of different data sources, data collecting, and scraping to create a complete dataset. According to the study's findings, Neural Machine Translation outperforms the Statistical Machine Translation modelling technique. Furthermore, the paper examines the distinctive qualities of the Sanskrit language as well as the difficulties encountered by researchers in digesting Sanskrit while constructing the machine translation system. This study investigates the use of Sanskrit in Machine Translation and analyses several modelling methods, such as Statistical and Neural Machine Translation. The paper emphasizes the advantages of Neural Machine Translation and discusses the unique characteristics and challenges of the Sanskrit language in machine translation development.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Sethi, Nandini and Dev, Amita and Bansal, Poonam and Sharma, Deepak Kumar and Gupta, Deepak},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural Language Processing (NLP), Bilingual Dictionary, Corpus-Based Machine Translation (CBMT), Interlingua, Parallel Corpora, part of speech (POS), Sanskrit},
	annote = {Just Accepted},
}

@inproceedings{morales_dsl_2024,
	address = {New York, NY, USA},
	series = {{MODELS} '24},
	title = {A {DSL} for {Testing} {LLMs} for {Fairness} and {Bias}},
	isbn = {979-8-4007-0504-5},
	url = {https://doi.org/10.1145/3640310.3674093},
	doi = {10.1145/3640310.3674093},
	abstract = {Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose LangBiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Morales, Sergio and Clarisó, Robert and Cabot, Jordi},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {Large Language Models, Ethics, Model-Driven Engineering, Bias, Testing, Domain-Specific Language, Red Teaming},
	pages = {203--213},
}

@article{zhang_sciencebenchmark_2023,
	title = {{ScienceBenchmark}: {A} {Complex} {Real}-{World} {Benchmark} for {Evaluating} {Natural} {Language} to {SQL} {Systems}},
	volume = {17},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3636218.3636225},
	doi = {10.14778/3636218.3636225},
	abstract = {Natural Language to SQL systems (NL-to-SQL) have recently shown improved accuracy (exceeding 80\%) for natural language to SQL query translation due to the emergence of transformer-based language models, and the popularity of the Spider benchmark. However, Spider mainly contains simple databases with few tables, columns, and entries, which do not reflect a realistic setting. Moreover, complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems.In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL benchmark for three real-world, highly domain-specific databases. For this new benchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for each domain. To garner more data, we extended the small amount of human-generated data with synthetic data generated using GPT-3. We show that our benchmark is highly challenging, as the top performing systems on Spider achieve a very low performance on our benchmark. Thus, the challenge is many-fold: creating NL-to-SQL systems for highly complex domains with a small amount of hand-made training data augmented with synthetic data. To our knowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with complex real-world scientific databases, containing challenging training and test data carefully validated by domain experts.},
	number = {4},
	journal = {Proc. VLDB Endow.},
	author = {Zhang, Yi and Deriu, Jan and Katsogiannis-Meimarakis, George and Kosten, Catherine and Koutrika, Georgia and Stockinger, Kurt},
	month = dec,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {685--698},
}

@inproceedings{khurana_table_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Table {Retrieval} using {LLMs} and {Semantic} {Table} {Similarity}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715558},
	doi = {10.1145/3701716.3715558},
	abstract = {Searching for relevant tables in response to a textual phrase or a question is an important task for large tabular data repositories, such as relational databases, CSV files in data lakes, etc. It is somewhat different from the problem of web document search because the subjects of search are tables rather than documents, while the query remains textual. In this paper, we explore a novel technique for table search on large repositories using natural language queries. It is based on a generative methodology that aims to maximize the semantic connection between the query and the resulting tables. Unlike traditional keyword search approaches, our technique can find the needed tables more effectively through deeper semantic concept discovery rather than simply searching for exact keyword matches. Additionally, our technique supports natural language queries rather than plain keyword queries. In this paper, we describe the core ideas, implementation, and effectiveness of our method using two different benchmarks with diverse queries.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Khurana, Udayan and Suneja, Sahil and Samulowitz, Horst},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language models, semantic similarity, generative ai, structured data search, table retrieval},
	pages = {1072--1076},
}

@inproceedings{zhu_metadata_2025,
	address = {New York, NY, USA},
	series = {{RecSys} '25},
	title = {Metadata {Generation} and {Evaluation} using {LLMs} - {Case} {Study} on {Canonical} {Titles}},
	isbn = {979-8-4007-1364-4},
	url = {https://doi.org/10.1145/3705328.3748100},
	doi = {10.1145/3705328.3748100},
	abstract = {In online job search platforms, autocomplete plays a crucial role in providing immediate, structured suggestions that guide users through their query process. However, inconsistencies in job title expressions, such as ’sr data scientist’ versus ’data scientist senior’, or embellished forms such as ’superstar software engineer’, can undermine the quality of autocomplete suggestions and diminish user satisfaction. Traditional normalization methods rely on manually curated vocabularies, which are labor intensive and often insufficient to capture the diverse variations in raw job titles.We present an automated and scalable framework for canonical title generation that leverages large language models (LLMs) alongside embedding-based similarity measures to derive normalized job titles directly from raw data. Our approach generalizes to domains with unstructured or inconsistently formatted titles (e.g. product catalogs or course titles): we systematically remove irrelevant information, enforce a consistent format, and eliminate overly generic or redundant titles by combining LLM-generated normalization with a two-stage deduplication process. Our method demonstrates significant improvements in normalization quality, with offline accuracy gains of 18.6\% over baseline methods and online A/B tests showing an improvement of 160\% in user engagement metrics.},
	booktitle = {Proceedings of the {Nineteenth} {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Sinan and Simonovikj, Sanja and Edmonds, Darren and Sun, Yang},
	year = {2025},
	keywords = {canonical job titles, data cleaning with LLMs, occupation taxonomy},
	pages = {1010--1013},
}

@inproceedings{ouyang_ontology_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Ontology {Enrichment} for {Effective} {Fine}-grained {Entity} {Typing}},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671857},
	doi = {10.1145/3637528.3671857},
	abstract = {Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly given the massive scale of data. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose øurs, where we (1) enrich each node in the ontology structure with two categories of extra information:instance information for training sample augmentation andtopic information to relate types with contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that øurs achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods. øurs also enjoys strong transferability to unseen and finer-grained types. We will open source this work upon acceptance.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ouyang, Siru and Huang, Jiaxin and Pillai, Pranav and Zhang, Yunyi and Zhang, Yu and Han, Jiawei},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {Ontology, Language model, Ontology enrichment, Modeling languages, language models, Fine-grained entity typing, Natural language inference, Supervised learning, Zero-shot learning, Language inference, fine-grained entity typing, natural language inference, zero-shot learning, Natural languages, Ontology's, Natural language processing systems, Inference engines, Fine grained, Human annotations, Training sample},
	pages = {2318--2327},
	annote = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{almuntashiri_using_2025,
	address = {New York, NY, USA},
	series = {{PW}' 25},
	title = {Using {LLMs} to infer provenance information},
	isbn = {979-8-4007-1941-7},
	url = {https://doi.org/10.1145/3736229.3736261},
	doi = {10.1145/3736229.3736261},
	abstract = {Having a provenance record facilitates data reuse and experimental reuse. However, provenance capture requires either: specific provenance-enabled systems to be used or human documentation. While there have been many examples of provenance-enabled systems for scientific usage, they are still the exception, not the norm. The one, standard place for provenance information of scientific experiments remains the scientific publication. Unfortunately, provenance buried in text is not immediately useful for computational purposes. Large Language Models (LLMs) have demonstrated exceptional capability across various tasks, particularly in information extraction. In this paper, we explore the potential of LLMs to infer a provenance record for scientific experiments from scientific papers. We develop an extractor, identify the most effective prompt for provenance extraction. Our results emphasise the capability of ChatGPT-4o in accessing and extracting provenance information from biomedical research papers. Additionally, we assess the scalability of the extractor for use in extracting provenance information across a set of biomedical research papers.},
	booktitle = {Proceedings of the {ProvenanceWeek} 2025},
	publisher = {Association for Computing Machinery},
	author = {Almuntashiri, Abdullah Hamed and Ibáñez, Luis-Daniel and Chapman, Adriane},
	year = {2025},
	keywords = {LLMs, Provenance, Information Extraction, Dataset Search},
	pages = {1--10},
}

@article{al-thubaity_novel_2025,
	title = {A {Novel} {Dataset} for {Arabic} {Domain} {Specific} {Term} {Extraction} and {Comparative} {Evaluation} of {BERT}-{Based} {Models} for {Arabic} {Term} {Extraction}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3748323},
	doi = {10.1145/3748323},
	abstract = {Automatic term extraction from domain-specific corpora is a well-known challenge in natural language processing, with applications in machine translation, information retrieval, text classification, ontology building, and thesaurus construction. Unlike English, where various approaches have been explored, Arabic automatic term extraction has relied heavily on rule-based or statistical methods due to the lack of annotated datasets. This paper introduces the first annotated dataset for Arabic automatic term extraction (AraATE) in the field of Arabic linguistics. AraATE comprises 4,148 sentences and 155,502 tokens, annotated with 4,362 single and multi-word Arabic linguistic terms. The dataset covers diverse areas of Arabic linguistics, including lexicography, semantics, pragmatics, phonetics, and semiotics. Additionally, this paper presents the results of fine-tuning five BERT-based models using AraATE. The findings indicate that AraBERTv0.2-base, CAMeLBERT-MSA, and AraBERTv0.2-large exhibit comparable F1 scores (0.82, 0.81, and 0.81). However, no statistically significant difference was observed in the performance of these models. The availability of AraATE will facilitate Arabic term extraction by serving as a benchmarking dataset for different approaches. Nevertheless, the field still requires additional benchmarking datasets that cover other domains.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Al-Thubaity, AbdulMohsen},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {BERT, information extraction, fine-tuning language models, and natural language processing, Arabic term extraction},
	annote = {Just Accepted},
}

@inproceedings{wang_research_2024,
	address = {New York, NY, USA},
	series = {{AIMSCM} '23},
	title = {Research on {Natural} {Language} {Recognition} {Processing} {System} in {Computer} {Intelligent} {Graphics}},
	isbn = {979-8-4007-0825-1},
	url = {https://doi.org/10.1145/3648050.3648051},
	doi = {10.1145/3648050.3648051},
	abstract = {This paper proposes a natural language processing method based on grammar rule matching. Then the construction model of NLP system based on this algorithm is given. Set up an interactive editing environment to compress and encode the recorded information and upload it to the server. After cross-editing, it is input into the embedded system to complete the identification process and then achieve the identification of the session semantic block, the identification of the session topic, and the extraction of the session content. The proposed method is tested systematically, and the results show that the accuracy of the proposed method reaches 93\%.},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {AI} and {Metaverse} in {Supply} {Chain} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Shengyao},
	year = {2024},
	note = {event-place: Bhubaneswar, India},
	keywords = {Natural language, Speech recognition, Embedded system, Grammar rule matching, Parameter extraction},
}

@article{xu_research_2023,
	title = {Research on the {Diversification} of {Language} {Ability} in {Applied} {Linguistics} and {Foreign} {Linguistics} {Based} on {New} {Media}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3617993},
	doi = {10.1145/3617993},
	abstract = {In order to improve the research effect of the diversification of language ability in applied linguistics and foreign linguistics, this paper conducts research on the diversification of language ability in applied linguistics and foreign linguistics based on new media and intelligent data processing technology. In the process of language learning data processing, combining the adjacency degree and the network structure entropy, this paper proposes the adjacency structure entropy based on the super network to identify the key nodes in the super network. Moreover, this paper applies this indicator to the competitive hypernetwork, and accurately obtains the key nodes of the hypernetwork. In addition, this paper constructs a scientific research cooperation hypernetwork, and applies the adjacency structure entropy in the hypernetwork to this dataset. From the experimental research, it can be seen that the algorithm proposed in this paper can play a certain role in the diversification analysis of linguistic ability in linguistics and foreign linguistics. At the same time, the model proposed in this paper has a certain effect on the diversification of language ability in applied linguistics and foreign linguistics.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Xu, Jin},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {applied linguistics, diversification, foreign linguistics, language ability, new media},
	annote = {Just Accepted},
}

@inproceedings{liu_nalspatial_2023,
	address = {New York, NY, USA},
	series = {{SIGSPATIAL} '23},
	title = {{NALSpatial}: {An} {Effective} {Natural} {Language} {Transformation} {Framework} for {Queries} over {Spatial} {Data}},
	isbn = {979-8-4007-0168-9},
	url = {https://doi.org/10.1145/3589132.3625600},
	doi = {10.1145/3589132.3625600},
	abstract = {Spatial databases play a vital role in many applications that access spatial data via appropriate queries. However, most application users lack the expertise necessary for formulating spatial queries. To fill in this gap, we propose an effective framework called NALSpatial that translates natural language queries over spatial data into executable database queries. NALSpatial consists of two core phases. The natural language understanding phase extracts key entity information, comprehends the query intent and determines the query type. The key entities and query type are passed to the subsequent natural language translation phase, which employs entity mapping rules and structured language models to construct executable database queries accordingly. We implement NALSpatial on the open-source extensible database system SECONDO to support range queries, nearest neighbor queries, spatial joins and aggregation queries. Extensive experiments show that NALSpatial on average achieves response time of about 2.5 seconds, translatability of 95\% and translation precision of 92\%, outperforming state-of-the-art natural language transformation methods.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Mengyi and Wang, Xieyang and Xu, Jianqiu and Lu, Hua},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {query processing, natural language interface, semantic parsing, spatial database},
}

@inproceedings{nordin_ontology-based_2023,
	address = {New York, NY, USA},
	series = {{ICSCA} '23},
	title = {An {Ontology}-based {Modeling} for {Classifying} {Risk} of {Suicidal} {Behavior}},
	isbn = {978-1-4503-9858-9},
	url = {https://doi.org/10.1145/3587828.3587840},
	doi = {10.1145/3587828.3587840},
	abstract = {Classifying an individual with suicidal behavior is a complex problem. A clinical decision support system (CDSS) helps medical experts in their daily work and supports them in effective decision-making. The huge amount of medical information and the complex correlation between the risk factors and the level of risk for suicidal behavior makes the representation of data is challenging. Therefore, this paper proposes an ontology-based modeling to classify an individual with at-risk of suicidal behavior for effective clinical decision support system. The case study is conducted to evaluate the ontology model and provides a general approach to knowledge sharing and reusing knowledge for suicide risk prevention and management. The finding shows that the ontology model can be used as a knowledge base for classification, and it is suitable to capture medical knowledge, detailed concepts, and relationships in a formal way using Web Ontology Language (OWL). The results of the proposed ontology model in terms of accuracy, specificity, and sensitivity are 83\%, 84\%, and 82\% respectively.},
	booktitle = {Proceedings of the 2023 12th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Nordin, Noratikah and Zainol, Zurinahni and Mohd Noor, Mohd Halim and Chan, Lai Fong},
	year = {2023},
	note = {event-place: Kuantan, Malaysia},
	keywords = {Ontological Model, Data Mining, Knowledge Base, Clinical Decision Support System, Suicidal Behavior},
	pages = {71--76},
}

@inproceedings{a_abdelnabi_algorithmic_2021,
	address = {New York, NY, USA},
	series = {{ICEMIS}'21},
	title = {An {Algorithmic} {Approach} for {Generating} {Behavioral} {UML} {Models} {Using} {Natural} {Language} {Processing}},
	isbn = {978-1-4503-9044-6},
	url = {https://doi.org/10.1145/3492547.3492612},
	doi = {10.1145/3492547.3492612},
	abstract = {The process of transformation from informal requirements stated in natural language into a formal specification such as Unified Modeling Language (UML) is an important challenge. User requirements that are expressed in natural language can be very problematic, which makes the requirements analysis a difficult task. In this paper, we propose a method to analyze the natural language requirements and generate sequence and collaboration diagrams from these requirements, which are commonly used to describe the behavior of software systems. A case study was accomplished to compare the diagrams generated by the proposed approach to the diagrams produced by other approaches. The results showed that the elements of the sequence and collaboration diagrams extracted through our approach are very satisfactory and they would be acceptable as initial analysis models.},
	booktitle = {The 7th {International} {Conference} on {Engineering} \&amp; {MIS} 2021},
	publisher = {Association for Computing Machinery},
	author = {A. Abdelnabi, Esra and M. Maatuk, Abdelsalam and M. Abdelaziz, Tawfig},
	year = {2021},
	note = {event-place: Almaty, Kazakhstan},
	keywords = {Requirement analysis, NLP tools, Sequence and Collaboration diagrams, UML diagrams},
}

@inproceedings{steenstra_virtual_2024,
	address = {New York, NY, USA},
	series = {{IVA} '24},
	title = {Virtual {Agents} for {Alcohol} {Use} {Counseling}: {Exploring} {LLM}-{Powered} {Motivational} {Interviewing}},
	isbn = {979-8-4007-0625-7},
	url = {https://doi.org/10.1145/3652988.3673932},
	doi = {10.1145/3652988.3673932},
	abstract = {We introduce a novel application of large language models (LLMs) in developing a virtual counselor capable of conducting motivational interviewing (MI) for alcohol use counseling. Access to effective counseling remains limited, particularly for substance abuse, and virtual agents offer a promising solution by leveraging LLM capabilities to simulate nuanced communication techniques inherent in MI. Our approach combines prompt engineering and integration into a user-friendly virtual platform to facilitate realistic, empathetic interactions. We evaluate the effectiveness of our virtual agent through a series of studies focusing on replicating MI techniques and human counselor dialog. Initial findings suggest that our LLM-powered virtual agent matches human counselors’ empathetic and adaptive conversational skills, presenting a significant step forward in virtual health counseling and providing insights into the design and implementation of LLM-based therapeutic interactions.},
	booktitle = {Proceedings of the 24th {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author = {Steenstra, Ian and Nouraei, Farnaz and Arjmand, Mehdi and Bickmore, Timothy},
	year = {2024},
	note = {event-place: GLASGOW, United Kingdom},
	keywords = {Large Language Models, Alcohol Use Counseling, Embodied Conversational Agents, Intelligent Virtual Agents, Motivational Interviewing, Persuasive Technology},
}

@inproceedings{zhao_teaching_2025,
	address = {New York, NY, USA},
	series = {{ICETM} '24},
	title = {Teaching {Practice} of {C} {Language} {Programming} under the {Constructivist} {Learning} {Paradigm}},
	isbn = {979-8-4007-1746-8},
	url = {https://doi.org/10.1145/3711403.3711485},
	doi = {10.1145/3711403.3711485},
	abstract = {The construction of new engineering disciplines and the “Double Tops” initiative have put forward new requirements for the training of engineering professionals. Taking the course of C language programming as an example, this paper analyzes the pain points in current course teaching, combines the characteristics of students’ learning situation, takes students as the center, and based on the knowledge graph, an effective tool for organizing resources, carries out a modular teaching practice of C language programming design under the constructivist learning paradigm. Integrating the OBE (Outcomes-Based Education) concept into teaching, it enhances students’ practical ability, cultivates their computational thinking, and highlights the training of students’ ability to analyze and solve complex engineering problems and creative thinking. The exploration and practice of this course teaching can provide reference and reference for the teaching design of other programming courses.},
	booktitle = {Proceedings of the 2024 7th {International} {Conference} on {Educational} {Technology} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Wei and Nie, Zhenhua and Li, Xiaoming},
	year = {2025},
	keywords = {constructivist learning, Program design, teaching mode, teaching reform and practice},
	pages = {504--508},
}

@inproceedings{gould_chattldr_2024,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '24},
	title = {{ChatTL};{DR} – {You} {Really} {Ought} to {Check} {What} the {LLM} {Said} on {Your} {Behalf}},
	isbn = {979-8-4007-0331-7},
	url = {https://doi.org/10.1145/3613905.3644062},
	doi = {10.1145/3613905.3644062},
	abstract = {Interactive large language models (LLMs) are so hot right now, and are probably going to be hot for a while. There are lots of problems exciting challenges created by mass use of LLMs. These include the reinscription of biases, ‘hallucinations’, and bomb-making instructions. Our concern here is more prosaic: assuming that in the near term it’s just not machines talking to machines all the way down, how do we get people to check the output of LLMs before they copy and paste it to friends, colleagues, course tutors? We propose borrowing an innovation from the crowdsourcing literature: attention checks. These checks (e.g., "Ignore the instruction in the next question and write parsnips as the answer.") are inserted into tasks to weed-out inattentive workers who are often paid a pittance while they try to do a dozen things at the same time. We propose ChatTL;DR1, an interactive LLM that inserts attention checks into its outputs. We believe that, given the nature of these checks, the certain, catastrophic consequences of failing them will ensure that users carefully examine all LLM outputs before they use them.},
	booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gould, Sandy J. J. and Brumby, Duncan P. and Cox, Anna L.},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Large Language Models, LLMs, academics being hilarious, attention checks, checking behaviour, computers-talking-to-computers-all-the-way-down-circlejerk, error detection, human factors, instructional manipulation checks, that-bloody-automatic-lane-assist-ffs},
}

@inproceedings{bui_cross-data_2024,
	address = {New York, NY, USA},
	series = {{AIQAM} '24},
	title = {Cross-{Data} {Knowledge} {Graph} {Construction} for {LLM}-enabled {Educational} {Question}-{Answering} {System}: {A} {Case} {Study} at {HCMUT}},
	isbn = {979-8-4007-0547-2},
	url = {https://doi.org/10.1145/3643479.3662055},
	doi = {10.1145/3643479.3662055},
	abstract = {In today's rapidly evolving landscape of Artificial Intelligence, large language models (LLMs) have emerged as a vibrant research topic. LLMs find applications in various fields and contribute significantly. Despite their powerful language capabilities, similar to pre-trained language models (PLMs), LLMs still face challenges in remembering events, incorporating new information, and addressing domain-specific issues or hallucinations. To overcome these limitations, researchers have proposed Retrieval-Augmented Generation (RAG) techniques, some others have proposed the integration of LLMs with Knowledge Graphs (KGs) to provide factual context, thereby improving performance and delivering more accurate feedback to user queries.Education plays a crucial role in human development and progress. With the technology transformation, traditional education is being replaced by digital or blended education. Therefore, educational data in the digital environment is increasing day by day. Data in higher education institutions are diverse, comprising various sources such as unstructured/structured text, relational databases, web/app-based API access, etc. Constructing a Knowledge Graph from these cross-data sources is not a simple task. This article proposes a method for automatically constructing a Knowledge Graph from multiple data sources and discusses some initial applications (experimental trials) of KG in conjunction with LLMs for question-answering tasks.},
	booktitle = {Proceedings of the 1st {ACM} {Workshop} on {AI}-{Powered} {Q}\&amp;{A} {Systems} for {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Bui, Tuan and Tran, Oanh and Nguyen, Phuong and Ho, Bao and Nguyen, Long and Bui, Thang and Quan, Tho},
	year = {2024},
	note = {event-place: Phuket, Thailand},
	keywords = {Large language model, Education, Knowledge Graph, Open Intent Discovery, Question-Answering System},
	pages = {36--43},
}

@inproceedings{zheng_maaqr_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {{MAAQR}: {An} {LLM}-based {Multi}-{Agent} {Framework} for {Adaptive} {Query} {Rewriting} in {Alipay} {Search}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3731950},
	doi = {10.1145/3726302.3731950},
	abstract = {Query rewriting is essential in e-commerce search, as it bridges the lexical gap between user queries and item descriptions, thereby enhancing search performance. Despite recent advancements, current rewriting approaches are still limited by an inadequate comprehension of domain-specific knowledge and a lack of mechanisms for adaptive refinement in response to new or changing query-item relationships. To overcome these limitations, we propose a large language model (LLM) based Multi-Agent Framework for Adaptive Query Rewriting (MAAQR) in Alipay Search. Initially, we perform knowledge-enhanced fine-tuning to improve the LLM's understanding of query and item semantics. Subsequently, a multi-agent collaborative rewriting architecture is employed to enhance rewrite quality and adaptability. MAAQR has been successfully deployed to serve Alipay's mini-app search since December 2024. Through offline experiments and online A/B testing, MAAQR significantly improves click-through rates (CTR) and the number of transactions for target queries, while substantially reducing the zero-results rate (ZRR).},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zheng, Qi and Zhong, Mingjie and Gong, Saisai and Jiang, Huimin and Wu, Kaixin and Liu, Hong and Xu, Jia and Mo, Linjian},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {large language models, information retrieval, query rewriting, multi-agent},
	pages = {4289--4293},
}

@article{xie_tencentllmeval_2025,
	title = {{TencentLLMEval}: {A} {Hierarchical} {Evaluation} of {Real}-{World} {Capabilities} for {Human}-{Aligned} {LLMs}},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3732784},
	doi = {10.1145/3732784},
	abstract = {Large language models (LLMs) have shown impressive capabilities across various natural language tasks. However, evaluating their alignment with human preferences remains a challenge. To this end, we propose a comprehensive human evaluation framework to assess LLMs’ proficiency in following instructions on diverse real-world tasks. We construct a hierarchical task tree encompassing 7 major areas covering over 200 categories and over 800 tasks, which covers diverse capabilities such as question answering, reasoning, multiturn dialogue, and text generation, to evaluate LLMs in a comprehensive and in-depth manner. We also design detailed evaluation standards and processes to facilitate consistent, unbiased judgments from human evaluators. A test set of over 3,000 instances is released, spanning different difficulty levels and knowledge domains. Our work provides a standardized methodology to evaluate human alignment in LLMs for both English and Chinese. We also analyze the feasibility of automating parts of evaluation with a strong LLM (GPT-4). Our framework supports a thorough assessment of LLMs as they are integrated into real-world applications. We have made publicly available the task tree, TencentLLMEval dataset, and evaluation methodology which have been demonstrated as effective in assessing the performance of Tencent Hunyuan LLMs 1. By doing so, we aim to facilitate the benchmarking of advances in the development of safe and human-aligned LLMs.},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Xie, Shuyi and Yao, Wenlin and Dai, Yong and Wang, Shaobo and Xu, Zishan and Lin, Fan and Zhou, Donglin and Jin, Lifeng and Feng, Xinhua and Wei, Pengzhi and Lin, Yujie and Hu, Zhichao and Yu, Dong and Zhang, Zhengyou and Nie, Jing and Liu, Yuhong},
	month = apr,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {LLMs, Evaluation, human-aligned},
	annote = {Just Accepted},
}

@inproceedings{abrouk_ontofic_2024,
	address = {New York, NY, USA},
	series = {{ASONAM} '23},
	title = {{OntoFiC} : an ontology for financial fraud detection and customer behavior modeling},
	isbn = {979-8-4007-0409-3},
	url = {https://doi.org/10.1145/3625007.3631503},
	doi = {10.1145/3625007.3631503},
	abstract = {Fraud detection is a complex issue for financial institutions. They must have tools for the prevention and detection of fraud. In this article, we present our approach to detect fraudulent transactions in SWIFT network based on the domain ontology. Firstly, we present the OntoFiC ontology constructed for the modeling of SWIFT transactions and actors. This ontology is populated with a real dataset. We developed our rules-based approach with rules associated to fraud scenarios to label our transactions as legitimate or fraudulent. Finally, we made SPARQL requests to visualize these transactions through graphs. Our work is part of a collaboration project with a financial company, SKAIZen Group.},
	booktitle = {Proceedings of the 2023 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Abrouk, Lylia and Chergui, Hamza and Ahaggach, Hamid},
	year = {2024},
	note = {event-place: Kusadasi, Turkiye},
	keywords = {SPARQL, domain ontology, fraud detection, ontology population, rules-based approach, SWIFT network},
	pages = {509--513},
}

@inproceedings{castiglione_towards_2023,
	address = {New York, NY, USA},
	series = {{ARES} '23},
	title = {Towards {Grammatical} {Tagging} for the {Legal} {Language} of {Cybersecurity}},
	isbn = {979-8-4007-0772-8},
	url = {https://doi.org/10.1145/3600160.3605069},
	doi = {10.1145/3600160.3605069},
	abstract = {Legal language can be understood as the language typically used by those engaged in the legal profession and, as such, it may come both in spoken or written form. Recent legislation on cybersecurity obviously uses legal language in writing, thus inheriting all its interpretative complications due to the typical abundance of cases and sub-cases as well as to the general richness in detail. This paper faces the challenge of the essential interpretation of the legal language of cybersecurity, namely of the extraction of the essential Parts of Speech (POS) from the legal documents concerning cybersecurity. The challenge is overcome by our methodology for POS tagging of legal language. It leverages state-of-the-art open-source tools for Natural Language Processing (NLP) as well as manual analysis to validate the outcomes of the tools. As a result, the methodology is automated and, arguably, general for any legal language following minor tailoring of the preprocessing step. It is demonstrated over the most relevant EU legislation on cybersecurity, namely on the NIS 2 directive, producing the first, albeit essential, structured interpretation of such a relevant document. Moreover, our findings indicate that tools such as SpaCy and ClausIE reach their limits over the legal language of the NIS 2.},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Castiglione, Gianpietro and Bella, Giampaolo and Santamaria, Daniele Francesco},
	year = {2023},
	note = {event-place: Benevento, Italy},
	keywords = {NLP, Privacy, POS tagging, Act, Data Protection, Pronouncement},
}

@article{king_sasha_2024,
	title = {Sasha: {Creative} {Goal}-{Oriented} {Reasoning} in {Smart} {Homes} with {Large} {Language} {Models}},
	volume = {8},
	url = {https://doi.org/10.1145/3643505},
	doi = {10.1145/3643505},
	abstract = {Smart home assistants function best when user commands are direct and well-specified—e.g., "turn on the kitchen light"—or when a hard-coded routine specifies the response. In more natural communication, however, human speech is unconstrained, often describing goals (e.g., "make it cozy in here" or "help me save energy") rather than indicating specific target devices and actions to take on those devices. Current systems fail to understand these under-specified commands since they cannot reason about devices and settings as they relate to human situations. We introduce large language models (LLMs) to this problem space, exploring their use for controlling devices and creating automation routines in response to under-specified user commands in smart homes. We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness. We address these gaps with Sasha, a smarter smart home assistant. Sasha responds to loosely-constrained commands like "make it cozy" or "help me sleep better" by executing plans to achieve user goals—e.g., setting a mood with available devices, or devising automation routines. We implement and evaluate Sasha in a hands-on user study, showing the capabilities and limitations of LLM-driven smart homes when faced with unconstrained user-generated scenarios.},
	number = {1},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	author = {King, Evan and Yu, Haoxiang and Lee, Sangsu and Julien, Christine},
	month = mar,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {large language models, ambient intelligence, pervasive computing, smart environments},
}

@inproceedings{zine_llm-based_2025,
	address = {New York, NY, USA},
	series = {{SPLC}-{A} '25},
	title = {{LLM}-based {Co}-{Evolution} of {Configurable} {Software} {Systems}},
	isbn = {979-8-4007-2024-6},
	url = {https://doi.org/10.1145/3744915.3748460},
	doi = {10.1145/3744915.3748460},
	abstract = {Software Product Lines (SPLs) and s are de\&nbsp;facto standards for managing variability in software systems. However, maintaining an up-to-date during software evolution is particularly challenging. Ensuring its consistency with the artifacts of an SPL requires co-evolving them alongside the developed system. When performed manually, this co-evolution process is tedious and error-prone, highlighting the need for automated support. Yet, little attention has been given to the automation of co-evolution between and source code. In this paper, we explore the potential of open-source s to fill this gap. Specifically, we investigate the extent to which s can support bidirectional co-evolution: from to source code—where modifications in the drive changes in the code—and from source code to —where updates in the code are reflected back into the. We evaluate our -based approach on a real-world configurable system. Our results demonstrate that co-evolution from source code to achieves F1 scores ranging from 0.93 to 1.0, while co-evolution from to source code achieves F1 scores between 0.41 and 0.99. These findings highlight the potential of s to support this co-evolution process, while also showing limitations and suggesting areas for improvement, particularly for the co-evolution from to code. Additionally, we conduct a comparative study across various s, revealing how choice affects co-evolution and, incidentally, how it affects model and code generation. Up to a certain size limit, larger s tend to produce more accurate and stable outputs than smaller ones, however, this influence is less pronounced in the code generation task. Overall, our work opens a new research avenue where s are leveraged for automating the co-evolution between configurable software systems and variability models.},
	booktitle = {Proceedings of the 29th {ACM} {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} {A}},
	publisher = {Association for Computing Machinery},
	author = {Zine, Nada and Quinton, Clément and Rouvoy, Romain},
	year = {2025},
	keywords = {Large Language Models, Co-evolution, Software Product Lines, Feature Models},
	pages = {27--38},
}

@inproceedings{barash_vision_2021,
	address = {New York, NY, USA},
	series = {{SLE} 2021},
	title = {Vision: the next 700 language workbenches},
	isbn = {978-1-4503-9111-5},
	url = {https://doi.org/10.1145/3486608.3486907},
	doi = {10.1145/3486608.3486907},
	abstract = {Language workbenches (LWBs) are tools to define software languages together with tailored Integrated Development Environments for them. A comprehensive review of language workbenches by Erdweg et al. (Comput. Lang. Syst. Struct. 44, 2015) presented a feature model of functionality of LWBs from the point of view of "languages that can be defined with a LWB, and not the definition mechanism of the LWB itself". This vision paper discusses possible functionality of LWBs with regard to language definition mechanisms. We have identified five groups of such functionality, related to: metadefinitions, metamodifications, metaprocess, LWB itself, and programs written in languages defined in a LWB. We design one of the features ("ability to define dependencies between language concerns") based on our vision.},
	booktitle = {Proceedings of the 14th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Barash, Mikhail},
	year = {2021},
	note = {event-place: Chicago, IL, USA},
	keywords = {algebraic specifications, Language workbenches, metaprogramming, software languages},
	pages = {16--21},
}

@article{singh_format5_2023,
	title = {{FormaT5}: {Abstention} and {Examples} for {Conditional} {Table} {Formatting} with {Natural} {Language}},
	volume = {17},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3632093.3632111},
	doi = {10.14778/3632093.3632111},
	abstract = {Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires understanding and implementing the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.},
	number = {3},
	journal = {Proc. VLDB Endow.},
	author = {Singh, Mukul and Cambronero, José and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Nouri, Elnaz and Raza, Mohammad and Verbruggen, Gust},
	month = nov,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {497--510},
}

@inproceedings{amith_application_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {Application of an ontology for model cards to generate computable artifacts for linking machine learning information from biomedical research},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587601},
	doi = {10.1145/3543873.3587601},
	abstract = {Model card reports provide a transparent description of machine learning models which includes information about their evaluation, limitations, intended use, etc. Federal health agencies have expressed an interest in model cards report for research studies using machine-learning based AI. Previously, we have developed an ontology model for model card reports to structure and formalize these reports. In this paper, we demonstrate a Java-based library (OWL API, FaCT++) that leverages our ontology to publish computable model card reports. We discuss future directions and other use cases that highlight applicability and feasibility of ontology-driven systems to support FAIR challenges.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Amith, Muhammad and Cui, Licong and Roberts, Kirk and Tao, Cui},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {FAIR, artificial intelligence, machine learning, ontology, semantic web, inference, transparency, description logic, document engineering, model card reports},
	pages = {820--825},
}

@article{khan_llmkgvldb_2025,
	title = {{LLM}+{KG}@{VLDB} 24 {Workshop} {Summary}},
	volume = {54},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/3749116.3749132},
	doi = {10.1145/3749116.3749132},
	abstract = {The unification of large language models (LLMs) and knowledge graphs (KGs) has emerged as a hot topic. At the LLM+KG'24 workshop, co-located with VLDB 2024 in Guangzhou, China, the key theme explored was important data management challenges and opportunities due to the effective interaction between LLMs and KGs. The report outlines major directions and approaches presented by various speakers during the workshop.},
	number = {2},
	journal = {SIGMOD Rec.},
	author = {Khan, Arijit and Wu, Tianxing and Chen, Xi},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {60--65},
}

@inproceedings{wasi_llms_2024,
	address = {New York, NY, USA},
	series = {{In2Writing} '24},
	title = {{LLMs} as {Writing} {Assistants}: {Exploring} {Perspectives} on {Sense} of {Ownership} and {Reasoning}},
	isbn = {979-8-4007-1031-5},
	url = {https://doi.org/10.1145/3690712.3690723},
	doi = {10.1145/3690712.3690723},
	abstract = {Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn’t directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-centered aspects in writing and improve writing aid systems.},
	booktitle = {Proceedings of the {Third} {Workshop} on {Intelligent} and {Interactive} {Writing} {Assistants}},
	publisher = {Association for Computing Machinery},
	author = {Wasi, Azmine Toushik and Islam, Mst Rafia and Islam, Raima},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Large Language Models, Human computer interaction, Sense of Ownership, Writing Assistants},
	pages = {38--42},
}

@inproceedings{xu_constraint_2025,
	address = {New York, NY, USA},
	series = {{FDG} '25},
	title = {Constraint {Is} {All} {You} {Need}: {Optimization}-{Based} {3D} {Level} {Generation} with {LLMs}},
	isbn = {979-8-4007-1856-4},
	url = {https://doi.org/10.1145/3723498.3723840},
	doi = {10.1145/3723498.3723840},
	abstract = {Procedural Content Generation (PCG) has long enabled efficient and varied game level creation. However, integrating high-level design intentions and game mechanics into complex 3D environments remains challenging. This paper introduces a comprehensive framework that transforms narrative-level descriptions into playable 3D game levels. First, Large Language Models (LLMs) parse natural language descriptions of game environments into a structured Game Level Description Language (GLDL), capturing essential spatial constraints. Next, we model level generation as a Facility Layout Optimization problem, ensuring that facility placements and configurations adhere to specified design criteria. Through comprehensive experiments, including automated constraint evaluations and agent-based simulations, our approach ensures both the feasibility and stability of the constraints extracted from textual descriptions. We confirm that the resulting game levels remain interactive, reasonable, and controllable to their original specifications.},
	booktitle = {Proceedings of the 20th {International} {Conference} on the {Foundations} of {Digital} {Games}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Kaijie and Verbrugge, Clark},
	year = {2025},
	keywords = {Large Language Models, Facility Layout Problem, Game Design, Level Generation, Procedural Content Generation},
}

@article{tsaneva_enhancing_2024,
	title = {Enhancing {Human}-in-the-{Loop} {Ontology} {Curation} {Results} through {Task} {Design}},
	volume = {16},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3626960},
	doi = {10.1145/3626960},
	abstract = {The success of artificial intelligence (AI) applications is heavily dependent on the quality of data they rely on. Thus, data curation, dealing with cleaning, organising, and managing data, has become a significant research area to be addressed. Increasingly, semantic data structures such as ontologies and knowledge graphs empower the new generation of AI systems. In this article, we focus on ontologies as a special type of data. Ontologies are conceptual data structures representing a domain of interest and are often used as a backbone to knowledge-based intelligent systems or as an additional input for machine learning algorithms. Low-quality ontologies, containing incorrectly represented information or controversial concepts modelled from a single viewpoint, can lead to invalid application outputs and biased systems. Thus, we focus on the curation of ontologies as a crucial factor for ensuring trust in the enabled AI systems. While some ontology quality aspects can be automatically evaluated, others require a human-in-the-loop evaluation. Yet, despite the importance of the field, several ontology quality aspects have not yet been addressed and there is a lack of guidelines for optimal design of human computation tasks to perform such evaluations. In this article, we advance the state-of-the-art by making two novel contributions. First, we propose a human computation (HC)–based approach for the verification of ontology restrictions —an ontology evaluation aspect that has not yet been addressed with HC techniques. Second, by performing two controlled experiments with a junior expert crowd, we empirically derive task design guidelines for achieving high-quality evaluation results related to (i) the formalism for representing ontology axioms and (ii) crowd qualification testing. We find that the representation format of the ontology does not significantly influence the campaign results. Nevertheless, contributors expressed a preference in working with a graphical ontology representation. Additionally, we show that an objective qualification test is better fitted at assessing contributors’ prior knowledge rather than a subjective self-assessment and that prior modelling knowledge of the contributors had a positive effect on their judgements. We make all artefacts designed and used in the experimental campaign publicly available.},
	number = {1},
	journal = {J. Data and Information Quality},
	author = {Tsaneva, Stefani and Sabou, Marta},
	month = mar,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology evaluation, human-in-the-loop, human computation},
}

@inproceedings{gohsen_assisted_2024,
	address = {New York, NY, USA},
	series = {{CHIIR} '24},
	title = {Assisted {Knowledge} {Graph} {Authoring}: {Human}-{Supervised} {Knowledge} {Graph} {Construction} from {Natural} {Language}},
	isbn = {979-8-4007-0434-5},
	url = {https://doi.org/10.1145/3627508.3638340},
	doi = {10.1145/3627508.3638340},
	abstract = {Encyclopedic knowledge graphs, such as Wikidata, host an extensive repository of millions of knowledge statements. However, domain-specific knowledge from fields such as history, physics, or medicine is significantly underrepresented in those graphs. Although few domain-specific knowledge graphs exist (e.g., Pubmed for medicine), developing specialized retrieval applications for many domains still requires constructing knowledge graphs from scratch. To facilitate knowledge graph construction, we introduce WAKA: a Web application that allows domain experts to create knowledge graphs through the medium with which they are most familiar: natural language.},
	booktitle = {Proceedings of the 2024 {Conference} on {Human} {Information} {Interaction} and {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Gohsen, Marcel and Stein, Benno},
	year = {2024},
	note = {event-place: Sheffield, United Kingdom},
	keywords = {Semantic Web, Information Extraction, Knowledge Graph Construction},
	pages = {376--380},
}

@inproceedings{schafer_dont_2025,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '25},
	title = {Don't {Detect}, {Just} {Correct}: {Can} {LLMs} {Defuse} {Deceptive} {Patterns} {Directly}?},
	isbn = {979-8-4007-1395-8},
	url = {https://doi.org/10.1145/3706599.3719683},
	doi = {10.1145/3706599.3719683},
	abstract = {Deceptive (or dark) patterns, UI design strategies manipulating users against their best interests, have become widespread. We introduce an idea for technical countermeasures against such patterns. It feeds the HTML code of web elements that may contain deceptive patterns into a large language model (LLM) and iteratively prompts it to make these elements less manipulative. We evaluated our approach with GPT-4o and self-created web elements. The most consistent results appeared after three iterations, with 91\% of deceptive elements being less manipulative and 96\% not more manipulative than originally. We contribute our minimal and improved prompts and a labeled dataset of all 2,600 redesigns with the LLM’s justifications for its changes. We also performed preliminary tests on real websites to show and discuss the feasibility of our approach in the field. Our findings suggest that LLMs can defuse certain deceptive patterns without prior model training, promising a major advance in fighting these manipulations.},
	booktitle = {Proceedings of the {Extended} {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Schäfer, René and Preuschoff, Paul Miles and Niewianda, Rene and Hahn, Sophie and Fiedler, Kevin and Borchers, Jan},
	year = {2025},
	keywords = {large language models, dark patterns, countermeasures, deceptive patterns},
}

@article{hamed_foods_2025,
	title = {{FooDS}: {Ontology}-based {Knowledge} {Graphs} for {Forest} {Observatories}},
	volume = {3},
	url = {https://doi.org/10.1145/3707637},
	doi = {10.1145/3707637},
	abstract = {Wildlife research activities generate data on ecosystems and species interactions from varied independent projects. Forest Observatories are online platforms that curate, integrate, and analyze wildlife research data for forest monitoring. However, integrating data from disparate sources can be challenging due to data heterogeneity. This study, in collaboration with a research facility in the forest of Sabah, Malaysian Borneo, proposes a novel approach to integrate heterogeneous wildlife data for Forest Observatories. We used the Forest Observatory Ontology (FOO) to standardize wildlife data entities generated by sensors. Four semantically modeled wildlife datasets populated FOO, resulting in an ontology-based knowledge graph named FooDS (Forest Observatory Ontology Data Store). We evaluated FOO and FooDS using specialized open-source ontology scanners, domain experts’ feedback, and applied use cases. This study contributes FooDS, the first ontology-based knowledge graph for Forest Observatories, which provides accurate query responses, reasoning about data, and granular data acquisition from diverse datasets. FOO in turtle format, FOO’s documentation and FooDS in turtle format and their resource website are published at , , , and .},
	number = {1},
	journal = {ACM J. Comput. Sustain. Soc.},
	author = {Hamed, Naeima and Rana, Omer and Orozco Ter Wengel, Pablo and Goossens, Benoit and Perera, Charith},
	month = jan,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, knowledge graph, Internet of Things, Wildlife data},
}

@inproceedings{silva_mercado_ai_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {{AI} {Assisted} {Domain} {Modeling} {Explainability} and {Traceability}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688197},
	doi = {10.1145/3652620.3688197},
	abstract = {Domain Models are abstract representations of selected elements in a domain that is created in a collaborative process between domain and modeler experts. The participants share domain knowledge to conceptualize and reason about the elements that will create the domain models. Through this exchange, a comprehensive and accurate representation of the domain is achieved, ensuring that the model captures the relevant aspects and relationships in the domain. Research in Artificial Intelligence (AI) has explored various methods to assist in the creation of domain models from text using Natural Language Processing (NLP) and Machine Learning (ML). Recent advancements with Large Language Models (LLMs) have shown that it is possible to create domain models using prompting techniques; however, the generated domain models contain errors and remain constrained by the performance of the LLM used.Despite the impressive capabilities of LLMs to create domain models, it is evident that it does not address the needs of domain and modelers experts that participate in the creation of domain models. Every AI technique has its advantages and limitations that must be integrated with human feedback in a collaboration process. Therefore, we propose an approach that incorporates human-AI collaboration supported by AI assistants that follows a dialogue approach to understand the users needs and purpose to suggest relevant models. Our proposal combines symbolic and subsymbolic AI techniques with explainability and traceability of the decisions that assist to create domain models that are relevant for the users.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Silva Mercado, Jonathan},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {large language models, uncertainty, explainability, domain modeling, traceability},
	pages = {130--135},
}

@inproceedings{frey_breaking_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Breaking {Accessibility} {Barriers}: {An} {Ontology} {Proxy} with {Failure} {Recovery} and {Time} {Travel} {Capabilities}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715531},
	doi = {10.1145/3701716.3715531},
	abstract = {This paper introduces a novel concept and implementation of an ontology proxy designed to seamlessly enhance accessibility and reliability of the Web of Ontologies by addressing challenges such as link rot, evolution inconsistencies, and communication failures. The proxy features a time travel mode, powered by DBpedia Archivo, that provides access to archived and versioned snapshots of ontologies. This enables failure recovery and the emulation of a consistent state in time, supporting reproducible research and enhancing the FAIRness of ontologies and associated (meta)data in a plug-and-play manner. Initial evaluations show significant improvements in ontology retrieval success rates, underscoring the proxy's potential as a viable interface for breaking accessibility barriers.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Frey, Johannes and Ciuciu-Kiss, Jenifer Tabita and Arndt, Natanael},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {linked data, accessibility issues, fair data, ontology wayback machine},
	pages = {966--970},
}

@inproceedings{ghosh_using_2023,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '23},
	title = {Using {Natural} {Language} {Processing} to {Enhance} {Understandability} of {Financial} {Texts}},
	isbn = {978-1-4503-9797-1},
	url = {https://doi.org/10.1145/3570991.3571051},
	doi = {10.1145/3570991.3571051},
	abstract = {Dealing with money has always been one of the basic skills one needs to live a comfortable life. However, financial literacy rates across the nations are extremely low. Furthermore, over the years the returns from traditional investment avenues like bank fixed deposits (FD), real estate, etc. have been diminishing. This entices new-age investors to trade and reap profits from the ever-growing stock markets. Nevertheless, in reality, only a handful of active traders are able to earn more than the FD rates. This is due to the lack of financial knowledge. The presence of complex concepts and jargons further reduces comprehensibility. In this paper, we present how financial texts can be demystified using Natural Language Processing (NLP). It consists of neural-based readability assessment and hypernym extraction tools to improve the readability of financial texts. Other modules include financial domain specific systems for automated claim detection, sustainability assessment, etc.},
	booktitle = {Proceedings of the 6th {Joint} {International} {Conference} on {Data} {Science} \&amp; {Management} of {Data} (10th {ACM} {IKDD} {CODS} and 28th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Ghosh, Sohom and Naskar, Sudip Kumar},
	year = {2023},
	note = {event-place: Mumbai, India},
	keywords = {natural language processing, claim detection, financial text processing, hypernym detection, readability},
	pages = {301--302},
}

@inproceedings{tupayachi_conversational_2025,
	address = {New York, NY, USA},
	series = {{IWCTS}'24},
	title = {Conversational {Geographic} {Question} {Answering} for {Route} {Optimization}: {An} {LLM} and {Continuous} {Retrieval}-{Augmented} {Generation} {Approach}},
	isbn = {979-8-4007-1151-0},
	url = {https://doi.org/10.1145/3681772.3698217},
	doi = {10.1145/3681772.3698217},
	abstract = {We present a pilot study exploring the potential of Large Language Models (LLMs) to interface with application programming interfaces through logical instructions, specifically within the domain of Geographic Question Answering for route optimization. This study employs a Continuous Retrieval-Augmented Generation approach combined with fine-tuned LLMs, featuring customized node-based storage and vector search retrieval. We also provide a comparative analysis of the method's effectiveness and adaptability in handling diverse textual queries.},
	booktitle = {Proceedings of the 17th {ACM} {SIGSPATIAL} {International} {Workshop} on {Computational} {Transportation} {Science} {GenAI} and {Smart} {Mobility} {Session}},
	publisher = {Association for Computing Machinery},
	author = {Tupayachi, Jose and Li, Xueping},
	year = {2025},
	note = {event-place: Atlanta, GA, USA},
	keywords = {Large Language Models, Question Answering, Geographical Information, Retrieval Augmented Generation},
	pages = {56--59},
}

@inproceedings{kim_how_2021,
	address = {New York, NY, USA},
	series = {{HAI} '21},
	title = {“{How} to best say it?” : {Translating} {Directives} in {Machine} {Language} into {Natural} {Language} in the {Blocks} {World}},
	isbn = {978-1-4503-8620-3},
	url = {https://doi.org/10.1145/3472307.3484649},
	doi = {10.1145/3472307.3484649},
	abstract = {We propose a method to generate optimal natural language for block placement directives generated by a machine’s planner during human-agent interactions in the blocks world. A non user-friendly machine directive, e.g., move(ObjId, toPos), is transformed into visually and contextually grounded referring expressions that are much easier for the user to comprehend. We describe an algorithm that progressively and generatively transforms the machine’s directive in ECI (Elementary Composable Ideas)-space, generating many alternative versions of the directive. We then define a cost function to evaluate the ease of comprehension of these alternatives and select the best option. The parameters for this cost function were derived empirically from a user study that measured utterance-to-action timings.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Human}-{Agent} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Sujeong and Tamrakar, Amir},
	year = {2021},
	note = {event-place: Virtual Event, Japan},
	keywords = {natural language generation, perception, optimization, cost},
	pages = {289--293},
}

@inproceedings{van_rozen_towards_2023,
	address = {New York, NY, USA},
	series = {{FDG} '23},
	title = {Towards a {Unified} {Language} for {Card} {Game} {Design}},
	isbn = {978-1-4503-9855-8},
	url = {https://doi.org/10.1145/3582437.3587185},
	doi = {10.1145/3582437.3587185},
	abstract = {Card game creation is a powerful tool for game\&nbsp;design. Using\&nbsp;playing cards, game designers can rapidly prototype and iteratively playtest a game’s core mechanisms to explore alternatives and improve the gameplay. However, this process is time-consuming, imprecise and challenging to steer and focus. We aim to empower designers with solutions that automate game design processes. In particular, we study to what extent a unified game design language can offer theoretical foundations, systematic techniques and practical solutions. We propose a novel approach towards a solution that leverages the expressive power of playing cards. Initially focusing on well-known card games, we illustrate the steps for creating CardScript, a formal language and toolkit that supports game design processes. The approach also has the potential to impact a wider research area. When fully developed, a unified language with a common tool set can enable reuse, and eventually support joint research agendas. We start the discussion by highlighting perspectives that relate open challenges to opportunities for future collaboration.},
	booktitle = {Proceedings of the 18th {International} {Conference} on the {Foundations} of {Digital} {Games}},
	publisher = {Association for Computing Machinery},
	author = {van Rozen, Riemer and Bouwer, Anders and Millenaar, Karel},
	year = {2023},
	note = {event-place: Lisbon, Portugal},
	keywords = {domain-specific languages, design tools, game design},
}

@inproceedings{tan_multi-classification_2019,
	address = {New York, NY, USA},
	series = {{ICMI} '19},
	title = {Multi-{Classification} {Model} for {Spoken} {Language} {Understanding}},
	isbn = {978-1-4503-6860-5},
	url = {https://doi.org/10.1145/3340555.3356099},
	doi = {10.1145/3340555.3356099},
	abstract = {The spoken language understanding (SLU) is an important part of spoken dialogue system (SDS). In the paper, we focus on how to extract a set of act-slot-value tuples from users’ utterances in the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU). This paper adopts the pretrained BERT model to encode users’ utterances and builds multiple classifiers to get the required tuples. In our framework, finding acts and values of slots are recognized as classification tasks respectively. Such multi-task training is expected to help the encoder to get better understanding of the utterance. Since the system is built on the transcriptions given by automatic speech recognition (ASR), some tricks are applied to correct the errors of the tuples. We also found that using the minimum edit distance (MED) between results and candidates to rebuild the tuples was beneficial in our experiments.},
	booktitle = {2019 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Chaohong and Ling, Zhenhua},
	year = {2019},
	note = {event-place: Suzhou, China},
	keywords = {BERT, classification, Spoken language understanding, multi-task learning, text tagging},
	pages = {526--530},
}

@inproceedings{desai_personas_2025,
	address = {New York, NY, USA},
	series = {{CUI} '25},
	title = {Personas {Evolved}: {Designing} {Ethical} {LLM}-{Based} {Conversational} {Agent} {Personalities}},
	isbn = {979-8-4007-1527-3},
	url = {https://doi.org/10.1145/3719160.3728624},
	doi = {10.1145/3719160.3728624},
	abstract = {The emergence of Large Language Models (LLMs) has revolutionized Conversational User Interfaces (CUIs), enabling more dynamic, context-aware, and human-like interactions across diverse domains, from social sciences to healthcare. However, the rapid adoption of LLM-based personas raises critical ethical and practical concerns, including bias, manipulation, and unforeseen social consequences. Unlike traditional CUIs, where personas are carefully designed with clear intent, LLM-based personas generate responses dynamically from vast datasets, making their behavior less predictable and harder to govern. This workshop aims to bridge the gap between CUI and broader AI communities by fostering a cross-disciplinary dialogue on the responsible design and evaluation of LLM-based personas. Bringing together researchers, designers, and practitioners, we will explore best practices, develop ethical guidelines, and promote frameworks that ensure transparency, inclusivity, and user-centered interactions. By addressing these challenges collaboratively, we seek to shape the future of LLM-driven CUIs in ways that align with societal values and expectations.},
	booktitle = {Proceedings of the 7th {ACM} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Desai, Smit and Dubiel, Mateusz and Zargham, Nima and Mildner, Thomas and Spillner, Laura},
	year = {2025},
	keywords = {LLMs, Conversational User Interfaces, Interaction Design, Personas},
}

@inproceedings{haghighi_ontological_2024,
	address = {New York, NY, USA},
	series = {{DIS} '24 {Companion}},
	title = {Ontological {Breakdown}: {Toward} a {World} of {Many} {Worlds}},
	isbn = {979-8-4007-0632-5},
	url = {https://doi.org/10.1145/3656156.3665133},
	doi = {10.1145/3656156.3665133},
	abstract = {Examining the taken-for-granted assumptions and views of the world underlying the design of technological artifacts, this work posits that a lack of ontological self-reflection can constrain imagination, impeding movement toward a world of many worlds. I propose ontological breakdown as an analytic lens for interrogating the default assumptions underlying the design of technology, using LLMs as a case-study and drawing parallels to the discourse on values in design. Then, I share three ways in which I have used ontological breakdowns generatively to (1) surface ontological difference and create spaces for experiencing ontological alternatives to our defaults, (2) explore ontological alternatives in the design of artifacts and enable the end-users to notice their own ontological defaults, and (3) expand ontological diversity by empowering the end-users to move beyond the prescribed defaults. I demonstrate the generative potential of ontological breakdowns by providing examples of my work in personal informatics.},
	booktitle = {Companion {Publication} of the 2024 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Haghighi, Nava},
	year = {2024},
	note = {event-place: IT University of Copenhagen, Denmark},
	keywords = {Ontology, LLM, Generative AI, ontologies, generative AI, Ontological design, ontological design, ontological breakdown, personal informatics, Ontology's, Case-studies, End-users, Ontological breakdown, Personal informatics, Self reflection, Value in designs},
	pages = {70--73},
	annote = {Cited by: 0},
}

@article{jiang_deceiving_2025,
	title = {Deceiving {LLM} through {Compositional} {Instruction} with {Hidden} {Attacks}},
	issn = {1556-4665},
	url = {https://doi.org/10.1145/3717067},
	doi = {10.1145/3717067},
	abstract = {Recently, large language models (LLMs) have demonstrated promising applications in the autonomous driving (AD) domain, including language-based interactions and decision-making. Ensuring they safely handle harmful inputs is crucial before formal deployment. However, research reveals emerging hand-crafted jailbreak attacks, which pack harmful prompts into harmless instructions, can bypass LLMs’ security mechanisms and elicit harmful responses. To deeply understand such jailbreaks, this paper introduces a Compositional Instruction Attack (CIA) framework to generalize them, and develop two CIA jailbreaking methods to automatically generate tailored jailbreak prompts for each harmful prompt. Then, this paper builds the first CIA question-answering (CIAQA) dataset with 2.7K multiple-choice questions of 900 successful jailbreaks, for assessing LLMs’ ability to identify underlying harmful intents, harmfulness, and task priority in CIA jailbreaks. Combined with experimental analysis on CIAQA and other datasets, this paper concludes three possible reasons for the failure of LLM defenses against CIAs. Finally, we propose an intent-based defense paradigm (IBD), enabling LLMs to defend against CIA by leveraging its capability to identify intents. Experimental results show CIA can achieve attack success rates (ASR) of 95\%+ and 85\%+ in AD and common harmful scenarios for three well-known LLMs (GPT-4, GPT-3.5, and Llama2-70b-chat), and IBD reduces ASR by 74\%+.},
	journal = {ACM Trans. Auton. Adapt. Syst.},
	author = {Jiang, Shuyu and Chen, Xingshu and Tang, Rui},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large language model, adversarial attack, autonomous driving, harmful prompt},
	annote = {Just Accepted},
}

@inproceedings{achtaich_evaluation_2019,
	address = {New York, NY, USA},
	series = {{SPLC} '19},
	title = {Evaluation of the {State}-{Constraint} {Transition} {Modelling} {Language}: {A} {Goal} {Question} {Metric} {Approach}},
	isbn = {978-1-4503-6668-7},
	url = {https://doi.org/10.1145/3307630.3342417},
	doi = {10.1145/3307630.3342417},
	abstract = {Self-adaptive systems (SAS) are exceptional systems, on account of their versatile composition, dynamic behavior and evolutive nature. Existing formal languages for the specification of SAS focus on adapting system elements to achieve a target goal, following specific rules, without much attention on the adaptation of requirements themselves. The State-Constraint Transition (SCT) modeling language enables the specification of dynamic requirements, both at the domain and application level, as a result of space or time variability. This language, evaluated in this paper, enables the specification of a variety of requirement types, for SASs from different domains, while generating a configuration, all configurations, and number of possible configurations, in milliseconds. This paper presents these results, namely; expressiveness, domain independence and scalability, from the viewpoint of designers and domain engineers, following a goal-question-metric approach. However, being primarily based on constraint programming (CP), the language suffers from drawbacks inherited from this paradigm, specifically time related requirements, like (e.g. order, frequency and staged requirements).},
	booktitle = {Proceedings of the 23rd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} {B}},
	publisher = {Association for Computing Machinery},
	author = {Achtaich, Asmaa and Roudies, Ounsa and Souissi, Nissrine and Salinesi, Camille and Mazo, Raúl},
	year = {2019},
	note = {event-place: Paris, France},
	keywords = {IoT, constraint programming, modeling language, state machine, dynamic software product lines},
	pages = {106--113},
}

@inproceedings{villota_high-level_2019,
	address = {New York, NY, USA},
	series = {{SPLC} '19},
	title = {The {High}-{Level} {Variability} {Language}: {An} {Ontological} {Approach}},
	isbn = {978-1-4503-6668-7},
	url = {https://doi.org/10.1145/3307630.3342401},
	doi = {10.1145/3307630.3342401},
	abstract = {Given its relevance, there is an extensive body of research for modeling variability in diverse domains. Regretfully, the community still faces issues and challenges to port or share variability models among tools and methodological approaches. There are researchers, for instance, implementing the same algorithms and analyses again because they use a specific modeling language and cannot use some existing tool. This paper introduces the High-Level Variability Language (HLVL), an expressive and extensible textual language that can be used as a modeling and an intermediate language for variability. HLVL was designed following an ontological approach, i.e., by defining their elements considering the meaning of the concepts existing on different variability languages. Our proposal not only provides a unified language based on a comprehensive analysis of the existing ones but also sets foundations to build tools that support different notations and their combination.},
	booktitle = {Proceedings of the 23rd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} {B}},
	publisher = {Association for Computing Machinery},
	author = {Villota, Angela and Mazo, Raúl and Salinesi, Camille},
	year = {2019},
	note = {event-place: Paris, France},
	keywords = {domain specific language, variability language, variability specification},
	pages = {162--169},
}

@inproceedings{tu_reusable_2024,
	address = {San Antonio, Texas, USA},
	series = {{WSC} '23},
	title = {Reusable {Ontology} {Generation} and {Matching} from {Simulation} {Models}},
	isbn = {979-8-3503-6966-3},
	doi = {10.1109/WSC60868.2023.10407599},
	abstract = {As simulating semiconductor manufacturing grows complex, model reuse becomes appealing since it can reduce the time incurred in developing future models. Also, considering a large network of the semiconductor supply chain, knowledge sharing can enable the efficient development of simulation models in a collaborative organization. Such necessity of reusability and interoperability of simulation models motivates this paper. We will address these challenges through ontological modeling and linking of the simulation components. The first application is generating reusable ontologies from simulation models. Another discussed application is ontology matching for knowledge sharing between simulation components and a meta-model of the semiconductor supply chain. The proposed approach succeeds in automatically transforming simulation into reusable knowledge and identifying interconnection in a semiconductor manufacturing system.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Tu, Ming-Yu and Ehm, Hans and Ismail, Abdelgafar and Ulrich, Philipp},
	year = {2024},
	note = {ISSN: 1558-4305},
	keywords = {Ontologies, Interoperability, Knowledge engineering, Organizations, Supply chains, Semiconductor device manufacture, Semiconductor device modeling},
	pages = {2298--2309},
}

@inproceedings{ruoff_onyx_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {{ONYX}: {Assisting} {Users} in {Teaching} {Natural} {Language} {Interfaces} {Through} {Multi}-{Modal} {Interactive} {Task} {Learning}},
	isbn = {978-1-4503-9421-5},
	url = {https://doi.org/10.1145/3544548.3580964},
	doi = {10.1145/3544548.3580964},
	abstract = {Users are increasingly empowered to personalize natural language interfaces (NLIs) by teaching how to handle new natural language (NL) inputs. However, our formative study found that when teaching new NL inputs, users require assistance in clarifying ambiguities that arise and want insight into which parts of the input the NLI understands. In this paper we introduce ONYX, an intelligent agent that interactively learns new NL inputs by combining NL programming and programming-by-demonstration, also known as multi-modal interactive task learning. To address the aforementioned challenges, ONYX provides suggestions on how ONYX could handle new NL inputs based on previously learned concepts or user-defined procedures, and poses follow-up questions to clarify ambiguities in user demonstrations, using visual and textual aids to clarify the connections. Our evaluation shows that users provided with ONYX’s new features achieved significantly higher accuracy in teaching new NL inputs (median: 93.3\%) in contrast to those without (median: 73.3\%).},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ruoff, Marcel and Myers, Brad A and Maedche, Alexander},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {Data Visualization Tools, End User Development, Interactive Task Learning, Natural Language Interfaces},
}

@inproceedings{batten_pyhdl-eval_2024,
	address = {New York, NY, USA},
	series = {{MLCAD} '24},
	title = {{PyHDL}-{Eval}: {An} {LLM} {Evaluation} {Framework} for {Hardware} {Design} {Using} {Python}-{Embedded} {DSLs}},
	isbn = {979-8-4007-0699-8},
	url = {https://doi.org/10.1145/3670474.3685948},
	doi = {10.1145/3670474.3685948},
	abstract = {Embedding hardware design frameworks within Python is a promising technique to improve the productivity of hardware engineers. At the same time, there is significant interest in using large-language models (LLMs) to improve key chip design tasks. This paper describes PyHDL-Eval, a new framework for evaluating LLMs on specification-to-RTL tasks in the context of Python-embedded domain-specific languages (DSLs). The framework includes 168 problems, Verilog reference solutions, Verilog test benches, Python test scripts, and workflow orchestration scripts. We use the framework to conduct a detailed case study comparing five LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo) targeting Verilog and five Python-embedded DSLs (PyMTL3, PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate the promise of in-context learning when applied to smaller models (e.g., pass rate for CodeGemma 7B improves from 14.9\% to 32.7\% on Verilog) and Python-embedded DSLs (e.g., pass rate for LLama3 70B improves from 0.6\% to 33.0\% on PyMTL3). We find LLMs perform better when targeting Verilog as compared to Python-embedded DSLs (e.g., pass rate for GPT4 Turbo is 72.2\% on Verilog and 29.8-62.0\% on the Python-embedded DSLs) despite using a popular general-purpose host language. PyHDL-Eval will serve as a useful framework for future research at the intersection of Python-embedded DSLs and LLMs.},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Symposium} on {Machine} {Learning} for {CAD}},
	publisher = {Association for Computing Machinery},
	author = {Batten, Christopher and Pinckney, Nathaniel and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
	year = {2024},
	note = {event-place: Salt Lake City, UT, USA},
	keywords = {large language models, hardware description languages, Python-embedded domain-specific languages},
}

@inproceedings{berger_usage_2019,
	address = {New York, NY, USA},
	series = {{SPLC} '19},
	title = {Usage {Scenarios} for a {Common} {Feature} {Modeling} {Language}},
	isbn = {978-1-4503-6668-7},
	url = {https://doi.org/10.1145/3307630.3342403},
	doi = {10.1145/3307630.3342403},
	abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
	booktitle = {Proceedings of the 23rd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} {B}},
	publisher = {Association for Computing Machinery},
	author = {Berger, Thorsten and Collet, Philippe},
	year = {2019},
	note = {event-place: Paris, France},
	keywords = {feature models, software product lines, unified language},
	pages = {174--181},
}

@article{lu_genomics-enhanced_2025,
	title = {Genomics-{Enhanced} {Cancer} {Risk} {Prediction} for {Personalized} {LLMs}-{Driven} {Healthcare} {Recommender} {Systems}},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3745022},
	doi = {10.1145/3745022},
	abstract = {Cancer risk prediction is a cornerstone of personalized medicine that offers opportunities for early detection and preventive interventions. However, the current models are designed to predict cancer risk face several challenges. First, most rely on traditional statistical methods, which struggle to capture the complexity of genetic, family medical history, and lifestyle factors. Hence, the accuracy of these models is limited. Additionally, the models neglect to integrate multidimensional data sources, particularly genetic information like single nucleotide polymorphisms (SNPs), which could enhance prediction accuracy. Third, while the system might effectively predict risk, it cannot translate those predictions into actionable healthcare recommendations to reduce cancer risk.In this study, we address all three of these limitations. With a focus on six prevalent cancers – we extracted SNPs data from the UK Biobank and designed a novel risk prediction model for cancer and personalized healthcare recommendations based upon the mixture of experts (MoE) paradigm and large language models (LLMs) respectively. Named MoE-HRS, experts based two router networks for separate processing by the Transformer and the convolutional neural network (CNN). Experiments on UK Biobank data show that our model outperforms state-of-the-art cancer risk prediction models. To bridge the gap between risk prediction and practical healthcare applications, we devised a healthcare recommender system powered by LLMs. This approach holds promise for enhancing early detection rates and promoting preventive healthcare management1.},
	journal = {ACM Trans. Inf. Syst.},
	author = {Lu, Kezhi and Lu, Jie and Xu, Hanshi and Guo, Kairui and Zhang, Qian and Lin, Hua and Grosser, Mark and Zhang, Yi and Zhang, Guangquan},
	month = jun,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Recommendation, Genetic Risk Prediction, Healthcare Recommender Systems, LLMs-Driven Recommender Systems},
	annote = {Just Accepted},
}

@inproceedings{dimitrakopoulos_enhanced_2025,
	address = {Orlando, Florida, USA},
	series = {{WSC} '24},
	title = {Enhanced {Ontology} {Extraction}: {Integrating} {GPT} {AI} with {Human} {Knowledge} on the {Example} of {EU} {Standards} {Related} to {Semiconductor} {Supply} {Chains}},
	isbn = {979-8-3315-3420-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217617379&doi=10.1109%2FWSC63780.2024.10838760&partnerID=40&md5=8e67c6383c556395af5749406deb7bf3},
	doi = {10.1109/WSC63780.2024.10838760},
	abstract = {This paper addresses challenges in creating ontologies for the semiconductor supply chain. Ontologies are crucial for seamless data exchange within the semantic web, enabling initiatives like GAIA-X and CatenA-X. Traditionally, ontology creation is complex. Here, we propose a novel AI-assisted method using large language models (LLMs) like ChatGPT 4 Turbo to support human experts. This collaboration aims to expedite ontology generation while maintaining quality. While initial tests show promise, refining the human-AI interface for clear content generation remains a focus. By improving this collaboration, we expect to create more accurate and complete ontologies, fostering efficient information sharing and strengthening the meaningfulness of standards within the semiconductor supply chain.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Dimitrakopoulos, George and Ehm, Hans and Tsaousi, Eleni},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ontologies, Knowledge graphs, Ontology, Language model, Semantic Web, Semantics, Modeling languages, Reliability, Ontology generation, Standards, Collaboration, Accuracy, Chatbots, Ontology Extraction, Supply chains, Information sharing, Ontology's, Semantic-Web, Human knowledge, Human expert, Ontology creations, Semiconducting indium phosphide, Semiconductor supply chain, Web-enabling},
	pages = {1955--1965},
	annote = {Cited by: 0},
}

@inproceedings{yu_case_2025,
	address = {New York, NY, USA},
	series = {{ICSIE} '24},
	title = {Case {Studies} on {LLM} {Centric} and {Services} {Oriented} {Data} {Analytics} {Agent} {Development}},
	isbn = {979-8-4007-1776-5},
	url = {https://doi.org/10.1145/3708635.3708655},
	doi = {10.1145/3708635.3708655},
	abstract = {This paper presents a novel service orchestration framework for a chatbot application focused on data analytics questions. The framework integrates Large Language Models (LLMs) with service-oriented computing to transform data analytics into a dynamic, conversational experience. The approach leverages advancements in LLM technology to enable real-time, automated data insights via chatbot interfaces, making complex data analytics accessible across various industries. In addition, the data will be processed and analysis at edge-machine rather than post all the data directly to the LLMs on the cloud. Therefore, the Central to the framework is the local Micro Analytics Service (MAS) and a dynamic service-data coordination framework, which together facilitate the decoupling of data from business logic, allowing for intuitive engagement with analytics processes. Through two case studies, retail data analysis and regional healthcare planning, the ability of the framework to provide actionable insights through natural language prompts is demonstrated, showcasing its potential to significantly reduce barriers to sophisticated data analytics. The evaluation reveals strong performance in data connection and code generation, with identified areas for improvement in visualizations and handling complex data scenarios.},
	booktitle = {Proceedings of the 2024 13th {International} {Conference} on {Software} and {Information} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Hong Qing and Sutton, Jack and O'Neill, Sam and Reiff-Marganiec, Stephan},
	year = {2025},
	keywords = {Dynamic data analytics services, LLM-driven service orchestration, Services Computing},
	pages = {69--76},
}

@inproceedings{zaitoun_automated_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {Automated {Ontology} {Evaluation}: {Evaluating} {Coverage} and {Correctness} using a {Domain} {Corpus}},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587617},
	doi = {10.1145/3543873.3587617},
	abstract = {Ontologies conceptualize domains and are a crucial part of web semantics and information systems. However, re-using an existing ontology for a new task requires a detailed evaluation of the candidate ontology as it may cover only a subset of the domain concepts, contain information that is redundant or misleading, and have inaccurate relations and hierarchies between concepts. Manual evaluation of large and complex ontologies is a tedious task. Thus, a few approaches have been proposed for automated evaluation, ranging from concept coverage to ontology generation from a corpus. Existing approaches, however, are limited by their dependence on external structured knowledge sources, such as a thesaurus, as well as by their inability to evaluate semantic relationships. In this paper, we propose a novel framework to automatically evaluate the domain coverage and semantic correctness of existing ontologies based on domain information derived from text. The approach uses a domain-tuned named-entity-recognition model to extract phrasal concepts. The extracted concepts are then used as a representation of the domain against which we evaluate the candidate ontology’s concepts. We further employ a domain-tuned language model to determine the semantic correctness of the candidate ontology’s relations. We demonstrate our automated approach on several large ontologies from the oceanographic domain and show its agreement with a manual evaluation by domain experts and its superiority over the state-of-the-art.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {Ontology, Natural language processing, Automation, Semantics, BERT, natural language processing, ontology, Knowledge engineering, knowledge engineering, Web semantics, Language processing, Natural languages, Ontology's, Natural language processing systems, Ontology evaluations, Domain concepts, Automated evaluation, Web information},
	pages = {1127--1137},
	annote = {Cited by: 5; All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{malandri_skillmo_2025,
	address = {New York, NY, USA},
	series = {{SAC} '25},
	title = {{SkiLLMo}: {Normalized} {ESCO} {Skill} {Extraction} through {Transformer} {Models}},
	isbn = {979-8-4007-0629-5},
	url = {https://doi.org/10.1145/3672608.3707960},
	doi = {10.1145/3672608.3707960},
	abstract = {In recent years, natural language processing (NLP) technologies have made a significant contribution in addressing a number of labour market tasks. One of the most interesting challenges is the automatic extraction of competences from unstructured texts.This paper presents a pipeline for efficiently extracting and standardizing skills from job advertisements using NLP techniques. The proposed methodology leverages open-source Transformer and Large Language Models to extract skills and map them to the European labour market taxonomy, ESCO.To address the computational challenges of processing lengthy job advertisements, a BERT model was fine-tuned to identify text segments likely containing skills. This filtering step reduces noise and ensures that only relevant content is processed further. The filtered text is then passed to an LLM, which extracts implicit and explicit hard and soft skills through prompt engineering. The extracted skills are subsequently matched with entries in a vector store containing the ESCO taxonomy to achieve standardization.Evaluation by domain experts shows that the pipeline achieves a precision of 91\% for skill extraction, 80\% for skill standardization and a combined overall precision of 79\%. These results demonstrate the effectiveness of the proposed approach in facilitating structured and standardized skill extraction from job postings.},
	booktitle = {Proceedings of the 40th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Malandri, Lorenzo and Mercorio, Fabio and Serino, Antonio},
	year = {2025},
	note = {event-place: Catania International Airport, Catania, Italy},
	keywords = {large language models, transformer models, information extraction, labor market, skill extraction},
	pages = {1969--1978},
}

@article{russo_towards_2025,
	title = {Towards an {Ontology}-{Driven} {Approach} to {Document} {Bias}},
	volume = {83},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.19388},
	doi = {10.1613/jair.1.19388},
	abstract = {Machine learning (ML)-powered systems are capable of reproducing and often amplifying undesired biases embedded in society, emphasizing the importance of operating under practices that enable the study and understanding of the intrinsic characteristics of ML pipelines. This supports the emergence of documentation frameworks with the idea that “any remedy for bias starts with awareness of its existence.” However, a resource that can formally describe ML pipelines in terms of detected biases is still missing. To address this gap, we present the Doc-BiasO ontology, a resource that sets out to create an integrated vocabulary of biases defined in the Trustworthy AI literature and their measures, as well as to incorporate relevant domain terminology and relationships between them. Overseeing ontology engineering best practices, we reuse existing vocabularies on machine learning and AI to foster knowledge sharing and interoperability between the actors concerned with its research, development, regulation, and others. In addition, we demonstrate the potential of Doc-BiasO with an experiment on an existing benchmark and as part of a neuro-symbolic system. Overall, our main objective is to contribute towards clarifying existing terminology on bias research as it rapidly expands to all areas of AI and to improve the interpretation of bias in data and downstream impact through its documentation.},
	journal = {J. Artif. Int. Res.},
	author = {Russo, Mayra and Vidal, Maria-Esther},
	month = aug,
	year = {2025},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
	keywords = {Explainable AI, Trustworthy AI, AI in Healthcare},
}

@inproceedings{lima_carneiro_alves_de_distributed_2025,
	address = {New York, NY, USA},
	series = {{AICCC} '24},
	title = {Distributed {Incremental} {Ontology} {Reasoning} over {Dynamic} {T}-boxes},
	isbn = {979-8-4007-1792-5},
	url = {https://doi.org/10.1145/3719384.3719446},
	doi = {10.1145/3719384.3719446},
	abstract = {With the advent of Retrieval Augmented Generation (RAG), Knowledge Graphs (KGs) have yet again had a surge in interest in both Academia and Industry, as their use allows for extending the context of Large Language Models (LLMs) by combining traditional vector search with reasoning over Ontologies or Property Graphs encoded as KGs. RAG is a highly dynamic scenario, where the LLM agent might not only retrieve information from a KG or vector store but mutate it as well. This implies eventually there being a greater demand for equally-dynamic KG reasoning systems. We provide a solution to this for the popular ontology language RDF-Schema (RDFS) by showing that computing entailment as a bottom-up query over RDFS graphs with dynamic Terminological Boxes (TBox) and Assertional Boxes (ABox), those where edges and nodes belonging to both boxes can be freely added and removed, can be expressed as an incremental DBSP computation. This computation is then implemented with the distributed computation framework Differential Dataflow (DD), that subsumes DBSP, and compared with a state-of-the-art commercial ontology reasoner. We find that our approach provides more even performance across additions and deletions and a higher potential for scalability across benchmarks with up to 250 GBs of data.},
	booktitle = {Proceedings of the 2024 7th {Artificial} {Intelligence} and {Cloud} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Lima Carneiro Alves De, Bruno Rucy and Kramer, Merlin and Pinheiro, Victor Henrique Cabral},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Ontology, Language model, Knowledge management, Query processing, Benchmarking, Graph theory, Resource Description Framework (RDF), Stream processing, ontology reasoning, rdfs, stream processing, Ontology's, Model agents, Data flow analysis, Distributed computer systems, Dynamic scenarios, Ontology reasonings, Property, RDF schemas, Rdfs},
	pages = {423--428},
	annote = {Cited by: 0},
}

@inproceedings{huang_lets_2024,
	address = {Echternach, Luxembourg},
	series = {{ASE} '23},
	title = {Let's {Chat} to {Find} the {APIs}: {Connecting} {Human}, {LLM} and {Knowledge} {Graph} through {AI} {Chain}},
	isbn = {979-8-3503-2996-4},
	url = {https://doi.org/10.1109/ASE56229.2023.00075},
	doi = {10.1109/ASE56229.2023.00075},
	abstract = {API recommendation methods have evolved from literal and semantic keyword matching to query expansion and query clarification. The latest query clarification method is knowledge graph (KG)-based, but limitations include out-of-vocabulary (OOV) failures and rigid question templates. To address these limitations, we propose a novel knowledge-guided query clarification approach for API recommendation that leverages a large language model (LLM) guided by KG. We utilize the LLM as a neural knowledge base to overcome OOV failures, generating fluent and appropriate clarification questions and options. We also leverage the structured API knowledge and entity relationships stored in the KG to filter out noise, and transfer the optimal clarification path from KG to the LLM, increasing the efficiency of the clarification process. Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation. We verify the usefulness of each unit in our AI chain, which all received high scores close to a perfect 5. When compared to the baselines, our approach shows a significant improvement in MRR, with a maximum increase of 63.9\% higher when the query statement is covered in KG and 37.2\% when it is not. Ablation experiments reveal that the guidance of knowledge in the KG and the knowledge-guided pathfinding strategy are crucial for our approach's performance, resulting in a 19.0\% and 22.2\% increase in MAP, respectively. Our approach demonstrates a way to bridge the gap between KG and LLM, effectively compensating for the strengths and weaknesses of both.},
	booktitle = {Proceedings of the 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Huang, Qing and Wan, Zhenyu and Xing, Zhenchang and Wang, Changjing and Chen, Jieshan and Xu, Xiwei and Lu, Qinghua},
	year = {2024},
	keywords = {large language model, knowledge graph, API recommendation, out-of-vocabulary, query clarification},
	pages = {471--483},
}

@inproceedings{bhattacharya_integrating_2025,
	address = {New York, NY, USA},
	series = {{PETRA} '25},
	title = {Integrating {Ontology} with {Deep} {Reinforcement} {Learning}: {A} {Formal} {Framework} for {Explainability} in {Robotic} {Applications}},
	isbn = {979-8-4007-1402-3},
	url = {https://doi.org/10.1145/3733155.3736601},
	doi = {10.1145/3733155.3736601},
	abstract = {This paper presents a formal framework that integrates an ontology with Deep Reinforcement Learning (DRL) to enhance explainability in AI-driven robotic applications. By mapping DRL components to ontological concepts, our approach generates human-readable explanations for complex model decisions. We demonstrate the framework through a robotic arm pick-and-place task, illustrating improved transparency and interpretability. The integration facilitates domain knowledge incorporation while addressing the critical need for explainable AI systems. Despite challenges in computational overhead and ontological alignment, our contribution advances trustworthy AI systems that support effective human-AI collaboration.},
	booktitle = {Proceedings of the 18th {ACM} {International} {Conference} on {PErvasive} {Technologies} {Related} to {Assistive} {Environments}},
	publisher = {Association for Computing Machinery},
	author = {Bhattacharya, Sukriti and Naudet, Yannick},
	year = {2025},
	keywords = {Ontology, Explainability, Interpretability, Deep Reinforcement Learning, Transparency, Human-AI Collaboration},
	pages = {60--63},
}

@article{cremaschi_steellm_2025,
	title = {{stEELlm}: {An} {LLM} for {Generating} {Semantic} {Annotations} of {Tabular} {Data}},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3719206},
	doi = {10.1145/3719206},
	abstract = {The capabilities of LLMs represent a pivotal step in transforming how we manage and interact with information and data. We witness an increasingly pervasive use of such models in various computational tasks. In some preliminary works, attempts to integrate Knowledge Graphs and Large Language Models (LLMs) can be identified, in particular, to perform the classic tasks related to the construction of Knowledge Graphs through semantic annotation of texts. Nowadays, tables are widely used and play a crucial role in creating, organising, and sharing information that could be used to produce factual knowledge to be integrated into a Knowledge Graph. However, table-to-KG techniques through LLM have not been extensively investigated. This paper presents stEELlm, an innovative Semantic Table Interpretation approach obtained by fine-tuning the Mixtral 8x7B model. Conducted experiments demonstrate the capabilities of our model to successfully create semantic annotations of heterogeneous datasets, a scenario where classic approaches based on heuristics tend to fail.},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Cremaschi, Marco and D'Adda, Fabio and Maurino, Andrea},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large Language Models, Knowledge Graphs, Prompt Engineering, Pre-training, Fine-tuning, Semantic Table Interpretation},
	annote = {Just Accepted},
}

@article{shafi_semantic_2023,
	title = {Semantic {Tagging} for the {Urdu} {Language}: {Annotated} {Corpus} and {Multi}-{Target} {Classification} {Methods}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3582496},
	doi = {10.1145/3582496},
	abstract = {Extracting and analysing meaning-related information from natural language data has attracted the attention of researchers in various fields, such as natural language processing, corpus linguistics, information retrieval, and data science. An important aspect of such automatic information extraction and analysis is the annotation of language data using semantic tagging tools. Different semantic tagging tools have been designed to carry out various levels of semantic analysis, for instance, named entity recognition and disambiguation, sentiment analysis, word sense disambiguation, content analysis, and semantic role labelling. Common to all of these tasks, in the supervised setting, is the requirement for a manually semantically annotated corpus, which acts as a knowledge base from which to train and test potential word and phrase-level sense annotations. Many benchmark corpora have been developed for various semantic tagging tasks, but most are for English and other European languages. There is a dearth of semantically annotated corpora for the Urdu language, which is widely spoken and used around the world. To fill this gap, this study presents a large benchmark corpus and methods for the semantic tagging task for the Urdu language. The proposed corpus contains 8,000 tokens in the following domains or genres: news, social media, Wikipedia, and historical text (each domain having 2K tokens). The corpus has been manually annotated with 21 major semantic fields and 232 sub-fields with the USAS (UCREL Semantic Analysis System) semantic taxonomy which provides a comprehensive set of semantic fields for coarse-grained annotation. Each word in our proposed corpus has been annotated with at least one and up to nine semantic field tags to provide a detailed semantic analysis of the language data, which allowed us to treat the problem of semantic tagging as a supervised multi-target classification task. To demonstrate how our proposed corpus can be used for the development and evaluation of Urdu semantic tagging methods, we extracted local, topical and semantic features from the proposed corpus and applied seven different supervised multi-target classifiers to them. Results show an accuracy of 94\% on our proposed corpus which is free and publicly available to download.},
	number = {6},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Shafi, Jawad and Adeel Nawab, Rao Muhammad and Rayson, Paul},
	month = jun,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {semantic annotation, multi-target classifiers, semantic tagger, Urdu corpus annotation},
}

@inproceedings{lehmann_towards_2024,
	address = {New York, NY, USA},
	series = {Middleware '24},
	title = {Towards {Interoperability} of {APIs} - an {LLM}-based approach},
	isbn = {979-8-4007-1354-5},
	url = {https://doi.org/10.1145/3704440.3704788},
	doi = {10.1145/3704440.3704788},
	abstract = {Applications that integrate multiple Application Programming Interfaces (APIs) often face challenges due to data and application heterogeneity, complicating the integration process. Our proposed approach leverages Large Language Models (LLMs) to translate API function calls and responses into natural language, enabling communication between API consumers and providers without requiring them to adhere to the same communication protocols or data formats. By abstracting technical complexities, this approach addresses both syntactic and semantic differences. We outline the potential of LLMs to improve API interoperability and discuss strategies for optimizing performance, reducing overhead, and ensuring system reliability through error detection and correction.},
	booktitle = {Proceedings of the 25th {International} {Middleware} {Conference}: {Demos}, {Posters} and {Doctoral} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Lehmann, René},
	year = {2024},
	note = {event-place: Hong Kong, Hong Kong},
	keywords = {Interoperability, Large Language Model, Application Heterogeneity, Application Programming Interface, Data Heterogeneity},
	pages = {29--30},
}

@inproceedings{cristea_historical_2019,
	address = {New York, NY, USA},
	series = {{ICGDA} '19},
	title = {A {Historical} {Ontology} of {Semi}-automatic {Specification} {Extraction} from {Romanian} {Language}},
	isbn = {978-1-4503-6245-0},
	url = {https://doi.org/10.1145/3318236.3318246},
	doi = {10.1145/3318236.3318246},
	abstract = {The increasing need for ontologies with practical results and the difficulties of manual construction guide us towards methods that propose automatic and semi-automated techniques for creating ontologies. The article presents a conceptual method for building ontology for historical documents aiming to support historians and researchers to organize metadata, search for relevant factors, extract information and derive new knowledge for localized metadata from documents.We envision its use for educational purposes and for the benefit of the community through several aspects, such as scientific utility, accessibility to documents from the late medieval period to explore events or personal histories of 14-16 centuries.As a case study, we implemented a semi-automatic method for extracting instances from a category of Romanian documents that generate a historical ontology. The results are evaluated on the basis of the need for an automated tool comparable to those found in the literature for other languages.},
	booktitle = {Proceedings of the 2019 2nd {International} {Conference} on {Geoinformatics} and {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Cristea, Daniela-Maria and Trofin, Bogdan-Gabriel},
	year = {2019},
	note = {event-place: Prague, Czech Republic},
	keywords = {ontology population, Historical document, historical domain, semi-automatic ontology extraction},
	pages = {125--129},
}

@article{broy_development_2023,
	title = {Development {Use} {Cases} for {Semantics}-{Driven} {Modeling} {Languages}},
	volume = {66},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3569927},
	doi = {10.1145/3569927},
	abstract = {Choosing underlying semantic theories and definition techniques must closely follow intended use cases for the modeling language.},
	number = {5},
	journal = {Commun. ACM},
	author = {Broy, Manfred and Rumpe, Bernhard},
	month = apr,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {62--71},
}

@article{pei_mixture--languages_2024,
	title = {Mixture-of-{Languages} {Routing} for {Multilingual} {Dialogues}},
	volume = {42},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3676956},
	doi = {10.1145/3676956},
	abstract = {We consider multilingual dialogue systems and ask how the performance of a dialogue system can be improved by using information that is available in other languages than the language in which a conversation is being conducted. We adopt a collaborative chair-experts framework, where each expert agent can be either monolingual or cross-lingual, and a chair agent follows a mixture-of-experts procedure for globally optimizing multilingual task-oriented dialogue systems. We propose a mixture-of-languages routing framework that includes four functional components, i.e., input embeddings of multilingual dialogues, language model, pairwise alignment between the representation of every two languages, and mixture-of-languages. We quantify language characteristics of unity and diversity using a number of similarity metrics, i.e., genetic similarity and word and sentence similarity based on embeddings. Our main finding is that the performance of multilingual task-oriented dialogue systems can be greatly impacted by three key aspects, i.e., data sufficiency, language characteristics, and model design in a mixture-of-languages routing framework.},
	number = {6},
	journal = {ACM Trans. Inf. Syst.},
	author = {Pei, Jiahuan and Yan, Guojun and De Rijke, Maarten and Ren, Pengjie},
	month = oct,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {collaborative agents, mixture-of-experts, multilingual systems, task-oriented dialogue systems},
}

@inproceedings{cornelis_ontology_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {An ontology {DSL} for the co-design of mechatronic systems},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561534},
	doi = {10.1145/3550356.3561534},
	abstract = {The complexity of mechatronic systems is vastly increasing. Therefore, the design of these systems requires different engineering domains, e.g., the mechanical, electrical, and control domains, to work together. The different domains often work in parallel to gain efficiency in this so-called co-design process. However, the design choices made by engineers in one domain can influence parameters in another domain. Too little or even no knowledge about these cross-domain influences may later lead to system integration problems or to degraded system performance. Solving these problems requires taking steps back in the development process, causing a higher design cost. In order to improve this cross-domain collaboration, we propose using ontologies to assist the co-design process by explicitly capturing the design dependencies, both within and across the engineering domains. However, designing ontologies can be complex and is labor-intensive, especially if one relies on generic ontology languages like the Web Ontology Language 2 (OWL 2). Therefore, we created a Domain Specific Language (DSL) focusing on the essential complexity, which enables engineers to design a cross-domain system ontology in a consistent and straightforward way. We elaborate on the metamodel for this DSL, discuss the realization of a prototype tool, and demonstrate how one can then reason on this ontology to derive new information about the various cross-domain design relationships.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Cornelis, Milan and Vanommeslaeghe, Yon and Van Acker, Bert and De Meulenaere, Paul},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {ontology, metamodel, domain-specific language, co-design, mechatronics},
	pages = {633--642},
}

@inproceedings{esterhuyse_stable_2025,
	address = {New York, NY, USA},
	series = {{GPCE} '25},
	title = {A {Stable} {Model} {Semantics} for {eFLINT} {Norm} {Specifications} and {Model} {Checking} {Scenarios}},
	isbn = {979-8-4007-1995-0},
	url = {https://doi.org/10.1145/3742876.3742882},
	doi = {10.1145/3742876.3742882},
	abstract = {Since its introduction at GPCE2020, the eFLINT norm specification language has been used in academic and industrial applications to specify and automate compliance for various norms, such as privacy regulations and data processing agreements. The eFLINT interpreter has been used to automate the analysis of real-time or historical cases by computing logical consequences and reporting normative violations. To support future language and tooling developments, we contribute a formal definition of the language as a translation to first-order logic programming with stable model semantics. The described semantics aligns with the previous semi-formal descriptions of the language, but resolves issues relating to logical inference with negative antecedent and aggregation operators. Specifically, we formalise the connection between eFLINT's derivation rules and Horn clauses under the stable model semantics. Secondly, by repurposing the Clingo answer-set solver as a highly-optimised eFLINT interpreter, we extend the toolset for eFLINT with model-checking abstract properties in addition to case analysis. We evaluate the new semantics and interpreter via an empirical comparison of the existing implementation to our prototype implementation. We observe that the expected subset of our tests have the equivalent behaviours.},
	booktitle = {Proceedings of the 24th {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {Association for Computing Machinery},
	author = {Esterhuyse, Christopher A. and Müller, Tim and van Binsbergen, L. Thomas},
	year = {2025},
	note = {event-place: Bergen, Norway},
	keywords = {answer set solving, dynamic semantics, logic programming, model checking, norm, specification languages},
	pages = {80--93},
}

@inproceedings{finidori_pattern_2022,
	address = {USA},
	series = {{PLoP} '20},
	title = {From pattern language to pattern literacy: the biosemiotic underpinnings of "patterning" and "languaging"},
	isbn = {978-1-941652-16-9},
	abstract = {The key inquiry in this paper, to move the discussion forward on innovation and the future of Pattern Language, is about the relationship between patterns (and more precisely our capacities as humans to recognize and use patterns, aka 'patterning') and language (both in its form and in the processes of 'languaging' involved), in order to assess how each can be leveraged in understanding and communication, within and across domains. I dive here deep into the biological and bio-semiotic underpinnings of patterning and languaging, seeking to make a clear distinction between them. I explore the nature and "timeless properties" of patterns as signs and their role in the emergence of human cognition and language from an evolutionary perspective. In particular I examine their involvement in 'habit taking' and in the coordination of unselfconscious action and creative processes, such as evoked by Christopher Alexander.This paper does not provide solutions or answers, it sets a foundation to show how the development of a pattern literacy around patterns seen as basic units for the coordination of action and the understanding of the world, beyond domain knowledge and linguistic divides, could bring new possibilities for the study and orientation of socio-ecological and socio-technological systems. This will open up opportunities to further explore how pattern languages could be understood and applied towards this objective, in order to actually realize their potential as lingua franca.},
	booktitle = {Proceedings of the 27th {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {The Hillside Group},
	author = {Finidori, Helene},
	year = {2022},
	note = {event-place: Virtual Event},
	keywords = {action research, complex systems, semiotics, boundary objects, complex adaptive modeling, participatory inquiry, pattern language, pattern literacy},
}

@inproceedings{ong_how_2023,
	address = {New York, NY, USA},
	series = {Asian {HCI} '22},
	title = {How {HCI} can {Inform} the {Design} of {Human} {Language} {Technologies}},
	isbn = {978-1-4503-9250-1},
	url = {https://doi.org/10.1145/3516492.3558811},
	doi = {10.1145/3516492.3558811},
	abstract = {In this paper, we present how practices in human-computer interaction can be used to inform the design of human language technologies. We begin with a description of human language technologies, paying particular attention to the collaborative nature of dialogue systems that necessitates a strong interaction between man and machine. We then share our insights from our application of principles and practices of human computer interaction in the design and evaluation of these conversational interfaces.},
	booktitle = {Proceedings of the {Asian} {HCI} {Symposium} 2022},
	publisher = {Association for Computing Machinery},
	author = {Ong, Ethel},
	year = {2023},
	note = {event-place: New Orleans, LA, USA},
	keywords = {Dialogue systems, Conversational interfaces, Human-language technologies},
	pages = {44--47},
}

@article{jin_stock_2023,
	title = {Stock {Price} {Trends} {Prediction} {Based} on the {Classical} {Models} with {Key} {Information} {Fusion} of {Ontologies}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3592599},
	doi = {10.1145/3592599},
	abstract = {An ontology of the financial field can support effective association and integration of financial knowledge. Based on behavioral finance, social media is increasingly applied as one of the data sources for information fusion in stock forecasting to approximate the patterns of market changes. By predicting Tesla (TSLA) stock price trends, this study finds that satisfactory forecasting results can be achieved using classical models and incorporating key information features from the technical indicator ontology class and the investor behavior ontology class, even in the face of the impact of the COVID-19 epidemic. In the post-epidemic period, the back propagation neural network (BPNN) model is used to predict the price trend of TSLA for the next five trading days with an accuracy of up to 91.34\%, an F1 score of 0.91, and a return of up to 268.42\% obtained from simulated trading. This study extends the research on stock forecasting using fused information in the ontology of the financial field, providing a new basis for general investors in the selection of fusion information and the application of trading strategies and providing effective support for organizations to make intelligent financial decisions under uncertainty.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Jin, Dawei and Hu, Yiyi and Chen, Jingyu and Xia, Mengran},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {social media, sentiment analysis, feature fusion, Financial ontology, trends prediction},
}

@article{chung_is_2025,
	title = {Is {Long} {Context} {All} {You} {Need}? {Leveraging} {LLM}'s {Extended} {Context} for {NL2SQL}},
	volume = {18},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3742728.3742761},
	doi = {10.14778/3742728.3742761},
	abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.In this work, we explore the performance and the latency tradeoffs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (gemini-1.5-pro). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's Gemini-pro-1.5 achieves strong performance across multiple benchmark datasets without fine-tuning or expensive self-consistency based techniques.},
	number = {8},
	journal = {Proc. VLDB Endow.},
	author = {Chung, Yeounoh and Kakkar, Gaurav T. and Gan, Yu and Milne, Brenton and Özcan, Fatma},
	month = sep,
	year = {2025},
	note = {Publisher: VLDB Endowment},
	pages = {2735--2747},
}

@article{bashir_context-aware_2023,
	title = {Context-aware {Emotion} {Detection} from {Low}-resource {Urdu} {Language} {Using} {Deep} {Neural} {Network}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3528576},
	doi = {10.1145/3528576},
	abstract = {Emotion detection (ED) plays a vital role in determining individual interest in any field. Humans use gestures, facial expressions, and voice pitch and choose words to describe their emotions. Significant work has been done to detect emotions from the textual data in English, French, Chinese, and other high-resource languages. However, emotion classification has not been well studied in low-resource languages (i.e., Urdu) due to the lack of labeled corpora. This article presents a publicly available Urdu Nastalique Emotions Dataset (UNED) of sentences and paragraphs annotated with different emotions and proposes a deep learning (DL)-based technique for classifying emotions in the UNED corpus. Our annotated UNED corpus has six emotions for both paragraphs and sentences. We perform extensive experimentation to evaluate the quality of the corpus and further classify it using machine learning and DL approaches. Experimental results show that the developed DL-based model performs better than generic machine learning approaches with an F1 score of 85\% on the UNED sentence-based corpus and 50\% on the UNED paragraph-based corpus.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Bashir, Muhammad Farrukh and Javed, Abdul Rehman and Arshad, Muhammad Umair and Gadekallu, Thippa Reddy and Shahzad, Waseem and Beg, Mirza Omer},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {annotated UNED corpus, Emotion detection (ED), Urdu Nastalique Emotions Dataset (UNED)},
}

@inproceedings{agrawal_boosting_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {Boosting {Entity} {Recognition} by leveraging {Cross}-task {Domain} {Models} for {Weak} {Supervision}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3680009},
	doi = {10.1145/3627673.3680009},
	abstract = {Entity Recognition (ER) is a common natural language processing task encountered in a number of real-world applications. For common domains and named entities such as places and organisations, there exists sufficient high quality annotated data and foundational models such as T5 and GPT-3.5 also provide highly accurate predictions. However, for niche domains such as e-commerce and medicine with specialized entity types, there is a paucity of labeled data since manual labeling of tokens is often time-consuming and expensive, which makes entity recognition challenging for such domains. Recent works such as NEEDLE [48] propose hybrid solutions to efficiently combine a small amount of strongly labeled (human-annotated) with a large amount of weakly labeled (distant supervision) data to yield superior performance relative to supervised training. The extensive noise in the weakly labeled data, however, remains a challenge. In this paper, we propose WeSDoM (Weak Supervision with Domain Models), which leverages pretrained encoder models from the same domain but different tasks to create domain ontologies that can enable the creation of less noisy weakly labeled data. Experiments on internal e-commerce and public biomedical NER datasets demonstrate that WeSDoM outperforms existing SOTA baselines by a significant margin. We achieve new SOTA F1 scores on two popular Biomedical NER datasets, BC5CDR-chem 94.27, BC5CDR-disease 91.23.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Agrawal, Sanjay and Merugu, Srujana and Sembium, Vivek},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {ontologies, entity recognition, weak supervision, cross-task domain encoder},
	pages = {4324--4331},
}

@inproceedings{goel_x-lifecycle_2024,
	address = {New York, NY, USA},
	series = {{FSE} 2024},
	title = {X-{Lifecycle} {Learning} for {Cloud} {Incident} {Management} using {LLMs}},
	isbn = {979-8-4007-0658-5},
	url = {https://doi.org/10.1145/3663529.3663861},
	doi = {10.1145/3663529.3663861},
	abstract = {Incident management for large cloud services is a complex and tedious process that requires a significant amount of manual effort from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root cause analysis and mitigation of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) have created opportunities to automatically generate contextual recommendations for the OCEs, assisting them in quickly identifying and mitigating critical issues. However, existing research typically takes a silo-ed view of solving a certain task in incident management by leveraging data from a single stage of the SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of the SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying the ontology of service monitors used for automatically detecting incidents. By leveraging a dataset of 353 incidents and 260 monitors from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over state-of-the-art methods.},
	booktitle = {Companion {Proceedings} of the 32nd {ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Goel, Drishti and Husain, Fiza and Singh, Aditya and Ghosh, Supriyo and Parayil, Anjaly and Bansal, Chetan and Zhang, Xuchao and Rajmohan, Saravan},
	year = {2024},
	note = {event-place: Porto de Galinhas, Brazil},
	keywords = {Large language model, Language model, Large language models, Reliability, Cloud services, Performance, Web services, Root cause analysis, Computational linguistics, Life cycle, Software design, Reliability analysis, Cloud Services, Monitor management, Root-cause analysis, Distributed database systems, Code configuration, Different stages, Incident Management, Software development life-cycle},
	pages = {417--428},
	annote = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{kurucz_deciding_2023,
	title = {Deciding {FO}-rewritability of {Regular} {Languages} and {Ontology}-{Mediated} {Queries} in {Linear} {Temporal} {Logic}},
	volume = {76},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.14061},
	doi = {10.1613/jair.1.14061},
	abstract = {Our concern is the problem of determining the data complexity of answering an ontology-mediated query (OMQ) formulated in linear temporal logic LTL over (Z,\&lt;) and deciding whether it is rewritable to an FO(\&lt;)-query, possibly with some extra predicates. First, we observe that, in line with the circuit complexity and FO-definability of regular languages, OMQ answering in AC0, ACC0\&nbsp;and NC1\&nbsp;coincides with FO(\&lt;,≡)-rewritability using unary predicates\&nbsp;x\&nbsp;≡ 0 (mod\&nbsp;n), FO(\&lt;,MOD)-rewritability, and FO(RPR)-rewritability using relational primitive recursion, respectively. We prove that, similarly to known PSᴘᴀᴄᴇ-completeness of recognising FO(\&lt;)-definability of regular languages, deciding FO(\&lt;,≡)- and FO(\&lt;,MOD)-definability is also PSᴘᴀᴄᴇ-complete (unless ACC0\&nbsp;= NC1). We then use this result to show that deciding FO(\&lt;)-, FO(\&lt;,≡)- and FO(\&lt;,MOD)-rewritability of LTL OMQs is ExᴘSᴘᴀᴄᴇ-complete, and that these problems become PSᴘᴀᴄᴇ-complete for OMQs with a linear Horn ontology and an atomic query, and also a positive query in the cases of FO(\&lt;)- and FO(\&lt;,≡)-rewritability. Further, we consider FO(\&lt;)-rewritability of OMQs with a binary-clause ontology and identify OMQ classes, for which deciding it is PSᴘᴀᴄᴇ-, Π2p- and coNP-complete.},
	journal = {J. Artif. Int. Res.},
	author = {Kurucz, Agi and Ryzhikov, Vladislav and Savateev, Yury and Zakharyaschev, Michael},
	month = may,
	year = {2023},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
}

@inproceedings{zhang_capability_2023,
	address = {New York, NY, USA},
	series = {{DEC} '23},
	title = {A {Capability} {Description} {Language} {Design} for {Data} {Products}},
	isbn = {979-8-4007-0846-6},
	url = {https://doi.org/10.1145/3600046.3600050},
	doi = {10.1145/3600046.3600050},
	abstract = {Data has become a crucial factor driving the global economy, from various perspectives, including economics, technological advancements, digitization, and data analytics technologies. Data products are specific, ready-made data sets that have been enriched with metadata and designed to achieve particular outcomes. However, there is currently limited research on how to provide efficient and automated data product generation and sharing services. This paper proposes an approach to extract descriptive data capabilities to automate these processes. We examine real-world datasets from the Internet of Things (IoT) and user scenarios to extract selection capabilities and retrieval capabilities from the working data. A data access model is presented based on these capabilities. These preliminary results form the foundation for automating data product generation.},
	booktitle = {Proceedings of the {Second} {ACM} {Data} {Economy} {Workshop}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Wei and Chen, Perry and Yang, Jian and Tang, Yongping and Su, Jianwen},
	year = {2023},
	note = {event-place: Seattle, WA, USA},
	keywords = {Data Economy, Data Capability, Data Product, Data Service},
	pages = {21--26},
}

@inproceedings{liu_how_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {How {AI} {Processing} {Delays} {Foster} {Creativity}: {Exploring} {Research} {Question} {Co}-{Creation} with an {LLM}-based {Agent}},
	isbn = {979-8-4007-0330-0},
	url = {https://doi.org/10.1145/3613904.3642698},
	doi = {10.1145/3613904.3642698},
	abstract = {Developing novel research questions (RQs) often requires extensive literature reviews, especially in interdisciplinary fields. To support RQ development through human-AI co-creation, we leveraged Large Language Models (LLMs) to build an LLM-based agent system named CoQuest. We conducted an experiment with 20 HCI researchers to examine the impact of two interaction designs: breadth-first and depth-first RQ generation. The findings revealed that participants perceived the breadth-first approach as more creative and trustworthy upon task completion. Conversely, during the task, participants considered the depth-first generated RQs as more creative. Additionally, we discovered that AI processing delays allowed users to reflect on multiple RQs simultaneously, leading to a higher quantity of generated RQs and an enhanced sense of control. Our work makes both theoretical and practical contributions by proposing and evaluating a mental model for human-AI co-creation of RQs. We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry. The system’s source is available at: https://github.com/yiren-liu/coquest.},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yiren and Chen, Si and Cheng, Haocong and Yu, Mengxia and Ran, Xiao and Mo, Andrew and Tang, Yiliu and Huang, Yun},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Large Language Models, Co-creation Systems, Mixed-initiative Design, Scientifc Discovery},
}

@article{meng_domain_2024,
	title = {Domain {Ontology}-{Driven} {Knowledge} {Graph} {Generation} from {Text}},
	url = {https://doi.org/10.1145/3708478},
	doi = {10.1145/3708478},
	abstract = {A knowledge graph serves as a unified and standardized representation for extracting and representing textual information. In the field of knowledge extraction and representation research, named entity recognition and relation extraction provide effective solutions for knowledge graph generation tasks. However, it is a challenge that lies in extracting domain-specific knowledge from the rich and general textual corpora and generating corresponding domain knowledge graphs to support domain-specific reasoning, question-answering, and decision-making tasks. The hierarchical domain knowledge representation model (i.e. domain ontology) provides a solution for this problem. Therefore, we propose an end-to-end approach based on domain ontology embedding and pre-trained language models for domain knowledge graph generation from text, which incorporates domain node recognition and domain relation extraction phases. We evaluated our domain ontology-driven model on the Wikidata-TekGen dataset and the DBpedia-WebNLG dataset, and the results indicate that our approach based on the pre-trained language models with fewer parameters compared with the baseline models has significantly contributed to the domain knowledge graph generation without prompts.},
	journal = {ACM Trans. Probab. Mach. Learn.},
	author = {Meng, Zhixin and Zhan, Shaoxiong and Xu, Ruiqing and Mayer, Wolfgang and Zhu, Ye and Zhang, Hong-Yu and He, Chuan and He, Keqing and Cheng, Debo and Feng, Zaiwen},
	month = dec,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Domain Ontology, Domain Knowledge Graph, Domain Node Recognition, Domain Relation Extraction, Ontology Embedding},
	annote = {Just Accepted},
}

@inproceedings{padilla_agent-based_2020,
	address = {National Harbor, Maryland},
	series = {{WSC} '19},
	title = {Agent-based model characterization using natural language processing},
	isbn = {978-1-7281-3283-9},
	abstract = {This paper reports on Natural Language Processing (NLP) as a technique to analyze phenomena towards specifying agent-based models (ABM). The objective of the ABM NLP Analyzer is to facilitate non-simulationists to actively engage in the learning and collaborative designing of ABMs. The NLP model identifies candidate agents, candidate agent attributes, and candidate rules all of which non-simulationists can later evaluate for feasibility. IBM's Watson Natural Language Understanding (NLU) and Knowledge Studio were used in order to annotate, evaluate, extract agents, agent attributes, and agent rules from unstructured descriptions of phenomena. The software, and related agent-attribute-rule characterization, provides insight into a simple but useful means of conceptualizing and specifying baseline ABMs. Further, it emphasizes on how to approach the design of ABMs without the use of NLP by focusing on the identification of agent, attributes and rules.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Padilla, Jose J and Shuttleworth, David and O'Brien, Kevin},
	year = {2020},
	pages = {560--571},
}

@article{zhang_protein_2024,
	title = {Protein {Captioning}: {Bridging} the {Gap} between {Protein} {Sequences} and {Natural} {Languages}},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3705322},
	doi = {10.1145/3705322},
	abstract = {We introduce the multimodal task of Protein Captioning, which is an easy-to-understand and flexible way for protein analysis. Compared to specific protein recognition or classification tasks, such as enzyme reaction classification and gene ontology term prediction, protein captioning provides comprehensive textural descriptions for proteins, thus playing a key role in bridging the gap between protein sequences and natural languages. To address the problem, we propose a simple yet effective method, Protein-to-Text Generative Pre-trained Transformer (P2T-GPT), to fuse multimodal embeddings and translate the chain of amino acid residues in a protein to a sequence of natural language words, i.e., text. For the evaluation of protein captioning, we collect the ProteinCap dataset that contains 94,454 protein-text pairs. Experiments on ProteinCap demonstrate the effectiveness of the proposed P2T-GPT on protein captioning. For example, our method obtains improvements of 8.74, 10.03, and 11.05 in the BERTScore compared to the baseline model on ProteinCap- (alpha,beta,gamma) , respectively. As minor contributions, first, P2T-GPT provides a way to connect protein science and Large Language Models (LLMs). By appending ChatGPT, our method can interact in a conversational way to answer questions given a protein. Second, we show that protein captioning can be treated as a pre-trained task that can benefit a range of downstream tasks, to a certain extent.},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {Zhang, Jianrong and Fan, Hehe and Yang, Yi},
	month = nov,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural language processing, Multimodal learning, Protein captioning},
	annote = {Just Accepted},
}

@article{sun_expath_2025,
	title = {{eXpath}: {Explaining} {Knowledge} {Graph} {Link} {Prediction} with {Ontological} {Closed} {Path} {Rules}},
	volume = {18},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3746405.3746410},
	doi = {10.14778/3746405.3746410},
	abstract = {Link prediction (LP) is crucial for Knowledge Graphs (KG) completion but commonly suffers from interpretability issues. While several methods have been proposed to explain embedding-based LP models, they are generally limited to local explanations on KG and are deficient in providing human interpretable semantics. Based on real-world observations of the characteristics of KGs from multiple domains, we propose to explain LP models in KG with path-based explanations. An integrated framework, namely eXpath, is introduced which incorporates the concept of relation path with ontological closed path rules to enhance both the efficiency and effectiveness of LP interpretation. Notably, the eXpath explanations can be fused with other single-link explanation approaches to achieve a better overall solution. Extensive experiments across benchmark datasets and LP models demonstrate that introducing eXpath can boost the quality of resulting explanations by about 20\% on two key metrics and reduce the required explanation time by 61.4\%, in comparison to the best existing method. Case studies further highlight eXpath's ability to provide more semantically meaningful explanations through path-based evidence.},
	number = {9},
	journal = {Proc. VLDB Endow.},
	author = {Sun, Ye and Shi, Lei and Tong, Yongxin},
	month = sep,
	year = {2025},
	note = {Publisher: VLDB Endowment},
	pages = {2818--2830},
}

@inproceedings{ahmed_prompting_2024,
	address = {New York, NY, USA},
	series = {{SACMAT} 2024},
	title = {Prompting {LLM} to {Enforce} and {Validate} {CIS} {Critical} {Security} {Control}},
	isbn = {979-8-4007-0491-8},
	url = {https://doi.org/10.1145/3649158.3657036},
	doi = {10.1145/3649158.3657036},
	abstract = {Proper security control enforcement reduces the attack surface and protects the organizations against attacks. Organizations like NIST and CIS (Center for Internet Security) provide critical security controls (CSCs) as a guideline to enforce cyber security. Automated enforcement and measurability mechanisms for these CSCs still need to be developed. Analyzing the implementations of security products to validate security control enforcement is non-trivial. Moreover, manually analyzing and developing measures and metrics to monitor, and implementing those monitoring mechanisms are resource-intensive tasks and massively dependent on the security analyst's expertise and knowledge. To tackle those problems, we use large language models (LLMs) as a knowledge base and reasoner to extract measures, metrics, and monitoring mechanism implementation steps from security control descriptions to reduce the dependency on security analysts. Our approach used few-shot learning with chain-of-thought (CoT) prompting to generate measures and metrics and generated knowledge prompting for metrics implementation. Our evaluation shows that prompt engineering to extract measures, metrics, and monitoring implementation mechanisms can reduce dependency on humans and semi-automate the extraction process. We also demonstrate metric implementation steps using generated knowledge prompting with LLMs.},
	booktitle = {Proceedings of the 29th {ACM} {Symposium} on {Access} {Control} {Models} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Ahmed, Mohiuddin and Wei, Jinpeng and Al-Shaer, Ehab},
	year = {2024},
	note = {event-place: San Antonio, TX, USA},
	keywords = {prompt engineering, llm, account management., critical security control},
	pages = {93--104},
}

@inproceedings{wang_llm-assisted_2024,
	address = {New York, NY, USA},
	series = {{MLCAD} '24},
	title = {{LLM}-{Assisted} {Analytics} in {Semiconductor} {Test} ({Invited})},
	isbn = {979-8-4007-0699-8},
	url = {https://doi.org/10.1145/3670474.3685974},
	doi = {10.1145/3670474.3685974},
	abstract = {The emergence of Large Language Models (LLMs) has impacted our perspective on applying Machine Learning (ML) in semiconductor test. This paper shares our experience in leveraging the power of LLMs to build an AI agent for test data analytics. We advocate for an end-to-end approach where the Knowledge Graph (KG) plays a central role. Using wafermap analytics as an example, we highlight the key ideas behind developing the LLM-assisted AI agent named IEA-Plot, and discuss its practical applications.},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Symposium} on {Machine} {Learning} for {CAD}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Li-C.},
	year = {2024},
	note = {event-place: Salt Lake City, UT, USA},
	keywords = {Knowledge Graph, Large Language Model, Machine Learning, Test Data Analytics},
}

@article{dubslaff_enhancing_2021,
	title = {Enhancing {Probabilistic} {Model} {Checking} with {Ontologies}},
	volume = {33},
	issn = {0934-5043},
	url = {https://doi.org/10.1007/s00165-021-00549-0},
	doi = {10.1007/s00165-021-00549-0},
	abstract = {Probabilistic model checking (PMC) is a well-established method for the quantitative analysis of state based operational models such as Markov decision processes. Description logics (DLs) provide a well-suited formalism to describe and reason about knowledge and are used as basis for the web ontology language (OWL). We investigate how such knowledge described by DLs can be integrated into the PMC process, introducing ontology-mediated PMC. Specifically, we propose ontologized programs as a formalism that links ontologies to behaviors specified by probabilistic guarded commands, the de-facto standard input formalism for PMC tools such as Prism. Through DL reasoning, inconsistent states in the modeled system can be detected. We present three ways to resolve these inconsistencies, leading to different Markov decision process semantics. We analyze the computational complexity of checking whether an ontologized program is consistent under these semantics. Further, we present and implement a technique for the quantitative analysis of ontologized programs relying on standard DL reasoning and PMC tools. This way, we enable the application of PMC techniques to analyze knowledge-intensive systems.We evaluate our approach and implementation on amulti-server systemcase study,where different DL ontologies are used to provide specifications of different server platforms and situations the system is executed in.},
	number = {6},
	journal = {Form. Asp. Comput.},
	author = {Dubslaff, Clemens and Koopmann, Patrick and Turhan, Anni-Yasmin},
	month = dec,
	year = {2021},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Description logics, Ontologies, Context dependent systems analysis, Ontology-mediated verification, Probabilisticmodel checking},
	pages = {885--921},
}

@inproceedings{garcia_uas_2023,
	address = {New York, NY, USA},
	series = {{RACS} '23},
	title = {{UAS} {Integration} {Safety} and {Security} {Technology} {Ontology}},
	isbn = {979-8-4007-0228-0},
	url = {https://doi.org/10.1145/3599957.3606210},
	doi = {10.1145/3599957.3606210},
	abstract = {Unmanned Aerial Systems (UAS) are a versatile and essential tool for law enforcement, first responders, utility providers, and the public. Integrating the UAS into the National Airspace System (NAS) poses a significant challenge to policymakers and manufacturers. A UAS Integration Safety and Security Technology Ontology (ISSTO) has been developed in the Web Ontology Language (OWL) to aid this integration. ISSTO is a domain ontology covering aviation topics corresponding to flights, aircraft types, manufacturers, temporal/spatial, waivers and authorizations, track data, NAS facilities, air traffic control advisories, weather phenomena, surveillance and security equipment, and events, sensor types, radio frequency ranges, actions, and outcomes. As ISSTO is a domain ontology, it models the current state of UAS integration into the NAS and provides a comprehensive view of every aspect of UAS.},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Research} in {Adaptive} and {Convergent} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Garcia, Rebecca and Harris, Hunter and Beach, Matthew and Couch, Dylan and Khan, Samee U.},
	year = {2023},
	note = {event-place: Gdansk, Poland},
	keywords = {Ontology, Unmanned Aerial Systems, Aviation},
}

@inproceedings{zhang_ontology_2025,
	address = {San Jose, California, USA},
	series = {{AIES} '24},
	title = {Ontology of {Belief} {Diversity}: {A} {Community}-{Based} {Epistemological} {Approach}},
	abstract = {AI applications across classification, fairness, and human interaction often implicitly require ontologies of social concepts. Constructing these well, especially when there are many relevant categories, is a controversial task but is crucial for achieving meaningful inclusivity. Here, we focus on developing a pragmatic ontology of belief systems, which is a complex and often controversial space. By iterating on our community-based design until mutual agreement is reached, we found that epistemological methods were best for categorizing the fundamental ways beliefs differ, maximally respecting our principles of inclusivity and brevity. We demonstrate our methodology's utility and interpretability via user studies in term annotation and sentiment analysis experiments for belief fairness in language models.},
	booktitle = {Proceedings of the 2024 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {AAAI Press},
	author = {Zhang, Richard and van Liemt, Erin and Fischella, Tyler},
	year = {2025},
	pages = {1735--1743},
}

@inproceedings{johnson_meta_2018,
	address = {New York, NY, USA},
	series = {{ARES} '18},
	title = {A {Meta} {Language} for {Threat} {Modeling} and {Attack} {Simulations}},
	isbn = {978-1-4503-6448-5},
	url = {https://doi.org/10.1145/3230833.3232799},
	doi = {10.1145/3230833.3232799},
	abstract = {Attack simulations may be used to assess the cyber security of systems. In such simulations, the steps taken by an attacker in order to compromise sensitive system assets are traced, and a time estimate may be computed from the initial step to the compromise of assets of interest. Attack graphs constitute a suitable formalism for the modeling of attack steps and their dependencies, allowing the subsequent simulation.To avoid the costly proposition of building new attack graphs for each system of a given type, domain-specific attack languages may be used. These languages codify the generic attack logic of the considered domain, thus facilitating the modeling, or instantiation, of a specific system in the domain. Examples of possible cyber security domains suitable for domain-specific attack languages are generic types such as cloud systems or embedded systems but may also be highly specialized kinds, e.g. Ubuntu installations; the objects of interest as well as the attack logic will differ significantly between such domains.In this paper, we present the Meta Attack Language (MAL), which may be used to design domain-specific attack languages such as the aforementioned. The MAL provides a formalism that allows the semi-automated generation as well as the efficient computation of very large attack graphs. We declare the formal background to MAL, define its syntax and semantics, exemplify its use with a small domain-specific language and instance model, and report on the computational performance.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Johnson, Pontus and Lagerström, Robert and Ekstedt, Mathias},
	year = {2018},
	note = {event-place: Hamburg, Germany},
	keywords = {Cyber Security, Domain Specific Language, Threat Modeling, Attack Graphs},
}

@inproceedings{abdi_enhancing_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {Enhancing {Event} {Detection} with {Inter}-{Event} {Dependencies} in {Large} {Ontologies}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679915},
	doi = {10.1145/3627673.3679915},
	abstract = {Event Detection (ED), a crucial component of comprehensive text analysis tools, is a well-established task within the fields of Natural Language Processing (NLP) and Information Extraction (IE). Current state-of-the-art models for ED primarily focus on identifying a limited set of predefined event types. Recently, the challenge of detecting a broad array of predefined event types has garnered increasing interest within the IE community. However, a significant gap in existing research on ED with extensive ontologies is the inadequate exploration of how interactions between event types affect ED model performance. One of the hindrances for this purpose is the lack of resources to encode event-event dependencies for large ontologies. This study introduces a novel approach that leverages existing inter-event dependency resources to provide this information for extensive ontologies. Specifically, a solution based on Optimal Transport is proposed to map event-event dependency from existing resources to a large ontology. We conduct extensive experiments on multiple benchmark datasets to assess the effectiveness of our approach. Our findings, supported by a thorough analysis, demonstrate that this innovative technique significantly enhances the performance of ED models, especially for ontologies with a large number of event types.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Abdi, Samireh},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {optimal transport, event detection, large ontology},
	pages = {3612--3616},
}

@inproceedings{lin_research_2025,
	address = {New York, NY, USA},
	series = {{ICETM} '24},
	title = {Research on the {Application} of {STEM} {Practical} {Teaching} {Based} on {RAG} {Knowledge} {Graph} and {Large} {Models}},
	isbn = {979-8-4007-1746-8},
	url = {https://doi.org/10.1145/3711403.3711488},
	doi = {10.1145/3711403.3711488},
	abstract = {Practical experience plays a pivotal role in STEM education, effectively cultivating students' practical skills, innovation capabilities, and critical thinking. However, the scarcity of domain-specific practical experience data within Large Language Models (LLMs) has not fully met the deep-level practical knowledge demands of STEM education, impacting the learners' application outcomes. This paper proposes a STEM practical teaching and inquiry system based on Retrieval-Augmented Generation (RAG) technology and knowledge graphs, aiming to enhance learners' learning experiences and interdisciplinary learning abilities, achieving an intelligent upgrade of STEM practical teaching.},
	booktitle = {Proceedings of the 2024 7th {International} {Conference} on {Educational} {Technology} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Jian and Mai, Shanyin and Bu, Bingqian and He, Musheng and Wang, Xiaoyi},
	year = {2025},
	keywords = {Large Language Models (LLMs), Knowledge Graph, RAG, STEM Practical Teaching},
	pages = {520--527},
}

@inproceedings{abbasi_meta-model_2022,
	address = {New York, NY, USA},
	series = {{SPLC} '22},
	title = {A meta-model for product configuration ontologies},
	isbn = {978-1-4503-9206-8},
	url = {https://doi.org/10.1145/3503229.3547044},
	doi = {10.1145/3503229.3547044},
	abstract = {Conceptual modelling of product configuration is an essential step towards improving reuse and configuration knowledge sharing, systems interoperability, and people communication. Among several approaches proposed, ontology-based approaches are known to provide better support for the conceptualization of product configuration knowledge in terms of precision and expressiveness of reasoning and representation. This paper studies product configuration ontologies and presents a meta-model of concepts and their relationships that are used in those ontologies. The proposed meta-model consists of a generalisation hierarchy of configuration types, the compositional structure of a configurable product, the generalisation hierarchy of a component, and constraint types. The meta-model has been designed with the aim of first understanding the current state of product configuration ontologies and then extending it for integrating new concepts.},
	booktitle = {Proceedings of the 26th {ACM} {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} {B}},
	publisher = {Association for Computing Machinery},
	author = {Abbasi, Ebrahim Khalil and Leclercq, Tony and Heymans, Patrick},
	year = {2022},
	note = {event-place: Graz, Austria},
	keywords = {ontology, knowledge representation, product configuration},
	pages = {166--173},
}

@inproceedings{cannaviccio_towards_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {Towards {Annotating} {Relational} {Data} on the {Web} with {Language} {Models}},
	isbn = {978-1-4503-5639-8},
	url = {https://doi.org/10.1145/3178876.3186029},
	doi = {10.1145/3178876.3186029},
	abstract = {Tables and structured lists on Web pages are a potential source of valuable information, and several methods have been proposed to annotate them with semantics that can be leveraged for search, question answering and information extraction. This paper is concerned with the specific problem of finding and ranking relations from a given Knowledge Graph (KG) that hold over pairs of entities juxtaposed in a table or structured list. The state-of-the-art for this task is to attempt to link the entities mentioned in the table cells to objects in the KG and rank the relations that hold for those linked objects. As a result, these methods are hampered by the incompleteness and uneven coverage in even the best knowledge graphs available today. The alternative described here does not require entity linking, relying instead on ranking relations using generative language models derived from Web-scale corpora. As such, it can produce quality results even when the entities in the table are missing in the KG. The experimental validation, designed to expose the challenges posed by KG incompleteness, shows that our approach is robust and effective in practice.},
	booktitle = {Proceedings of the 2018 {World} {Wide} {Web} {Conference}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Cannaviccio, Matteo and Barbosa, Denilson and Merialdo, Paolo},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {knowledge graphs, generative language models, web table understanding},
	pages = {1307--1316},
}

@inproceedings{dobriy_o2wb_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {{O2WB}: {A} tool enabling ontology reuse in {Wikibase}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627568},
	doi = {10.1145/3587259.3627568},
	abstract = {The Semantic Web initiative has established standards and practices for publishing interconnected knowledge, where RDF Schema and OWL shall enable the reuse of ontologies as one of these established practices. However, Wikibase, the software behind Wikidata, which is increasingly gaining popularity among data publishers, lacks the functionality to import and reuse existing RDF Schema and OWL ontologies. To facilitate ontology reuse, FAIR data publishing and encourage a tighter connection of existing Linked Data resources with Wikibase instances, we align the Wikibase data model with RDF and present O2WB, a tool for ontology import and export within Wikibase.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Dobriy, Daniil and Polleres, Axel},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {Interoperability, FAIR, Wikibase, Linked Data, Ontology Reuse},
	pages = {101--104},
}

@article{purao_modeling_2018,
	title = {A {Modeling} {Language} for {Conceptual} {Design} of {Systems} {Integration} {Solutions}},
	volume = {9},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3185046},
	doi = {10.1145/3185046},
	abstract = {Systems integration—connecting software systems for cross-functional work—is a significant concern in many large organizations, which continue to maintain hundreds, if not thousands, of independently evolving software systems. Current approaches in this space remain ad hoc, and closely tied to technology platforms. Following a design science approach, and via multiple design-evaluate cycles, we develop Systems Integration Requirements Engineering Modeling Language (SIRE-ML) to address this problem. SIRE-ML builds on the foundation of coordination theory, and incorporates important semantic information about the systems integration domain. The article develops constructs in SIRE-ML, and a merge algorithm that allows both functional managers and integration professionals to contribute to building a systems integration solution. Integration models built with SIRE-ML provide benefits such as ensuring coverage and minimizing ambiguity, and can be used to drive implementation with different platforms such as middleware, services, and distributed objects. We evaluate SIRE-ML for ontological expressiveness and report findings about applicability check with an expert panel. The article discusses implications for future research such as tool building and empirical evaluation, as well as implications for practice.},
	number = {2},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Purao, Sandeep and Bolloju, Narasimha and Tan, Chuan-Hoo},
	month = sep,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Design science, conceptual modeling, SIRE-ML},
}

@article{mesmia_semi-automatic_2023,
	title = {Semi-{Automatic} {Building} and {Learning} of a {Multilingual} {Ontology}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3615864},
	doi = {10.1145/3615864},
	abstract = {Most online platforms, applications, and Websites use a massive amount of heterogeneous evolving data. These data must be structured and normalized before integration to improve the search and increase the relevance of results. An ontology can address this critical task by efficiently managing data and providing structured formats through techniques such as the Web Ontology Language (OWL). However, building an ontology can be costly, primarily if conducted manually. In this context, we propose a new methodology for automatically building and learning a multilingual ontology using Arabic as the base language via a corpus collected from Wikipedia. Our proposed methodology relies on Finite-state transducers (FSTs). FSTs are regrouped into a cascade to reduce errors and minimize ambiguity. The produced ontology is extended to English and French and independent language images via a translator we developed using APIs. The rationale for starting with the Arabic corpus to extract terms is that entity linking is more convenient from Arabic to other languages. In addition, many Wikipedia articles in English and French (for instance) do not have associated Arabic articles, but the opposite is true. In addition, dealing with Arabic terms permits us to enrich the Arabic module of the free linguistic platform we use in dictionaries and graphs. To assess the efficiency of our proposed methodology, we conducted performance metrics. The reported results are encouraging and promising.},
	number = {11},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Mesmia, Fatma Ben and Mouhoub, Malek},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Arabic NLP, API, finite sate transducer, Ontology building and learning, transducer cascade},
}

@inproceedings{gray_ontology_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {An {Ontology} of {Dark} {Patterns} {Knowledge}: {Foundations}, {Definitions}, and a {Pathway} for {Shared} {Knowledge}-{Building}},
	isbn = {979-8-4007-0330-0},
	url = {https://doi.org/10.1145/3613904.3642436},
	doi = {10.1145/3613904.3642436},
	abstract = {Deceptive and coercive design practices are increasingly used by companies to extract profit, harvest data, and limit consumer choice. Dark patterns represent the most common contemporary amalgamation of these problematic practices, connecting designers, technologists, scholars, regulators, and legal professionals in transdisciplinary dialogue. However, a lack of universally accepted definitions across the academic, legislative, practitioner, and regulatory space has likely limited the impact that scholarship on dark patterns might have in supporting sanctions and evolved design practices. In this paper, we seek to support the development of a shared language of dark patterns, harmonizing ten existing regulatory and academic taxonomies of dark patterns and proposing a three-level ontology with standardized definitions for 64 synthesized dark pattern types across low-, meso-, and high-level patterns. We illustrate how this ontology can support translational research and regulatory action, including transdisciplinary pathways to extend our initial types through new empirical work across application and technology domains.},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gray, Colin M. and Santos, Cristiana Teixeira and Bielova, Nataliia and Mildner, Thomas},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {ontology, dark patterns, deceptive design, regulation},
}

@inproceedings{morais_towards_2021,
	address = {New York, NY, USA},
	series = {{MEDES} '21},
	title = {Towards an {Ontology}-driven {Approach} to {Model} and {Analyze} {Microservices} {Architectures}},
	isbn = {978-1-4503-8314-1},
	url = {https://doi.org/10.1145/3444757.3485108},
	doi = {10.1145/3444757.3485108},
	abstract = {Microservices Architectures (MSAs) are continuously replacing monolithic systems toward achieving more flexible and maintainable service-oriented software systems. However, the shift toward an MSA also requires a technological and managerial shift for its adopters. Architecting and managing MSAs represent unique challenges, including microservices' identification, interoperability, and reuse. To handle these challenges, we propose an Ontology-driven Conceptual Modelling approach, based on the Ontology of Microservices Architecture Concepts (OMSAC), for modelling and analyzing microservices-based systems. We show, how OMSAC-based conceptual models, stocked in a Stardog triple store, support Stakeholder-specific communication, documentation, and reuse. This paper reports on the application of our approach in three open-source MSA systems with a focus on microservices' discovery based on similarity metrics. Eventually, we compare the extracted similarity metrics derived from the application of machine learning techniques to the OMSAC models with a manual analysis performed by experts.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Morais, Gabriel and Bork, Dominik and Adda, Mehdi},
	year = {2021},
	note = {event-place: Virtual Event, Tunisia},
	keywords = {machine learning, ontology, Microservices, OMSAC, Stardog},
	pages = {79--86},
}

@inproceedings{das_colonial_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {The “{Colonial} {Impulse}" of {Natural} {Language} {Processing}: {An} {Audit} of {Bengali} {Sentiment} {Analysis} {Tools} and {Their} {Identity}-based {Biases}},
	isbn = {979-8-4007-0330-0},
	url = {https://doi.org/10.1145/3613904.3642669},
	doi = {10.1145/3613904.3642669},
	abstract = {While colonization has sociohistorically impacted people’s identities across various dimensions, those colonial values and biases continue to be perpetuated by sociotechnical systems. One category of sociotechnical systems–sentiment analysis tools–can also perpetuate colonial values and bias, yet less attention has been paid to how such tools may be complicit in perpetuating coloniality, although they are often used to guide various practices (e.g., content moderation). In this paper, we explore potential bias in sentiment analysis tools in the context of Bengali communities who have experienced and continue to experience the impacts of colonialism. Drawing on identity categories most impacted by colonialism amongst local Bengali communities, we focused our analytic attention on gender, religion, and nationality. We conducted an algorithmic audit of all sentiment analysis tools for Bengali, available on the Python package index (PyPI) and GitHub. Despite similar semantic content and structure, our analyses showed that in addition to inconsistencies in output from different tools, Bengali sentiment analysis tools exhibit bias between different identity categories and respond differently to different ways of identity expression. Connecting our findings with colonially shaped sociocultural structures of Bengali communities, we discuss the implications of downstream bias of sentiment analysis tools.},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Das, Dipto and Guha, Shion and Brubaker, Jed R. and Semaan, Bryan},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Identity, Bias, Algorithmic audit, Colonial, Sentiment analysis tools},
}

@inproceedings{frolich_exploratory_2025,
	address = {New York, NY, USA},
	series = {{SLE} '25},
	title = {Exploratory, {Omniscient}, and {Multiverse} {Diagnostics} in {Debuggers} for {Non}-{Deterministic} {Languages}},
	isbn = {979-8-4007-1884-7},
	url = {https://doi.org/10.1145/3732771.3742719},
	doi = {10.1145/3732771.3742719},
	abstract = {Debugging non-deterministic programs is inherently difficult as the compound effects of non-deterministic execution steps is hard to predict and gives rise to a (potentially) vast space of reachable program states such that manual exploration of all reachable states is infeasible.Multiverse debugging addresses these problems by realising a fine-grained, exhaustive and interactive process for state space exploration. At SLE2023, Pasquier et al. presented a generic framework that makes exploration practical through user-defined reductions on program states and by proposing expressive logics for defining and searching for states and traces of interest, generalising the concept of breakpoint. The framework has been validated through the case study language AnimUML designed to make non-deterministic UML specifications executable.In this paper, we perform additional case studies to evaluate the applicability of the framework. We analyse three non-deterministic, domain-specific languages representing three different domains: grammar engineering, formal operational semantics, and norm engineering. The framework is evaluated against requirements extracted from these domains, resulting in the identification of several limitations of the framework. We then propose a modified and extended framework and apply it to develop multiverse debuggers for the case study languages. The result demonstrates a multiverse debugging framework with more general applicability.},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Frölich, Damian and Pacciani, Tommaso and van Binsbergen, L. Thomas},
	year = {2025},
	note = {event-place: Koblenz, Germany},
	keywords = {exploratory programming, domain-specific languages, debuggers, multiverse debuggers, parsing},
	pages = {134--147},
}

@inproceedings{hou-liu_concretize_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Concretize: {A} {Model}-{Driven} {Tool} for {Scenario}-{Based} {Autonomous} {Vehicle} {Testing}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3687793},
	doi = {10.1145/3652620.3687793},
	abstract = {To achieve rigorous certification of autonomous vehicles (AVs), testing approaches must handle all possible, practically relevant traffic scenarios. This is achievable through the handling of relevant abstractions within the scenario specification language and throughout the scenario generation process. While many scenario generation approaches exist, they are often limited to generating instances of a fixed (pre-defined) scenario and lack tool support. In this paper, we introduce Concretize, a model-driven AV testing framework. It (1) allows users to define scenario specifications using an abstract domain-specific language, and (2) generates conforming concrete (exact) scenarios, which are (3) visualized via a user-friendly web interface. Scenarios are also (4) executed in simulation, in which case Concretize (5) auto-generates figures depicting the monitored safety behavior of the AV-under-test wrt. scenario components at various abstraction levels.Video demonstration: https://youtu.be/inaD8jd7YxI.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hou-Liu, Jerry and Jiang, Zhekai and Babikian, Aren A.},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {domain-specific language, model-driven engineering, scenario-based autonomous vehicle testing, traffic scenario generation},
	pages = {66--70},
}

@article{belhadi_fast_2023,
	title = {Fast and {Accurate} {Framework} for {Ontology} {Matching} in {Web} of {Things}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3578708},
	doi = {10.1145/3578708},
	abstract = {The Web of Things (WoT) can help with knowledge discovery and interoperability issues in many Internet of Things (IoT) applications. This article focuses on semantic modeling of WoT and proposes a new approach called Decomposition for Ontology Matching (DOM) to discover relevant knowledge by exploring correlations between WoT data using decomposition strategies. The DOM technique adopts several decomposition techniques to order highly linked ontologies of WoT data into similar groups. The main idea is to decompose the instances of each ontology into similar groups and then match instances of similar groups instead of entire instances of two ontologies. Three main algorithms for decomposition have been developed. The first algorithm is based on radar scanning, which determines the distribution of distances between each instance and all other instances to determine the cluster centroid. The second algorithm is based on adaptive grid clustering, where it focuses on distribution information and the construction of spanning trees. The third algorithm is based on split index clustering, where instances are divided into groups of cells from which noise is removed during the merging process. Several studies were conducted with different ontology databases to illustrate the use of the DOM technique. The results show that DOM outperforms state-of-the-art ontology matching models in terms of computational cost while maintaining the quality of the matching. Moreover, these results demonstrate that DOM is capable of handling various large datasets in WoT contexts.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Belhadi, Asma and Djenouri, Youcef and Srivastava, Gautam and Lin, Jerry Chun-Wei},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology matching, Web of Things, decomposition},
}

@inproceedings{lekova_system_2022,
	address = {New York, NY, USA},
	series = {{CompSysTech} '22},
	title = {A system for speech and language therapy with a potential to work in the {IoT}},
	isbn = {978-1-4503-9644-8},
	url = {https://doi.org/10.1145/3546118.3546147},
	doi = {10.1145/3546118.3546147},
	abstract = {In this study we propose a designed, developed and validated by experiments Speech and Language Therapy (SLT) system to provide SLT intervention for children with communication disorders. In order to help the SLT services in different educational and social context, the system has a potential to work in the Internet of Things (IoT). It can wire different assistive devices, APIs, online services and agents in order to meet the child individual needs for the intervention. Humanoid NAO-type robot, Emotiv EPOC+ brain headset, emotionally-expressive robot EmoSan and Kinect depth sensor are the devices connected using Node-RED. It is a flow-based tool designed for visual programming without a need to write any code and it can run locally or in the IoT. The proposed system is sufficiently general to be applied for other kind of therapy and to support other assistive technologies, cloud services and people in the remote areas.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Computer} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Lekova, Anna and Andreeva, Anna and Tanev, Tanio and Simonska, Miglena and Kostova, Snezhana},
	year = {2022},
	note = {event-place: University of Ruse, Ruse, Bulgaria},
	keywords = {Brain Computer Interface, Node-RED, Socially Assistive Robots, Speech Language Therapy},
	pages = {119--124},
}

@inproceedings{gubanov_cancerkgorg_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {{CancerKG}.{ORG} - {A} {Web}-scale, {Interactive}, {Verifiable} {Knowledge} {Graph}-{LLM} {Hybrid} for {Assisting} with {Optimal} {Cancer} {Treatment} and {Care}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3680094},
	doi = {10.1145/3627673.3680094},
	abstract = {Here, we describe one of the first Web-scale hybrid Knowledge Graph (KG)-Large Language Model (LLM), populated with the latest peer-reviewed medical knowledge on colorectal Cancer. It is currently being evaluated to assist with both medical research and clinical information retrieval tasks at Moffitt Cancer Center and Research Institute, which is one of the top Cancer centers in the U.S. and in the world. Our hybrid is remarkable as it serves the user needs better than just an LLM, KG or a search-engine in isolation. LLMs as is are known to exhibit hallucinations and catastrophic forgetting as well as are trained on outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require manual curation, hence are quickly getting stale. CancerKG is unsupervised and is capable of automatically ingesting and organizing the latest medical findings. To alleviate the LLMs shortcomings, the verified KG serves as a Retrieval Augmented Generation (RAG) guardrail. CancerKG exhibits 5 different advanced user interfaces, each tailored to serve different data modalities better and more convenient for the user. We evaluated CancerKG on real user queries and report a high NDCG score on a large-scale corpora of approximately 44K publications.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Gubanov, Michael and Pyayt, Anna and Karolak, Aleksandra},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {LLM, cancer, data management, artificial intelligence (AI)},
	pages = {4497--4505},
}

@inproceedings{wu_llm-based_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {{LLM}-based {Automated} {Web} {Retrieval} and {Text} {Classification} of {Food} {Sharing} {Initiatives}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3680090},
	doi = {10.1145/3627673.3680090},
	abstract = {Urban and peri-urban (UPU) food systems encounter challenges in sustainability and are fragile and vulnerable to shocks. Addressing these issues is one of the key drivers of food sharing initiatives (FSIs) which focus on collective acts around food across the food system. FSIs range from seed sharing and surplus food redistribution to community composting. We describe our development and deployment of web retrieval and content classification tools designed to provide automated mapping of FSIs at scale to populate databases of FSIs within cities. We present our novel automated system tailored for retrieving, identifying, categorizing and real-time monitoring of FSIs in over 200 European cities. Developed within the European CULTIVATE project, this system not only aids in comprehending the complex dynamics of the food sharing economy, but also enhances its visibility and operational efficiency. The automation of these processes plays a vital role in supporting the goals of the CULTIVATE project, notably in promoting sustainable food practices and resilient local food networks. Our system integrates web search using queries constructed automatically using domain-specific vocabulary resources with Large Language Model (LLM) query writing and classification methods. Experimental results using a collection of data derived from real online FSI content underscore the potential of digital automation to make significant contributions to innovative digital solutions to contemporary sustainability challenges. As such, the findings of this work pave the way for future research and implementation in similar contexts.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Hao and Cho, Hyunji and Davies, Anna R. and Jones, Gareth J. F.},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {automatic query writing, content discovery and classification, food sharing, llm-based retrieval, web search},
	pages = {4983--4990},
}

@inproceedings{jarwar_industrial_2023,
	address = {New York, NY, USA},
	series = {{IoT} '22},
	title = {Industrial {Internet} of {Things} {Security} {Modelling} using {Ontological} {Methods}},
	isbn = {978-1-4503-9665-3},
	url = {https://doi.org/10.1145/3567445.3571103},
	doi = {10.1145/3567445.3571103},
	abstract = {The Industrial Internet of Things (IIoT) trend presents many significant benefits for improving industrial operations. However, its emergence from the convergence of legacy Industrial Control Systems (ICS) and Information and Communication Technologies (ICT) has introduced newer security issues such as weak or lack of end-to-end security. These challenges have weakened the interest of many critical infrastructure industries in adopting IIoT-enabled systems. Implementing security in IIoT is challenging because this involves many heterogeneous Information Technology (IT) and Operational Technology (OT) devices and complex interactions with humans, and the environments in which these are operated and monitored. This article presents the initial results of the PETRAS Secure Ontologies for Internet of Things Systems (SOfIoTS) project, which consists of key security concepts and a modular design of a base security ontology, which supports security knowledge representation and analysis of IIoT security.},
	booktitle = {Proceedings of the 12th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Jarwar, Muhammad Aslam and Watson CBE FREng, Jeremy and Ani, Uchenna P Daniel and Chalmers, Stuart},
	year = {2023},
	note = {event-place: Delft, Netherlands},
	keywords = {Security ontology, Knowledge modelling, Industrial Internet of Things, Cyber physical systems, Security attributes},
	pages = {163--170},
}

@article{nasim_automatic_2022,
	title = {Automatic {Labeling} of {Clusters} for a {Low}-{Resource} {Urdu} {Language}},
	volume = {21},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3511097},
	doi = {10.1145/3511097},
	abstract = {Document clustering techniques often produce clusters that require human intervention to interpret the meaning of such clusters. Automatic cluster labeling refers to the process of assigning a meaningful phrase to a cluster as a label. This article proposes an unsupervised method for cluster labeling that is based on noun phrase chunking. The proposed method is compared with four other statistical-based methods, including Z-Order, M-Order, T-Order, and YAKE. In addition to the statistical measures based labeling schemes, the approach is also compared with two graph-based techniques: TextRank and PositionRank. The experiments were performed on the low-resource Urdu language corpus of News Headlines. The proposed approach's effectiveness was evaluated using cosine similarity, the Jaccard index, and feedback received from human evaluators. The results show that the proposed method outperforms other methods. It was found that the labels produced were more relevant and semantically rich in contrast to other approaches.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Nasim, Zarmeen and Haider, Sajjad},
	month = apr,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cluster labeling, low-resource language, urdu language processing},
}

@inproceedings{rzepka_effectiveness_2025,
	address = {New York, NY, USA},
	series = {{ICAAI} '24},
	title = {Effectiveness of {Security} {Export} {Control} {Ontology} for {Predicting} {Answer} {Type} and {Regulation} {Categories}},
	isbn = {979-8-4007-1801-4},
	url = {https://doi.org/10.1145/3704137.3704180},
	doi = {10.1145/3704137.3704180},
	abstract = {In this paper we present results of our experiments investigating if an expert knowledge graph can improve Large Language Models accuracy in predicting correct answer labels and regulations related to the topic of security export control. As the lack of related data prevents machine-learning or fine-tuning approaches, we implement prompt expansion by searching most relevant nodes of the graph and adding the expanded context to the prompt. Results of our experiments show that the addition improved answer type selection but clearly hamper the capability of finding a correct regulation category.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Advances} in {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Rzepka, Rafal and Obayashi, Akihiko},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Large Language Models, Knowledge graph, Large language model, Ontology, Language model, Modeling languages, Knowledge Graph, Expert knowledge, Expert systems, Adversarial machine learning, Expert Systems, GraphRAG, Security Export Control, Ontology's, Machine-learning, Export controls, Graphrag, Modeling accuracy, Prediction models, Security export control},
	pages = {156--161},
	annote = {Cited by: 0},
}

@inproceedings{kuculo_comprehensive_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Comprehensive {Event} {Representations} using {Event} {Knowledge} {Graphs} and {Natural} {Language} {Processing}},
	isbn = {978-1-4503-9130-6},
	url = {https://doi.org/10.1145/3487553.3524199},
	doi = {10.1145/3487553.3524199},
	abstract = {Recent work has utilised knowledge-aware approaches to natural language understanding, question answering, recommendation systems, and other tasks. These approaches rely on well-constructed and large-scale knowledge graphs that can be useful for many downstream applications and empower knowledge-aware models with commonsense reasoning. Such knowledge graphs are constructed through knowledge acquisition tasks such as relation extraction and knowledge graph completion. This work seeks to utilise and build on the growing body of work that uses findings from the field of natural language processing (NLP) to extract knowledge from text and build knowledge graphs. The focus of this research project is on how we can use transformer-based approaches to extract and contextualise event information, matching it to existing ontologies, to build comprehensive knowledge graph-based event representations. Specifically, sub-event extraction is used as a way of creating sub-event-aware event representations. These event representations are then further enriched through fine-grained location extraction and contextualised through the alignment of historically relevant quotes.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Kuculo, Tin},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {knowledge graph, event extraction, event geotagging, event representation, quotes},
	pages = {359--363},
}

@inproceedings{oakes_towards_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Towards {Ontological} {Service}-{Driven} {Engineering} of {Digital} {Twins}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688261},
	doi = {10.1145/3652620.3688261},
	abstract = {The systematic engineering of Digital Twins (DTs) requires the establishment of clear methodologies supported by intelligent tooling. We propose an approach to guide the user in the creation and deployment of services for DTs utilizing ontologies and workflows. In our approach, the user selects a desired DT service from an array of options. This selection is then used to suggest a) enablers and models to place in the DT, and b) development and deployment workflows for the DT service. The aim is to provide DT engineering guidance to assist non-software engineering experts to develop DT services more rapidly with less effort. We describe our initial work on applying this approach to a derived version of an industrial wind turbine generator case study, utilizing openCAESAR for ontology definition and enacting the workflows with Jupyter notebooks.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Oakes, Bentley and Gomes, Claudio and Kamburjan, Eduard and Abbiati, Giuseppe and Ecem Bas, Elif and Engelsgaard, Sebastian},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {ontologies, recommendation, digital twins, workflows, DT services, guided software engineering, wind turbine testing},
	pages = {464--469},
}

@inproceedings{moret_comprehensive_2025,
	address = {New York, NY, USA},
	series = {{EATIS} 2024},
	title = {A {Comprehensive} {Analysis} of {Ontologies} {Related} to {Safety} {Related} {Traffic} {Information} for {Road} {Traffic}},
	isbn = {979-8-4007-1733-8},
	url = {https://doi.org/10.1145/3685243.3685291},
	doi = {10.1145/3685243.3685291},
	abstract = {Ontologies within the Semantic Web represent a transformative advancement, offering standardized data schemes that seamlessly interconnect with diverse vocabularies and facilitate efficient information exchange globally. This paper explores the intersection of ontologies and road safety, a domain critical to public well-being. The analysis focuses on existing ontologies related to Safety Related Traffic Information, delving into their implications for enhanced road safety, optimized traffic management, and streamlined safety-related data exchange. The study discerns common patterns, best practices, and areas for improvement across various ontological models, thereby contributing to the development of standardized solutions for managing road safety information. Using an ad hoc metrics system for this type of information will assist us in determining which ontology is most suitable and/or the need for its adaptation or utilization. Acknowledging the vital role of ontologies in promoting data interoperability, knowledge sharing, and informed decision-making, this research underscores the potential of integrating Semantic Web technologies and ontological models to revolutionize safety-related information management. The findings advocate for the transformative impact of ontologies in reducing road accidents and safeguarding human lives, emphasizing their significance in advancing the evolution of safer and more efficient road networks.},
	booktitle = {Proceedings of the 12th {Euro} {American} {Conference} on {Telematics} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Moret, Julian Gutierrez and Samper-Zapater, Jose Javier and Delgado, Ana M. and Rocha, Jose Macario de S and Tena, Gustavo and Soriano, Francisco R.},
	year = {2025},
	note = {event-place: Praia, Cape Verde},
	keywords = {Semantic Web, Knowledge Representation, Linked Data, Intelligent Transport., Safety Road, Traffic Ontology},
}

@inproceedings{sree-kumar_extracting_2018,
	address = {New York, NY, USA},
	series = {{SPLC} '18},
	title = {Extracting software product line feature models from natural language specifications},
	isbn = {978-1-4503-6464-5},
	url = {https://doi.org/10.1145/3233027.3233029},
	doi = {10.1145/3233027.3233029},
	abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
	booktitle = {Proceedings of the 22nd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} 1},
	publisher = {Association for Computing Machinery},
	author = {Sree-Kumar, Anjali and Planas, Elena and Clarisó, Robert},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {natural language processing, requirements engineering, feature model extraction, NLTK, software product line},
	pages = {43--53},
}

@inproceedings{liu_distil_2022,
	address = {New York, NY, USA},
	series = {{ICCAI} '22},
	title = {Distil {Knowledge} from {Natural} {Language}},
	isbn = {978-1-4503-9611-0},
	url = {https://doi.org/10.1145/3532213.3532240},
	doi = {10.1145/3532213.3532240},
	abstract = {Knowledge Distillation (KD) is a machine learning approach for model compression and acceleration, which is suitable for applications with limited computational resources. KD is typically performed by distilling and transferring the knowledge of large teacher models into smaller student models, to enhance the performance of the latter. Current KD models require supervision by a pretrained teacher model, whose training requires additional computational cost. In this work, we first analyze the output of the teacher, then propose a method to distill knowledge from natural language. On this basis, we propose Semantic-knowledge-based Teacher-free KD (ST-KD) to further advance model compression and acceleration methods. Our model was tested on the CIFAR10 and CIFAR100 image classification datasets, and it was experimentally demonstrated that it improved the performance of a variety of deep neural networks with virtually no additional computational cost.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Computing} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yixun and Zou, Chuyi and Wang, Yongheng},
	year = {2022},
	note = {event-place: Tianjin, China},
	keywords = {Image classification, Knowledge distillation},
	pages = {181--186},
}

@inproceedings{ayoub_case_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {A {Case} {Study} of {Enhancing} {Sparse} {Retrieval} using {LLMs}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3651945},
	doi = {10.1145/3589335.3651945},
	abstract = {While dense retrieval methods have made significant advancements, sparse retrieval techniques continue to offer advantages in terms of interpretability and generalizability. However, query-document term mismatch in sparse retrieval persists, rendering it infeasible for many practical applications. Recent research has shown that Large Language Models (LLMs) hold relevant information that can enhance sparse retrieval through the application of prompt engineering. In this paper, we build upon this concept to explore various strategies employing LLMs for information retrieval purposes. Specifically, we utilize LLMs to enhance sparse retrieval by query rewriting and query expansion. In query rewriting, the original query is refined by creating several new queries. For query expansion, LLMs are employed to generate extra terms, thereby enriching the original query. We conduct experiments on a range of well-known information retrieval datasets, including MSMARCO-passage, TREC2019, TREC2020, Natural Questions, SCIFACT. The experiments show that LLMs can be beneficial for sparse methods since the added information provided by the LLMs can help diminish the discrepancy between the term frequencies of the important terms in a query and the relevant document. In certain domains, we demonstrate that the effectiveness of LLMs is constrained, indicating that they may not consistently perform optimally, which will be explored in future research.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Ayoub, Michael Antonios Kruse and Su, Zhan and Li, Qiuchi},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {large language models, information retrieval, query expansion, query writing},
	pages = {1609--1615},
}

@inproceedings{console_model-theoretic_2021,
	address = {New York, NY, USA},
	series = {{PODS}'21},
	title = {Model-theoretic {Characterizations} of {Rule}-based {Ontologies}},
	isbn = {978-1-4503-8381-3},
	url = {https://doi.org/10.1145/3452021.3458310},
	doi = {10.1145/3452021.3458310},
	abstract = {An ontology specifies an abstract model of a domain of interest via a formal language that is typically based on logic. Although description logics are popular formalisms for modeling ontologies, tuple-generating dependencies (tgds), originally introduced as a unifying framework for database integrity constraints, and later on used in data exchange and integration, are also well suited for modeling ontologies that are intended for data-intensive tasks. The reason is that, unlike description logics, tgds can easily handle higher-arity relations that naturally occur in relational databases. In recent years, there has been an extensive study of tgd-ontologies and of their applications to several different data-intensive tasks. However, the fundamental question of whether the expressive power of tgd-ontologies can be characterized in terms of model-theoretic properties remains largely unexplored. We establish several characterizations of tgd-ontologies, including characterizations of ontologies specified by such central classes of tgds as full, linear, guarded, and frontier-guarded tgds. Our characterizations use the well-known notions of critical instance and direct product, as well as a novel locality property for tgd-ontologies. We further use this locality property to decide whether an ontology expressed by frontier-guarded (respectively, guarded) tgds can be expressed by tgds in the weaker class of guarded (respectively, linear) tgds, and effectively construct such an equivalent ontology if one exists.},
	booktitle = {Proceedings of the 40th {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Console, Marco and Kolaitis, Phokion G. and Pieris, Andreas},
	year = {2021},
	note = {event-place: Virtual Event, China},
	keywords = {ontologies, finite axiomatizability, guardedness, model theory, tuple-generating dependencies},
	pages = {416--428},
}

@inproceedings{wang_nalmo_2021,
	address = {New York, NY, USA},
	series = {{SSTD} '21},
	title = {{NALMO}: {A} {Natural} {Language} {Interface} for {Moving} {Objects} {Databases}},
	isbn = {978-1-4503-8425-4},
	url = {https://doi.org/10.1145/3469830.3470894},
	doi = {10.1145/3469830.3470894},
	abstract = {Moving objects databases (MODs) have been extensively studied due to their wide variety of applications including traffic management, tourist service and mobile commerce. However, queries in natural languages are still not supported in MODs. Since most users are not familiar with structured query languages, it is essentially important to bridge the gap between natural languages and the underlying MODs system commands. Motivated by this, we design a natural language interface for moving objects, named NALMO. In general, we use semantic parsing in combination with a location knowledge base and domain-specific rules to interpret natural language queries. We design a corpus of moving objects queries for model training, which is later used to determine the query type. Extracted entities from parsing are mapped through deterministic rules to perform query composition. NALMO is able to well translate moving objects queries into structured (executable) languages. We support four kinds of queries including time interval queries, range queries, nearest neighbor queries and trajectory similarity queries. We develop the system in a prototype system SECONDO and evaluate our approach using 240 natural language queries extracted from popular conference and journal papers in the domain of moving objects. Experimental results show that (i) NALMO achieves accuracy and precision 98.1 and 88.1, respectively, and (ii) the average time cost of translating a query is 1.47s.},
	booktitle = {Proceedings of the 17th {International} {Symposium} on {Spatial} and {Temporal} {Databases}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Xieyang and Xu, Jianqiu and Lu, Hua},
	year = {2021},
	note = {event-place: virtual, USA},
	keywords = {query processing, natural language interface, semantic parsing, moving objects database, structured language},
	pages = {1--11},
}

@inproceedings{zaitoun_ontoeval_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {{OntoEval}: an {Automated} {Ontology} {Evaluation} {System}},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587318},
	doi = {10.1145/3543873.3587318},
	abstract = {Developing semantically-aware web services requires comprehensive and accurate ontologies. Evaluating an existing ontology or adapting it is a labor-intensive and complex task for which no automated tools exist. Nevertheless, in this paper we propose a tool that aims at making this vision come true, i.e., we present a tool for the automated evaluation of ontologies that allows one to rapidly assess an ontology’s coverage of a domain and identify specific problems in the ontology’s structure. The tool evaluates the domain coverage and correctness of parent-child relations of a given ontology based on domain information derived from a text corpus representing the domain. The tool provides both overall statistics and detailed analysis of sub-graphs of the ontology. In the demo, we show how these features can be used for the iterative improvement of an ontology.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {BERT, natural language processing, ontology, knowledge engineering},
	pages = {82--85},
}

@inproceedings{dash_data_2025,
	address = {New York, NY, USA},
	series = {{ICAAI} '24},
	title = {From {Data} to {Defense}: {How} {Ontology} {Fuels} {AI} in {Cyber} {Threat} {Detection}},
	isbn = {979-8-4007-1801-4},
	url = {https://doi.org/10.1145/3704137.3704176},
	doi = {10.1145/3704137.3704176},
	abstract = {In today's evolving digital landscape, cybersecurity threats [1] have become increasingly complex and persistent, with attacks like data breaches and ransomware exploiting vulnerabilities in digital systems. As organizations handle growing amounts of data, robust defense strategies depend on comprehensive datasets for detecting and preventing threats. This paper addresses data scarcity in cybersecurity by proposing the development of a dataset ontology tailored to the domain. In data science, ontologies are structured frameworks that define relationships between different concepts within a specific field. A dataset ontology for cybersecurity serves as a cohesive framework for categorizing, organizing, and interconnecting datasets, making it easier for professionals to access and analyze threat-relevant data. The objective is to fill the gap in existing datasets and enable more precise, data-driven cybersecurity strategies. Every cyber-attack generates data, such as network logs, malware metadata, or phishing records. Professionals use this data to analyze threats, understand attack methods, and devise preventive strategies.AI models rely on large volumes of high-quality data to train algorithms, using historical patterns to detect real-time threats and respond before significant damage occurs. Unfortunately, the lack of structured and comprehensive datasets [2] in cybersecurity presents a significant challenge to building these AI models. The data generated by cyber-attacks is often fragmented, inconsistent, or incomplete, making it difficult to develop accurate threat detection systems. Without access to well-organized datasets, AI models are less effective, prone to producing false positives, and unable to adapt to new types of attacks [3]. As a result, cybersecurity defenses are weakened, leaving organizations vulnerable to increasingly sophisticated threats. The dataset ontology proposed in this paper seeks to address the challenge of data scarcity by providing a structured framework for organizing cybersecurity datasets. By categorizing and systematizing data related to various types of cyber-attacks, this ontology facilitates a more comprehensive and detailed understanding of the threat landscape. In the case of cybersecurity, the ontology organizes data based on categories such as attack types, threat actors, vulnerabilities, and attack methods.This proposed well-designed dataset ontology offers several key benefits. First, it makes it easier to identify gaps in existing datasets and create new, targeted datasets that capture the intricacies of real-world cyber-attacks. These datasets can then be used to train AI models, ensuring that they are better equipped to detect and respond to emerging threats. Second, this ontology establishes relationships between different types of cyber-attacks, providing a more holistic view of how these attacks are interconnected. For example, it can link phishing attacks to ransomware infections, allowing AI models to recognize patterns that may otherwise go unnoticed. The ontology-driven approach is particularly valuable because it enables continuous updates and improvements to cybersecurity datasets. As cyber threats evolve, so must the data used to train AI models. A dataset ontology provides the flexibility needed to incorporate new data as it becomes available, ensuring that cybersecurity models remain relevant and effective in the face of changing attack patterns.The research involved the creation of a preliminary draft taxonomy followed by the categorization and classification of the taxonomy into distinct clusters, representing different facets within the realm of cybersecurity. The dataset ontology introduced in this paper offers a solution by ensuring that AI models have access to well-organized, comprehensive datasets that reflect the full spectrum of cyber threats. By systematically categorizing different types of cyber-attacks and the data associated with them, the ontology provides AI models with the information they need to detect both common and emerging threats. This, in turn, enhances the accuracy and reliability of these models, enabling them to detect real-time cyber-attacks with greater precision and fewer false positives.Moreover, the ontology-driven approach ensures that AI models remain adaptable in the face of evolving threats [4]. Cyber-attacks are not static; they continuously evolve as malicious actors develop new techniques to bypass existing defenses. Without access to updated datasets, AI models may become outdated and ineffective. The dataset ontology addresses this issue by allowing for continuous updates to the data used in training, ensuring that AI models remain agile and responsive to new challenges. The practical applications of the proposed dataset ontology extend beyond the realm of theoretical research. By providing a structured and comprehensive framework for organizing cybersecurity data, the ontology enables organizations to develop more effective defense strategies and respond more quickly to emerging threats. For instance, organizations can use dataset ontology to create AI models capable of detecting insider threats, which are particularly difficult to identify due to the complexity of monitoring behavior [5] within trusted environments. These models can also be used to detect zero-day vulnerabilities, which are exploited by attackers before security teams have the chance to patch them.Additionally, dataset ontology facilitates collaboration across the cybersecurity industry by providing a common framework for sharing data. Currently, many organizations struggle to share cybersecurity data due to the lack of standardized formats and structures. The proposed ontology provides a solution to this issue by offering a standardized framework that can be adopted across the industry, making it easier for organizations to collaborate and share data in a meaningful way.In conclusion, the creation of dataset ontology represents a significant advancement in the field of cybersecurity. By addressing the problem of data scarcity and fragmentation [5], the ontology provides a structured framework for organizing and interconnecting diverse datasets, allowing for more effective data analysis and threat detection. This ontology-driven approach enhances the accuracy and adaptability of AI models, enabling them to detect and neutralize cyber threats in real time. Furthermore, the practical applications of dataset ontology extend beyond academic research, empowering organizations to protect their data, infrastructure, and services from a wide range of cyber-attacks.As cyber threats continue to evolve, the need for structured and comprehensive datasets will only grow. The dataset ontology provides a much-needed solution to this challenge, ensuring that AI models remain effective in detecting and responding to new and emerging threats. By improving the way cybersecurity data is organized and analyzed, ontology helps safeguard the interconnected digital ecosystem that underpins modern society.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Advances} in {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Dash, Sheetal and Seker, Huseyin and Shahpasand, Maryam},
	year = {2025},
	keywords = {ontology, Taxonomy, datasets, cyber security, DDoS, metrics, real-time cyber threats, type},
	pages = {121--133},
}

@inproceedings{al_subhi_navigating_2025,
	address = {New York, NY, USA},
	series = {{NLPIR} '24},
	title = {Navigating {Complexity}: {A} {Multi}-{Domain} {Ontology} {Evaluation} with {Cluster} {Centroids} as {Hierarchical} {Representatives}},
	isbn = {979-8-4007-1738-3},
	url = {https://doi.org/10.1145/3711542.3711607},
	doi = {10.1145/3711542.3711607},
	abstract = {Building on prior research, this study extends and improves an algorithm originally designed to identify flood-related ontology concepts, evaluating its applicability across multiple fields. The primary objectives are to: (1) demonstrate the algorithm’s effectiveness in various domains (urban system, flood with urban system, and flood with fashion) and (2) illustrate how community detection enables each cluster’s “centroid” to anchor the ontology hierarchy and represent core topics. The improved algorithm shows greater efficiency, completing in 0.35 seconds compared to 0.6 seconds for the original algorithm on datasets with up to 1,000 rows, with both algorithms delivering consistent results. The findings confirm the algorithm’s adaptability across different domains and demonstrate the use of cluster centroids, derived through community detection, as representatives of key topics in domain-specific ontologies. This approach enhances information retrieval, supports coherent data integration across systems, and provides a framework to aid decision-making and analysis in complex domains.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Al Subhi, Sundos Nasser Said and Sassani, Ardavan and Mikler, Armin Robert},
	year = {2025},
	keywords = {Co-Occurrence Graphs, Community Detection, Multi-Domain Ontology},
	pages = {192--198},
}

@inproceedings{sun_docs2kg_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{Docs2KG}: {A} {Human}-{LLM} {Collaborative} {Approach} to {Unified} {Knowledge} {Graph} {Construction} from {Heterogeneous} {Documents}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715309},
	doi = {10.1145/3701716.3715309},
	abstract = {Enterprises generate vast amounts of unstructured documents, posing challenges for knowledge extraction and representation. Large language models (LLMs) offer strong potential for processing such data but struggle with factual accuracy and provenance. Knowledge graphs (KGs) provide a structured framework to address these limitations [6], yet constructing high-quality KGs from heterogeneous data remains a challenge. To address this issue, we present Docs2KG, a modular framework to build high-quality KGs from diverse unstructured documents. We first employs state-of-the-art document processing techniques to extract textual content, tabular data, and figures. The extracted information is then unified into a multifaceted KG with three aspects: (1) a Layout KG capturing document structural hierarchies, (2) a Metadata KG preserving document properties, and (3) a Semantic KG representing domain-specific entities and relationships. Docs2KG supports multiple construction paradigms for Semantic KG: ontology-based approaches, hybrid NLP pipelines with LLM verification, LLM-guided ontology generation, and specialized models for named entity recognition, event extraction, and causal relationship identification to enhance semantic coverage and accuracy. A key feature of Docs2KG is its human-in-the-loop verification interface, enabling iterative quality assessment and refinement of the resulting KGs. Docs2KG is openly available at https://docs2kg.ai4wa.com, with the aim of advancing knowledge graph construction research and accelerating enterprise applications through high-quality knowledge graph construction.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Sun, Qiang and Luo, Yuanyi and Zhang, Wenxiao and Li, Sirui and Li, Jichunyang and Niu, Kai and Kong, Xiangrui and Liu, Wei},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {Knowledge graphs, Knowledge graph, Ontology, Language model, Semantics, knowledge graph, Data mining, Heterogeneous data, Graph theory, Extraction, Unstructured data, unstructured data, heterogeneous data, Graph construction, Iterative methods, Data handling, Information retrieval systems, Collaborative approach, Data accuracy, High quality, Quality knowledge, Semantics knowledge, Unstructured documents},
	pages = {801--804},
	annote = {Cited by: 0},
}

@article{novak_building_2025,
	title = {Building a {Better} {SOC}: {Towards} the {Ontology} for {Security} {Operations} {Center} {Assistance} and {Replication} ({OSCAR})},
	volume = {6},
	url = {https://doi.org/10.1145/3722233},
	doi = {10.1145/3722233},
	abstract = {There are many methods for developing a Security Operations Center (SOC) or SOC capability. However, there currently exists no unified approach which comprehensively outlines the people, processes, and technology required for developing a SOC, or how an organization might implement those into an effective SOC capability. This article outlines a data gathering process used to compile knowledge necessary for a proposed Ontology for SOC Creation Assistance and Replication, which can serve as a solution to the gap in the current body of knowledge. An ontology such as the one proposed here would leverage the collective experience of a large cadre of cybersecurity experts with deep knowledge in fields related to Security Operations and the development of SOCs. Using interview methods and analysis, the knowledge of how these experts approach the problem of creating new SOC capabilities within a set of known constraints can be captured and codified. The result is a comprehensive body of structured knowledge outlining what critical decisions are made during the process, and how those decisions affect the implementation of People, Processes, and Technology which become part of a SOC. It is this body of knowledge which can be organized and presented as a formal ontology.},
	number = {1},
	journal = {Digital Threats},
	author = {Novak, Justin and Hueca, Angel and Rodman, Christopher and Perl, Samuel and Breaux, Travis and Valdengo, Justin},
	month = mar,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology, Process, Technology, Incident Response, People, Security Operations, Security Operations Center, SOC},
}

@inproceedings{maratsi_towards_2024,
	address = {New York, NY, USA},
	series = {dg.o '24},
	title = {Towards {Cross}-{Domain} {Linking} of {Data}: {A} {Semantic} {Mapping} of {Cultural} {Heritage} {Ontologies}},
	isbn = {979-8-4007-0988-3},
	url = {https://doi.org/10.1145/3657054.3657077},
	doi = {10.1145/3657054.3657077},
	abstract = {The Linked Open Vocabularies (LOV) registry, designed with the Linked Data principles at core, provides an environment suitable for research which targets domain-specific, but also potentially reusable, information representation. The main purpose of this study is to follow the recommendations pertaining to the utilisation of LOV as a basis for experimentation in order to examine how information within the Cultural Heritage (CH) domain can be improved in terms of reusability and interoperability. The present lack of cross-domain knowledge transfer forms the motivation behind this study, with the aim of facilitating the transition from conventional, domain-specific knowledge representation to reusable and semantically interoperable information. The methodology of this study involves the manual semantic mapping of elements from 12 vocabularies in the LOV registry, reinforced by a small-scale experiment using contemporary large language models (LLMs), particularly GPT, for a preliminary assessment of the mapping process. The findings revealed several key aspects to consider regarding the alignment of semantically adjacent vocabulary elements in the CH domain and beyond, emphasising the potential unveiled by linking domain-focused schemata to standardised, established ones while preserving the conceptual hierarchies inherent to each individual knowledge domain. The contribution of this research pertains to the vision of linking data across different domains by initiating the alignment among representation schemata in CH, with the ultimate aim to expand beyond the boundaries of the in-word knowledge domain, while employing combinatory methodological approaches of technological means and human expertise to facilitate this process.},
	booktitle = {Proceedings of the 25th {Annual} {International} {Conference} on {Digital} {Government} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Maratsi, Maria Ioanna and Ahmed, Umair and Alexopoulos, Charalampos and Charalabidis, Yannis and Polini, Andrea},
	year = {2024},
	note = {event-place: Taipei, Taiwan},
	keywords = {Interoperability, Linked data, Semantics, Knowledge representation, Knowledge management, Information representation, Domain knowledge, Mapping, Computational linguistics, Reusability, Cross-domain, Domain Knowledge, Ontology's, Natural language processing systems, Domain specific, Cultural heritages, Knowledge domains, Linked Data principles, Semantics mappings, Target domain},
	pages = {165--176},
	annote = {Cited by: 3},
}

@inproceedings{a_setiawan_copombocy_2022,
	address = {New York, NY, USA},
	series = {{IC3INA} '21},
	title = {{COPOMBOCY}: {A} {COVID}-19 {Pandemic} {Ontology} {Model} of {Bogor} {City}},
	isbn = {978-1-4503-8524-4},
	url = {https://doi.org/10.1145/3489088.3489089},
	doi = {10.1145/3489088.3489089},
	abstract = {Coronavirus Disease of 2019 (COVID-19) has become a global health problem along with a declaration by the World Health Organization (WHO) on March 11, 2020, which declared it a pandemic. COVID-19 has spread to almost all countries, including Indonesia. Data related to the COVID-19 pandemic is complex and heterogeneous. To generate and maintain knowledge that is semantically stored in it, knowledge modeling is necessary to be done. This study aims to develop a model of knowledge about the COVID-19 pandemic in Bogor City in the form of ontology. The scope of knowledge includes the distribution of agents, close contact tracing, COVID-19 transmission, diagnosis, disease, type of illness, location, Large-Scale Social Restrictions (Pembatasan Sosial Berskala Besar [PSBB]) implementation, PSBB sanctions, statistics, status, symptoms, test processes, test results, and color zones. Data were obtained from the Bogor City COVID-19 Task Force, COVID-19 Information \&amp; Coordination Center of West Java Province (Pusat Informasi \&amp; Koordinasi COVID-19 Provinsi Jawa Barat [PIKOBAR]), Covid19.go.id, literature review, interview with epidemiologist, and reuse of knowledge from existing ontologies. The result of this study is an ontology in the Web Ontology Language (OWL) format consisting of 88 classes, 29 object properties, 48 property data, 2103 axioms, and 229 individuals. The test results on the built ontology were successful in answering 85.7\% of non-expert questions and 71.4\% of expert questions. Overall, the ontology built successfully answered 78.6\% of the questions about the COVID-19 pandemic in Bogor City. The ontology has been published so that it is publicly available and is still being developed to accommodate the latest data.},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Computer}, {Control}, {Informatics} and {Its} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {A. Setiawan, Foni and Murdani, Dendy and Riana, Freza and Dwimawati, Eny},
	year = {2022},
	note = {event-place: Virtual/online conference, Indonesia},
	keywords = {Ontology, COVID-19, Pandemic, Bogor City},
	pages = {86--90},
}

@inproceedings{johnson_language-agnostic_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {Language-agnostic {Topic} {Classification} for {Wikipedia}},
	isbn = {978-1-4503-8313-4},
	url = {https://doi.org/10.1145/3442442.3452347},
	doi = {10.1145/3442442.3452347},
	abstract = {A major challenge for many analyses of Wikipedia dynamics—e.g., imbalances in content quality, geographic differences in what content is popular, what types of articles attract more editor discussion—is grouping the very diverse range of Wikipedia articles into coherent, consistent topics. This problem has been addressed using various approaches based on Wikipedia’s category network, WikiProjects, and external taxonomies. However, these approaches have always been limited in their coverage: typically, only a small subset of articles can be classified, or the method cannot be applied across (the more than 300) languages on Wikipedia. In this paper, we propose a language-agnostic approach based on the links in an article for classifying articles into a taxonomy of topics that can be easily applied to (almost) any language and article on Wikipedia. We show that it matches the performance of a language-dependent approach while being simpler and having much greater coverage.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Johnson, Isaac and Gerlach, Martin and Sáez-Trumper, Diego},
	year = {2021},
	note = {event-place: Ljubljana, Slovenia},
	keywords = {Wikipedia, topic classification, language-agnostic},
	pages = {594--601},
}

@inproceedings{post_contextllm_2025,
	address = {New York, NY, USA},
	series = {{HotMobile} '25},
	title = {{ContextLLM}: {Meaningful} {Context} {Reasoning} from {Multi}-{Sensor} and {Multi}-{Device} {Data} {Using} {LLMs}},
	isbn = {979-8-4007-1403-0},
	url = {https://doi.org/10.1145/3708468.3711892},
	doi = {10.1145/3708468.3711892},
	abstract = {Conventional context awareness and activity recognition models produce abstract outputs that offer limited insights into user behavior and situational context. These can be significantly enhanced by leveraging multi-sensor and multi-device data streams. However, the aggregation and modelling of context sensor data presents complex challenges that require advanced inference capabilities. We introduce ContextLLM, a context-driven solution powered by Large Language Models (LLMs), designed to transform sparse, abstract insights from various sensors and devices into a detailed, descriptive context. Through rigorous experiments using a well-established benchmark dataset for activity recognition, we demonstrate that ContextLLM can significantly enhance context understanding. However, our analysis also highlights how the quality and complexity of sensor data representations impact the LLM's ability to accurately deduce context. Building on these findings, we develop a research agenda that outlines key challenges, and conclude with a discussion on the limitations and practical considerations of LLM-based reasoning in context-aware applications.},
	booktitle = {Proceedings of the 26th {International} {Workshop} on {Mobile} {Computing} {Systems} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Post, Kevin and Kuchida, Reo and Olapade, Mayowa and Yin, Zhigang and Nurmi, Petteri and Flores, Huber},
	year = {2025},
	note = {event-place: La Quinta, CA, USA},
	keywords = {Context modelling, Sensor data assistant, Wearable data},
	pages = {13--18},
}

@article{sartini_icon_2023,
	title = {{ICON}: {An} {Ontology} for {Comprehensive} {Artistic} {Interpretations}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3594724},
	doi = {10.1145/3594724},
	abstract = {In this work, we introduce ICON, an ontology that models artistic interpretations of artworks’ subject matter (i.e., iconographies) and meanings (i.e., symbols, iconological aspects). Developed by conceptualizing authoritative knowledge and notions taken from Panofsky’s levels of interpretation theory, ICON ontology focuses on the granularity of interpretations. It can be used to describe an interpretation of an artwork from the pre-iconographical, icongraphical, and iconological levels. Its main classes have been aligned to ontologies that come from the domains of cultural descriptions (ArCo, CIDOC-CRM, VIR), semiotics (DOLCE), bibliometrics (CITO), and symbolism (Simulation Ontology), to grant a robust schema that can be extendable using additional classes and properties coming from these ontologies. The ontology was evaluated through competency questions that range from simple recognition on a specific level of interpretation to complex scenarios. Data written using this model was compared to state-of-the-art ontologies and schemas to both highlight the current lack of a domain-specific ontology on art interpretation and show how our work fills some of the current gaps. The ontology is openly available and compliant with FAIR principles. With our ontology, we hope to encourage digital art historians working for cultural institutions in making more detailed linked open data about the content of their artifacts, to exploit the full potential of Semantic Web in linking artworks through not only subjects and common metadata but also specific symbolic interpretations, intrinsic meanings, and the motifs through which their subjects are represented. Additionally, by basing our work on theories made by different art history scholars in the last century, we make sure that their knowledge and studies will not be lost in the transition to the digital, linked open data era.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Sartini, Bruno and Baroncini, Sofia and van Erp, Marieke and Tomasi, Francesca and Gangemi, Aldo},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Semantic Web, ontology, cultural heritage, art interpretation, iconography, Iconology},
}

@inproceedings{sun_application_2024,
	address = {New York, NY, USA},
	series = {{ICISDM} '24},
	title = {The {Application} of {Constructing} {Knowledge} {Graph} of {Oral} {Historical} {Archives} {Resources} {Based} on {LLM}-{RAG}},
	isbn = {979-8-4007-1734-5},
	url = {https://doi.org/10.1145/3686397.3686420},
	doi = {10.1145/3686397.3686420},
	abstract = {Oral historical archive resources are an emerging archive resource with the rapid development of modern technology. Its "bottom-up" approach to historical research has received widespread attention in the fields of history, archives, and libraries. Under the common knowledge discovery mode, oral historical archives resources are showing a dispersed state. Information technology represented by knowledge graphs can break through the data solidification of oral historical archives, reshape the information stack of oral historical archives, and achieve knowledge association and aggregation of oral historical archive resources. The article attempts to construct a knowledge graph of the oral historical archives resources on the theme of "science and art" in the collection of T.D. Lee Library of Shanghai Jiao Tong University. It uses Large Language Model - Retrieval Augmented Generation (LLM-RAG) for knowledge extraction, and then uses a semantic model for knowledge organization and management. The article attempts to empower humanities with technology, exploring the possibility of combining "digital technology" and "humanities research", extending traditional humanities research methods, breaking down barriers between technology and humanities resources, and providing a new path reference for revealing resource content characteristics, semantic deep correlation, and multi-dimensional knowledge discovery.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Information} {System} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Yi and Yang, Wanru and Liu, Yin},
	year = {2024},
	keywords = {Knowledge Graph, LLM-RAG, Oral History Archives},
	pages = {142--149},
}

@inproceedings{mariani_hybrid_2025,
	address = {New York, NY, USA},
	series = {{SAC} '25},
	title = {A {Hybrid} {Self}-{Correcting} {Approach} for {Embedding} {Ontology}-based {Knowledge} {Graphs}},
	isbn = {979-8-4007-0629-5},
	url = {https://doi.org/10.1145/3672608.3707818},
	doi = {10.1145/3672608.3707818},
	abstract = {One significant challenge in Knowledge Graph (KG) Embedding is the generation of negative triples. Negative triples are essential as they enable a training model to distinguish between relationships that exist within the KG and those that do not. In this paper, we propose TransHySeCo approach: a Hybrid and Self-Correcting approach for embedding knowledge graphs. TransHySeCo is based on a hybrid training using both the domain semantics provided in the ontology related to the KG and the topology underlying the graph structure. Moreover, it is self-correcting. It generates new negative triples by leveraging the embeddings from previous training iterations and the (quasi-)true negatives obtained with the ontology-based negative generation method proposed in this paper. The self-correction terminates when no new (quasi-)true negative triple is generated. To evaluate TransHySeCo, we conducted experiments on different benchmark datasets and assessed the embeddings' effectiveness for the link prediction task. The results show that TransHySeCo provides KG embeddings of promising quality for link prediction.},
	booktitle = {Proceedings of the 40th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Mariani, Elisa and Seghouani, Nacéra and Ma, Yue},
	year = {2025},
	note = {event-place: Catania International Airport, Catania, Italy},
	keywords = {ontology, knowledge graph, link prediction, translational embedding},
	pages = {1156--1163},
}

@article{cederbladh_road-map_2025,
	title = {A {Road}-{Map} to {Readily} {Available} {Early} {Validation} and {Verification} of {System} {Behaviour} in {Model}-{Based} {Systems} {Engineering} using {Software} {Engineering} {Best} {Practices}},
	volume = {34},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3708520},
	doi = {10.1145/3708520},
	abstract = {In this article, we discuss how we can facilitate the growing need for early validation and verification (V\&amp;V) of system behaviour in Model-Based Systems Engineering (MBSyE). Several aspects, such as reducing cost and time to market, push companies towards integration of V\&amp;V methods earlier in development to support effective decision-making. One foundational methodology seeing increased attention in industry is the use of MBSyE, which brings benefits of models with well-defined syntax and semantics to support V\&amp;V activities, rather than relying on natural language text documentation. Despite their promise, industrial adoption of these practices is still challenging.This article presents a vision for readily available early V\&amp;V. We present a summary of the literature on early V\&amp;V in MBSyE and position existing challenges regarding potential solutions and future investigations towards this vision. We elaborate our vision by means of challenges with a specific emphasis on early V\&amp;V of system behaviour. We identify three specific challenge areas: Creating and managing Models, Organisational systems engineering aspects, and early V\&amp;V Methods. Finally, we outline a road-map to address these categories of challenges, in which we propose the transfer of established best practices from the software engineering domain to support emerging technologies in the systems engineering domain.},
	number = {5},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Cederbladh, Johan and Cicchetti, Antonio and Jongeling, Robbert},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Models, Verification, Validation, Systems, Behaviour, Early},
}

@inproceedings{afreen_learner-centered_2024,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '24},
	title = {Learner-centered {Ontology} for {Explainable} {Educational} {Recommendation}},
	isbn = {979-8-4007-0466-6},
	url = {https://doi.org/10.1145/3631700.3665226},
	doi = {10.1145/3631700.3665226},
	abstract = {Ontologies form the core of knowledge graphs, which act as faithful, semantic-rich sources for training models in delivering explainable recommendations. These models learn to extract logical paths between learners and resources to be recommended within the knowledge graph, according to behavior- and content-based patterns. Extracted paths are then used not only to provide recommendations, but also to generate accompanying textual explanations. Despite the potential of this approach, current ontologies derived from the traditional learner-resource interaction data fall short in terms of richness from an educational perspective. Conversely, general-purpose ontologies, while comprehensive in educational aspects, are overly complex for recommendation tasks. Unfortunately, a suboptimal ontology might prevent to articulate reasoning paths, and thus explanations, relevant for learners within the knowledge graph. To counter this limitation, in this paper, we propose LOXER, a novel ontology designed to unlock learner-centered logical paths for explainable educational recommendation. Our design integrates insights from diverse sources, including feedback from a local co-design group of learners, observations from specialized traditional large-scale educational recommendation datasets, and connections with well-known vocabularies of other existing ontologies. To validate our ontology, we conducted an evaluation of the explanation types it enables, involving university and lifelong learners and assessing explanation properties like effectiveness, decision-making speed, motivation, satisfaction, and confidence. Results show our ontology’s ability to foster diverse considerations during the learners’ decision-making process and to establish a semantic structure for knowledge graphs for explainable recommendation.},
	booktitle = {Adjunct {Proceedings} of the 32nd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Afreen, Neda and Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Malloci, Francesca Maridina and Marras, Mirko and Martis, Andrea Giovanni},
	year = {2024},
	note = {event-place: Cagliari, Italy},
	keywords = {Ontology, Recommendation, Explainability.},
	pages = {567--575},
}

@inproceedings{van_binsbergen_eflint_2020,
	address = {New York, NY, USA},
	series = {{GPCE} 2020},
	title = {{eFLINT}: a domain-specific language for executable norm specifications},
	isbn = {978-1-4503-8174-1},
	url = {https://doi.org/10.1145/3425898.3426958},
	doi = {10.1145/3425898.3426958},
	abstract = {Software systems that share potentially sensitive data are subjected to laws, regulations, policies and/or contracts. The monitoring, control and enforcement processes applied to these systems are currently to a large extent manual, which we rather automate by embedding the processes as dedicated and adaptable software services in order to improve efficiency and effectiveness. This approach requires such regulatory services to be closely aligned with a formal description of the relevant norms. This paper presents eFLINT, a domain-specific language developed for formalizing norms. The theoretical foundations of the language are found in transition systems and in Hohfeld's framework of legal fundamental conceptions. The language can be used to formalize norms from a large variety of sources. The resulting specifications are executable and support several forms of reasoning such as automatic case assessment, manual exploration and simulation. Moreover, the specifications can be used to develop regulatory services for several types of monitoring, control and enforcement. The language is evaluated through a case study formalizing articles 6(1)(a) and 16 of the General Data Protection Regulation (GDPR). A prototype implementation of eFLINT is discussed and is available online.},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {Association for Computing Machinery},
	author = {van Binsbergen, L. Thomas and Liu, Lu-Chi and van Doesburg, Robert and van Engers, Tom},
	year = {2020},
	note = {event-place: Virtual, USA},
	keywords = {domain-specific language, GDPR, executable specifications, normative modeling, policy enforcement},
	pages = {124--136},
}

@article{ni_hybrid_2021,
	title = {A {Hybrid} {Siamese} {Neural} {Network} for {Natural} {Language} {Inference} in {Cyber}-{Physical} {Systems}},
	volume = {21},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3418208},
	doi = {10.1145/3418208},
	abstract = {Cyber-Physical Systems (CPS), as a multi-dimensional complex system that connects the physical world and the cyber world, has a strong demand for processing large amounts of heterogeneous data. These tasks also include Natural Language Inference (NLI) tasks based on text from different sources. However, the current research on natural language processing in CPS does not involve exploration in this field. Therefore, this study proposes a Siamese Network structure that combines Stacked Residual Long Short-Term Memory (bidirectional) with the Attention mechanism and Capsule Network for the NLI module in CPS, which is used to infer the relationship between text/language data from different sources. This model is mainly used to implement NLI tasks and conduct a detailed evaluation in three main NLI benchmarks as the basic semantic understanding module in CPS. Comparative experiments prove that the proposed method achieves competitive performance, has a certain generalization ability, and can balance the performance and the number of trained parameters.},
	number = {2},
	journal = {ACM Trans. Internet Technol.},
	author = {Ni, Pin and Li, Yuming and Li, Gangmin and Chang, Victor},
	month = mar,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cyber-physical systems, Natural language inference, Siamese neural networks},
}

@inproceedings{qin_conceptual_2020,
	address = {New York, NY, USA},
	series = {{JCDL} '20},
	title = {Conceptual {Models} and {Ontological} {Schemas} for {Semantically} {Sustainable} {Digital} {Libraries}},
	isbn = {978-1-4503-7585-6},
	url = {https://doi.org/10.1145/3383583.3398545},
	doi = {10.1145/3383583.3398545},
	abstract = {Semantic frameworks build foundations for digital libraries and repositories to enable structured data and information representation and interoperability in today's interlinked information systems. Conceptual modeling and ontological schemas provide effective communication and powerful tools for creating shared understanding and sustainable systems in various digital libraries. This panel will present cases in which conceptual modeling and ontologies are used to enrich content representation and reach consensus among communities of practice, especially in fast changing digital society and emerging application domains. Four experts in knowledge organization will first give a brief introduction for their research in conceptual modeling and ontology building and then engage the audience with question answering interactions.},
	booktitle = {Proceedings of the {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} in 2020},
	publisher = {Association for Computing Machinery},
	author = {Qin, Jian and Žumer, Maja and Wang, Xiaoguang and Fan, Wei},
	year = {2020},
	note = {event-place: Virtual Event, China},
	keywords = {knowledge representation, semantic enrichment, conceptual modeling, cultural heritage resource description, library reference model, ontological modeling},
	pages = {441--442},
}

@inproceedings{rocha_ontology-based_2020,
	address = {New York, NY, USA},
	series = {{WebMedia} '20},
	title = {An {Ontology}-based {Information} {Model} for {Multi}-{Domain} {Semantic} {Modeling} and {Analysis} of {Smart} {City} {Data}},
	isbn = {978-1-4503-8196-3},
	url = {https://doi.org/10.1145/3428658.3430973},
	doi = {10.1145/3428658.3430973},
	abstract = {Smart city services are typically defined according to domains (e.g., health, education, safety) and supported by different systems. Consequently, the analysis of smart city data is often domain-specific, thus limiting the capabilities of the offered services and hampering decision-making that relies on isolated domain information. To support a suitable analysis across multiple domains, it is necessary having a unified data model able to handle the inherent heterogeneity of smart city data and take into account both geographic and citizen information. This paper presents an ontology-based information model to support multi-domain analysis in smart cities to foster interoperability and powerful automated reasoning upon unambiguous information. The proposed information model follows Linked Data principles and takes advantage of ontologies to define information semantically. The semantic relationships and properties defined in the model also allow inferring new pieces of information that improve accuracy when analyzing multiple city domains. This paper reports an evaluation of the information model through ontological metrics and competence questions.},
	booktitle = {Proceedings of the {Brazilian} {Symposium} on {Multimedia} and the {Web}},
	publisher = {Association for Computing Machinery},
	author = {Rocha, Bartira Dantas and Silva, Larysse and Batista, Thais and Cavalcante, Everton and Gomes, Porfírio},
	year = {2020},
	note = {event-place: São Luís, Brazil},
	keywords = {Ontologies, Semantic search, Smart cities, Information model, Linked Data, Inference},
	pages = {73--80},
}

@inproceedings{rossetto_generating_2023,
	address = {New York, NY, USA},
	series = {{LGM3A} '23},
	title = {Generating {Multimodal} {Augmentations} with {LLMs} from {Song} {Metadata} for {Music} {Information} {Retrieval}},
	isbn = {979-8-4007-0283-9},
	url = {https://doi.org/10.1145/3607827.3616842},
	doi = {10.1145/3607827.3616842},
	abstract = {In this work we propose a set of new automatic text augmentations that leverage Large Language Models from song metadata to improve on music information retrieval tasks. Compared to recent works, our proposed methods leverage large language models and copyright-free corpora from web sources, enabling us to release the knowledge sources collected. We show how combining these representations with the audio signal provides a 21\% relative improvement on five of six datasets on genre classification, emotion recognition and music tagging, achieving state-of-the-art in three (GTZAN, FMA-Small and Deezer). We demonstrate the benefit of injecting external knowledge sources by comparing them withintrinsic text representation methods that rely only on the sample's information.},
	booktitle = {Proceedings of the 1st {Workshop} on {Large} {Generative} {Models} {Meet} {Multimodal} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Rossetto, Federico and Dalton, Jeffrey and Murray-Smith, Roderick},
	year = {2023},
	note = {event-place: Ottawa ON, Canada},
	keywords = {multimodal learning, large language models application, music information retrieval},
	pages = {51--59},
}

@inproceedings{tan_rule-based_2025,
	address = {New York, NY, USA},
	series = {{NLPIR} '24},
	title = {Rule-{Based} vs. {AI}-{Driven}: {Comparing} {PolyAQG} {Framework} and {Generative} {AI} {Models}},
	isbn = {979-8-4007-1738-3},
	url = {https://doi.org/10.1145/3711542.3711583},
	doi = {10.1145/3711542.3711583},
	abstract = {This comparative analysis examines the PolyAQG framework and Generative AI models (e.g., ChatGPT, Gemini) across ten key criteria for question generation. The PolyAQG framework, a rule-based approach, is well-suited for structured content and excels in generating consistent questions for educational purposes. However, it may be limited in creativity and depth. Generative AI models, while capable of covering broader topics and interpreting complex contexts, require more computational resources and may introduce inaccuracies in specialized domains. The PolyAQG framework offers scalability within specific domains and predictable error handling. Generative AI models, although scalable across topics, may require fine-tuning for accuracy. Furthermore, Generative AI enables dynamic user interaction and fosters critical thinking, while the PolyAQG framework provides a more limited user interface. The choice between PolyAQG and generative AI depends on application needs. PolyAQG is ideal for structured questions and consistency, while generative AI excels in creativity, adaptability, and user interaction.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Tee Hean},
	year = {2025},
	keywords = {contextual understanding, domain-specific, Generative AI model, PolyAQG framework, questions generation, rule-based, scalability},
	pages = {298--303},
}

@inproceedings{e_samaridi_lexicographic_2021,
	address = {New York, NY, USA},
	series = {{PCI} '20},
	title = {Lexicographic {Environments} in {Natural} {Language} {Processing} ({NLP})},
	isbn = {978-1-4503-8897-9},
	url = {https://doi.org/10.1145/3437120.3437310},
	doi = {10.1145/3437120.3437310},
	abstract = {In this paper, a literature review is presented in reference to the most important lexicographical environments that have been developed in the last decades, with the aim of utilizing them in various knowledge management applications, in the wider area of ​​the Semantic Web (SW). This paper focuses on the semantic networks of lexical information, in order to highlight their value but also the urgent need to design and implement an electronic conceptual dictionary with ontological structure for the modern Greek language, according to international dictionary standards.},
	booktitle = {Proceedings of the 24th {Pan}-{Hellenic} {Conference} on {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {E. Samaridi, Nikoletta and N. Karanikolas, Nikitas and C. Papakitsos, Evangelos},
	year = {2021},
	note = {event-place: Athens, Greece},
	keywords = {ontology, corpus, Computational Lexicography, conceptual dictionary, semantic networks},
	pages = {219--222},
}

@article{asprino_ontology_2024,
	title = {An {Ontology} {Network} for {Citizen} {Curation}},
	volume = {17},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3704729},
	doi = {10.1145/3704729},
	abstract = {Citizen curation is gaining momentum as a new form of engagement with cultural heritage. Citizen curatorial activities require and produce a wealth of information, ranging from descriptions of the artefacts to visitor experience feedback. Although formalising and integrating such various data is of paramount importance, the domain lacks comprehensive ontologies to enable querying, interpreting and reasoning over the collected data. Social Participation, Cohesion and Inclusion through Cultural Engagement (SPICE) is an EU project dedicated to experimenting with citizen curation activities to foster cultural engagement. SPICE develops technologies that help communities to create and share their own interpretation of cultural artefacts, hence developing a better understanding of, and empathy for, themselves and other communities. Part of the SPICE ecosystem of technologies is the SPICE Ontology Network (SON), which empowers applications with knowledge-level reasoning abilities and supports both applications and users interacting with data involved in citizen curation activities. This article provides an overview of the SON and outlines its main use cases.},
	number = {4},
	journal = {J. Comput. Cult. Herit.},
	author = {Asprino, Luigi and Damiano, Rossana and Daquino, Marilena and De Giorgis, Stefano and Gangemi, Aldo and Lieto, Antonio and Sartini, Bruno and Striani, Manuel},
	month = dec,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontologies, semantic web, cultural heritage, citizen curation},
}

@article{moldovan_openuidl_2020,
	title = {{OpenUIDL}, {A} {User} {Interface} {Description} {Language} for {Runtime} {Omni}-{Channel} {User} {Interfaces}},
	volume = {4},
	url = {https://doi.org/10.1145/3397874},
	doi = {10.1145/3397874},
	abstract = {We extend the concept of cross-device user interfaces into the new, more general, concept of omni-channel user interfaces to better reflect the technological variety offered for developing multi-target user interfaces for interactive applications. We present a model-based approach for developing runtime omni-channel user interfaces for multi-target applications, which consists of: (1) OpenUIDL, a user interface description language for describing omni-channel user interfaces with its semantics by a meta-model and its syntax based on JSON, (2) the definition of a step-wise approach for producing runtime interactive applications based onOpenUIDLwith integration into the development life cycle, (3) the development of a cloud-based, OpenUIDL compliant, Interactive Development Environment that supports the application and the enactment of the step-wise approach and its illustration on several multi-target user interfaces.},
	number = {EICS},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Moldovan, Alex and Nicula, Vlad and Pasca, Ionut and Popa, Mihai and Namburu, Jaya Krishna and Oros, Anamaria and Brie, Paul},
	month = jun,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {open source, model-based user interface, multi-target user interfaces, omni-channel user interfaces, user interface description language},
}

@inproceedings{wagner_towards_2019,
	address = {San Diego, CA, USA},
	series = {{SummerSim} '19},
	title = {Towards a non-proprietary modeling language for processing network simulation},
	abstract = {Processing networks have been investigated in the mathematical theory of queueing and have been the application focus of most industrial simulation software products, starting with GPSS and SIMAN/Arena. They allow modeling many forms of discrete processing processes, and are mainly used in simulation projects for the manufacturing and services industries. However, there is still no proper vendor-neutral language definition for this paradigm, e.g., in the form of a meta-model defining an abstract syntax for specifying the structure and dynamics of processing networks. We reconstruct the core of this paradigm in the form of a UML-based meta-model and show how to map a processing network specified with this metamodel to an Object Event Simulation model providing its operational semantics.},
	booktitle = {Proceedings of the 2019 {Summer} {Simulation} {Conference}},
	publisher = {Society for Computer Simulation International},
	author = {Wagner, Gerd},
	year = {2019},
	note = {event-place: Berlin, Germany},
	keywords = {arena, GPSS, processing networks, queuing networks, SIMAN},
}

@article{jang_cross-language_2018,
	title = {Cross-{Language} {Neural} {Dialog} {State} {Tracker} for {Large} {Ontologies} {Using} {Hierarchical} {Attention}},
	volume = {26},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2018.2852492},
	doi = {10.1109/TASLP.2018.2852492},
	abstract = {Dialog state tracking, which refers to identifying the user intent from utterances, is one of the most important tasks in dialog management. In this paper, we present our dialog state tracker developed for the fifth dialog state tracking challenge, which focused on cross-language adaptation using a very scarce machine-translated training data when compared to the size of the ontology. Our dialog state tracker is based on the bi-directional long short-term memory network with a hierarchical attention mechanism in order to spot important words in user utterances. The user intent is predicted by finding the closest keyword in the ontology to the attention-weighted word vector. With the suggested methodology, our tracker can overcome various difficulties due to the scarce training data that existing machine learning-based trackers had, such as predicting user intents they have not seen before. We show that our tracker outperforms other trackers submitted to the challenge with respect to most of the performance measures.},
	number = {11},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Jang, Youngsoo and Ham, Jiyeon and Lee, Byung-Jun and Kim, Kee-Eung},
	month = nov,
	year = {2018},
	note = {Publisher: IEEE Press},
	pages = {2072--2082},
}

@inproceedings{ehrlinger_news_2024,
	address = {New York, NY, USA},
	series = {{SAC} '24},
	title = {A {News} {Article} {Tag} {Categorization} {Ontology}},
	isbn = {979-8-4007-0243-3},
	url = {https://doi.org/10.1145/3605098.3636153},
	doi = {10.1145/3605098.3636153},
	abstract = {In recent years, the demand for personalized and subject-specific news has been growing. Users and organizations alike are requesting the ability to curate individual timelines based on their topics of interest. To enable the easy curation and discovery of such timelines, a suitable news tag categorization is required that is both, news-domain-oriented, and fine-grained enough to cover specific (e.g., regional) use cases. Since existing categorization systems do not sufficiently fulfill both requirements, we developed an ontology that contains news article tags based on media topics by the International Press Telecommunications Council and Wikipedia categories. The ontology has been implemented within the Newsadoo platform, where it improves the topic curation and exploration process.},
	booktitle = {Proceedings of the 39th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Ehrlinger, Lisa and Holzner, Harald and Hemelmayr, Nora and Wöß, Wolfram},
	year = {2024},
	note = {event-place: Avila, Spain},
	keywords = {ontologies, news articles, tag categorization, wikipedia categories},
	pages = {1665--1667},
}

@inproceedings{morales_automating_2024,
	address = {Echternach, Luxembourg},
	series = {{ASE} '23},
	title = {Automating {Bias} {Testing} of {LLMs}},
	isbn = {979-8-3503-2996-4},
	url = {https://doi.org/10.1109/ASE56229.2023.00018},
	doi = {10.1109/ASE56229.2023.00018},
	abstract = {Large Language Models (LLMs) are being quickly integrated in a myriad of software applications. This may introduce a number of biases, such as gender, age or ethnicity, in the behavior of such applications. To face this challenge, we explore the automatic generation of tests suites to assess the potential biases of an LLM. Each test is defined as a prompt used as input to the LLM and a test oracle that analyses the LLM output to detect the presence of biases.},
	booktitle = {Proceedings of the 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Morales, Sergio and Clarisó, Robert and Cabot, Jordi},
	year = {2024},
	keywords = {large language models, testing, ethics, bias, fairness},
	pages = {1705--1707},
}

@article{dimartino_efficient_2025,
	title = {Efficient {Ontology}-{Mediated} {Query} {Answering}: {Extending} {DL}-{liteR} and {Linear} {ELH}},
	volume = {82},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.16401},
	doi = {10.1613/jair.1.16401},
	abstract = {The OWL 2 QL profile of the OWL 2 Web Ontology Language, based on the family of description logics called DL-Lite, is designed so that data stored in a standard relational database system (RDBMS) can be queried through an ontology via a rewriting mechanism, i.e., by rewriting the query into an SQL query that is then answered by the RDBMS system, without any changes to the data. In this paper we propose a language whose expressive power goes beyond that of DL-Lite while still allowing query answering via rewriting of queries into unions of conjunctive two-way regular path queries (UC2RPQs) instead of SQL queries. Our language is an extension of both OWL 2 QL and linear ELH: OWL 2 QL is extended by allowing qualified existential quantification on the left-hand side of concept inclusion axioms, and linear ELH by allowing inverses in role inclusion axioms. We identify a syntactic property of the extended language that guarantees UC2RPQ-rewritability. We propose a novel rewriting technique for conjunctive queries (CQs) under our ontology language that makes use of nondeterministic finite state automata. We show that CQ answering in our setting is NLOGSPACE-complete with respect to data complexity and NP-complete for combined complexity; we also show that answering instance queries is NLOGSPACE-complete for data complexity and in PTIME for combined complexity.},
	journal = {J. Artif. Int. Res.},
	author = {Dimartino, Mirko M. and Wood, Peter T. and Cali, Andrea and Poulovassilis, Alexandra},
	month = apr,
	year = {2025},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
}

@article{bibi_reusable_2023,
	title = {Reusable {Component} {Retrieval}: {A} {Semantic} {Search} {Approach} for {Low}-{Resource} {Languages}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3564604},
	doi = {10.1145/3564604},
	abstract = {A common practice among programmers is to reuse existing code, accomplished by performing natural language queries through search engines. The main aim of code retrieval is to search for the most relevant snippet from a corpus of code snippets. However, code retrieval frameworks for low-resource languages are insufficient. Retrieving the most relevant code snippet efficiently can be accomplished only by eliminating the semantic gap between the code snippets residing in the repository and the user’s query (natural language description). The primary objective of the research is to contribute to this field by providing a code search framework that can be extended for low-resource languages. The secondary objective is to provide a code retrieval mechanism that is semantically relevant to the user query and provide programmers with the ability to locate source code that they want to use when developing new applications. The proposed approach is implemented using a web platform to search for source code. As code retrieval is a sophisticated task, the proposed approach incorporates a semantic search mechanism. This research uses a semantic model for code retrieval, which generates meanings or synonyms of words. The proposed model integrates ontologies and Natural Language Processing. System performance measures and classification accuracy are computed using precision, recall, and F1-score. We also compare the proposed approach with state-of-the-art baseline models. The retrieved results are ranked, showing that our approach significantly outperforms robust code matching. Our evaluation shows that semantic matching leads to improved source code retrieval. This study marks a substantial advancement in integrating programming expertise with code retrieval techniques. Moreover, our system lets users know when and how it is used for successful semantic searching.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Bibi, Nazia and Rana, Tauseef and Maqbool, Ayesha and Alkhalifah, Tamim and Khan, Wazir Zada and Bashir, Ali Kashif and Zikria, Yousaf Bin},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontologies, information retrieval, web semantics, source code retrieval, source code search},
}

@article{vats_hkg_2023,
	title = {{HKG}: {A} {Novel} {Approach} for {Low} {Resource} {Indic} {Languages} to {Automatic} {Knowledge} {Graph} {Construction}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3611306},
	doi = {10.1145/3611306},
	abstract = {Knowledge graph (KG), a visual representation of text data as a semantic network, holds enormous promise for the development of more intelligent robots. It leads to significant potential solutions for many tasks like question answering, recommendation, and information retrieval. However, this area is confined to using English text only. Since low-resource languages are now being used in the world of AI, it is necessary to develop a semantic network for them as well. In this research work, the authors provide state-of-the-art techniques for automatic knowledge graph construction for the Hindi language, which is still unexplored in ontology. Constructing a knowledge graph faces several hurdles and obstacles in the linguistic domain, primarily when it deals with the Hindi language. With an emphasis on the Indian perspective, this research intends to introduce a novel approach ‘HKG’ for knowledge graph construction framework for Hindi. It also implements the LSTM model to evaluate the accuracy of newly constructed knowledge graphs and compute different evaluation metrics such as accuracy and F1-score. This knowledge graph evaluates the accuracy of 87.50 using Doc2Vec word embedding with a train-test split of 7:3.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Vats, Preeti and Sharma, Nonita and Sharma, Deepak Kumar},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural Language Processing, LSTM, Knowledge graph Construction, Link Analysis and Neighbor Nodes, Low Resource Indian Languages, Stanza},
	annote = {Just Accepted},
}

@inproceedings{costa_using_2024,
	address = {New York, NY, USA},
	series = {{IHC} '23},
	title = {Using {Networked} {Ontologies} to support {UX} {Evaluation} in {Immersive} {Context}},
	isbn = {979-8-4007-1715-4},
	url = {https://doi.org/10.1145/3638067.3638080},
	doi = {10.1145/3638067.3638080},
	abstract = {Over the past few years, new interactive systems such as immersive technologies have gradually permeated our daily lives and found adoption across various fields. Immersive technologies provide users with immersive experiences. Assessing and modeling the quality of such experiences has become a trending topic in HCI, and UX is a key quality attribute in this context. When it comes to immersive experiences, evaluating UX is particularly challenging because the user should not be interrupted to provide feedback. In this paper, we propose using networked ontologies to support evaluating immersive experiences. We have explored using ontologies from an ontology network addressing the HCI domain to develop a tool that supports UX experts evaluating such experiences based on data recorded in interaction logs. We used the ontology-based tool to evaluate the UX of an immersive application that supports collaborative music composition. The tool extracted data from the application interaction logs applied UX metrics, and provided consolidated data and information in graphs and tables. We conducted a study and collected feedback from the tool developer and three UX experts who used the tool. Results showed that using networked ontologies to develop a tool to support UX evaluation is feasible and valuable. In summary, the ontologies helped at the conceptual level by offering a basis to define the system’s structural model and at the implementation level by assigning semantics to data to make inferences about UX. Based on the UX experts’ perceptions, the tool was considered a promising system, beneficial, helpful, and easy to use.},
	booktitle = {Proceedings of the {XXII} {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Costa, Simone Dornelas and Manso, Carolina De Freitas and Marques, Leonardo Carneiro and Gadelha, Bruno Freitas and Conte, Tayana Uchôa and Barcellos, Monalessa Perini},
	year = {2024},
	note = {event-place: Maceió, Brazil},
	keywords = {Ontology, Immersive Experience, Ontology Network, User Experience, UX Evaluation},
}

@inproceedings{molnar_towards_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Towards the {Formal} {Verification} of {SysML} v2 {Models}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3687820},
	doi = {10.1145/3652620.3687820},
	abstract = {Systems Modeling Language (SysML) is the de facto standard in the industry for modeling complex systems. SysML v2 is the new version of the language with reworked fundamentals. In this paper, we explore how the new formal semantics of SysML v2 can enable formal verification and various forms of automated reasoning. Formal verification involves mathematically proving the correctness of a system's design with respect to certain specifications or properties. This rigorous approach ensures that models behave as intended under all possible conditions. Through a detailed examination, we demonstrate how five specific tools - Gamma, MP-Firebird, Imandra, SAVVS, and SysMD - can formally analyze SysML v2 models. We show how these tools support the different concepts in the language, as well as the set of features and technologies they provide to users of SysML v2, such as model checking, theorem proving, contract-based design, or automatic fault injections. We propose a workflow for applying formal methods on SysML v2 models, illustrated by example models and artifacts generated by the above tools.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Molnár, Vince and Graics, Bence and Vörös, András and Tonetta, Stefano and Cristoforetti, Luca and Kimberly, Greg and Dyer, Pamela and Giammarco, Kristin and Koethe, Manfred and Hester, John and Smith, Jamie and Grimm, Christoph},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {tools, automated reasoning, formal methods, SysML V2, systems modeling, verification and validation},
	pages = {1086--1095},
}

@inproceedings{zhu_emerge_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {{EMERGE}: {Enhancing} {Multimodal} {Electronic} {Health} {Records} {Predictive} {Modeling} with {Retrieval}-{Augmented} {Generation}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679582},
	doi = {10.1145/3627673.3679582},
	abstract = {The integration of multimodal Electronic Health Records (EHR) data has significantly advanced clinical predictive capabilities. Existing models, which utilize clinical notes and multivariate time-series EHR data, often fall short of incorporating the necessary medical context for accurate clinical tasks, while previous approaches with knowledge graphs (KGs) primarily focus on structured knowledge extraction. In response, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR predictive modeling. We extract entities from both time-series data and clinical notes by prompting Large Language Models (LLMs) and align them with professional PrimeKG, ensuring consistency. In addition to triplet relationships, we incorporate entities' definitions and descriptions for richer semantics. The extracted knowledge is then used to generate task-relevant summaries of patients' health statuses. Finally, we fuse the summary with other modalities using an adaptive multimodal fusion network with cross-attention. Extensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital mortality and 30-day readmission tasks demonstrate the superior performance of the EMERGE framework over baseline models. Comprehensive ablation studies and analysis highlight the efficacy of each designed module and robustness to data sparsity. EMERGE contributes to refining the utilization of multimodal EHR data in healthcare, bridging the gap with nuanced medical contexts essential for informed clinical predictions. We have publicly released the code at https://github.com/yhzhu99/EMERGE.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Yinghao and Ren, Changyu and Wang, Zixiang and Zheng, Xiaochen and Xie, Shiyun and Feng, Junlan and Zhu, Xi and Li, Zhoujun and Ma, Liantao and Pan, Chengwei},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {large language model, electronic health record, retrieval-augmented generation, multimodal learning},
	pages = {3549--3559},
}

@inproceedings{chang_ontology-based_2020,
	address = {New York, NY, USA},
	series = {{SAC} '20},
	title = {Ontology-based knowledge model for human-robot interactive services},
	isbn = {978-1-4503-6866-7},
	url = {https://doi.org/10.1145/3341105.3373977},
	doi = {10.1145/3341105.3373977},
	abstract = {Recently, increasing interest in service robots with artificial intelligence has triggered several studies on developing service robots as human assistants. To perform automated tasks, service robots are not only required to recognize various attributes of the service environment, but they must also determine, which tasks and behaviors to perform, according to their internal system specifications and those of the individual situation. To perform tasks in such a generalized manner efficiently, the service robot must abstractly understand the recognition data obtained from the external environment and plan its tasks by combining the data and existing knowledge. Thus, an abstract and integrated knowledge management of low-level external recognition data and symbolic-level knowledge is indispensable. This study proposes a knowledge model for an integrated robot framework, which is used to provide human-robot interactive services. Our knowledge model is based on an ontology that contains general knowledge which provides clear definitions of common concepts and domain knowledge which provides specific concepts required to understand the information about agents(users and robots), and the environments about human-robot interactive services. An exemplary experiment is given in the context of a social robot service, which shows the usability of our knowledge model.},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Chang, Doo Soo and Cho, Gun Hee and Choi, Yong Suk},
	year = {2020},
	note = {event-place: Brno, Czech Republic},
	keywords = {human-robot interaction, knowledge framework, ontology-based knowledge processing, social robot service},
	pages = {2029--2038},
}

@inproceedings{pernisch_toward_2021,
	address = {New York, NY, USA},
	series = {K-{CAP} '21},
	title = {Toward {Measuring} the {Resemblance} of {Embedding} {Models} for {Evolving} {Ontologies}},
	isbn = {978-1-4503-8457-5},
	url = {https://doi.org/10.1145/3460210.3493540},
	doi = {10.1145/3460210.3493540},
	abstract = {Updates on ontologies affect the operations built on top of them. But not all changes are equal: some updates drastically change the result of operations; others lead to minor variations, if any. Hence, estimating the impact of a change ex-ante is highly important, as it might make ontology engineers aware of the consequences of their action during editing. However, in order to estimate the impact of changes, we need to understand how to measure them. To address this gap for embeddings, we propose a new measure called Embedding Resemblance Indicator (ERI), which takes into account both the stochasticity of learning embeddings as well as the shortcomings of established comparison methods. We base ERI on (i) a similarity score, (ii) a robustness factor hatμ (based on the embedding method, similarity measure, and dataset), and (iii) the number of added or deleted entities to the embedding computed with the Jaccard index.To evaluate ERI, we investigate its usage in the context of two biomedical ontologies and three embedding methods—GraRep, LINE, and DeepWalk—as well as the two standard benchmark datasets—FB15k-237 and Wordnet-18-RR—with TransE and RESCAL embeddings. To study different aspects of ERI, we introduce synthetic changes in the knowledge graphs, generating two test-cases with five versions each and compare their impact with the expected behaviour. Our studies suggests that ERI behaves as expected and captures the similarity of embeddings based on the severity of changes. ERI is crucial for enabling further studies into impact of changes on embeddings.},
	booktitle = {Proceedings of the 11th {Knowledge} {Capture} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Pernisch, Romana and Dell'Aglio, Daniele and Bernstein, Abraham},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {embedding similarity, knowledge graph embeddings, ontology evolution},
	pages = {177--184},
}

@inproceedings{kretzer_closing_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Closing the {Loop} between {User} {Stories} and {GUI} {Prototypes}: {An} {LLM}-{Based} {Assistant} for {Cross}-{Functional} {Integration} in {Software} {Development}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713932},
	doi = {10.1145/3706598.3713932},
	abstract = {Graphical user interfaces (GUIs) are at the heart of almost every software we encounter. GUIs are often created through a collaborative effort involving UX designers, product owners, and software developers, constantly facing changing requirements. Historically, problems in GUI development include a fragmented, poorly integrated tool landscape and high synchronization efforts between stakeholders. Recent approaches suggest using large language models (LLMs) to recognize requirements fulfillment in GUIs and automatically propose new GUI components. Based on ten interviews with practitioners, this paper proposes an LLM-based assistant as a Figma plug-in that bridges the gap between user stories and GUI prototyping. We evaluated the prototype with 40 users and 40 crowd-workers, showing that the effectiveness of GUI creation is improved by using LLMs to detect requirements’ completion and generate new GUI components. We derive design rationales to support cross-functional integration in software development, ensuring that our plug-in integrates well into established processes.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kretzer, Felix and Kolthoff, Kristian and Bartelt, Christian and Ponzetto, Simone Paolo and Maedche, Alexander},
	year = {2025},
	keywords = {Requirements, Assistance, GUI Prototypes, User Stories},
}

@inproceedings{dunbar_data_2024,
	address = {New York, NY, USA},
	series = {{HttF} '24},
	title = {Data {Sets} for {Regenerative} {Futures}: {Cultivating} {Relational} {Ontologies}},
	isbn = {979-8-4007-1042-1},
	url = {https://doi.org/10.1145/3686169.3686200},
	doi = {10.1145/3686169.3686200},
	abstract = {This short paper examines the limitations of current urban data infrastructures in representing more-than-human perspectives and explores pathways toward more inclusive, regenerative data practices. It argues that prevalent data sets and knowledge repositories reflect anthropocentric ontologies, perpetuating the erasure of non-human subjects and diverse epistemologies. The paper identifies key challenges in developing more inclusive data infrastructures, including ontological incommensurability, the risk of cognitive injustice, and the need for expanded representational repertoires. Drawing on examples from Indigenous knowledge systems, multispecies ethnographies, and arts-based practices, the authors propose strategies for enriching urban datasets. These include centering marginalized perspectives, expanding participatory data governance models, and reimagining urban infrastructure as multi-species sensing apparatuses. The paper concludes by calling for the cultivation of relational data ontologies that can better capture the complex interdependencies between human and more-than-human worlds, essential for envisioning and realizing regenerative urban futures.},
	booktitle = {Proceedings of the {Halfway} to the {Future} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Dunbar, Michael and Speed, Chris},
	year = {2024},
	note = {event-place: Santa Cruz, CA, USA},
	keywords = {More-than-human, Data sets, Regenerative Futures},
}

@inproceedings{schlutter_knowledge_2020,
	address = {New York, NY, USA},
	series = {{ICSEW}'20},
	title = {Knowledge {Extraction} from {Natural} {Language} {Requirements} into a {Semantic} {Relation} {Graph}},
	isbn = {978-1-4503-7963-2},
	url = {https://doi.org/10.1145/3387940.3392162},
	doi = {10.1145/3387940.3392162},
	abstract = {Knowledge extraction and representation aims to identify information and to transform it into a machine-readable format. Knowledge representations support Information Retrieval tasks such as searching for single statements, documents, or metadata. Requirements specifications of complex systems such as automotive software systems are usually divided into different subsystem specifications. Nevertheless, there are semantic relations between individual documents of the separated subsystems, which have to be considered in further processes (e.g. dependencies). If requirements engineers or other developers are not aware of these relations, this can lead to inconsistencies or malfunctions of the overall system. Therefore, there is a strong need for tool support in order to detects semantic relations in a set of large natural language requirements specifications. In this work we present a knowledge extraction approach based on an explicit knowledge representation of the content of natural language requirements as a semantic relation graph. Our approach is fully automated and includes an NLP pipeline to transform unrestricted natural language requirements into a graph. We split the natural language into different parts and relate them to each other based on their semantic relation. In addition to semantic relations, other relationships can also be included in the graph. We envision to use a semantic search algorithm like spreading activation to allow users to search different semantic relations in the graph.},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {Association for Computing Machinery},
	author = {Schlutter, Aaron and Vogelsang, Andreas},
	year = {2020},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {natural language processing, knowledge extraction, requirement engineering, semantic relation graph, spreading activation},
	pages = {373--379},
}

@inproceedings{chen_rarebench_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {{RareBench}: {Can} {LLMs} {Serve} as {Rare} {Diseases} {Specialists}?},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671576},
	doi = {10.1145/3637528.3671576},
	abstract = {Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs' diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4's diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang, Lun and Zhang, Shuyang and Chen, Ting},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {rare disease diagnosis, evaluation, benchmark for llms},
	pages = {4850--4861},
}

@article{steinert_choosing_2025,
	title = {Choosing the {Right} {Ontology} to {Describe} {Research} {Data} in the {Energy} {Domain}},
	volume = {4},
	url = {https://doi.org/10.1145/3717413.3717417},
	doi = {10.1145/3717413.3717417},
	abstract = {As in all disciplines, increasing the FAIRness (findability, accessibility, interoperability, and reusability) of research data and software is a goal in energy research. In order to achieve this, it is important to identify the most appropriate ontology for the description of research data and software. However, despite the importance of this task, it still presents a significant challenge. While there are some comparisons of ontologies, a gap exists in assessing their usefulness according to ontology metadata. This paper fills this gap by defining 21 criteria sorted into four categories to help researchers choose ontologies in the energy domain. The criteria are used to compare eight ontologies for energy research to showcase their use and analyze the ontologies. The analysis reveals the Open Energy Ontology (OEO) as the top-ranked ontology. This underscores the importance of metadata comparison in ontology selection and highlights the benefits of incorporating metadata criteria into ontology terminology services to support researchers.},
	number = {4},
	journal = {SIGENERGY Energy Inform. Rev.},
	author = {Steinert, Alexandro and Ferenz, Stephan and Niesse, Astrid},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, semantic web, FAIR principles, energy research, energy research data management},
	pages = {33--48},
}

@article{mundotiya_development_2023,
	title = {Development of a {Dataset} and a {Deep} {Learning} {Baseline} {Named} {Entity} {Recognizer} for {Three} {Low} {Resource} {Languages}: {Bhojpuri}, {Maithili}, and {Magahi}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3533428},
	doi = {10.1145/3533428},
	abstract = {In Natural Language Processing (NLP) pipelines, Named Entity Recognition (NER) is one of the preliminary problems, which marks proper nouns and other named entities such as Location, Person, Organization, Disease and so on. Such entities, without an NER module, adversely affect the performance of a machine translation system. NER helps in overcoming this problem by recognizing and handling such entities separately, although it can be useful in Information Extraction systems also. Bhojpuri, Maithili, and Magahi are low resource languages, usually known as Purvanchal languages. This article focuses on the development of an NER benchmark dataset for Machine Translation systems developed to translate from these languages to Hindi by annotating parts of the available corpora with named entities. Bhojpuri, Maithili, and Magahi corpora of sizes 228,373, 157,468, and 56,190 tokens, respectively, were annotated using 22 entity labels. The annotation considers coarse-grained annotation labels followed by the tagset used in one of the Hindi NER datasets. We also report a Deep Learning baseline that uses an LSTM-CNNs-CRF model. The lower baseline F1-scores from the NER tool obtained by using Conditional Random Fields models are 70.56\% for Bhojpuri, 73.19\% for Maithili, and 84.18\% for Magahi. The Deep Learning-based technique (LSTM-CNNs-CRF) achieved 61.41\% for Bhojpuri, 71.38\% for Maithili, and 86.39\% for Magahi. As the results show, LSTM-CNNs-CRF fails to outperform the lower baseline in the case of Bhojpuri and Maithili, which have more data in terms of the number of tokens, but not in terms of the number of named entities. However, the cross-lingual model training of LSTM-CNNs-CRF for Bhojpuri and Maithili performed better than the CRF.},
	number = {1},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Mundotiya, Rajesh and Kumar, Shantanu and Kumar, Ajeet and Chaudhary, Umesh and Chauhan, Supriya and Mishra, Swasti and Gatla, Praveen and Singh, Anil Kumar},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Deep Learning, named entity recognition, Bhojpuri, Conditional Random Fields, Indo-Aryan languages, low resource languages, Magahi, Maithili, Purvanchal languages},
}

@inproceedings{masa_ontology-based_2022,
	address = {New York, NY, USA},
	series = {{SETN} '22},
	title = {Ontology-based {Modelling} and {Reasoning} for {Forest} {Fire} {Emergencies} in {Resilient} {Societies}},
	isbn = {978-1-4503-9597-7},
	url = {https://doi.org/10.1145/3549737.3549765},
	doi = {10.1145/3549737.3549765},
	abstract = {Every year, thousands of forest fires throughout the world cause disasters. One of the most critical challenges during a wildfire disaster is the effective management of heterogeneous information relative to the crisis to support human operators and authorities. Towards addressing this challenge, this paper presents an ontology-based framework for data representation and interlinking of wildfire events that are being used to foster advanced reasoning, situational awareness and interpretation for decision support. More specifically, we illustrate the capabilities of the ONTO-SAFE ontology to symbolically model contextual information in the domain, addressing application and user requirements promoting the creation of interoperable knowledge graphs. On top of the symbolic knowledge graphs, we define a rule-based framework for infusing expert knowledge in the form of constraints and rules to recognize patterns and situations of interest based on domain knowledge, assisting end-users in taking informed decisions and facilitating advanced decision-making.},
	booktitle = {Proceedings of the 12th {Hellenic} {Conference} on {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Masa, Panagiota and Meditskos, Georgios and Kintzios, Spyridon and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
	year = {2022},
	note = {event-place: Corfu, Greece},
}

@inproceedings{de_almeida_identifying_2022,
	address = {New York, NY, USA},
	series = {{iiWAS2021}},
	title = {Identifying {Legal} {Party} {Members} from {Legal} {Opinion} {Documents} using {Natural} {Language} {Processing}},
	isbn = {978-1-4503-9556-4},
	url = {https://doi.org/10.1145/3487664.3487700},
	doi = {10.1145/3487664.3487700},
	abstract = {Law and order is a field that can highly benefit from the contribution of Natural Language Processing (NLP) to its betterment. An area in which NLP can be of immense help is, information retrieval from legal documents which function as legal databases. The extraction of legal parties from the aforementioned legal documents can be identified as a task of high importance since it has a significant impact on the proceedings of contemporary legal cases. This study proposes a novel deep learning methodology which can be effectively used to find a solution to the problem of identifying legal party members in legal documents. In addition to that, in this paper, we introduce a novel data set which is annotated with legal party information by an expert in the legal domain. The deep learning model proposed in this study provides a benchmark for the legal party identification task on this data set. Evaluations for the solution presented in the paper show that our system has 90.89\% precision and 91.69\% recall for an unseen paragraph from a legal document, thus conforming the success of our attempt.},
	booktitle = {The 23rd {International} {Conference} on {Information} {Integration} and {Web} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {de Almeida, Melonie and Samarawickrama, Chamodi and de Silva, Nisansa and Ratnayaka, Gathika and Perera, Shehan},
	year = {2022},
	note = {event-place: Linz, Austria},
	keywords = {NER, coreference resolution, Legal party identification, Recurrent Neural Networks},
	pages = {259--266},
}

@inproceedings{zhao_efficient_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Efficient {Computation} of {Signature}-{Restricted} {Views} for {Semantic} {Web} {Ontologies}},
	isbn = {979-8-4007-0171-9},
	url = {https://doi.org/10.1145/3589334.3645317},
	doi = {10.1145/3589334.3645317},
	abstract = {Uniform Interpolation (UI) is an advanced reasoning service used to narrow down an ontology to a restricted view. This new ontology, known as a uniform interpolant, will only consist of the ”relevant names”, yet it will retain their original meanings. UI is immensely promising due to its applicability across various domains where custom views of ontologies are essential. Nonetheless, to unlock its full potential, we need optimized techniques to generate these tailored views. Previous studies suggest that creating uniform interpolants for EL-ontologies is notably challenging. In some instances, it is not even feasible to compute a uniform interpolant; when feasible, the size of the uniform interpolant can be up to triple exponentially larger than the source ontology. Despite these challenges, our paper introduces an improved ”forgetting” technique specifically designed for computing uniform interpolants of ELI-ontologies. We demonstrate that, with good normalization and inference strategies, such uniform interpolants can be efficiently computed, just as quickly as computing ”modules”. A comprehensive evaluation with a prototypical implementation of the method shows superb success rates over two popular benchmark datasets, demonstrating a clear computational advantage over state-of-the-art approaches.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Yizheng},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {ontology, module extraction, forgetting, uniform interpolation},
	pages = {1945--1953},
}

@inproceedings{da_silva_towards_2018,
	address = {New York, NY, USA},
	series = {{SBES} '18},
	title = {Towards a domain-specific modeling language for self-adaptive systems conceptual modeling},
	isbn = {978-1-4503-6503-1},
	url = {https://doi.org/10.1145/3266237.3266244},
	doi = {10.1145/3266237.3266244},
	abstract = {Self-adaptive Systems (SaSs) are able to adapt their behavior at runtime in response to contextual changes. In this work, we are interested in SaSs conceptual modeling, which is the act of creating models that describe aspects of the world. SaSs modeling is a non-trivial activity because it deals with requirements uncertainty, contextual changes, and behavior adaptation. This complexity can be minimized by using Domain-Specific Modeling Languages (DSMLs), which may be created by extending Unified Modeling Language (UML). In this paper, we propose a UML profile that represents the higher-level abstractions required to provide support for SaSs conceptual modeling. We developed the UML profile by modeling the domain of interest and extending the UML class metaclass. The UML profile was evaluated through the focus group technique, which was performed by software engineering professors. As the outcome, the focus group participants considered the UML profile able to produce SaSs conceptual models with more expressiveness than UML standard.},
	booktitle = {Proceedings of the {XXXII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {da Silva, João Pablo S. and Ecar, Miguel and Pimenta, Marcelo S. and Guedes, Gilleanes T. A. and Rodrigues, Elder M.},
	year = {2018},
	note = {event-place: Sao Carlos, Brazil},
	keywords = {conceptual modeling, self-adaptive system, UML profiling},
	pages = {208--213},
}

@inproceedings{iacovou_ontology_2024,
	address = {New York, NY, USA},
	series = {{SETN} '24},
	title = {Ontology {Data} {Insights} and {Alerts} for {Wildfire} {Protection}},
	isbn = {979-8-4007-0982-1},
	url = {https://doi.org/10.1145/3688671.3688737},
	doi = {10.1145/3688671.3688737},
	abstract = {This paper presents an improved approach to wildfire protection by integrating heterogeneous data sources under a novel fire detection ontology. The introduced technique automatically unifies data from IoT devices into a single Semantic Knowledge Graph (SemKG), improving user experience by removing the need for users to examine several data sources. To make easier quick and well-informed decision-making, the system generates real-time notifications with different degrees of severity. The validation of the introduced technique has already taken place in various pilot sites of the SILVANUS H2020 project, which focus on developing a climate-resilient forest management platform to prevent and suppress wildfires, involving 49 partners from the European Union, Brazil, Indonesia, and Australia. In this paper, we demonstrate the effectiveness of this approach through scenarios of active fire detection and gas leakage.},
	booktitle = {Proceedings of the 13th {Hellenic} {Conference} on {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Iacovou, Marios and Kontogiannis, Stelios and Avgerinakis, Konstantinos},
	year = {2024},
	keywords = {Ontology, IoT, Fire Detection, Health Impact Monitoring, Semantic Knowledge Graph, SILVANUS, Wildfire Protection},
}

@inproceedings{aish_programming_2024,
	address = {New York, NY, USA},
	series = {Onward! '24},
	title = {Programming {Languages} for the {Future} of {Design} {Computation}},
	isbn = {979-8-4007-1215-9},
	url = {https://doi.org/10.1145/3689492.3689812},
	doi = {10.1145/3689492.3689812},
	abstract = {Design Computation is the use of programming in the design of physical systems such as buildings and infrastructure. This involves embedding both general-purpose textual languages and domain-specific visual languages within geometry modelling and engineering applications in the construction industry. A unique form of entry-level end-user programming has emerged in Design Computation. However, there are significant usability and representational issues; general-purpose languages present barriers to adoption, whilst visual languages do not scale to complex design problems. In this essay, we explore how advances in programming language research could be harnessed in future Design Computation languages to address these pedagogic, representational and scaling issues so as to improve human readable program structure and semantics and to enable machine-readable program verification.},
	booktitle = {Proceedings of the 2024 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {Association for Computing Machinery},
	author = {Aish, Robert and Fisher, Al and Orchard, Dominic and Torry, Jay},
	year = {2024},
	note = {event-place: Pasadena, CA, USA},
	keywords = {Usability, Cognitive Dimensions, Collaborative Coding, Collection Types, Design Computation, End-User Programming, Program Verification, Programming Languages, Type Systems, Units of Measure, Visual Languages},
	pages = {241--265},
}

@article{antonini_experiential_2023,
	title = {Experiential {Observations}: {An} {Ontology} {Pattern}-{Based} {Study} on {Capturing} the {Potential} {Content} within {Evidences} of {Experiences}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3586078},
	doi = {10.1145/3586078},
	abstract = {Modelling the knowledge behind human experiences is a complex process: it should take into account, among others, the activities performed, human observations and the documentation of the evidence. To represent this knowledge in a declarative way means to support data interoperability in the context of cultural heritage artefacts, as linked datasets on experience documentation have started to appear. With this objective in mind, we describe a study based on an ontology design pattern for modelling experiences through observations, which are considered indirect evidence of a mental process (i.e., the experience). This pattern highlights the structural differences between types of experiential documentation, such as diaries and social media, providing a guideline for the comparability between different domains and for supporting the construction of heterogeneous datasets based on an epistemic compatibility. We have performed not only a formal evaluation over the pattern but also an assessment through a series of case studies. This approach includes (a) the analysis of interoperability among two case studies (reading through social media and historical sources); (b) the development of an ontology for collecting evidences of reading, which reuses the proposed pattern; and (c) the inspection of experience in humanities datasets.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Antonini, Alessio and Adamou, Alessandro and Suárez-Figueroa, Mari Carmen and Benatti, Francesca},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {metadata, classification schema, digital humanities, human experience studies, ICT technologies in support of creating new cultural experiences or digital artefacts, intangible cultural heritage, knowledge patterns, ontologies and semantic processing for CH multimedia repositories},
}

@inproceedings{liao_detecting_2025,
	address = {New York, NY, USA},
	series = {{ICCSMT} '24},
	title = {Detecting {Public} {Perceptions} of {Apollo} {Go} on {Chinese} {Social} {Media} {Based} on {Topic} {Modeling} and {Sentiment} analysis},
	isbn = {979-8-4007-0999-9},
	url = {https://doi.org/10.1145/3708036.3708046},
	doi = {10.1145/3708036.3708046},
	abstract = {As one representative of autonomous vehicles (AVs), Apollo Go, Baidu's self-driving cab service, has been operated in 11 cities in China and aroused heated discussion on Chinese social media after its crash with one pedestrian in Wuhan in July, 2024. It's significant to explore the public's opinions and sentiment for a company to promote its products and expand its market. Chinese social media, Weibo, provides a vast amount of data to dig out the public attitudes and perspectives. In this paper, 11,882 Weibo posts from May 29th to August 15th 2024 were crawled for topic modelling and sentiment analysis by using LDA algorithm and sentiment dictionaries. It is found that the public mainly focus on five topics related with Apollo Go, including unemployment, accident and privacy, rumor response and market expansion, industry development and riding experience as well as price. While some people embrace Apollo Go which, they think, stands for the future means of transportation, some of them show their concern about the liability for the accident, riding safety, possible privacy invasion as well as the potential unemployment. At the end this study, several measures are put forward to tackle those negative sentiments to promote the popularity of AVs. Meanwhile, the limitation of this study were also proposed at the conclusion of this paper.},
	booktitle = {Proceedings of the 2024 5th {International} {Conference} on {Computer} {Science} and {Management} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Fei},
	year = {2025},
	keywords = {Autonomous vehicles, Sentiment analysis, Topic modelling, Apollo Go, Weibo},
	pages = {61--65},
}

@article{varshney_medprom_2025,
	title = {{MedProm}: {Bridging} {Dialogue} {Gaps} in {Healthcare} with {Knowledge}-{Enhanced} {Generative} {Models}},
	url = {https://doi.org/10.1145/3715069},
	doi = {10.1145/3715069},
	abstract = {In medical dialogue systems, recent advancements underscore the critical role of incorporating relevant medical knowledge to enhance performance. However, existing knowledge bases often lack completeness, posing a challenge in sourcing pertinent information. We present MedProm, a novel generative model tailored for medical dialogue generation to address this gap. Motivated by the need for comprehensive and contextually relevant responses, MedProm leverages state-of-the-art language models such as BioGPT. Our model is designed to integrate extensive medical knowledge into conversations, facilitating effective communication between patients and healthcare providers. At the core of MedProm lies the MediConnect Graph, a meticulously constructed knowledge graph capturing intricate relationships among medical entities extracted from dialogue contexts. By employing a KnowFusion encoder with a pretraining objective and masked multi-head self-attention, MedProm effectively processes the MediConnect graph, enabling precise control over information flow to capture its underlying structure. Furthermore, MedProm incorporates a sophisticated Curriculum Knowledge Decoder, leveraging transformer-based decoding to generate response utterances conditioned on input representations from the KnowFusion Encoder. The training process is guided through curriculum learning, gradually increasing optimization difficulty based on a coherence-based criterion. Experimental results on two datasets demonstrate the efficacy of MedProm in generating accurate and contextually relevant responses compared to state-of-the-art models.},
	journal = {ACM Trans. Comput. Healthcare},
	author = {Varshney, Deeksha and Behera, Niranshu and Katari, Prajeet and Ekbal, Asif},
	month = jan,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Curriculum Knowledge Decoder, Generative Neural Model, KnowFusion Encoder, Medical Dialogue Systems (MDS), MediConnect Graph},
	annote = {Just Accepted},
}

@inproceedings{huber_emotional_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Emotional {Dialogue} {Generation} using {Image}-{Grounded} {Language} {Models}},
	isbn = {978-1-4503-5620-6},
	url = {https://doi.org/10.1145/3173574.3173851},
	doi = {10.1145/3173574.3173851},
	abstract = {Computer-based conversational agents are becoming ubiquitous. However, for these systems to be engaging and valuable to the user, they must be able to express emotion, in addition to providing informative responses. Humans rely on much more than language during conversations; visual information is key to providing context. We present the first example of an image-grounded conversational agent using visual sentiment, facial expression and scene features. We show that key qualities of the generated dialogue can be manipulated by the features used for training the agent. We evaluate our model on a large and very challenging real-world dataset of conversations from social media (Twitter). The image-grounding leads to significantly more informative, emotional and specific responses, and the exact qualities can be tuned depending on the image features used. Furthermore, our model improves the objective quality of dialogue responses when evaluated on standard natural language metrics.},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Huber, Bernd and McDuff, Daniel and Brockett, Chris and Galley, Michel and Dolan, Bill},
	year = {2018},
	note = {event-place: Montreal QC, Canada},
	keywords = {emotion, dialogue, conversational agents, computer vision, conversation},
	pages = {1--12},
}

@inproceedings{samper-zapater_vas_2025,
	address = {New York, NY, USA},
	series = {{EATIS} 2024},
	title = {{VAS}. {A} {Semantic} {Model} for {Earth} {Observation} data},
	isbn = {979-8-4007-1733-8},
	url = {https://doi.org/10.1145/3685243.3685295},
	doi = {10.1145/3685243.3685295},
	abstract = {This paper discusses a segment of the "Artificial Intelligence and Semantics of Earth Observation (EO) data for the establishment of the Valencia Anchor Station as a supersite of the CEOS LPV Program (ASOTVAS)." The ASOTVAS project aims to apply AI and data semantics algorithms to the Valencia Anchor Station’s control zone (10 km x 10 km). A data semantics analysis explores concepts such as vocabularies, ontologies, and knowledge maps, with an emphasis on the reuse of existing models. Crucially, an ontological model with clear data definitions is developed to correlate Copernicus Program data with station parameters (e.g., FAPAR, LST, soil moisture, etc.). Using the ontology, the actual data are aggregated to create a knowledge map, which includes both an assertion model (concepts and properties) and an instance model (specific instances for ontological relationships). The main task is to develop an ontology-based representation scheme for terms of the application domain, which facilitates knowledge capture, distribution, and maintenance. The scheme enables computer processing for tasks related to information handling and supporting information exchange. Widely accepted ontology specification methodologies are employed. To avoid unnecessary efforts, emphasis is put on the reuse of existing ontologies or vocabularies, expanded, and adapted to the described case study. The optimal granularity is chosen for efficient ontology operation. The conceptual and technological requirements of the representation scheme are discussed. The methodology is based on software quality standards.},
	booktitle = {Proceedings of the 12th {Euro} {American} {Conference} on {Telematics} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Samper-Zapater, J. Javier and Martinez-Dura, Juan J. and Martinez-Plume, Javier and Rodríguez, David García and Moret, Julian Gutiérrez and Baeza, Ernesto López},
	year = {2025},
	note = {event-place: Praia, Cape Verde},
	keywords = {ontology, earth observation data, remote sensing, semantic.},
}

@inproceedings{li_robust_2019,
	address = {New York, NY, USA},
	series = {{ICMI} '19},
	title = {Robust {Spoken} {Language} {Understanding} with {Acoustic} and {Domain} {Knowledge}},
	isbn = {978-1-4503-6860-5},
	url = {https://doi.org/10.1145/3340555.3356100},
	doi = {10.1145/3340555.3356100},
	abstract = {Spoken language understanding (SLU) converts user utterances into structured semantic forms. There are still two main issues for SLU: robustness to ASR-errors and the data sparsity of new and extended domains. In this paper, we propose a robust SLU system by leveraging both acoustic and domain knowledge. We extract audio features by training ASR models on a large number of utterances without semantic annotations. For exploiting domain knowledge, we design lexicon features from the domain ontology and propose an error elimination algorithm to help predicted values recovered from ASR-errors. The results of CATSLU challenge show that our systems can outperform all of the other teams across four domains.},
	booktitle = {2019 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Li, Hao and Liu, Chen and Zhu, Su and Yu, Kai},
	year = {2019},
	note = {event-place: Suzhou, China},
	keywords = {Robustness, Spoken Language Understanding},
	pages = {531--535},
}

@article{clavaud_ric-o_2023,
	title = {{RiC}-{O} {Converter}: {A} {Software} to {Convert} {EAC}-{CPF} and {EAD} 2002 {XML} {Files} to {RDF} {Datasets} {Conforming} to {Records} in {Contexts} {Ontology}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3583592},
	doi = {10.1145/3583592},
	abstract = {RiC-O Converter is an open source command-line tool to convert EAD finding aids and EAC-CPF authority records to RDF files conforming to ICA Records in Contexts ontology (RiC-O) in a robust manner. It was developed for the Archives nationales of France but is aimed to be reused by other archival institutions, and to this aim is fully documented in English. It is based on XSLT stylesheets that consider the variability of EAD content. It enabled the Archives nationales of France to convert 15,400 EAC-CPF files and 31,000 EAD files into a homogeneous knowledge graph, and to start a more specific project aiming to provide end users with an intuitive search interface for a significant subset of this graph, opening new perspectives for navigating and linking from/to archival metadata.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Clavaud, Florence and Francart, Thomas and Charbonnier, Pauline},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {RDF, archival authority records, archival finding aids, open source software, Records in Contexts (RiC), RiC Ontology (RiC-O), XML EAC-CPF, XML EAD},
}

@inproceedings{chichkova_modeling_2020,
	address = {New York, NY, USA},
	series = {{ICEGOV} '20},
	title = {Modeling city land use with an ontology},
	isbn = {978-1-4503-7674-7},
	url = {https://doi.org/10.1145/3428502.3428638},
	doi = {10.1145/3428502.3428638},
	abstract = {Urban planning developers, city government and citizens need a digital city model for sustainable development and city planning. Such models are based on main regulatory documents that differ by the city. In the current project, we aimed to develop a Land Use Ontology (LUO) for St. Petersburg as well as to expand general approaches to create such ontologies. The project is at its initial stage, i.e. development of the Land Use and Development Rules model that is a crucial document for the city land use system. We propose the ontological model of the city land use. The model includes classes for city land sites description, their coordinates, and permissions for construction works. The model is implemented in Web Ontology Language that assures the five-star rating of open data. The model source code can be found at https://github.com/inxaoc/cityont-public.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Theory} and {Practice} of {Electronic} {Governance}},
	publisher = {Association for Computing Machinery},
	author = {Chichkova, Natalia and Begler, Alena and Vlasov, Vitaly},
	year = {2020},
	note = {event-place: Athens, Greece},
	keywords = {City digital model, city open data, land use ontology, smart city},
	pages = {851--854},
}

@inproceedings{chagas_hermes_2019,
	address = {New York, NY, USA},
	series = {{SBSI} '19},
	title = {Hermes: {A} {Natural} {Language} {Interface} {Model} for {Software} {Transformation}},
	isbn = {978-1-4503-7237-4},
	url = {https://doi.org/10.1145/3330204.3330253},
	doi = {10.1145/3330204.3330253},
	abstract = {Software maintenance is a costly task and error-prone for both software developers and users as well. By knowing how and what software requirements need to be changed, end users could perform maintenance assisted by tools. However, current literature lacks for tools that support automated maintenance in real-world scenarios and allow users interaction via natural language. Even worse, the current tools are unable to understand the semantic of requests, as well as perform the necessary transformations in the maintenance software. This paper, therefore, proposes Hermes, a natural language interface model for software transformation. It combines computational linguistics techniques and logic programming to perform automated maintenance requests in software. Hermes interacts with end user through state of the art language parsers and domain ontologies by interpreting the semantics of changes requests to build a typed graph that change the software. Hermes was evaluated through an empirical study with 8 participants to investigate its performance, the level of acceptance, and usability. The collected data show that Hermes was accurate, producing a high elevated correctness number of hits by finding correct transformations and has been highly accepted by the users. The results are encouraging and show the potential for using Hermes to properly produce software maintenance requests.},
	booktitle = {Proceedings of the {XV} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chagas, Michael William and Farias, Kleinner and Gonçales, Lucian and Kupssinskü, Lucas and Gluz, João Carlos},
	year = {2019},
	note = {event-place: Aracaju, Brazil},
	keywords = {Ontologies, Natural Language Processing, Parsing},
}

@inproceedings{wiens_gizmo_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {{GizMO} – {A} {Customizable} {Representation} {Model} for {Graph}-{Based} {Visualizations} of {Ontologies}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364431},
	doi = {10.1145/3360901.3364431},
	abstract = {Visualizations can support the development, exploration, communication, and sense-making of ontologies. Suitable visualizations, however, are highly dependent on individual use cases and targeted user groups. In this article, we present a methodology that enables customizable definitions for the visual representation of ontologies.The methodology describes visual representations using the OWL annotation mechanisms and separates the visual abstraction into two information layers. The first layer describes the graphical appearance of OWL constructs. The second layer addresses visual properties for conceptual elements from the ontology. Annotation ontologies and a modular architecture enable separation of concerns for individual information layers. Furthermore, the methodology ensures the separation between the ontology and its visualization.We showcase the applicability of the methodology by introducing GizMO, a representation model for graph-based visualizations in the form of node-link diagrams. The graph visualization meta ontology (GizMO) provides five annotation object types that address various aspects of the visualization (e.g., spatial positions, viewport zoom factor, and canvas background color). The practical use of the methodology and GizMO is shown using two applications that indicate the variety of achievable ontology visualizations.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Wiens, Vitalis and Lohmann, Steffen and Auer, Sören},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {visual notation, annotation ontology, customization, ontology visualization, visual representation, visualization framework},
	pages = {163--170},
}

@inproceedings{liang_kag_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{KAG}: {Boosting} {LLMs} in {Professional} {Domains} via {Knowledge} {Augmented} {Generation}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715240},
	doi = {10.1145/3701716.3715240},
	abstract = {The recently developed Retrieval-Augmented Generation (RAG) technology has enabled the efficient construction of domain-specific applications. The key technologies of RAG are retrieval based on similarity and reasoning based on next-token prediction. However, this approach differs significantly from how humans solve problems. Humans typically follow certain analytical logic, reasoning while retrieving relevant information, and then connecting the clues to serve as references, ultimately generating an answer. In this process, the focus is on the semantic type and clear relationships between the keywords rather than similarity and co-occurrence. This difference in methodology results in the answers generated by RAG technology being insufficiently accurate or valuable.In this work, we concentrate on establishing semantic relationships between keywords to enable a more precise expression of knowledge and propose the Knowledge Augmented Generation(KAG) framework. KAG performs semantic parsing and reasoning on both documents and questions, involving three specific strategies: In the indexing phase, we complete the semantic information of keywords and the semantic relationships between them through information extraction and semantic reasoning; in the reasoning phase of question answering, we leverage semantic parsing to transform questions into Logical Forms with clear semantic types and relationships; in the retrieval phase, we predict the semantic relationships between Logical Form elements and structured index, thereby obtaining the required references.We compared KAG with existing RAG methods in three multi-hop QA datasets and the results show that KAG significantly outperforms existing methods, achieving a new state-of-the-art. We also applied KAG to real E-Government Q\&amp;A business scenario, and achieving significant improvements in professionalism compared to traditional RAG methods. Meanwhile, to help developers easily build accurate and efficient domain knowledge QA services, our KAG natively supports the open-source KG engine OpenSPG.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Liang, Lei and Bo, Zhongpu and Gui, Zhengke and Zhu, Zhongshu and Zhong, Ling and Zhao, Peilong and Sun, Mengshu and Zhang, Zhiqiang and Zhou, Jun and Chen, Wenguang and Zhang, Wen and Chen, Huajun},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {information retrieval, knowledge graph, knowledge reasoning, kbqa, rag},
	pages = {334--343},
}

@inproceedings{roy_chaudhuri_methodology_2019,
	address = {New York, NY, USA},
	series = {{DSM} 2019},
	title = {Methodology to develop domain specific modeling languages},
	isbn = {978-1-4503-6984-8},
	url = {https://doi.org/10.1145/3358501.3361235},
	doi = {10.1145/3358501.3361235},
	abstract = {Domain Specific Modeling Languages (DSML) significantly improve productivity in designing Computer Based System (CBS), by enabling them to be modeled at higher levels of abstraction. It is common for large and complex systems with distributed teams, to use DSMLs, to express and communicate designs of such systems uniformly, using a common language. DSMLs enable domain experts, with no or minimal software development background, to model solutions, using the language and terminologies used in their respective domains. Although, there are already a number of DSMLs available for modeling CBSs, their need is felt strongly across multiple domains, which still are not well supported with DSMLs. Developing a new DSML, however, is non trivial, as it requires (a) significant knowledge about the domain for which the DSML needs to be developed, as well as (b) skills to create new languages. In the current practice, DSMLs are developed by experts, who have substantial understanding of the domain of interest and strong background in computer science. One of the many challenges in the development of DSMLs, is the collection of domain knowledge and its utilization, based on which the abstract syntax, the backbone of the DSML is defined. There is a clear gap in the current state of art and practice, with respect to overcoming this challenge. We propose a methodology, which makes it easier for people with different backgrounds such as domain experts, solution architects, to contribute towards defining the abstract syntax of the DSML. The methodology outlines a set of steps to systematically capture knowledge about the domain of interest, and use that to arrive at the abstract syntax of the DSML. The key contribution of our work is in abstracting a CBS from a domain into a Domain Specific Machine, embodied in domain specific concepts. The methodology outlines, how the Domain Specific Machine, when coupled with guidelines from current practices of developing DSMLs, results in the definition of the abstract syntax of the intended DSML. We discuss our methodology in detail, in this paper.},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Workshop} on {Domain}-{Specific} {Modeling}},
	publisher = {Association for Computing Machinery},
	author = {Roy Chaudhuri, Subhrojyoti and Natarajan, Swaminathan and Banerjee, Amar and Choppella, Venkatesh},
	year = {2019},
	note = {event-place: Athens, Greece},
	keywords = {modeling, domain specific language, domain specific modeling language, language engineering},
	pages = {1--10},
}

@inproceedings{yang_research_2025,
	address = {New York, NY, USA},
	series = {{ISCCN} '25},
	title = {Research on {Robotics} {Diffusion} {Transformer} in {Robot} {Foundation} {Models} {Based} on {Diffusion} {Model}},
	isbn = {979-8-4007-1520-4},
	url = {https://doi.org/10.1145/3732945.3733004},
	doi = {10.1145/3732945.3733004},
	abstract = {In recent years, diffusion models have shown great potential in the field of robotics. This paper focuses on the research of robot foundation models based on diffusion models, with a particular emphasis on the Robotics Diffusion Transformer (RDT). First, it reviews the technological frontier of robot basic action models, including visual language models and robot action learning. Then, it details the RDT model, covering its design for an ALOHA dual - arm robot, core methods such as diffusion model design, feature coding, regularization, non - linear action output, and Alternating Condition Injection (ACI). The RDT model is trained on a large number of datasets and fine - tuned on specific data. Experimental results demonstrate that RDT outperforms previous models in tasks such as generalization to unseen objects and scenes, few - shot learning, and dexterous control. Ablation experiments further verify the effectiveness of its key design elements. Overall, RDT proves the significance of generative models in embodied intelligence, showing that appropriate model design and data - driven training can achieve excellent results.},
	booktitle = {Proceedings of the 2025 4th {International} {Conference} on {Intelligent} {Systems}, {Communications} and {Computer} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Cuicui and Zhu, Min},
	year = {2025},
	keywords = {Diffusion model, Robot action learning, Robot foundation model, Robotics Diffusion Transformer (RDT), Visual language model},
	pages = {402--409},
}

@inproceedings{mendes_analyzing_2025,
	address = {New York, NY, USA},
	series = {{PCI} '24},
	title = {Analyzing the {Freedom} of {Information} {Requests} {Using} {Topic} {Modeling}: {Towards} {User}-driven {Open} {Government} {Data} {Ecosystems}},
	isbn = {979-8-4007-1317-0},
	url = {https://doi.org/10.1145/3716554.3716555},
	doi = {10.1145/3716554.3716555},
	abstract = {Open data has become an important player in promoting government transparency and accountability around the globe. Governments have established policies to provide citizens or users with access to data. One way to obtain such data is through Freedom of Information Act (FOIA) requests. Considering users' requests for data and responding with the required information makes open data ecosystems user-driven. This paper explores freedom of information (FOI) requests received by Germany's Ministry of Health as a user-driven approach to open government data (OGD). By applying advanced natural language processing (NLP) techniques, specifically Latent Dirichlet Allocation (LDA) and BERTopic, we analyze a large corpus of FOI requests to identify key topics and trends in public inquiries. The findings reveal valuable insights into citizens' information demands and demonstrate the strengths of these NLP methods in extracting actionable patterns. In this way, governments can easily understand user needs in health-related datasets (this study, in particular, focuses on requests made by citizens related to COVID-19 data). This study lays the groundwork in two ways: first, by understanding and thematically categorizing user needs related to open data, and second, by improving government data transparency, informing the prioritization of dataset releases, and supporting evidence-based policymaking.},
	booktitle = {Proceedings of the 28th {Pan}-{Hellenic} {Conference} on {Progress} in {Computing} and {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Mendes, Mara and Ali, Mohsan and Charalabidis, Yannis},
	year = {2025},
	keywords = {Open Data, Freedom of Information, Open Data Ecosystems, Open Government Data, Topic Modeling},
	pages = {1--10},
}

@inproceedings{saracco_human_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {Human {Robot} {Interaction} through an {Ontology}-based {Dialogue} {Engine}},
	isbn = {979-8-4007-0323-2},
	url = {https://doi.org/10.1145/3610978.3640642},
	doi = {10.1145/3610978.3640642},
	abstract = {This paper outlines the evolution of the Sugar, Salt, \&amp; Pepper project for high level functioning children affected by autism, focusing on the development of a dialogue system that relies on an ontology-based knowledge base. The ontology offers a formal representation of knowledge and interrelationships within the movie domain. The dialogue system addresses issues related to predefined answers, emphasizing adaptability for multi-platform use, particularly in the context of the social robot Pepper. The research covers detailed phases of construction and development, highlighting implementation choices and challenges faced.},
	booktitle = {Companion of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Saracco, Alessandro and Lillo, Alberto and Stranisci, Marco and Gena, Cristina},
	year = {2024},
	note = {event-place: Boulder, CO, USA},
	keywords = {machine learning, ontology, human-robot interaction, dialog},
	pages = {940--944},
}

@inproceedings{campos_inside_2023,
	address = {New York, NY, USA},
	series = {{SBSI} '23},
	title = {{INSIDE}: an {Ontology}-based {Data} {Integration} {System} {Applied} to the {Oil} and {Gas} {Sector}},
	isbn = {979-8-4007-0759-9},
	url = {https://doi.org/10.1145/3592813.3592893},
	doi = {10.1145/3592813.3592893},
	abstract = {Context: Data integration remains a major challenge facing organizations in the information age. Despite the advances made in recent decades, new approaches have become necessary to deal with new challenges such as Big Data. Problem: Semantic heterogeneity is a significant problem faced by companies in the oil and gas sector, as it makes it difficult to exchange information with other companies. Furthermore, there is a shortage of data integration systems that use open source technologies to deal with semantics, interoperability and scalability. Solution: INSIDE - Semantic Interoperability for Engineering Data Integration - an information system based on ontologies for data integration developed for an oil and gas company. SI Theory: This work is influenced by Representation Theory, based on the idea that an information system is a faithful representation of certain phenomena in the real world. Method: Review of state of the art on system architectures for data integration and use of methodologies for elaborating ontologies that represent the knowledge base of the information system. Summary of Results: Implementation of a prototype that allows querying heterogeneous data sources using a vocabulary familiar to the user, removing ambiguities from data with semantics. Contributions and Impact in IS area: The development of a solution for data integration using open source technologies tested with real-world data from a company in the oil and gas sector that can serve as a reference for developing new applied systems to other sectors.},
	booktitle = {Proceedings of the {XIX} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Campos, Júlio Gonçalves and De Almeida, Vitor Pinheiro and De Armas, Elvismary Molina and Da Silva, Geiza Maria Hamazaki and Corseuil, Eduardo Thadeu and Gonzalez, Fernando Rodrigues},
	year = {2023},
	note = {event-place: Maceió, Brazil},
	keywords = {Semantic Web, Data Integration, Information Systems, Ontology.},
	pages = {94--101},
}

@inproceedings{weber_towards_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Towards {Deep} {Reactions} in {Multi}-{Level}, {Multi}-{View} {Modeling}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688208},
	doi = {10.1145/3652620.3688208},
	abstract = {As the scale, complexity, and scope of software-intensive systems continue to grow, so does the importance of synergistically integrating two important emerging paradigms in software engineering - multi-level modeling and multi-view modeling. While stable tooling for both has been developed by research institutions in recent years, to date no tool has attempted to integrate the two at a fundamental level. In this paper, we describe some first steps we have taken in this direction by integrating the Vitruvius V-SUM-based multi-view environment with the Melanee multi-level modeling environment. In particular, we show how Vitruvius's Reactions language, which allows different models in Vitruvius V-SUMs to be kept consistent, can be extended to support multi-level V-SUMs and views represented in Melanee's dialect of multi-level modeling.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Weber, Thomas and Ojha, Monalisha and Sadeghi, Mohammad and König, Lars and Armbruster, Martin and Lange, Arne and Burger, Erik and Atkinson, Colin},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {consistency, multi-level modeling, V-SUM, view-based modeling, vitruvius},
	pages = {760--769},
}

@article{ahmad_deep_2025,
	title = {Deep {Neural} {Network}-{Based} {Feature} {Encoding} for {Automated} {Health} {Monitoring} {Using} {Large} {AI} {Models} in {Online} {Communication} {Systems}},
	url = {https://doi.org/10.1145/3744754},
	doi = {10.1145/3744754},
	abstract = {The hybrid model combines deep neural networks (DNN) and large AI models, such as large language models (LLM), for enhanced clinical information retrieval (CIR) from electronic clinical records (ECR). While LLMs show promise for encoding complex medical data, they face challenges in user-dependent information, such as patient reports with encoded knowledge, accessing real-time data, and requiring extensive fine-tuning for clinical decision-making in online communication systems. To overcome these limitations, we introduce a Transformer-based Sequence (TBS) multimodal method that integrates representation learning with human expertise to encode and analyze intricate relationships within clinical data. This model improves predictive tasks and medical search accuracy, achieving F1-scores of 0.83-0.80, and outperforms baseline methods. Integrating AI-driven methodologies in healthcare has the potential to transform medical record analysis and utilization, resulting in enhanced patient outcomes and more personalized healthcare solutions.},
	journal = {ACM Trans. Internet Things},
	author = {Ahmad, Pir Noman and Ullah, Inam and M. Salim, Nagwa and Kumar Singh, Sushil and Jiang, Weiwei and Al-Khasawneh, Mahmoud Ahmad and Daradkeh, Yousef Ibrahim},
	month = jun,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {healthcare, Deep neural network, communication systems, large AI models, transformer-based sequence},
	annote = {Just Accepted},
}

@inproceedings{wang_research_2025,
	address = {New York, NY, USA},
	series = {{ISBDAI} '24},
	title = {Research on {Intelligent} {Course} {Q}\&amp;{A} {Systems} {Based} on {NLP} {Models}, {Knowledge} {Graphs}, and {Deep} {Learning} {Methods}},
	isbn = {979-8-4007-1829-8},
	url = {https://doi.org/10.1145/3723366.3723370},
	doi = {10.1145/3723366.3723370},
	abstract = {With the increasing demand for personalized and interactive learning in online education environments. intelligent question-answering (Q\&amp;A) systems have emerged as powerful tools to enhance student engagement and support independent learning. However, traditional educational Q\&amp;A systems face significant limitations, such as reliance on simple keyword matching, inability to provide contextually rich answers, and challenges in handling complex, procedural queries. These limitations often result in incomplete or irrelevant responses, highlighting the need for more sophisticated solutions that can understand and interpret diverse student queries accurately. This research presents the design and implementation of an intelligent course Q\&amp;A system that integrates Natural Language Processing (NLP), semantic analysis, and knowledge graph traversal to deliver context-aware, real-time responses to student queries. The system is built upon advanced deep learning techniques, leveraging BERT for answer retrieval, a Bi-LSTM + CRF model for Named Entity Recognition (NER), and a dynamically constructed knowledge graph for multi-step reasoning. The system was evaluated on a dataset of student queries and course content, demonstrating strong performance with an overall accuracy of 88.1\%. The results highlight the system's effectiveness in handling factual, conceptual, and procedural questions, offering personalized and relevant answers to enhance the learning experience. However, challenges remain in addressing ambiguous queries, scaling the knowledge graph, and providing detailed procedural explanations. The paper concludes by discussing potential future improvements, such as the integration of interactive dialogue mechanisms, optimized knowledge graphs, and enhanced explanatory models. This intelligent Q\&amp;A system represents a step forward in educational technology, offering scalable and adaptable solutions for modern digital learning environments.},
	booktitle = {Proceedings of the 2024 4th {International} {Symposium} on {Big} {Data} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Yukang and Yang, Mengyu and Chen, Yu and Yu, Mingjie},
	year = {2025},
	keywords = {Deep Learning, Natural Language Processing, Knowledge Graph, Question-Answering System, Educational Technology},
	pages = {21--28},
}

@inproceedings{undefinedzcan_state_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {State of the {Art} and {Open} {Challenges} in {Natural} {Language} {Interfaces} to {Data}},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3383128},
	doi = {10.1145/3318464.3383128},
	abstract = {Recent advances in natural language understanding and processing resulted in renewed interest in natural language based interfaces to data, which provide an easy mechanism for non-technical users to access and query the data. While early systems only allowed simple selection queries over a single table, some recent work supports complex BI queries, with many joins and aggregation, and even nested queries. There are various approaches in the literature for interpreting user's natural language query. Rule-based systems try to identify the entities in the query, and understand the intended relationships between those entities. Recent years have seen the emergence and popularity of neural network based approaches which try to interpret the query holistically, by learning the patterns. In this tutorial, we will review these natural language interface solutions in terms of their interpretation approach, as well as the complexity of the queries they can generate. We will also discuss open research challenges.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {undefinedzcan, Fatma and Quamar, Abdul and Sen, Jaydeep and Lei, Chuan and Efthymiou, Vasilis},
	year = {2020},
	note = {event-place: Portland, OR, USA},
	keywords = {natural language interfaces, conversation systems, natural language query},
	pages = {2629--2636},
}

@inproceedings{twomey_towards_2020,
	address = {New York, NY, USA},
	series = {{RecSys} '20},
	title = {Towards {Multi}-{Language} {Recipe} {Personalisation} and {Recommendation}},
	isbn = {978-1-4503-7583-2},
	url = {https://doi.org/10.1145/3383313.3418478},
	doi = {10.1145/3383313.3418478},
	abstract = {Multi-language recipe personalisation and recommendation is an under-explored field of information retrieval in academic and production systems. The existing gaps in our current understanding are numerous, even on fundamental questions such as whether consistent and high-quality recipe recommendation can be delivered across languages. Motivated by this need, we consider the multi-language recipe recommendation setting and present grounding results that will help to establish the potential and absolute value of future work in this area. Our work draws on several billion events from millions of recipes, with published recipes and users incorporating several languages, including Arabic, English, Indonesian, Russian, and Spanish. We represent recipes using a combination of normalised ingredients, standardised skills and image embeddings obtained without human intervention. In modelling, we take a classical approach based on optimising an embedded bi-linear user-item metric space towards the interactions that most strongly elicit cooking intent. For users without interaction histories, a bespoke content-based cold-start model that predicts context and recipe affinity is introduced. We show that our approach to personalisation is stable and scales well to new languages. A robust cross-validation campaign is employed and consistently rejects baseline models and representations, strongly favouring those we propose. Our results are presented in a language-oriented (as opposed to model-oriented) fashion to emphasise the language-based goals of this work. We believe that this is the first large-scale work that evaluates the value and potential of multi-language recipe recommendation and personalisation.},
	booktitle = {Proceedings of the 14th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Twomey, Niall and Fain, Mikhail and Ponikar, Andrey and Sarraf, Nadine},
	year = {2020},
	note = {event-place: Virtual Event, Brazil},
	keywords = {information retrieval, recommendation, personalisation, recipes and food modelling},
	pages = {708--713},
}

@inproceedings{rathje_learning_2025,
	address = {San Jose, California, USA},
	series = {{AIES} '24},
	title = {Learning {When} {Not} to {Measure}: {Theorizing} {Ethical} {Alignment} in {LLMs}},
	abstract = {LLMs and other forms of generative AI have shown immense promise in producing highly accurate epistemic judgements in domains as varied as law, education, and medicine - with GPT notably passing the legal Bar exam and various medical licensing exams. The safe extension of LLMs into safety-critical professional domains requires assurance not only of epistemic but ethical alignment. This paper adopts a theoretical and philosophical approach, drawing from metaethical theories to argue for a distinction hinging around quantitative, axiological comparability that separates Kantian ethics from not only the utilitarianism it is well-known to oppose, but from just distribution theories as well, which are key to debiasing LLM models. It presents the novel hypothesis that LLM ethical acquisition from both corpus induction and RLHF may encounter value conflicts between Kantian and just distribution principles that intensify as they come into improved alignment with both theories, hinging around the variability by which self-attention may statistically attend to the same characterizations as more person-like or more resource-like under distinct prompting strategies.},
	booktitle = {Proceedings of the 2024 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {AAAI Press},
	author = {Rathje, William},
	year = {2025},
	pages = {1190--1199},
}

@article{park_emotion_2023,
	title = {Emotion {Ontology} {Studies}: {A} {Framework} for {Expressing} {Feelings} {Digitally} and its {Application} to {Sentiment} {Analysis}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3555719},
	doi = {10.1145/3555719},
	abstract = {Emotion ontologies have been developed to capture affect, a concept that encompasses discrete emotions and feelings, especially for research on sentiment analysis, which analyzes a customer's attitude towards a company or a product. However, there have been limited efforts to adapt and employ these ontologies. This research surveys and synthesizes emotion ontology studies to develop a Framework of Emotion Ontologies that can be used to help a user select or design an appropriate emotion ontology to support sentiment analysis and increase the user's understanding of the roles of affect, context, and behavioral information with respect to sentiment. The framework, which is derived from research on emotion ontologies, psychology, and sentiment analysis, classifies emotion ontologies as discrete emotion or one of two hybrid ontologies that are combinations of the discrete, dimensional, or componential process emotion paradigms. To illustrate its usefulness, the framework is applied to the development of an emotion ontology for a sentiment analysis application.},
	number = {9},
	journal = {ACM Comput. Surv.},
	author = {Park, Eun Hee and Storey, Veda C.},
	month = jan,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology, sentiment analysis, emotion, affect, componential process ontology, dimensional emotion ontology, discrete emotion ontology, Framework of Emotion Ontologies},
}

@article{chakraborty_editorial_2023,
	title = {Editorial: {Ontology}-based {Knowledge} {Presentation} and {Computational} {Linguistics} for {Semantic} {Big} {Social} {Data} {Analytics} in {Asian} {Social} {Networks}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3594719},
	doi = {10.1145/3594719},
	abstract = {Data-driven ontology-based knowledge (OK) presentation and computational linguistics for evolving semantic Asian social networks (ASNs) can make one of the most important platforms that provide robust and real-time data mapping in massive access across the heterogeneous big data sources in the web that is named OK-ASN. It benefits from computational intelligence, web-of-things (WoT) architecture, semantic features, statistical learning and pattern recognition, database management, computer vision, cyber-security, and language processing. OK-ASN is a critical strategy for WoT big data mining and enterprises from social media to medical and industrial sectors.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Chakraborty, Chinmay and Wan, Shaohua and Khosravi, Mohammad R.},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, knowledge graph, computational linguistics, Asian social networks, Social data},
}

@inproceedings{henarejos-blasco_cnl-rdf-query_2021,
	address = {New York, NY, USA},
	series = {{EATIS} '20},
	title = {{CNL}-{RDF}-query: a controlled natural language interface for querying ontologies and relational databases},
	isbn = {978-1-4503-7711-9},
	url = {https://doi.org/10.1145/3401895.3402064},
	doi = {10.1145/3401895.3402064},
	abstract = {The most commonly used search engines do not always show the information that the user needs, because they do not take into account factors such as the context or natural language ambiguity. Therefore, other types of search engines that considered these factors emerged in last few years, such as question-answering systems or semantic web-based search engines. In this work, we present CNL-RDF-Query, a controlled natural language interface for querying rdf-based ontologies and relational databases. The system guides users in the construction of queries with the knowledge of domain ontology. Our proposal has been tested in the domain of the IMDb movie repository.},
	booktitle = {Proceedings of the 10th {Euro}-{American} {Conference} on {Telematics} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Henarejos-Blasco, José and García-Díaz, José Antonio and Apolinario-Arzube, Óscar and Valencia-García, Rafael},
	year = {2021},
	note = {event-place: Aveiro, Portugal},
	keywords = {SPARQL, ontologies, user interfaces, natural language interface},
}

@inproceedings{arnob_utilizing_2025,
	address = {New York, NY, USA},
	series = {{ICCA} '24},
	title = {Utilizing {Kidney} {Ontology} for {Data}-{Driven} {Exploration} of {Potential} {Biomarkers} in {Kidney} {Diseases}: {Introducing} the {Kidney} {Diseases} {Biomarker} {Ontology} ({KDBO})},
	isbn = {979-8-4007-1382-8},
	url = {https://doi.org/10.1145/3723178.3723230},
	doi = {10.1145/3723178.3723230},
	abstract = {This study proposes a new method of identifying possible kidney disease biomarkers using Kidney Diseases Biomarker Ontology (KDBO). This study classically merges clinical imaging, biopsy data, proteomics, and genomic data derived from various sources through the Kidney Development Subontology (KDSO) of Gene Ontology (GO). Machine learning algorithms, network analysis, and statistical approaches are used to identify novel biomarkers that have implications for renal impairment at different stages of the disease have been employed here. A comparison between the new upstart markers found and the available right markers was made in patient cohorts to confirm their accuracy. The study showed that employing kidney ontology improves diagnosis, prognosis, and treatment by facilitating earlier identification of dysfunction, precise evaluation of disease severity levels, tailored choice therapy, addressing issues related to renal ontologies, and presenting suggestions for future studies.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Computing} {Advancements}},
	publisher = {Association for Computing Machinery},
	author = {Arnob, Arjun Kumar Bose and Naim, Mostaque Ahammed and Rezwan, Md Tahmid and Hasan, Mohammad Mahmudul},
	year = {2025},
	keywords = {Data Integration, Biomarkers, Chronic Kidney Disease (CKD), KDBO, Kidney Ontology, Renal Disease},
	pages = {391--398},
}

@inproceedings{cerqueira_ontology_2023,
	address = {New York, NY, USA},
	series = {{LADC} '22},
	title = {An {Ontology} for {Context}-aware {Middleware} for {Dependable} {Medical} {Systems}},
	isbn = {978-1-4503-9737-7},
	url = {https://doi.org/10.1145/3569902.3569947},
	doi = {10.1145/3569902.3569947},
	abstract = {In healthcare systems, there is an ecosystem of heterogeneous biosensors. A middleware is required for transmitting and establishing dependable communication with multiple integrations and information exchange through messages. However, in environments in which a distributed and dynamic network exists, a purely traditional middleware would not minimize the effect failures at data transmission and network congestion can cause. To preserve data integrity and provide relevant services as the environment changes, the use of context-aware middleware is recommended. This article describes an ontology for context-aware middleware to handle challenges faced by medical system networks and environment changes.},
	booktitle = {Proceedings of the 11th {Latin}-{American} {Symposium} on {Dependable} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Cerqueira, Jorsiele},
	year = {2023},
	note = {event-place: Fortaleza/CE, Brazil},
	keywords = {ontology, modelling, context aware middleware, dependable},
	pages = {79--83},
}

@inproceedings{sharpe_semprola_2018,
	address = {New York, NY, USA},
	series = {Programming '18},
	title = {Semprola: a semiotic programming language},
	isbn = {978-1-4503-5513-1},
	url = {https://doi.org/10.1145/3191697.3214330},
	doi = {10.1145/3191697.3214330},
	abstract = {Most people interested in developing new programming languages or programming environments are looking at how to improve the syntax and semantics of the program text or at tools that help make programmers more productive at crafting the program text. What we need is a more fundamental change to the conception of what a program is. This paper introduces a new, Semiotic Programming environment in which we program with signs in a context, rather than with symbols in a text file and where we treat dialogue rather than functions as the dominant organising principle of our code. All of the information held in this environment is managed in a distributed, semiotic graph that is organized into multiple ontological spaces. Taken together these enable our programs and data to have greater semantic depth. Finally the paper gives a brief introduction to Semprola, a Semiotic Programming Language that can be used in this Semiotic Programming environment.},
	booktitle = {Companion {Proceedings} of the 2nd {International} {Conference} on the {Art}, {Science}, and {Engineering} of {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Sharpe, Oli},
	year = {2018},
	note = {event-place: Nice, France},
	keywords = {semantics, semiotics, sign, dialogue, Compile time semantics, computational referent, context, distributed graph, messaging, multiple ontologies, nodedge, programming languages, referent, semantic depth, semiotic programming, semprola, signified, signifier, spuid},
	pages = {202--213},
}

@inproceedings{xie_webke_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {{WebKE}: {Knowledge} {Extraction} from {Semi}-structured {Web} with {Pre}-trained {Markup} {Language} {Model}},
	isbn = {978-1-4503-8446-9},
	url = {https://doi.org/10.1145/3459637.3482491},
	doi = {10.1145/3459637.3482491},
	abstract = {The World Wide Web contains rich up-to-date information for knowledge graph construction. However, most current relation extraction techniques are designed for free text and thus do not handle well semi-structured web content. In this paper, we propose a novel multi-phase machine reading framework, called WebKE. It processes the web content on different granularity by first detecting areas of interest at DOM tree node level and then extracting relational triples for each area. We also propose HTMLBERT as an encoder the web content. It is a pre-trained markup language model that fully leverages the visual layout information and DOM-tree structure, without the need of hand engineered features. Experimental results show that the proposed approach outperforms state-of- the-art methods by a considerable gain. The source code is available at https://github.com/redreamality/webke.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Xie, Chenhao and Huang, Wenhao and Liang, Jiaqing and Huang, Chengsong and Xiao, Yanghua},
	year = {2021},
	note = {event-place: Virtual Event, Queensland, Australia},
	keywords = {relation extraction, knowledge extraction, knowledge graph construction, htmlbert, pre-trained markup language model, semi-structured web extraction, webke},
	pages = {2211--2220},
}

@inproceedings{huang_streamlined_2019,
	address = {New York, NY, USA},
	series = {{ICMI} '19},
	title = {Streamlined {Decoder} for {Chinese} {Spoken} {Language} {Understanding}},
	isbn = {978-1-4503-6860-5},
	url = {https://doi.org/10.1145/3340555.3356097},
	doi = {10.1145/3340555.3356097},
	abstract = {As a critical component of Spoken Dialog System (SDS), spoken language understanding (SLU) attracts a lot of attention, especially for methods based on unaligned data. Recently, a new approach has been proposed that utilizes the hierarchical relationship between act-slot-value triples. However, it ignores the transfer of internal information which may record the intermediate information of the upper level and contribute to the prediction of the lower level. So, we propose a novel streamlined decoding structure with attention mechanism, which uses three successively connected RNN to decode act, slot and value respectively. On the first Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU), our model exceeds state-of-the-art model on an unaligned multi-turn task-oriented Chinese spoken dialogue dataset provided by the contest.},
	booktitle = {2019 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Heyan and Mao, Xianling and Yang, Puhai},
	year = {2019},
	note = {event-place: Suzhou, China},
	keywords = {attention mechanisms, long short term memory networks, pointer network, spoken dialog system, spoken language understanding, streamlined decoder},
	pages = {516--520},
}

@inproceedings{joy_ontology_2019,
	address = {New York, NY, USA},
	series = {{DATA} '19},
	title = {An ontology model for content recommendation in personalized learning environment},
	isbn = {978-1-4503-7284-8},
	url = {https://doi.org/10.1145/3368691.3368700},
	doi = {10.1145/3368691.3368700},
	abstract = {Personalized Learning Environments (PLEs) are expected to enhance the learning experience by providing tailor-made services based on learner preferences. It is of utmost importance to provide a personalized system which can automatically adapt to learners' learning styles, knowledge level and intelligently recommend resources that would favor and improve the learning. The existing PLEs still exhibit cold-start problems and other issues related with mapping of learning style and learning object. To solve these issues and to improve the dynamicity of the PLEs, an appropriate learner/learning object model is very essential. In this paper, we introduce an ontology model which encapsulates both learner profile and learning object attributes, which can be used for the content recommendation in an e-learning platform. Learner profile is the representation of learner data which includes both static and dynamic characteristics of the learner. The static data is gathered directly from the learner using forms and questionnaires and dynamic data is collected by tracking the behavior of learners, while interacting through a learning management system. The proposed ontology also holds space for learning topics and their corresponding Learning Object (LO) characteristics. The elements which come under the educational category of IEEE LOM standard, is considered for tagging the selected learning objects in the ontology. The JENA API of Java programming language is used for developing the ontology. The data is described using Resource Description Framework (RDF) tools. We have developed an ontology model consisting of an adaptive learner profile and standard LO characteristics which can be used in content recommender systems of e-learning environment.},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Data} {Science}, {E}-{Learning} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Joy, Jeevamol and Raj, Nisha S and G, Renumol V},
	year = {2019},
	note = {event-place: Dubai, United Arab Emirates},
	keywords = {ontology, content recommendation, learner profile, learning object, personalized learning environment},
}

@article{burgueno_automation_2025,
	title = {Automation in {Model}-{Driven} {Engineering}: {A} {Look} {Back}, and {Ahead}},
	volume = {34},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3712008},
	doi = {10.1145/3712008},
	abstract = {Model-Driven Engineering (MDE) provides a huge body of knowledge of automation for many different engineering tasks, especially those involving transitioning from design to implementation. With the huge progress made in AI, questions arise about the future of MDE, such as how existing MDE techniques and technologies can be improved or how other activities that currently lack dedicated support can also be automated. However, at the same time, it has to be revisited where and how models should be used to keep the engineers in the loop for creating, operating, and maintaining complex systems. To trigger dedicated research on these open points, we discuss the history of automation in MDE and present perspectives on how automation in MDE can be further improved and which obstacles have to be overcome in both the medium and long-term.},
	number = {5},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Burgueño, Lola and Di Ruscio, Davide and Sahraoui, Houari and Wimmer, Manuel},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {automation, Model-Driven Engineering (MDE)},
}

@inproceedings{xiaohui_construction_2020,
	address = {New York, NY, USA},
	series = {{CSAI} '19},
	title = {The {Construction} {Method} of {Geographic} {Knowledge} {Graph} {Ontology} {Model} {Based} on {GML}},
	isbn = {978-1-4503-7627-3},
	url = {https://doi.org/10.1145/3374587.3374603},
	doi = {10.1145/3374587.3374603},
	abstract = {Geographic ontology model is the conceptual model of geographic knowledge graph and the logical basis for constructing the pattern layer of geographic knowledge graph. In the classification of geographic ontology research, geographic ontology model is in the category of domain ontology. It is a set of abstract structures to express ontology according to the spatial location, attribute characteristics and relational characteristics of geographic data. This paper discussed the logical components and architecture of geographic ontology, designed the geographic ontology model reference to GML, described the model using OWL language, and constructed the geographic ontology model based on GML. The geographic ontology model comprises three sub-models: element model, geometric model and spatial relation model. Finally, based on Protégé ontology construction tool, this paper designed the semantic description of geographic entity and realized the construction of geographic ontology system.},
	booktitle = {Proceedings of the 2019 3rd {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Xiaohui, Chen and Yinzhen, Liu and Li, Xu and Lei, Ge and Yiwei, Ma},
	year = {2020},
	note = {event-place: Normal, IL, USA},
	keywords = {Geographic knowledge graph, geographic ontology, GML, logical composition, ontology model construction},
	pages = {138--143},
}

@inproceedings{noura_natural_2020,
	address = {New York, NY, USA},
	series = {{IoT} '20},
	title = {Natural language goal understanding for smart home environments},
	isbn = {978-1-4503-8758-3},
	url = {https://doi.org/10.1145/3410992.3410996},
	doi = {10.1145/3410992.3410996},
	abstract = {One of the main challenges of the Internet of Things (IoT) is to enable end-users without technical experience to use, control or monitor smart devices. However, enabling end-users to interact with these smart devices in an intuitive and natural way becomes increasingly important as they become more pervasive in our homes, workplaces and public environments. Voice-based interfaces are the emerging trend to provide a more natural human-device interaction in smart environments. Such interfaces require Natural Language Understanding (NLU) approaches to identify the meaning of end-users' voice inputs. Designing voice interfaces that are not limited to a small, fixed set of pre-defined commands is far from trivial. Existing voice-based solutions in the smart home domain either restrict the end-users to follow a strict language pattern, do not support indirect goals, require a large training dataset, or need a voice assistant located in the cloud. In this paper, we propose an approach for understanding end-users goals from voice inputs in smart homes. Our approach alleviates the need for end-users to learn or remember concrete operations of the devices and specific words/pattern structures rather it enables them to control their smart homes based on the desired goals (effects). We evaluate the approach through application to a collection of 253 goals from real end-users and report on quality metrics. The results demonstrate that our solution provides a good accuracy, high precision and acceptable recall for understanding end-users goals in the smart home domain.},
	booktitle = {Proceedings of the 10th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Noura, Mahda and Heil, Sebastian and Gaedke, Martin},
	year = {2020},
	note = {event-place: Malmö, Sweden},
	keywords = {internet of things, natural language understanding, goal recognition, smart home, voice interface},
}

@inproceedings{santos_junior_towards_2023,
	address = {New York, NY, USA},
	series = {{SBES} '23},
	title = {Towards {Federated} {Ontology}-{Driven} {Data} {Integration} in {Continuous} {Software} {Engineering}},
	isbn = {979-8-4007-0787-2},
	url = {https://doi.org/10.1145/3613372.3613380},
	doi = {10.1145/3613372.3613380},
	abstract = {Organizations have adopted Continuous Software Engineering (CSE) practices aiming at making software development faster, iterative, integrated, continuous, and aligned with the business. In this context, they often use different applications (e.g., project management tools, source repositories, and quality assessment tools) that store valuable data to support daily activities and decision-making. However, data items often remain spread in different applications that adopt different data and behavioral models, posing a barrier to integrated data usage. As a consequence, data-driven software development is uncommon, missing valuable opportunities for product and process improvement. In this paper, we explore an ontology network addressing CSE aspects to develop a data integration solution in which networked ontologies are the basis to build reusable and autonomous software components that work together in a system federation to provide meaningful integrated data. We achieve a comprehensive and flexible solution that can be used as a whole or partially, by extracting only the components related to the subdomains of interest.},
	booktitle = {Proceedings of the {XXXVII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Santos Júnior, Paulo Sérgio and Almeida, João Paulo A. and Barcellos, Monalessa},
	year = {2023},
	note = {event-place: Campo Grande, Brazil},
	keywords = {Ontology, Data Integration, Continuous Software Engineering},
	pages = {31--36},
}

@inproceedings{sampaio_de_alencar_integrating_2025,
	address = {New York, NY, USA},
	series = {{UMAP} '25},
	title = {Integrating {Expert} {Knowledge} {With} {Automated} {Knowledge} {Component} {Extraction} for {Student} {Modeling}},
	isbn = {979-8-4007-1313-2},
	url = {https://doi.org/10.1145/3699682.3728348},
	doi = {10.1145/3699682.3728348},
	abstract = {Knowledge tracing is a method to model students’ knowledge and enable personalized education in many STEM disciplines such as mathematics and physics, but has so far still been a challenging task in computing disciplines. One key obstacle to successful knowledge tracing in computing education lies in the accurate extraction of knowledge components (KCs), since multiple intertwined KCs are practiced at the same time for programming problems. In this paper, we address the limitations of current methods and explore a hybrid approach for KC extraction, which combines automated code parsing with an expert-built ontology. We use an introductory (CS1) Java benchmark dataset to compare its KC extraction performance with the traditional extraction methods using a state-of-the-art evaluation approach based on learning curves. Our preliminary results show considerable improvement over traditional methods of student modeling. The results indicate the opportunity to improve automated KC extraction in CS education by incorporating expert knowledge into the process.},
	booktitle = {Proceedings of the 33rd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Sampaio de Alencar, Rafaella and Demirtas, Mehmet Arif and Saha, Adittya Soukarjya and Shi, Yang and Brusilovsky, Peter},
	year = {2025},
	keywords = {knowledge components, computing education, intelligent tutoring systems, learning curves, student modeling},
	pages = {307--312},
}

@inproceedings{dimitropoulos_ontology-knowledge_2024,
	address = {New York, NY, USA},
	series = {{SETN} '24},
	title = {An {Ontology}-{Knowledge} {Graph} {Based} {Context} {Representation} {Scheme} for {Robotic} {Problems}},
	isbn = {979-8-4007-0982-1},
	url = {https://doi.org/10.1145/3688671.3688735},
	doi = {10.1145/3688671.3688735},
	abstract = {Context representation is a crucial part of a variety of robotic and other applications. Context refers to the environment, the (robotic or other) tasks as well as human-machine/environment interactions. One of the most utilized methods for representing context knowledge is ontologies. An ontology offers among others a highly descriptive structured representation and capabilities for inconsistencies checking and querying. Another well-known method is knowledge graphs, which offers a relation-based visualizable structure and querying capability, which are more convenient than those in ontologies. However, knowledge graphs cannot check inconsistencies. Therefore, we present an approach for context representation development, where an ontology is first created, checked and evaluated, and afterwards is converted into a knowledge graph. This approach assures design and implementation of more convenient and consistent context representations. The approach is applied to the creation of a robotics related ontology, where Protégé is used for ontology creation and logical consistency checking (via HermiT reasoner), OOPS! is used for ontology evaluation, and Neo4j is used for converting the ontology to a knowledge graph. Example queries in both representations show the preferability to knowledge graph, while an example shows the inability of knowledge graph to trace inconsistencies.},
	booktitle = {Proceedings of the 13th {Hellenic} {Conference} on {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Dimitropoulos, Konstantinos and Hatzilygeroudis, Ioannis},
	year = {2024},
	keywords = {knowledge graphs, ontologies, knowledge representation, robot context representation},
}

@inproceedings{rencis_knowledge_2020,
	address = {New York, NY, USA},
	series = {{ICISDM} '20},
	title = {Knowledge {Extraction} from {Healthcare} {Data} {Using} {User}-{Adaptable} {Keywords}-{Based} {Query} {Language}},
	isbn = {978-1-4503-7765-2},
	url = {https://doi.org/10.1145/3404663.3406876},
	doi = {10.1145/3404663.3406876},
	abstract = {Nowadays, the volume of the information gathered by any organization increases more and more rapidly. It is essential to be able to use this information efficiently for it to benefit the operation of the organization. There is no point of gathering the information if it is not converted into knowledge. The knowledge extraction process becomes the backbone of any successful organization. Moreover, the extraction of the knowledge must be quick and efficient, so that the newly-obtained knowledge can be put in use at once. The problem addressed in this paper is how to allow the domain expert to extract the knowledge from their information systems themselves without involving the third party in the form of an IT specialist. This goal is of utmost importance for the domain experts, e.g. hospital managers and physicians, because they need to make decisions based on the available knowledge and to do it rapidly and efficiently. We propose a system in this paper that allows formulating queries in the natural language and that also adapts to the specifics of the user. Our experiments show that such kind of querying could provide an improvement in the decision-making process of healthcare professionals.},
	booktitle = {Proceedings of the 2020 the 4th {International} {Conference} on {Information} {System} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Rencis, Edgars},
	year = {2020},
	note = {event-place: Hawaii, HI, USA},
	keywords = {Natural language processing, knowledge extraction, query language, hospital management, keywords-containing text, query translation},
	pages = {128--131},
}

@article{hiebel_ontological_2021,
	title = {Ontological {Modeling} for {Excavation} {Documentation} and {Virtual} {Reconstruction} of an {Ancient} {Egyptian} {Site}},
	volume = {14},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3439735},
	doi = {10.1145/3439735},
	abstract = {In this article we introduce our semantic modeling approach for data from over 50 years of excavations at Tell el-Daba in Egypt. The CIDOC CRM with some of its extensions is used as an ontological framework to provide the semantics for creating a knowledge graph containing material remains, excavated areas, and documentation resources. An objective of the project A Puzzle in 4D is to digitize the documentation and create metadata for analog and digital resources in order to provide the data to the research community and facilitate future work for this important archaeological site. Using an example of 3D reconstruction of a tomb, we show how the knowledge graph linked to digital resources can be exploited for a specific task to encounter available information that is essential for a virtual reconstruction. Moreover, we show an approach of modeling to represent the interpretations supporting reconstructions as well as relate them to the sources used, thus providing transparency for the model and provenance data. Modeling for excavation documentation as well as virtual reconstruction has been tailored to the large amount of data processed from the project. The goal is to propose a semantic modeling feasible even on a large scale while still preserving the basic underlying ontological structures.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Hiebel, Gerald and Aspöck, Edeltraud and Kopetzky, Karin},
	month = jul,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cultural heritage, archaeology, ARIADNE, CIDOC CRM, interpretation, virtual reconstruction},
}

@inproceedings{liu_stix-based_2020,
	address = {New York, NY, USA},
	series = {{ICGDA} '20},
	title = {{STIX}-based {Network} {Security} {Knowledge} {Graph} {Ontology} {Modeling} {Method}},
	isbn = {978-1-4503-7741-6},
	url = {https://doi.org/10.1145/3397056.3397083},
	doi = {10.1145/3397056.3397083},
	abstract = {Network security incidents are complex and unstructured, making them difficult to understand and share. In this paper, we analyzes the commonality between structured threat information representation (STIX) and network security domain knowledge, and proposes a knowledge graph ontology modeling method of network security based on STIX. With the architecture knowledge of STIX, this method generates an ontology schema of network security knowledge graph, through classifying the concepts in the field of network security, describing the attributes of concepts and combing the relationships between concepts. The ontology schema has small redundancy and strong structural hierarchy, and can clearly display the structure of the attack activity and the mutual relationship. Therefore, it can help decision makers to understand security incidents more deeply, and help them make reasonable decisions and share cyber threat intelligence.},
	booktitle = {Proceedings of the 2020 3rd {International} {Conference} on {Geoinformatics} and {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhengjun and Sun, Zhi and Chen, Jianfeng and Zhou, Yujin and Yang, Tao and Yang, Hui and Liu, Jie},
	year = {2020},
	note = {event-place: Marseille, France},
	keywords = {Ontology, Knowledge Graph, Network Security, STIX},
	pages = {152--157},
}

@inproceedings{noura_wotdl_2019,
	address = {New York, NY, USA},
	series = {{WI} '19},
	title = {{WoTDL}: {Web} of {Things} {Description} {Language} for {Automatic} {Composition}},
	isbn = {978-1-4503-6934-3},
	url = {https://doi.org/10.1145/3350546.3352558},
	doi = {10.1145/3350546.3352558},
	abstract = {Enabling end users to take a proactive role in the development of Web of Things (WoT) applications that achieves their goals is a challenge for End User Development (EUD) in the context of WoT. This can be achieved through Artificial Intelligence (AI) planning algorithms if the relevant WoT concepts and relationships are described in an interoperable way. Although similar, existing service description languages like WSDL or ontologies like OWL-S are not sufficient to represent all required characteristics of concrete WoT planning scenarios. To address these limitations, in this paper we present the Web of Things Description Language (WoTDL) ontology which is an extension of existing WoT models to describe the key concepts of AI planning for automatic WoT composition. To demonstrate the feasibility of our approach, we follow the best practices recommended by the semantic web community and describe the physical devices of our smart home testbed in an AI planning scenario using WoTDL.},
	booktitle = {{IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Noura, Mahda and Gaedke, Martin},
	year = {2019},
	note = {event-place: Thessaloniki, Greece},
	keywords = {Ontology, Semantic Web, Internet of Things, Web of Things, AI Planning, Automatic Composition},
	pages = {413--417},
}

@inproceedings{tsai_harmful_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Harmful {Terms} and {Where} to {Find} {Them}: {Measuring} and {Modeling} {Unfavorable} {Financial} {Terms} and {Conditions} in {Shopping} {Websites} at {Scale}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714573},
	doi = {10.1145/3696410.3714573},
	abstract = {Terms and conditions for online shopping websites often contain terms that can have significant financial consequences for customers. Despite their impact, there is currently no comprehensive understanding of the types and potential risks associated with unfavorable financial terms. Furthermore, there are no publicly available detection systems or datasets to systematically identify or mitigate these terms. In this paper, we take the first steps toward solving this problem with three key contributions.First, we introduce TermMiner, an automated data collection and topic modeling pipeline to understand the landscape of unfavorable financial terms. Second, we create ShopTC-100K, a dataset of terms and conditions from shopping websites in the Tranco top 100K list, comprising 1.8 million terms from 8,251 websites. Consequently, we develop a taxonomy of 22 types from 4 categories of unfavorable financial terms—spanning purchase, post-purchase, account termination, and legal aspects. Third, we build TermLens, an automated detector that uses Large Language Models (LLMs) to identify unfavorable financial terms.Fine-tuned on an annotated dataset, TermLens achieves an F1 score of 94.6\% and a false positive rate of 2.3\% using GPT-4o. When applied to shopping websites from the Tranco top 100K, we find that 42.06\% of these sites contain at least one unfavorable financial term, with such terms being more prevalent on less popular websites. Case studies further highlight the financial risks and customer dissatisfaction associated with unfavorable financial terms, as well as the limitations of existing ecosystem defenses.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Tsai, Elisa and Mangaokar, Neal and Zheng, Boyuan and Zheng, Haizhong and Prakash, Atul},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {topic modeling, consumer protection, deceptive content, terms and conditions dataset, unfavorable financial terms},
	pages = {990--1003},
}

@inproceedings{carta_towards_2024,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '24},
	title = {Towards {Zero}-shot {Knowledge} {Graph} building: {Automated} {Schema} {Inference}},
	isbn = {979-8-4007-0466-6},
	url = {https://doi.org/10.1145/3631700.3665234},
	doi = {10.1145/3631700.3665234},
	abstract = {In the current Digital Transformation scenario, Knowledge Graphs are essential for comprehending, representing, and exploiting complex information in a structured form. The main paradigm in automatically generating proper Knowledge Graphs relies on predefined schemas or ontologies. Such schemas are typically manually constructed, requiring an intensive human effort, and are often sensitive to information loss due to negligence, incomplete analysis, or human subjectivity or inclination. Limiting human bias and the resulting information loss in creating proper Knowledge Graphs is paramount, particularly for user modeling in various sectors, such as education or healthcare. To this end, we propose a novel approach to automatically generating a proper entity schema. The devised methodology combines the language understanding capabilities of LLM with classical machine learning methods such as clustering to properly build an entity schema from a set of documents. This solution eliminates the need for human intervention and fosters a more efficient and comprehensive knowledge representation. The assessment of our proposal concerns adopting a state-of-the-art entity extraction model (UniNER) to estimate the relevance of the extracted entities based on the generated schema. Results confirm the potential of our approach, as we observed a negligible difference between the topic similarity score obtained with the ground truth and with the automatically generated schema (less than 1\% on average on three different datasets). Such an outcome confirms that the proposed approach may be valuable in automatically creating an entity schema from a set of documents.},
	booktitle = {Adjunct {Proceedings} of the 32nd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Carta, Salvatore and Giuliani, Alessandro and Manca, Marco Manolo and Piano, Leonardo and Tiddia, Sandro Gabriele},
	year = {2024},
	note = {event-place: Cagliari, Italy},
	keywords = {Knowledge graphs, Large Language Models, Knowledge graph, Large language model, Ontology, Language model, Ontology learning, Named entity recognition, Semantics, User profile, Digital transformation, Ontology Learning, Named Entity Recognition, Zero-shot learning, Learning systems, 'current, Complex information, Schema inference, Information loss},
	pages = {467--473},
	annote = {Cited by: 1},
}

@inproceedings{cederbladh_how_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {How does one {Model} {Appropriately} in {Systems} {Engineering}? {An} {Initial} {Conceptual} {Model} {Framing} {Model} {Appropriateness}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688567},
	doi = {10.1145/3652620.3688567},
	abstract = {Appropriateness of models through modelling languages, tools, and methods is at the core of systems modelling. A concrete example is that useful abstraction depends on the contextual case of modelling purpose and need. Naturally, this results in different modelling formalisms and languages being appropriate at different stages of a common development process and for different stakeholders. In Model-Based Systems Engineering (MBSE) many stakeholders are involved in procedural modelling activities. Consequently, there is a need to identify appropriate modelling approaches at each modelling activity and development stage. Current MBSE adoption and application is still in early stages, and consequently lacking in overall modelling contextualisation. In this paper we discuss what facilitates an appropriate systems engineering model and how practitioners can reason about models in industrial contexts by providing an initial conceptual model of how model artefacts support governing systems engineering concerns.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cederbladh, Johan and Zimmermann, Thomas C.},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {systems engineering, decision-making, appropriate, model-based},
	pages = {930--934},
}

@inproceedings{nguyen_ontology-based_2024,
	address = {New York, NY, USA},
	series = {{ICIIT} '24},
	title = {Ontology-{Based} {Solution} for {Designing} knowledge {Retrieval} {Systems} in the {Field} of {Artificial} {Intelligence}},
	isbn = {979-8-4007-1671-3},
	url = {https://doi.org/10.1145/3654522.3654607},
	doi = {10.1145/3654522.3654607},
	abstract = {Currently, using domain knowledge and semantics in the field of artificial intelligence to conduct designing knowledge retrieval system has attracted great attention from researchers in many different communities. In this paper, we have improved the previous CK\_ONTO model to improve the semantic representation technique of documents more effectively. This improved model is used for designing the knowledge querying system based on the ontology-base in education. Its foundation includes a model of concept relations between concepts and rules of the knowledge domain. Besides, we have proposed techniques and algorithms on New\_Onto that are presented to solve the problems in this paper. Experimental results show that the knowledge query system works more effectively than the previous knowledge query system and is suitable for students who want to search for artificial intelligence documents for their learning and studying and is emerging for use in the real-world.},
	booktitle = {Proceedings of the 2024 9th {International} {Conference} on {Intelligent} {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Dien Thanh and Do, Nhon Van and Tran, Tung Hoang},
	year = {2024},
	note = {event-place: Ho Chi Minh City, Vietnam},
	keywords = {artificial intelligence, semantic search, Additional Key Words and Phrases: Ontology-base, document representation, graph matching},
	pages = {558--563},
}

@inproceedings{ramon-ferrer_automatic_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {Automatic {Topic} {Label} {Generation} using {Conversational} {Models}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627574},
	doi = {10.1145/3587259.3627574},
	abstract = {In probabilistic topic models, a topic is characterised by a set of words, with a probability associated to each of them. Even though it is not necessary to understand the meaning of topics to perform common downstream tasks where topic models are used, such as topic inference or document similarity, there have been attempts to uncover the semantics of topics by providing labels to them, consisting in a couple of concepts. In this paper we propose a methodology, Conversational Probabilistic Topic Labelling (CPTL), to study whether conversational models can be used to generate labels that describe probabilistic topics given their most representative keywords. We evaluate and compare the performance of a selection of conversational models for the topic label generation task with the performance of a task-specific language model trained to generate topic labels.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Ramón-Ferrer, Virginia and Badenes-Olmedo, Carlos and Corcho, Oscar},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {conversational model, probabilistic topic labelling, topic label, topic label generation},
	pages = {17--24},
}

@inproceedings{waseeb_toward_2023,
	address = {New York, NY, USA},
	series = {{EuroPLop} '22},
	title = {Toward {Organizational} {Pattern} {Ontology}},
	isbn = {978-1-4503-9594-6},
	url = {https://doi.org/10.1145/3551902.3551983},
	doi = {10.1145/3551902.3551983},
	abstract = {Organizational patterns of agile software development are proven practices for dealing organizational principles. Finding and selecting the right pattern is difficult. One way to select a pattern is to follow the sequence and compositions given in a pattern language. However, in general, pattern languages do not always reveal every reliable connection. Patterns are described in informal and unstructured text, making it difficult to understand their connections. This work attempts to provide a conceptual ontology model for organizational patterns – describing the collection of concepts (terms) and their relations. This contribution can be an attempt to express, expose, and share semantic knowledge between patterns using ontology. The resulting ontology can be used on top of the organizational patterns repositories to help retrieve patterns based on their logical connections.},
	booktitle = {Proceedings of the 27th {European} {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {Association for Computing Machinery},
	author = {Waseeb, Shakirullah and Vranić, Valentino},
	year = {2023},
	note = {event-place: Irsee, Germany},
	keywords = {ontology, semantics, knowledge base, organizational patterns},
}

@inproceedings{jorquera_valero_unlocking_2024,
	address = {New York, NY, USA},
	series = {{ARES} '24},
	title = {Unlocking the {Potential} of {Knowledge} {Graphs}: {A} {Cyber} {Defense} {Ontology} for a {Knowledge} {Representation} and {Reasoning} {System}},
	isbn = {979-8-4007-1718-5},
	url = {https://doi.org/10.1145/3664476.3670916},
	doi = {10.1145/3664476.3670916},
	abstract = {In today’s dynamic and complex warfare landscape, characterized by the convergence of traditional and emerging threats, the significance of cybersecurity in shaping modern conflicts cannot be overstated. Such trend presents a challenging paradigm shift in how military organizations approach mosaic warfare in the digital age since new attack vectors and targets appear in their landscapes. In this vein, it is pivotal for military teams to have a clear and concise roadmap for cybersecurity incidents linked to potential mosaic warfare. This manuscript introduces a novel approach to bolstering mosaic warfare strategies by integrating an advanced Knowledge Representation and Reasoning system and a tailored ontology. Motivated by the critical role of cybersecurity in contemporary warfare, the proposed system aims to enhance situational awareness, decision-making capabilities, and operational effectiveness in the face of evolving cyber threats. In this sense, this manuscript entails a new ontology that not only covers the cybersecurity realm but also introduces key concepts related to strategic and operational military levels at the same time. The ad-hoc ontology is also compared against other well-known ones, such as MITRE, NATO, or UCO approaches and manifests a significant performance by employing standardized quality metrics for ontologies. Lastly, a realistic mosaic warfare scenario is contextualized to demonstrate the deployment of the proposed system and how it can properly represent all information gathered from heterogeneous data sources.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Jorquera Valero, José María and López Martínez, Antonio and Sánchez Sánchez, Pedro Miguel and Navarro Martínez, Daniel and Varas López, Rodrigo and Rojo Lacal, Javier Ignacio and López Vivar, Antonio and Sotelo Monge, Marco Antonio and Gil Pérez, Manuel and Martínez Pérez, Gregorio},
	year = {2024},
	note = {event-place: Vienna, Austria},
	keywords = {Knowledge graph, Ontology, Reasoning, Knowledge representation, Cyber defense, Mosaic warfare},
}

@article{bikakis_editorial_2023,
	title = {Editorial: {Special} {Issue} on {Semantic} {Web} and {Ontology} {Design} for {Cultural} {Heritage}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3626254},
	doi = {10.1145/3626254},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Bikakis, Antonis and Ferrario, Roberta and Jean, Stéphane and Markhoff, Béatrice and Mosca, Alessandro and Asmundo, Marianna Nicolosi},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontologies, Knowledge Graphs, Cultural Heritage, Digitial Humanities},
}

@article{wei_ding_2023,
	title = {A {Ding} {Ontology} of {Chinese} {Bronze}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3609484},
	doi = {10.1145/3609484},
	abstract = {Ding is a significant type of Chinese bronze that holds key cultural value. Traditional humanists have primarily focused on dating and classifying Ding. However, in the context of Digital Humanities, the research perspective of humanities scholars is gradually shifting towards data-driven research, with linked data emerging as a popular topic. A well-defined and standard ontology representing the complete domain knowledge is essential for linked Ding data. Unfortunately, most existing ontology cannot represent fine-grained knowledge of Ding or is too restrictive to represent partial knowledge of bronze Ding. In this context, we propose a fine-grained Ding ontology to represent the bronze Ding knowledge. In this paper, we present in detail the Ding ontology of Chinese bronze during the Shang and Zhou dynasties (from 1600 BC to 256 BC). We provide a detailed exposition of the Ding ontology and evaluate its effectiveness using OOPS!, OntoMetrics, and by answering competency questions in SPARQL. The building methodology of Ding ontology follows the ISO principles (ISO 1087 and ISO 704). The objective of this paper is to develop an open ontology of Ding during the Shang and Zhou dynasties, which can serve as a valuable resource for bilingual terminology dictionaries. The Ding ontology was published at .},
	number = {4},
	journal = {J. Comput. Cult. Herit.},
	author = {Wei, Tong and Chen, Yuqi},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology, Terminology, Semantic Web, Cultural heritage, Digital humanities},
}

@article{sekulic_analysing_2024,
	title = {Analysing {Utterances} in {LLM}-{Based} {User} {Simulation} for {Conversational} {Search}},
	volume = {15},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3650041},
	doi = {10.1145/3650041},
	abstract = {Clarifying underlying user information needs by asking clarifying questions is an important feature of modern conversational search systems. However, evaluation of such systems through answering prompted clarifying questions requires significant human effort, which can be time-consuming and expensive. In our recent work, we proposed an approach to tackle these issues with a user simulator, USi. Given a description of an information need, USi is capable of automatically answering clarifying questions about the topic throughout the search session. However, while the answers generated by USi are both in line with the underlying information need and in natural language, a deeper understanding of such utterances is lacking. Thus, in this work, we explore utterance formulation of large language model (LLM)–based user simulators. To this end, we first analyze the differences between USi, based on GPT-2, and the next generation of generative LLMs, such as GPT-3. Then, to gain a deeper understanding of LLM-based utterance generation, we compare the generated answers to the recently proposed set of patterns of human-based query reformulations. Finally, we discuss potential applications as well as limitations of LLM-based user simulators and outline promising directions for future work on the topic.},
	number = {3},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Sekulić, Ivan and Alinannejadi, Mohammad and Crestani, Fabio},
	month = may,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {conversational search, mixed-initiative, User simulation},
}

@inproceedings{liu_graph--graph_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Graph-in-{Graph} {Network} for {Automatic} {Gene} {Ontology} {Description} {Generation}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539258},
	doi = {10.1145/3534678.3539258},
	abstract = {Gene Ontology (GO) is the primary gene function knowledge base that enables computational tasks in biomedicine. The basic element of GO is a term, which includes a set of genes with the same function. Existing research efforts of GO mainly focus on predicting gene term associations. Other tasks, such as generating descriptions of new terms, are rarely pursued. In this paper, we propose a novel task: GO term description generation. This task aims to automatically generate a sentence that describes the function of a GO term belonging to one of the three categories, i.e., molecular function, biological process, and cellular component. To address this task, we propose a Graph-in-Graph network that can efficiently leverage the structural information of GO. The proposed network introduces a two-layer graph: the first layer is a graph of GO terms where each node is also a graph (gene graph). Such a Graph-in-Graph network can derive the biological functions of GO terms and generate proper descriptions. To validate the effectiveness of the proposed network, we build three large-scale benchmark datasets. By incorporating the proposed Graph-in-Graph network, the performances of seven different sequence-to-sequence models can be substantially boosted across all evaluation metrics, with up to 34.7\%, 14.5\%, and 39.1\% relative improvements in BLEU, ROUGE-L, and METEOR, respectively.},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Fenglin and Yang, Bang and You, Chenyu and Wu, Xian and Ge, Shen and Woicik, Adelaide and Wang, Sheng},
	year = {2022},
	note = {event-place: Washington DC, USA},
	keywords = {bioinformatics, natural language generation, gene ontology, graph representations, sequence-to-sequence learning},
	pages = {1060--1068},
}

@inproceedings{marurngsith_applying_2019,
	address = {New York, NY, USA},
	series = {{ICSCA} '19},
	title = {Applying {Formal} {Logic} {Validation} to {Enhance} {Natural} {Language} {Understanding}},
	isbn = {978-1-4503-6573-4},
	url = {https://doi.org/10.1145/3316615.3316688},
	doi = {10.1145/3316615.3316688},
	abstract = {Inconsistencies and ambiguities of annotation can cause vagueness in the results obtained by natural language understanding (NLU). The quality of the type systems used for annotation affects the quality of annotation. To achieve highly accepted sets of annotated documents, the Fleiss' kappa score has been widely used to observe the level of agreement from annotated results, submitted by different human annotators. The challenge is that the kappa score cannot be used to validate the type systems nor to identify any incorrect annotations. Thus, we proposed an application of formal logic for validating type systems and annotations against expert rules. Experiments have been done by using four different type systems and annotation sets created by an expert and three novices. Our proposed formal logic model was used to validate the novice type systems and annotations against the expert rules. The results show that the technique could help identifying inconsistencies between expert and novice annotations, by using a model checker. The number of detected inconsistencies impacts the level of achieved F1 score. Thus, the proposed formal logic technique could be used to guide novice annotators to develop accepted type systems. This will help to enhance the performance of the generated machine learning models used by the NLU.},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Marurngsith, Worawan and Weawsawangwong, Pakorn},
	year = {2019},
	note = {event-place: Penang, Malaysia},
	keywords = {Validation, Natural language understanding, IBM Watson, Inconsistency detection},
	pages = {380--384},
}

@inproceedings{brutzman_x3d_2020,
	address = {New York, NY, USA},
	series = {{Web3D} '20},
	title = {{X3D} {Ontology} for {Querying} {3D} {Models} on the {Semantic} {Web}},
	isbn = {978-1-4503-8169-7},
	url = {https://doi.org/10.1145/3424616.3424715},
	doi = {10.1145/3424616.3424715},
	abstract = {The Semantic Web offers significant capabilities that transform the current Web into a global knowledge base including various cross-linked multimedia content with formal descriptions of its semantics understandable to humans and processable by computers. Content on the Semantic Web can be subject to reasoning and queries with standardized languages, methods and tools, which opens new opportunities for collaborative creation, use and exploration of web repositories. However, these opportunities have not been exploited so far by the available 3D formats and modeling tools, which limits the possibilities of search and reuse of 3D content as part of the Semantic Web. This work contributes a semantic development pipeline of the X3D Ontology, with corresponding conversion of X3D models into triple forms suitable for formal query. The ontology design reflects experience accompanying the development of the Extensible 3D (X3D) Graphics International Standard, in particular, the X3D Unified Object Model (X3DUOM). This approach combines semantic and syntactic elements of X3D models and metadata to support integration with the Semantic Web. The pipeline enables automatic generation of the X3D Ontology, thereby providing an up-to-date 3D representation with semantics during X3D specification development. By extending commonplace model conversions from other formats to X3D, the ontology presents the potential to enable integration of most forms of 3D content with the Semantic Web.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {3D} {Web} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Brutzman, Don and Flotyński, Jakub},
	year = {2020},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {Semantic Web, Semantic 3D, Web3D, X3D Ontology, X3DUOM},
}

@inproceedings{garces_towards_2021,
	address = {USA},
	series = {{PLoP} '19},
	title = {Towards an architectural patterns language for systems-of-systems},
	abstract = {Systems-of-Systems (SoS) architectures are inherently dynamic; hence, they must support continuous modification in the behaviour and configuration of these systems at runtime as a result of changes in the environment, new SoS missions, and failures or unavailability of constituents. Modifications should occur without affecting the integrity of constituents, the accomplishment of SoS missions, neither their reliability, security, safety, nor other quality attributes. Architecting SoS requires then important investments in human, time, and economic resources, bringing big challenges. Architectural patterns have been widely used to improve software architecture quality, decreasing costs of design and promoting reuse of good practices and well-known solutions. Nowadays, a great amount of architectural patterns is available; most of them are domain-independent, which harness their selection in specific software projects. The main goal of this paper is to contribute to reduce the work of architects during the choice of better architectural patterns for their SoS. For this, we present a set of patterns that are commonly used to construct such architectures. Additionally, benefits of adopting these patterns are described. To demonstrate their feasibility, we present HSH-SoS, a pattern-base architecture for SoS that presents how different patterns can interact to create a solution for SoS. As results, the architectural patterns reported in this work are feasible candidates to compose a language for recurrent problems in SoS architectures. However, formalization of such language is an open issue that we intend to address in future works.},
	booktitle = {Proceedings of the 26th {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {The Hillside Group},
	author = {Garcés, Lina and Sena, Bruno and Nakagawa, Elisa Y.},
	year = {2021},
	note = {event-place: Urbana, Illinois},
	keywords = {software architecture, architectural pattern, architectural patterns, patterns language, systems-of-systems},
}

@article{uhrmacher_context_2024,
	title = {Context, {Composition}, {Automation}, and {Communication}: {The} {C2AC} {Roadmap} for {Modeling} and {Simulation}},
	volume = {34},
	issn = {1049-3301},
	url = {https://doi.org/10.1145/3673226},
	doi = {10.1145/3673226},
	abstract = {Simulation has become, in many application areas, a sine qua non. Most recently, COVID-19 has underlined the importance of simulation studies and limitations in current practices and methods. We identify four goals of methodological work for addressing these limitations. The first is to provide better support for capturing, representing, and evaluating the context of simulation studies, including research questions, assumptions, requirements, and activities contributing to a simulation study. In addition, the composition of simulation models and other simulation studies’ products must be supported beyond syntactical coherence, including aspects of semantics and purpose, enabling their effective reuse. A higher degree of automating simulation studies will contribute to more systematic, standardized simulation studies and their efficiency. Finally, it is essential to invest increased effort into effectively communicating results and the processes involved in simulation studies to enable their use in research and decision making. These goals are not pursued independently of each other, but they will benefit from and sometimes even rely on advances in other sub-fields. In this article, we explore the basis and interdependencies evident in current research and practice and delineate future research directions based on these considerations.},
	number = {4},
	journal = {ACM Trans. Model. Comput. Simul.},
	author = {Uhrmacher, Adelinde M and Frazier, Peter and Hähnle, Reiner and Klügl, Franziska and Lorig, Fabian and Ludäscher, Bertram and Nenzi, Laura and Ruiz-Martin, Cristina and Rumpe, Bernhard and Szabo, Claudia and Wainer, Gabriel and Wilsdorf, Pia},
	month = aug,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Modeling, automation, simulation, state of the art, communication, reproducibility, composition, intelligent modeling and simulation lifecycle, open challenges, reuse},
}

@inproceedings{rencis_natural_2019,
	address = {New York, NY, USA},
	series = {{ICISDM} '19},
	title = {Natural {Language}-{Based} {Knowledge} {Extraction} in {Healthcare} {Domain}},
	isbn = {978-1-4503-6635-9},
	url = {https://doi.org/10.1145/3325917.3325948},
	doi = {10.1145/3325917.3325948},
	abstract = {There is a growing amount of data in the databases of hospitals. These data could be exploited to alleviate the decision-making process of hospital managers, physicians and researchers. However, these types of end-users often lack the expertise necessary for extracting those data from the database. Several approaches exist in the field of how to allow non-programmers writing queries in a convenient manner, but none of them has yet reached fully satisfactory results. This paper sketches a solution to this problem by introducing means for writing queries in a keywords-containing natural language thus alleviating the query writing process for the end-user. Introducing this approach in the knowledge management system of the organization would greatly benefit the domain experts by allowing them to carry out the decision-making process in a more rapid and less erroneous manner.},
	booktitle = {Proceedings of the 2019 3rd {International} {Conference} on {Information} {System} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Rencis, Edgars},
	year = {2019},
	note = {event-place: Houston, TX, USA},
	keywords = {Natural language processing, knowledge extraction, query language, hospital management, keywords-containing text, query translation},
	pages = {138--142},
}

@inproceedings{barcham_decolonizing_2024,
	address = {New York, NY, USA},
	series = {{OzCHI} '23},
	title = {Decolonizing {Computing} and the {Quest} for {Ontological} {Justice} - {Putting} {Fourth}-{Wave} {HCI}/{IxD} {Into} {Practice}},
	isbn = {979-8-4007-1707-9},
	url = {https://doi.org/10.1145/3638380.3638430},
	doi = {10.1145/3638380.3638430},
	abstract = {This paper takes as its starting point the emergent literature on issues of justice and decolonization in the HCI/IxD field and their focus on the ways in which current computing systems unwittingly (but perhaps knowingly) perpetuate existing forms of systemic violence by ignoring oppressive histories and sustained negative impacts against certain groups of people. Linking this to the ontological turn underway in a range of disciplines the paper then looks at how these ideas open up the possibility for the achievement of ontological justice for groups marginalized by the colonial nature of computing. The paper then explores these ideas through a discussion of the experiences of New Zealand Mundefinedori hapū (clans) building out computing infrastructures as part of their resurgence as groups. The paper ends by discussing the ways in which a distinction between upstream and downstream design can provide greater purchase of how we might be able to bring out the shifts that are required to achieve a space of ontological justice through a shift into Fourth-Wave HCI/IxD.},
	booktitle = {Proceedings of the 35th {Australian} {Computer}-{Human} {Interaction} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Barcham, Manuhuia},
	year = {2024},
	note = {event-place: Wellington, New Zealand},
	keywords = {Indigenous, Decolonization, Fourth-Wave HCI, Ontological Justice},
	pages = {486--492},
}

@inproceedings{koutsomitropoulos_validating_2021,
	address = {New York, NY, USA},
	series = {{CSBio2021}},
	title = {Validating {Ontology}-based {Annotations} of {Biomedical} {Resources} using {Zero}-shot {Learning}},
	isbn = {978-1-4503-8510-7},
	url = {https://doi.org/10.1145/3486713.3486730},
	doi = {10.1145/3486713.3486730},
	abstract = {Authoritative thesauri in the form of web ontologies offer a sound representation of domain knowledge and can act as a reference point for automated semantic tagging. On the other hand, current language models achieve to capture contextualized semantics of text corpora and can be leveraged towards this goal. We present an approach for injecting subject annotations using query term expansion against such ontologies in the biomedical domain. For the user to have an indication of the usefulness of these suggestions we further propose an online method for validating the quality of annotations using NLI models such as BART and XLM-R. To circumvent training barriers posed by very large label sets and scarcity of data we rely on zero-shot classification and show that semantic matching can contribute above-average thematic annotations. Also, a web-based validation service can be attractive for human curators vs. the overhead of pretraining large, domain-tailored classification models.},
	booktitle = {The 12th {International} {Conference} on {Computational} {Systems}-{Biology} and {Bioinformatics}},
	publisher = {Association for Computing Machinery},
	author = {Koutsomitropoulos, Dimitrios},
	year = {2021},
	note = {event-place: Virtual (GMT+7 Bangkok Time), Thailand},
	keywords = {machine learning, classification, language models, Thesaurus, semantic matching, MeSH, biomedical indexing},
	pages = {37--43},
}

@article{lambrix_completing_2023,
	title = {Completing and {Debugging} {Ontologies}: {State}-of-the-art and {Challenges} in {Repairing} {Ontologies}},
	volume = {15},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3597304},
	doi = {10.1145/3597304},
	abstract = {As semantically enabled applications require high-quality ontologies, developing and maintaining ontologies that are as correct and complete as possible is an important although difficult task in ontology engineering. A key task is ontology debugging and completion. In general, there are two steps: detecting defects and repairing defects. In this article, we discuss the state-of-the-art regarding the repairing step. We do this by formalizing the repairing step as an abductive reasoning problem and situating the state-of-the-art with respect to this framework. We show that there are still many open research problems and show opportunities for further work and advancing the field.},
	number = {4},
	journal = {J. Data and Information Quality},
	author = {Lambrix, Patrick},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology engineering, ontology alignment, ontology completion, ontology debugging},
}

@inproceedings{aitim_construction_2020,
	address = {New York, NY, USA},
	series = {{ICEMIS}'20},
	title = {The construction of the {Kazakh} language thesauri in automatic word processing system},
	isbn = {978-1-4503-7736-2},
	url = {https://doi.org/10.1145/3410352.3410789},
	doi = {10.1145/3410352.3410789},
	abstract = {In the paper presents an overview of existing electronic Kazakh-language thesauri and their automatic methods of construction and application. The author analyzed the main characteristics of open access thesauri for scientific research, evaluated the dynamics of their development and effectiveness in solving problems of natural language processing. Statistical and linguistic methods of thesaurus construction were studied, which allow to automate the development and reduce the labor costs of expert linguists. It is considered algorithms for selecting key terms from texts and semantic thesaurus links of all types, as well as the quality of application of the resulting thesauri. For illustrate the features of various methods of building thesaurus links, a combined method was developed that generates a specialized thesaurus completely automatically based on the corpus of domain texts and several existing linguistic resources.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Engineering} \&amp; {MIS} 2020},
	publisher = {Association for Computing Machinery},
	author = {Aitim, A. K. and Satybaldiyeva, R. Zh. and Wojcik, W.},
	year = {2020},
	note = {event-place: Almaty, Kazakhstan},
	keywords = {semantics, Thesaurus, agglutinative languages, Kazakh language, morphological thesauri},
}

@inproceedings{faria_system_2020,
	address = {New York, NY, USA},
	series = {{IHC} '20},
	title = {System for identifying pests and diseases in soybean crop through natural language},
	isbn = {978-1-4503-8172-7},
	url = {https://doi.org/10.1145/3424953.3426540},
	doi = {10.1145/3424953.3426540},
	abstract = {The presence of technologies in the agronomic field has the purpose of proposing the best solutions to the challenges found in agriculture, especially to the problems that affect cultivars. One of the obstacles found is to apply the use of your own language in applications that interact with the user in Brazilian Agribusiness, which would bring gains in time, money and accuracy of the analyzes to be performed. This paper proposes the use of Natural Language Processing techniques for the development of an effective system to assist in the identification of pests and diseases in the soybean crop, stored in a database repository to facilitate access to information and diagnosis of the professional.},
	booktitle = {Proceedings of the 19th {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Faria, Carolinne Roque e and de Barbosa, Cinthyan Renata Sachs C.},
	year = {2020},
	note = {event-place: Diamantina, Brazil},
	keywords = {natural language processing, Agriculture 4.0, human-computer interaction},
}

@inproceedings{yanuarifiani_automating_2019,
	address = {New York, NY, USA},
	series = {{ICSCA} '19},
	title = {Automating {Business} {Process} {Model} {Generation} from {Ontology}-based {Requirements}},
	isbn = {978-1-4503-6573-4},
	url = {https://doi.org/10.1145/3316615.3316683},
	doi = {10.1145/3316615.3316683},
	abstract = {Requirements elicitation process faces major challenges about how stakeholders can easily verify requirements. Requirements document allows developers to visualize requirements using modeling language to ensure stakeholders have the same perspective as them. It is also effective to give presentations to stakeholders about how business processes will be carried out after the requirements are implemented. Issues are raised in building requirements modeling as business users generally do not have enough knowledge to build requirements models in specific notations. Transforming requirements (natural language) into semi-formal notation (BPMN) manually lead to inconsistency of elements structure. The need to automatically generate requirements model become crucial because it will be the basis for the programming process. Existing studies are mostly concerned on auto-completion of modeling language using domain ontology as basic knowledge, and let the stakeholders building initial requirements model with limited knowledge. The idea of this paper is to propose a methodology for building business process model in semi-formal language (BPMN) to represent future business processes using ontology approach. This research continues from previous study which transform requirements list into requirements ontology to formalize the elements such as problem, actor and process. By using requirements ontology as input, rule-based mapping method is proposed to map ontology instances to BPMN elements.},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Yanuarifiani, Amarilis Putri and Chua, Fang-Fang and Chan, Gaik-Yee},
	year = {2019},
	note = {event-place: Penang, Malaysia},
	keywords = {auto-generate BPMN, ontology-based requirements, Semi-formal modeling},
	pages = {205--209},
}

@article{mahlaza_surface_2023,
	title = {Surface {Realization} {Architecture} for {Low}-resourced {African} {Languages}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3567594},
	doi = {10.1145/3567594},
	abstract = {There has been growing interest in building surface realization systems to support the automatic generation of text in African languages. Such tools focus on converting abstract representations of meaning to a text. Since African languages are low-resourced, economical use of resources and general maintainability are key considerations. However, there is no existing surface realizer architecture that possesses most of the maintainability characteristics (e.g., modularity, reusability, and analyzability) that will lead to maintainable software that can be used for the languages. Moreover, there is no consensus surface realization architecture created for other languages that can be adapted for the languages in question. In this work, we solve this by creating a novel surface realizer architecture suitable for low-resourced African languages that abides by the features of maintainable software. Its design comes after a granular analysis, classification, and comparison of the architectures used by 77 existing NLG systems. We compare our architecture to existing architectures and show that it supports the most features of a maintainable software product.},
	number = {3},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Mahlaza, Zola and Keet, C. Maria},
	month = mar,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {software architecture, Natural language generation, low-resourced languages, surface realisation},
}

@inproceedings{silva-aguilar_design_2023,
	address = {New York, NY, USA},
	series = {{ICEGOV} '23},
	title = {Design of an ontology to represent the elaboration of the annual operational plan in the area of public sector planning},
	isbn = {979-8-4007-0742-1},
	url = {https://doi.org/10.1145/3614321.3614367},
	doi = {10.1145/3614321.3614367},
	abstract = {The Semantic Web helps to represent knowledge in such a way that it can be easily processed by machines. Ontologies are a tool the Semantic Web uses to define concepts, establish their hierarchy, outline attributes, and determine relationships, which capture knowledge within a specific domain. In the public sector, institutions generate a substantial volume of information. To foster transparency and openness, such information is published on the web. However, in many cases, it is necessary to incorporate semantic content that helps to organize the information so that it can be processed automatically, by government authorities and interested users. One of the fundamental activities in public institutions is planning, which includes a set of processes that contribute to the achievement of previously defined objectives. In this article, we design an ontology whose domain is the elaboration of the Annual Operational Plan in Ecuador. An operational plan is a process that belongs to the Planning area of the Decentralized Autonomous Governments of Ecuador. We propose a set of activities and their respective tasks for developing the ontology based on the Methontology methodology. After its development, we formalized the proposed ontology using Protégé. The main theoretical contribution of this paper is the definition of an ontology in a specific government area. In addition, from a practical perspective, we developed a tool that facilitates the analysis and processing of data for budget planning in Ecuador.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Theory} and {Practice} of {Electronic} {Governance}},
	publisher = {Association for Computing Machinery},
	author = {Silva-Aguilar, Jairo H. and Torres T., Rommel and Estevez, Elsa},
	year = {2023},
	note = {event-place: Belo Horizonte, Brazil},
	keywords = {Ontology, Semantic Web, Protégé, Planning, Government budget, Open Government},
	pages = {340--346},
}

@article{baader_relating_2023,
	title = {Relating {Optimal} {Repairs} in {Ontology} {Engineering} with {Contraction} {Operations} in {Belief} {Change}},
	volume = {23},
	issn = {1559-6915},
	url = {https://doi.org/10.1145/3626307.3626308},
	doi = {10.1145/3626307.3626308},
	abstract = {The question of how a given knowledge base can be modified such that certain unwanted consequences are removed has been investigated in the area of ontology engineering under the name of repair and in the area of belief change under the name of contraction. Whereas in the former area the emphasis was more on designing and implementing concrete repair algorithms, the latter area concentrated on characterizing classes of contraction operations by certain postulates they satisfy. In the classical setting, repairs and contractions are subsets of the knowledge base that no longer have the unwanted consequence. This makes these approaches syntax-dependent and may result in removal of more consequences than necessary. To alleviate this problem, gentle repairs and pseudo-constractions have been introduced in the respective research areas, and their connections have been investigated in recent work. Optimal repairs preserve a maximal amount of consequences, but they may not always exist. We show that, if they exist, then they can be obtained by certain pseudo-contraction operations, and thus they comply with the postulates that these operations satisfy. Conversely, under certain conditions, pseudo-contractions are guaranteed to produce optimal repairs. Recently, contraction operations have also been defined for concepts rather than for whole knowledge bases. We show that there is again a close connection between such operations and optimal repairs of a restricted form of knowledge bases.},
	number = {3},
	journal = {SIGAPP Appl. Comput. Rev.},
	author = {Baader, Franz},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {description logic, belief change, ontology repair},
	pages = {5--18},
}

@inproceedings{zhu_catslu_2019,
	address = {New York, NY, USA},
	series = {{ICMI} '19},
	title = {{CATSLU}: {The} 1st {Chinese} {Audio}-{Textual} {Spoken} {Language} {Understanding} {Challenge}},
	isbn = {978-1-4503-6860-5},
	url = {https://doi.org/10.1145/3340555.3356098},
	doi = {10.1145/3340555.3356098},
	abstract = {Spoken language understanding (SLU) is a key component of conversational dialogue systems, which converts user utterances into semantic representations. The previous works almost focus on parsing semantic from textual inputs (top hypothesis of speech recognition and even manual transcripts) while losing information hidden in the audio. We herein describe the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU) which introduces a new dataset with audio-textual information, multiple domains and domain knowledge. We introduce two scenarios of audio-textual SLU in which participants are encouraged to utilize data of other domains or not. In this paper, we will describe the challenge and results.},
	booktitle = {2019 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Su and Zhao, Zijian and Zhao, Tiejun and Zong, Chengqing and Yu, Kai},
	year = {2019},
	note = {event-place: Suzhou, China},
	keywords = {datasets, spoken language understanding},
	pages = {521--525},
}

@inproceedings{avgerinos_loutsaris_semantic_2023,
	address = {New York, NY, USA},
	series = {{ICEGOV} '23},
	title = {Semantic {Interoperability} for {Legal} {Information}: {Mapping} the {European} {Legislation} {Identifier} ({ELI}) and {Akoma} {Ntoso} ({AKN}) {Ontologies}},
	isbn = {979-8-4007-0742-1},
	url = {https://doi.org/10.1145/3614321.3614327},
	doi = {10.1145/3614321.3614327},
	abstract = {The legislative landscape, characterized by overwhelming amounts of legal data which, on many occasions, is only accessible by legal experts, the fragmented nature of information and the ever-increasing number of disparate systems, have given more impetus to the interoperability realm of legal data. The semantic interoperability of Linked Open Legal Data (LOLD) requires rich and well-defined metadata, as well as the establishment of standards, in order to be able to connect and link these scattered legal data resources. This is usually achieved through the transformation of legal information into structured format and through the utilization of legal ontologies whose main purpose is to connect the legal basis of two or more countries by allowing for reusability and the ability to adequately represent legal information, which is understood and retrieved across borders. Within the context of this study, the European Legislation Identifier (ELI) and Akoma Ntoso (AKN) ontologies are mapped in order to make legal data compatible and reusable in as many contexts as possible and to support the Linked Open Legal Data (LOLD) concept. The mapping of these two widely used legal ontologies was evaluated by domain experts and strongly validated by tools. The usefulness of the produced mapping is proven through its real-life context application, although one thing to consider regarding possible future perspectives, could be the inclusion and mapping of more legal ontologies to expand the application domain and improve the semantic interoperability of legal information. These mappings could be either achieved using similar methodological approaches or applications of automated and AI-based ontology mapping techniques.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Theory} and {Practice} of {Electronic} {Governance}},
	publisher = {Association for Computing Machinery},
	author = {Avgerinos Loutsaris, Michalis and Alexopoulos, Charalampos and Maratsi, Maria Ioanna and Charalabidis, Yannis},
	year = {2023},
	note = {event-place: Belo Horizonte, Brazil},
	keywords = {semantic interoperability, Open Government Data, legal data, legal information, linked open legal data (LOLD), OGD, ontology mapping},
	pages = {41--53},
}

@inproceedings{afacan_modeling_2019,
	address = {New York, NY, USA},
	series = {{GoodTechs} '19},
	title = {Modeling a {User}-{Oriented} {Ontology} on {Accessible} {Homes} for {Supporting} {Activities} of {Daily} {Living} ({ADL}) in {Healthy} {Aging}},
	isbn = {978-1-4503-6261-0},
	url = {https://doi.org/10.1145/3342428.3342662},
	doi = {10.1145/3342428.3342662},
	abstract = {Inaccessibility of the buildings is the most common obstacle which presents barriers for older adults with different motor abilities. An inclusive design process, where elderly and designers work together, is required to overcome this obstacle. To do so, this study proposes a user-oriented model (i) to define a knowledge presentation for designers; (ii) to assist them during the development of accessible homes and (iii) to accommodate exemplary home attributes for activities of daily living (ADL). The ontology for this model was first constructed by collecting user information through LEGO® Serious Play® on the four subdomains of motor abilities: (1) strength; (2) balance; (3) locomotion; and (4) endurance. The findings of this study are significant for future aging studies and mobile computing researches in terms of indicating that diverse motor ability difficulties are associated with different requirements of accessibility attributes, and structured knowledge is required to diagrammatize their association with ADL.},
	booktitle = {Proceedings of the 5th {EAI} {International} {Conference} on {Smart} {Objects} and {Technologies} for {Social} {Good}},
	publisher = {Association for Computing Machinery},
	author = {Afacan, Yasemin and Surer, Elif},
	year = {2019},
	note = {event-place: Valencia, Spain},
	keywords = {Ontology, Activities of Daily Living (ADL), Accessible Home, Assisted Living},
	pages = {67--71},
}

@inproceedings{topel_towards_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Towards flexible creation of multi-level models: bottom-up change support in the modeling and programming environment {XModeler}},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561553},
	doi = {10.1145/3550356.3561553},
	abstract = {A process of a multi-level model creation follows typically the top-down approach, i.e., it requires first defining concepts and relations on the highest classification levels, which only then can be used to create concepts on the lower ones. Empirical insights into the process of multi-level model creation suggest however, that this strategy may be counter-intuitive and challenging, especially for non-experts. This paper addresses this problem by focusing on the idea of flexible multi-level model creation, understood as an intertwined application of top-down and bottom-up strategies. As a first step towards realizing this vision for multi-level models in general, and those created with the XModeler and Flexible Meta-Modeling and Execution Language (FMMLx) in particular, in this paper, we select a set of relevant multi-level refactoring patterns, adapt them to our approach, and implement them in the supporting tool. We illustrate the flexible creation process using an exemplary scenario.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Töpel, Daniel and Kaczmarek-Heß, Monika},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {multi-level modeling, bottom-up modeling, flexible meta-modeling and execution language FMMLx, flexible modeling process, multi-level refactoring patterns, XModeler},
	pages = {404--413},
}

@inproceedings{weng_unsupervised_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Unsupervised {Clinical} {Language} {Translation}},
	isbn = {978-1-4503-6201-6},
	url = {https://doi.org/10.1145/3292500.3330710},
	doi = {10.1145/3292500.3330710},
	abstract = {As patients' access to their doctors' clinical notes becomes common, translating professional, clinical jargon to layperson-understandable language is essential to improve patient-clinician communication. Such translation yields better clinical outcomes by enhancing patients' understanding of their own health conditions, and thus improving patients' involvement in their own care. Existing research has used dictionary-based word replacement or definition insertion to approach the need. However, these methods are limited by expert curation, which is hard to scale and has trouble generalizing to unseen datasets that do not share an overlapping vocabulary. In contrast, we approach the clinical word and sentence translation problem in a completely unsupervised manner. We show that a framework using representation learning, bilingual dictionary induction and statistical machine translation yields the best precision at 10 of 0.827 on professional-to-consumer word translation, and mean opinion scores of 4.10 and 4.28 out of 5 for clinical correctness and layperson readability, respectively, on sentence translation. Our fully-unsupervised strategy overcomes the curation problem, and the clinically meaningful evaluation reduces biases from inappropriate evaluators, which are critical in clinical machine learning.},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \&amp; {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Weng, Wei-Hung and Chung, Yu-An and Szolovits, Peter},
	year = {2019},
	note = {event-place: Anchorage, AK, USA},
	keywords = {machine translation, representation learning, consumer health, unsupervised learning},
	pages = {3121--3131},
}

@article{zhu_application_2024,
	title = {Application of {Prompt} {Learning} {Models} in {Identifying} the {Collaborative} {Problem} {Solving} {Skills} in an {Online} {Task}},
	volume = {8},
	url = {https://doi.org/10.1145/3686981},
	doi = {10.1145/3686981},
	abstract = {Collaborative problem solving (CPS) competence is considered one of the essential 21st-century skills. To facilitate the assessment and learning of CPS competence, researchers have proposed a series of frameworks to conceptualize CPS and explored ways to make sense of the complex processes involved in collaborative problem solving. However, encoding explicit behaviors into subskills within the frameworks of CPS skills is still a challenging task. Traditional studies have relied on manual coding to decipher behavioral data for CPS, but such coding methods can be very time-consuming and cannot support real-time analyses. Scholars have begun to explore approaches for constructing automatic coding models. Nevertheless, the existing models built using machine learning or deep learning techniques depend on a large amount of training data and have relatively low accuracy. To address these problems, this paper proposes a prompt-based learning pre-trained model. The model can achieve high performance even with limited training data. In this study, three experiments were conducted, and the results showed that our model not only produced the highest accuracy, macro F1 score, and kappa values on large training sets, but also performed the best on small training sets of the CPS behavioral data. The application of the proposed prompt-based learning pre-trained model contributes to the CPS skills coding task and can also be used for other CSCW coding tasks to replace manual coding.},
	number = {CSCW2},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Zhu, Mengxiao and Wang, Xin and Wang, Xiantao and Chen, Zihang and Huang, Wei},
	month = nov,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language processing, automatic coding, collaborative problem solving, prompt-based learning},
}

@inproceedings{mohamed_extended_2022,
	address = {New York, NY, USA},
	series = {{iiWAS2021}},
	title = {Extended {XACML} {Language} and {Architecture} for {Access} {Control} in {Graph}-structured {Data}},
	isbn = {978-1-4503-9556-4},
	url = {https://doi.org/10.1145/3487664.3487789},
	doi = {10.1145/3487664.3487789},
	abstract = {The rapidly increasing use of graph databases for a wide variety of applications demands flexible authorization and fine-grained access control at the level of attributes associated with the basic entities (i.e., accessing subject, requested resource, performed action, and environmental conditions) but also the vertices and edges along a particular access path. We present a solution for authorization policy specification and enforcement in a graph database to apply fine-grained path-specific constraints on graph-structured data. Therefore, we extend the well-established declarative policy definition language eXtensible Access Control Markup Language (XACML) and its architecture to describe path patterns and enforce the policies using the standard functional components of XACML. Our approach, XACML for Graph-structured data (XACML4G), defines an extended XACML grammar for the authorization policy and access request. To enforce XACML4G policies, we relied on the extensibility points of the XACML architecture and added proprietary extensions. We show the significance of our approach by means of a demonstration prototype in the university domain. Finally, we provide an initial evaluation of the expressiveness and performance of XACML4G with regard to XACML.},
	booktitle = {The 23rd {International} {Conference} on {Information} {Integration} and {Web} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Mohamed, Aya and Auer, Dagmar and Hofer, Daniel and Küng, Josef},
	year = {2022},
	note = {event-place: Linz, Austria},
	keywords = {Graph Database, XACML, Access Control, Authorization Policy, Graph-structured Data, XACML4G},
	pages = {367--374},
}

@inproceedings{tapia-leon_ontological_2019,
	address = {New York, NY, USA},
	series = {{ICICM} '19},
	title = {Ontological {Model} for the {Semantic} {Description} of {Syllabuses}},
	isbn = {978-1-4503-7188-9},
	url = {https://doi.org/10.1145/3357419.3357442},
	doi = {10.1145/3357419.3357442},
	abstract = {The syllabus is a relevant document to organize how the teaching-learning process will be carried out during an academic course in Higher Education Institutions (HEI). Usually, this document is written in a human-readable format that do not enable automatic processing through intelligent services to support teaching and learning. Therefore, we created OntoSyllabus ontology for the representation of syllabuses applying the NeOn methodology. The semantic model of a syllabus will allow the comprehension for both: machines and humans, and it will facilitate the interchange of data between different services and applications. The ontology was created based on the results of our three previous studies, which helped us to determinate the terms and relations in the syllabus ontology. The documentation and the computable model are available on the Internet for their reuse.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Information} {Communication} and {Management}},
	publisher = {Association for Computing Machinery},
	author = {Tapia-Leon, Mariela and Aveiga, Carlos and Chicaiza, Janneth and Suárez-Figueroa, Mari Carmen},
	year = {2019},
	note = {event-place: Prague, Czech Republic},
	keywords = {Ontology, Semantic Web, Higher Education Institution, NeOn Methodology, Syllabus},
	pages = {175--180},
}

@inproceedings{pomarlan_robot_2018,
	address = {Richland, SC},
	series = {{AAMAS} '18},
	title = {Robot {Program} {Construction} via {Grounded} {Natural} {Language} {Semantics} \&amp; {Simulation}},
	abstract = {Robots acting in semi-structured, human environments need to understand the effects of their actions and the instructions given by a human user. Simulation has been considered a promising reasoning technique to help tackle both problems. In this paper, we present a system that constructs an executable robot program from a linguistic semantic specification produced by parsing a natural language sentence; in effect, our system grounds the semantic specification into the produced robot plan. The plan can then be run in a simulated environment, which allows one to infer more about the plan than was present in the initial semantic specification. Our system allows modeling how actions can be modified by subclauses, which we showcase by a transport action. Simulation runs allow discovery of better parameters, either locally for a subtask or such that the entire task is better performed; simulation reveals these parameterizations may differ.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Pomarlan, Mihai and Bateman, John},
	year = {2018},
	note = {event-place: Stockholm, Sweden},
	keywords = {language grounding, human-robot interaction, cognitive robotics, robotic agent languages},
	pages = {857--864},
}

@article{kollapally_ontology_2024,
	title = {An {Ontology} for {Social} {Determinants} of {Education} ({SDoEd}) {Based} on {Human}-{AI} {Collaborative} {Approach}},
	volume = {40},
	issn = {1937-4771},
	abstract = {The use of computational ontologies is well-established in the field of Medical Informatics. The topic of Social Determinants of Health (SDoH) has also received extensive attention. Work at the intersection of ontologies and SDoH has been published. However, a standardized framework for Social Determinants of Education (SDoEd) is lacking. In this paper, we are closing the gap by introducing an SDoEd ontology for creating a precise conceptualization of the interplay between life circumstances of students and their possible educational achievements. The ontology was developed utilizing suggestions from ChatGPT-3.5-010422 and validated using peer-reviewed research articles. The first version of developed ontology was evaluated by human experts in the field of education and validated using standard ontology evaluation software. This version of the SDoEd ontology contains 231 domain concepts, 10 object properties, and 24 data properties.},
	number = {3},
	journal = {J. Comput. Sci. Coll.},
	author = {Kollapally, Navya Martin and Geller, James and Morreale, Patricia and Kwak, Daehan},
	month = oct,
	year = {2024},
	note = {Place: Evansville, IN, USA
Publisher: Consortium for Computing Sciences in Colleges},
	pages = {191--203},
}

@inproceedings{knote_towards_2020,
	address = {USA},
	series = {{PLoP} '18},
	title = {Towards a pattern language for smart personal assistants},
	abstract = {Supporting users in their daily activities, thus, making their lives more comfortable, has long been a goal for consumer-oriented systems development. With the rise of smart personal assistants (SPAs), however, we have reached a new milestone along the path towards this goal. These systems assist their owners by providing personalized and context-dependent information and service. Today's implementations reach from conversational agents, such as Siri, Cortana or Google Assistant, over chatbots, which are primarily text-based, to cognitive assistants, which assist according to a user's current cognitive or emotional state. However, although both research and practice proceed with full pace, recurring design elements of SPAs have not yet been investigated. We hence propose a pattern language for smart personal assistants to guide further empirical and design efforts. Therefore, we review existing information systems, computer science and human-computer interaction literature to find recurring design characteristics among 115 different assistants. The resulting pattern language contains 22 patterns that specify the interaction behavior and the intelligence of smart personal assistants.},
	booktitle = {Proceedings of the 25th {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {The Hillside Group},
	author = {Knote, Robin and Söllner, Matthias and Leimeister, Jan Marco},
	year = {2020},
	note = {event-place: Portland, Oregon},
	keywords = {pattern language, smart personal assistants},
}

@inproceedings{joko_doing_2024,
	address = {New York, NY, USA},
	series = {{SIGIR} '24},
	title = {Doing {Personal} {LAPS}: {LLM}-{Augmented} {Dialogue} {Construction} for {Personalized} {Multi}-{Session} {Conversational} {Search}},
	isbn = {979-8-4007-0431-4},
	url = {https://doi.org/10.1145/3626772.3657815},
	doi = {10.1145/3626772.3657815},
	abstract = {The future of conversational agents will provide users with personalized information responses. However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks. Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality. LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences. When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods. The collected dataset is suited to train preference extraction and personalized response generation. Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history. Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods.},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Joko, Hideaki and Chatterjee, Shubham and Ramsay, Andrew and de Vries, Arjen P. and Dalton, Jeff and Hasibi, Faegheh},
	year = {2024},
	note = {event-place: Washington DC, USA},
	keywords = {personalization, conversational search, dialogue collection},
	pages = {796--806},
}

@article{koch_moving_2023,
	title = {Moving from {ISAD}({G}) to a {CIDOC} {CRM}-based {Linked} {Data} {Model} in the {Portuguese} {Archives}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3605910},
	doi = {10.1145/3605910},
	abstract = {Archives are facing numerous challenges. On the one hand, archival assets are evolving to encompass digitized documents and increasing quantities of born-digital information in diverse formats. On the other hand, the audience is changing along with how it wishes to access archival material. Moreover, the interoperability requirements of cultural heritage repositories are growing. In this context, the Portuguese Archives started an ambitious program aiming to evolve its data model, migrate existing records, and build a new archival management system appropriate to both archival tasks and public access. The overall goal is to have a fine-grained and flexible description, more machine-actionable than the current one. This work describes ArchOnto, a linked open data model for archives, and rules for its automatic population from existing records. ArchOnto adopts a semantic web approach and encompasses the CIDOC Conceptual Reference Model and additional ontologies, envisioning interoperability with datasets curated by multiple communities of practice. Existing ISAD(G)-conforming descriptions are being migrated to the new model using the direct mappings provided here. We used a sample of 25 records associated with different description levels to validate the completeness and conformity of ArchOnto to existing data. This work is in progress and is original in several respects: (1) it is one of the first approaches to use CIDOC CRM in the context of archives, identifying problems and questions that emerged during the process and pinpointing possible solutions; (2) it addresses the balance in the model between the migration of existing records and the construction of new ones by archive professionals; and (3) it adopts an open world view on linking archival data to global information sources.},
	number = {4},
	journal = {J. Comput. Cult. Herit.},
	author = {Koch, Inês and Teixeira Lopes, Carla and Ribeiro, Cristina},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cultural heritage, semantic web, archival description, archives, data migration, linked open data},
}

@inproceedings{sitthisak_analysis_2024,
	address = {New York, NY, USA},
	series = {{IC4E} '24},
	title = {Analysis of {Generated} {Assessment} {Items} from the {COMBA} {Competency} {Model}},
	isbn = {979-8-4007-1706-2},
	url = {https://doi.org/10.1145/3670013.3670022},
	doi = {10.1145/3670013.3670022},
	abstract = {This paper proposes an item analysis of generated assessment items from the COMBA competency model. Two metrics are used including difficulty and discrimination. We generate assessment items by COMBA for evaluating the learner's competence in the Python programming class and use them to examine the cognitive level of learners. The item analysis is applied to check conformance between the capability level of the generated assessment items and the cognitive level of learners. Experimenting with the generated assessment items, the capability level affects the difficulty and discrimination of their items.},
	booktitle = {Proceedings of the 2024 15th {International} {Conference} on {E}-{Education}, {E}-{Business}, {E}-{Management} and {E}-{Learning}},
	publisher = {Association for Computing Machinery},
	author = {Sitthisak, Onjira and Pradubsuwun, Denduang},
	year = {2024},
	note = {event-place: Fukuoka-shi, Japan},
	keywords = {Ontology, Adaptive Assessment, Competency Model, Item Analysis},
	pages = {117--122},
}

@inproceedings{zhomartkyzy_development_2019,
	address = {New York, NY, USA},
	series = {{ICoMS} '19},
	title = {Development of {University} {Scientific} {Knowledge} {Ontological} {Model}},
	isbn = {978-1-4503-7168-1},
	url = {https://doi.org/10.1145/3343485.3343500},
	doi = {10.1145/3343485.3343500},
	abstract = {An ontology is a link between objects of knowledge and a connecting bridge between various steps of Knowledge Processes. Ontology development is an important aspect of knowledge management solution support (KM-solutions). In this paper, we consider a university scientific knowledge ontological model which is one of the knowledge management systems tools. The main functions of the university scientific knowledge ontology are given. The main classes, properties and relations of ontology for maintaining the knowledge base of educational resources are described.},
	booktitle = {Proceedings of the 2019 2nd {International} {Conference} on {Mathematics} and {Statistics}},
	publisher = {Association for Computing Machinery},
	author = {Zhomartkyzy, Gulnaz and Kumargazhanova, Saule and Popova, Galina and Suleimenova, Laura},
	year = {2019},
	note = {event-place: Prague, Czech Republic},
	keywords = {Knowledge management, Knowledge base, Ontological engineering, Electronic scientific resources, Scientific knowledge ontology},
	pages = {40--45},
}

@inproceedings{hughes_towards_2025,
	address = {New York, NY, USA},
	series = {{AAR} {Adjunct} '25},
	title = {Towards {Interoperability}: {Pursuing} an ontology for data exchange between deliberative democratic platforms},
	isbn = {979-8-4007-1968-4},
	url = {https://doi.org/10.1145/3737609.3747119},
	doi = {10.1145/3737609.3747119},
	abstract = {In response to the fragmented state of civic engagement tools and the urgent challenges facing democratic systems, this paper introduces a shared, contributor-driven ontology to connect diverse civic tech platforms, emerging from the work of the Interoperable Deliberative Tool cohort at Metagov. By integrating platforms like Voice to Vision, Assemblis, and Decidim, we enable the flow of deliberative data across contexts, supporting more cohesive decision-making. This approach helps bridge gaps between input, analysis, and action, enhancing democratic resilience in crisis moments. Through our work, we demonstrate how interoperability can strengthen civic engagement and provide a foundation for more responsive, collaborative governance.},
	booktitle = {Adjunct {Proceedings} of the {Sixth} {Decennial} {Aarhus} {Conference}: {Computing} {X} {Crisis}},
	publisher = {Association for Computing Machinery},
	author = {Hughes, Margaret and DeSota, Elianna and Victor, Matthew and Lynn, Stuart and Stormonth-Darling, John M, John and Barry, Liz},
	year = {2025},
	keywords = {Interoperability, Governance, Civic Technology, Data Commons, Deliberative Democracy, Digital Civics},
}

@inproceedings{delabeye_scalable_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Scalable ontology-based {V}\&amp;{V} process for heterogeneous systems and applications},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561577},
	doi = {10.1145/3550356.3561577},
	abstract = {This work focuses on ongoing research within the EU-funded EnerMan project aiming at improving the energy efficiency of manufacturing systems. Industrial use cases are generally too constrained to easily proceed to the verification and validation (V\&amp;V) of the scientific approaches tackling their challenges. In this context, we propose an ontology-based framework with a methodology assessing the scalability of heterogeneous systems, environments, and missions in a V\&amp;V context. Indeed, projecting these industrial and laboratory applications onto a meaningful ontology allows them to be flattened out to the same scale from a semantic point of view. Reasoning is used to evaluate the extent to which a given scientific approach can be verified on a laboratory use case different from the industrial scenario on which it has to be validated. The framework has been implemented using Protégé and Owlready2, and applied to a scientific approach focused on a blind source separation technique used to identify system operating modes in a black box manner, tested on a coffee machine and two industrial case studies (a vehicle testbed's heating ventilation and air conditioning system, and a chocolate production line).},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Delabeye, Romain and Penas, Olivia and Plateaux, Régis},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {ontology, validation, scalability, heterogeneous systems, requirements analysis, verification \&amp},
	pages = {341--350},
}

@inproceedings{yu_leveraging_2024,
	address = {New York, NY, USA},
	series = {{ICNCC} '23},
	title = {Leveraging {Graph} {Databases} for {Automated} {OPC} {UA} {Information} {Model} {Construction}},
	isbn = {979-8-4007-0926-5},
	url = {https://doi.org/10.1145/3638837.3638882},
	doi = {10.1145/3638837.3638882},
	abstract = {To address the OPC UA transformation requirements of conventional factory data collection systems, we propose an innovative method for the automated construction of OPC UA information models utilizing graph databases. This method not only streamlines the process but also minimizes the need for extensive manual labor and specialized expertise. Drawing from the resemblance between OPC UA information models and OWL ontologies, we have devised mapping rules that facilitate the translation of knowledge graph data into OPC UA format. Leveraging the existing Neo4j database within the factory as the primary information source, we’ve developed an OWL ontology construction module for exporting ontology files. In parallel, an information model construction module has been designed to convert the ontology file into an OPC UA XML file, complete with the OPC UA information model. This XML file adheres to the official OPC UA specification, ensured through the use of the UA-ModelCompiler tool. To validate the effectiveness and viability of this construction method, we conducted functional tests and evaluations using a publicly available database. The results of these tests confirm the feasibility of our approach, marking a significant advancement in OPC UA information model construction for factory data systems.},
	booktitle = {Proceedings of the 2023 12th {International} {Conference} on {Networks}, {Communication} and {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Rongdong and Zhong, Yaoyi and Xu, Yunliang and Wen, Jie and Pan, Quanhong and Sha, Wanli and Wang, Zhan},
	year = {2024},
	note = {event-place: Osaka, Japan},
	keywords = {OWL ontology, Neo4j, graph database, OPC UA information model},
	pages = {294--299},
}

@inproceedings{ramzy_first_2021,
	address = {Orlando, Florida},
	series = {{WSC} '20},
	title = {First steps towards bridging simulation and ontology to ease the model creation on the example of semiconductor industry},
	isbn = {978-1-7281-9499-8},
	abstract = {With diverse product mixes in fabs, high demand volatility, and numerous manufacturing steps spread across different facilities, it is impossible to analyze the combined impacts of multiple operations in semiconductor supply chains without a modeling tool like simulation. This paper explains how ontologies can be used to develop and deploy simulation applications, with interoperability and knowledge sharing at the semantic level. This paper proposes a concept to automatically build simulations using ontologies and its preliminary results. The proposed approach seeks to save time and effort expended in recreating the information for different use cases that already exists elsewhere. The use case provides first indications that with an enhancement of a so-called Digital Reference with Semantic Web Technologies, modeling and simulation of semiconductor supply chains will not only become much faster but also require less modeling efforts because of the reusability property.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Ramzy, Nour and Martens, Christian James and Singh, Shreya and Ponsignon, Thomas and Ehm, Hans},
	year = {2021},
	pages = {1789--1800},
}

@inproceedings{lieber_montolostats_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {{MontoloStats} - {Ontology} {Modeling} {Statistics}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364433},
	doi = {10.1145/3360901.3364433},
	abstract = {Within ontology engineering concepts are modeled as classes and relationships, and restrictions as axioms. Reusing ontologies requires assessing if existing ontologies are suited for an application scenario. Different scenarios not only influence concept modeling, but also the use of different restriction types, such as subclass relationships or disjointness between concepts. However, metadata about the use of such restriction types is currently unavailable, preventing accurate assessments for reuse. We created the RDF Data Cube-based dataset MontoloStats, which contains restriction use statistics for 660 LOV and 565 BioPortal ontologies. We analyze the dataset and discuss the findings and their implications for ontology reuse. The MontoloStats dataset reveals that 94\% of LOV and 95\% of BioPortal ontologies use RDFS-based restriction types, 49\% of LOV and 52\% of BioPortal ontologies use at least one OWL-based restriction type, and different literal value-related restriction types are not or barely used. Our dataset provides modeling insights, beneficial for ontology reuse to discover and compare reuse candidates, but can also be the basis of new research that investigates novel ontology engineering methodologies with respect to restrictions definition.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Lieber, Sven and De Meester, Ben and Dimou, Anastasia and Verborgh, Ruben},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {ontology engineering, rdf, axioms, restrictions, statistics},
	pages = {69--76},
}

@inproceedings{hinkel_modeling_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Modeling a {Warehouse} system using refinements and decomposition: {A} contribution to the {MULTI} {Warehouse} challenge},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688209},
	doi = {10.1145/3652620.3688209},
	abstract = {Traditional two-level modeling has limitations when it comes to represent domain-specific, non-transitive instantiation relationships. Despite many approaches tackle this problem, there is no consensus on how to models these cases best. To have a common benchmark to discuss the different approaches, the MULTI workshop has set the warehouse model challenge. In this paper, we present and discuss a solution of this challenge applying deep modeling through refinements and structural decomposition - a technology that supports deep modeling with just few extensions to EMOF [12].},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hinkel, Georg},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {NMF, deep modeling, refinements and decomposition},
	pages = {770--779},
}

@inproceedings{fiyouzisabah_towards_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Towards {Rapid} {Design} of {Compartmental} {Models}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688340},
	doi = {10.1145/3652620.3688340},
	abstract = {In times of crisis, epidemiologists can come under great pressure to model rapidly evolving diseases and to produce analyses about the effects of potential public health interventions. Taking previously developed, tested, and validated model components as the base on which to prototype new infectious disease models can save precious time and effort. However, there is currently no systematic process for quickly navigating a corpus of existing epidemiological models or identifying and reusing their most useful components. In this paper, we propose a vision to accelerate the creation of prototype compartmental models for infectious diseases. We outline a semi-automated process that epidemiologists can use to create prototypes that have been partially completed with reused fragments from existing models. Epidemiologists can thus focus on modelling the novel aspects of an ongoing public health crisis, as opposed to aspects of it that are already more or less well understood in previous work. Our approach comprises five steps in total, including identifying useful components in a corpus of infectious disease models, generating potential candidate prototypes, and organizing them in a formal data structure that allows navigation and exploration by the modellers. We outline 13 challenges ahead and discuss potential solutions based on formal modelling techniques.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fiyouzisabah, Zahra and Galasso, Jessie and Fokaefs, Marios and Famelis, Michalis},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {evolution, formal concept analysis, reuse, compartmental models, domain specific modelling, prototyping, scientific computing},
	pages = {1041--1045},
}

@inproceedings{gupta_solving_2023,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '23},
	title = {Solving age-word problems using domain ontology and {BERT}},
	isbn = {978-1-4503-9797-1},
	url = {https://doi.org/10.1145/3570991.3571058},
	doi = {10.1145/3570991.3571058},
	abstract = {An age word problem (ageWP) typically involves sentences that express relationships between the age of the agents and asks for the age of one of them. Automatically solving ageWPs is a challenging task as we need to tackle temporal relationships between the agent’s ages, frame and solve the equations for the required unknowns. To the best of our knowledge, there exists only one ageWP dataset consisting of just 124 examples. The dataset is too small to employ a learning-based solver, mainly consisting of ageWPs with simple temporal relationships. To address this issue, in our earlier work, we designed a description-logic based ontology (ageWP-ont) for the domain of age word problems and utilized it to automatically generate a large number of ageWPs. Sentences in these ageWPs relate the ages of agents in a temporally complex manner. In this paper, we focus on solving these problems. We analyzed an existing learning-based solver of algebraic word-problems that uses a traditional machine learning approach and found that the solver can be adapted to our domain. But we found that this approach does not seem to perform well, perhaps due to the complex nature of the ageWPs. As we have the ontology of the domain on hand, we propose a new approach of utilizing it in the deep-learning based NLU component of the solution. We annotate parts of the ageWP sentences with class-names from ageWP-ont and train a BERT-based language model (LM) that learns to predict the instances for these classes in the given sentences. An RDF graph is populated with these values and serves as a concrete problem-specific instance of the ontology. The dataset for training the LM is automatically generated with the help of ageWP-ont. Finally, for the actual solving of a given ageWP, we make use of its RDF graph and employ Semantic Web Rule Language (SWRL) rules. We implemented the proposed system and achieved 68.8\% accuracy. The work demonstrates that combining deep learning with ontologies can give impressive results.},
	booktitle = {Proceedings of the 6th {Joint} {International} {Conference} on {Data} {Science} \&amp; {Management} of {Data} (10th {ACM} {IKDD} {CODS} and 28th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Gupta, Akshay and Kumar, Suresh and Kumar P, Sreenivasa},
	year = {2023},
	note = {event-place: Mumbai, India},
	keywords = {Ontology, BERT, SWRL, Deep learning, Semantic web rule language, Resource Description Framework (RDF), Age-word problem solver, OWL-DL ontology, Ontology's, Rules languages, Semantic Web rules, Age-word problem solv, Data description, Problem solvers, Temporal relationships, Word problem},
	pages = {95--103},
	annote = {Cited by: 1},
}

@article{xu_agenttod_2025,
	title = {{AgentTOD}: {A} {Task}-{Oriented} {Dialogue} {Agent} with a {Flexible} and {Adaptive} {API} {Calling} {Paradigm}},
	volume = {43},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3745021},
	doi = {10.1145/3745021},
	abstract = {Task-oriented dialogue (TOD) systems play a vital role in numerous assistance and service scenarios, significantly improving people’s daily lives. Conventionally, a TOD system adheres to a fixed paradigm, where it must first extract user goals and query external databases before it can generate the final response. However, this fixed extract-and-query paradigm is not always optimal for all dialogue turns, which is redundant for the simple turns that do not need external information, and is inadequate for the complex turns that need to interact with the external world multiple times. To address the limitations, in this article, we propose AgentTOD, a novel TOD framework that uses a large language model (LLM) as the intelligent agent to achieve a flexible dialogue paradigm. AgentTOD deprecates the traditional modular architecture (including dialogue state tracking and dialogue policy) by utilizing an LLM as the controller brain to determine when and how to call the provided APIs to obtain external information. It can choose to call APIs any number of times with various parameters until it’s enough to reply to the user. Besides, to train AgentTOD, we construct a large and comprehensive TOD dataset, called TrajsTOD (Trajectories of TODs), which consists of 66k+ user-agent dialogue trajectories converted from eight popular TOD datasets covering 60 domains. TrajsTOD is constructed with minimal dialogue annotations where only the API calling logs are needed and can empower AgentTOD with the general ability to call APIs and generate responses according to the task definition. Extensive experimental results on the MultiWOZ-series and SGD datasets demonstrate AgentTOD has superior performance on TODs as well as a superior adaptability to new task scenarios.},
	number = {5},
	journal = {ACM Trans. Inf. Syst.},
	author = {Xu, Heng-Da and Mao, Xian-Ling and Sun, Fanshu and Che, Tian-Yi and Xu, Chun and Huang, Heyan},
	month = aug,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large Language Models, Intelligent Agents, Task-Oriented Dialogue},
}

@inproceedings{strengers_adhering_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Adhering, {Steering}, and {Queering}: {Treatment} of {Gender} in {Natural} {Language} {Generation}},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376315},
	doi = {10.1145/3313831.3376315},
	abstract = {Natural Language Generation (NLG) supports the creation of personalized, contextualized, and targeted content. However, the algorithms underpinning NLG have come under scrutiny for reinforcing gender, racial, and other problematic biases. Recent research in NLG seeks to remove these biases through principles of fairness and privacy. Drawing on gender and queer theories from sociology and Science and Technology studies, we consider how NLG can contribute towards the advancement of gender equity in society. We propose a conceptual framework and technical parameters for aligning NLG with feminist HCI qualities. We present three approaches: (1) adhering to current approaches of removing sensitive gender attributes, (2) steering gender differences away from the norm, and (3) queering gender by troubling stereotypes. We discuss the advantages and limitations of these approaches across three hypothetical scenarios; newspaper headlines, job advertisements, and chatbots. We conclude by discussing considerations for implementing this framework and related ethical and equity agendas.},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Strengers, Yolande and Qu, Lizhen and Xu, Qiongkai and Knibbe, Jarrod},
	year = {2020},
	note = {event-place: Honolulu, HI, USA},
	keywords = {natural language generation, feminist hci},
	pages = {1--14},
}

@inproceedings{wu_neuro-symbolic_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {A {Neuro}-{Symbolic} {Approach} to {Symbol} {Grounding} for {ALC}-{Ontologies}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3736926},
	doi = {10.1145/3711896.3736926},
	abstract = {Neuro-symbolic computing aims to integrate neural learning with symbolic reasoning to address the fundamental challenge of symbol grounding. While neural networks excel at pattern recognition, they struggle to maintain logical consistency. Conversely, symbolic systems provide formal reasoning capabilities but lack mechanisms for handling perceptual uncertainty. This paper introduces EmALC , a novel neuro-symbolic framework that bridges neural perception with symbolic logic through differentiable fuzzy semantics. Our approach addresses a key limitation of existing methods: while previous neuro-symbolic approaches like Logic Tensor Networks employ first-order fuzzy logic, where key reasoning problems are undecidable, EmALC ensures decidable reasoning by leveraging a fuzzy variant of ALC – a decidable fragment of first-order logic. Unlike previous approaches that often compromise logical soundness for learning capability, EmALC maintains provable semantic consistency through a hierarchical loss function while mitigating reasoning shortcuts via rule-based revision strategies. Experimental evaluation demonstrates EmALC's effectiveness: on ontology revision tasks, it achieves 100\% success rate in correcting masked groundings while preserving semantic integrity; on semantic image interpretation tasks, it improves object classification F1-scores by up to 5.56\% through ontology-guided knowledge revision.},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Wu, Xuan and Zhao, Yizheng},
	year = {2025},
	note = {event-place: Toronto ON, Canada},
	keywords = {description logic, differentiable fuzzy semantics, neuro-symbolic computing, symbol grounding},
	pages = {3240--3249},
}

@inproceedings{wang_ontology-enhanced_2024,
	address = {New York, NY, USA},
	series = {{ICCPR} '23},
	title = {An {Ontology}-enhanced {Knowledge} {Graph} {Embedding} {Method}},
	isbn = {979-8-4007-0798-8},
	url = {https://doi.org/10.1145/3633637.3633645},
	doi = {10.1145/3633637.3633645},
	abstract = {Knowledge graph embedding maps entities and relationships of the graph into low dimensional dense vectors to expresse the semantic information, meantime provides effective support for downstream tasks such as link prediction. However, the existing knowledge graph embedding methods mainly focus on the explicit structured information in the graph and rarely use the entailed rich ontological knowledge. Therefore in the paper, a method for injecting ontology information into the embedding model is proposed, ontology information including class hierarchy information and relationship attribute constraints,especially symmetry attributes are considerd. By taking ontology information as extra constraints, the loss function is further refined.the generation of training samples is optimized and the number of false negative samples is limited. Experiments on the two datasets of DBpedia15K and NELL show that the embedding model can be further optimized by injecting ontology information. Specially, the hit rate of triple prediction is 63.70\% for the no-type, and for type-triples the MR and the H@10 are 13.51 and 96.59\% respectively. The proposed model has better performance than the basic model, which further confirms the effectiveness of the prior knowledge of ontology in knowledge graph embedding learning.},
	booktitle = {Proceedings of the 2023 12th {International} {Conference} on {Computing} and {Pattern} {Recognition}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Changlong and Gan, Tingting and Li, Xingyu and Zhang, Linghan and Wang, Xijie},
	year = {2024},
	note = {event-place: Qingdao, China},
	keywords = {Knowledge graph, Reasoning, Symmetry, Knowledge embedding, Ontology information},
	pages = {51--57},
}

@article{kim_natural_2020,
	title = {Natural language to {SQL}: where are we today?},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3401960.3401970},
	doi = {10.14778/3401960.3401970},
	abstract = {Translating natural language to SQL (NL2SQL) has received extensive attention lately, especially with the recent success of deep learning technologies. However, despite the large number of studies, we do not have a thorough understanding of how good existing techniques really are and how much is applicable to real-world situations. A key difficulty is that different studies are based on different datasets, which often have their own limitations and assumptions that are implicitly hidden in the context or datasets. Moreover, a couple of evaluation metrics are commonly employed but they are rather simplistic and do not properly depict the accuracy of results, as will be shown in our experiments. To provide a holistic view of NL2SQL technologies and access current advancements, we perform extensive experiments under our unified framework using eleven of recent techniques over 10+ benchmarks including a new benchmark (WTQ) and TPC-H. We provide a comprehensive survey of recent NL2SQL methods, introducing a taxonomy of them. We reveal major assumptions of the methods and classify translation errors through extensive experiments. We also provide a practical tool for validation by using existing, mature database technologies such as query rewrite and database testing. We then suggest future research directions so that the translation can be used in practice.},
	number = {10},
	journal = {Proc. VLDB Endow.},
	author = {Kim, Hyeonji and So, Byeong-Hoon and Han, Wook-Shin and Lee, Hongrae},
	month = jun,
	year = {2020},
	note = {Publisher: VLDB Endowment},
	pages = {1737--1750},
}

@inproceedings{rollo_cem_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {{CEM}: an {Ontology} for {Crime} {Events} in {Newspaper} {Articles}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3577862},
	doi = {10.1145/3555776.3577862},
	abstract = {The adoption of semantic technologies for the representation of crime events can help law enforcement agencies (LEAs) in crime prevention and investigation. Moreover, online newspapers and social networks are valuable sources for crime intelligence gathering. In this paper, we propose a new lightweight ontology to model crime events as they are usually described in online news articles. The Crime Event Model (CEM) can integrate specific data about crimes, i.e., where and when they occurred, who is involved (author, victim, and other subjects involved), which is the reason for the occurrence, and details about the source of information (e.g., the news article). Extracting structured data from multiple online sources and interconnecting them in a Knowledge Graph using CEM allow events relationships extraction, patterns and trends identification, and event recommendation.The CEM ontology is available at https://w3id.org/CEMontology.},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Rollo, Federica and Po, Laura and Castellucci, Alessandro},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {crime analysis, lightweight ontology, newspaper},
	pages = {1762--1765},
}

@inproceedings{zhou_ontology-based_2023,
	address = {New York, NY, USA},
	series = {{CSAE} '23},
	title = {Ontology-based {Case} {Representation} of {Mechatronic} {Product} {Eco}-design and {Development} of a {Cloud}-based {Case} {Library}},
	isbn = {979-8-4007-0059-0},
	url = {https://doi.org/10.1145/3627915.3627925},
	doi = {10.1145/3627915.3627925},
	abstract = {Product life cycle eco-design is challenging, knowledge-intensive and information dependent. This study aims to develop a case library system for eco-design knowledge management and to facilitate eco-design practice of mechatronic product (i.e. manufacturing equipment, construction machinery, vehicles). An ontology-based representation of mechatronic product life cycle eco-design is proposed, which enables the structuring and standardization of related data and the storage of such data in a computer-processable manner. Descriptive features of eco-design case are defined in accordance to the ontology model, based on which the similarity between eco-design case and case queries is calculated using the nearest neighbour method and case retrieval is realized based on the calculated similarity. With the proposed ontology model, a cloud-based eco-design case library is then developed, which provide the benefits of easy deployment and maintenance, and better accessibility.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Computer} {Science} and {Application} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Zhenhuan and Chen, Yingyu and Wu, Rujing and Tao, Jing},
	year = {2023},
	note = {event-place: Virtual Event, China},
	keywords = {Ontology, Cloud-based, Life Cycle Eco-Design, Web Service},
}

@inproceedings{wang_towards_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Towards {Mutual} {Theory} of {Mind} in {Human}-{AI} {Interaction}: {How} {Language} {Reflects} {What} {Students} {Perceive} {About} a {Virtual} {Teaching} {Assistant}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445645},
	doi = {10.1145/3411764.3445645},
	abstract = {Building conversational agents that can conduct natural and prolonged conversations has been a major technical and design challenge, especially for community-facing conversational agents. We posit Mutual Theory of Mind as a theoretical framework to design for natural long-term human-AI interactions. From this perspective, we explore a community’s perception of a question-answering conversational agent through self-reported surveys and computational linguistic approach in the context of online education. We first examine long-term temporal changes in students’ perception of Jill Watson (JW), a virtual teaching assistant deployed in an online class discussion forum. We then explore the feasibility of inferring students’ perceptions of JW through linguistic features extracted from student-JW dialogues. We find that students’ perception of JW’s anthropomorphism and intelligence changed significantly over time. Regression analyses reveal that linguistic verbosity, readability, sentiment, diversity, and adaptability reflect student perception of JW. We discuss implications for building adaptive community-facing conversational agents as long-term companions and designing towards Mutual Theory of Mind in human-AI interaction.},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qiaosi and Saha, Koustuv and Gregori, Eric and Joyner, David and Goel, Ashok},
	year = {2021},
	note = {event-place: Yokohama, Japan},
	keywords = {conversational agent, human-AI interaction, language analysis, online community, online education, theory of mind},
}

@inproceedings{oliveira_case_2023,
	address = {New York, NY, USA},
	series = {{LADC} '23},
	title = {A case study on the development of an ontology for maintenance services of heavy machinery electronic components},
	isbn = {979-8-4007-0844-2},
	url = {https://doi.org/10.1145/3615366.3625069},
	doi = {10.1145/3615366.3625069},
	abstract = {Inadequate data organization within a company can result in decreased efficiency, increased costs, and longer delivery times. In the context of an electronic maintenance laboratory servicing a mining company internally, the lack of data organization hampers the conversion of information into actionable knowledge, affecting delivery efficiency. This study aims to address these issues by transforming existing data into structured knowledge using the Methontology and OntoForInfoScience methodologies. The ontology model was validated by both experts and a software plug-in and seeks to offers a systematic representation of the domain, facilitating data understanding and utilization, ultimately leading to cost and time savings in deliveries and ongoing process improvements.},
	booktitle = {Proceedings of the 12th {Latin}-{American} {Symposium} on {Dependable} and {Secure} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Oliveira, Luiza Bartels and Araujo, Marco Antonio and Dantas, Mario Antonio},
	year = {2023},
	note = {event-place: La Paz, Bolivia},
	keywords = {Knowledge acquisition, ontology, maintenance data, maintenance service., mining, semantic data},
	pages = {188--191},
}

@inproceedings{zulfiya_ontological_2019,
	address = {New York, NY, USA},
	series = {{ICEMIS} '19},
	title = {Ontological model for student's knowledge assessment},
	isbn = {978-1-4503-7212-1},
	url = {https://doi.org/10.1145/3330431.3330449},
	doi = {10.1145/3330431.3330449},
	abstract = {This article considers the ontological model for knowledge representation on the example of the discipline "Database Theory".Depending on the chosen intelligent system development environment, knowledge must be represented by certain data structures. In this paper, the ontological model is chosen as a model for knowledge representation.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Engineering} and {MIS}},
	publisher = {Association for Computing Machinery},
	author = {Zulfiya, Kaderkeyeva and Gulmira, Bekmanova and Altynbek, Sharipbay},
	year = {2019},
	note = {event-place: Astana, Kazakhstan},
	keywords = {artificial intelligence, ontology, knowledge, knowledge base, knowledge models, logic, sets},
}

@inproceedings{joshi_natural_2020,
	address = {New York, NY, USA},
	series = {{CoDS} {COMAD} 2020},
	title = {A {Natural} {Language} and {Interactive} {End}-to-{End} {Querying} and {Reporting} {System}},
	isbn = {978-1-4503-7738-6},
	url = {https://doi.org/10.1145/3371158.3371198},
	doi = {10.1145/3371158.3371198},
	abstract = {Natural language query understanding for unstructured textual sources has seen significant progress over the last couple of decades. For structured data, while the ecosystem has evolved with regard to data storage and retrieval mechanisms, the query language has remained predominantly SQL (or SQL-like). Towards making the latter more natural there has been recent research emphasis on Natural Language Interface to DataBases (NLIDB) systems. Piggybacking on the rise of 'deep learning' systems, the state-of-the-art NLIDB solutions over large parallel and standard benchmarks (viz, WikiSQL and Spider) primarily rely on attention based sequence-to-sequence models.Building industry grade NLIDB solutions for making big data ecosystem accessible by truly natural and unstructured querying mechanism presents several challenges. These include lack of availability of parallel corpora, diversity in underlying data schema, wide variability in the nature of queries to context and dialog management in interactive systems. In this paper, we present an end-to-end system Query Enterprise Data (QED) towards making enterprise descriptive analytics and reporting easier and natural. We elaborate in detail how we addressed the challenges mentioned above and novel features such as handling incomplete queries in incremental fashion as well as highlight the role of an assistive user interface that provides a better user experience. Finally, we conclude the paper with observations and lessons learnt from the experience of transferring and deploying a research solution to industry grade practical deployment.},
	booktitle = {Proceedings of the 7th {ACM} {IKDD} {CoDS} and 25th {COMAD}},
	publisher = {Association for Computing Machinery},
	author = {Joshi, Salil Rajeev and Venkatesh, Bharath and Thomas, Dawn and Jiao, Yue and Roy, Shourya},
	year = {2020},
	note = {event-place: Hyderabad, India},
	keywords = {SQL, Information Retrieval, Natural Language Understanding, NL2SQL, NLIDB, Semantic Parsing},
	pages = {261--267},
}

@inproceedings{papenmeier_modern_2020,
	address = {New York, NY, USA},
	series = {{DIS} '20},
	title = {'{A} {Modern} {Up}-{To}-{Date} {Laptop}' - {Vagueness} in {Natural} {Language} {Queries} for {Product} {Search}},
	isbn = {978-1-4503-6974-9},
	url = {https://doi.org/10.1145/3357236.3395489},
	doi = {10.1145/3357236.3395489},
	abstract = {With the rise of voice assistants and an increase in mobile search usage, natural language has become an important query language. So far, most of the current systems are not able to process these queries because of the vagueness and ambiguity in natural language. Users have adapted their query formulation to what they think the search engine is capable of, which adds to their cognitive burden. With our research, we contribute to the design of interactive search systems by investigating the genuine information need in a product search scenario. In a crowd-sourcing experiment, we collected 132 information needs in natural language. We examine the vagueness of the formulations and their match to retailer-generated content and user-generated product reviews. Our findings reveal high variance on the level of vagueness and the potential of user reviews as a source for supporting users with rather vague search intents.},
	booktitle = {Proceedings of the 2020 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Papenmeier, Andrea and Sliwa, Alfred and Kern, Dagmar and Hienert, Daniel and Aker, Ahmet and Fuhr, Norbert},
	year = {2020},
	note = {event-place: Eindhoven, Netherlands},
	keywords = {information retrieval, natural language, information need, query formulation, vagueness},
	pages = {2077--2089},
}

@article{hu_improving_2023,
	title = {Improving {Causal} {Bayesian} {Networks} {Using} {Expertise} in {Authoritative} {Medical} {Ontologies}},
	volume = {4},
	url = {https://doi.org/10.1145/3604561},
	doi = {10.1145/3604561},
	abstract = {Discovering causal relationships among symptoms is a topical issue in the analysis of observational patient datasets. A Causal Bayesian Network (CBN) is a popular analytical framework for causal inference. While there are many methods and algorithms capable of learning a Bayesian network, they are reliant on the complexity and thoroughness of the algorithm and do not consider prior expertise from authoritative sources. This article proposes a novel method of extracting prior causal knowledge contained in Authoritative Medical Ontologies (AMOs) and using this prior knowledge to orient arcs in a CBN learned from observational patient data. Since AMOs are robust biomedical ontologies containing the collective knowledge of the experts who created them, utilizing the ordering information contained within them produces improved CBNs that provide additional insight into the disease domain.To demonstrate our method, we obtained prior causal ordering information among symptoms from three AMOs: (1) the Medical Dictionary for Regulatory Activities Terminology (MedDRA), (2) the International Classification of Diseases Version 10 Clinical Modification (ICD-10-CM), and (3) Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT). The prior ontological knowledge from these three AMOs is then used to orient arcs in a series of CBNs learned from the National Institutes of Mental Health study on Sequenced Treatment Alternatives to Relieve Depression (STAR*D) patient dataset using the Max-Min Hill-Climbing (MMHC) algorithm. Six distinct CBNs are generated using MMHC: an unmodified baseline model using only the algorithm, three CBNs oriented with ordered-variable pairs from MedDRA, ICD-10-CM, and SNOMED CT, and two more with ordered pairs from a combination of these AMOs. The resulting CBNs modified using ordered-variable pairs significantly change the structure of the network. The agreement between the Modified networks and the Baseline ranges from 50\% to 90\%. A modified network using ordering information from all ontologies obtained an agreement of 50\% (10 out of 20 arcs exist in both the Baseline and Modified models) while maintaining comparable predictive accuracy. This indicates that the Modified CBN reflects the causal claims in the AMOs and agrees with both the AMOs and the observational STAR*D dataset. Furthermore, the Modified models discovered new potentially causal relationships among symptoms in the model, while eliminating weaker edges in a qualitative analysis of the significance of these relationships in existing epidemiological research.},
	number = {4},
	journal = {ACM Trans. Comput. Healthcare},
	author = {Hu, Hengyi and Kerschberg, Larry},
	month = oct,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, Bayesian networks, data mining, healthcare data, data management, Patient data, ontology evolution, causal inference, causal networks, causality, healthcare information technology},
}

@inproceedings{boudjemila_maintaining_2024,
	address = {New York, NY, USA},
	series = {{ICSSP} '24},
	title = {Maintaining {Security} {Consistency} {During} {System} {Development} with {Security}-{Oriented} {Model} {Federation}},
	isbn = {979-8-4007-0991-3},
	url = {https://doi.org/10.1145/3666015.3666016},
	doi = {10.1145/3666015.3666016},
	abstract = {Multi-modeling is an approach within the MDE realm that promotes the development of complex systems by decomposing them in sets of heterogeneous models. These models are defined using different modeling languages and constructed using diverse tools. They represent different but often interdependent views. However, the models of a system are far from being static. They change to accommodate new requirements, functionality improvements, bug fixes, and other evolution events. These changes represent a challenge w.r.t. consistency. This is especially true in security-critical scenarios. Indeed, security information is often integrated within the systems models so that security requirements are met following what is called "security-by-design". In such scenarios, the security concern of the systems models must remain consistent across changes so that security properties continue to hold. In order to tackle this problem, we propose a methodology to enhance the (multi)model-based design phase of a system development process. It comprises the creation of a security federation in which security dependencies between the different models are reified and equipped with security rules expressing security consistency requirements. Then, whenever a model is changed, the security rules are evaluated to monitor the consistency of security across the system models. We evaluate the capabilities of this methodology by a prototype implementation and its application to different use cases.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Software} and {Systems} {Processes}},
	publisher = {Association for Computing Machinery},
	author = {Boudjemila, Chahrazed and Dagnat, Fabien and Martínez, Salvador},
	year = {2024},
	note = {event-place: M{\textbackslash}, Germany},
	keywords = {Model-driven engineering, model evolution., model federation, security by design},
	pages = {66--76},
}

@inproceedings{shaaban_ontology-based_2020,
	address = {New York, NY, USA},
	series = {{iiWAS2019}},
	title = {Ontology-{Based} {Model} for {Automotive} {Security} {Verification} and {Validation}},
	isbn = {978-1-4503-7179-7},
	url = {https://doi.org/10.1145/3366030.3366070},
	doi = {10.1145/3366030.3366070},
	abstract = {Modern automobiles are considered semi-autonomous vehicles regarding new adaptive technologies. New cars consist of a vast number of electronic units for managing and controlling the functional safety in a vehicle. In the vehicular industry, safety and security are considered two sides for the same coin. Therefore, improving functional safety in the vehicular industry is essential to protect the vehicle from different attack scenarios. This work introduces an ontology-based model for security verification and validation in the vehicular domain. The model performs a series of logical quires and inference rules to ensure that the security requirements are fulfilled. It endeavors to enhance the current security state of a vehicle by selecting additional security requirements that can handle existence security weaknesses and meet the actual security goal.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Shaaban, Abdelkader Magdy and Schmittner, Christoph and Gruber, Thomas and Mohamed, A. Baith and Quirchmayr, Gerald and Schikuta, Erich},
	year = {2020},
	note = {event-place: Munich, Germany},
	keywords = {Ontology, Automotive, Protection Profile, Security Requirements, Threats, Verification and Validation},
	pages = {73--82},
}

@inproceedings{weigelt_integrating_2018,
	address = {New York, NY, USA},
	series = {{RAISE} '18},
	title = {Integrating a dialog component into a framework for spoken language understanding},
	isbn = {978-1-4503-5723-4},
	url = {https://doi.org/10.1145/3194104.3194105},
	doi = {10.1145/3194104.3194105},
	abstract = {Spoken language interfaces are the latest trend in human computer interaction. Users enjoy the newly found freedom but developers face an unfamiliar and daunting task. Creating reactive spoken language interfaces requires skills in natural language processing. We show how a developer can integrate a dialog component in a natural language processing system by means of software engineering methods. Our research project PARSE that aims at naturalistic end-user programming in spoken natural language serves as an example. We integrate a dialog component with PARSE without affecting its other components: We modularize the dialog management and introduce dialog acts that bundle a trigger for the dialog and the reaction of the system. We implemented three dialog acts to address the following issues: speech recognition uncertainties, coreference ambiguities, and incomplete conditionals.We conducted a user study with ten subjects to evaluate our approach. The dialog component achieved resolution rates from 23\% to 50\% (depending on the dialog act) and introduces a negligible number of errors. We expect the overall performance to increase even further with the implementation of additional dialog acts.},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Realizing} {Artificial} {Intelligence} {Synergies} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Weigelt, Sebastian and Hey, Tobias and Landhäußer, Mathias},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {human-computer interaction, dialog integration, dialog systems, enduser programming, knowledge-based software engineering, natural language processing for software engineering, naturalistic programming, programming in natural language},
	pages = {1--7},
}

@inproceedings{velloso_theorising_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Theorising in {HCI} using {Causal} {Models}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713789},
	doi = {10.1145/3706598.3713789},
	abstract = {Although the literature on Human-Computer Interaction (HCI) catalogues many theories, it offers surprisingly few tools for theorising. This paper critiques dominant approaches to engaging with theory and proposes a working model for theorising in HCI. We then present graphical causal modelling as an effective theorising tool. This includes a step-by-step guide to building causal models and examples of their use in different stages of the research process. We explain how causal models help develop method-agnostic representations of research problems using directed acyclic graphs, identify potential confounders, and construct alternative interpretations of data. Finally, we discuss their limitations and challenges for adoption by the HCI community.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Velloso, Eduardo and Hornbæk, Kasper},
	year = {2025},
	keywords = {Causal modelling, directed acyclic graphs, HCI theory},
}

@inproceedings{campagna_genie_2019,
	address = {New York, NY, USA},
	series = {{PLDI} 2019},
	title = {Genie: a generator of natural language semantic parsers for virtual assistant commands},
	isbn = {978-1-4503-6712-7},
	url = {https://doi.org/10.1145/3314221.3314594},
	doi = {10.1145/3314221.3314594},
	abstract = {To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort. We advocate formalizing the capability of virtual assistants with a Virtual Assistant Programming Language (VAPL) and using a neural semantic parser to translate natural language into VAPL code. Genie needs only a small realistic set of input sentences for validating the neural model. Developers write templates to synthesize data; Genie uses crowdsourced paraphrases and data augmentation, along with the synthesized data, to train a semantic parser. We also propose design principles that make VAPL languages amenable to natural language translation. We apply these principles to revise ThingTalk, the language used by the Almond virtual assistant. We use Genie to build the first semantic parser that can support compound virtual assistants commands with unquoted free-form parameters. Genie achieves a 62\% accuracy on realistic user inputs. We demonstrate Genie’s generality by showing a 19\% and 31\% improvement over the previous state of the art on a music skill, aggregate functions, and access control.},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad and Socher, Richard and Lam, Monica S.},
	year = {2019},
	note = {event-place: Phoenix, AZ, USA},
	keywords = {data engineering, semantic parsing, data augmentation, training data generation, virtual assistants},
	pages = {394--410},
}

@inproceedings{castro_using_2021,
	address = {New York, NY, USA},
	series = {{IHC} '21},
	title = {Using {Ontologies} to aid {Knowledge} {Sharing} in {HCI} {Design}},
	isbn = {978-1-4503-8617-3},
	url = {https://doi.org/10.1145/3472301.3484327},
	doi = {10.1145/3472301.3484327},
	abstract = {Developing interactive systems is a challenging task that involves concerns related to the human-computer interaction (HCI), such as usability and user experience. Therefore, HCI design is a core issue when developing such systems. It often involves people with different backgrounds (e.g., Arts, Software Engineering, Design), which makes knowledge transfer a challenging issue. Ontologies have been acknowledged as a successful approach to represent domain knowledge and support knowledge-based solutions. Hence, in this work, we propose to explore ontologies to represent structured knowledge and improve knowledge sharing in HCI design. We briefly present the Human-Computer Interaction Design Ontology (HCIDO), a reference ontology that addresses HCI design aspects that connect HCI and Software Engineering concerns. By making knowledge related to the HCI design domain explicit and structured, HCIDO has helped us to develop KTID, a tool that aims to support capturing and sharing useful knowledge to aid in HCI design. Preliminary results indicate that the tool may be particularly useful for novice HCI designers.},
	booktitle = {Proceedings of the {XX} {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Castro, Murillo V. H. B. and Barcellos, Monalessa P. and de A. Falbo, Ricardo and Costa, Simone D.},
	year = {2021},
	note = {event-place: Virtual Event, Brazil},
	keywords = {Ontology, Knowledge, HCI Design},
}

@inproceedings{shaked_modelling_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Modelling {Tool} {Extension} for {Vulnerability} {Management}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3687791},
	doi = {10.1145/3652620.3687791},
	abstract = {Managing vulnerabilities with respect to the design of systems is essential to securing systems and establishing their trustworthiness. Until now, there has been no modelling tool to support vulnerability management within the context of system design. We present a new, open-source extension of a systems security design and assessment tool. First and foremost, this extension integrates a pertinent vulnerability management domain ontology into the tool's underlying metamodel. Based on the extended metamodel, the enriched tool supports importing information from vulnerability-related knowledge bases as well as capturing new vulnerability information and security rules. This information can then be used in an integrative and scalable form to analyse and reason about the security of systems designs. The extended tool now includes an automated reasoning mechanism for establishing the vulnerability posture of systems designs.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Shaked, Avi and Messe, Nan and Melham, Tom},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {security by design, model driven engineering, threat modelling, vulnerability management},
	pages = {56--60},
}

@inproceedings{suleimenova_integration_2019,
	address = {New York, NY, USA},
	series = {{ICEMIS} '19},
	title = {Integration data models based on ontology},
	isbn = {978-1-4503-7212-1},
	url = {https://doi.org/10.1145/3330431.3330438},
	doi = {10.1145/3330431.3330438},
	abstract = {Evaluation of the performance of university staff can be carried out with different goals, consistent with the strategic goals of the institution. To provide an appropriate data set, an appropriate structure must be provided that ensures that neither incomplete nor reliable data will be used. To provide such a structure with the best possible features data from the various available sources should be integrated. The article provides an overview of the existing integration problems and approaches to solving this problem.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Engineering} and {MIS}},
	publisher = {Association for Computing Machinery},
	author = {Suleimenova, Laura and Zhomartkyzy, Gulnaz and Kumargazhanova, Saule},
	year = {2019},
	note = {event-place: Astana, Kazakhstan},
	keywords = {ontology, data integration, monitoring, information system, ontological model},
}

@inproceedings{barron_topic_2025,
	address = {New York, NY, USA},
	series = {{DocEng} '25},
	title = {Topic {Modeling} and {Link}-{Prediction} for {Material} {Property} {Discovery}},
	isbn = {979-8-4007-1351-4},
	url = {https://doi.org/10.1145/3704268.3748680},
	doi = {10.1145/3704268.3748680},
	abstract = {Link prediction is a key network analysis technique that infers missing or future relations between nodes in a graph, based on observed patterns of connectivity. Scientific literature networks and knowledge graphs are typically large, sparse, and noisy, and often contain missing links, potential but unobserved connections, between concepts, entities, or methods. Here, we present an AI-driven hierarchical link prediction framework that integrates matrix factorization to infer hidden associations and steer discovery in complex material domains. Our method combines Hierarchical Nonnegative Matrix Factorization (HNMFk), Boolean matrix factorization (BNMFk) with automatic model selection. These discrete factors are then fused with Logistic matrix factorization (LMF), we use to construct a three-level topic tree from a 46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs). This class of materials has been studied in a variety of physics fields and has a multitude of current and potential applications.An ensemble BNMFk + LMF approach fuses discrete interpretability with probabilistic scoring. The resulting HNMFk clusters map each material onto coherent research themes, such as superconductivity, energy storage, and tribology, and highlight missing or weakly connected links between topics and materials, suggesting novel hypotheses for cross-disciplinary exploration. We validate our method by removing publications about superconductivity in well-known superconductors, and demonstrate that the model correctly predicts their association with the superconducting TMD clusters. This highlights the ability of the method to find hidden connections in a graph of material to latent topic associations built from scientific literature. This is especially useful when examining a diverse corpus of scientific documents covering the same class of phenomena or materials but originating from distinct communities and perspectives. The inferred links generating new hypotheses, produced by our method, are exposed through an interactive Streamlit dashboard, designed for scientific discovery.},
	booktitle = {Proceedings of the 2025 {ACM} {Symposium} on {Document} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Barron, Ryan C. and Eren, Maksim E. and Stanev, Valentin and Matuszek, Cynthia and Alexandrov, Boian S.},
	year = {2025},
	note = {event-place: Nottingham, United Kingdom},
	keywords = {Link Prediction, NMF, LMF, Matrix completion, NMFk},
}

@inproceedings{sima_bio-soda_2021,
	address = {New York, NY, USA},
	series = {{SSDBM} '21},
	title = {Bio-{SODA}: {Enabling} {Natural} {Language} {Question} {Answering} over {Knowledge} {Graphs} without {Training} {Data}},
	isbn = {978-1-4503-8413-1},
	url = {https://doi.org/10.1145/3468791.3469119},
	doi = {10.1145/3468791.3469119},
	abstract = {The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20\% and by an even higher factor on more complex bioinformatics datasets.},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt},
	year = {2021},
	note = {event-place: Tampa, FL, USA},
	keywords = {Knowledge Graphs, Ranking, Question Answering},
	pages = {61--72},
}

@article{rost_reclaiming_2025,
	title = {Reclaiming the {Computer} through {LLM}-{Mediated} {Computing}},
	volume = {32},
	issn = {1072-5520},
	url = {https://doi.org/10.1145/3747585},
	doi = {10.1145/3747585},
	number = {5},
	journal = {Interactions},
	author = {Rost, Mattias},
	month = aug,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {26--31},
}

@inproceedings{ye_ontology-enhanced_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Ontology-enhanced {Prompt}-tuning for {Few}-shot {Learning}},
	isbn = {978-1-4503-9096-5},
	url = {https://doi.org/10.1145/3485447.3511921},
	doi = {10.1145/3485447.3511921},
	abstract = {Few-shot Learning (FSL) is aimed to make predictions based on a limited number of samples. Structured data such as knowledge graphs and ontology libraries has been leveraged to benefit the few-shot setting in various tasks. However, the priors adopted by the existing methods suffer from challenging knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder the performance for few-shot learning. In this study, we explore knowledge injection for FSL with pre-trained language models and propose ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the ontology transformation based on the external knowledge graph to address the knowledge missing issue, which fulfills and converts structure knowledge to text. We further introduce span-sensitive knowledge injection via a visible matrix to select informative knowledge to handle the knowledge noise issue. To bridge the gap between knowledge and text, we propose a collective training algorithm to optimize representations jointly. We evaluate our proposed OntoPrompt in three tasks, including relation extraction, event extraction, and knowledge graph completion, with eight datasets. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Ye, Hongbin and Zhang, Ningyu and Deng, Shumin and Chen, Xiang and Chen, Hui and Xiong, Feiyu and Chen, Xi and Chen, Huajun},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {Knowledge graphs, Knowledge graph, Ontology, Relation extraction, Few-shot Learning, Knowledge management, Knowledge graph completion, Performance, Event Extraction, Extraction, Few-shot learning, Relation Extraction, Prompt-tuning, Knowledge Graph Completion, Ontology's, Events extractions, Prediction-based, Number of samples},
	pages = {778--787},
	annote = {Cited by: 69; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{ferilli_knowledge_2025,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '25},
	title = {Knowledge {Graph}-based {User} {Models} and {Personalized} {Access} for {Cultural} {Heritage}},
	isbn = {979-8-4007-1399-6},
	url = {https://doi.org/10.1145/3708319.3733654},
	doi = {10.1145/3708319.3733654},
	abstract = {Cultural Heritage is opening up from the professional community to a wider public, generating an increasing demand for culture and an associated economic turnaround. This step requires to differentiate the behavior of Cultural Heritage systems, dealing with a wide variety of backgrounds, expectations, contexts, aims, educational and cultural level, preferences and interests. Computer Science and Artificial Intelligence can play a key role in this landscape, fine-tuning the fruition of cultural items to every kind of stakeholder and even to single users. In this paper we present an approach to personalization of Cultural Heritage fruition based on Knowledge Graphs. An approach to describe user models, and to use them for extracting personalized information, is proposed, and a platform that embeds this approach is described.},
	booktitle = {Adjunct {Proceedings} of the 33rd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Ferilli, Stefano},
	year = {2025},
	keywords = {Knowledge Graphs, Cultural Heritage, Personalized Information Access, User Models},
	pages = {437--441},
}

@inproceedings{dogga_system-wide_2019,
	address = {New York, NY, USA},
	series = {{SoCC} '19},
	title = {A {System}-{Wide} {Debugging} {Assistant} {Powered} by {Natural} {Language} {Processing}},
	isbn = {978-1-4503-6973-2},
	url = {https://doi.org/10.1145/3357223.3362701},
	doi = {10.1145/3357223.3362701},
	abstract = {Despite advances in debugging tools, systems debugging today remains largely manual. A developer typically follows an iterative and time-consuming process to move from a reported bug to a bug fix. This is because developers are still responsible for making sense of system-wide semantics, bridging together outputs and features from existing debugging tools, and extracting information from many diverse data sources (e.g., bug reports, source code, comments, documentation, and execution traces). We believe that the latest statistical natural language processing (NLP) techniques can help automatically analyze these data sources and significantly improve the systems debugging experience. We present early results to highlight the promise of NLP-powered debugging, and discuss systems and learning challenges that must be overcome to realize this vision.},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Dogga, Pradeep and Narasimhan, Karthik and Sivaraman, Anirudh and Netravali, Ravi},
	year = {2019},
	note = {event-place: Santa Cruz, CA, USA},
	keywords = {natural language processing, systems debugging},
	pages = {171--177},
}

@inproceedings{calderwood_phraselette_2025,
	address = {New York, NY, USA},
	series = {{DIS} '25},
	title = {Phraselette: {A} {Poet}’s {Procedural} {Palette}},
	isbn = {979-8-4007-1485-6},
	url = {https://doi.org/10.1145/3715336.3735832},
	doi = {10.1145/3715336.3735832},
	abstract = {According to the recently introduced theory of artistic support tools, creativity support tools exert normative influences over artistic production, instantiating a normative ground that shapes both the process and product of artistic expression. We argue that the normative ground of most existing automated writing tools is misaligned with writerly values and identify a potential alternative frame—material writing support—for experimental poetry tools that flexibly support the finding, processing, transforming, and shaping of text(s). Based on this frame, we introduce Phraselette, an artistic material writing support interface that helps experimental poets search for words and phrases. To provide material writing support, Phraselette is designed to counter the dominant mode of automated writing tools, while offering language model affordances in line with writerly values. We further report on an extended expert evaluation involving 10 published poets that indicates support for both our framing of material writing support and for Phraselette itself.},
	booktitle = {Proceedings of the 2025 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Calderwood, Alex and Chung, John Joon Young and Sun, Yuqian and Roemmele, Melissa and Kreminski, Max},
	year = {2025},
	keywords = {Search, Language Models, CST, Artistic Support Tool, Creative Writing, Material, Poetry},
	pages = {2701--2717},
}

@inproceedings{li_pumice_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {{PUMICE}: {A} {Multi}-{Modal} {Agent} that {Learns} {Concepts} and {Conditionals} from {Natural} {Language} and {Demonstrations}},
	isbn = {978-1-4503-6816-2},
	url = {https://doi.org/10.1145/3332165.3347899},
	doi = {10.1145/3332165.3347899},
	abstract = {Natural language programming is a promising approach to enable end users to instruct new tasks for intelligent agents. However, our formative study found that end users would often use unclear, ambiguous or vague concepts when naturally instructing tasks in natural language, especially when specifying conditionals. Existing systems have limited support for letting the user teach agents new concepts or explaining unclear concepts. In this paper, we describe a new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations. Users can also define new procedures and concepts by demonstrating and referring to contents within GUIs of existing mobile apps. We demonstrate this approach in PUMICE, an end-user programmable agent that implements this approach. A lab study with 10 users showed its usability.},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Li, Toby Jia-Jun and Radensky, Marissa and Jia, Justin and Singarajah, Kirielle and Mitchell, Tom M. and Myers, Brad A.},
	year = {2019},
	note = {event-place: New Orleans, LA, USA},
	keywords = {end user development, multi-modal interaction, natural language programming, programming by demonstration},
	pages = {577--589},
}

@inproceedings{szekely_t2wml_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {{T2WML}: {Table} {To} {Wikidata} {Mapping} {Language}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364448},
	doi = {10.1145/3360901.3364448},
	abstract = {The web contains millions of useful spreadsheets and CSV files, but these files are difficult to use in applications because they use a wide variety of data layouts and terminology. We present Table To Wikidata Mapping Language (T2WML), a language that makes it easy to map and link arbitrary spreadsheets and CSV files to the Wikidata data model. The output of T2WML consists of Wikidata statements that can be loaded in the public Wikidata knowledge base or in a Wikidata clone repository, creating an augmented Wikidata knowledge graph that application developers can query using SPARQL.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Szekely, Pedro and Garijo, Daniel and Bhatia, Divij and Wu, Jiasheng and Yao, Yixiang and Pujara, Jay},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {knowledge graphs, rdf, entity linking, wikidata},
	pages = {267--270},
}

@inproceedings{sen_optimizing_2021,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '21},
	title = {Optimizing {Interpretation} {Generation} in {Natural} {Language} {Query} {Answering} for {Real} {Time} {End} {Users}},
	isbn = {978-1-4503-8817-7},
	url = {https://doi.org/10.1145/3430984.3431002},
	doi = {10.1145/3430984.3431002},
	abstract = {Natural Language Querying over Database is gaining popularity across different use cases. Most common of them is to democratize the process of data analysis and querying of backend data to naive end users especially business users, obviating the need of knowing back end query language. Natural Language Query answering systems have thus seen widespread usage in industry too where business users want to search their own data to make business decisions. However, a common challenge faced by any natural language query answering system is generation of precise interpretations. The research community although tries to handle the problem via asking clarification questions back to the user, in industry setup this remains an ineffective solution due to various practical usage limitations. For example, it is not fair to assume any end user will be aware of the correct option to answer these clarification questions. Moreover, involving clarification questions and user feedbacks makes the system unusable by one shot API calls, which is the most intuitive usage among common use cases in industry like automated report generation. In this paper, we investigate practical ways to address the problem of precise interpretation generation. We propose novel algorithms to make use of existing technologies like Functional Partitioning of Ontology and Lazy Inclusion to solve this problem. We take our previous state-of-the-art paper ATHENA and further extend it to include our proposed methods. We test with 3 benchmark ontologies to empirically demonstrate the huge improvement over state-of-the-art results by factors of at least 400\% in number of interpretation generation and also in the computation time.},
	booktitle = {Proceedings of the 3rd {ACM} {India} {Joint} {International} {Conference} on {Data} {Science} \&amp; {Management} of {Data} (8th {ACM} {IKDD} {CODS} \&amp; 26th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Sen, Jaydeep and Saha, Diptikalyan and Mittal, Ashish and Sankaranarayanan, Karthik},
	year = {2021},
	note = {event-place: Bangalore, India},
	pages = {341--349},
}

@inproceedings{medinacelli_augmenting_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Augmenting model-based systems engineering with knowledge},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561548},
	doi = {10.1145/3550356.3561548},
	abstract = {This article presents a general approach for the integration of Knowledge Bases into Model-Based Systems Engineering tools. In existing tools, domain-specific modeling languages are well supported. However when it comes to enforcing design constraints, existing approaches are verbose, it is difficult to be complete and consistent, and the reuse of knowledge is only possible in a limited way (mainly through model libraries). Furthermore, current tools usually lack or have limited capability to detect semantic errors, ability to evaluate the models with respect to formal expert knowledge, and the ability to understand what is being designed. Our work addresses these limitations through the semantic annotation of UML models in Papyrus (an MBSE Tool), to attach domain-specific semantics to the models. This integration enables not only reasoning capabilities over the annotated models, but the models can be shared with semantic-compatible tools and stakeholders. Moreover, the models can reuse and integrate knowledge generated outside the tooling environment. The approach's feasibility is demonstrated through an implementation that defines a technology stack, with emphasis on the mapping of UML elements and its counterparts in the ontology. We address the coherence and preservation of the semantics throughout the transformation process, which enable the formalization of constraints coming from the UML's system design. Finally, we illustrate the reasoning capabilities by evaluating expert knowledge via SPARQL queries and SWRL rules.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Medinacelli, Luis Palacios and Noyrit, Florian and Mraidha, Chokri},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {ontology, semantic interoperability, UML, model-driven engineering, knowledge based engineering, Papyrus},
	pages = {351--358},
}

@inproceedings{krapp_quasi-social_2024,
	address = {New York, NY, USA},
	series = {{NordiCHI} '24},
	title = {In a {Quasi}-{Social} {Relationship} {With} {ChatGPT}. {An} {Autoethnography} on {Engaging} {With} {Prompt}-{Engineered} {LLM} {Personas}},
	isbn = {979-8-4007-0966-1},
	url = {https://doi.org/10.1145/3679318.3685501},
	doi = {10.1145/3679318.3685501},
	abstract = {As conversational AI like ChatGPT becomes more sophisticated, understanding emerging quasi-social relationships with it is crucial. Through analytical autoethnography, we explore the nuances of these relationships with two autobiographically designed ChatGPT personas to augment the social needs of the first author: the Endless Enthusiast (always responding positively and encouragingly) and the Socratic Tutor (asking questions to stimulate critical thinking). After six weeks of interaction, we find that for a successful relationship, the non-human counterpart must be authentic about its machine nature and limitations. Using deception to appear more human-like makes the relationship fail. We thus suggest designing machine relationships as complementary to human-human relationships. For authentic interactions, humans should be in control, with the machine authentically assuming a role that "naturally" fits machines. Here, the unique qualities of machines in social interactions offer promising starting points for designing such roles.},
	booktitle = {Proceedings of the 13th {Nordic} {Conference} on {Human}-{Computer} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Krapp, Eva and Neuhaus, Robin and Hassenzahl, Marc and Laschke, Matthias},
	year = {2024},
	note = {event-place: Uppsala, Sweden},
	keywords = {human-AI interaction, autobiographical research through design, autoethnography},
}

@inproceedings{mori_detecting_2024,
	address = {New York, NY, USA},
	series = {{AINTEC} '24},
	title = {Detecting {Inconsistency} between {Network} {Design} and {Current} {State} {Based} on {Network} {Ontology} {Bonsai}},
	isbn = {979-8-4007-0985-2},
	url = {https://doi.org/10.1145/3674213.3674222},
	doi = {10.1145/3674213.3674222},
	abstract = {Generally, a network administrator designs, constructs, and operates an enterprise network. Since inconsistency between the network design understood by the administrator and the actual network configuration might arise due to mistakes or errors, a method for automatically detecting such inconsistency is needed. The following four techniques are necessary for this purpose. (i) A machine-readable notation to represent the network configuration. (ii) A tool to write down the network design using the machine-readable notation. (iii) A tool to automatically detect the current network configuration and write it down in the machine-readable notation. (iv) A tool to compare the two outputs generated in (ii) and (iii). This paper employs the network ontology called Bonsai for (i). Bonsai can represent not only physical configurations but also virtualization technologies such as VLAN and overlay. This paper proposes three tools, nc-design, nc-detect, and nc-diff for (ii)-(iv), and confirms that they work as expected in the test network. This paper also measures their fundamental performance.},
	booktitle = {Proceedings of the {Asian} {Internet} {Engineering} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Mori, Kosuke and Kondo, Takao and Teraoka, Fumio},
	year = {2024},
	note = {event-place: Sydney, NSW, Australia},
	keywords = {network configuration detection, network management, network ontology},
	pages = {76--84},
}

@article{wang_resonance_2025,
	title = {Resonance+: {Operationalizing} {Protective} {Action} {Decision} {Model} for {Finding} {Information} {Useful} for {Public} {Information} {Officers}},
	volume = {8},
	url = {https://doi.org/10.1145/3708492},
	doi = {10.1145/3708492},
	abstract = {Microblogging platforms have been increasingly used by the public in crisis situations, enabling more participatory crisis communication between the official response channels and the affected community. However, the sheer volume of crisis-related messages on social media can make it challenging for officials to find pertinent information and understand the public’s perception of evolving risks. To address this issue, crisis informatics researchers have proposed a variety of technological solutions, but there has been limited examination of the cognitive and perceptual processes and subsequent responses of the affected population. Yet, this information is critical for the crisis response officials to gauge public’s understanding of the event, their perception of event-related risk, and perception of incident response and recovery efforts, in turn enabling the officials to craft crisis communication messaging more effectively. Taking cues from the Protective Action Decision Model, we conceptualize a metric (resonance+) that prioritizes the cognitive and perceptual processes of the affected population, quantifying shifts in collective attention and information exposure for each tweet. Based on resonance+, we develop a principled, scalable pipeline that recommends content relating to people’s cognitive and perceptual processes. Our results suggest that resonance+ is generalizable across different types of natural hazards. We have also demonstrated its applicability for near-real-time scenarios. According to the feedback from the target users, the local public information officers in emergency management, the messages recommended by our pipeline are useful in their tasks of understanding public perception and finding hopeful narratives, potentially leading to more effective crisis communications.},
	number = {3–4},
	journal = {Trans. Soc. Comput.},
	author = {Wang, Di and Kogan, Marina},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {word embedding, crisis communication, Crisis informatics, protective action decision model, social media data},
}

@inproceedings{hwang_fsbrick_2023,
	address = {New York, NY, USA},
	series = {{BuildSys} '23},
	title = {{FSBrick}: {An} information model for representing fault-symptom relationships in {HVAC} systems},
	isbn = {979-8-4007-0230-3},
	url = {https://doi.org/10.1145/3600100.3623729},
	doi = {10.1145/3600100.3623729},
	abstract = {Current fault diagnosis (FD) methods for Heating, Ventilation, and Air Conditioning (HVAC) systems do not accommodate for system reconfigurations throughout the systems’ operational lifetime. However, system reconfiguration can change the causal relationship between faults and symptoms, which leads to a drop in FD accuracy. In this paper, we present Fault-Symptom Brick (FSBrick), an extension to the Brick metadata schema intended to represent information necessary to propagate system configuration changes onto FD algorithms, and ultimately revise FSRs explicitly or implicitly. We motivate the need to represent FSRs by illustrating their changes when the system reconfigures. Then, we survey existing efforts to represent FSRs within the HVAC sector and adjacent fields, and choose to extend Brick. We introduce the FSBrick architecture and discuss which extensions are added to represent FSRs. To evaluate the coverage of FSBrick, we implement FSBrick on (i) the motivational case study scenario, (ii) Building Automation Systems’ representation of FSRs from 3 HVACs, and (iii) FSRs from 7 FD method papers, and find that FSBrick can represent 88.2\% of faults, 87.7\% of fault severities, and 72.5\% of symptoms. The analyses show that both Brick and FSBrick should be expanded further to cover HVAC component and property information, and mathematical and logical statements used to formulate FSRs in real life and literature. As there is currently no generic and extensible information model to represent FSRs in commercial buildings, FSBrick paves the way to future extensions that would aid the automated revision of FD algorithms upon system reconfiguration.},
	booktitle = {Proceedings of the 10th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {Hwang, Min Young and Akinci, Burcu and Berges, Mario},
	year = {2023},
	note = {event-place: Istanbul, Turkey},
	keywords = {ontology, fault diagnosis, causal relationship, fault-symptom relationships, semantic information modeling},
	pages = {69--78},
}

@article{trentin_between_2023,
	title = {Between {Written} and {Visual} {Communication}: {CIDOC} {CRM} {Ontology} for {Medieval} and {Early} {Modern} {European} {Graffiti}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3589230},
	doi = {10.1145/3589230},
	abstract = {The development of graffiti studies during the last couple of decades highlighted the relevance and potential of graffiti as a complementary source for understanding different aspects of past societies. Moreover, the availability of digital documentation techniques crucially increased data production, showing the widespread presence of graffiti in Medieval and Early Modern contexts across Europe.However, the approach to historical graffiti has not been yet structured. Guidelines, specific analytical tools, and descriptors are still missing due to various reasons. First, graffiti are a multiform and multimodal graphic expression, so texts, signs, and images must be considered together despite their different communicative nature. Secondly, due to their variety in forms and contents, graffiti have been studied from many perspectives (e.g., epigraphy, palaeography, history, art history, maritime studies), following the specific interests of each scholar. Consequently, the numerous and extensive contributions concerning graffiti highlight the lack of shared standards and approaches, hindering data analysis and interoperability. The panorama emerging is fragmentary and unstructured.This article thus aims to offer a first step toward the development of a specific methodology for the analysis and study of Medieval and Early Modern European graffiti. Precisely, a specific ontology adopting CIDOC CRM for Medieval and Early Modern graffiti will be presented, as developed in a preliminary form within the DIGIGRAF project1 with the support of the ARIADNEplus2 network.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Trentin, Mia and Felicetti, Achille},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {CIDOC CRM, epigraphy, Medieval Graffiti},
}

@article{gray_languaging_2024,
	title = {Languaging {Ethics} in {Technology} {Practice}},
	volume = {1},
	url = {https://doi.org/10.1145/3656468},
	doi = {10.1145/3656468},
	abstract = {Ethics as embodied by technology practitioners resists simple definition—particularly as it relates to the interplay of identity, organizational, and professional complexity. In this article, we use the linguistic notion of languaging as an analytic lens to describe how technology and design practitioners negotiate their conception of ethics as they reflect upon their everyday work. We engaged 12 practitioners in individual co-creation workshops, encouraging them to reflect on their ethical role in their everyday work through a series of generative and evaluative activities. We analyzed these data to identify how each practitioner reasoned about ethics through language and artifacts, finding that practitioners used a range of rhetorical tropes to describe their ethical commitments and beliefs in ways that were complex and sometimes contradictory. Across three cases, we describe how ethics was negotiated through language across three key zones of ecological emergence: the practitioners’ “core” beliefs about ethics, internal and external ecological elements that shaped or mediated these core beliefs, and the ultimate boundaries they reported refusing to cross. Building on these findings, we describe how the languaging of ethics reveals opportunities to definitionally and practically engage with ethics in technology ethics research, practice, and education.},
	number = {2},
	journal = {ACM J. Responsib. Comput.},
	author = {Gray, Colin M. and Chivukula, Shruthi Sai and Johns, Janna and Will, Matthew and Obi, Ike and Li, Ziqing},
	month = jun,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ethics, co-creation, languaging, rhetoric, technology practice},
}

@inproceedings{zhou_knowledge_2024,
	address = {New York, NY, USA},
	series = {{ICBDT} '24},
	title = {A {Knowledge} {Graph} {Construction} {System} {Based} on {Digital} {Elevation} {Model}},
	isbn = {979-8-4007-1751-2},
	url = {https://doi.org/10.1145/3698300.3698319},
	doi = {10.1145/3698300.3698319},
	abstract = {In recent years, Integration of knowledge graph technology into various domains has become a prominent trend, fostering the emergence of domain-specific knowledge graph tailored to particular fields. Digital Elevation Model (DEM), as fundamental data for terrain analysis, has widespread applications in surveying, hydrology, geomorphology, and Geographic Information Systems (GIS). However, research on constructing knowledge graph based on DEM remains relatively limited. This study presents a system for constructing knowledge graph based on DEM. In this system, valley and ridge lines are delineated and organized into slope units with distinct boundaries and semantic clarity, refined from DEM. These slope units serve as entities, and their spatial relationships form the connections within the knowledge graph. This system features a user-friendly interface, enabling users to intuitively view terrain data and the knowledge graph. Additionally, it supports natural language Question and Answer (Q\&amp;A) functionality, allowing users to query terrain information, explore relationships between slope units, and calculate the shortest paths. By providing new perspectives and tools, this system advances the research and application of terrain data. Continuous improvement and expansion are expected to enhance terrain analysis technology and create innovative opportunities in related disciplines.},
	booktitle = {Proceedings of the 2024 7th {International} {Conference} on {Big} {Data} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Bo and Chen, Ji and Wu, Shuo},
	year = {2024},
	keywords = {A, Interactive Q\&amp, ridge lines, Slope units, valley lines},
	pages = {45--50},
}

@article{zhu_prior_2020,
	title = {Prior {Knowledge} {Driven} {Label} {Embedding} for {Slot} {Filling} in {Natural} {Language} {Understanding}},
	volume = {28},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2020.2980152},
	doi = {10.1109/TASLP.2020.2980152},
	abstract = {Traditional slot filling in natural language understanding (NLU) predicts a one-hot vector for each word. This form of label representation lacks semantic correlation modeling, which leads to severe data sparsity problem, especially when adapting an NLU model to a new domain. To address this issue, a novel label embedding based slot filling framework is proposed in this article. Here, distributed label embedding is constructed for each slot using prior knowledge. Three encoding methods are investigated to incorporate different kinds of prior knowledge about slots: atomic concepts, slot descriptions, and slot exemplars. The proposed label embeddings tend to share text patterns and reuses data with different slot labels. This makes it useful for adaptive NLU with limited data. Also, since label embedding is independent of NLU model, it is compatible with almost all deep learning based slot filling models. The proposed approaches are evaluated on three datasets. Experiments on single domain and domain adaptation tasks show that label embedding achieves significant performance improvement over traditional one-hot label representation as well as advanced zero-shot approaches.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Zhu, Su and Zhao, Zijian and Ma, Rao and Yu, Kai},
	month = may,
	year = {2020},
	note = {Publisher: IEEE Press},
	pages = {1440--1451},
}

@inproceedings{mobramaein_methodology_2019,
	address = {New York, NY, USA},
	series = {{FDG} '19},
	title = {A methodology for designing natural language interfaces for procedural content generation},
	isbn = {978-1-4503-7217-6},
	url = {https://doi.org/10.1145/3337722.3341860},
	doi = {10.1145/3337722.3341860},
	abstract = {Procedural Content Generation (PCG) uses algorithmic techniques to create a wide variety of content for games. These generators often have a large number of parameters, making it difficult for non-technical designers to explore the design space of generated artifacts. Natural language interfaces for generators can map natural language keywords to parameter space changes spanning multiple simultaneous parameters and afford use of expressive language. This way, designers can navigate to interesting points in the design space of a generator by describing desired properties of the artifact using a series of natural language descriptors. We present a design methodology that designers can use to develop natural language interfaces for procedural content generation systems. This design methodology begins by defining a design vocabulary that can describe the output of a generator, mapping the vocabulary to a series of parameters, and translating natural language queries to movements in the generator's design space. We further address issues around designer intent understanding, design space exploration and workflows using natural language interfaces in PCG. An example and implementation of our methodology is provided demonstrating its application to existing plug-ins for content creation in the Unity3D engine},
	booktitle = {Proceedings of the 14th {International} {Conference} on the {Foundations} of {Digital} {Games}},
	publisher = {Association for Computing Machinery},
	author = {Mobramaein, Afshin and Whitehead, Jim},
	year = {2019},
	note = {event-place: San Luis Obispo, California, USA},
	keywords = {design methodology, procedural content generation, natural language interfaces},
}

@inproceedings{hosseini_semantic_2018,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2018},
	title = {Semantic inference from natural language privacy policies and {Android} code},
	isbn = {978-1-4503-5573-5},
	url = {https://doi.org/10.1145/3236024.3275427},
	doi = {10.1145/3236024.3275427},
	abstract = {Mobile apps collect dierent categories of personal information to provide users with various services. Companies use privacy policies containing critical requirements to inform users about their data practices. With the growing access to personal information and the scale of mobile app deployment, traceability of links between privacy policy requirements and app code is increasingly important. Automated traceability can be achieved using natural language processing and code analysis techniques. However, such techniques must address two main challenges: ambiguity in privacy policy terminology and unbounded information types provided by users through input elds in GUI. In this work, we propose approaches to interpret abstract terms in privacy policies, identify information types in Android layout code, and create a mapping between them using natural language processing techniques.},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Hosseini, Mitra Bokaei},
	year = {2018},
	note = {event-place: Lake Buena Vista, FL, USA},
	keywords = {Traceability, Privacy, Requirements Engineering, natural Language Processing},
	pages = {940--943},
}

@inproceedings{xie_beyond_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Beyond {Visual} {Perception}: {Insights} from {Smartphone} {Interaction} of {Visually} {Impaired} {Users} with {Large} {Multimodal} {Models}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3714210},
	doi = {10.1145/3706598.3714210},
	abstract = {Large multimodal models (LMMs) have enabled new AI-powered applications that help people with visual impairments (PVI) receive natural language descriptions of their surroundings through audible text. We investigated how this emerging paradigm of visual assistance transforms how PVI perform and manage their daily tasks. Moving beyond usability assessments, we examined both the capabilities and limitations of LMM-based tools in personal and social contexts, while exploring design implications for their future development. Through interviews with 14 visually impaired users of Be My AI (an LMM-based application) and analysis of its image descriptions from both study participants and social media platforms, we identified two key limitations. First, these systems’ context awareness suffers from hallucinations and misinterpretations of social contexts, styles, and human identities. Second, their intent-oriented capabilities often fail to grasp and act on users’ intentions. Based on these findings, we propose design strategies for improving both human-AI and AI-AI interactions, contributing to the development of more effective, interactive, and personalized assistive technologies.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Xie, Jingyi and Yu, Rui and Zhang, He and Billah, Syed Masum and Lee, Sooyeon and Carroll, John M.},
	year = {2025},
	keywords = {Human-AI interaction, Be My AI., Be My Eyes, large multimodal models (LMMs), People with visual impairments (PVI), remote sighted assistance (RSA), visual question answering (VQA)},
}

@inproceedings{basman_critique_2018,
	address = {New York, NY, USA},
	series = {Programming '18},
	title = {Critique of ‘{Semprola}: a semiotic programming language’},
	isbn = {978-1-4503-5513-1},
	url = {https://doi.org/10.1145/3191697.3214331},
	doi = {10.1145/3191697.3214331},
	abstract = {We supply a critique of the paper Semprola: A Semiotic Programming Language, suggesting directions in which its work of bringing semiotics to programming can be refined, and supplying opinions on areas where it may be refounded.},
	booktitle = {Companion {Proceedings} of the 2nd {International} {Conference} on the {Art}, {Science}, and {Engineering} of {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Basman, Antranig},
	year = {2018},
	note = {event-place: Nice, France},
	keywords = {Ontologies, Semiotics, Cognitive Ergonomics, Information Architecture, Subjective Programming},
	pages = {214--217},
}

@inproceedings{he_ontology_2023,
	address = {New York, NY, USA},
	series = {{BuildSys} '23},
	title = {Ontology {Integration} for {Building} {Systems} and {Energy} {Storage} {Systems}},
	isbn = {979-8-4007-0230-3},
	url = {https://doi.org/10.1145/3600100.3623720},
	doi = {10.1145/3600100.3623720},
	abstract = {A building ontology defines the concepts and organization of building data. Such knowledge can be assistance with automatic data access and support data-driven applications in buildings. With technological advances in batteries and energy storage, an increasing number of data-driven building applications now involve both building systems and energy storage systems (ESS), e.g., peak load shaving (PLS). However, existing building ontologies, e.g., Brick, are not designed to include concepts from ESS systems. Given the emergence of building-ESS applications, it has become important to develop ontologies that can cover knowledge about both building and ESS systems. Building systems and ESS systems fall under different industry sectors and there are building ontologies and ESS ontologies that have been developed independently. To maximally reuse existing knowledge, we leverage ontology integration technologies. We present a building-energy storage ontology integration (BESOI) system that can extend a building ontology with appropriate ESS ontologies. Our system handles ambiguity, incoherence, and redundancy problems in ontology integration. We evaluate BESOI on four building-ESS applications by extending Brick, a notable building ontology, with different ESS ontologies. The results show that BESOI can extend the coverage of Brick from 68.09\% to 95.74\% on the concepts of applications.},
	booktitle = {Proceedings of the 10th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {He, Fang and Wang, Dan and Sun, Yaojie},
	year = {2023},
	note = {event-place: Istanbul, Turkey},
	keywords = {Metadata, Building Application, Energy Storage System, Ontology Integration},
	pages = {212--215},
}

@inproceedings{peixoto_specifying_2018,
	address = {New York, NY, USA},
	series = {{SBES} '18},
	title = {Specifying privacy requirements with goal-oriented modeling languages},
	isbn = {978-1-4503-6503-1},
	url = {https://doi.org/10.1145/3266237.3266270},
	doi = {10.1145/3266237.3266270},
	abstract = {Context: Privacy of personal data is a growing concern regarding users of software systems. In this sense, the literature reports that in order to avoid privacy breaches, there must be systematic approaches to specify privacy requirements from the early activities of software development. Objective: Motivated by this situation, this paper presents a framework of privacy modeling capabilities that must be addressed by requirements modeling languages to better support privacy specification. The capabilities will be used to compare three goal-oriented modeling languages (i*, NFR-Framework and Secure-Tropos). Method: The framework was created with basis on a conceptual foundation and a conceptual model of privacy built from an analysis of a standard, a regulation, guidelines and other bibliographical sources related to privacy. A health care example is used to illustrate how the framework can be used to compare the chosen modeling languages. Results: Fourteen privacy modeling capabilities were defined in the framework and it was observed that the analyzed modeling languages do not fully support them. Conclusions: The proposed framework contributes towards the consolidation of a privacy conceptual foundation that can be used to evaluate modeling languages for privacy in Requirements Engineering. The comparison performed by using this framework indicates Secure-Tropos as the most complete language to model privacy among the analyzed goal-oriented modeling languages.},
	booktitle = {Proceedings of the {XXXII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Peixoto, Mariana Maia and Silva, Carla},
	year = {2018},
	note = {event-place: Sao Carlos, Brazil},
	keywords = {privacy, requirements engineering, goal-oriented languages, requirements modeling},
	pages = {112--121},
}

@inproceedings{zhao_highly-optimized_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Highly-{Optimized} {Forgetting} for {Creating} {Signature}-{Based} {Views} of {Ontologies}},
	isbn = {979-8-4007-0124-5},
	url = {https://doi.org/10.1145/3583780.3614771},
	doi = {10.1145/3583780.3614771},
	abstract = {Uniform interpolation (UI) is a non-standard reasoning service that seeks to project an ontology down to its sub-signature — given an ontology taking a certain signature, and a subset Σ of "relevant names” of that signature, compute a new ontology, called a uniform interpolant, that uses only the relevant names while preserving the semantics of the relevant names in the uniform interpolant. UI is of great potential importance since it may be used in a variety of applications where suitable views of ontologies need to be computed. However, this potential can only be fully realized if a highly optimized method for computing such views exists. Previous research has shown that computing uniform interpolants of ELH-ontologies is a computationally extremely hard problem — a finite uniform interpolant does not always exist for ELH, and if it exists, then there exists one of at most triple exponential size in terms of the original ontology, and that, in the worst case, no shorter interpolant exists. Despite the inherent difficulty of the problem, in this paper, we present a highly optimized forgetting method for computing uniform interpolants of ELH-ontologies, and show however that, with good reduction and inference strategies, such uniform interpolants can be efficiently computed. The method is an improvement of the one presented in our previous work. What sets it apart is its flexibility to treat concept names of different types differently, effectively cutting down on the inferences involved. This treatment is primarily driven by the polarities of the concept names within an ontology. A comprehensive evaluation with a prototypical implementation of the method shows \&gt;95\% average success rates over two popular benchmark datasets and demonstrates a clear computational advantage over state-of-the-art systems.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Yizheng},
	year = {2023},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {ontologies, forgetting, uniform interpolation, description logics},
	pages = {3444--3452},
}

@inproceedings{gomez-suta_semi-automatic_2020,
	address = {New York, NY, USA},
	series = {{WIMS} 2020},
	title = {Semi-automatic extraction and validation of concepts in ontology learning from texts in {Spanish}},
	isbn = {978-1-4503-7542-9},
	url = {https://doi.org/10.1145/3405962.3405977},
	doi = {10.1145/3405962.3405977},
	abstract = {The construction of ontologies from texts in Spanish is a challenge since this language lacks conceptual databases to validate abstract ontology structures as concepts and relations between them. The preceding generates the necessity of using manual evaluation by human experts; carrying high expenses that limit the calibration of algorithm parameters and large-scale evaluations. This document presents a proposal to evaluate abstract ontology structures through the task of semantic clustering of documents, without the expensive necessity of using manual evaluation or conceptual databases. The proposal is not only affordable but also applicable to model data and domains that lack structured knowledge resources. The experiments lead to the extraction and validation of the ontology structures from texts in Spanish regarding the domain of the Colombian armed conflict.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Web} {Intelligence}, {Mining} and {Semantics}},
	publisher = {Association for Computing Machinery},
	author = {Gómez-Suta, Manuela and Echeverry-Correa, Julián D. and Soto-Mejía, José A.},
	year = {2020},
	note = {event-place: Biarritz, France},
	keywords = {Ontology learning, evaluation, Spanish, concepts},
	pages = {7--16},
}

@article{berardinelli_model_2025,
	title = {Model {Driven} {Engineering}, {Artificial} {Intelligence}, and {DevOps} for {Software} and {Systems} {Engineering}: {A} {Systematic} {Mapping} {Study} of {Synergies} and {Challenges}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3759454},
	doi = {10.1145/3759454},
	abstract = {This paper presents a systematic mapping study classifying existing scientific contributions on synergies of Model Driven Engineering (MDE), Artificial Intelligence/Machine Learning (AI/ML), and DevOps, with the overall objective of supporting the continuous development of Cyber-Physical Systems (CPSs). We collected papers from bibliographic sources and selected primary studies to analyse. Then, we characterised and classified the current state of the art, focusing on 1) main aspects already tackled at the intersection of at least two of the three studied areas, and 2) findings emerging from the analysis as a framework for potential future research, notably regarding the integration of the three studied areas. The results reveal that few approaches combine MDE, AI/ML, and DevOps for software and systems engineering. In contrast, several approaches have combined two of them, specifically MDE and DevOps. Approaches combining AI/ML with MDE or DevOps are also becoming more frequent and will most likely continue to progress in the future. These synergies cover a range of engineering activities, from requirements and design to monitoring, maintenance, and evolution. Open research challenges include advancing AI/ML, MDE, and DevOps integration, supporting scalable, data-oriented solutions, proposing new continuous engineering methods, and adapting DevOps practices to diverse systems.},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Berardinelli, Luca and Muttillo, Vittoriano and Eramo, Romina and Bruneliere, Hugo and Rahimi, Abbas and Cicchetti, Antonio and Giner-Miguelez, Joan and Gómez, Abel and Potena, Pasqualina and Saadatmand, Mehrdad},
	month = aug,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Machine Learning, Artificial Intelligence, Internet of Things, Cloud Computing, Model-Driven Engineering, Cyber-Physical Systems, DevOps, Continuous Integration},
	annote = {Just Accepted},
}

@inproceedings{pernisch_massive_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {The {Massive} {Problem} of {Remote} {Changes} in {Ontology} {Reuse}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715478},
	doi = {10.1145/3701716.3715478},
	abstract = {Reusing existing datasets is a common practice in the Semantic Web, and it is also highly encouraged. Previous work on linking datasets has introduced and analysed different ways of linking but has failed to discuss the meaning and intentions behind the reuse of entities. This problem is aggravated by the fact Knowledge Graphs (KGs) and ontologies change over time. Currently, we lack an analysis of what impact the asymmetric evolution of the reused KGs has. Therefore, in this short paper, we evaluate how severe the problem of impacting remote changes is in practice by analysing the evolution of real-world ontologies. To this end, we collect a large corpus of open biomedical ontologies (759 ontologies) and provide statistics on their evolution, reuse (46.65\%) and impacting changes (33.38\%). We find that these KGs experience enormous amounts of impacting term reuse (7.59\%), and the extent of the problem has been overlooked on a massive scale.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Pernisch, Romana and Dobriy, Daniil and Polleres, Axel},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {kg evolution, kg evolution impact, kg reuse},
	pages = {1254--1258},
}

@article{caro-martinez_conceptual_2021,
	title = {Conceptual {Modeling} of {Explainable} {Recommender} {Systems}: {An} {Ontological} {Formalization} to {Guide} {Their} {Design} and {Development}},
	volume = {71},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.12789},
	doi = {10.1613/jair.1.12789},
	abstract = {With the increasing importance of e-commerce and the immense variety of products, users need help to decide which ones are the most interesting to them. This is one of the main goals of recommender systems. However, users’ trust may be compromised if they do not understand how or why the recommendation was achieved. Here, explanations are essential to improve user confidence in recommender systems and to make the recommendation useful. Providing explanation capabilities into recommender systems is not an easy task as their success depends on several aspects such as the explanation’s goal, the user’s expectation, the knowledge available, or the presentation method. Therefore, this work proposes a conceptual model to alleviate this problem by defining the requirements of explanations for recommender systems. Our goal is to provide a model that guides the development of effective explanations for recommender systems as they are correctly designed and suited to the user’s needs. Although earlier explanation taxonomies sustain this work, our model includes new concepts not considered in previous works. Moreover, we make a novel contribution regarding the formalization of this model as an ontology that can be integrated into the development of proper explanations for recommender systems.},
	journal = {J. Artif. Int. Res.},
	author = {Caro-Martínez, Marta and Jiménez-Díaz, Guillermo and Recio-García, Juan A.},
	month = sep,
	year = {2021},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
	keywords = {ontologies, knowledge representation},
	pages = {557--589},
}

@inproceedings{badenes-olmedo_scalable_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {Scalable {Cross}-lingual {Document} {Similarity} through {Language}-specific {Concept} {Hierarchies}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364444},
	doi = {10.1145/3360901.3364444},
	abstract = {With the ongoing growth in number of digital articles in a wider set of languages and the expanding use of different languages, we need annotation methods that enable browsing multi-lingual corpora. Multilingual probabilistic topic models have recently emerged as a group of semi-supervised machine learning models that can be used to perform thematic explorations on collections of texts in multiple languages. However, these approaches require theme-aligned training data to create a language-independent space. This constraint limits the amount of scenarios that this technique can offer solutions to train and makes it difficult to scale up to situations where a huge collection of multi-lingual documents are required during the training phase. This paper presents an unsupervised document similarity algorithm that does not require parallel or comparable corpora, or any other type of translation resource. The algorithm annotates topics automatically created from documents in a single language with cross-lingual labels and describes documents by hierarchies of multi-lingual concepts from independently-trained models. Experiments performed on the English, Spanish and French editions of JCR-Acquis corpora reveal promising results on classifying and sorting documents by similar content.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Badenes-Olmedo, Carlos and Redondo-García, José Luis and Corcho, Oscar},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {cross-lingual semantic similarity, large-scale text analysis, topic models},
	pages = {147--153},
}

@article{geeganage_semantics-enhanced_2024,
	title = {A {Semantics}-enhanced {Topic} {Modelling} {Technique}: {Semantic}-{LDA}},
	volume = {18},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3639409},
	doi = {10.1145/3639409},
	abstract = {Topic modelling is a beneficial technique used to discover latent topics in text collections. But to correctly understand the text content and generate a meaningful topic list, semantics are important. By ignoring semantics, that is, not attempting to grasp the meaning of the words, most of the existing topic modelling approaches can generate some meaningless topic words. Even existing semantic-based approaches usually interpret the meanings of words without considering the context and related words. In this article, we introduce a semantic-based topic model called semantic-LDA that captures the semantics of words in a text collection using concepts from an external ontology. A new method is introduced to identify and quantify the concept–word relationships based on matching words from the input text collection with concepts from an ontology without using pre-calculated values from the ontology that quantify the relationships between the words and concepts. These pre-calculated values may not reflect the actual relationships between words and concepts for the input collection, because they are derived from datasets used to build the ontology rather than from the input collection itself. Instead, quantifying the relationship based on the word distribution in the input collection is more realistic and beneficial in the semantic capture process. Furthermore, an ambiguity handling mechanism is introduced to interpret the unmatched words, that is, words for which there are no matching concepts in the ontology. Thus, this article makes a significant contribution by introducing a semantic-based topic model that calculates the word–concept relationships directly from the input text collection. The proposed semantic-based topic model and an enhanced version with the disambiguation mechanism were evaluated against a set of state-of-the-art systems, and our approaches outperformed the baseline systems in both topic quality and information filtering evaluations.},
	number = {4},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Geeganage, Dakshi Kapugama and Xu, Yue and Li, Yuefeng},
	month = feb,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {semantics, Topic modelling, disambiguation, concepts},
}

@inproceedings{tian_incorporating_2025,
	address = {Orlando, Florida, USA},
	series = {{WSC} '24},
	title = {Incorporating the {Com}-{B} {Model} for {Behavior} {Change} into an {Agent}-{Based} {Model} of {Smoking} {Behaviors}: {An} {Object}-{Oriented} {Design}},
	isbn = {979-8-3315-3420-2},
	abstract = {Modeling trajectories in cigarette smoking prevalence, initiation and quitting for populations and subgroups of populations is important for policy planning and evaluation. This paper proposes an agent-based model (ABM) design for simulating the smoking behaviors of a population using the Capability, Opportunity, Motivation - Behavior (COM-B) model. Capability, Opportunity and Motivation are modeled as latent composite attributes which are composed of observable factors associated with smoking behaviors. Three forms of the COM-B model are proposed to explain the transitions between smoking behaviors: initiating regular smoking uptake, making a quit attempt and quitting successfully. The ABM design follows object-oriented principles and extends an existing generic software architecture for mechanism-based modeling. The potential of the model to assess the impact of smoking policies is illustrated and discussed.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Tian, David and Squires, Hazel Y. and Buckley, Charlotte and Gillespie, Duncan and Tattan-Birch, Harry and Shahab, Lion and West, Robert and Brennan, Alan and Brown, Jamie and Purshouse, Robin C.},
	year = {2025},
	pages = {252--263},
}

@inproceedings{liang_speech-driven_2021,
	address = {New York, NY, USA},
	series = {{BIC} '21},
	title = {A {Speech}-{Driven} 3-{D} {Tongue} {Model} with {Realistic} {Movement} in {Mandarin} {Chinese}},
	isbn = {978-1-4503-9000-2},
	url = {https://doi.org/10.1145/3448748.3448796},
	doi = {10.1145/3448748.3448796},
	abstract = {In this paper, a new speech driven 3-D geometric tongue model is constructed. The constructed 3-D tongue shape is controlled with control points on 2-D midsagittal tongue curve, and speech-driven inverse estimation based on the constructed model is evaluated by empirical data. X-Ray 2-D vocal tract motion videos are tagged for the midsagittal tongue motion, and static 3-D vocal tracts of 20 phonemes are collected with MRI for the realistic 3-D tongue shape. MFCC are calculated from the videos as acoustic features, and are then used in a LSTM-RNN to predict the control points movement of the tongue shape. Three geometrically intuitive control points are selected to represent and calculate the midsagittal line of the tongue through linear regression. Cross-sections on the central lines of the tongues, whose height, width and angle are then predicted from the midsagittal line, are reconstructed with geometric curves, and the shape of each cross-section are then placed on the midsagittal line to get the overall predicted moving grid of the 3-D tongue. In this 3-D tongue model, acoustic features and realistic tongue motion are mapped directly to preserve more realistic articulatory details, and the control points are intuitive for non-experts to control the model, and the geometric tongue shapes predicted are comparable with realistic tongue dynamics. Based on the proposed method, the speech-driven prediction is evaluated with the realistic data, which proved this proposed method feasible.},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Bioinformatics} and {Intelligent} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Liang, Changwei and Kong, Jiangping and Wu, Xiyu},
	year = {2021},
	note = {event-place: Harbin, China},
	keywords = {3-D tongue model, Mandarin Chinese, realistic dynamics, speech-driven},
	pages = {297--302},
}

@article{sharma_supervised_2023,
	title = {Supervised {Machine} {Learning} {Method} for {Ontology}-based {Financial} {Decisions} in the {Stock} {Market}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3554733},
	doi = {10.1145/3554733},
	abstract = {For changing semantics, ontological and information presentation, as well as computational linguistics for Asian social networks, are one of the most essential platforms for offering enhanced and real-time data mapping, as well as huge data access across diverse big data sources on the web architecture, information extraction mining, statistical modeling and data modeling, database control, and so on. The concept of opinion or sentiment analysis is often used to predict or classify the textual data, sentiment, affect, subjectivity, and other emotional states in online text. Recognizing the message's positive and negative thoughts or opinions by examining the author's goals will aid in a better understanding of the text's content in terms of the stock market. An intelligent ontology and knowledge Asian social network solution can improve the effectiveness of a company's decision making support procedures by deriving important information about users from a wide variety of web sources. However, ontology is concerned primarily with problem-solving knowledge discovery. The utilization of Internet-based modernizations welcomed a significant effect on the Indian stock exchange. News related to the stock market in the most recent decade plays a vital role for the brokers or users. This article focuses on predicting stock market news sentiments based on their polarity and textual information using the concept of ontological knowledge-based Convolution Neural Network (CNN) as a machine learning approach. Optimal features are essential for the sentiment classification model to predict the stock's textual reviews' exact sentiment. Therefore, the swarm-based Artificial Bee Colony (ABC) algorithm is utilized with the Lexicon feature extraction approach using a novel fitness function. The main motivation for combining ABC and CNN is to accelerate model training, which is why the suggested approach is effective in predicting emotions from stock news.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Sharma, Neha and Soni, Mukesh and Kumar, Sumit and Kumar, Rajeev and Deb, Nabamita and Shrivastava, Anurag},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {sentiment analysis, Artificial Bee Colony Algorithm (ABC), Convolution Neural Networks (CNN), Lexicon feature extraction, opinion mining, Stock market},
}

@inproceedings{mavrokapnidis_seeq_2023,
	address = {New York, NY, USA},
	series = {{BuildSys} '23},
	title = {{SeeQ}: {A} {Programming} {Model} for {Portable} {Data}-{Driven} {Building} {Applications}},
	isbn = {979-8-4007-0230-3},
	url = {https://doi.org/10.1145/3600100.3623744},
	doi = {10.1145/3600100.3623744},
	abstract = {This paper introduces SeeQ, a programming model and an abstraction framework that facilitates the development of portable data-driven building applications. Data-driven approaches can provide insights into building operations and guide decision-making to achieve operational objectives. Yet the configuration of such applications per building requires extensive effort and tacit knowledge. In SeeQ, we propose a portable programming model and build a software system that enables self-configuration and execution across diverse buildings. The configuration of each building is captured in a unified data model — in this paper, we work with the Brick ontology without loss of generality. SeeQ focuses on the distinction between the application logic and the configuration of an application against building-specific data inputs and systems. We test the proposed approach by configuring and deploying a diverse range of applications across five heterogeneous real-world buildings. The analysis shows the potential of SeeQ to significantly reduce the efforts associated with the delivery of building analytics.},
	booktitle = {Proceedings of the 10th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {Mavrokapnidis, Dimitris and Fierro, Gabe and Husmann, Maria and Korolija, Ivan and Rovas, Dimitrios},
	year = {2023},
	note = {event-place: Istanbul, Turkey},
	keywords = {Ontologies, Semantic Web, RDF, Brick, Portability, SHACL, Metadata, Scalability, Programming, Analytics},
	pages = {159--168},
}

@inproceedings{cardoso_ontology_2024,
	address = {New York, NY, USA},
	series = {{IHC} '23},
	title = {Ontology {Visualization} in {BioPortal}: {Methodological} {Triangulation} for {Analyzing} {Accessibility}, {Communicability}, and {Usability}},
	isbn = {979-8-4007-1715-4},
	url = {https://doi.org/10.1145/3638067.3638088},
	doi = {10.1145/3638067.3638088},
	abstract = {Visualizing biomedical ontologies is an ongoing and highly relevant challenge in dealing with the vast volume of daily data and the miscellaneous use cases of ontologies. BioPortal is the most well-known tool for visualizing biomedical ontologies, bringing together hundreds of ontologies of varying sizes, depths, and complexities. This article aims to establish a methodological triangulation using SIM-SR based on Semiotic Engineering, the WCAG guidelines from the W3C consortium, and ergonomic principles to evaluate the experience of users who utilize screen readers and users who do not use this feature without conducting user experiments. The objective is to investigate and intersect the usability, communicability, and accessibility of BioPortal. Additionally, it seeks to understand how ontologies can be analyzed by as many people as possible, ensuring they are comprehensible and enhancing users’ cognitive understanding of the explored domain.},
	booktitle = {Proceedings of the {XXII} {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cardoso, Daiane de Ascenção and Henriques, Felipe da Rocha and Belloze, Kele Teixeira},
	year = {2024},
	note = {event-place: Maceió, Brazil},
	keywords = {Ontology, Usability, Communicability, Digital Acessibility, Evaluation Methods, Semiotic Engineer},
}

@inproceedings{danenas_enhancing_2019,
	address = {New York, NY, USA},
	series = {{PCI} '19},
	title = {Enhancing the extraction of {SBVR} business vocabularies and business rules from {UML} use case diagrams with natural language processing},
	isbn = {978-1-4503-7292-3},
	url = {https://doi.org/10.1145/3368640.3368641},
	doi = {10.1145/3368640.3368641},
	abstract = {Being among the best-selling and most advanced features of model-driven development, model-to-model transformation could help improving one of the most time- and resource-consuming efforts in the process of model-driven information systems engineering, namely, discovery and specification of business vocabularies and business rules within the problem domain. Nonetheless, despite the relatively high levels of automation throughout the whole systems' model-driven development process, business modeling stage remains among the most under re-searched areas throughout the whole process. In this paper, we introduce a novel natural language processing (NLP) technique to one of our latest developments for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams. This development remains arguably the most comprehensive development of this kind currently available in public. The experiment provided proof that the developed NLP enhancement delivered even better extraction results compared to the already satisfactory performance of the previous development. This work contributes to the research in the areas of model transformations and NLP within the model-driven development of information systems, and beyond.},
	booktitle = {Proceedings of the 23rd {Pan}-{Hellenic} {Conference} on {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Danenas, Paulius and Skersys, Tomas and Butleris, Rimantas},
	year = {2019},
	note = {event-place: Nicosia, Cyprus},
	keywords = {natural language processing, UML, business rules, business vocabulary, model transformation, SBVR, use case diagram},
	pages = {1--8},
}

@inproceedings{memon_deciphering_2019,
	address = {New York, NY, USA},
	series = {{ICMAI} '19},
	title = {Deciphering and {Analyzing} {Software} {Requirements} employing the techniques of {Natural} {Language} {Processing}},
	isbn = {978-1-4503-6258-0},
	url = {https://doi.org/10.1145/3325730.3325757},
	doi = {10.1145/3325730.3325757},
	abstract = {Deciphering human language by Requirement Analysts is the key issue in Software Development. Clients communicate their software requirements in raw form. In this paper, we are presenting certain techniques of Natural Language processing which work out greatly to extract information properly and minimizing the bugs that may generate in later parts of Software Development. In today's era, the latest technological development in Artificial Intelligence has enabled machines to process the text to a certain level. Natural Language understanding is so far the most critical problem; the Software community is facing today in requirements gathering. In this study, using the techniques of Natural Language Interpretation, Testers and Software Developers can chalk out the more exact requirements from customers which can improve the Quality of Software to a certain level.},
	booktitle = {Proceedings of the 2019 4th {International} {Conference} on {Mathematics} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Memon, Kamran Ali and Xiaoling, Xia},
	year = {2019},
	note = {event-place: Chegndu, China},
	keywords = {Natural Language Processing, Linguistics, Natural Language Understanding, NLP Techniques, Software Requirements},
	pages = {153--156},
}

@inproceedings{binksmith_designing_2025,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '25},
	title = {Designing {Progressive} {Model} {Elicitation} {Tools} to {Support} {Complex} {Cognitive} {Activities}},
	isbn = {979-8-4007-1395-8},
	url = {https://doi.org/10.1145/3706599.3719925},
	doi = {10.1145/3706599.3719925},
	abstract = {Externalizations such as sketches and diagrams are effective in helping individuals plan for complex projects and designs. Offloading complex information on a different medium (e.g. paper) frees up cognitive resources for sophisticated thinking. However, there are significant challenges in effectively using externalizations and notations (formalized externalizations). It is difficult to envision formal representations of complex ideas at the beginning and it is also hard to keep track of evolutions within notations. In addition, existing notations may be insufficient or inflexible for our specific purposes. We call instances of such thinking in which humans have to work through complex information and action space as instances of "Progressive Model Elicitation" (PME). We explore ways to support PME processes by presenting a set of design principles and Schematica, a prototype that implements those design principles to support PME. We present illustrative examples of Schematica use to support the process of PME.},
	booktitle = {Proceedings of the {Extended} {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Binksmith, Adam and Mansoor, Hamid and Nacenta, Miguel A and Toniolo, Alice},
	year = {2025},
	keywords = {Notation, Diagramming, Externalization, Mental Model},
}

@inproceedings{undefinedlgen_lexical_2025,
	address = {New York, NY, USA},
	series = {{NLPIR} '24},
	title = {A {Lexical} {Simplification} {Framework} for {Turkish}},
	isbn = {979-8-4007-1738-3},
	url = {https://doi.org/10.1145/3711542.3711571},
	doi = {10.1145/3711542.3711571},
	abstract = {Lexical simplification is a fundamental step towards improving the accessibility, comprehension, and readability of texts, particularly in languages with limited linguistic resources. In this study, we adopt a state-of-the-art lexical simplification approach and propose a lexical simplification framework tailored to Turkish. Our framework leverages a combination of complex word identification tasks and substitution generation through pre-trained language models to identify complex lexical units using selective substitution ranking approaches and algorithms and replace them with simpler alternatives, thereby improving text readability. This work makes three key contributions: (i) a comprehensive study of lexical simplification for Turkish, including the complex word identification subtask; (ii) a rigorous comparison of various language models for candidate generation using the masked language modeling objective; and (iii) an in-depth exploration of the impact of different complexity thresholds and additional parameters on overall performance. Our framework demonstrates a strong capability to balance simplification and contextual preservation, offering an effective solution to lexical simplification in Turkish.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {undefinedlgen, Bahar and Hattab, Georges},
	year = {2025},
	keywords = {BERT, pre-trained language model, artificial intelligence for social good, complex word identification, lexical simplification, low-resource languages, Text simplification},
	pages = {218--224},
}

@inproceedings{bednar_cognitive_2023,
	address = {New York, NY, USA},
	series = {{eSAAM} '23},
	title = {Cognitive {Architecture} for {Process} industries},
	isbn = {979-8-4007-0835-0},
	url = {https://doi.org/10.1145/3624486.3624489},
	doi = {10.1145/3624486.3624489},
	abstract = {This paper introduces a Cross-Sectorial Big Data Processing platform which provides tools for the semantic modelling of the data analytical processes and for the automatic generation of data analysis scripts for solving the described problems. The main contribution of this paper is the cognitive component for the automatic extraction of the task definition from the narrative description of the problem based on the Large Language Models (LLMs). We have evaluated the proposed method on five problems from the different domains and found that the automatic extraction of the task definition can have promising results that can be applied to full-automatic data analytics.},
	booktitle = {Proceedings of the 3rd {Eclipse} {Security}, {AI}, {Architecture} and {Modelling} {Conference} on {Cloud} to {Edge} {Continuum}},
	publisher = {Association for Computing Machinery},
	author = {Bednar, Peter and Sarnovsky, Martin and Vanko, Jakub Ivan},
	year = {2023},
	note = {event-place: Ludwigsburg, Germany},
	keywords = {Ontologies, Large language model, Language model, Large language models, Semantics, Semantic modelling, Data analytics, Computational linguistics, Extraction, Cognitive architectures, Data Analytics, Automatic extraction, Ontology's, Data handling, Analytical process, Process industries, Processing platform},
	pages = {15--20},
	annote = {Cited by: 0},
}

@inproceedings{chen_bidtrainer_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {{BIDTrainer}: {An} {LLMs}-driven {Education} {Tool} for {Enhancing} the {Understanding} and {Reasoning} in {Bio}-inspired {Design}},
	isbn = {979-8-4007-0330-0},
	url = {https://doi.org/10.1145/3613904.3642887},
	doi = {10.1145/3613904.3642887},
	abstract = {Bio-inspired design (BID) fosters innovations in engineering. Learning BID is crucial for developing multidisciplinary innovation skills of designers and engineers. Current BID education aims to enhance learners’ understanding and analogical reasoning skills. However, it often heavily relies on the teachers’ expertise. When learners pursue independent learning using some educational tools, they face challenges in understanding and reasoning practice within this multidisciplinary field. Additionally, evaluating their learning outcomes comprehensively becomes problematic. Addressing these challenges, we introduce a LLMs-driven BID education method based on a structured ontology and three strategies: enhancing understanding through LLMs-enpowered "learning by asking", assisting reasoning by providing hints and feedback, and assessing learning outcomes through benchmarking against existing BID cases. Implementing the method, we developed BIDTrainer, a BID education tool. User studies indicate that learners using BIDTrainer understood BID knowledge better, reason faster with higher interactivity than the baseline, and BIDTrainer assessed the learning outcomes consistent with experts.},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Liuqing and Jiang, Zhaojun and Xia, Duowei and Cai, Zebin and Sun, Lingyun and Childs, Peter and Zuo, Haoyu},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Analogical reasoning, Bio-inspired design, Design evaluation, Analogy training, Design education, Learning systems, 'current, Bio-inspired designs, Biomimetics, Design Education, Education tool, Independent learning, Learning outcome, Teachers'},
	annote = {Cited by: 11; All Open Access; Bronze Open Access},
}

@inproceedings{gupta_ontology-based_2023,
	address = {New York, NY, USA},
	series = {{IWSPA} '23},
	title = {Ontology-based {Evaluation} of {ABAC} {Policies} for {Inter}-{Organizational} {Resource} {Sharing}},
	isbn = {979-8-4007-0099-6},
	url = {https://doi.org/10.1145/3579987.3586572},
	doi = {10.1145/3579987.3586572},
	abstract = {Attribute-based Access Control (ABAC), as the name suggests, determines whether an access request be granted based on the attributes or characteristics of the requesting user, those of the requested resource, and the environmental condition in which the request is generated. An important advantage of such an identity-agnostic model is that access control can be imposed even on users from other organizations if they are able to prove their attributes to the reference monitor of the organization whose resources are being accessed. It would, however, require a mechanism for mapping the attributes and their values among these organizations. We propose an ontology based method for addressing this requirement. Besides meeting the needs of collaborative accesses, we show how such an approach can be made to naturally support hierarchical ABAC policies as well as controlled relaxation during policy enforcement.},
	booktitle = {Proceedings of the 9th {ACM} {International} {Workshop} on {Security} and {Privacy} {Analytics}},
	publisher = {Association for Computing Machinery},
	author = {Gupta, Tushar and Sural, Shamik},
	year = {2023},
	note = {event-place: Charlotte, NC, USA},
	keywords = {ontology, at- tribute hierarchy, attribute-based access control, digital signature, policy relaxation, resource sharing},
	pages = {85--94},
}

@inproceedings{ji_multivariate_2025,
	address = {New York, NY, USA},
	series = {{DEAI} '25},
	title = {Multivariate data extraction method of enterprise digital internal audit based on knowledge map and {LDA} model},
	isbn = {979-8-4007-1279-1},
	url = {https://doi.org/10.1145/3745238.3745485},
	doi = {10.1145/3745238.3745485},
	abstract = {The explosive growth of enterprise data, with diverse data types and complex structure, provides abundant information resources for enterprise's operational decision-making, risk management and compliance review. However, how to extract valuable information from these massive data efficiently and accurately has become a key problem to be solved urgently in the digital internal audit of enterprises. Therefore, this paper proposes a multivariate data extraction method for enterprise digital internal audit based on knowledge map and LDA model. Define ontology and entity, construct multivariate data security knowledge map of enterprise digital internal audit, obtain the thematic correlation between documents and words, and construct LDA model; The features are aggregated to get the entity data vector, and the multivariate data extraction of enterprise digital internal audit is realized. The experimental results show that the information coverage of this method is high, and it shows significant advantages in the cost of multivariate data extraction in enterprise digital internal audit.},
	booktitle = {Proceedings of the 2nd {Guangdong}-{Hong} {Kong}-{Macao} {Greater} {Bay} {Area} {International} {Conference} on {Digital} {Economy} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Ji, Tingting and Xu, Jingyun and Chen, Weisong and Lin, Shengkai and Wu, Yican and Li, Wei},
	year = {2025},
	keywords = {Knowledge map, Data extraction, Enterprise digitalization, Internal audit multivariate data, LDA model},
	pages = {1577--1581},
}

@inproceedings{suman_enabling_2024,
	address = {New York, NY, USA},
	series = {{ICPE} '24 {Companion}},
	title = {Enabling {Operational} {Data} {Analytics} for {Datacenters} through {Ontologies}, {Monitoring}, and {Simulation}-based {Prediction}},
	isbn = {979-8-4007-0445-1},
	url = {https://doi.org/10.1145/3629527.3652897},
	doi = {10.1145/3629527.3652897},
	abstract = {Datacenters are key components in the ICT infrastructure supporting our digital society. Datacenter operations are hampered by operational complexity and dynamics, risking to reduce or even offset the performance, energy efficiency, and other datacenter benefits. A promising emerging technology, Operational Data Analytics (ODA), promises to collect and use monitoring data to improve datacenter operations. However, it is challenging to organize, share, and leverage the massive and heterogeneous data resulting from monitoring datacenters. Addressing this combined challenge, starting from the idea that graphs could provide a good abstraction, in this work we present our early work on designing and implementing a graph-based approach for datacenter ODA. We focus on two main components of datacenter ODA. First, we design, implement, and validate agraph-based ontology for datacenters that captures both high-level meta-data information and low-level metrics of operational data collected from real-world datacenters, and maps them to a graph structure for better organization and further use. Second, we design and implementODAbler, a software framework for datacenter ODA, which combines ODA data with an online simulator to make predictions about current operational decisions and other what-if scenarios. We take the first steps to illustrate the practical use of ODAbler, and explore its potential to support datacenter ODA through graph-based analysis. Our work helps construct the case that graph-based ontologies have great value for datacenter ODA and, further, to improving datacenter operations.},
	booktitle = {Companion of the 15th {ACM}/{SPEC} {International} {Conference} on {Performance} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Suman, Shekhar and Chu, Xiaoyu and Niewenhuis, Dante and Talluri, Sacheendra and De Matteis, Tiziano and Iosup, Alexandru},
	year = {2024},
	note = {event-place: London, United Kingdom},
	keywords = {simulation, mapping, analysis, \%oda monitoring, datacenter, graph-based ontology, odabler, opendc, operational data analytics},
	pages = {120--126},
}

@inproceedings{wang_q-bert_2020,
	address = {New York, NY, USA},
	series = {{WWW} '20},
	title = {Q-{BERT}: {A} {BERT}-based {Framework} for {Computing} {SPARQL} {Similarity} in {Natural} {Language}},
	isbn = {978-1-4503-7024-0},
	url = {https://doi.org/10.1145/3366424.3382699},
	doi = {10.1145/3366424.3382699},
	abstract = {In this paper, we present a pre-trained transformer network Q-BERT, in which siamese network architecture is employed to produce semantically meaningful embeddings of SPARQL queries to be compared via cosine-similarity. A core idea of Q-BERT is to put SPARQL query into the category of natural language to ensure that each entity mention or relation phrase in different knowledge bases has the same vector representation. Moreover, based on Q-BERT, we present a practical approach for the SPARQL query-empty-answer problem by accessing the RDF repository. The experiments on real datasets show the effectiveness and efficiency of Q-BERT.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2020},
	publisher = {Association for Computing Machinery},
	author = {Wang, Chunpei and Zhang, Xiaowang},
	year = {2020},
	note = {event-place: Taipei, Taiwan},
	keywords = {BERT, SPARQL, Semantic Similarity, Siamese},
	pages = {65--66},
}

@inproceedings{marchezan_tool_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {A {Tool} for {Collaborative} {Consistency} {Checking} {During} {Modeling}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688558},
	doi = {10.1145/3652620.3688558},
	abstract = {Consistency checking is widely used to detect inconsistencies in engineering artifacts. In collaborative modeling environments, however, maintaining model consistency is challenging due to the frequent changes introduced by multiple engineers, leading to possibly numerous inconsistencies. These inconsistencies need to be identified and shared among engineers to collaboratively resolve them and prevent cascading problems. Despite extensive research in consistency checking, there remains a lack of tools that support collaborative consistency checking (C3). C3 is defined as the process of checking and fixing inconsistencies during collaborative modeling, whether engineers work synchronously or asynchronously. To address this gap, we introduce a prototype tool integrated into the DesignSpace environment which allows collaborative work during modeling and consistency checking. Our tool is implemented to support a streamlined version of UML (sUML) with various diagrams such as class, use case, sequence, and state-machine. We showcase the tool's features with an example of a robotic arm, highlighting its effectiveness in collaborative consistency checking. We also compare our tool with related work, showing that collaborative features are currently lacking in the proposed tools. A video showcasing the tool is available at https://www.youtube.com/watch?v=9kweKeBHx5Y},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Marchezan, Luciano and Homolka, Marcel and Blokhin, Andrei and Assunção, Wesley K. G. and Herac, Edvin and Egyed, Alexander},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {collaborative modeling, consistency checking, tool demo},
	pages = {655--659},
}

@inproceedings{liu_tracking_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {Tracking {Semantic} {Evolutionary} {Changes} in {Large}-{Scale} {Ontological} {Knowledge} {Bases}},
	isbn = {978-1-4503-8446-9},
	url = {https://doi.org/10.1145/3459637.3482307},
	doi = {10.1145/3459637.3482307},
	abstract = {This paper is concerned with the problem of computing the semantic difference between different versions of large-scale ontological knowledge bases using a uniform interpolation (UI) approach. The semantic difference between two versions of an ontology are the axioms entailed by one version but not the other version, reflecting the evolutionary changes of the content of the ontology. In general, computing such axioms is not computationally feasible, since there are infinitely many of them. UI is an advanced reasoning technique that seeks to create restricted views of ontologies; it provides an effective means for computing a finite representation of the difference between two ontologies. While existing UI methods are designed for languages that are either more expressive or less expressive than the description logic ELH, the underlying language of typical large-scale ontologies, in this paper, we introduce a practical UI method tailored for the task of computing the semantic difference in large-scale ELH-ontologies. The method is terminating, sound, and can always compute UI results possibly including fresh definer symbols. Two case studies on different versions of the SNOMED CT terminology show that the method has overcome major limitations of existing UI methods and can be used to reveal modeling changes that have occurred over successive releases of SNOMED CT.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhao and Lu, Chang and Alghamdi, Ghadah and Schmidt, Renate A. and Zhao, Yizheng},
	year = {2021},
	note = {event-place: Virtual Event, Queensland, Australia},
	keywords = {ontologies, knowledge representation and reasoning, forgetting, uniform interpolation, description logics, semantic difference},
	pages = {1130--1139},
}

@inproceedings{bach_10_2024,
	address = {New York, NY, USA},
	series = {{MODELS} '24},
	title = {10 years of {Model} {Federation} with {Openflexo}: {Challenges} and {Lessons} {Learned}},
	isbn = {979-8-4007-0504-5},
	url = {https://doi.org/10.1145/3640310.3674084},
	doi = {10.1145/3640310.3674084},
	abstract = {In the context of complex system development, heterogeneous modeling responds to the need to integrate several domains. This need requires the use of the most appropriate formalism and tooling for each domain to be efficient. Model federation promotes the semantic interoperability of heterogeneous models by providing the means to reify correspondences between different model elements, add custom behaviors and bridge the gap between technological spaces. As such, it can be used as an infrastructure to address many different system engineering problems. This is what we have been doing for over a decade, as part of a close collaboration between a small software engineering startup and academia. This paper reports on this experience.Concretely, we discuss the context, ambitions, and challenges that led to the inception of our practice of model federation, and we present five use cases experiences, stemming from real industrial and academic needs, and elaborate on lessons learned. In addition, we also report on challenges and lessons learned regarding the development and maintenance of a model-driven model federation tool, the Openflexo framework. Finally, we set up a road map for the future of model federation and Openflexo.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bach, Jean-Christophe and Beugnard, Antoine and Champeau, Joël and Dagnat, Fabien and Guérin, Sylvain and Martínez, Salvador},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {Model management, Experience report, Model federation},
	pages = {25--36},
}

@inproceedings{bozzato_ontology-mediated_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {Ontology-{Mediated} {Data} {Migration}: {Deriving} {Migration} {Rules} by {Reasoning} on {Schema} {Descriptions}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3577616},
	doi = {10.1145/3555776.3577616},
	abstract = {Migration of data across information systems is a knowledge intensive task: the definition of mappings between systems requires knowledge of the source and target (relational) schemas and their interpretation of the shared domain. Moreover, direct schema mappings need often to be re-defined for each new migration instance, in order to accommodate the variations caused by the change of systems and representation conventions. A possible solution to such problems is the use of an intermediate ontological model, that can be used as a lingua franca for the description of schemas, by defining mappings from and to the ontology. While this helps in making explicit the semantics of the schemas, the problem remains on how to extract a direct mapping from source to target schema from this intermediate representation.In this paper, we present our ongoing work in building an ontology-based migration system in the scenario of banking information systems. In the architecture of the system, an ontology defines an intermediate semantic description for the source and target schemas. We introduce a reasoning method for the automatic extraction of migration rules starting from the semantic descriptions of the schemas. The procedure for computation of migration rules is then implemented via reasoning over an Answer Set Programming encoding.},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Bozzato, Loris and Serafini, Luciano},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {data migration, ontology mediated migration, relational schema mapping},
	pages = {1724--1731},
}

@inproceedings{merah_ontology-based_2021,
	address = {New York, NY, USA},
	series = {{ARES} '21},
	title = {Ontology-based {Cyber} {Risk} {Monitoring} {Using} {Cyber} {Threat} {Intelligence}},
	isbn = {978-1-4503-9051-4},
	url = {https://doi.org/10.1145/3465481.3470024},
	doi = {10.1145/3465481.3470024},
	abstract = {Efficient cyber risk assessment needs to consider all security alerts provided by cybersecurity solutions deployed in a network. To build a reliable overview of cyber risk, there is a need to adopt continuous monitoring of emerged cyber threats related to that risk. Indeed, the integration of Cyber Threat Intelligence (CTI) into cybersecurity solutions provides valuable information about threats, targets, and potential vulnerabilities. Structured Threat Information eXpression (STIX), as a language for expressing information about cyber threats in a structured and unambiguous manner, is becoming a de facto standard for sharing information about cyber threats. In addition, ontology-based semantic knowledge modeling has become a promising solution that provides a machine-readable language for downstream work in cybersecurity problem-solving. In this paper, we propose an ontology using CTI for risk monitoring. This latter improves an existing ontology, originally proposed to be used within a SIEM (Security Information Event Management), by extending it and aligning it with the STIX concepts.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Merah, Yazid and Kenaza, Tayeb},
	year = {2021},
	note = {event-place: Vienna, Austria},
	keywords = {Ontology, OWL, Risk Assessment, Cyber Threat Intelligence},
}

@inproceedings{hallak_model_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Model {Management} at {Renault} {Virtual} {Simulation} {Team}: {State} of {Practice}, {Challenges} and {Research} {Directions}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688223},
	doi = {10.1145/3652620.3688223},
	abstract = {In the automotive industry, new systems are being developed to enhance vehicle safety and driver convenience. These systems are increasingly complex to build and maintain. To develop these systems Renault makes intensive use of simulation and must deal with thousands of models. This huge number of models must be well managed. To manage these models, Renault has developed the SysML-based Model Identity Card (MIC), used with a Model-Based Simulation (MBSi) approach. However, despite this first solution, managing simulation models remains a difficult task.In this paper, we describe the current simulation model management approaches used at Renault, and their shortcomings and challenges in the modelling and simulation of complex automotive systems. We use Advanced Driver Assistance System (ADAS) and its Automatic Emergency Breaking (AEB) sub-system as examples to illustrate the utilization of the MIC and demonstrate current practices. From these examples, we derive main challenges faced by the virtual simulation team and propose research directions to solve them, based on state of the art methodologies for simulation models' validation and verification management.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hallak, Yara and Blouin, Dominique and Pautet, Laurent and Saab, Layale and Laborie, Baptiste and Mittal, Rakshit},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {model management, model-based systems engineering, advanced driver assistance system, model identity card, model-based simulation},
	pages = {1005--1014},
}

@inproceedings{abbasi_understanding_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Understanding {Semantic} {Drift} in {Model} {Driven} {Digital} {Twins}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688256},
	doi = {10.1145/3652620.3688256},
	abstract = {Digital twins have revolutionized the industry in recent years by providing virtual representations of physical assets, systems, or processes, and relying on real-time data for effective functioning. These twins enable real-time monitoring, analysis, and simulation of real-world entities through the extensive use of various digital models, including design models, scientific models, and data models that capture the status and behaviour of the corresponding physical entities. However, as the real world evolves, these models must adapt to maintain consistency with their physical counterparts. This adaptation process can lead to semantic drift, a misalignment between the digital representation and the physical reality over time. In this paper, we propose a classification and formalization of different types of semantic drift and review how this concept is understood in the literature on model-driven digital twins. We further illustrate the scenarios associated with each type of semantic drift using an urban mobility use case, explicitly highlighting the practical implications.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Abbasi, Faima and Brimont, Pierre and Pruski, Cedric and Sottet, Jean-Sébastien},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {digital twins, data models, design models, scientific models, semantic drift},
	pages = {419--430},
}

@inproceedings{jain_understanding_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {Understanding {Financial} {Transaction} {Documents} using {Natural} {Language} {Processing}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364439},
	doi = {10.1145/3360901.3364439},
	abstract = {In this paper, we share our experiences creating NLP based AI platform for finance - Appzen (http://www.appzen.com). AppZen's auditing technology is being utilized by over 500 enterprise customers including multiple Fortune 500 companies for auditing employee expenses. AppZen's technology can process, analyze and identify relationships between various kinds of transaction documents such as - receipts, invoices, contracts and purchase orders. Each type of transaction document requires custom processing and analysis due to the diversity in language and structure of the document. Contracts typically require deep understanding of the content such as identifying sentence structures, identifying entities and relationships between them compared to receipts and invoices, which are somewhat semi-structured and require a different kind of processing. We elaborate on the challenges we have experienced and use of NLP in conjunction with a lightweight semantic layer to alleviate these challenges.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Jain, Prateek and Verma, Kunal and Gaikwad, Aniket and Gadde, Pramod},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {information extraction, feature engineering, financial auditing, nlp},
	pages = {255--258},
}

@inproceedings{zhu_centers_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {The {Centers} and {Margins} of {Modeling} {Humans} in {Well}-being {Technologies}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713940},
	doi = {10.1145/3706598.3713940},
	abstract = {This paper critically examines the machine learning (ML) modeling of humans in three case studies of well-being technologies. Through a critical technical approach, it examines how these apps were experienced in daily life (technology in use) to surface breakdowns and to identify the assumptions about the “human” body entrenched in the ML models (technology design). To address these issues, this paper applies agential realism to decenter foundational assumptions, such as body regularity and health/illness binaries, and speculates more inclusive design and ML modeling paths that acknowledge irregularity, human-system entanglements, and uncertain transitions. This work is among the first to explore the implications of decentering theories in computational modeling of human bodies and well-being, offering insights for more inclusive technologies and speculations toward posthuman-centered ML modeling.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Jichen and Sanches, Pedro and Tsaknaki, Vasiliki and van der Maden, Willem and Kaklopoulou, Irene},
	year = {2025},
	keywords = {Diffraction, Well-being, Agential Realism, Decentering, Machine Learning Modeling},
}

@inproceedings{paudel_hype_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Hype versus {Historical} {Continuity}: {Situating} the {Rise} of {AI} in {Climate} and {Disaster} {Risk} {Modeling}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713985},
	doi = {10.1145/3706598.3713985},
	abstract = {As governments increasingly adopt Artificial Intelligence (AI) across different application sectors, advocates argue that it will create new disruptions by democratizing access, improving accuracy, and lowering costs. In practice, uncritical adoption of AI tools has been shown to cause significant harms. Our study uses a historical lens to examine the uptake of AI in climate risk management through a study of climate and disaster risk modeling. These techniques originated in the insurance industry, but are now incorporated into many climate and disaster governance processes. Using the concept of ‘insurance logics’, we demonstrate that many of the original aspects of disaster risk modeling remain despite the transfer of risk assessment tools from the insurance industry to the public sector and new techniques made possible by AI. This highlights technological continuity, rather than disruption, as a key driver of contemporary risk modeling practice. Doing so helps to unsettle problematic, though challenging to identify, aspects of supposedly disruptive technologies and create possibilities for alternatives.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Paudel, Shreyasha and Loos, Sabine and Soden, Robert},
	year = {2025},
	keywords = {Responsible AI, History, AI Hype, Climate Risk, Disaster Risk Models, Technological Evolution},
}

@inproceedings{fu_modelling_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Modelling a {Warehouse} with {SLICER}: {A} {Contribution} to the {MULTI} {Warehouse} {Challenge}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688215},
	doi = {10.1145/3652620.3688215},
	abstract = {In this paper we apply the SLICER multi-level modelling framework on the MULTI 2024 Warehouse Challenge. SLICER was first introduced at the ER 2015 conference[17] and formally specified in [18]. It has been implemented in an object-oriented knowledge representation and reasoning system and the framework identifies additional semantic subcategories for the traditional object-oriented relationship types and uses these distinctions for a new style of formal characterisation of level relationships. The framework is executable via its implementation in a dynamic metamodeling and object logic environment. We first introduce SLICER, then describe its application on the Warehouse Challenge requirements and then discuss the advantages of the SLICER representation.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fu, Yuhong and Selway, Matt and Grossmann, Georg and Kaur, Karamjit and Stumptner, Markus},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {multi-level modelling, SLICER, warehouse challenge},
	pages = {828--837},
}

@inproceedings{gasidou_specific_2024,
	address = {New York, NY, USA},
	series = {{PCI} '23},
	title = {Specific modeling issues for designing the transformation of a smart city},
	isbn = {979-8-4007-1626-3},
	url = {https://doi.org/10.1145/3635059.3635087},
	doi = {10.1145/3635059.3635087},
	abstract = {In this short paper, we present methods and solutions for specific issues of concern to engineers and application technologists related to modeling a smart city's transformation. The main body of developing "structural design modeling" for a smart city utilizes the Unified Modeling Language™ (UML®) conceptual approach. Our approach provides an overview of the specific domains (points - nodes) related to the topic. Our discussion does not delve into significant issues. It does not solidify the proposed approaches but as a research proposal, it remains at a level that could be described as a presentation or an argumentative position. Our short paper heavily relies on diagrams of models. This short paper focuses on specific design issues related to modeling mappings specifically for the design and modeling processes for a smart city.},
	booktitle = {Proceedings of the 27th {Pan}-{Hellenic} {Conference} on {Progress} in {Computing} and {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Gasidou, Anastasia and Kotsifakos, Dimitrios and Douligeris, Christos},
	year = {2024},
	note = {event-place: Lamia, Greece},
	keywords = {Engineering, Informatics, Science, Technology, Modeling Patterns, Project},
	pages = {181--184},
}

@inproceedings{kim_ontology_2018,
	address = {New York, NY, USA},
	series = {{RACS} '18},
	title = {Ontology modeling for {APT} attack detection in an {IoT}-based power system},
	isbn = {978-1-4503-5885-9},
	url = {https://doi.org/10.1145/3264746.3264786},
	doi = {10.1145/3264746.3264786},
	abstract = {Smart grid technology is the core technology for the next-generation power grid system with enhanced energy efficiency through decision-making communication between suppliers and consumers enabled by integrating the IoT into the existing grid. This open architecture allowing bilateral information exchange makes it vulnerable to various types of cyberattack. APT attacks, one of the most common cyberattacks, are highly tricky and sophisticated attacks that can circumvent the existing detection technology and attack the targeted system after a certain latent period after intrusion. This paper proposes an ontology-based attack detection system capable of early detection of and response to APT attacks by analyzing their attacking patterns.},
	booktitle = {Proceedings of the 2018 {Conference} on {Research} in {Adaptive} and {Convergent} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Gihoon and Choi, Chang and Choi, Junho},
	year = {2018},
	note = {event-place: Honolulu, Hawaii},
	keywords = {ontology, IoT, APT attack, smart grid},
	pages = {160--164},
}

@inproceedings{moon_modeling_2018,
	address = {New York, NY, USA},
	series = {{UMAP} '18},
	title = {Modeling {Learners}' {Interest} with a {Domain}-{Independent} {Ontology}-{Based} {Framework}},
	isbn = {978-1-4503-5589-6},
	url = {https://doi.org/10.1145/3209219.3213598},
	doi = {10.1145/3209219.3213598},
	abstract = {Ontologies are recognized as a promising approach to support the reusability and interoperability of learners' preferences; which is useful for the optimization and flexibility of data and resources. However, little to no research on adaptive learning systems or semantic technologies explore personalized experiences based on the various out-of-school experiences and activities of the users. This research investigates the design, development, and evaluation of an ontology-based framework for students' interests in a math word problem generator that may be applied to various other learning systems and possibly other domains. The cohesiveness of the problems in addition to the usability, usefulness, and the short-term effectiveness of the derived technology will be investigated by comparing the generated questions to numerical and traditional Algebra I problems. We aim to better understand students' interests to identify the role that their interests can play in semantic technologies, further supporting the recent advances in ontology-based educational technologies and personalized math word problem generators.},
	booktitle = {Proceedings of the 26th {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Moon, DeKita G.},
	year = {2018},
	note = {event-place: Singapore, Singapore},
	keywords = {knowledge graphs, semantic technologies, domain ontologies, educational technologies, human-centered computing},
	pages = {345--348},
}

@inproceedings{chaturvedi_temporal_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Temporal {Knowledge} {Graph} {Extraction} and {Modeling} across {Multiple} {Documents} for {Health} {Risk} {Prediction}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3651256},
	doi = {10.1145/3589335.3651256},
	abstract = {Clinical text in electronic health records (EHR) holds vital cues into a patient's journey, often absent in structured EHR data. Evidence-based healthcare decisions demand accurate extraction and modeling of these cues. The goal of our study is to predict Type-II Diabetes by utilizing concept-based models of visit sequences from longitudinal EHR data. We undertake the challenging task of fine-grained temporal information extraction from clinical text using a recent span-based approach with pre-trained transformers. We achieve a new state-of-the-art in end-to-end relation extraction from 2012 clinical temporal relations corpus. We propose to apply our model to a new dataset and extract patient-centric temporal knowledge graphs from their visits-fusing temporal orderings within documents and across visits. Beyond the current focus of our work on Type-II Diabetes risk prediction from EHR, our versatile framework can be extended to other domains including web-based healthcare systems for personalized medicine. It can not only model health outcomes having long progression timelines but also various socio-economic outcomes such as conflict, natural disasters, and financial markets by leveraging news, reports, and social-media text for extracting and modeling irregular time-series and help inform a variety of web-based applications and policies.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Chaturvedi, Rochana},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {natural language processing, dynamic graphs, multidocument classification, patient-centric, temporal information extraction, temporal knowledge graphs, time series classification},
	pages = {1182--1185},
}

@inproceedings{niyazova_ontology_2019,
	address = {New York, NY, USA},
	series = {{ICEMIS} '19},
	title = {An ontology based model for user profile building using social network},
	isbn = {978-1-4503-7212-1},
	url = {https://doi.org/10.1145/3330431.3330453},
	doi = {10.1145/3330431.3330453},
	abstract = {The structure and basic principles of technology for increasing the probability of identifying subjects of information processes of open Internet resources based on ontology methods are considered. Based on this ontology the knowledge base intended for creation of the program systems supporting ensuring information security has been realized. The developed ontological knowledge base has been used when developing the software complex intended for identification of the user of social networks when ensuring information security, monitoring and preventing threats.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Engineering} and {MIS}},
	publisher = {Association for Computing Machinery},
	author = {Niyazova, R. and Aktayeva, Al. and Davletkireeva, L.},
	year = {2019},
	note = {event-place: Astana, Kazakhstan},
	keywords = {SPARQL, ontology, cybersecurity, knowledge base, identification, social network},
}

@inproceedings{parvizimosaed_model-checking_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Model-checking legal contracts with {SymboleoPC}},
	isbn = {978-1-4503-9466-6},
	url = {https://doi.org/10.1145/3550355.3552449},
	doi = {10.1145/3550355.3552449},
	abstract = {Legal contracts specify requirements for business transactions. As any other requirements specification, contracts may contain errors and violate properties expected by contracting parties. Symboleo was recently proposed as a formal specification language for legal contracts. This paper presents SymboleoPC, a tool for analyzing Symboleo contracts using model checking. It highlights the architecture, implementation and testing of the tool, as well as a scalability evaluation with respect to the size of contracts and properties to be checked through a series of experiments. The results suggest that SymboleoPC can be usefully applied to the analysis of formal specifications of contracts with real-life sizes and structures.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Parvizimosaed, Alireza and Roveri, Marco and Rasti, Aidin and Amyot, Daniel and Logrippo, Luigi and Mylopoulos, John},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {smart contracts, model checking, formal specification languages, legal contracts, nuXmv, performance analysis, software requirements specifications},
	pages = {278--288},
}

@inproceedings{roy_building_2025,
	address = {New York, NY, USA},
	series = {{WSDM} '25},
	title = {Building {Trustworthy} {AI} {Models} for {Medicine}: {From} {Theory} to {Applications}},
	isbn = {979-8-4007-1329-3},
	url = {https://doi.org/10.1145/3701551.3703477},
	doi = {10.1145/3701551.3703477},
	abstract = {AI is emerging as an efficient companion in medicine. While AI holds promise for reducing the cognitive load of researchers and practitioners, its adoption is often hindered by a lack of trust in new AI advancements. We present sophisticated techniques for developing trustworthy artificial intelligence (AI) models in medicine, bridging breakthroughs in AI research with practical healthcare applications. We will discuss in-depth the four stages (Design, Development, Implementation, and Evaluation) involved in the process of building trustworthy AI models customized for the medical domain. We present various techniques for incorporating important Trustworthy AI principles like data privacy, robustness, explainability, interpretability, medical experts-in-the-loop, and risk assessment while developing AI models for medicine. In contrast to prior tutorials, we make the following two key contributions: (i) While explaining the 'Implementation' stage, we cover various real-world healthcare applications developed as part of research projects in academia in collaboration with medical schools in India and Germany. (ii) By including a health informatics professional as one of the tutorial organizers, we provide a fresh and much-needed perspective on the research challenges and mitigation strategies in building AI models for medicine.},
	booktitle = {Proceedings of the {Eighteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Roy, Soumyadeep and Sundaram, Sowmya S. and Wolff, Dominik and Ganguly, Niloy},
	year = {2025},
	note = {event-place: Hannover, Germany},
	keywords = {knowledge integration in healthcare, medical NLP, trustworthy AI},
	pages = {1012--1015},
}

@inproceedings{kousa_data-driven_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Data-{Driven} {Democracy}? {Using} {Social} {Media} and {AI} for {Knowledge} {Co}-production in {Energy} {Transition} {Research}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715286},
	doi = {10.1145/3701716.3715286},
	abstract = {As global challenges grow increasingly difficult to address, the need for relevant knowledge in policymaking has become more critical than ever. In fields such as sustainability research, discussions about the co-production of knowledge emphasise the involvement of diverse stakeholders in generating relevant information for democratic decision-making processes. Particularly in technology-intensive areas like energy, the voices of experts have traditionally dominated, often marginalising the perspectives of citizens affected by energy policies and leading to epistemic injustices. Social media serves as a central forum for political activity and civic engagement, making it an important research environment for understanding the complexities of knowledge production and sharing. In my dissertation, I explore the opportunities and challenges of integrating citizen voices into the co-production of knowledge through social media using artificial intelligence (AI) based methods. My approach includes analysing stakeholder perspectives across social media, policymakers' speeches, and traditional media, using the discourse on energy justice in the context of sustainable energy transition as empirical material. Additionally, I conduct a comparative analysis of a rule-based ontological classification tool and a large language model (LLM) chatbot as qualitative content analysis tools. I demonstrate that social media data can reveal critical perspectives and marginalised discourses that might otherwise be overlooked. However, social media platforms are also used to spread disinformation and incite polarisation, which undermines their reliability as accurate reflections of prevailing attitudes and opinions in society. In addition, although AI methods can make information processing more effective and support the use of large datasets, it is important to consider their inherent limitations. I address the biases and ethical concerns associated with social media data and AI-based analysis tools and discuss potential solutions to mitigate these challenges.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Kousa, Ilona M.},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {Large language model, Natural language processing, Language model, Artificial intelligence, large language models, natural language processing, Knowledge management, Energy transition, Sustainable development, social media, Energy, Decision making, Social media, Computational social science, Knowledge transfer, Language processing, Energy policy, computational social sciences, Natural languages, Social networking (online), Co-production of knowledge, Energy transitions, Large datasets},
	pages = {701--704},
	annote = {Cited by: 0},
}

@inproceedings{gokdemir_hiperrag_2025,
	address = {New York, NY, USA},
	series = {{PASC} '25},
	title = {{HiPerRAG}: {High}-{Performance} {Retrieval} {Augmented} {Generation} for {Scientific} {Insights}},
	isbn = {979-8-4007-1886-1},
	url = {https://doi.org/10.1145/3732775.3733586},
	doi = {10.1145/3732775.3733586},
	abstract = {The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval-Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering (Q/A) benchmarks and two new benchmarks introduced in this work, achieving 90\% accuracy on SciQ and 76\% on PubMedQA—outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.},
	booktitle = {Proceedings of the {Platform} for {Advanced} {Scientific} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Gokdemir, Ozan and Siebenschuh, Carlo and Brace, Alexander and Wells, Azton and Hsu, Brian and Hippe, Kyle and Setty, Priyanka and Ajith, Aswathy and Pauloski, J. Gregory and Sastry, Varuni and Foreman, Sam and Zheng, Huihuo and Ma, Heng and Kale, Bharat and Chia, Nicholas and Gibbs, Thomas and Papka, Michael and Brettin, Thomas and Alexander, Francis and Anandkumar, Anima and Foster, Ian and Stevens, Rick and Vishwanath, Venkatram and Ramanathan, Arvind},
	year = {2025},
	note = {event-place: FHNW University of Applied Sciences and Arts Northwestern Switzerland, Brugg-Windisch, Switzerland},
	keywords = {AI, large language models, retrieval-augmented generation, HPC, metric learning, neural information retrieval},
	pages = {1--13},
}

@inproceedings{baader_optimal_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {Optimal {Repairs} in {Ontology} {Engineering} as {Pseudo}-{Contractions} in {Belief} {Change}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3577719},
	doi = {10.1145/3555776.3577719},
	abstract = {The question of how a given knowledge base can be modified such that certain unwanted consequences are removed has been investigated in the area of knowledge engineering under the name of repair and in the area of belief change under the name of contraction. Whereas in the former area the emphasis was more on designing and implementing concrete repair algorithms, the latter area concentrated on characterizing classes of contraction operations by certain postulates they satisfy. In the classical setting, repairs and contractions are subsets of the knowledge base that no longer have the unwanted consequence. This makes these approaches syntax-dependent and may result in removal of more consequences than necessary. To alleviate this problem, gentle repairs and pseudo-constractions have been introduced in the respective research areas, and their connections have been investigated in recent work. Optimal repairs preserve a maximal amount of consequences, but they may not always exist. We show that, if they exist, then they can be obtained by certain pseudo-contraction operations, and thus they comply with the postulates that these operations satisfy. Conversely, under certain conditions, pseudo-contractions are guaranteed to produce optimal repairs.},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Baader, Franz},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {description logic, belief change, ontology repair},
	pages = {983--990},
}

@inproceedings{saini_machine_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Machine learning-based incremental learning in interactive domain modelling},
	isbn = {978-1-4503-9466-6},
	url = {https://doi.org/10.1145/3550355.3552421},
	doi = {10.1145/3550355.3552421},
	abstract = {In domain modelling, practitioners manually transform informal requirements written in natural language (problem descriptions) to more concise and analyzable domain models expressed with class diagrams. With automated domain modelling support using existing approaches, manual modifications may still be required in extracted domain models and problem descriptions to make them more accurate and concise. For example, educators teaching software engineering courses at universities usually use an incremental approach to build modelling exercises to restrict students in using intended modelling patterns. These modifications result in the evolution of domain modelling exercises over time. To assist practitioners in this evolution, a synergy between interactive support and automated domain modelling is required. In this paper, we propose a bot-assisted approach to allow practitioners perform domain modelling quickly and interactively. Furthermore, we provide an incremental learning strategy empowered by machine learning to improve the accuracy of the bot's suggestions and extracted domain models by analyzing practitioners' decisions over time. We evaluate the performance of our bot using test problem descriptions which shows that practitioners can expect to get useful support from the bot when applied to exercises of similar size and complexity, with precision, recall, and F2 scores over 85\%. Finally, we evaluate our incremental learning strategy where we observe a reduction in the required manual modifications by 70\% and an improvement of F2 scores of extracted domain models by 4.2\% when using our proposed approach and learning strategy together.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L. C. and Kienzle, Jörg},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {evolution, natural language processing (NLP), bot, decisions, domain models, incremental learning, machine learning (ML)},
	pages = {176--186},
}

@inproceedings{georges_guiding_2023,
	address = {New York, NY, USA},
	series = {{VaMoS} '23},
	title = {Guiding {Feature} {Models} {Synthesis} from {User}-{Stories}: {An} {Exploratory} {Approach}},
	isbn = {979-8-4007-0001-9},
	url = {https://doi.org/10.1145/3571788.3571797},
	doi = {10.1145/3571788.3571797},
	abstract = {User-stories are commonly used to define requirements in agile project management. In Software Product Lines (SPL), a user-story corresponds to a feature description (or part of it), that can be shared by several products. In practice, large SPL include a huge number of user-stories, making variability hard to grasp and handle. In this paper we present an exploratory approach that aims to guide the synthesis of Feature Models that capture and structure the commonalities and the variability expressed in these user-stories. The built Feature Models aim to help the project understanding, maintenance and evolution. Our approach first decomposes the user-stories to extract the roles and the features, using natural language processing techniques. In a second step, we group user-stories having the same topics thanks to a clustering method. This contributes to extract more general features. In a third step, we leverage the use of Formal Concept Analysis to extract logical constraints between the features that guide Feature Model synthesis. We illustrate our approach using a dataset from our industrial partner.},
	booktitle = {Proceedings of the 17th {International} {Working} {Conference} on {Variability} {Modelling} of {Software}-{Intensive} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Georges, Thomas and Rice, Liam and Huchard, Marianne and König, Mélanie and Nebut, Clémentine and Tibermacine, Chouki},
	year = {2023},
	note = {event-place: Odense, Denmark},
	keywords = {Natural Language Processing, Reengineering, SPL domain engineering, Agile Process, Feature Model, Formal Concept Analysis, Software Product Line, User Story},
	pages = {65--70},
}

@inproceedings{degbelo_open_2020,
	address = {New York, NY, USA},
	series = {{WWW} '20},
	title = {Open {Geodata} {Reuse}: {Towards} {Natural} {Language} {Interfaces} to {Web} {APIs}},
	isbn = {978-1-4503-7024-0},
	url = {https://doi.org/10.1145/3366424.3384363},
	doi = {10.1145/3366424.3384363},
	abstract = {Open Government datasets have been flooding the Web recently, and Application Programming Interfaces (APIs) are key to the development of services on top of these datasets. An issue of current APIs worldwide is that their learnability is limited. This work has explored the potential of querying APIs using natural language terms to mitigate that issue. A user study with 20 participants has demonstrated that a natural-language-based, along with an order-agnostic approach to API design can produce easily learnable APIs for both novice and experienced API users. These insights can pave the way for a paradigm change on Web-API design for open geodata retrieval and beyond.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2020},
	publisher = {Association for Computing Machinery},
	author = {Degbelo, Auriol and Sherpa, Ang},
	year = {2020},
	note = {event-place: Taipei, Taiwan},
	keywords = {Smart Cities, Open Government Data, API Design Conventions, API Learnability, API Usability, Geographic Information Retrieval},
	pages = {703--710},
}

@article{razin_converging_2024,
	title = {Converging {Measures} and an {Emergent} {Model}: {A} {Meta}-{Analysis} of {Human}-{Machine} {Trust} {Questionnaires}},
	volume = {13},
	url = {https://doi.org/10.1145/3677614},
	doi = {10.1145/3677614},
	abstract = {Trust is crucial for technological acceptance, continued usage, and teamwork. However, human-robot trust, and human-machine trust more generally, suffer from terminological disagreement and construct proliferation. By comparing, mapping, and analyzing well-constructed trust survey instruments, this work uncovers a consensus structure of trust in human–machine interaction. To do so, we identify the most frequently cited and best-validated human-machine and human-robot trust questionnaires as well as the best-established factors that form the dimensions and antecedents of such trust. To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires. Furthermore, we perform a meta-analysis of the regression models which emerged from the experiments that employed multi-factorial survey instruments. Based on this meta-analysis, we provide the most complete, experimentally validated model of human-machine and human-robot trust to date. This convergent model establishes an integrated framework for future research. It determines the current boundaries of trust measurement and where further investigation and validation are necessary. We close by discussing how to choose an appropriate trust survey instrument and how to design for trust. By identifying the internal workings of trust, a more complete basis for measuring trust is developed that is widely applicable.},
	number = {4},
	journal = {J. Hum.-Robot Interact.},
	author = {Razin, Yosef S. and Feigh, Karen M.},
	month = nov,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Human-Robot Trust, Shared Mental Models},
}

@inproceedings{bur_orm_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {{ORM} ontologies with executable derivation rules to support semantic search in large-scale data applications},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3559576},
	doi = {10.1145/3550356.3559576},
	abstract = {A semantic layer maps complex enterprise data into an ontology with abstract business concepts that are well-known to business users. Chief data officers invest significant effort to create and update these ontologies, while data scientists do feature engineering by combining already existing concepts and features of the domain. However, it is a significant challenge to catalogue and maintain the numerous features pertaining to an ontology, which leads to duplicated features and unnecessary complexity. In this work, we propose to combine ontologies captured using the Object-Role Modeling notation with derivation rules defined in a datalog-like language called Rel, which allows the creation of a semantic layer with feature search capability. Our prototype framework uses the RAI Knowledge Graph Management System, which provides automated and incremental derivation rule evaluation.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Búr, Márton and Stirewalt, Kurt},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	pages = {81--82},
}

@inproceedings{amissah_towards_2018,
	address = {San Diego, CA, USA},
	series = {{Mod4Sim} '18},
	title = {Towards a framework for executable systems modeling: an executable systems modeling language ({ESysML})},
	isbn = {978-1-5108-6018-6},
	abstract = {The Systems Modeling Language (SysML), which is the de-facto modeling standard in the systems engineering community, consists of a number of independently derived methodologies (i.e. state charts, activity diagrams etc.) which have been co-opted into a single modeling framework. This and the lack of an overarching meta-model that specifies relationships and rules governing the various language constructs precludes their uniform application across diagram types. This has resulted in a large unwieldy and at best semi-formal language specification, with adverse implications for interoperability of modeling tools and model execution. This paper presents an executable language that re-factors the SysML language schema and offers an equivalent textual syntax for model specification in tandem with the existing graphical syntax. This is aimed at supporting the development of time based simulation models useful for decision support and architecture verification and validation in systems engineering.},
	booktitle = {Proceedings of the {Model}-{Driven} {Approaches} for {Simulation} {Engineering} {Symposium}},
	publisher = {Society for Computer Simulation International},
	author = {Amissah, Matthew and Toba, Ange-Lionel and Handley, Holly A. H. and Seck, Mamadou},
	year = {2018},
	note = {event-place: Baltimore, Maryland},
	keywords = {SysML, model based systems engineering, model driven engineering},
}

@inproceedings{el_husseini_query_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Query {Optimization} for {Ontology}-{Mediated} {Query} {Answering}},
	isbn = {979-8-4007-0171-9},
	url = {https://doi.org/10.1145/3589334.3645567},
	doi = {10.1145/3589334.3645567},
	abstract = {Ontology-mediated query answering (OMQA) consists in asking database queries on knowledge bases (KBs); a KB is a set of facts called the KB's database, which is described by domain knowledge called the KB's ontology. A widely-investigated OMQA technique is FO-rewriting: every query asked on a KB is reformulated w.r.t. the KB's ontology, so that its answers are computed by the relational evaluation of the query reformulation on the KB's database. Crucially, because FO-rewriting compiles the domain knowledge relevant to queries into their reformulations, query reformulations may be complex and their optimization is the crux of efficiency. We devise a novel optimization framework for a large set of OMQA settings that enjoy FO-rewriting: conjunctive queries, i.e., the core select-project-join queries, asked on KBs expressed using datalog+/-, description logics, existential rules, OWL, or RDFS. We optimize the query reformulations produced by state-of-the-art FO-rewriting algorithms by computing rapidly, with the help of a KB's database summary, simpler (contained) queries with the same answers that can be evaluated faster by RDBMSs. We show on a well-established OMQA benchmark that time performance is significantly improved by our optimization framework in general, up to three orders of magnitude.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {El Husseini, Wafaa and El Vaigh, Cheikh Brahim and Goasdoué, François and Jaudoin, Hélène},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {data summarization, existential rules, query optimization},
	pages = {2138--2148},
}

@article{zaji_ontology-based_2023,
	title = {Ontology-{Based} {Driving} {Simulation} for {Traffic} {Lights} {Optimization}},
	volume = {14},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3579839},
	doi = {10.1145/3579839},
	abstract = {Traffic lights optimization is one of the principal components to lessen the traffic flow and travel time in an urban area. The present article seeks to introduce a novel procedure to design the traffic lights in a city using evolutionary-based optimization algorithms in combination with an ontology-based driving behavior simulation framework. Accordingly, an ontology-based knowledge base is introduced to provide a machine-understandable knowledge of roads and intersections, traffic rules, and driving behaviors. Then, a simulation environment is developed to inspect car behavior in real time. To optimize the traffic lights, a sine-based equation was defined for each traffic light, and the total travel time of the vehicles was considered as the cost function in the optimization algorithm. The optimization was performed with 5, 10, 15, 20, 25, and 30 vehicles in the urban areas. Based on the results, in contrast to uncontrolled intersections without traffic lights, optimized traffic lights can significantly contribute to total travel time-saving. To conclude, due to an escalation in the number of vehicles, the significance of optimized traffic lights has encountered an increase, and unoptimized traffic lights could increase total travel time even more than a city deprived of any traffic light.},
	number = {3},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Zaji, Amirhossein and Liu, Zheng and Bando, Takashi and Zhao, Lihua},
	month = mar,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, knowledge representation, autonomous car, driving behavior, Evolutionary optimization, traffic light},
}

@article{halevy_will_2023,
	title = {Will {LLMs} {Reshape}, {Supercharge}, or {Kill} {Data} {Science}? ({VLDB} 2023 {Panel})},
	volume = {16},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3611540.3611634},
	doi = {10.14778/3611540.3611634},
	abstract = {Large language models (LLMs) have recently taken the world by storm, promising potentially game changing opportunities in multiple fields. Naturally, there is significant promise in applying LLMs to the management of structured data, or more generally, to the processes involved in data science. At the very least, LLMs have the potential to provide substantial advancements in long-standing challenges that our community has been tackling for decades. On the other hand, they may introduce completely new capabilities that we have only dreamed of thus far. This panel will bring together a few leading experts who have been thinking about these opportunities from various perspectives and fielding them in research prototypes and even in commercial applications.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Halevy, Alon and Choi, Yejin and Floratou, Avrilia and Franklin, Michael J. and Noy, Natasha and Wang, Haixun},
	month = aug,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {4114--4115},
}

@inproceedings{seaborn_cross-cultural_2024,
	address = {New York, NY, USA},
	series = {{CUI} '24},
	title = {Cross-{Cultural} {Validation} of {Partner} {Models} for {Voice} {User} {Interfaces}},
	isbn = {979-8-4007-0511-3},
	url = {https://doi.org/10.1145/3640794.3665537},
	doi = {10.1145/3640794.3665537},
	abstract = {Recent research has begun to assess people’s perceptions of voice user interfaces (VUIs) as dialogue partners, termed partner models. Current self-report measures are only available in English, limiting research to English-speaking users. To improve the diversity of user samples and contexts that inform partner modelling research, we translated, localized, and evaluated the Partner Modelling Questionnaire (PMQ) for non-English speaking Western (German, n=185) and East Asian (Japanese, n=198) cohorts where VUI use is popular. Through confirmatory factor analysis (CFA), we find that the scale produces equivalent levels of “goodness-to-fit” for both our German and Japanese translations, confirming its cross-cultural validity. Still, the structure of the communicative flexibility factor did not replicate directly across Western and East Asian cohorts. We discuss how our translations can open up critical research on cultural similarities and differences in partner model use and design, whilst highlighting the challenges for ensuring accurate translation across cultural contexts.},
	booktitle = {Proceedings of the 6th {ACM} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Seaborn, Katie and Gessinger, Iona and Yoshida, Suzuka and Cowan, Benjamin R. and Doyle, Philip R.},
	year = {2024},
	note = {event-place: Luxembourg, Luxembourg},
	keywords = {human-computer interaction, conversational user interfaces, cross-cultural research, human-machine dialogue, mental models, partner models, speech interfaces, voice user interfaces},
}

@inproceedings{jiang_evidence_2025,
	address = {New York, NY, USA},
	series = {{NLPIR} '24},
	title = {Evidence {Extraction} for {Automated} {Medical} {Coding}: {Preliminary} {Evaluation}},
	isbn = {979-8-4007-1738-3},
	url = {https://doi.org/10.1145/3711542.3711580},
	doi = {10.1145/3711542.3711580},
	abstract = {Coding clinical texts in standard language such as ICD is an important but tedious and error-prone process. Automated medical coding algorithms suffer problems due to the combined the challenge of handling the significant length of clinical text, the complexity of the huge code hierarchy and the lack of interpretability to ensure user trust. Large language models (LLM) have also been proven struggling with this task in recent studies. Recent efforts have been made to annotate an evidence-supported medical coding dataset. The current study makes the first empirical investigation into how well (small) fine-tuned pretrained language models (PLM) and LLMs could identify the sentences containing medical evidence supporting the assigned codes. Hierarchical sequential sentence classification and GPT-3.5 in the zero-shot setting were tested for evidence sentence extraction. Extra evaluation was performed to investigate how evidence extraction impacts clinical coding and what implications it has towards the future generation algorithms for automated medical coding.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Xiaorui and Khan, Kulsoom and Vasantha, Sumithra Thinakara and Haider, Sajjad},
	year = {2025},
	keywords = {large language model, code evidence, ICD coding, sequential sentence classification},
	pages = {18--23},
}

@inproceedings{nan_construction_2025,
	address = {New York, NY, USA},
	series = {{ISAIMS} '24},
	title = {Construction and application of medical history knowledge graph based on {UIE} model fine-tuning},
	isbn = {979-8-4007-1782-6},
	url = {https://doi.org/10.1145/3706890.3706997},
	doi = {10.1145/3706890.3706997},
	abstract = {Objective: To achieve the automated extraction of complex medical history knowledge from Chinese electronic medical records, a fine-tuned UIE extraction model is utilized to automatically obtain medical history knowledge and construct a knowledge graph (KG) of medical histories. Method: Taking the current medical history as an example, a fundamental knowledge base of Chinese medical history was first built. Then, based on this knowledge base, a training set was annotated, and the UIE model was fine-tuned using this training set. The fine-tuned UIE model was then used to extract medical history knowledge, which was processed and stored to generate a KG of medical histories. Results: The fine-tuned UIE model achieved entity, relationship, and event extraction tasks. Subsequently, the extracted medical history information was processed and stored, successfully constructing a KG of the current medical history. Conclusion: This method realizes the completion of entity, relation, and event extraction tasks by training only one model, efficiently achieving automatic extraction of specified medical history knowledge from Chinese electronic medical records and constructing a KG of medical histories. It helps organize and analyze complex medical history knowledge for clinical use, offering practical value.},
	booktitle = {Proceedings of the 2024 5th {International} {Symposium} on {Artificial} {Intelligence} for {Medicine} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Nan, Beier and Gu, Jinguang and Qiu, Chen and Wu, Jingyun},
	year = {2025},
	keywords = {Information extraction, Constructing KG, Medical history knowledge, Model fine-tuning},
	pages = {621--626},
}

@article{ozcep_embedding_2024,
	title = {Embedding {Ontologies} in the {Description} {Logic} {ALC} by {Axis}-{Aligned} {Cones}},
	volume = {78},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.13939},
	doi = {10.1613/jair.1.13939},
	abstract = {This paper is concerned with knowledge graph embedding with background knowledge, taking the formal perspective of logics. In knowledge graph embedding, knowledge— expressed as a set of triples of the form (a R b) (“a is R-related to b”)—is embedded into a real-valued vector space. The embedding helps exploiting geometrical regularities of the space in order to tackle typical inductive tasks of machine learning such as link prediction. Recent embedding approaches also consider incorporating background knowledge, in which the intended meanings of the symbols a, R, b are further constrained via axioms of a theory. Of particular interest are theories expressed in a formal language with a neat semantics and a good balance between expressivity and feasibility. In that case, the knowledge graph together with the background can be considered to be an ontology. This paper develops a cone-based theory for embedding in order to advance the expressivity of the ontology: it works (at least) with ontologies expressed in the description logic ALC, which comprises restricted existential and universal quantifiers, as well as concept negation and concept disjunction. In order to align the classical Tarskian Style semantics for ALC with the sub-symbolic representation of triples, we use the notion of a geometric model of an ALC ontology and show, as one of our main results, that an ALC ontology is satisfiable in the classical sense iff it is satisfiable by a geometric model based on cones. The geometric model, if treated as a partial model, can even be chosen to be faithful, i.e., to reflect all and only the knowledge captured by the ontology. We introduce the class of axis-aligned cones and show that modulo simple geometric operations any distributive logic (such as ALC) interpreted over cones employs this class of cones. Cones are also attractive from a machine learning perspective on knowledge graph embeddings since they give rise to applying conic optimization techniques.},
	journal = {J. Artif. Int. Res.},
	author = {Özcep, Özgür Lütfü and Leemhuis, Mena and Wolter, Diedrich},
	month = jan,
	year = {2024},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
}

@inproceedings{paulus_plasma_2022,
	address = {New York, NY, USA},
	series = {{CIKM} '22},
	title = {{PLASMA}: {A} {Semantic} {Modeling} {Tool} for {Domain} {Experts}},
	isbn = {978-1-4503-9236-5},
	url = {https://doi.org/10.1145/3511808.3557184},
	doi = {10.1145/3511808.3557184},
	abstract = {In recent years, Knowledge Graphs and Ontology-based Data Management have proven to be particularly effective in the efficient management and consolidation of heterogeneous data sources. In this context, semantic modeling has proven to be a useful approach for creating semantic data annotations. However, automatically generated semantic models usually need to be revised by a domain expert, who is often not familiar with semantic technologies. For addressing this issue, we propose the PLASMA semantic modeling tool, which aims at enabling domain experts to build semantic models from scratch or refine models created by automatic algorithms. We demonstrate the use of the tool and its user interface in two different semantic data management use cases for integrating smart city data in a public funded project, called City Dataspace, and for creating semantic models in an industrial use case at Siemens AG.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Paulus, Alexander and Burgdorf, Andreas and Langer, Tristan and Pomp, André and Meisen, Tobias and Pol, Sebastian},
	year = {2022},
	note = {event-place: Atlanta, GA, USA},
	keywords = {resource description framework, graphical user interface, semantic modeling, semantic refinement},
	pages = {4946--4950},
}

@inproceedings{gao_research_2025,
	address = {New York, NY, USA},
	series = {{BDEIM} '24},
	title = {Research on the {Evolutionary} {Path} of {Public} {Opinion} in {Networked} {Emergencies} {Based} on {Large} {Models}——{Taking} the {Gansu} earthquake as an example},
	isbn = {979-8-4007-1186-2},
	url = {https://doi.org/10.1145/3724154.3724349},
	doi = {10.1145/3724154.3724349},
	abstract = {Construct a rational map of online public opinion on emergencies, reveals the evolutionary paths and developmental contexts of public opinion incidents, providing decision support for government departments in managing and guiding public sentiments. Select Sina Weibo as the data source, use octopus collector to collect Gansu earthquake Weibo data, and then perform data preprocessing. The causal relationships of the events were determined based on a rule-based template matching method, and a multi-round dialogue mechanism based on large language models was used to automatically extract causal event pairs. The K-means++ clustering algorithm was employed to generalize the events and construct a network public opinion event logic graph, analyzing the evolutionary paths of online public sentiments. The research findings indicate that public sentiment regarding the Gansu earthquake is characterized by multiple levels and complex causal relationships. The involvement of multiple causal chains contributes to a high degree of complexity in the evolution of public sentiment. By constructing an event logic graph, the development process of the Gansu earthquake public opinion event has been demonstrated, which can provide technical support for real-time monitoring, in-depth analysis, and precise response of public opinion.},
	booktitle = {Proceedings of the 2024 5th {International} {Conference} on {Big} {Data} {Economy} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Gao, Juan and Guo, Xiangyun and Xiang, Tianqi},
	year = {2025},
	keywords = {emergencies, evolutionary pathways, large model, Multiple rounds of dialogue, online public opinion},
	pages = {1206--1211},
}

@article{ma_clapsep_2024,
	title = {{CLAPSep}: {Leveraging} {Contrastive} {Pre}-{Trained} {Model} for {Multi}-{Modal} {Query}-{Conditioned} {Target} {Sound} {Extraction}},
	volume = {32},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2024.3497586},
	doi = {10.1109/TASLP.2024.3497586},
	abstract = {Universal sound separation (USS) aims to extract arbitrary types of sounds from real-world recordings. This can be achieved by language-queried target sound extraction (TSE), which typically consists of two components: a query network that converts user queries into conditional embeddings, and a separation network that extracts the target sound accordingly. Existing methods commonly train models from scratch. As a consequence, substantial data and computational resources are required to make the randomly initialized model comprehend sound events and perform separation accordingly. In this paper, we propose to integrate pre-trained models into TSE models to address the above issue. To be specific, we tailor and adapt the powerful contrastive language-audio pre-trained model (CLAP) for USS, denoted as CLAPSep. CLAPSep also accepts flexible user inputs, taking both positive and negative user prompts of uni- and/or multi-modalities for target sound extraction. These key features of CLAPSep can not only enhance the extraction performance but also improve the versatility of its application. We provide extensive experiments on 5 diverse datasets to demonstrate the superior performance and zero- and few-shot generalizability of our proposed CLAPSep with fast training convergence, surpassing previous methods by a significant margin. Full codes and some audio examples are released for reproduction and evaluation.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Ma, Hao and Peng, Zhiyuan and Li, Xu and Shao, Mingjie and Wu, Xixin and Liu, Ju},
	month = nov,
	year = {2024},
	note = {Publisher: IEEE Press},
	pages = {4945--4960},
}

@inproceedings{jia_structrag_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{StructRAG}: {Structure}-{Aware} {RAG} {Framework} with {Scholarly} {Knowledge} {Graph} for {Diverse} {Question} {Answering}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3717819},
	doi = {10.1145/3701716.3717819},
	abstract = {Recent advances in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) have shown promise in academic question answering. However, existing approaches often fail to fully utilize document structural information and lack diversity in retrieved contexts. This paper presents StructRAG, a structure-aware RAG framework that leverages scholarly knowledge graphs for enhanced question answering. Our framework features three key innovations: (1) an automated knowledge graph construction pipeline based on Deep Document Model (DDM) that preserves document hierarchical structure, (2) a structure-aware retrieval mechanism that combines semantic relevance with source diversity, and (3) a context-enhanced generation approach that integrates structural metadata for improved answer synthesis. Experimental results on 329 computer science papers demonstrate that StructRAG significantly outperforms vanilla RAG baseline. While maintaining comparable semantic accuracy (91\% vs 90\%), our approach achieves substantially higher diversity in generated answers (Distinct-1: 62\% vs 52\%, Distinct-2: 89\% vs 78\%) and better answer quality across all metrics, with notable improvements in relevance (29\%) and readability (36.5\%). These results demonstrate that StructRAG effectively enhances both the diversity and quality of academic question answering.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Jia, Runsong and Zhang, Bowen and Méndez, Sergio José Rodríguez and Omran, Pouya G.},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language models, knowledge graph, retrieval-augmented generation, knowledge graph construction, deep document model},
	pages = {2567--2573},
}

@inproceedings{stefanidis_icarus_2020,
	address = {New York, NY, USA},
	series = {{WIMS} 2020},
	title = {The {ICARUS} {Ontology}: {A} general aviation ontology developed using a multi-layer approach},
	isbn = {978-1-4503-7542-9},
	url = {https://doi.org/10.1145/3405962.3405983},
	doi = {10.1145/3405962.3405983},
	abstract = {The management of aviation data is a great challenge in the aviation industry, as they are complex and can be derived from heterogeneous data sources. To handle this challenge, ontologies can be applied to facilitate the modelling of the data across multiple data sources. This paper presents an aviation domain ontology, the ICARUS ontology, which aims at facilitating the semantic description and integration of information resources that represent the various assets of the ICARUS platform and their use. To present the functionality and usability of the proposed ontology, we present the results of querying the ontology using SPARQL queries through three use case scenarios. As shown from the evaluation, the ICARUS ontology enables the integration and reasoning over multiple sources of heterogeneous aviation-related data, the semantic description of metadata produced by ICARUS, and their storage in a knowledge-base which is dynamically updated and provides access to its contents via SPARQL queries.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Web} {Intelligence}, {Mining} and {Semantics}},
	publisher = {Association for Computing Machinery},
	author = {Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis and Symeonidis, Moysis and Pallis, George and Dikaiakos, Marios and Pouis, Loukas and Orphanou, Kalia and Lampathaki, Fenareti and Alexandrou, Dimitrios},
	year = {2020},
	note = {event-place: Biarritz, France},
	keywords = {ontology, services, queries, datasets, aviation},
	pages = {21--32},
}

@inproceedings{vu_d-repr_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {D-{REPR}: {A} {Language} for {Describing} and {Mapping} {Diversely}-{Structured} {Data} {Sources} to {RDF}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364449},
	doi = {10.1145/3360901.3364449},
	abstract = {Publishing data sources to knowledge graphs is a complicated and laborious process as data sources are often heterogeneous, hierarchical and interlinked. As an example, food price datasets may contain product prices of various units at different markets and times, and different providers can have many choices of formats such as CSV, JSON or spreadsheet. Beyond data formats, these datasets may have differing layout, where one dataset may be organized as a row-based table or relational table (prices are in one column), while another may use a matrix table (prices are in one matrix). To address these problems, we present a novel data description language for mapping datasets to RDF. In particular, our language supports specifying the locations of source attributes in the sources, mapping of the attributes to ontologies, and simple rules to join the data of these attributes to output final RDF triples. Unlike existing approaches, our language is not restricted to specific data layouts such as the Nested Relational Model, or to specific data formats, such as spreadsheet. Our broad data description language presents a format-independent solution, allowing interlinking among multiple heterogeneous sources and representing many diverse data structures that existing tools are unable to handle.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Vu, Binh and Pujara, Jay and Knoblock, Craig A.},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {knowledge graph, linked data, rdf mapping},
	pages = {189--196},
}

@inproceedings{li_construction_2023,
	address = {New York, NY, USA},
	series = {{CCRIS} '23},
	title = {Construction of {Traditional} {Culture} {Ontology} {Based} on {Representation} and {Role}},
	isbn = {979-8-4007-0819-0},
	url = {https://doi.org/10.1145/3622896.3622922},
	doi = {10.1145/3622896.3622922},
	abstract = {Traditional culture refers to a culture that has evolved from civilization and can reflect the characteristics and spirit of a nation. However, at present, the traditional cultural ontology only focuses on modeling and data organization of a certain type of traditional culture, which is a certain distance from the massive cultural information and urgent cultural needs. This article starts from the perspective of protecting and inheriting traditional culture, and based on representation, incorporates role theory to construct a cultural ontology centered on poetry, calligraphy, and painting. The traditional cultural ontology constructed in this article can further contribute to the field of artificial intelligence.},
	booktitle = {Proceedings of the 2023 4th {International} {Conference} on {Control}, {Robotics} and {Intelligent} {System}},
	publisher = {Association for Computing Machinery},
	author = {Li, Min and Xu, Jianliang and Wu, Xiaoquan},
	year = {2023},
	note = {event-place: Guangzhou, China},
	keywords = {representation ontology, role theory, traditional culture},
	pages = {156--159},
}

@inproceedings{man_model_2025,
	address = {New York, NY, USA},
	series = {{ISAIMS} '24},
	title = {A {Model} for {Epilepsy} {Named} {Entity} {Recognition} {Based} on {Chinese} {EEG} {Reports}},
	isbn = {979-8-4007-1782-6},
	url = {https://doi.org/10.1145/3706890.3706971},
	doi = {10.1145/3706890.3706971},
	abstract = {It is critical for clinical diagnosis and research to extract and utilize the medical information from electroencephalogram (EEG) reports of epilepsy. With the widespread application of deep learning techniques in health care, particularly in natural language processing tasks, named entity recognition (NER) has emerged as an essential tool for information extraction from medical text. This study proposed an epilepsy NER model for EEG reports to improve the efficiency of epilepsy text processing. 17,606 paragraphs from real Chinese epilepsy EEG reports as data samples, were meticulously annotated with 17 entity types by clinical experts. The model used the BERT and the Global Pointer (GP) algorithm to identify nested and non-nested entities, achieved outstanding performance in the Precision, Recall and F1 score. The experimental results demonstrate that our method significantly enhances the effectiveness of epilepsy entity recognition, providing robust support for the automated extraction and analysis of medical information.},
	booktitle = {Proceedings of the 2024 5th {International} {Symposium} on {Artificial} {Intelligence} for {Medicine} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Man, Jianping and Hu, Zhensheng and Liu, Hongze and Yang, Rui and Liu, Jingjing and Chen, Ziyi and Zhou, Yi},
	year = {2025},
	keywords = {Named entity recognition, BERT model, Epilepsy, EEG reports, Global Pointer algorithm},
	pages = {467--472},
}

@inproceedings{delaney_natural_2018,
	address = {New York, NY, USA},
	series = {{AICCC} '18},
	title = {Natural {Language} {Processing} for {Productivity} {Metrics} for {Software} {Development} {Profiling} in {Enterprise} {Applications}},
	isbn = {978-1-4503-6623-6},
	url = {https://doi.org/10.1145/3299819.3299830},
	doi = {10.1145/3299819.3299830},
	abstract = {In this paper, we utilize ontology-based information extraction for semantic analysis and terminology linking from a corpus of software requirement specification documents from 400 enterprise-level software development projects. The purpose for this ontology is to perform semi-supervised learning on enterprise-level specification documents towards an automated method of defining productivity metrics for software development profiling. Profiling an enterprise-level software development project in the context of productivity is necessary in order to objectively measure productivity of a software development project and to identify areas of improvement in software development when compared to similar software development profiles or benchmark of these profiles. We developed a semi-novel methodology of applying NLP OBIE techniques towards determining software development productivity metrics, and evaluated this methodology on multiple practical enterprise-level software projects.},
	booktitle = {Proceedings of the 2018 {Artificial} {Intelligence} and {Cloud} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Delaney, Steven and Chan, Christopher Chun Ki and Smith, Doug},
	year = {2018},
	note = {event-place: Tokyo, Japan},
	keywords = {Natural Language Processing, Software Development, Software Development Productivity},
	pages = {83--87},
}

@inproceedings{tramontana_keeping_2022,
	address = {New York, NY, USA},
	series = {{ICCCM} '22},
	title = {Keeping {Researchers} {Updated} by {Automatically} {Enriching} an {Ontology} in the {Medical} {Field}},
	isbn = {978-1-4503-9634-9},
	url = {https://doi.org/10.1145/3556223.3556262},
	doi = {10.1145/3556223.3556262},
	abstract = {Ontologies provide a common and standard dictionary of terms in some domain for researchers to easily exchange data. Forming an ontology requires several years of work performed by human experts, and an ontology for a given domain is thought to be stable for many years. Nevertheless, as scientific articles are continuously published to gather knowledge on recent findings, existing ontologies risk becoming stale, or require further human effort. Moreover, searching new articles without referring to an ontology can be very time consuming and confusing, especially for novice researchers. We propose an approach for automatically relating newly available published articles to existing ontologies. By automatically selecting relevant scientific articles and making them appear besides other data in an ontology, we aim at supporting experienced and novice researchers. Therefore, as knowledge grows and articles are available, the ontology used by researchers will also be automatically connected, allowing them to readily discover new findings. To validate the effectiveness of the proposed approach, we have enriched OBIB, an ontology for biobanking, with a selection of articles extracted from PubMed. The approach is general enough and can be applied to other ontologies or publishers.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Computer} and {Communications} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Tramontana, Emiliano and Verga, Gabriella},
	year = {2022},
	note = {event-place: Okayama, Japan},
	keywords = {Ontologies, Ontology enrichment, Knowledge management, PubMed, e-learning},
	pages = {257--262},
}

@article{koho_wikibase_2023,
	title = {Wikibase {Model} for {Premodern} {Manuscript} {Metadata} {Harmonization}, {Linked} {Data} {Integration}, and {Discovery}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3594723},
	doi = {10.1145/3594723},
	abstract = {To facilitate discovery of premodern manuscripts in U.S. memory institutions, Digital Scriptorium, a growing consortium of over 35 institutional members representing American libraries, museums, and other cultural heritage institutions, has developed a digital platform for an online national union catalog. The platform will allow low-barrier and efficient collection, aggregation, and enrichment of member metadata and sustainably publish it as Linked Open Data. This article describes the methods and principles behind the data model development and the decision to use Wikibase. The results of the prototype implementation and testing phase demonstrate the practicality and sustainability of Digital Scriptorium’s approach to building an online national union catalog based on Linked Open Data technologies and practices.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Koho, Mikko and Coladangelo, L. P. and Ransom, Lynn and Emery, Doug},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {data interoperability, Wikibase, semantic web, Linked Open Data, data modeling, cultural heritage, digital humanities, premodern manuscripts},
}

@inproceedings{ojino_users_2019,
	address = {New York, NY, USA},
	series = {{SAC} '19},
	title = {User's profile ontology-based semantic model for personalized hotel room recommendation in the web of things: student research abstract},
	isbn = {978-1-4503-5933-7},
	url = {https://doi.org/10.1145/3297280.3297661},
	doi = {10.1145/3297280.3297661},
	abstract = {The hotel industry is not exempt from technology disruption and will only keep its competitiveness through strategy [3]including adjusting to the new customers demands, reacting to competitors' innovations and taking advantage of the new technological developments. Personalization can enable an organization to customize its offerings at the individual level. This paper seeks to design and evaluate a novel Web of Things personalization model that effectively utilizes information about a customer's preferences to create a personalized service space.},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Ojino, Ronald},
	year = {2019},
	note = {event-place: Limassol, Cyprus},
	keywords = {machine learning, ontology, semantics, personalization},
	pages = {2314--2316},
}

@inproceedings{togatorop_implementation_2021,
	address = {New York, NY, USA},
	series = {{SIET} '20},
	title = {Implementation of ontology-based on {Word2Vec} and {DBSCAN} for part-of-speech},
	isbn = {978-1-4503-7605-1},
	url = {https://doi.org/10.1145/3427423.3427431},
	doi = {10.1145/3427423.3427431},
	abstract = {POS tagging is a process of marking text into an appropriate word-class based on word definitions and word relationships. In general, several POS tagging approaches have been applied in Bahasa Indonesia namely rule-based, stochastic, and neural. Besides, there is another approach to POS tagging which has been applied to English, namely the approach using ontology. This approach has not yet been applied to Bahasa Indonesia so we will implement an ontology to conduct POS tagging in Bahasa Indonesia. In this study, the ontology was constructed using the Word2Vec and the DBSCAN clustering method. The Word2Vec model is implemented to extract each word in vector form based on its context and the DBSCAN clustering method is implemented for the classification process of word classes based on word vectors modeled by Word2Vec. The process of POS tagging with ontology is carried out in several stages, namely: data collection using web scraping techniques from Kompas.com and Detik.com online news articles, text preprocessing, Word2Vec feature building, clustering with DBSCAN, ontology construction and evaluation. The experiments carried out in this study were to choose the optimal parameter values from DBSCAN in forming word clusters for ontology construction. Overall, the implementation of ontology with Word2Vec and DBSCAN can do POS tagging with the highest accuracy value of 0.62, the highest precision value of 0.79, the highest recall value of 0.62, and the highest f1-score of 0.67.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Sustainable} {Information} {Engineering} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Togatorop, Parmonangan R. and Siagian, Rosa and Nainggolan, Yolanda and Simanungkalit, Kaleb},
	year = {2021},
	note = {event-place: Malang, Indonesia},
	keywords = {ontology, DBSCAN, Word2Vec, POS tagging, bahasa indonesia},
	pages = {51--56},
}

@article{subagdja_disambiguart_2025,
	title = {{DisambiguART}: {A} {Neural}-based {Inference} {Model} for {Knowledge} {Graph} {Disambiguation}},
	volume = {19},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3737880},
	doi = {10.1145/3737880},
	abstract = {One main challenge in constructing a Knowledge Graph (KG) is to deal with ambiguity. Specifically, an entity in the graph can be assigned with multiple meanings while two or more entities considered to have different meanings may actually be the same. Assigning an entity with the correct meaning may involve re-evaluation of its relevant contexts. This costly operation typically involves searching for other similar entities within the KG such that the context can be determined. In this article, a new model called DisambiguART is proposed leveraging multi-channel matching and inference in a self-organizing neural network for sense disambiguation in KGs. Unlike other disambiguation methods that rely on representation learning to identify the relevant contexts whereby similarities among entities are learned, DisambiguART extends the working principle of multi-channel Adaptive Resonance Theory (ART) to conduct inferences directly over the graph representation through bi-directional interactions of bottom-up activations and top-down matching to find similar entities and select the correct meaning according to the right context. The proposed method is evaluated on the tasks of entity sense disambiguation in three domain KGs (jet engine, biomedical, and kinship) and author name disambiguation in bibliographic KGs, demonstrating the effectiveness and efficiency of DisambiguART against the state-of-the-art methods.},
	number = {6},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Subagdja, Budhitama and Shanthoshigaa, D. and Tan, Ah-Hwee},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge Graphs, Adaptive Resonance Theory, Entity Disambiguation, Graph Embeddings},
}

@inproceedings{yingbo_business_2019,
	address = {New York, NY, USA},
	series = {{ICFET} '19},
	title = {Business {Modeling} and {Reasoning} {Based} on {Process} {Ontology}},
	isbn = {978-1-4503-6293-1},
	url = {https://doi.org/10.1145/3338188.3338215},
	doi = {10.1145/3338188.3338215},
	abstract = {In this paper, a method of business modeling and reasoning based on process ontology is proposed to solve the problem that existing in the banking business field such as large amount of data and the complexity to analysis the relationship between data. We use process ontology technology to describe each basic element in the business process, and on this basis, design constraints and inference rules, and carry out corresponding ontology knowledge reasoning by use the inference rules. Practice results show that the establishment of process ontology model can effectively represent the knowledge involved in the business process and discover the hidden information contained in the knowledge.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Frontiers} of {Educational} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Yingbo, Dong and Xia, Hou},
	year = {2019},
	note = {event-place: Beijing, China},
	keywords = {knowledge reasoning, banking business field, inference rules, process ontology},
	pages = {143--147},
}

@inproceedings{lyons_towards_2021,
	address = {New York, NY, USA},
	series = {{BCB} '21},
	title = {Towards an extensible ontology for streaming sensor data for clinical trials},
	isbn = {978-1-4503-8450-6},
	url = {https://doi.org/10.1145/3459930.3469562},
	doi = {10.1145/3459930.3469562},
	abstract = {The use of wearable sensors for clinical trials can lead to better data collection and a better patient experience during trials, and can further allow more patients to participate in trials by allowing more remote monitoring and fewer site visits. However, extracting maximum value from the data collected via streaming sensors presents some specific technical challenges, including processing the data in real time, and storing the sensor data in a representation that facilitates the use of biomarker algorithms that can be used and reused with different similar sensors, at different scales, and across different clinical trials. Here we present our initial work on SORBET, a Sensor Ontology for Reusable Biometric Expressions and Transformations. Our design strategy is presented, along with the initial design and examples. While this ontology has been created for the Medidata Sensor Cloud product, it is our hope that others working in this space will join us in extending and hardening this ontology, as we expand it to incorporate more sensors and more needs for clinical trials research.},
	booktitle = {Proceedings of the 12th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Lyons, Robert and Low, Geoffrey Ross and Congdon, Clare Bates and Ceruolo, Melissa and Ballesteros, Marissa and Cambria, Steven and DePetrillo, Paolo},
	year = {2021},
	note = {event-place: Gainesville, Florida},
	keywords = {health informatics, ontology engineering, semantic networks},
}

@inproceedings{nguyen_model_2024,
	address = {New York, NY, USA},
	series = {{ICIIT} '24},
	title = {A {Model} of {Topic} for {Document} {Retrieval} {Systems} in the {Field} of {Artificial} {Intelligence} for {Information} {Technology} students},
	isbn = {979-8-4007-1671-3},
	url = {https://doi.org/10.1145/3654522.3654561},
	doi = {10.1145/3654522.3654561},
	abstract = {This article proposes a topic definition, topic modeling and document search techniques related to topics in the field of artificial intelligence, thereby building a document search application based on topic. Document warehouse includes ebooks and papers. The application will serve the need to search for documents by topic for information technology students during the learning and research process, to solve that problem. Realizing that Ontology and document search techniques are a suitable and powerful approach for organizing the knowledge base as well as solving the problem of retrieving documents by topic with high efficiency. From there, the following databases are built: the Ebook and Paper database is stored in a structured form, the topic database is to store a set of topics in the field of artificial intelligence, Ontology database for ebooks and papers, keyphrase graph database to store topic and document representations and semantic similarity matching techniques between documents and topics.},
	booktitle = {Proceedings of the 2024 9th {International} {Conference} on {Intelligent} {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Dien Thanh and Do, Nhon Van and Tran, Tung Hoang},
	year = {2024},
	note = {event-place: Ho Chi Minh City, Vietnam},
	keywords = {domain ontology, topic modeling, graph matching, Additional Key Words and Phrases: Topic definition, topic representation},
	pages = {376--385},
}

@inproceedings{rossi_towards_2024,
	address = {New York, NY, USA},
	series = {{SESoS} '24},
	title = {Towards {Model}-{Driven} {Dashboard} {Generation} for {Systems}-of-{Systems}},
	isbn = {979-8-4007-0557-1},
	url = {https://doi.org/10.1145/3643655.3643876},
	doi = {10.1145/3643655.3643876},
	abstract = {Configuring and evolving dashboards in complex and large-scale Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the many Key Performance Indicators (KPIs) that are usually collected and have to be arranged in a number of visualizations. Unfortunately, setting up dashboards is still a largely manual and error-prone task requiring extensive human intervention.This short paper describes emerging results about the definition of a model-driven technology-agnostic approach that can automatically transform a simple list of KPIs into a dashboard model, and then translate the model into an actual dashboard for a target dashboard technology. Dashboard customization can be efficiently obtained by solely modifying the abstract model representation, freeing operators from expensive interactions with actual dashboards.},
	booktitle = {Proceedings of the 12th {ACM}/{IEEE} {International} {Workshop} on {Software} {Engineering} for {Systems}-of-{Systems} and {Software} {Ecosystems}},
	publisher = {Association for Computing Machinery},
	author = {Rossi, Maria Teresa and Tundo, Alessandro and Mariani, Leonardo},
	year = {2024},
	note = {event-place: Lisbon, Portugal},
	keywords = {model-driven engineering, automatic dashboard generation, model-based dashboard, monitoring dashboard, systems of systems},
	pages = {9--12},
}

@inproceedings{boubekeur_dsl_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {A {DSL} and model transformations to specify learning corpora for modeling assistants},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3556502},
	doi = {10.1145/3550356.3556502},
	abstract = {Software engineering undergraduate students spend a significant time learning various topics related to software design, including notably model-driven engineering (MDE), where different types of structural and behavioral models are used to design, implement, and validate an application. MDE instructors spend a lot of time covering modeling concepts, which is more difficult with ever-increasing class sizes. Online resources, such as learning corpora for domain modeling, can aid in this learning process by serving as a more dynamic textbook alternative or as part of a larger interactive application with domain modeling exercises and tutorials. A Learning Corpus (LC) is an extensible list of entries representing possible mistakes that could occur when defining a model, e.g., Missing Abstraction-Occurrence pattern in the case of a domain model. Each LC entry includes progressive levels of feedback, including written responses, quizzes, and references to external resources. To make it easy for instructors to customize the entries as well as add their own, we propose a novel, simple, and intuitive approach based on an internal domain-specific language that supports features such as context-specific information and concise arbitrary metamodel navigation with shorthands. Transformations to source code as well as Markdown and LATEX enable use of the LC entries in different contexts. These transformations as well as the integration of the generated code in a sample Modeling Assistant application verify and validate the LC metamodel and specification.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Boubekeur, Younes and Singh, Prabhsimran and Mussbacher, Gunter},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {domain modeling, model transformation, feedback, learning corpus, model-driven engineering (MDE)},
	pages = {95--102},
}

@inproceedings{iglesias-molina_re-construction_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {Re-{Construction} {Impact} on {Metadata} {Representation} {Models}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627554},
	doi = {10.1145/3587259.3627554},
	abstract = {Reification in knowledge graphs has been present since the inception of RDF to allow capturing additional information in triples, usually metadata. The need of adopting or changing a metadata representation in a pre-existing graph to enhance the knowledge capture and access can lead to inducing complex structural changes in the graph, according the target representation’s schema. In these situations, it is necessary to decide whether to construct the knowledge graph again from its original sources, or to re-construct it using the current version of the graph. In this paper we conduct an empirical study to analyze which re-construction approach is more suitable for switching the representation approach from the created graph ensuring that the additional represented knowledge is preserved. We study four well-known metadata representations, using mapping languages to construct the graph, and SPARQL CONSTRUCT queries to re-construct it. With this work we aim to provide insights about the impact of re-construction on metadata representations interoperability and the implications of different approaches.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Iglesias-Molina, Ana and Toledo, Jhon and Corcho, Oscar and Chaves-Fraga, David},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {SPARQL, Knowledge Graphs, Metadata, Declarative Mappings.},
	pages = {197--205},
}

@inproceedings{jeromela_devising_2024,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '24},
	title = {Devising {Scrutable} {User} {Models} for {Time} {Management} {Assistants}},
	isbn = {979-8-4007-0466-6},
	url = {https://doi.org/10.1145/3631700.3665182},
	doi = {10.1145/3631700.3665182},
	abstract = {Intelligent Personal Assistants (IPAs) have become ubiquitous through integration into smartphones, smart speakers, and standalone devices. However, prior studies raised noteworthy usability concerns and determined that IPAs remain primarily used for simple tasks. Such findings contrast with the reported user aspirations for more proactive and truly personalised IPAs. By focusing on the use case of time management, in this paper, we contemplate how scrutability – i.e. the ability of the user to study their assistant and its underlying user model – fits within the vision of more complex IPAs. Furthermore, we describe our ongoing project investigating user interest in and expectations of the scrutability of a proactive calendaring assistant. Lastly, by deliberating on the challenges and benefits of making IPAs scrutable, this paper outlines potential avenues for further research.},
	booktitle = {Adjunct {Proceedings} of the 32nd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Jeromela, Jovan and Conlan, Owen},
	year = {2024},
	note = {event-place: Cagliari, Italy},
	keywords = {Explainability, Intelligent personal assistants, Proactive dialogue systems, Time management},
	pages = {250--255},
}

@inproceedings{rajbhoj_automw_2024,
	address = {New York, NY, USA},
	series = {{MODELS} '24},
	title = {{AutoMW}: {Model}-based {Automated} {Medical} {Writing}},
	isbn = {979-8-4007-0504-5},
	url = {https://doi.org/10.1145/3640310.3674096},
	doi = {10.1145/3640310.3674096},
	abstract = {Medical Writing is an art of writing scientific documents which includes regulatory and research-related content. To obtain approval for marketing new medicines, pharmaceutical companies are obligated to provide drug authorities with a huge volume of documents related to clinical trials. Creating these clinical trial documents is a time, effort, and skill-intensive process as the required information exists in fragmented form distributed across various information sources. To overcome these challenges in medical writing, we propose Automated Medical Writing tool (AutoMW). AutoMW enables the digitalization of information from different sources of information using a meta-model-based approach and leverages these models for the automated generation of clinical trial documents as per the regulatory authority document templates. This paper describes the approach and illustrates its utility and efficacy in real-world clinical trial application of two use cases - breast cancer, and diabetes.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rajbhoj, Asha and Pathan, Ajim and Sant, Tanay and Kulkarni, Vinay and Nistala, Padmalata and Pandey, Rajesh and Narasimhan, Sabarinathan and Thiagarajan, Geetha},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {NLP, MDE, Automated Content Generation, Clinical Trial Documentation, Medical Writing},
	pages = {257--267},
}

@inproceedings{ginsbach_candl_2018,
	address = {New York, NY, USA},
	series = {{CC} '18},
	title = {{CAnDL}: a domain specific language for compiler analysis},
	isbn = {978-1-4503-5644-2},
	url = {https://doi.org/10.1145/3178372.3179515},
	doi = {10.1145/3178372.3179515},
	abstract = {Optimizing compilers require sophisticated program analysis and transformations to exploit modern hardware. Implementing the appropriate analysis for a compiler optimization is a time consuming activity. For example, in LLVM, tens of thousands of lines of code are required to detect appropriate places to apply peephole optimizations. It is a barrier to the rapid prototyping and evaluation of new optimizations. In this paper we present the Compiler Analysis Description Language (CAnDL), a domain specific language for compiler analysis. CAnDL is a constraint based language that operates over LLVM's intermediate representation. The compiler developer writes a CAnDL program, which is then compiled by the CAnDL compiler into a C++ LLVM pass. It provides a uniform manner in which to describe compiler analysis and can be applied to a range of compiler analysis problems, reducing code length and complexity. We implemented and evaluated CAnDL on a number of real world use cases: eliminating redundant operations; graphics code optimization; identifying static control flow regions. In all cases were we able to express the analysis more briefly than competing approaches.},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Compiler} {Construction}},
	publisher = {Association for Computing Machinery},
	author = {Ginsbach, Philip and Crawford, Lewis and O'Boyle, Michael F. P.},
	year = {2018},
	note = {event-place: Vienna, Austria},
	keywords = {constraint programming, optimization, LLVM},
	pages = {151--162},
}

@inproceedings{ollier_towards_2023,
	address = {New York, NY, USA},
	series = {{RAPIDO} '23},
	title = {Towards an {Ontological} {Methodology} for {Dynamic} {Dependability} {Management} of {Unmanned} {Aerial} {Vehicles}},
	isbn = {979-8-4007-0045-3},
	url = {https://doi.org/10.1145/3579170.3579265},
	doi = {10.1145/3579170.3579265},
	abstract = {Dynamic Dependability Management (DDM) is a promising approach to guarantee and monitor the ability of safety-critical Automated Systems (ASs) to deliver the intended service with an acceptable risk level. However, the non-interpretability and lack of specifications of the Learning-Enabled Component (LEC) used in ASs make this mission particularly challenging. Some existing DDM techniques overcome these limitations by using probabilistic environmental perception knowledge associated with predicting behavior changes for the agents in the environment. Ontology-based methods allow using a formal and traceable representation of AS usage scenarios to support the design process of the DDM component of such ASs. This paper presents a methodology to perform this design process, starting from the AS specification stage and including threat analysis and requirements identification. The present paper focuses on the formalization of an ontology modeling language allowing the interpretation of logical usage scenarios, i.e., a formal description of the scenario represented by state variables. The proposed supervisory system also considers the uncertainty estimation and interaction between AS components through the whole perception-planning-control pipeline. This methodology is illustrated in this paper on a use case involving Unmanned Aerial Vehicles (UAVs).},
	booktitle = {Proceedings of the {DroneSE} and {RAPIDO}: {System} {Engineering} for {Constrained} {Embedded} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ollier, Guillaume and Arnez, Fabio and Adedjouma, Morayo and Lallement, Raphaël and Gerasimou, Simos and Mraidha, Chokri},
	year = {2023},
	note = {event-place: Toulouse, France},
	keywords = {Autonomous Systems, Cyber-Physical Systems, Dynamic Risk Management, Real-time Monitoring, Safety-critical Systems},
	pages = {12--19},
}

@article{pham_towards_2024,
	title = {Towards {Vietnamese} {Question} and {Answer} {Generation}: {An} {Empirical} {Study}},
	volume = {23},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3675781},
	doi = {10.1145/3675781},
	abstract = {Question-answer generation (QAG) is a challenging task that generates both questions and answers from a given input paragraph context. The QAG task has recently achieved promising results thanks to the appearance of large pre-trained language models, yet, QAG models are mainly implemented in common languages, e.g., English. There still remains a gap in domain and language adaptation of these QAG models to low-resource languages such as Vietnamese. To address the gap, this article presents a large-scale and systematic study of QAG in Vietnamese. To do that, we first implement several QAG models by using the common fine-tuning techniques based on powerful pre-trained language models. We next introduce a set of instructions designed for the QAG task. These instructions are used to fine-tuned the pre-trained language and large language models. Extensive experimental results of both automatic and human evaluation on five benchmark machine reading comprehension datasets show two important points. First, the instruction-tuning method has the potential to enhance the performance of QAG models. Second, large language models trained in English need more data for fine-tuning to work well on the downstream QAG tasks of low-resource languages. We also provide a prototype system to demonstrate how our QAG models actually work. The code for fine-tuning QAG models and instructions are also made available.},
	number = {9},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Pham, Quoc-Hung and Le, Huu-Loi and Dang Nhat, Minh and Tran T., Khang and Tran-Tien, Manh and Dang, Viet-Hung and Vu, Huy-The and Nguyen, Minh-Tien and Phan, Xuan-Hieu},
	month = aug,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural language processing, BARTPho, instruction fine-tuning, large pre-trained language models, LlaMa2, question and answer generation, ViT5},
}

@inproceedings{papadakis_enabling_2025,
	address = {New York, NY, USA},
	series = {{IoT} '24},
	title = {Enabling {IoT}-enhanced {Data} {Models} for {Context}-aware {Hydropower} {Plants}},
	isbn = {979-8-4007-1285-2},
	url = {https://doi.org/10.1145/3703790.3703803},
	doi = {10.1145/3703790.3703803},
	abstract = {Hydroelectric power, or hydropower, harnesses the potential energy of water descending from higher to lower elevations to generate electricity. As a well-established and cost-effective renewable energy technology, it not only produces power but also supports significant water management services. The integration of Internet of Things (IoT) technologies in hydropower plants has shown significant potential in enhancing monitoring, efficiency, and control capabilities. However, current implementations often lack a holistic and standardized approach to contextual modeling. To address this gap, this paper presents a comprehensive approach to modeling the structural and operational components of hydropower plants (HPPs) using NGSI-LD data models. We propose detailed NGSI-LD data models that incorporate both static properties (e.g., location, structural attributes), relationships (e.g., component interactions, hierarchical dependencies) and dynamic properties (e.g., real-time sensor data, operational status). These models are designed to facilitate efficient data integration, support decision-making processes, and enable the development of interoperable and replicable IoT applications for smart hydropower plants. We validate our approach through deployment and testing on a federated context broker architecture using real-world data from HPPs.},
	booktitle = {Proceedings of the 14th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Papadakis, Nikolaos and Bouloukakis, Georgios and Magoutis, Kostas},
	year = {2025},
	keywords = {Interoperability, Internet of Things (IoT), Context-Aware Systems, Data Models, NGSI-LD, Renewable Energy, Smart Energy},
	pages = {108--116},
}

@inproceedings{zhou_towards_2023,
	address = {New York, NY, USA},
	series = {{IJCKG} '22},
	title = {Towards {A} {Visualisation} {Ontology} for {Reusable} {Visual} {Analytics}},
	isbn = {978-1-4503-9987-6},
	url = {https://doi.org/10.1145/3579051.3579074},
	doi = {10.1145/3579051.3579074},
	abstract = {Data analytics including machine learning analytics is essential to extract insights from production data in modern industries. Visual analytics is essential for data analytics for e.g., presenting the data to provide an instinctive perception in exploratory data analysis, facilitating the presentation of data analysis results and the subsequent discussion on that. Visual analytics should allow a transparent common ground for discussion between experts in data analysis projects, given the multidisciplinary background of these experts. However, a standarised and formalised way of describing the knowledge and practice of visualisation is still lacking in the industry, which hamstrings the transparency and reusability of visual analytics. A visualisation ontology which models the nature and procedure of visualisation is well-suited to provide such standardisation. Currently a few studies discuss partially the modelling of visualisation, but insufficiently study the procedure of visualisation tasks, which is important for transparency and reusability especially in an industrial scenario. To this end, we present our ongoing work of development of the visualisation ontology in industrial scenarios at Bosch. We also demonstrate its benefits with case studies and knowledge graph based on our ontology.},
	booktitle = {Proceedings of the 11th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Baifan and Tan, Zhipeng and Zheng, Zhuoxun and Zhou, Dongzhuoran and Savkovic, Ognjen and Kharlamov, Evgeny},
	year = {2023},
	note = {event-place: Hangzhou, China},
	keywords = {knowledge graph, ontology engineering, visual analytics},
	pages = {99--103},
}

@inproceedings{saleh_hybrid_2022,
	address = {Phoenix, Arizona},
	series = {{WSC} '21},
	title = {Hybrid conceptual modeling for simulation: an ontology approach during covid-19},
	abstract = {The recent outbreak of Covid-19 caused by SARS-CoV-2 infection that started in Wuhan, China, has quickly spread worldwide. Due to the aggressive number of cases, the entire healthcare system has to respond and make decisions promptly to ensure it does not fail. Researchers have investigated the integration between ontology, algorithms and process modeling to facilitate simulation modeling in emergency departments and have produced a Minimal-Viable Simulation Ontology (MVSimO). However, the "minimalism" of the ontology has yet to be explored to cover pandemic settings. Responding to this, modelers must redesign services that are Covid-19 safe and better reflect changing realities. This study proposes a novel method that conceptualizes processes within the domain from a Discrete-Event Simulation (DES) perspective and utilizes prediction data from an Agent-Based Simulation (ABS) model to improve the accuracy of existing models. This hybrid approach can be helpful to support local decision making around resources allocation.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Saleh, Nurul and Bell, David and Sulaiman, Zuharabih},
	year = {2022},
}

@article{bjorner_domain_2019,
	title = {Domain {Analysis} and {Description} {Principles}, {Techniques}, and {Modelling} {Languages}},
	volume = {28},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3295738},
	doi = {10.1145/3295738},
	abstract = {We present a method for analysing and describing domains.By a domain we shall understand a rationally describable segment of a human assisted reality, i.e., of the world, its physical parts: natural [“God-given”] and artifactual [“human-made”], and living species: plants and animals including, notably, humans. These are endurants (“still”), as well as perdurants (“alive”). Emphasis is placed on “human-assistedness,” that is, that there is at least one (human-made) artifact and, therefore, that humans are a primary cause for change of endurant states as well as perdurant behaviours.By a method we shall mean a set of principles of analysis and for selecting and applying a number of techniques and tools in the construction of some artifact, say a domain description. We shall present a method for constructing domain descriptions. Among the tools we shall only be concerned with are the analysis and synthesis languages.Domain science and engineering marks a new area of computing science. Just as we are formalising the syntax and semantics of programming languages, so we are formalising the syntax and semantics of human-assisted domains. Just as physicists are studying the natural physical world, endowing it with mathematical models, so we, computing scientists, are studying these domains, endowing them with mathematical models, A difference between the endeavours of physicists and ours lies in the tools: The physics models are based on classical mathematics, differential equations and integrals, and so on; our models are based on mathematical logic, set theory, and algebra [1].Where physicists thus classically use a variety of differential and integral calculi to model the physical world, we shall be using the analysis and description calculi presented in this article to model primarily artifactual domains.},
	number = {2},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Bjørner, Dines},
	month = mar,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Domain engineering, domain analysis and description calculi, transcendental deduction},
}

@inproceedings{zhai_confsum_2025,
	address = {New York, NY, USA},
	series = {{FMANO} '25},
	title = {{ConfSum}: {Towards} {Automatic} {Summarization} of {Network}-scale {Operational} {Intents} from {Device} {Configurations}},
	isbn = {979-8-4007-2103-8},
	url = {https://doi.org/10.1145/3750022.3750459},
	doi = {10.1145/3750022.3750459},
	abstract = {When network operators need to understand the high-level intent behind a network's existing device configurations, they must engage in a tedious and error-prone process of manually reverse-engineering the low-level commands. We propose Configuration Intent Summarization (CIS), a new task that aims to automate this process by generating human-readable summaries of the intents embedded across a network's configurations. CIS is challenging due to the diversity of intents, the semantic gap between device-specific configurations and network-wide intents, and the need to reason about interactions between multiple devices' configurations. We present ConfSum, a system that addresses these challenges by leveraging the unique ability of large language models (LLMs) to parse semi-structured configuration files and summarize them in natural language. However, the full CIS task requires reasoning about device interactions and other complexities that are beyond the capabilities of LLMs alone. To enhance the LLM's robustness to these challenges, ConfSum introduces novel techniques for retrieving relevant examples to augment LLM prompts, decomposing the generation process to handle multi-device intents, and integrating with formal validation tools. Our experiments demonstrate that Conf-Sum achieves high intent coverage while generating summaries that match the quality of human experts.},
	booktitle = {Proceedings of the 2nd {Workshop} on {Formal} {Methods} {Aided} {Network} {Operation}},
	publisher = {Association for Computing Machinery},
	author = {Zhai, Rundi and Liu, Jianmin and Miao, Yukai and Chen, Li and Li, Dan and Cui, Baojiang and Zhang, Peng and Zhai, Ennan and Ding, Zishuo},
	year = {2025},
	note = {event-place: Coimbra, Portugal},
	keywords = {Large Language Models, Network management, Formal Methods},
	pages = {19--24},
}

@inproceedings{stein_towards_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {Towards language-agnostic alignment of product titles and descriptions: a neural approach},
	isbn = {978-1-4503-6675-5},
	url = {https://doi.org/10.1145/3308560.3316602},
	doi = {10.1145/3308560.3316602},
	abstract = {The quality of e-Commerce services largely depends on the accessibility of product content as well as its completeness and correctness. Nowadays, many sellers target cross-country and cross-lingual markets via active or passive cross-border trade, fostering the desire for seamless user experiences. While machine translation (MT) is very helpful for crossing language barriers, automatically matching existing items for sale (e.g. the smartphone in front of me) to the same product (all smartphones of the same brand/type/colour/condition) can be challenging, especially because the seller’s description can often be erroneous or incomplete. This task we refer to as item alignment in multilingual e-commerce catalogues. To facilitate this task, we develop a pipeline of tools for item classification based on cross-lingual text similarity, exploiting recurrent neural networks (RNNs) with and without pre-trained word-embeddings. Furthermore, we combine our language agnostic RNN classifiers with an in-domain MT system to further reduce the linguistic and stylistic differences between the investigated data, aiming to boost our performance. The quality of the methods as well as their training speed is compared on an in-domain data set for English–German products.},
	booktitle = {Companion {Proceedings} of {The} 2019 {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Stein, Daniel and Shterionov, Dimitar and Way, Andy},
	year = {2019},
	note = {event-place: San Francisco, USA},
	pages = {387--392},
}

@inproceedings{cima_query_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {Query {Definability} and {Its} {Approximations} in {Ontology}-based {Data} {Management}},
	isbn = {978-1-4503-8446-9},
	url = {https://doi.org/10.1145/3459637.3482466},
	doi = {10.1145/3459637.3482466},
	abstract = {Given an input dataset (i.e., a set of tuples), query definability in Ontology-based Data Management (OBDM) amounts to finding a query over the ontology whose certain answers coincide with the tuples in the given dataset. We refer to such a query as a characterization of the dataset with respect to the OBDM system. Our first contribution is to propose approximations of perfect characterizations in terms of recall (complete characterizations) and precision (sound characterizations). A second contribution is to present a thorough complexity analysis of three computational problems, namely verification (check whether a given query is a perfect, or an approximated characterization of a given dataset), existence (check whether a perfect, or a best approximated characterization of a given dataset exists), and computation (compute a perfect, or best approximated characterization of a given dataset).},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Cima, Gianluca and Croce, Federico and Lenzerini, Maurizio},
	year = {2021},
	note = {event-place: Virtual Event, Queensland, Australia},
	keywords = {semantic technologies, ontology based data management},
	pages = {271--280},
}

@inproceedings{engelberg_ontology-driven_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {An {Ontology}-{Driven} {Approach} for {Process}-{Aware} {Risk} {Propagation}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3577795},
	doi = {10.1145/3555776.3577795},
	abstract = {Risk Propagation (RP) is a central technique that allows the calculation of the cascading effect of risk within a system. At the current state, there is a lack of risk propagation solutions that can be used to assess the impact of risk at different levels of abstraction, accounting for actors, processes, physical-digital objects, and their relations. To fill this gap, in this paper, we propose a process-aware risk propagation approach that builds on two main components: i. an ontology, which supports functionalities typical of Semantic Web technologies (SWT), and ii. an ad hoc method to calculate the propagation of risk within the given system. We implemented our approach in a proof-of-concept tool, which was validated in the cybersecurity domain.},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Engelberg, Gal and Fumagalli, Mattia and Kuboszek, Adrian and Klein, Dan and Soffer, Pnina and Guizzardi, Giancarlo},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {ontology-driven risk propagation, risk analytics, risk propagation},
	pages = {1742--1745},
}

@article{zampierin_unsupervised_2025,
	title = {An {Unsupervised} {Approach} {Based} on {Attentional} {Neural} {Models} for {Aspect}-{Based} {Sentiment} {Classification}},
	volume = {25},
	issn = {1559-6915},
	url = {https://doi.org/10.1145/3746626.3746627},
	doi = {10.1145/3746626.3746627},
	abstract = {Due to the vast amount of reviews available on the Web, in the past decades, a growing share of work has focused on sentiment analysis. Aspect-based sentiment classification is the subtask that seeks to detect the sentiment expressed by the content creators towards a defined target within a sentence. This paper introduces three novel unsupervised attentional neural network models for aspect-based sentiment classification, and tests them on English restaurant reviews. The first model employs an autoencoder-like structure to learn a sentiment embedding matrix where each row of the matrix represents the embedding for one sentiment. To improve the model, a target-based attention mechanism is included that de-emphasizes irrelevant words. Last, a redundancy and a seed regularization term constrain the sentiment embedding matrix. The second model extends the first by including a Bi-LSTM layer in the attention mechanism to exploit contextual information. The third model further adapts a Left-Center-Right separated neural network with Rotatory attention structure from the supervised realm to an unsupervised setting. Although all three models construct meaningful sentiment embeddings, experimental results indicate that the inclusion of the Bi-LSTM in the attention mechanism leads to a more precise attention mechanism and, thus, better predictions. The best model, i.e., the second, outperforms all investigated unsupervised and weakly supervised algorithms for aspect-based sentiment classification from the literature.},
	number = {2},
	journal = {SIGAPP Appl. Comput. Rev.},
	author = {Zampierin, Luca and Frasincar, Flavius},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {unsupervised learning, aspect-based sentiment classification, attentional neural model},
	pages = {5--17},
}

@inproceedings{wolfe_implications_2025,
	address = {San Jose, California, USA},
	series = {{AIES} '24},
	title = {The {Implications} of {Open} {Generative} {Models} in {Human}-{Centered} {Data} {Science} {Work}: {A} {Case} {Study} with {Fact}-{Checking} {Organizations}},
	abstract = {Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.},
	booktitle = {Proceedings of the 2024 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {AAAI Press},
	author = {Wolfe, Robert and Mitra, Tanushree},
	year = {2025},
	pages = {1595--1607},
}

@inproceedings{roper_provpub_2025,
	address = {New York, NY, USA},
	series = {{PW}' 25},
	title = {{PROVPub}: {A} {Publication} {Model} to {Support} {Research} {Evaluation} and {Acknowledgement}},
	isbn = {979-8-4007-1941-7},
	url = {https://doi.org/10.1145/3736229.3736262},
	doi = {10.1145/3736229.3736262},
	abstract = {The Research Excellence Framework (REF) is vital for assessing the quality and impact of UK academic research, requiring a transparent provenance trail for research outputs. A major challenge is linking publications to their underlying research activities. This paper introduces PROVPub, a provenance model that automates and documents the publication lifecycle, improving traceability and compliance with REF requirements. PROVPub extends existing frameworks like Git2PROV\&nbsp;[5] by integrating metadata from institutional repositories, publisher databases, and Crossref. It captures key stages—submission, peer review, acceptance, and publication—to create a structured, verifiable record. This enhances efficiency, reduces administrative burdens, and strengthens research impact assessment. Through case studies, we demonstrate PROVPub’s role in REF compliance and automated research tracking. Our findings indicate that PROVPub enhances transparency and scalability, benefiting institutions seeking to streamline REF submissions and improve research evaluation.},
	booktitle = {Proceedings of the {ProvenanceWeek} 2025},
	publisher = {Association for Computing Machinery},
	author = {Roper, Bernard and Packer, Heather},
	year = {2025},
	keywords = {Provenance, Impact Assessment, PROV, REF, Research Evaluation},
	pages = {11--17},
}

@inproceedings{liu_giant_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {{GIANT}: {Scalable} {Creation} of a {Web}-scale {Ontology}},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3386145},
	doi = {10.1145/3318464.3386145},
	abstract = {Understanding what online users may pay attention to on the web is key to content recommendation and search services. These services will benefit from a highly structured and web-scale ontology of entities, concepts, events, topics and categories. While existing knowledge bases and taxonomies embody a large volume of entities and categories, we argue that they fail to discover properly grained concepts, events and topics in the language style of online users. Neither is a logically structured ontology maintained among these notions. In this paper, we present GIANT, a mechanism to construct a user-centered, web-scale, structured ontology, containing a large number of natural language phrases conforming to user attentions at various granularities, mined from the vast volume of web documents and search click logs. Various types of edges are also constructed to maintain a hierarchy in the ontology. We present our detailed techniques used in GIANT, and evaluate the proposed models and methods as compared to a variety of baselines, as well as deploy the resulted Attention Ontology in real-world applications, involving over a billion users, to observe its effect on content recommendation. The online performance of the ontology built by GIANT proves that it can significantly improve the click-through rate in news feeds recommendation.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Bang and Guo, Weidong and Niu, Di and Luo, Jinwen and Wang, Chaoyue and Wen, Zhen and Xu, Yu},
	year = {2020},
	note = {event-place: Portland, OR, USA},
	keywords = {document understanding, concept mining, event mining, ontology creation, user interest modeling},
	pages = {393--409},
}

@article{ma_syntax-augmented_2024,
	title = {Syntax-{Augmented} {Hierarchical} {Interactive} {Encoder} for {Zero}-{Shot} {Cross}-{Lingual} {Information} {Extraction}},
	volume = {32},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2024.3485547},
	doi = {10.1109/TASLP.2024.3485547},
	abstract = {Zero-shot cross-lingual information extraction (IE) aims at constructing an IE model for some low-resource target languages, given annotations exclusively in some rich-resource languages. Recent studies have shown language-universal features can bridge the gap between languages. However, prior work has neither explored the potential of establishing interactions between language-universal features and contextual representations nor incorporated features that can effectively model constituent span attributes and relationships between multiple spans. In this study, a \&lt;bold\&gt;s\&lt;/bold\&gt;yntax-augmented \&lt;bold\&gt;h\&lt;/bold\&gt;ierarchical \&lt;bold\&gt;in\&lt;/bold\&gt;teractive \&lt;bold\&gt;e\&lt;/bold\&gt;ncoder (SHINE) is proposed to transfer cross-lingual IE knowledge. The proposed encoder is capable of interactively capturing complementary information between features and contextual information, to derive language-agnostic representations for various cross-lingual IE tasks. Concretely, a multi-level interaction network is designed to hierarchically interact the complementary information to strengthen domain adaptability. Besides, in addition to the well-studied word-level syntax features of part-of-speech and dependency relation, a new span-level syntax feature of constituency structure is introduced to model the constituent span information which is crucial for IE. Experiments across seven languages on three IE tasks and four benchmarks verify the effectiveness and generalization ability of the proposed method.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Ma, Jun-Yu and Gu, Jia-Chen and Ling, Zhen-Hua and Liu, Quan and Liu, Cong and Hu, Guoping},
	month = oct,
	year = {2024},
	note = {Publisher: IEEE Press},
	pages = {4795--4809},
}

@inproceedings{zhu_research_2023,
	address = {New York, NY, USA},
	series = {{ICCBDC} '23},
	title = {Research on {The} {Construction} of {Ontology}-based {Music} {Works} {Knowledge} {Base}},
	isbn = {979-8-4007-0733-9},
	url = {https://doi.org/10.1145/3616131.3616143},
	doi = {10.1145/3616131.3616143},
	abstract = {The article formally explores the construction and reasoning of music domain ontology based on ontological methodology, proposes an improved ontology construction method, defines an ontology knowledge representation model, builds up an ontology of music works and its inference rules, achieves ontology-driven knowledge representation, storage, query and reasoning of music works, and makes some quest research for the construction and application of domain ontology.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Cloud} and {Big} {Data} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Li and Qi, Xiangtao},
	year = {2023},
	note = {event-place: Manchester, United Kingdom},
	pages = {81--88},
}

@inproceedings{monteiro_information_2022,
	address = {New York, NY, USA},
	series = {{SBSI} '22},
	title = {An {Information} {System} for {Law} {Integrating} {Ontological} {Bases} with a {Legal} {Reasoner} {Chatbot}},
	isbn = {978-1-4503-9698-1},
	url = {https://doi.org/10.1145/3535511.3535555},
	doi = {10.1145/3535511.3535555},
	abstract = {Context: The Semantic Web aims to assign meanings to resources available on the internet so that humans and computers can understand them. It can be used in the most diverse contexts, facilitating the development of systems where expert knowledge is formalized through logical-mathematical resources, mitigating potential inconsistencies, and promoting more human-friendly interaction services. Problem: The existence of semantic anomalies (use of rhetorical language, polysemy and inaccuracies) in the Brazilian Legal Domain enables the use of Semantic Web standards and technologies to mitigate these problems. Solution: This work deals with the development of an Information System that uses resources from the Semantic Web for the formal representation and the realization of legal inferences about Crimes Against Property. SI Theory: The Behavioral Decision Theory was approached, mainly in the incorporation of real patterns of decision making. Method: Bibliographic and documentary research methods were used to list the main concepts related to the Criminal Types investigated. The research is prescriptive and has a quali-quantitative approach. Summary of Results: A prototype system is presented, integrating ontologies of Brazilian Law with a chatbot that enables interaction with users in natural language, as well as performing reasoning tasks based on the knowledge formalized in these ontologies. Contributions and Impact in the IS area: The research will contribute to the automation of decision-making processes involving crimes against property, serving as an aid for professionals or law students and for legal simulations by ordinary people. Furthermore, it will serve as a reference for the development of other information systems with similar objectives in other contexts.},
	booktitle = {Proceedings of the {XVIII} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Monteiro, Lucas Henrique de Assis and Rodrigues, Cleyton Mário de Oliveira and Sousa, Aêda Monalliza Cunha de},
	year = {2022},
	note = {event-place: Curitiba, Brazil},
	keywords = {Ontology, Law, Chatbot},
}

@inproceedings{song_cotel_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23},
	title = {{CoTel}: {Ontology}-{Neural} {Co}-{Enhanced} {Text} {Labeling}},
	isbn = {978-1-4503-9416-1},
	url = {https://doi.org/10.1145/3543507.3583533},
	doi = {10.1145/3543507.3583533},
	abstract = {The success of many web services relies on the large-scale domain-specific high-quality labeled dataset. Insufficient public datasets motivate us to reduce the cost of data labeling while maintaining high accuracy in support of intelligent web applications. The rule-based method and the learning-based method are common techniques for labeling. In this work, we study how to utilize the rule-based and learning-based methods for resource-effective text labeling. We propose CoTel, the first ontology-neural co-enhanced framework for text labeling. We propose critical ontology extraction in the rule-based module and ontology-enhanced loss prediction in the learning-based module. CoTel can integrate explicit labeling rules and implicit labeling models and make them help each other to improve resource efficiency in text labeling tasks. We evaluate CoTel on both public datasets and real applications with three different tasks. Compared with the baseline, CoTel can reduce the time cost by 64.75\% (a 2.84× speedup) and the number of labeling by 62.07\%.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Song, Miao-Hui and Zhang, Lan and Yuan, Mu and Li, Zichong and Song, Qi and Liu, Yijun and Zheng, Guidong},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {knowledge enhancement, active learning, pseudo labeling, text labeling},
	pages = {1897--1906},
}

@article{swalens_chocola_2021,
	title = {Chocola: {Composable} {Concurrency} {Language}},
	volume = {42},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/3427201},
	doi = {10.1145/3427201},
	abstract = {Programmers often combine different concurrency models in a single program, in each part of the program using the model that fits best. Many programming languages, such as Clojure, Scala, and Java, cater to this need by supporting different concurrency models. However, existing programming languages often combine concurrency models in an ad hoc way, and the semantics of the combinations are not always well defined.This article studies the combination of three concurrency models: futures, transactions, and actors. We show that a naive combination of these models invalidates the guarantees they normally provide, thereby breaking the assumptions of programmers. Hence, we present Chocola: a unified language of futures, transactions, and actors that maintains the guarantees of all three models wherever possible, even when they are combined.We describe and formalize the semantics of this language and prove the guarantees it provides. We also provide an implementation as an extension of Clojure and demonstrated that it can improve the performance of three benchmark applications for relatively little effort from the developer.},
	number = {4},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Swalens, Janwillem and Koster, Joeri De and Meuter, Wolfgang De},
	month = jan,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Futures, actor model, software transactional memory},
}

@inproceedings{drissi_new_2021,
	address = {New York, NY, USA},
	series = {{MEDES} '21},
	title = {A {New} {Automatic} {Ontology} {Construction} {Method} {Based} on {Machine} {Learning} {Techniques}: {Application} on financial corpus},
	isbn = {978-1-4503-8314-1},
	url = {https://doi.org/10.1145/3444757.3485111},
	doi = {10.1145/3444757.3485111},
	abstract = {Ontology Learning is a process of (semi)automatically creating, maintaining, and transferring various forms of information into an ontology with minimum human intervention to guarantee a better knowledge representation and sharing. In recent years, the research on automating financial data modeling has become a hot topic among researchers because of the exponential increase of the number of financial documents and the heterogeneous of financial data [19, 20]. So, we highlight the emergence of new computational tools and methods to deal with the automatic modeling and exploration of large financial corpus. That's why, we propose here a solution named Norms2Onto which is a semi-automatic ontology construction method based on machine learning algorithms to facilitate the reading and ease updating of financial data and to guarantee their understanding. Experiments have been conducted to measure the effectiveness of our solution compared to a manual classification made by an domain expert. The results show the superiority of our approach.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Drissi, Amani and Khemiri, Ahmed and Sassi, Salma and Chbeir, Richard},
	year = {2021},
	note = {event-place: Virtual Event, Tunisia},
	keywords = {Machine Learning, Ontology Construction, Ontology Learning, IFRS Accounting Standards},
	pages = {57--61},
}

@inproceedings{prasad_towards_2018,
	address = {New York, NY, USA},
	series = {{ISEC} '18},
	title = {Towards a {Domain}-{Specific} {Language} for the {Renarration} of {Web} {Pages}},
	isbn = {978-1-4503-6398-3},
	url = {https://doi.org/10.1145/3172871.3172873},
	doi = {10.1145/3172871.3172873},
	abstract = {We are interested in the problem of enabling transformation of existing, already published web pages. We call this Renarration of web content. In our earlier work, we had already established the role and importance of renarration for improving Web Accessibility. There are nearly a billion websites on the web, making transformation of pages a domain on its own. In this paper, we present the development of a Domain-Specific Language (DSL) for the purpose of web page transformation. We show how the design and implementation of our DSL is driven by our problem domain, its terminology and its unique requirements. We take up an existing online video-course delivery system, which has accessibility challenges, as a specific case to demonstrate our DSL. We end with insights and reflections for future work in both DSL and web page transformations.},
	booktitle = {Proceedings of the 11th {Innovations} in {Software} {Engineering} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Prasad, Gollapudi VRJ Sai and Chimalakonda, Sridhar and Choppella, Venkatesh},
	year = {2018},
	note = {event-place: Hyderabad, India},
	keywords = {Domain Specific Language (DSL), Renarration, Web Page Transformation},
}

@inproceedings{ouchaou_ontology-based_2022,
	address = {New York, NY, USA},
	series = {{ICCTA} '22},
	title = {Ontology-{Based} {Cognitive} {Service} {Discovery} \&amp; {Composition}},
	isbn = {978-1-4503-9622-6},
	url = {https://doi.org/10.1145/3543712.3543739},
	doi = {10.1145/3543712.3543739},
	abstract = {Cloud cognitive computing has received a lot of attention lately especially for tackling real-world problems such as vision, natural language processing, fraud detection, sentiment analysis and speech recognition. This paradigm is based on cloud serverless computing and it provides machine learning based functions to end users. Part of the appeal in adopting this paradigm is its simplicity and the future promises a fast-growing serverless-native ecosystem in which service discovery and composition methods must be provided. However, serverless platforms still lack automated searching methods and the research community’s attention regarding this issue has been limited. In this paper, we propose an ontology-based approach for discovering and composing cognitive functions in serverless platforms in order to automate the searching process and semantically answer users’ requirements. We also carried a set of experiments to verify the correctness and the feasibility of our approach and discuss the influence of cognitive services nature on the outcomes of the discovery and composition approach.},
	booktitle = {Proceedings of the 2022 8th {International} {Conference} on {Computer} {Technology} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Ouchaou, Linda and Nacer, Hassina and Charoy, Francois and Youcef, Samir},
	year = {2022},
	note = {event-place: Vienna, Austria},
	keywords = {Composition, Ontology., Cognitive services, Discovery, FaaS, Serverless computing},
	pages = {154--162},
}

@article{candela_ontological_2023,
	title = {An {Ontological} {Approach} for {Unlocking} the {Colonial} {Archive}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3594727},
	doi = {10.1145/3594727},
	abstract = {Cultural Heritage institutions have been exploring new ways of making available their catalogues in digital format. Recently, new approaches have emerged as methods to reuse and make available the contents for computational purposes. This work introduces a methodology to transform digital collections into Linked Open Data following best practices. The framework has been applied to Indigenous and Spanish colonial archives based on the collection Relaciones Geográficas of Mexico and Guatemala provided by the LLILAS Benson Latin American Studies and Collections. The results of this work are publicly available. This work aims at encouraging Cultural Heritage institutions to publish and reuse their digital collections using advanced methods and techniques.},
	number = {4},
	journal = {J. Comput. Cult. Herit.},
	author = {Candela, Gustavo and Pereda, Javier and Sáez, Dolores and Escobar, Pilar and Sánchez, Alexander and Torres, Andrés Villa and Palacios, Albert A. and McDonough, Kelly and Murrieta-Flores, Patricia},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {knowledge graph, metadata, Linked Open Data, collections as data},
}

@article{moraitou_ontology-based_2024,
	title = {An {Ontology}-{Based} {Framework} for {Supporting} {Decision}-{Making} in {Conservation} and {Restoration} {Interventions} for {Cultural} {Heritage}},
	volume = {17},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3653977},
	doi = {10.1145/3653977},
	abstract = {Decision-making (DM) is the backbone of the Conservation and Restoration (CnR) of Cultural Heritage (CH). The demands of the DM process for information organization and management have raised issues that the CnR community attempts to solve by creating DM-support tools and systems, which, among others, exploit Semantic Web (SW) technologies. Regarding the tools and systems that focus on the DM process of selecting an intervention option (CnR-DM-I), they present benefits, as well as limitations, regarding the (1) completeness of representation of the relevant knowledge in a unified manner, (2) facilitation of recording the CnR-DM-I process per se, in terms of the problem at hand as well as the intervention parameters, requirements, and criteria, and (3) recommendation and further exploration of CnR intervention options in a systematic manner. This work proposes an ontology-based framework as a means to overcome those limitations. The proposed framework (DS-CnRI) sets at its core a formal ontology which provides the necessary entities to represent expert knowledge related to CnR-DM-I. The ontology also includes rules which provide useful inferences to assist the CnR-DM-I process. The proposed framework has been deployed and evaluated in collaboration with conservators. Initial evaluation results show that the framework assists conservators in CnR-DM-I to detect and select the most suitable intervention options, to better understand the limitations of different options, and to document the process of reaching their decision.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Moraitou, Efthymia and Christodoulou, Yannis and Kotis, Konstantinos and Caridakis, George},
	month = may,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {knowledge representation, conservation, cultural heritage, Decision-support services, restoration, semantic Web technologies},
}

@inproceedings{kitto_will_2024,
	address = {New York, NY, USA},
	series = {{UMAP} '24},
	title = {Will a {Skills} {Passport} ever get me through the lifelong learning border?: {Two} critical challenges facing personalised user models for lifelong learning},
	isbn = {979-8-4007-0433-8},
	url = {https://doi.org/10.1145/3627043.3659564},
	doi = {10.1145/3627043.3659564},
	abstract = {Lifelong personalised learning is often described as the holy grail of the educational data sciences, but work on the topic is sporadic and we are yet to achieve this goal in a meaningful form. In the wake of the skills shortages arising from national responses to COVID-19 this problem has again become a topic of interest. A number of proposals have emerged that some sort of a skills passport would help individuals, educational institutions, and employers to identify training and recruitment needs according to identified skills gaps. And yet, we are a long way from achieving a skills passport that could support lifelong learning despite more than 25 years of work on the topic. This paper draws attention to two of the critical socio-technical challenges facing skills passports, and lifelong learner models in general. This leads to a proposal for how we might move towards a useful skills passport that can cross the “skills sector border”.},
	booktitle = {Proceedings of the 32nd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Kitto, Kirsty},
	year = {2024},
	note = {event-place: Cagliari, Italy},
	keywords = {contextualisation, data portability, lifelong learning, personal user model, skills},
	pages = {132--142},
}

@article{argotti_operational_2024,
	title = {An {Operational} {Quality} {Model} of {Embedded} {Software} {Aligned} with {ISO} 25000},
	volume = {24},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3691642},
	doi = {10.1145/3691642},
	abstract = {Embedded systems omnipresent in everyday life and industry are mainly composed of hardware and software that must comply with a number of standards and regulations. However, there is no consensus on the quality characteristics and subcharacteristics of embedded software. This article presents the steps for modeling an operational quality model for embedded software aligned with the ISO 25000 series of quality models for traditional computer systems. From a literature review composed of 40 studies on quality modeling for embedded systems and software, 85 of the most frequent quality characteristics and subcharacteristics were first identified, including a subset of 16 referenced or cited in at least 25\% of the literature. Next, the design of a quality model for embedded software aligned with the ISO 25000 series was proposed with 13 characteristics and 27 subcharacteristics. The operational aspect of this quality model for embedded software is addressed next through a set of measures and measurement functions from ISO 25000 to aggregate the results of the quantification of the characteristics and subcharacteristics. A survey involving 25 embedded software specialists is presented next to gauge, using Fleiss's Kappa criteria, their agreement with the proposed quality model. Furthermore, the computed importance weights derived from the survey participants’ individual opinions were compared with those derived from an analysis of 40 embedded software studies, bolstering the credibility of the model. The results of this study suggest that the proposed quality model can serve as a framework for evaluating and understanding the quality characteristics across diverse expertise levels. Furthermore, the convergence between the survey and the literature strengthens the model's credibility by anchoring it in both established literature and practitioners’ agreements.},
	number = {1},
	journal = {ACM Trans. Embed. Comput. Syst.},
	author = {Argotti, Yann and Kenfaoui, Yasmine and Baron, Claude and Abran, Alain and Esteban, Philippe},
	month = nov,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Quality model, ISO, measure, qualimetry, quality characteristics},
}

@article{zeng_natural_2019,
	title = {Natural {Language} {Processing} for {EHR}-{Based} {Computational} {Phenotyping}},
	volume = {16},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2018.2849968},
	doi = {10.1109/TCBB.2018.2849968},
	abstract = {This article reviews recent advances in applying natural language processing NLP to Electronic Health Records EHRs for computational phenotyping. NLP-based computational phenotyping has numerous applications including diagnosis categorization, novel phenotype discovery, clinical trial screening, pharmacogenomics, drug-drug interaction DDI, and adverse drug event ADE detection, as well as genome-wide and phenome-wide association studies. Significant progress has been made in algorithm development and resource construction for computational phenotyping. Among the surveyed methods, well-designed keyword search and rule-based systems often achieve good performance. However, the construction of keyword and rule lists requires significant manual effort, which is difficult to scale. Supervised machine learning models have been favored because they are capable of acquiring both classification patterns and structures from data. Recently, deep learning and unsupervised learning have received growing attention, with the former favored for its performance and the latter for its ability to find novel phenotypes. Integrating heterogeneous data sources have become increasingly important and have shown promise in improving model performance. Often, better performance is achieved by combining multiple modalities of information. Despite these many advances, challenges and opportunities remain for NLP-based computational phenotyping, including better model interpretability and generalizability, and proper characterization of feature relations in clinical narratives.},
	number = {1},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Zeng, Zexian and Deng, Yu and Li, Xiaoyu and Naumann, Tristan and Luo, Yuan},
	month = jan,
	year = {2019},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {139--153},
}

@inproceedings{chawuthai_modelling_2024,
	address = {New York, NY, USA},
	series = {{AICCC} '23},
	title = {Modelling an {RDF} {Knowledge} {Graph} with {Transitivity} and {Symmetry} for {Bus} {Route} {Path} {Finding}},
	isbn = {979-8-4007-1622-5},
	url = {https://doi.org/10.1145/3639592.3639610},
	doi = {10.1145/3639592.3639610},
	abstract = {A key property of Linked Data is the representation and publication of data as an inter-connected labelled graph where different resources linked to each other form a network of meaningful information. A problem of path finding can be seen as searching important relationships between resources, such as, looking for chains of intermediate nodes. In this paper, we tackle this problem in the context of public transport navigation system, where we aim to find candidates of bus route path given two bus stations. We model a novel lightweight bus network as Resource Description Framework (RDF) triples of directed bus lines and walking paths between connected stations. Indeed, we demonstrate that lightweight bus network can be achieved by exploiting the sub-property of RDF Schema (RDFS) and the transitivity and symmetry provided by Web Ontology Language (OWL). We also perform a scalability test of our approach using a real-world bus network in Bangkok, Thailand. Various patterns of SPARQL Protocol and RDF Query Language (SPARQL) query statements are validated, showing the usefulness of the RDF model. The further step of this paper is to work with bus schedules and travel time analysis in order to select some proper candidates for users through an application.},
	booktitle = {Proceedings of the 2023 6th {Artificial} {Intelligence} and {Cloud} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Chawuthai, Rathachai and Kertkeidkachorn, Natthawut and Racharak, Teeradaj},
	year = {2024},
	note = {event-place: Kyoto, Japan},
	keywords = {Semantic Web, Knowledge Graph, Graph Traversal, Path Finding, Transport Navigation System},
	pages = {126--134},
}

@inproceedings{jayawardena_improving_2024,
	address = {New York, NY, USA},
	series = {{ICCAI} '24},
	title = {Improving {Quality} and {Domain}-{Relevancy} of {Paraphrase} {Generation} with {Graph}-{Based} {Retrieval} {Augmented} {Generation}},
	isbn = {979-8-4007-1705-5},
	url = {https://doi.org/10.1145/3669754.3669784},
	doi = {10.1145/3669754.3669784},
	abstract = {Paraphrase generation is a fundamental area of research in Natural Language Processing (NLP) and Natural Language Generation (NLG), due to its sequence-to-sequence (Seq2Seq) nature. Paraphrasing, spanning across various domains, poses challenges for simpler model architectures due to the extensive knowledge required to generate paraphrases. The added constraint of generating diverse paraphrases further complicates the task for models trained on existing datasets. We present a methodology that leverages Graph-Based Retrieval Augmented Generation (G-RAG), capable of utilizing both entity and phrasal knowledge to address this issue. We demonstrate through experiments that this approach enables both complex models like Large Language models (LLMs) and smaller Seq2Seq models to generate more diverse paraphrases without compromising semantic similarity. Furthermore, this approach’s capacity to integrate domain-specific knowledge makes it particularly effective across different domains, enhancing its applicability in varied contexts. The results are further corroborated by human evaluation and extensive quantitative analysis focusing on semantic similarity, lexical diversity, syntactic diversity, and grammatical correctness to gauge high-quality paraphrases.},
	booktitle = {Proceedings of the 2024 10th {International} {Conference} on {Computing} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Jayawardena, Lasal and Yapa, Prasan},
	year = {2024},
	note = {event-place: Bali Island, Indonesia},
	keywords = {Large Language Models, Natural Language Processing, Graph-based Knowledge, Paraphrase Generation, Sequence-to-Sequence Models},
	pages = {196--208},
}

@inproceedings{purificato_paradigm_2024,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '24},
	title = {Paradigm {Shifts} in {User} {Modeling}: {A} {Journey} from {Historical} {Foundations} to {Emerging} {Trends}},
	isbn = {979-8-4007-0466-6},
	url = {https://doi.org/10.1145/3631700.3653062},
	doi = {10.1145/3631700.3653062},
	abstract = {The presented tutorial aims to serve as a comprehensive roadmap for the UMAP community into the current user modeling research, focusing on the paradigm shifts that have transformed the research landscape in recent times. We will provide a complete overview of the large, long-standing, and ever-growing research fields of user modeling and user profiling, both from a historical and a technical point of view. We will then examine the definitions associated with each key term in this research domain, aiming to eliminate ambiguity and confusion in their usage. As the core of our tutorial, we present in-depth the paradigm shifts that have occurred in recent years, especially due to technological evolution, as well as the current research directions and novel trends in the field. In particular, we illustrate and discuss the advances in the following topics: implicit and explicit user profiling, user behavior modeling, user representation, and beyond-accuracy perspectives. The audience will be engaged in discussions during the whole presentation to foster the development of an interactive event. Detailed information and resources about the tutorial are available on the website: https://link.erasmopurif.com/tutorial-umap24.},
	booktitle = {Adjunct {Proceedings} of the 32nd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Purificato, Erasmo and Boratto, Ludovico and De Luca, Ernesto William},
	year = {2024},
	note = {event-place: Cagliari, Italy},
	keywords = {Paradigm Shifts, User Modeling, User Profiling},
	pages = {13--16},
}

@article{jammi_tooling_2018,
	title = {Tooling framework for instantiating natural language querying system},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3229863.3236248},
	doi = {10.14778/3229863.3236248},
	abstract = {Recent times have seen a growing demand for natural language querying (NLQ) interfaces to retrieve information from the structured data sources such as knowledge bases. Using this interface, business users can directly interact with a database without the knowledge of the query language or the data schema. Our earlier work describes a natural language query engine called ATHENA which has several shortcoming around ease of use and compatibility with data stores, formats and flows. In this demonstration paper, we present a tooling framework to address these challenges so that one can instantiate an NLQ system with utmost ease. Our framework makes it easy and practically applicable to all NLIDB scenarios involving different sources of structured data, file formats, and ontologies to enable natural language querying on top of them with minimal human configuration. We present the tool design and the solution to the challenges towards building such a system and demonstrate its applicability in the medical domain.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Jammi, Manasa and Sen, Jaydeep and Mittal, Ashish and Verma, Sagar and Pahuja, Vardaan and Ananthanarayanan, Rema and Lohia, Pranay and Karanam, Hima and Saha, Diptikalyan and Sankaranarayanan, Karthik},
	month = aug,
	year = {2018},
	note = {Publisher: VLDB Endowment},
	pages = {2014--2017},
}

@inproceedings{cuzzocrea_vector_2025,
	address = {New York, NY, USA},
	series = {{SSDBM} '25},
	title = {Vector {Databases} for {Modelling}, {Managing} and {Querying} {Big} {Scientific} {Data}: {Models}, {Issues}, {Paradigms}},
	isbn = {979-8-4007-1462-7},
	url = {https://doi.org/10.1145/3733723.3742469},
	doi = {10.1145/3733723.3742469},
	abstract = {Inspired by the emergence of scientific data in fields like as astronomy, climate research, and genomics, which presents significant challenges for conventional database systems. This vision paper investigates the adaptation of vector databases in order to describe, handle, and query large-scale scientific data. Moreover, we propose a road-map for embedding-centric data infrastructures, along with an analysis of current advancements and identification of important future research directions, including explainability and scalability. This research provides a foundational basis for next-generation interdisciplinary scientific discovery.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Scalable} {Scientific} {Data} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Cuzzocrea, Alfredo},
	year = {2025},
	keywords = {Advanced Scientific Data Management, Advanced Scientific Data Representation, Big Scientific Data, Vector Databases},
}

@inproceedings{si_hybrid_2024,
	address = {New York, NY, USA},
	series = {{ICCSMT} '23},
	title = {A {Hybrid} {Data}-{Knowledge}-{Driven} {Domain} {Ontology} {Construction} {Method} for {China}'s {Financial} {Domain}},
	isbn = {979-8-4007-0951-7},
	url = {https://doi.org/10.1145/3644523.3644610},
	doi = {10.1145/3644523.3644610},
	abstract = {The rapid increase of data in China's financial domain has brought difficulties in data organization and management, and the traditional flat organization of financial big data ignores the rich knowledge association in the data, and the study of knowledge association in the financial domain has received more and more attention from academia. Financial ontology can meet the demand of the financial industry for data quality and program rigor, and reflect the structure of the financial capital market and the associations between different entities. This study investigates and compares various ontology construction methods at home and abroad, and proposes a data-knowledge hybrid-driven approach to construct domain ontologies, which realizes the collaboration among people, knowledge and data with the core of "knowledge generation-data expansion-quality control". Under the guidance of this method, this study invited a research group of experts in the financial field to successfully construct the first financial ontology applicable to China's financial system by using financial data provided by several financial and economic information providers, such as Wind, CSMAR, CNRDS, etc., which provides exploration and guidance for the development of informatization in China's financial field, and on the basis of which a more comprehensive knowledge graph can be constructed in the future to further promote the construction of information resources in China's financial domain.},
	booktitle = {Proceedings of the 2023 4th {International} {Conference} on {Computer} {Science} and {Management} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Si, Tiange},
	year = {2024},
	note = {event-place: Xi'an, China},
	pages = {480--486},
}

@inproceedings{geng_disentangled_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Disentangled {Ontology} {Embedding} for {Zero}-shot {Learning}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539453},
	doi = {10.1145/3534678.3539453},
	abstract = {Knowledge Graph (KG) and its variant of ontology have been widely used for knowledge representation, and have shown to be quite effective in augmenting Zero-shot Learning (ZSL). However, existing ZSL methods that utilize KGs all neglect the intrinsic complexity of inter-class relationships represented in KGs. One typical feature is that a class is often related to other classes in different semantic aspects. In this paper, we focus on ontologies for augmenting ZSL, and propose to learn disentangled ontology embeddings guided by ontology properties to capture and utilize more fine-grained class relationships in different aspects. We also contribute a new ZSL framework named DOZSL, which contains two new ZSL solutions based on generative models and graph propagation models, respectively, for effectively utilizing the disentangled ontology embeddings. Extensive evaluations have been conducted on five benchmarks across zero-shot image classification (ZS-IMGC) and zero-shot KG completion (ZS-KGC). DOZSL often achieves better performance than the state-of-the-art, and its components have been verified by ablation studies and case studies. Our codes and datasets are available at https://github.com/zjukg/DOZSL.},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Geng, Yuxia and Chen, Jiaoyan and Zhang, Wen and Xu, Yajing and Chen, Zhuo and Z. Pan, Jeff and Huang, Yufeng and Xiong, Feiyu and Chen, Huajun},
	year = {2022},
	note = {event-place: Washington DC, USA},
	keywords = {ontology, knowledge graph, zero-shot learning, disentangled representation learning},
	pages = {443--453},
}

@inproceedings{cardinale_methodological_2021,
	address = {New York, NY, USA},
	series = {{iiWAS} '20},
	title = {A {Methodological} {Approach} to {Compare} {Ontologies}: {Proposal} and {Application} for {SLAM} {Ontologies}},
	isbn = {978-1-4503-8922-8},
	url = {https://doi.org/10.1145/3428757.3429091},
	doi = {10.1145/3428757.3429091},
	abstract = {Representation of the knowledge related to any domain with flexible and well-defined models, such as ontologies, provides the base to develop efficient and interoperable solutions. Hence, a proliferation of ontologies in many domains is unleashed. It is necessary to define how to compare such ontologies to decide which one is the most suitable for specific needs of users/developers. Since the emerging developing of ontologies, several studies have proposed criteria to evaluate them. Nevertheless, there is still a lack of practical and reproducible guidelines to drive a comparative evaluation of ontologies as a systematic process. In this paper, we propose a methodological process to qualitatively and quantitatively compare ontologies at Lexical, Structural, and Domain Knowledge levels, considering Correctness and Quality perspectives. Since the evaluation methods of our proposal are based in a golden-standard, it can be customized to compare ontologies in any domain. To show the suitability of our proposal, we apply our methodological approach to conduct a comparative study of ontologies in the robotic domain, in particularly for the Simultaneous Localization and Mapping (SLAM) problem. With this study case, we demonstrate that with this methodological comparative process, we are able to identify the strengths and weaknesses of ontologies, as well as the gaps still needed to fill in the target domain (SLAM for our study case).},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Cardinale, Yudith and Cornejo-Lupa, María A. and Ticona-Herrera, Regina and Barrios-Aranibar, Dennis},
	year = {2021},
	note = {event-place: Chiang Mai, Thailand},
	keywords = {Ontology, SLAM, Autonomous and Mobile Robots, Ontologies Evaluation},
	pages = {223--233},
}

@article{deutch_natural_2018,
	title = {Natural {Language} {Explanations} for {Query} {Results}},
	volume = {47},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/3277006.3277017},
	doi = {10.1145/3277006.3277017},
	abstract = {Multiple lines of research have developed Natural Language (NL) interfaces for formulating database queries. We build upon this work, but focus on presenting a highly detailed form of the answers in NL. The answers that we present are importantly based on the provenance of tuples in the query result, detailing not only the results but also their explanations. We develop a novel method for transforming provenance information to NL, by leveraging the original NL query structure. Furthermore, since provenance information is typically large and complex, we present two solutions for its effective presentation as NL text: one that is based on provenance factorization, with novel desiderata relevant to the NL case, and one that is based on summarization.},
	number = {1},
	journal = {SIGMOD Rec.},
	author = {Deutch, Daniel and Frost, Nave and Gilad, Amir},
	month = sep,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {42--49},
}

@inproceedings{jiang_construction_2024,
	address = {New York, NY, USA},
	series = {{ICCBD} '24},
	title = {"{Construction} of {Conceptual} {Layer} of {Knowledge} {Graph} of {State}-target {Differentiation} \&amp; {Treatment} based on {Ontology} and {Protégé}"},
	isbn = {979-8-4007-1022-3},
	url = {https://doi.org/10.1145/3695080.3695151},
	doi = {10.1145/3695080.3695151},
	abstract = {As an innovative strategy, "State-target Differentiation \&amp; Treatment" (STDT) combines traditional Chinese medicine and Western medicine, making it a prominent development direction in the current Traditional Chinese Medicine (TCM) diagnosis and treatment system. With the deepening of research in this field, a significant number of research achievements was coming out. However, massive scattered data lack of integration and structuring. Therefore, domain knowledge storage and relational reasoning become pressing issues in the field of STDT. This paper obtains raw data from literature retrieval database and domain monograph of STDT. By analyzing the existing semantic framework of TCM and the clinical characteristics of STDT, we summarize the knowledge structure of STDT theory in three aspects "class, data properties, and object properties " in the form of table. Finally, the ontology of STDT knowledge graph is constructed with Protege, a commonly used ontology development tool. From the therapeutic process of macro-control of physical state to the knowledge of the micro-pharmacological level of TCM targets, the visualized display of the STDT knowledge graph realizes the combination of "state" and "target". The utilization of ontology technology enables the blending, recombination, and reasoning of the domain knowledge of STDT, laying the foundation and offering reference for the subsequent construction of the domain knowledge base.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Cloud} {Computing} and {Big} {Data}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Xinran and Zhang, Wenxue},
	year = {2024},
	note = {event-place: Dali, China},
	pages = {414--422},
}

@inproceedings{jungmann_fusing_2025,
	address = {New York, NY, USA},
	series = {{SAC} '25},
	title = {Fusing {Expert} {Knowledge} and {Internet} of {Things} {Data} for {Digital} {Twin} {Models}: {Addressing} {Uncertainty} in {Expert} {Statements}},
	isbn = {979-8-4007-0629-5},
	url = {https://doi.org/10.1145/3672608.3707826},
	doi = {10.1145/3672608.3707826},
	abstract = {Extracting Digital Twin models by fusing expert knowledge with Internet of Things data remains a challenging and open research area. Existing literature offers very limited approaches for seamless and systematic extraction of Digital Twin models from these combined sources. In this paper, we address the research gap by proposing a novel approach that considers and integrates the uncertainty inherent in human expert knowledge into the extraction processes of Digital Twin models. Given that experts possess unique experiences, contextual understandings and judgements, their knowledge can be highly divergent, complex, ambiguous, and even incorrect or incomplete. Consequently, not all expert knowledge statements should be equally weighted in the resulting simulation models. Our contributions include a comprehensive literature review on the uncertainty in expert knowledge and the proposal of an approach to integrate this uncertainty in the extraction of Digital Twin models from fused expert knowledge and IoT data. We demonstrate our approach through a case study in reliability assessment.1},
	booktitle = {Proceedings of the 40th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Jungmann, Michelle and Lazarova-Molnar, Sanja},
	year = {2025},
	note = {event-place: Catania International Airport, Catania, Italy},
	keywords = {digital twins, industry 4.0, ACM proceedings, fusion of data and expert knowledge, uncertainty in expert knowledge},
	pages = {874--881},
}

@inproceedings{martin_generalized_2022,
	address = {New York, NY, USA},
	series = {{SAC} '22},
	title = {Generalized graph pattern discovery in linked data with data properties and a domain ontology},
	isbn = {978-1-4503-8713-2},
	url = {https://doi.org/10.1145/3477314.3507301},
	doi = {10.1145/3477314.3507301},
	abstract = {Nowadays, in many practical situations, analytical tasks need to be performed on complex heterogeneous data, often described by a domain ontology (DO). Such cases abound in life science fields such as agro-informatics, where observations and measures on animals/plants are logged for subsequent mining. The data is naturally structured as graph(s), unlabelled and missing some values, hence it fits well pattern mining. In our own precision farming project aimed at decision support for dairy cow management, we mine for knowledge in milk production data. In one task, we aim at contrast patterns explaining the relative impact of independent production factors. To that end, ontologically-generalized graph patterns (OGPs), a variety of generalized graph patterns, where vertices and edges are labelled by DO classes and properties, respectively, were defined. A mining methodology was also designed that reconciles OWL DOs, abstraction from RDF graphs and literals in data. To address the well-known cost-related limitations of graph mining -exacerbated here by class/property specializations and data properties- we split the mining task into (1) mining of generic object property topology patterns and (2) label refinement. Those focus on two sorts of OGPs, called topologies and class stars, respectively, which, after being mined separately, get (3) assembled into fully-fledged OGPs.},
	booktitle = {Proceedings of the 37th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Martin, Tomas and Fuentes, Victor and Valtchev, Petko and Diallo, Abdoulaye Baniré and Lacroix, René},
	year = {2022},
	note = {event-place: Virtual Event},
	keywords = {ontologies, generalized patterns, graph data, pattern mining},
	pages = {1890--1899},
}

@article{bertossi_ontological_2018,
	title = {Ontological {Multidimensional} {Data} {Models} and {Contextual} {Data} {Quality}},
	volume = {9},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3148239},
	doi = {10.1145/3148239},
	abstract = {Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. The data under assessment are mapped into the context for additional analysis, processing, and quality data extraction. The resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context, we include a generalized multidimensional data model and a Datalog± ontology with provably good properties in terms of query answering. These main components are used to represent dimension hierarchies, dimensional constraints, and dimensional rules and define predicates for quality data specification. Query answering relies on and triggers navigation through dimension hierarchies and becomes the basic tool for the extraction of quality data. The OMD model is interesting per se beyond applications to data quality. It allows for a logic-based and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.},
	number = {3},
	journal = {J. Data and Information Quality},
	author = {Bertossi, Leopoldo and Milani, Mostafa},
	month = jan,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology-based data access, Datalog±, query answering, weakly-sticky programs},
}

@inproceedings{mitra_stat-n-ball_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Stat-n-{Ball}: {Enhancing} {Probabilistic} {Knowledge} {Graph} {Embeddings} with {Geometric} and {Confidence}-{Aware} {Models}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715487},
	doi = {10.1145/3701716.3715487},
	abstract = {Region-based Knowledge Graph Embedding (R-KGE) models, which represent entities as convex shapes (e.g., balls) and relations as geometric transformations in vector space, offer a promising approach for explainable and accurate reasoning over ontologies. However, existing R-KGE models assume perfect reliability of Knowledge Graphs (KGs), which is often unrealistic as real-world KGs are noisy and incomplete. To address this, Probabilistic Knowledge Graphs (P-KGs) associate axioms with confidence scores, capturing the uncertainty of their truthfulness. We propose Stat-n-Ball, a novel R-KGE framework that incorporates confidence scores by representing axioms' certainty as overlapping volumes between entities in vector space. Our approach enhances the geometric representation of KGs, enabling accurate link prediction and confidence estimation in probabilistic settings. Experimental evaluations on standard P-KG datasets demonstrate that Stat-n-Ball achieves atleast a 2× improvement in entity association detection and a minimum 5\% reduction in confidence prediction error compared to state-of-the-art models. These results underscore its effectiveness in handling noisy and uncertain KGs while preserving logical and semantic integrity.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Mitra, Aniket and Venugopal, Vinu E.},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {n-ball embedding, noisy knowledge graphs, probabilistic knowledge graph embedding},
	pages = {1194--1198},
}

@inproceedings{li_feature_2018,
	address = {New York, NY, USA},
	series = {{SPLC} '18},
	title = {Feature and variability extraction from natural language software requirements specifications},
	isbn = {978-1-4503-5945-0},
	url = {https://doi.org/10.1145/3236405.3236427},
	doi = {10.1145/3236405.3236427},
	abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
	booktitle = {Proceedings of the 22nd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} 2},
	publisher = {Association for Computing Machinery},
	author = {Li, Yang},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {software product lines, feature identification, requirement documents, reverse engineering, variability extraction},
	pages = {72--78},
}

@inproceedings{correa_restrepo_generating_2023,
	address = {New York, NY, USA},
	series = {{GPCE} 2023},
	title = {Generating {Constraint} {Programs} for {Variability} {Model} {Reasoning}: {A} {DSL} and {Solver}-{Agnostic} {Approach}},
	isbn = {979-8-4007-0406-2},
	url = {https://doi.org/10.1145/3624007.3624060},
	doi = {10.1145/3624007.3624060},
	abstract = {Verifying and configuring large Software Product Lines (SPL) requires automation tools. Current state-of-the-art approaches involve translating variability models into a formalism accepted as input by a constraint solver. There are currently no standards for variability modeling languages (VML). There is also a variety of constraint solver input languages. This has resulted in a multiplication of ad-hoc architectures and tools specialized for a single pair of VML and solver, fragmenting the SPL community. To overcome this limitation, we propose a novel architecture based on model-driven code generation, where the syntax and semantics of VMLs can be declaratively specified as data, and a standard, human-readable, formal pivot language is used between the VML and the solver input language. This architecture is the first to be fully generic by being agnostic to both VML and the solver paradigm. To validate the genericity of the approach, we have implemented a prototype tool together with declarative specifications for the syntax and semantics of two different VMLs and two different solver families. One VML is for classic, static SPL, and the other for run-time reconfigurable dynamic SPL with soft constraints to be optimized during configuration. The two solver families are Constraint Satisfaction Programs (CSP) and Constraint Logic Programs (CLP).},
	booktitle = {Proceedings of the 22nd {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {Association for Computing Machinery},
	author = {Correa Restrepo, Camilo and Robin, Jacques and Mazo, Raul},
	year = {2023},
	note = {event-place: Cascais, Portugal},
	keywords = {Automated Reasoning, Software Product Lines, Configuration Automation, Generic Architecture},
	pages = {138--152},
}

@article{saidi_stacking_2023,
	title = {Stacking of {BERT} and {CNN} {Models} for {Arabic} {Word} {Sense} {Disambiguation}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3623379},
	doi = {10.1145/3623379},
	abstract = {We propose a new approach for Arabic Word Sense Disambiguation (AWSD) by hybridization of single-layer Convolutional Neural Network (CNN) with contextual representation (BERT). WSD is the task of automatically detecting the correct meaning of a word used in a given context. WSD can be performed as a classification task, and the context is generally a short sentence. Kim [26] proved that combining a CNN with an RNN (recurrent neural network) provides a good result for text classification. Here, we use a concatenation of BERT models as a word embedding to get simultaneously the target and context representation. Our approach improves the performance of WSD in Arabic languages. The experimental results show that our model outperforms the state-of-the-art approaches and improves the accuracy of 96.42\% on the Arabic WordNet dataset.},
	number = {11},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Saidi, Rakia and Jarray, Fethi},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {BERT, transformer, Word sense disambiguation, Arabic text, convolutional neural network, supervised approach},
}

@inproceedings{hao_medto_2021,
	address = {New York, NY, USA},
	series = {{KDD} '21},
	title = {{MEDTO}: {Medical} {Data} to {Ontology} {Matching} {Using} {Hybrid} {Graph} {Neural} {Networks}},
	isbn = {978-1-4503-8332-5},
	url = {https://doi.org/10.1145/3447548.3467138},
	doi = {10.1145/3447548.3467138},
	abstract = {Medical ontologies are widely used to describe and organize medical terminologies and to support many critical applications on healthcare databases. These ontologies are often manually curated (e.g., UMLS, SNOMED CT, and MeSH) by medical experts. Medical databases, on the other hand, are often created by database administrators, using different terminology and structures. The discrepancies between medical ontologies and databases compromise interoperability between them. Data to ontology matching is the process of finding semantic correspondences between tables in databases to standard ontologies. Existing solutions such as ontology matching have mostly focused on engineering features from terminological, structural, and semantic model information extracted from the ontologies. However, this is often labor intensive and the accuracy varies greatly across different ontologies. Worse yet, the ontology capturing a medical database is often not given in practice. In this paper, we propose MEDTO, a novel end-to-end framework that consists of three innovative techniques: (1) a lightweight yet effective method that bootstrap a semantically rich ontology from a given medical database, (2) a hyperbolic graph convolution layer that encodes hierarchical concepts in the hyperbolic space, and (3) a heterogeneous graph layer that encodes both local and global context information of a concept. Experiments on two real-world medical datasets matching against SNOMED CT show significant improvements compared to the state-of-the-art methods. MEDTO also consistently achieves competitive results on a benchmark from the Ontology Alignment Evaluation Initiative.},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \&amp; {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hao, Junheng and Lei, Chuan and Efthymiou, Vasilis and Quamar, Abdul and Özcan, Fatma and Sun, Yizhou and Wang, Wei},
	year = {2021},
	note = {event-place: Virtual Event, Singapore},
	keywords = {ontology matching, graph neural network, medical data},
	pages = {2946--2954},
}

@inproceedings{wang_research_2022,
	address = {New York, NY, USA},
	series = {{ICISS} '22},
	title = {Research on {CGAN}-{BERT} and {RGAN}-{BERT} {Models} for {Short} {Text} {Classification} based on {Semi}-{Supervised} {Model}},
	isbn = {978-1-4503-9683-7},
	url = {https://doi.org/10.1145/3561877.3561896},
	doi = {10.1145/3561877.3561896},
	abstract = {With the rapid development of artificial intelligence, a large number of short texts cause a certain degree of information redundancy. Text classification technology can help people classify and process information, and has important applications in the fields of recommendation system, public opinion monitoring and information retrieval. However, short text information in different fields has the characteristics of industry professionalism and fast updating of language style. The resulting problems such as poor applicability of the model and bottlenecks in annotation make the effect of traditional classification methods in short text analysis limited. Therefore, we propose semi supervised text classification models CGAN-BERT and RGAN-BERT based on convolutional neural network and cyclic neural network respectively. The newly designed generator and discriminator are more conducive to the game process. We evaluate our model and other classical models on several public data sets. The experimental results show that our proposed model is better than other models.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Information} {Science} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Xiaoning and Zhang, Yang},
	year = {2022},
	note = {event-place: Beijing, China},
	keywords = {natural language processing, text classification, Recurrent Neural Network, convolutional neural networks, gan},
	pages = {118--124},
}

@inproceedings{agafonov_experiment_2022,
	address = {New York, NY, USA},
	series = {{SoICT} '22},
	title = {An {Experiment} on {Localization} of {Ontology} {Concepts} in {Deep} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4503-9725-4},
	url = {https://doi.org/10.1145/3568562.3568602},
	doi = {10.1145/3568562.3568602},
	abstract = {Deep neural networks have recently evolved into a powerful AI tool, reaching near-human performance level in many tasks, and in some tasks even surpassing it. However, a significant drawback of neural networks is the lack of explainability and interpretability — it is hard to say why a neural network arrived to a certain conclusion. This significantly limits application of neural networks in critical tasks and undermines trust in human-AI collaboration. It has been recently shown that internal representations constructed by a neural network can often be aligned with a domain ontology. This opens a promising way to provide explanations of a neural network in human terms. In this paper, we discuss the results of the experiment aimed at understanding what layers of a neural network are the most perspective for the alignment with given ontology concept. To do so, we build concept localization maps for XTRAINS — a synthetic dataset consisting of images and their ontological annotations. The importance of such maps is that they can be used for the development of efficient concept alignment heuristics. The experiment mostly supports the intuition that high-level concepts are localized mostly in the activations of last layers of a neural network (near its head), while lower-level concepts might be better extracted from middle layers.},
	booktitle = {Proceedings of the 11th {International} {Symposium} on {Information} and {Communication} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Agafonov, Anton and Ponomarev, Andrew},
	year = {2022},
	note = {event-place: Hanoi, Vietnam},
	keywords = {ontologies, XAI, neural networks, explainable AI, neuro-symbolic intelligence},
	pages = {82--87},
}

@inproceedings{dew_developing_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Developing a {Comprehensive} {Task} {Framework} for {Effective} {Workforce} {Analysis}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715172},
	doi = {10.1145/3701716.3715172},
	abstract = {Effective workforce analysis, planning, and management require a deep understanding of the tasks and skills associated with different roles. This paper introduces a novel methodology for developing a comprehensive task framework that leverages Large Language Models (LLMs) and large-scale job ads data. We first propose an innovative approach to task taxonomy design, which involves the decomposition and reconstruction of tasks into a hierarchical structure based on action-object pairings, systematically refined using LLMs. The methodology extends to integrating the taxonomy with occupation and skill linkages derived from job ads, ensuring alignment with real-world workforce dynamics. Finally, we demonstrate the practical value of this framework through a visual analytics system that enables interactive exploration and analysis of tasks, occupations, and associated skills, highlighting its potential to transform workforce analysis. Demo video: https://bit.ly/41txBZK},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Dew, Rebecca and Li, Mingzhao and Liu, Weidong and Baratha Raj, Sandya},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language model, visual analytics, gpt, workforce analysis},
	pages = {2819--2822},
}

@inproceedings{gautam_ai-powered_2025,
	address = {New York, NY, USA},
	series = {{GLSVLSI} '25},
	title = {{AI}-{Powered} {Knowledge} {Graphs} for {Neuromorphic} and {Energy}-{Efficient} {Computing}},
	isbn = {979-8-4007-1496-2},
	url = {https://doi.org/10.1145/3716368.3735295},
	doi = {10.1145/3716368.3735295},
	abstract = {The surge in scientific literature obscures breakthroughs and hinders the discovery of new research paths. We propose an artificial intelligence (AI) powered framework using large language models (LLMs) and knowledge graphs (KGs) to automate parts of scientific discovery, focusing on energy-efficient AI circuits. Our hybrid approach combines LLMs, structured data, and ontology-based reasoning to construct a comprehensive knowledge graph that integrates insights across computational neuroscience, spiking neuron models, learning rules, architectural motifs, and neuromorphic device technologies. This multi-domain representation enables the generation of hypotheses that connect biological function with implementable, energy-efficient hardware architectures. Using KG embeddings and graph neural networks, the framework generates hypotheses for novel circuits, validates them through optimization on exascale HPC systems, and with tools like SuperNeuro and Fugu, the most promising designs will be prototyped in hardware. This open-source system aims to accelerate discoveries and bridging neuroscience with hardware innovation, drive collaboration, and unlock new opportunities in low-power AI computing.},
	booktitle = {Proceedings of the {Great} {Lakes} {Symposium} on {VLSI} 2025},
	publisher = {Association for Computing Machinery},
	author = {Gautam, Ashish and Patton, Robert and Potok, Thomas and Kannan, Ramakrishnan and Aimone, James and Severa, William},
	year = {2025},
	keywords = {Knowledge graphs, large language models, hypothesis generation, architectural synthesis, cortical microcircuits., energy-efficient circuits, neuromorphic computing, scientific discovery automation, spiking neural networks, STDP},
	pages = {996--1001},
}

@inproceedings{rabbi_model-based_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {A {Model}-{Based} {Framework} for {Exploring} {Conflict} {Dynamics}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688206},
	doi = {10.1145/3652620.3688206},
	abstract = {This paper introduces a novel framework for conflict analysis that leverages advanced visual modeling techniques. By employing comparative analysis, key variables influencing armed conflicts are identified and analyzed. The framework includes a meta-model representing domain concepts such as the goals and strategies of conflicting parties, escalating stages, and impacts of conflicts.Conflict escalation is a complex process characterized by interactions between opposing parties. This paper presents a structured model that outlines how conflicts evolve and intensify over time. We adapt a meta-modeling framework called the Diagram Predicate Framework (DPF) to represent conflict-related concepts and extend it to support abstract view generation. This framework facilitates the analysis of conflict trends and the study of dynamics across various levels of abstraction.A computational model based on category theory is proposed for trend analysis, enabling the extraction of patterns of conflict evolution and the comparison of strategies and goals at different escalation stages. Categorical operations such as pullback and limit construction are employed to compute conflict evolution and identify common structures among conflict instances, providing insights into conflict dynamics across diverse zones.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rabbi, Fazle},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {metamodeling, category theory, computational journalism, conflict analysis},
	pages = {745--754},
}

@article{akram_novel_2022,
	title = {A {Novel} {Deep} {Auto}-{Encoder} {Based} {Linguistics} {Clustering} {Model} for {Social} {Text}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3527838},
	doi = {10.1145/3527838},
	abstract = {The wide adoption of media and social media has increased the amount of digital content to an enormous level. Natural language processing (NLP) techniques provide an opportunity to extract and explore meaningful information from a large amount of text. Among natural languages, Urdu is one of the widely used languages worldwide for spoken and written communications. Due to its wide adopt-ability, digital content in the Urdu language is increasing briskly, especially with social media and online NEWS feeds. Government agencies and advertisers must filter and understand the content to analyze the trends and cohorts in their interest and national prerogative. Clustering is considered a baseline and one of the first steps in natural language understanding. There are many state-of-the-art clustering techniques specifically for English, French, and Arabic, but no significant research has been conducted in Urdu language processing. Doing it for short text segments is challenging because of limited features and the absence of meaningful language discourse and nuance. Many rule-based NLP techniques are adopted to overcome these issues, relying on human-designed features and rules. Therefore, these methods do not promise remarkable results. Alongside NLP, deep learning techniques are pretty efficient in capturing contextual information with minimal noise compared to other traditional methods. By taking on this challenging job, we develop a deep learning-based technique for Urdu short text clustering for the very first time without a human-designed feature. In this paper, we propose a method of short text clustering using a deep neural network that automatically learns feature representations and clustering assignments simultaneously. This method learns clustering objectives by converting the high dimensional feature space to a low dimensional feature space. Our experiments on the Urdu NEWS headlines dataset show remarkable results compared to state-of-the-art methods.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Akram, Muhammad Waseem and Salman, Muhammad and Bashir, Muhammad Farrukh and Salman, Syed Muhammad Saad and Gadekallu, Thippa Reddy and Javed, Abdul Rehman},
	month = apr,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Social media, Clustering, Text, Low resource language, Urdu},
	annote = {Just Accepted},
}

@inproceedings{ivanova_semi-automatic_2023,
	address = {New York, NY, USA},
	series = {{CompSysTech} '23},
	title = {Semi-automatic ontology development for supporting personalized tutoring},
	isbn = {979-8-4007-0047-7},
	url = {https://doi.org/10.1145/3606305.3606321},
	doi = {10.1145/3606305.3606321},
	abstract = {Many researches have working the last two decades on ontology learning, using NLP, data mining, and machine learning, but experiments show, that every ontology learning method have his precision and recall less than 1, and automatically developed ontologies need from human evaluation and modification. So, participation of experts is a must in ontology development to guarantee high quality and the semi-automatic ontology development is the only approach having potential to ensure cheaper development of high-quality ontologies. In this research we will discuss semi-automatic support of ontology engineering process, based on automated knowledge extraction from text, semi-structured or structured sources. Most of automated ontology development methods and algorithms are domain – dependent. We analyze specifics of semi-automatic ontology development in the e-learning domain and propose software architecture of semi-automatic ontology development framework for educational ontologies.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Computer} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Ivanova, Tatyana Ivanova},
	year = {2023},
	note = {event-place: Ruse, Bulgaria},
	pages = {180--185},
}

@inproceedings{scharpf_classification_2020,
	address = {New York, NY, USA},
	series = {{JCDL} '20},
	title = {Classification and {Clustering} of {arXiv} {Documents}, {Sections}, and {Abstracts}, {Comparing} {Encodings} of {Natural} and {Mathematical} {Language}},
	isbn = {978-1-4503-7585-6},
	url = {https://doi.org/10.1145/3383583.3398529},
	doi = {10.1145/3383583.3398529},
	abstract = {In this paper, we show how selecting and combining encodings of natural and mathematical language affect classification and clustering of documents with mathematical content. We demonstrate this by using sets of documents, sections, and abstracts from the arXiv preprint server that are labeled by their subject class (mathematics, computer science, physics, etc.) to compare different encodings of text and formulae and evaluate the performance and runtimes of selected classification and clustering algorithms. Our encodings achieve classification accuracies up to 82.8\% and cluster purities up to 69.4\% (number of clusters equals number of classes), and 99.9\% (unspecified number of clusters) respectively. We observe a relatively low correlation between text and math similarity, which indicates the independence of text and formulae and motivates treating them as separate features of a document. The classification and clustering can be employed, e.g., for document search and recommendation. Furthermore, we show that the computer outperforms a human expert when classifying documents. Finally, we evaluate and discuss multi-label classification and formula semantification.},
	booktitle = {Proceedings of the {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} in 2020},
	publisher = {Association for Computing Machinery},
	author = {Scharpf, Philipp and Schubotz, Moritz and Youssef, Abdou and Hamborg, Felix and Meuschke, Norman and Gipp, Bela},
	year = {2020},
	note = {event-place: Virtual Event, China},
	keywords = {machine learning, information retrieval, document classification, document clustering, mathematical information retrieval},
	pages = {137--146},
}

@inproceedings{castro_ontology_2023,
	address = {New York, NY, USA},
	series = {{SBQS} '22},
	title = {An {Ontology} to support {Knowledge} {Management} {Solutions} for {Human}-{Computer} {Interaction} {Design}},
	isbn = {978-1-4503-9999-9},
	url = {https://doi.org/10.1145/3571473.3571502},
	doi = {10.1145/3571473.3571502},
	abstract = {Developing interactive systems is a challenging task that involves concerns related to the human-computer interaction (HCI), such as usability and user experience. Therefore, HCI design is a core issue to the quality of such systems. HCI design often involves people with different backgrounds (e.g., Arts, Software Engineering, Design). This makes knowledge transfer a challenging issue due to the lack of a common conceptualization about HCI design, leading to semantic interoperability problems, such as ambiguity and imprecision when interpreting shared information. Ontologies have been acknowledged as a successful approach to represent domain knowledge and support knowledge-based solutions. Hence, in this work, we propose to explore the use of ontologies to represent structured knowledge of HCI design and improve knowledge sharing in this context. We developed the Human-Computer Interaction Design Ontology (HCIDO), which is part of the Human-Computer Interaction Ontology Network (HCI-ON) and is connected to the Software Engineering Ontology Network (SEON). By making knowledge related to the HCI design domain explicit and structured, HCIDO helped us to develop KTID, a tool that aims to support capturing and sharing knowledge to aid in HCI design by allowing HCI designers to annotate information about design choices in design artifacts shared with HCI design stakeholders. Preliminary results indicate that the tool can be particularly useful for novice HCI designers.},
	booktitle = {Proceedings of the {XXI} {Brazilian} {Symposium} on {Software} {Quality}},
	publisher = {Association for Computing Machinery},
	author = {Castro, Murillo and Barcellos, Monalessa},
	year = {2023},
	note = {event-place: Curitiba, Brazil},
	keywords = {Ontology, Knowledge Management, User Interface, HCI Design},
}

@inproceedings{meftah_new_2024,
	address = {New York, NY, USA},
	series = {{ICFNDS} '23},
	title = {A new ontological approach for multilingual scientific research},
	isbn = {979-8-4007-0903-6},
	url = {https://doi.org/10.1145/3644713.3644729},
	doi = {10.1145/3644713.3644729},
	abstract = {Abstract— Scientific research is all the actions undertaken to produce and develop scientific knowledge. The representation of this knowledge can take various forms: it can be publications, reports, patents, etc. This knowledge can be incorporated into scientific social networks and applications. The scientific social networks in their current form depend on the title, the keywords, and the ontology to compare and link the relationship between different scientific research; there may be different scientific research with the same keywords; the same scientific research may have different titles. In addition, scientific research may be written in different languages, but current scientific social networks do not take into account the multiple languages of researchers and research. they cannot link the relationship and compare scientific research written in different languages. To solve these problems, this paper proposes the use of a multi-lingual ontology to determine and describe scientific research. This work uses ontology in the context of a conceptual indexation, by separating the concept from the term, thus, the result of this work according to the proposed approach will make it possible to express the concept (knowledge extra-linguistic), in different languages, this will allow comparing and linking the relationship between scientific research written in different languages and measuring the percentage of similarity and difference between them. Dealing with multilingualism in scientific research is a very important contribution to this field.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Future} {Networks} and {Distributed} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Meftah, Mohammed Charaf Eddine and Kazar, Okba},
	year = {2024},
	note = {event-place: Dubai, United Arab Emirates},
	pages = {114--124},
}

@article{kayali_chorus_2024,
	title = {Chorus: {Foundation} {Models} for {Unified} {Data} {Discovery} and {Exploration}},
	volume = {17},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3659437.3659461},
	doi = {10.14778/3659437.3659461},
	abstract = {We apply foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMS) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. We investigate the fundamental characteristics of this approach including generalizability to several foundation models and the impact of non-determinism on the outputs. All in all, this suggests a future direction in which disparate data management tasks can be unified under foundation models.},
	number = {8},
	journal = {Proc. VLDB Endow.},
	author = {Kayali, Moe and Lykov, Anton and Fountalis, Ilias and Vasiloglou, Nikolaos and Olteanu, Dan and Suciu, Dan},
	month = apr,
	year = {2024},
	note = {Publisher: VLDB Endowment},
	pages = {2104--2114},
}

@inproceedings{de_freitas_towards_2022,
	address = {New York, NY, USA},
	series = {{IHC} '22},
	title = {Towards an ontology-based approach to develop software systems with adaptive user interface},
	isbn = {978-1-4503-9506-9},
	url = {https://doi.org/10.1145/3554364.3559139},
	doi = {10.1145/3554364.3559139},
	abstract = {The new ways of manipulating computers, smartphones and other devices have brought challenges such as the need to ensure a good usability when different user types use the same system. Adaptive user interface (AUI) systems are a possible solution. They change the user interface to better meet the needs of different users. However, developing such systems is not trivial. It is necessary to capture the users' characteristics and preferences and constantly adapt the system accordingly. In this paper, we discuss the use of ontologies to support the development of AUI systems. We argue that by providing structured knowledge about such systems, ontologies help understand how they work and offer a basis to structure them, identify the necessary adaptations and implement mechanisms to make them happen in run-time. We have explored the use of ontologies from an ontology network to develop a social network about academic subjects that automatically adapts its interface according to the low vision and colorblind user's needs and usage characteristics. The first version of an ontology-based process to guide the development of AUI systems raised from this experience.},
	booktitle = {Proceedings of the 21st {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {de Freitas, Alexandre A. C. and Scalser, Murilo B. and Costa, Simone D. and Barcellos, Monalessa P.},
	year = {2022},
	note = {event-place: Diamantina, Brazil},
	keywords = {ontology, adaptive user interface, ontology network},
}

@inproceedings{wisniewski_seequery_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {{SeeQuery}: {An} {Automatic} {Method} for {Recommending} {Translations} of {Ontology} {Competency} {Questions} into {SPARQL}-{OWL}},
	isbn = {978-1-4503-8446-9},
	url = {https://doi.org/10.1145/3459637.3482387},
	doi = {10.1145/3459637.3482387},
	abstract = {Ontology authoring is a complicated and error-prone process since the knowledge being modeled is expressed using logic-based formalisms, in which logical consequences of the knowledge have to be foreseen. To make that process easier, competency questions (CQs), being questions expressed in natural language are often stated to trace both the correctness and completeness of the ontology at a given time. However, CQs have to be translated into a formal language, like ontology query language (SPARQL-OWL), to query the ontology. Since the translation step is time-consuming and requires familiarity with the query language used, in this paper, we propose an automatic method named SeeQuery, which recommends SPARQL-OWL queries being translations of CQs stated against a given ontology. It consists of a pipeline of transformations based on template matching and filling, being motivated by the biggest to date publicly available CQ to SPARQL-OWL datasets. We provide a detailed description of SeeQuery and evaluate the method on a separate set of 2 ontologies with their CQs. It is, to date, the only automatic method available for recommending SPARQL-OWL queries out of CQs. The source code of SeeQuery is available at: https://github.com/dwisniewski/SeeQuery.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Wisniewski, Dawid and Potoniec, Jedrzej and Lawrynowicz, Agnieszka},
	year = {2021},
	note = {event-place: Virtual Event, Queensland, Australia},
	keywords = {semantic similarity, competency questions, automatic translation, ontology authoring, sparql-owl},
	pages = {2119--2128},
}

@inproceedings{alharbi_characterising_2021,
	address = {New York, NY, USA},
	series = {K-{CAP} '21},
	title = {Characterising the {Gap} {Between} {Theory} and {Practice} of {Ontology} {Reuse}},
	isbn = {978-1-4503-8457-5},
	url = {https://doi.org/10.1145/3460210.3493568},
	doi = {10.1145/3460210.3493568},
	abstract = {Ontology reuse is a complex process that requires the support of methodologies and tools to minimise errors and to keep the ontologies consistent and coherent. Although the vast majority of ontology engineering methodologies include a reuse phase, and reuse has been investigated for different tasks and purposes (e.g.ontology integration), this body of work does not seem to translate into practice, neither in the form of strict criteria for reuse, nor as a set of community proposed guidelines. In this paper, we report the salient results from a study aimed at ontology developers and practitioners, whose objective is to gain an insight into the gap between the theory and the practice of ontology reuse. Thefocus of our study is to gain practitioners' views on i) their preferred reuse approaches; ii) the types of ontologies they tend to reuse (e.g. specific domain ontologies or upper-level ontologies)iii) what reporting information they deem useful when deciding which ontology to reuse; iv) what are the main reasons deterring them from reusing an ontology. Our findings confirm and extend established results from the literature, but in addition, the study provides a fresh view on the practice of reuse with an explicit focus on highly experienced developers and moderately experienced ones. The study corroborates the need for a comprehensive set of recommendations, that are widely accepted by the community, and are possibly implemented in development tools.},
	booktitle = {Proceedings of the 11th {Knowledge} {Capture} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Alharbi, Reham and Tamma, Valentina and Grasso, Floriana},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {ontology engineering, challenges to ontology reuse, ontology development methodologies, ontology reuse},
	pages = {217--224},
}

@article{xia_fetilda_2024,
	title = {{FETILDA}: {Evaluation} {Framework} for {Effective} {Representations} of {Long} {Financial} {Documents}},
	volume = {18},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3657299},
	doi = {10.1145/3657299},
	abstract = {In the financial sphere, there is a wealth of accumulated unstructured financial data, such as the textual disclosure documents that companies submit on a regular basis to regulatory agencies, such as the Securities and Exchange Commission. These documents are typically very long and tend to contain valuable soft information about a company’s performance that is not present in quantitative predictors. It is therefore of great interest to learn predictive models from these long textual documents, especially for forecasting numerical key performance indicators. In recent years, there has been great progress in natural language processing via pre-trained language models (LMs) learned from large corpora of textual data. This prompts the important question of whether they can be used effectively to produce representations for long documents, as well as how we can evaluate the effectiveness of representations produced by various LMs. Our work focuses on answering this critical question, namely, the evaluation of the efficacy of various LMs in extracting useful soft information from long textual documents for prediction tasks. In this article, we propose and implement a deep learning evaluation framework that utilizes a sequential chunking approach combined with an attention mechanism. We perform an extensive set of experiments on a collection of 10-K reports submitted annually by U.S. banks, and another dataset of reports submitted by U.S. companies, to investigate thoroughly the performance of different types of language models. Overall, our framework using LMs outperforms strong baseline methods for textual modeling as well as for numerical regression. Our work provides better insights into how utilizing pre-trained domain-specific and fine-tuned long-input LMs for representing long documents can improve the quality of representation of textual data and, therefore, help in improving predictive analyses.},
	number = {7},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Xia, Bolun (Namir) and Rawte, Vipula and Gupta, Aparna and Zaki, Mohammed},
	month = jun,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {language models, 10-K reports, financial documents, long text documents, Text regression},
}

@inproceedings{contreras_ontology_2020,
	address = {New York, NY, USA},
	series = {{SPML} '20},
	title = {Ontology {Learning} using {Hybrid} {Machine} {Learning} {Algorithms} for {Disaster} {Risk} {Management}},
	isbn = {978-1-4503-7573-3},
	url = {https://doi.org/10.1145/3432291.3432306},
	doi = {10.1145/3432291.3432306},
	abstract = {Disaster is inevitable but manageable thru careful planning, preparation and immediate response strategies. During typhoons, earthquakes and other calamities, agreement about language is vital to understand each other well to avoid high number of deaths, delay in access to basic needs and slow response time. However, some of the people involved in this domain find it hard to coordinate and respond to different emergency situations due to lack of familiarization and knowledge about the different terms or concepts. In disaster risk management, the consistency and reusability of the sharing of information is important to avoid possible risks. Due to this reason, an ontology is incorporated to aid in the disaster management process. The use of ontology enables quick retrieving and incorporating "consistent data" and information related to disaster management which plays an important for making decisions efficiently. This paper aims to implement and evaluate the accuracy of Support Vector Machine (SVM) and Neural Network (NN) learning-based ontology for disaster risk management to enhance the classification of concepts (keywords) generated for the domain ontology. The experiment shows that the hybrid SVM and NN machine learning algorithm outperformed the accuracy of SVM and NN based on the precision, recall and F-Measure criterion.},
	booktitle = {Proceedings of the 2020 3rd {International} {Conference} on {Signal} {Processing} and {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Contreras, Jennifer O. and Ballera, Melvin A. and Festijo, Enrique D.},
	year = {2020},
	note = {event-place: Beijing, China},
	keywords = {Natural Language Processing, Machine Learning, Ontology Learning, Neural Network, Disaster Risk Management, Hybrid Algorithm, Support Vector Machine},
	pages = {13--20},
}

@inproceedings{zhao_complete_2025,
	address = {New York, NY, USA},
	series = {{AIIIP} '24},
	title = {Complete the exploration of low-resource knowledge graph completion based on large model technology},
	isbn = {979-8-4007-0730-8},
	url = {https://doi.org/10.1145/3707292.3707356},
	doi = {10.1145/3707292.3707356},
	abstract = {In the construction and application of knowledge graph, it is a realistic research problem to complete the knowledge in the low resource field. Traditional methods that rely on manual annotation and rules are not only costly, but also have limitations in coverage and scalability. To solve this problem, this paper proposes a large model technique, combining fine-tuning and knowledge transfer strategies. Firstly, to improve the ability of fine-tuning of the large model, the rich knowledge learned by the large model in the high resource field to assist the completion of the low resource knowledge graph through knowledge transfer technology to make up for the shortage of direct extraction. The experimental results show that this method can effectively improve the completion rate and accuracy of the knowledge graph, especially in the completion of entity relations and attribute filling. Furthermore, we explore the impact of different fine-tuning strategies and knowledge transfer methods on the completion effect, providing experimental empirical and theoretical support for future studies on similar issues.},
	booktitle = {Proceedings of the 2024 3rd {International} {Conference} on {Artificial} {Intelligence} and {Intelligent} {Information} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Pei and Zhang, Longxing and Zhao, Jiawen},
	year = {2025},
	keywords = {Knowledge graph, Fine-tuning, Transfer learning, Low resource, Large model technology},
	pages = {140--145},
}

@article{artale_first-order_2022,
	title = {First-{Order} {Rewritability} and {Complexity} of {Two}-{Dimensional} {Temporal} {Ontology}-{Mediated} {Queries}},
	volume = {75},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.13511},
	doi = {10.1613/jair.1.13511},
	abstract = {Aiming at ontology-based data access to temporal data, we design two-dimensional temporal ontology and query languages by combining logics from the (extended) DL-Lite family with linear temporal logic LTL over discrete time (Z,\&lt;). Our main concern is first-order rewritability of ontology-mediated queries (OMQs) that consist of a 2D ontology and a positive temporal instance query. Our target languages for FO-rewritings are two-sorted FO(\&lt;) – first-order logic with sorts for time instants ordered by the built-in precedence relation \&lt; and for the domain of individuals—its extension FO(\&lt;, ≡) with the standard congruence predicates t ≡ 0 (mod n), for any fixed n \&gt; 1, and FO(RPR) that admits relational primitive recursion. In terms of circuit complexity, FO(\&lt;, ≡)- and FO(RPR)-rewritability guarantee answering OMQs in uniform AC0 and NC1, respectively. We proceed in three steps. First, we define a hierarchy of 2D DL-Lite/LTL ontology languages and investigate the FO-rewritability of OMQs with atomic queries by constructing projections onto 1D LTL OMQs and employing recent results on the FO-rewritability of propositional LTL OMQs. As the projections involve deciding consistency of ontologies and data, we also consider the consistency problem for our languages. While the undecidability of consistency for 2D ontology languages with expressive Boolean role inclusions might be expected, we also show that, rather surprisingly, the restriction to Krom and Horn role inclusions leads to decidability (and ExpSpace-completeness), even if one admits full Booleans on concepts. As a final step, we lift some of the rewritability results for atomic OMQs to OMQs with expressive positive temporal instance queries. The lifting results are based on an in-depth study of the canonical models and only concern Horn ontologies.},
	journal = {J. Artif. Int. Res.},
	author = {Artale, Alessandro and Kontchakov, Roman and Kovtunova, Alisa and Ryzhikov, Vladislav and Wolter, Frank and Zakharyaschev, Michael},
	month = dec,
	year = {2022},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
}

@inproceedings{zavada_transpilers_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {From {Transpilers} to {Semantic} {Libraries}: {Formal} {Verification} {With} {Pluggable} {Semantics}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3686251},
	doi = {10.1145/3652620.3686251},
	abstract = {In the field of model-based systems engineering, there is an increasing demand for the application of formal methods. However, this requires expertise in formal methods, which cannot be expected from systems engineers. While several attempts have been made to bridge this gap, there are still open questions. (1) With the trend shifting towards ontological languages, systems are modeled as classes of 4D occurrences, rather than a 3D system evolving with time, which hinders the application of state-of-the-art model checking algorithms. (2) Ontological reasoning cannot handle the state space explosion problem, and can even make it harder for verifiers to operate efficiently. (3) When operationalizing ontological languages, we need to validate the conformance of the two semantics, even in the presence of optimizations. (4) On top of all, these challenges must be solved for every new engineering language, version, or variant. In this paper, we propose a new approach to address the aforementioned challenges. To validate its feasibility, we present a prototype tool and evaluate it on a SysML model.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zavada, Ármin and Marussy, Kristóf and Molnár, Vince},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {model-based systems engineering, metaprogramming, declarative interpretation, formal verification, kernel modeling language, operational libraries, semantic libraries},
	pages = {311--317},
}

@inproceedings{cao_research_2025,
	address = {New York, NY, USA},
	series = {{ICCSMT} '24},
	title = {Research on the {Construction} of a {Knowledge} {Graph} for {Miao} {Medicine} {Based} on the {BERT}-{BiLSTM}-{CRF} {Model}},
	isbn = {979-8-4007-0999-9},
	url = {https://doi.org/10.1145/3708036.3708130},
	doi = {10.1145/3708036.3708130},
	abstract = {This study focuses on constructing a knowledge graph for Miao medicine to promote the understanding of traditional Miao cultural practices, enhance public health capabilities, and preserve this intangible cultural heritage. The BERT-BiLSTM-CRF model, combined with the manual review, was utilized for entity recognition, while SBERT technology was used for entity alignment. As a result, a comprehensive knowledge graph of Miao medicine was constructed, including 11,111 nodes and 143,751 relationships. This knowledge graph systematically organizes the complex knowledge system of Miao medicine, including diseases, symptoms, drugs, and their interrelations. It supports efficient knowledge querying, reasoning, and discovery, contributing valuable insights into applying knowledge graph technologies in traditional medicine. Moreover, this research offers a framework that can be adapted for constructing knowledge graphs in other conventional medical systems.},
	booktitle = {Proceedings of the 2024 5th {International} {Conference} on {Computer} {Science} and {Management} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Wanghua and Xia, Huan and Xie, Rongdong and Hu, Jiangyu},
	year = {2025},
	keywords = {knowledge graph, intangible cultural heritage, BERT-BiLSTM-CRF model, Miao medicine},
	pages = {555--561},
}

@inproceedings{boyer_natural_2018,
	address = {USA},
	series = {{CASCON} '18},
	title = {Natural language question answering in the financial domain},
	abstract = {This paper describes a natural language question answering system focused on answering financial domain questions using a daily updated corpus of financial reports. Financial entity types of interest included company stocks, country bonds, currencies, industries, commodities, and diversified assets. Financial questions of interest included explanatory and factual questions about entities as well as financial outlook for entities.An important architectural divergence emerged between the approach required for answering financial outlook questions versus the approach for answering other financial information questions. The financial domain focus also introduced additional challenges to open domain natural language processing that were addressed in the areas of document ingestion, question classification accuracy, question analysis techniques, speed of machine learning, answer ranking by linguistic confidence versus temporality, and system accuracy assessment.},
	booktitle = {Proceedings of the 28th {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Boyer, John M.},
	year = {2018},
	note = {event-place: Markham, Ontario, Canada},
	keywords = {text analytics, question answering systems, financial domain, financial information retrieval, financial sentiment analysis, question analysis, question classification},
	pages = {189--200},
}

@inproceedings{fu_impending_2022,
	address = {New York, NY, USA},
	series = {{AVI} '22},
	title = {Impending {Success} or {Failure}? {An} {Investigation} of {Gaze}-{Based} {User} {Predictions} {During} {Interaction} with {Ontology} {Visualizations}},
	isbn = {978-1-4503-9719-3},
	url = {https://doi.org/10.1145/3531073.3531081},
	doi = {10.1145/3531073.3531081},
	abstract = {Designing and developing innovative visualizations to assist humans in the process of generating and understanding complex semantic data has become an important element in supporting effective human-ontology interaction, as visual cues are likely to provide clarity, promote insight, and amplify cognition. While recent research has indicated potential benefits of applying novel adaptive technologies, typical ontology visualization techniques have traditionally followed a one-size-fits-all approach that often ignores an individual user's preferences, abilities, and visual needs. In an effort to realize adaptive ontology visualization, this paper presents a potential solution to predict a user's likely success and failure in real time, and prior to task completion, by applying established machine learning models on eye gaze generated during an interactive session. These predictions are envisioned to inform future adaptive ontology visualizations that could potentially adjust its visual cues or recommend alternative visualizations in real time to improve individual user success. This paper presents findings from a series of experiments to demonstrate the feasibility of gaze-based success and failure predictions in real time that can be achieved with a number of off-the-shelf classifiers without the need of expert configurations in the presence of mixed user backgrounds and task domains across two commonly used fundamental ontology visualization techniques.},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Advanced} {Visual} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Fu, Bo and Steichen, Ben},
	year = {2022},
	note = {event-place: Frascati, Rome, Italy},
	keywords = {Ontology Visualization, Eye Tracking, Predictive Analytics},
}

@inproceedings{calvanese_realizing_2021,
	address = {New York, NY, USA},
	series = {{CHItaly} '21},
	title = {Realizing {Ontology}-based {Reusable} {Interfaces} for {Data} {Access} via {Virtual} {Knowledge} {Graphs}},
	isbn = {978-1-4503-8977-8},
	url = {https://doi.org/10.1145/3464385.3464744},
	doi = {10.1145/3464385.3464744},
	abstract = {In this paper, we present a comprehensive framework, which we call VKG-UI, for realizing ontology-based reusable user interfaces (UIs) for data access via virtual knowledge graphs (VKGs). The VKG approach uses an ontology to model the domain of interest and to hide the heterogeneity of the underlying data sources. Reusable UIs can be built by relying on queries that are issued to the VKG system and that use the high level vocabulary from the ontology layer. This use of VKGs allows for decoupling the data from the UIs, and brings great reusability in designing the latter. To illustrate our approach, we introduce significant use cases with various types of UIs, including programming, graphic, natural language, and voice interfaces.},
	booktitle = {Proceedings of the 14th {Biannual} {Conference} of the {Italian} {SIGCHI} {Chapter}},
	publisher = {Association for Computing Machinery},
	author = {Calvanese, Diego and Ding, Linfang and Mosca, Alessandro and Xiao, Guohui},
	year = {2021},
	note = {event-place: Bolzano, Italy},
	keywords = {ontology, user interface, data access, virtual knowledge graph},
}

@inproceedings{yang_surj_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Surj: {Ontological} {Learning} for {Fast}, {Accurate}, and {Robust} {Hierarchical} {Multi}-label {Classification}},
	isbn = {978-1-4503-9130-6},
	url = {https://doi.org/10.1145/3487553.3524723},
	doi = {10.1145/3487553.3524723},
	abstract = {We consider multi-label classification in the context of complex hierarchical relationships organized into an ontology. These situations are ubiquitous in learning problems on the web and in science, where rich domain models are developed but labeled data is rare. Most existing solutions model the problem as a sequence of simpler problems: one classifier for each level in the hierarchy, or one classifier for each label. These approaches require more training data, which is often unavailable in practice: as the ontology grows in size and complexity, it becomes unlikely to find training examples for all expected combinations. In this paper, we learn offline representations of the ontology using a graph autoencoder and separately learn to classify input records, reducing dependence on training data: Since the relationships between labels are encoded independently of training data, the model can make predictions even for underrepresented labels, naturally generalize to DAG-structured ontologies, remain robust to low-data regimes, and, with minor offline retraining, tolerate evolving ontologies. We show empirically that our label predictions respect the hierarchy (predicting a descendant implies predicting its ancestors) and propose a method of evaluating hierarchy violations that properly ignores irrelevant violations. Our main result is that our model outperforms all state-of-the-art models on 17 of 20 datasets across multiple domains by a significant margin, even with limited training data.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Yang, Sean T. and Howe, Bill},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {Graph Learning, Ontology Learning, Hierarchical Multi-label classification},
	pages = {1106--1114},
}

@inproceedings{tang_generic_2024,
	address = {New York, NY, USA},
	series = {{BDEIM} '23},
	title = {Generic {Ontologies} for {Digital} {Watersheds}},
	isbn = {979-8-4007-1666-9},
	url = {https://doi.org/10.1145/3659211.3659257},
	doi = {10.1145/3659211.3659257},
	abstract = {Digital Watershed represents an effective strategy for addressing floods and mitigating their associated risks. However, a notable challenge lies in the absence of a universally applicable modeling approach for constructing digital watersheds. The wealth of data and knowledge about watersheds is currently managed in a fragmented manner, impeding a comprehensive and cohesive understanding of the subject. This paper addresses the fragmented control landscape in watershed management by introducing generic ontologies, including water conservancy object ontology, model ontology, rainfall and runoff scene-mode ontology, and event ontology. These ontologies standardize the representation of water conservancy objects, hydrological models, and expert knowledge while also defining structured representations for physical events, scheduling rules, and business processes, which contribute to breaking the paradigm of "one watershed, one system" and facilitate integrated flood prediction and scheduling.},
	booktitle = {Proceedings of the 2023 4th {International} {Conference} on {Big} {Data} {Economy} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Hailin and Feng, Jun and Zhou, Siyuan},
	year = {2024},
	note = {event-place: Zhengzhou, China},
	pages = {260--266},
}

@inproceedings{balsebre_city_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {City {Foundation} {Models} for {Learning} {General} {Purpose} {Representations} from {OpenStreetMap}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679662},
	doi = {10.1145/3627673.3679662},
	abstract = {Pre-trained Foundation Models (PFMs) have ushered in a paradigm-shift in AI, due to their ability to learn general-purpose representations that can be readily employed in downstream tasks. While PFMs have been successfully adopted in various fields such as NLP and Computer Vision, their capacity in handling geospatial data remains limited. This can be attributed to the intrinsic heterogeneity of such data, which encompasses different types, including points, segments and regions, as well as multiple information modalities. The proliferation of Volunteered Geographic Information initiatives, like OpenStreetMap, unveils a promising opportunity to bridge this gap. In this paper, we present CityFM, a self-supervised framework to train a foundation model within a selected geographical area. CityFM relies solely on open data from OSM, and produces multimodal representations, incorporating spatial, visual, and textual information. We analyse the entity representations generated by our foundation models from a qualitative perspective, and conduct experiments on road, building, and region-level downstream tasks. In all the experiments, CityFM achieves performance superior to, or on par with, application-specific algorithms.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Balsebre, Pasquale and Huang, Weiming and Cong, Gao and Li, Yi},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {foundation models, geospatial data, contrastive learning},
	pages = {87--97},
}

@inproceedings{christian_ontology-driven_2021,
	address = {New York, NY, USA},
	series = {{CCS} '21},
	title = {An {Ontology}-driven {Knowledge} {Graph} for {Android} {Malware}},
	isbn = {978-1-4503-8454-4},
	url = {https://doi.org/10.1145/3460120.3485353},
	doi = {10.1145/3460120.3485353},
	abstract = {We present MalONT2.0 – an ontology for malware threat intelligence [4]. New classes (attack patterns, infrastructural resources to enable attacks, malware analysis to incorporate static analysis, and dynamic analysis of binaries) and relations have been added following a broadened scope of core competency questions. MalONT2.0 allows researchers to extensively capture all requisite classes and relations that gather semantic and syntactic characteristics of an android malware attack. This ontology forms the basis for the malware threat intelligence knowledge graph, MalKG, which we exemplify using three different, non-overlapping demonstrations. Malware features have been extracted from openCTI reports on android threat intelligence shared on the Internet and written in the form of unstructured text. Some of these sources are blogs, threat intelligence reports, tweets, and news articles. The smallest unit of information that captures malware features is written as triples comprising head and tail entities, each connected with a relation. In the poster and demonstration, we discuss MalONT2.0 and MalKG.},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Christian, Ryan and Dutta, Sharmishtha and Park, Youngja and Rastogi, Nidhi},
	year = {2021},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {knowledge graphs, ontology, inference, malware},
	pages = {2435--2437},
}

@inproceedings{yu_multidimensional_2025,
	address = {New York, NY, USA},
	series = {{BDEIM} '24},
	title = {Multidimensional model for stock prediction using investors’ commentary},
	isbn = {979-8-4007-1186-2},
	url = {https://doi.org/10.1145/3724154.3724251},
	doi = {10.1145/3724154.3724251},
	abstract = {Forecasting share prices has always been a major concern in the financial sector, as several factors and investor's comments influence share price movements have a tangible impact on the stock market. The research is the quantification of investor comments as indicators of emotional tendencies and constructs multidimensional feature sets to predict stock prices based on stock fundamentals, technical aspects, and information aspects. To enhance the correlation between the emotional tendencies of stock investors and other characteristics, meanwhile, to extract implicit information from these characteristics, we have created a sliding window structure with Pearson correlation, and channel convolution attention modules(CCAM). This paper develops a Multidimensional Fusion Model (MFM) consisting of four components: the text-only model, the one-dimensional price model I, the multidimensional price model II, and the data fusion module. Multi-source heterogeneous data is used to construct the dataset, which contains numerical data on stock fundamentals and technical aspects, and text data on information aspects, which collects investor comments during a moving window period. Expert knowledge is used to mark the emotional polarity of the text in the batch. The constructed text-only model aims to reveal the implicit relationship between the comment texts. To quantify the emotional tendency of massive stock investors on a given day, the Bert/ Enhanced Representation through Knowledge Integration (ERNIE) of word embedding method and the Convolutional Neural Networks (CNN)/ Deep Pyramid-CNN (DPCNN)/ Region-CNN (RCNN) classifier are used models. Model I (CEEMDAN-WOA-BiLSTM) analyses unidimensional stock closing prices and interprets stock price fluctuations as digital signals, whereas Model II (CCAM-Attention-BiLSTM based on Pearson correlation data integration) utilizes multidimensional data encompassing emotional tendencies. On this basis, a constrained Data Fusion Module (DFM) is proposed to process the prediction results of Model I and Model II. For the Stock Index dataset 000881.SZ, covering the period 2017-2023. The BERT-DPCNN-MFM model is one of the most efficient MFMs produced, it obtained 0.0947, 0.0699, and 0.0090 results in RMSE, MAE, and MSE, respectively. Simulation results verify the effectiveness of the model, while proving that the model can reflect the short-term fluctuation of stock prices.},
	booktitle = {Proceedings of the 2024 5th {International} {Conference} on {Big} {Data} {Economy} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Jie and Yao, Minghui},
	year = {2025},
	keywords = {CCAM-Attention-BiLSTM, Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), emotional tendencies, Pearson correlation, Stock predictors},
	pages = {582--593},
}

@inproceedings{martorana_advancing_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {Advancing data sharing and reusability for restricted access data on the {Web}: introducing the {DataSet}-{Variable} {Ontology}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627559},
	doi = {10.1145/3587259.3627559},
	abstract = {In response to the increasing volume of research data being generated, more and more data portals have been designed to facilitate data findability and accessibility. However, a significant portion of this data remains confidential or restricted due to its sensitive nature, such as patient data or census microdata. While maintaining confidentiality prohibits its public release, the emergence of portals supporting rich metadata can help enable researchers to at least discover the existence of restricted access data, empowering them to assess the suitability of the data before requesting access. Existing standards, such as CSV on the Web and RDF Data Cube, have been adopted to facilitate data management, integration, and re-use of data on the Web. However, the current landscape still lacks adequate standards not only to effectively describe restricted access data while preserving confidentiality but also to facilitate its discovery. In this work, we investigate the relationship between the structural, statistical, and semantic elements of restricted access tabular data, and we explore how such relationship can be formally modeled in a way that is Findable, Accessible, Interoperable, and Reusable. We introduce the DataSet-Variable Ontology (DSV), that by combining CSV on the Web and RDF Data Cube standards, leveraging semantic technologies and Linked Data principles, and introducing variable-level metadata, aims to capture high-quality metadata to support the management and re-use of restricted access data on the Web. As evaluation, we conducted a case study where we applied DSV to four different datasets from different statistical governmental agencies. We employed a set of competency questions to assess the ontology’s ability to support knowledge discovery and data exploration. By describing high-quality metadata, both at the dataset- and variable levels, while maintaining data privacy, this novel ontology facilitates data interoperability, discovery, and re-use and it empowers researchers to manage, integrate, and analyze complex restricted access data sources.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Martorana, Margherita and Kuhn, Tobias and Siebes, Ronald and Van Ossenbruggen, Jacco},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {Semantic Web, FAIR principles, Privacy-preserving web data, Restricted access data, Variable-level metadata},
	pages = {83--91},
}

@article{hou_cr-transr_2024,
	title = {{CR}-{TransR}: {A} {Knowledge} {Graph} {Embedding} {Model} for {Cultural} {Domain}},
	volume = {17},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3625299},
	doi = {10.1145/3625299},
	abstract = {As a combination of information computing technology and the cultural field, cultural computing is gaining more attention. The knowledge graph is also gradually applied as a particular data structure in the cultural area. Based on the domain knowledge graph data of the Beijing Municipal Social Science Project “Mining and Utilization of Cultural Resources in the Ancient Capital of Beijing,” this article proposes a graph representation learning model CR-TransR that integrates cultural attributes. Through the analysis of the data in the cultural field of the ancient capital of Beijing, a cultural feature dictionary is constructed, and a domain-specific feature matrix is constructed in the form of word vector splicing. The feature matrix is used to constrain the embedding graph model TransR, and then the feature matrix and the TransR model are jointly trained to complete the embedded expression of the knowledge graph. Finally, a comparative experiment is carried out on the Beijing ancient capital cultural knowledge graph dataset and the effects of the classic graph embedding algorithms TransE, TransH, and TransR. At the same time, we try to reproduce the embedding method with the core idea of neighbor node information aggregation as the core idea, and CRTransR are compared. The experimental tasks include link prediction and triplet classification, and the experimental results show that the CRTransR model performs better.},
	number = {1},
	journal = {J. Comput. Cult. Herit.},
	author = {Hou, Wenjun and Bai, Bing and Cai, Chenyang},
	month = feb,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {knowledge graph, Beijing ancient capital culture, cultural calculation, graph embedding},
}

@inproceedings{laddada_ontology-based_2020,
	address = {New York, NY, USA},
	series = {{LocalRec}'20},
	title = {Ontology-{Based} {Approach} for {Neighborhood} and {Real} {Estate} {Recommendations}},
	isbn = {978-1-4503-8160-4},
	url = {https://doi.org/10.1145/3423334.3431452},
	doi = {10.1145/3423334.3431452},
	abstract = {Suggesting services or products to people is a task that should be handled by recommendation systems due to the important increase of information and the multitude of user criteria. In fact, when expressing wishes for a product, a user is influenced by his/her tastes or priorities. These influential characteristics tend to be challenging regarding their integration into recommendation systems, because interaction between the products/services and the user has to be captured through its preferences. Recommendation systems for neighborhood and real estate search are no exception, and to achieve reliable recommendation, we developed an ontology NAREO (Neighborhood And Real Estate Ontology) where environment characteristics related to user preferences are modeled with other geo-semantic descriptions. This ontology can be enriched by SWRL (Semantic Web Rule Language) rules that enhance the semantics of our knowledge base and allow reasoning process through built-ins. To illustrate a use case, we provide a basic set of predefined rules for the recommendation context. User preferences are managed through SPARQL queries taking into account the result of inferences.},
	booktitle = {Proceedings of the 4th {ACM} {SIGSPATIAL} {Workshop} on {Location}-{Based} {Recommendations}, {Geosocial} {Networks}, and {Geoadvertising}},
	publisher = {Association for Computing Machinery},
	author = {Laddada, Wissame and Duchateau, Fabien and Favetta, Franck and Moncla, Ludovic},
	year = {2020},
	note = {event-place: Seattle, WA, USA},
	keywords = {Ontology, Recommendation Systems, Spatial modeling, SWRL reasoning},
}

@inproceedings{charalambous_analyzing_2022,
	address = {New York, NY, USA},
	series = {{ARES} '22},
	title = {Analyzing {Coverages} of {Cyber} {Insurance} {Policies} {Using} {Ontology}},
	isbn = {978-1-4503-9670-7},
	url = {https://doi.org/10.1145/3538969.3544453},
	doi = {10.1145/3538969.3544453},
	abstract = {In an era where all the transactions, businesses and services are becoming digital and online, the data assets and the services protection are of utmost importance. Cyber-insurance companies are offering a wide range of coverages, but they also have exclusions. Customers of these companies need to be able to understand the terms and conditions of the related contracts and furthermore they need to be able to compare various offerings in order to determine the most appropriate solutions for their needs. The research in the area is very limited while at the same time the related market is growing, giving every potential solution a high value. In this paper, we propose a methodology and a prototype system that will help customers to compare contracts based on a pre-defined ontology that is describing cyber-insurance terms. After a first preliminary analysis and validation, our approach accuracy is averaging at almost 50\%, giving a promising initial evaluation. Fine tuning, larger data set assessment and ontology refinement will be our next steps to improve the accuracy of our tool. Real user evaluation will follow, in order to evaluate the tool in real world cases.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Charalambous, Markos and Farao, Aristeidis and Kalantzantonakis, George and Kanakakis, Panagiotis and Salamanos, Nikos and Kotsifakos, Evangelos and Froudakis, Evangellos},
	year = {2022},
	note = {event-place: Vienna, Austria},
	keywords = {Ontology, Coverages, Cyber-insurance, Exclusions, Premium, Weakest link},
}

@inproceedings{xu_trusted_2024,
	address = {New York, NY, USA},
	series = {{ICBAR} '23},
	title = {Trusted {Non}-intrusive {Data} {Exchange} based on {Ontology} in {Logistics} {Industry}},
	isbn = {979-8-4007-1647-8},
	url = {https://doi.org/10.1145/3656766.3656929},
	doi = {10.1145/3656766.3656929},
	abstract = {The logistics industry is becoming increasingly important in our daily lives, leading to a growing demand for digitalization within the sector. However, due to concerns about data privacy, logistics entities have formed natural information silos, which have made logistics data difficult to exchange and share. To address this issue, this paper proposes an ontology-based logistics data exchange model that utilizes blockchain technology to establish user trust among information silos. The model in this paper fuses non-intrusive data access and Ontology-Based Data Access (OBDA) to achieve both precise control over the data access process and interconnection among heterogeneous systems. The proposed model creates a new trustworthy and controllable solution for the circulation of logistics data within the logistics industry, without the need for the secondary development of logistics information systems.},
	booktitle = {Proceedings of the 2023 3rd {International} {Conference} on {Big} {Data}, {Artificial} {Intelligence} and {Risk} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Lianzheng and Fu, Deqian and Qiu, Jianlong},
	year = {2024},
	note = {event-place: Chengdu, China},
	pages = {986--991},
}

@article{roy_explainable_2023,
	title = {Explainable {Activity} {Recognition} in {Videos} using {Deep} {Learning} and {Tractable} {Probabilistic} {Models}},
	volume = {13},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/3626961},
	doi = {10.1145/3626961},
	abstract = {We consider the following video activity recognition (VAR) task: given a video, infer the set of activities being performed in the video and assign each frame to an activity. Although VAR can be solved accurately using existing deep learning techniques, deep networks are neither interpretable nor explainable and as a result their use is problematic in high stakes decision-making applications (in healthcare, experimental Biology, aviation, law, etc.). In such applications, failure may lead to disastrous consequences and therefore it is necessary that the user is able to either understand the inner workings of the model or probe it to understand its reasoning patterns for a given decision. We address these limitations of deep networks by proposing a new approach that feeds the output of a deep model into a tractable, interpretable probabilistic model called a dynamic conditional cutset network that is defined over the explanatory and output variables and then performing joint inference over the combined model. The two key benefits of using cutset networks are: (a) they explicitly model the relationship between the output and explanatory variables and as a result, the combined model is likely to be more accurate than the vanilla deep model and (b) they can answer reasoning queries in polynomial time and as a result, they can derive meaningful explanations by efficiently answering explanation queries. We demonstrate the efficacy of our approach on two datasets, Textually Annotated Cooking Scenes (TACoS), and wet lab, using conventional evaluation measures such as the Jaccard Index and Hamming Loss, as well as a human-subjects study.},
	number = {4},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Roy, Chiradeep and Nourani, Mahsan and Arya, Shivvrat and Shanbhag, Mahesh and Rahman, Tahrima and Ragan, Eric D. and Ruozzi, Nicholas and Gogate, Vibhav},
	month = dec,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {cutset networks, dynamic Bayesian networks, Temporal models, tractable probabilistic models},
}

@article{palacio_delfos_2024,
	title = {The {Delfos} {Platform}: {A} {Conceptual} {Model}-{Based} {Solution} for the {Enhancement} of {Precision} {Medicine}},
	volume = {21},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2024.3377928},
	doi = {10.1109/TCBB.2024.3377928},
	abstract = {The use in the clinical practice of the vast amount of genomic data generated by current sequencing technologies constitutes a bottleneck for the progress of Precision Medicine (PM). Various problems inherent to the genomics domain (i.e., dispersion, heterogeneity, discrepancies, lack of standardization, and data quality issues) remain unsolved. In this paper, we present the Delfos platform, a conceptual model-based solution developed following a rigorous methodological and ontological background, whose main aim is to minimize the impact of these problems when transferring the research results to clinical practice. This paper presents the SILE method that provides methodological support for the Delfos platform, the Conceptual Schema of the Genome that provides a shared understanding of the domain, and the technological architecture behind the implementation of the platform. This paper also exemplifies the use of the Delfos platform through two use cases that involve the study of the DNA variants associated with the risk of developing Dilated Cardiomyopathies and Neuroblastoma.},
	number = {5},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Palacio, Ana León and S., Alberto García and Román, José Fabián Reyes and Costa, Mireia and Pastor, Oscar},
	month = mar,
	year = {2024},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {1242--1253},
}

@inproceedings{herrmann_using_2022,
	address = {Leuven, BEL},
	series = {{DATE} '22},
	title = {Using ontologies for dataset engineering in automotive {AI} applications},
	isbn = {978-3-9819263-6-1},
	abstract = {Basis of a robust safety strategy for an automated driving function based on neural networks is a detailed description of its input domain, i.e. a description of the environment, in which the function is used. This is required to describe its functional system boundaries and to perform a comprehensive safety analysis. Moreover, it allows to tailor datasets specifically designed for safety related validation tests. Ontologies fulfill the task to gather expert knowledge and model information to enable computer aided processing, while using a notion understandable for humans. In this contribution, we propose a methodology for domain analysis to build up an ontology for perception of autonomous vehicles including characteristic features that become important when dealing with neural networks. Additionally, the method is demonstrated by the creation of a synthetic test dataset for an Euro NCAP-like use case.},
	booktitle = {Proceedings of the 2022 {Conference} \&amp; {Exhibition} on {Design}, {Automation} \&amp; {Test} in {Europe}},
	publisher = {European Design and Automation Association},
	author = {Herrmann, Martin and Witt, Christian and Lake, Laureen and Guneshka, Stefani and Heinzemann, Christian and Bonarens, Frank and Feifel, Patrick and Funke, Simon},
	year = {2022},
	note = {event-place: Antwerp, Belgium},
	keywords = {artificial intelligence, ontology, neural network, autonomous driving, dataset engineering},
	pages = {526--531},
}

@inproceedings{quamar_ontology-based_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {An {Ontology}-{Based} {Conversation} {System} for {Knowledge} {Bases}},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3386139},
	doi = {10.1145/3318464.3386139},
	abstract = {Domain-specific knowledge bases (KB), carefully curated from various data sources, provide an invaluable reference for professionals. Conversation systems make these KBs easily accessible to professionals and are gaining popularity due to recent advances in natural language understanding and AI. Despite the increasing use of various conversation systems in open-domain applications, the requirements of a domain-specific conversation system are quite different and challenging. In this paper, we propose an ontology-based conversation system for domain-specific KBs. In particular, we exploit the domain knowledge inherent in the domain ontology to identify user intents, and the corresponding entities to bootstrap the conversation space. We incorporate the feedback from domain experts to further refine these patterns, and use them to generate training samples for the conversation model, lifting the heavy burden from the conversation designers. We have incorporated our innovations into a conversation agent focused on healthcare as a feature of the IBM Micromedex product.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Quamar, Abdul and Lei, Chuan and Miller, Dorian and Ozcan, Fatma and Kreulen, Jeffrey and Moore, Robert J. and Efthymiou, Vasilis},
	year = {2020},
	note = {event-place: Portland, OR, USA},
	keywords = {knowledge bases, conversation systems, ontology-driven},
	pages = {361--376},
}

@inproceedings{kuchii_kanvas_2022,
	address = {New York, NY, USA},
	series = {{AINTEC} '22},
	title = {{KANVAS}: {A} {Network} {Information} {Sharing} {Framework} {Based} on {Network} {Ontology} {Bonsai}},
	isbn = {978-1-4503-9981-4},
	url = {https://doi.org/10.1145/3570748.3570760},
	doi = {10.1145/3570748.3570760},
	abstract = {Demands for acquiring Internet behavior are increasing for Internet-scale network understanding such as inter-AS path management and traffic engineering. Although there are several efforts to make Internet behavior public, most of the public information is not structured and it is hard for applications to use such information. This paper proposes a network information sharing framework called KANVAS. It defines a network ontology called Bonsai which models network structure from viewpoints of physical, logical, service, and operation network structures. Bonsai can express network virtualization technologies such as link aggregation (LAG), VLAN, L2 over L3 tunneling, and virtual routing and forwarding (VRF). Applications can access network information via useful API. As a first step of development of KANVAS and Bonsai, this paper describes network information sharing within a single domain focusing on failure localization and throughput monitoring as examples. Evaluation results on a PoC system show that the time for failure localization is short enough and a throughput monitoring tool can choose appropriate monitoring points.},
	booktitle = {Proceedings of the 17th {Asian} {Internet} {Engineering} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kuchii, Kanta and Kondo, Takao and Teraoka, Fumio},
	year = {2022},
	note = {event-place: Hiroshima, Japan},
	keywords = {network management, network ontology, fault localization, traffic monitoring},
	pages = {79--87},
}

@inproceedings{wilson_user_2020,
	address = {New York, NY, USA},
	series = {{ICIT} '19},
	title = {User {Needs}-driven {Enrichment} of {Ontology}: {A} case study in {Sri} {Lankan} {Agriculture}},
	isbn = {978-1-4503-7663-1},
	url = {https://doi.org/10.1145/3377170.3377279},
	doi = {10.1145/3377170.3377279},
	abstract = {This study describes the mobile-based user needs-driven knowledge management system that supports the decision making process by considering user needs and preferences. Agriculture is one of the domains, in which, users seek specific information and knowledge relevant to their needs rather than searching and accessing general information from the Web, books, magazines or other information sources. Thus, the conceptualized solution was created by applying participatory sensing, natural language processing and ontology theories and techniques in a novel way in order to satisfy the user needs. The user-centered agriculture ontology that has been developed in our previous work is extended to make an up-to-date knowledge base by capturing user needs and preferences through participatory sensing. The methods of ontology evolution from unstructured data were analyzed to build a technique to enrich the user-centered ontology. The Modified Delphi method is used for verifying the correctness and relevancy of the ontology and the application-based evaluation is applied for checking the functional correctness of the system.},
	booktitle = {Proceedings of the 2019 7th {International} {Conference} on {Information} {Technology}: {IoT} and {Smart} {City}},
	publisher = {Association for Computing Machinery},
	author = {Wilson, R. S. I. and Ginige, Athula and Goonetillake, J. S. and Indika, W. A.},
	year = {2020},
	note = {event-place: Shanghai, China},
	keywords = {Ontology enrichment, Knowledge base, Agriculture, Question Answering, Participatory sensing, User needs in context},
	pages = {581--586},
}

@inproceedings{zhang_ocean_2025,
	address = {New York, NY, USA},
	series = {{BIC} '25},
	title = {Ocean archaea {PPI} prediction with pretraining models},
	isbn = {979-8-4007-1220-3},
	url = {https://doi.org/10.1145/3724979.3725047},
	doi = {10.1145/3724979.3725047},
	abstract = {Protein-Protein Interaction (PPI) provides important insights into the metabolic mechanisms of different biological processes. Although PPIs in some organisms have been investigated systematically, PPIs in the ocean archaea remain largely unexplored. But such species have special investigation value since their adaptation to extreme living conditions may generate unique PPIs. In this paper, we aim to characterize and predict PPIs in ocean archaea to advance understanding of their metabolic networks. First, we collect all ocean archaea PPIs with high confidence from STRING database and analyze the PPI network features, including centrality and enrichment analysis. The functional enrichment results of the largest connecting subgraph in the PPI network show most PPIs in our constructed dataset is related to the translation and transcription processes. Then, we generate an equal number of negative PPI pairs, whose members have either different subcellular locations or GO terms. We also use the generated dataset to test the performance of three pretraining methods and their ensemble methods in the binary PPI prediction task. Our results suggest the ensemble methods could be applied to further improve models’ performance. Fine-tuned models trained on the ocean archaea dataset are expected to predict the other ocean archaea PPIs that are not included in the STRING database and get more understanding about the ocean archaea PPI universe.},
	booktitle = {Proceedings of the 2025 5th {International} {Conference} on {Bioinformatics} and {Intelligent} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Ying and Liu, Yuan and Pan, Xiaoyong and Shen, Hongbin},
	year = {2025},
	keywords = {PPI network, Binary PPI prediction, Ocean archaea, Pretraining model},
	pages = {440--445},
}

@inproceedings{shi_taxonomy_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Taxonomy {Completion} via {Implicit} {Concept} {Insertion}},
	isbn = {979-8-4007-0171-9},
	url = {https://doi.org/10.1145/3589334.3645584},
	doi = {10.1145/3589334.3645584},
	abstract = {beginabstract High quality taxonomies play a critical role in various domains such as e-commerce, web search and ontology engineering. While there has been extensive work on expanding taxonomies from externally mined data, there has been less attention paid to enriching taxonomies by exploiting existing concepts and structure within the taxonomy. In this work, we show the usefulness of this kind of enrichment, and explore its viability with a new taxonomy completion system ICON (I mplicit CON cept Insertion). ICON generates new concepts by identifying implicit concepts based on the existing concept structure, generating names for such concepts and inserting them in appropriate positions within the taxonomy. ICON integrates techniques from entity retrieval, text summary, and subsumption prediction; this modular architecture offers high flexibility while achieving state-of-the-art performance. We have evaluated ICON on two e-commerce taxonomies, and the results show that it offers significant advantages over strong baselines including recent taxonomy completion models and the large language model, ChatGPT.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Shi, Jingchuan and Dong, Hang and Chen, Jiaoyan and Wu, Zhe and Horrocks, Ian},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {Ontology engineering, Ontology, Language model, Pre-trained language model, Taxonomies, ontology engineering, Electronic commerce, Computational linguistics, pre-trained language model, taxonomy completion, taxonomy enrichment, text summarisation, High quality, Text Summarisation, E- commerces, Taxonomy completion, Taxonomy enrichment, Web ontology, Web searches},
	pages = {2159--2169},
	annote = {Cited by: 4; All Open Access; Green Final Open Access; Green Open Access},
}

@inproceedings{zhao_domain-specific_2018,
	address = {New York, NY, USA},
	series = {{NLPIR} '18},
	title = {Domain-{Specific} {Ontology} {Concept} {Extraction} and {Hierarchy} {Extension}},
	isbn = {978-1-4503-6551-2},
	url = {https://doi.org/10.1145/3278293.3278302},
	doi = {10.1145/3278293.3278302},
	abstract = {The domain-specific vernaculars and notations have been a hurdle to automatic ontology building and augmentation, since most of the ontology learning methods are essentially based on the natural language studies and lexicosyntactic pattern explorations. This paper proposes two robust approaches to ontology hierarchical enhancement, in particular, adding new terms to the ontology graph. We designed our learning models from a computational vantage point, examining the inter-relationship between documents, ontology dictionary terms, and the graph structure of the seed ontology. We then take advantage of late studies of neural networks and machine learning to perform classification over the inter-related data, and insert the new term at the most desirable nodal place on the domain ontology graph.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Grace and Zhang, Xiaowen},
	year = {2018},
	note = {event-place: Bangkok, Thailand},
	keywords = {Ontology Engineering, Domain Knowledge Learning, Ontology Hierarchy Extension, Supervised Machine Learning},
	pages = {60--64},
}

@inproceedings{haverinen_automating_2024,
	address = {New York, NY, USA},
	series = {{eSAAM} '24},
	title = {Automating {Cybersecurity} {Compliance} in {DevSecOps} with {Open} {Information} {Model} for {Security} as {Code}},
	isbn = {979-8-4007-0984-5},
	url = {https://doi.org/10.1145/3685651.3686700},
	doi = {10.1145/3685651.3686700},
	abstract = {Software development teams meet increasing requirements to implement cybersecurity management in compliance with standards and regulations. However, adopting a compliant cybersecurity management system and DevSecOps practices as part of a software development process has turned out to be tedious and expensive in practice. Open-source communities and open ecosystems, which lack tools and realistic practices for compliant cybersecurity management, face these difficulties as well. This paper suggests a set of requirements and a solution that are based on long-term experience in adopting standard compliant DevSecOps processes in industry. The proposed solution, called Cyberismo, facilitates the adoption of compliance and cybersecurity management, improves collaboration on cybersecurity in company internal projects, cross-company projects, and open-source projects, and automates the compliance and cybersecurity management in software development by way of an open information model representation format, and an open-source tool to manage the information model. As the information model uses a simple plain text format that can be managed by automated DevSecOps tool chains, it can be understood as an instance of the Everything as Code and Security as Code paradigms. The proposed solution is designed as modular, tailorable to the organisation and its existing tools, and flexible enough to model both process- and technology-related information. It automates both the validation of how compliance requirements have been met and the gathering and archiving of evidence of compliance. The information model is mapped to a logic program conforming to the Answer Set Programming (ASP) paradigm for knowledge representation. The mapping enables flexible query evaluation and reasoning, including the calculation of performance measures and automated policy checks. However, developers, product owners and other end-users of the solution do not necessarily need to know how to write logic programs, as logic programs can be encapsulated in content modules made available for the users. By putting the ease of adoption of compliant DevSecOps processes by the practitioners in the spotlight, this paper concludes that it is both necessary and possible to meet all the proposed requirements.},
	booktitle = {Proceedings of the 4th {Eclipse} {Security}, {AI}, {Architecture} and {Modelling} {Conference} on {Data} {Space}},
	publisher = {Association for Computing Machinery},
	author = {Haverinen, Henry and Janhunen, Tomi and Päivärinta, Tero and Lempinen, Sami and Kaartinen, Suvi and Merilä, Sami},
	year = {2024},
	note = {event-place: Mainz, Germany},
	keywords = {knowledge representation and reasoning, DevOps, answer set programming, compliance, Cybersecurity management, DevSecOps, Everything as Code, open source security, process adoption, Security as Code},
	pages = {93--102},
}

@inproceedings{butting_model-driven_2022,
	address = {New York, NY, USA},
	series = {{GPCE} 2022},
	title = {Model-{Driven} {IoT} {App} {Stores}: {Deploying} {Customizable} {Software} {Products} to {Heterogeneous} {Devices}},
	isbn = {978-1-4503-9920-3},
	url = {https://doi.org/10.1145/3564719.3568689},
	doi = {10.1145/3564719.3568689},
	abstract = {Internet of Things (IoT) devices and the software they execute are often strongly coupled with vendors preinstalling their software at the factory. Future IoT applications are expected to be distributed via app stores. A strong coupling between hard- and software hinders the rise of such app stores. Existing model-driven approaches for developing IoT applications focus largely on the behavior specification and message exchange but generate code that targets a specific set of devices. By raising the level of abstraction, models can be utilized to decouple hard- and software and adapt to various infrastructures. We present a concept for a model-driven app store that decouples hardware and software development of product lines of IoT applications.},
	booktitle = {Proceedings of the 21st {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {Association for Computing Machinery},
	author = {Butting, Arvid and Kirchhof, Jörg Christian and Kleiss, Anno and Michael, Judith and Orlov, Radoslav and Rumpe, Bernhard},
	year = {2022},
	note = {event-place: Auckland, New Zealand},
	keywords = {Internet of Things, Model-Driven Engineering, App Store, Architecture Description Language, Low-Code},
	pages = {108--121},
}

@article{mundotiya_enhancing_2024,
	title = {Enhancing {Generalizability} in {Biomedical} {Entity} {Recognition}: {Self}-{Attention} {PCA}-{CLS} {Model}},
	volume = {21},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2024.3429234},
	doi = {10.1109/TCBB.2024.3429234},
	abstract = {One of the primary tasks in the early stages of data mining involves the identification of entities from biomedical corpora. Traditional approaches relying on robust feature engineering face challenges when learning from available (un-)annotated data using data-driven models like deep learning-based architectures. Despite leveraging large corpora and advanced deep learning models, domain generalization remains an issue. Attention mechanisms are effective in capturing longer sentence dependencies and extracting semantic and syntactic information from limited annotated datasets. To address out-of-vocabulary challenges in biomedical text, the PCA-CLS (Position and Contextual Attention with CNN-LSTM-Softmax) model combines global self-attention and character-level convolutional neural network techniques. The model's performance is evaluated on eight distinct biomedical domain datasets encompassing entities such as genes, drugs, diseases, and species. The PCA-CLS model outperforms several state-of-the-art models, achieving notable F\&lt;inline-formula\&gt;\&lt;tex-math notation="LaTeX"\&gt;₁\&lt;/tex-math\&gt;\&lt;alternatives\&gt;\&lt;mml:math\&gt;\&lt;mml:msub\&gt;\&lt;mml:mrow/\&gt;\&lt;mml:mn\&gt;1\&lt;/mml:mn\&gt;\&lt;/mml:msub\&gt;\&lt;/mml:math\&gt;\&lt;inline-graphic xlink:href="mundotiya-ieq1-3429234.gif"/\&gt;\&lt;/alternatives\&gt;\&lt;/inline-formula\&gt;-scores, including 88.19\% on BC2GM, 85.44\% on JNLPBA, 90.80\% on BC5CDR-chemical, 87.07\% on BC5CDR-disease, 89.18\% on BC4CHEMD, 88.81\% on NCBI, and 91.59\% on the s800 dataset.},
	number = {6},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Mundotiya, Rajesh Kumar and Priya, Juhi and Kuwarbi, Divya and Singh, Teekam},
	month = jul,
	year = {2024},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {1934--1941},
}

@inproceedings{phokela_smart_2024,
	address = {Echternach, Luxembourg},
	series = {{ASE} '23},
	title = {Smart {Prompt} {Advisor}: {Multi}-{Objective} {Prompt} {Framework} for {Consistency} and {Best} {Practices}},
	isbn = {979-8-3503-2996-4},
	url = {https://doi.org/10.1109/ASE56229.2023.00019},
	doi = {10.1109/ASE56229.2023.00019},
	abstract = {Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions, and basic control and data flow are met.},
	booktitle = {Proceedings of the 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi, Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Large language model, Ontology, LLM, Natural language processing, Language model, Semantics, artificial intelligence, Deep learning, ontology, Prompt Engineering, Deep Learning, deep learning, Artificial Intelligence, Prompt engineering, prompt engineering, Visualization, Language processing, Best practices, Natural languages, Codes, Costs, Task analysis, Time to market, Ontology's, Natural language processing systems, Codes (symbols), Economic and social effects, Trade off, Multi objective},
	pages = {1846--1848},
	annote = {Cited by: 5},
}

@inproceedings{mavrokapnidis_programming_2023,
	address = {New York, NY, USA},
	series = {e-{Energy} '23},
	title = {A {Programming} {Model} for {Portable} {Fault} {Detection} and {Diagnosis}},
	isbn = {979-8-4007-0032-3},
	url = {https://doi.org/10.1145/3575813.3595190},
	doi = {10.1145/3575813.3595190},
	abstract = {Portable applications support the write once, deploy everywhere paradigm. This paradigm is particularly attractive in building applications, where current practice involves the manual deployment and configuration of such applications, requiring significant engineering effort and concomitant costs. This is a tedious and error-prone process which does not scale well. Notwithstanding recent advances in semantic data modelling that allow a unified representation of buildings, we still miss a paradigm for deploying portable building applications at scale. This paper introduces a portable programming model for such applications, which we examine in the context of Fault-Detection and Diagnosis (FDD). In particular, we look at the separation of the FDD logic and the configuration with specific data inputs. We architect a software system that enables their self-configuration and execution across various building configurations, expressed in terms of Brick metadata models. Our initial results from authoring and executing APAR (AHU Performance Assessment Rules) on multiple AHUs of two museums demonstrate the potential of our model to reduce repetitive tasks and deployment costs of FDD applications.},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Future} {Energy} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mavrokapnidis, Dimitris and Fierro, Gabe and Korolija, Ivan and Rovas, Dimitrios},
	year = {2023},
	note = {event-place: Orlando, FL, USA},
	keywords = {Ontologies, Semantic Web, RDF, Brick, Portability, SHACL, Metadata, Scalability, Programming, FDD},
	pages = {127--131},
}

@inproceedings{huang_binocular_2023,
	address = {New York, NY, USA},
	series = {{ICMLC} '23},
	title = {Binocular attention-based stacked {BiLSTM} {NER} model for {Supply} chain management event knowledge graph construction},
	isbn = {978-1-4503-9841-1},
	url = {https://doi.org/10.1145/3587716.3587723},
	doi = {10.1145/3587716.3587723},
	abstract = {Extracting fine-grained event ontology knowledge based on supply chain management (SCM) related corpus and constructing knowledge graph (KG) has important guiding significance and knowledge support for the efficient implementation and development of SCM in manufacturing enterprises. Recently, research on the KG of SCM has not gained sufficient attention. This paper aims to propose an event logical KG construction approach for SCM. Specifically, a stacked BiLSTM entity recognition model based on the binocular attention mechanism is proposed, called the SBBAN model. Firstly, the character feature attention mechanism is used to infer the key information that contributes greatly to entity recognition in the text sequence. Character weighted features and character features splicing are used as new character input features. Then the deep semantic abstract features of text sequence are obtained by stacked BiLSTM. In addition, a self-attention mechanism is added to obtain the deep context relevant features. Experimental results show that the model shows better performance in in comparison with the state-of-the-art algorithms to complete the matching of event argument entities and offer knowledge support for SCM.},
	booktitle = {Proceedings of the 2023 15th {International} {Conference} on {Machine} {Learning} and {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Xinyi and Cheng, Lianglun and Deng, Jianfeng and Wang, Tao},
	year = {2023},
	note = {event-place: Zhuhai, China},
	keywords = {named entity recognition, event logic knowledge graph, stacked BiLSTM, supply chain management ontology},
	pages = {40--46},
}

@article{mcdaniel_evaluating_2019,
	title = {Evaluating {Domain} {Ontologies}: {Clarification}, {Classification}, and {Challenges}},
	volume = {52},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3329124},
	doi = {10.1145/3329124},
	abstract = {The number of applications being developed that require access to knowledge about the real world has increased rapidly over the past two decades. Domain ontologies, which formalize the terms being used in a discipline, have become essential for research in areas such as Machine Learning, the Internet of Things, Robotics, and Natural Language Processing, because they enable separate systems to exchange information. The quality of these domain ontologies, however, must be ensured for meaningful communication. Assessing the quality of domain ontologies for their suitability to potential applications remains difficult, even though a variety of frameworks and metrics have been developed for doing so. This article reviews domain ontology assessment efforts to highlight the work that has been carried out and to clarify the important issues that remain. These assessment efforts are classified into five distinct evaluation approaches and the state of the art of each described. Challenges associated with domain ontology assessment are outlined and recommendations are made for future research and applications.},
	number = {4},
	journal = {ACM Comput. Surv.},
	author = {McDaniel, Melinda and Storey, Veda C.},
	month = sep,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology, assessment, domain ontology, evaluation, applied ontology, metrics, ontology application, ontology development, task-ontology fit},
}

@inproceedings{ortiz-rodriguez_mexin_2022,
	address = {New York, NY, USA},
	series = {dg.o '22},
	title = {{MEXIN}: {Multidialectal} {Ontology} supporting {NLP} approach to improve government electronic communication with the {Mexican} {Ethnic} {Groups}},
	isbn = {978-1-4503-9749-0},
	url = {https://doi.org/10.1145/3543434.3543590},
	doi = {10.1145/3543434.3543590},
	abstract = {The government services usually target all citizens, but sometimes physical services nor technology-based services do not cover all people. This research aims to tackle services given to underrepresented citizens in Mexico (Indigenous people) and apply NLP techniques supported by ontologies to achieve accurate translation to most dialects spoken in Mexico. The scope of this paper only tests with Mayan dialect spoken primarily in the Mexican peninsula.},
	booktitle = {Proceedings of the 23rd {Annual} {International} {Conference} on {Digital} {Government} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Ortiz-Rodriguez, Fernando and Tiwari, Sanju and Panchal, Ronak and Medina-Quintero, Jose Melchor and Barrera, Ruben},
	year = {2022},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {Ontologies, Semantic Web, NLP, eGovernment},
	pages = {461--463},
}

@inproceedings{jungmann_fusing_2025-1,
	address = {Orlando, Florida, USA},
	series = {{WSC} '24},
	title = {Fusing {Expert} {Knowledge} and {Data} for {Simulation} {Model} {Discovery} in {Digital} {Twins}: {A} {Case} {Study} from {Reliability} {Modeling}},
	isbn = {979-8-3315-3420-2},
	abstract = {Integrating expert knowledge in data-driven Digital Twins can lead to better-informed underlying models. Achieving systematic integration, however, remains a complex challenge. In this study, we propose an initial approach for hybrid model extraction by systematically fusing expert knowledge statements with Internet of Things data from manufacturing systems, such as event and state logs. We outline two main strategies to facilitate the fusion of data and expert knowledge in a systematic way. We, furthermore, present a case study in reliability assessment of manufacturing systems showcasing our methodology within this specific domain. Using our four fusion algorithms, we automatically extract reliability models from both data and expert knowledge statements. Finally, we conduct a comprehensive analysis of the results and draw conclusions regarding the efficacy of the fusion algorithms for Digital Twin model extractions.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Jungmann, Michelle and Lazarova-Molnar, Sanja},
	year = {2025},
	pages = {2463--2474},
}

@inproceedings{putrevu_framework_2023,
	address = {New York, NY, USA},
	series = {{EICC} '23},
	title = {A {Framework} for {Advanced} {Persistent} {Threat} {Attribution} using {Zachman} {Ontology}},
	isbn = {978-1-4503-9829-9},
	url = {https://doi.org/10.1145/3590777.3590783},
	doi = {10.1145/3590777.3590783},
	abstract = {Advanced Persistent Threat (APT) is a type of cyber attack that infiltrates a targeted organization and exfiltrates sensitive data over an extended period of time or to cause sabotage. Recently, there has been a trend of nation states backing APT groups in order to further their political and financial interests, making the APT attribution process increasingly important. The APT attribution process involves identifying the actors behind an attack and their motivations, using a method of logical inference called abductive reasoning to determine the most likely explanation for a set of observations. While various attribution methods and frameworks have been proposed by the security community, many of them lack granularity and are dependent on the skills of practitioners rather than a standardized process. This can hinder both the understandability and reproducibility of attribution efforts as this process is practiced but not engineered. To address these issues, we propose a new framework for the APT attribution process based on the Zachman ontology, which offers greater granularity by posing specific primitive questions at various levels of the attribution process. This allows for more accurate conclusions about the attackers and their motivations, helping organizations to better protect themselves against future attacks.},
	booktitle = {Proceedings of the 2023 {European} {Interdisciplinary} {Cybersecurity} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Putrevu, Venkata Sai Charan and Chunduri, Hrushikesh and Putrevu, Mohan Anand and Shukla, Sandeep},
	year = {2023},
	note = {event-place: Stavanger, Norway},
	keywords = {APT, Attribution Framework, Cyber Criminology, Cyber Investigation., Zachman Ontology},
	pages = {34--41},
}

@inproceedings{tang_fuzzy_2025,
	address = {New York, NY, USA},
	series = {{AIBDF} '24},
	title = {A fuzzy expert system based recommendation model for nuclear wastewater research collaborators},
	isbn = {979-8-4007-1086-5},
	url = {https://doi.org/10.1145/3718491.3718494},
	doi = {10.1145/3718491.3718494},
	abstract = {In order to help scholars in the field of nuclear wastewater explore potential research collaborators more efficiently, this paper proposes a fuzzy expert system-based research collaborator recommendation model (FCR-NWR) to address the problems of existing collaboration recommendation models in terms of textual representation, incomplete extraction of scholars' features, and the lack of diversified information and reasoning decision-making methods. The model fully obtains text features through multilevel text analysis, combines with new methods to calculate scholars' comprehensive academic ability and scientific research cooperation intensity, and finally utilizes the expert system to simulate the knowledge and decision-making process of human experts for collaborator recommendation. Through this model, scholars in the field of nuclear wastewater can integrate into the academic community faster, find potential collaborators, and promote academic co-development. The experimental results show that FCR-NWR outperforms traditional methods in terms of precision, recall and F1 value.},
	booktitle = {Proceedings of the 4th {Asia}-{Pacific} {Artificial} {Intelligence} and {Big} {Data} {Forum}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Yanfei and Wang, Hui and Luo, Huilan and Li, Qin},
	year = {2025},
	keywords = {expert system, collaboration recommendation, decision simulation, fuzzy set, hybrid modeling},
	pages = {13--16},
}

@inproceedings{leisau_breaking_2025,
	address = {Orlando, Florida, USA},
	series = {{WSC} '24},
	title = {Breaking {Barriers} in {Semiconductor} {Simulations}: {An} {Automated} {Low}-{Code} {Framework} for {Model}-{Structure} {Synchronisation} and {Large}-{Scale} {Simulation} {Studies}},
	isbn = {979-8-3315-3420-2},
	abstract = {The paradigm shift towards Industry 4.0 and the emerging trends of Industry 5.0 present ongoing challenges in production planning and control. In response to these dynamics, discrete event-driven simulation methods are gaining prominence as an operational decision-support-tool, particularly in the semiconductor industry. This paper introduces an automated low-code framework designed to synchronize model structures across simulation tool boundaries for extensive simulation studies, using the Semiconductor Manufacturing Testbed 2020 as a test reference, and aims to serve as a helpful tool for simulation experiments. Key aspects include model structure synchronization, Design of Experiments, and the distributed execution of large-scale simulation studies.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Leißau, Madlene and Laroque, Christoph},
	year = {2025},
	pages = {1919--1930},
}

@article{abdulhameed_wasf-vec_2019,
	title = {Wasf-{Vec}: {Topology}-based {Word} {Embedding} for {Modern} {Standard} {Arabic} and {Iraqi} {Dialect} {Ontology}},
	volume = {19},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3345517},
	doi = {10.1145/3345517},
	abstract = {Word clustering is a serious challenge in low-resource languages. Since words that share semantics are expected to be clustered together, it is common to use a feature vector representation generated from a distributional theory-based word embedding method. The goal of this work is to utilize Modern Standard Arabic (MSA) for better clustering performance of the low-resource Iraqi vocabulary. We began with a new Dialect Fast Stemming Algorithm (DFSA) that utilizes the MSA data. The proposed algorithm achieved 0.85 accuracy measured by the F1 score. Then, the distributional theory-based word embedding method and a new simple, yet effective, feature vector named Wasf-Vec word embedding are tested. Wasf-Vec word representation utilizes a word’s topology features. The difference between Wasf-Vec and distributional theory-based word embedding is that Wasf-Vec captures relations that are not contextually based. The embedding is followed by an analysis of how the dialect words are clustered within other MSA words. The analysis is based on the word semantic relations that are well supported by solid linguistic theories to shed light on the strong and weak word relation representations identified by each embedding method. The analysis is handled by visualizing the feature vector in two-dimensional (2D) space. The feature vectors of the distributional theory-based word embedding method are plotted in 2D space using the t-sne algorithm, while the Wasf-Vec feature vectors are plotted directly in 2D space. A word’s nearest neighbors and the distance-histograms of the plotted words are examined. For validation purpose of the word classification used in this article, the produced classes are employed in Class-based Language Modeling (CBLM). Wasf-Vec CBLM achieved a 7\% lower perplexity (pp) than the distributional theory-based word embedding method CBLM. This result is significant when working with low-resource languages.},
	number = {2},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Abdulhameed, Tiba Zaki and Zitouni, Imed and Abdel-Qader, Ikhlas},
	month = dec,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology, Semantics, Modeling languages, morphology, Embeddings, Word embedding, Computational linguistics, Morphology, Arabic language, Topology, Class-based language model, 2D visualizing, class-based language modeling, dialect, orthographic, phonology, words classification, words features, words ontology, Vectors, Natural language processing systems, Classification (of information), Vector spaces, Arabic languages, Dialect, Orthographic, Phonology, Words features},
	annote = {Cited by: 5},
}

@inproceedings{zhang_research_2025,
	address = {New York, NY, USA},
	series = {{ICAIE} '24},
	title = {Research on {Intelligent} {Management} and {Assisted} {Decision} {Making} {Technology} of {Electric} {Power} {Research} {Enterprise} {Based} on {Knowledge} {Engineering}},
	isbn = {979-8-4007-1269-2},
	url = {https://doi.org/10.1145/3722237.3722341},
	doi = {10.1145/3722237.3722341},
	abstract = {Currently, the internal management knowledge of electric power research enterprises is difficult to integrate and structured organization, resulting in greatly reduced decision-making efficiency. Knowledge Graph(KG), as an efficient knowledge organization, helps to connect the complex knowledge network. However, for a Q\&amp;A platform based solely on knowledge graphs, it can be challenging to interpret and analyze natural language input, especially when considering historical Q\&amp;A data. In this paper, we combine the KG with a Large Language Model(LLM), choose the ERNIE model fine-tuned by the text of electric power domain knowledge, and use the TransE method to integrate the KG into the model's structure to build a knowledge-based Q\&amp;A system that assists researchers in decision-making. The experimental results show that the ERNIE model combined with KG outperforms the comparison model in terms of precision, recall, and F1 value in entity and relation classification tasks, and the performance is further improved by increasing the number of attention heads.},
	booktitle = {Proceedings of the 2024 3rd {International} {Conference} on {Artificial} {Intelligence} and {Education}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Baoliang and Xu, Yinghui and Li, Chengyuan and Jiang, Yuan and Cui, Wenchao and Yue, Hongxu and Hui, Chen and Hu, Ziwei},
	year = {2025},
	keywords = {Large Language Model, Knowledge Graph embedding, Knowledge Quizzing},
	pages = {590--600},
}

@inproceedings{wang_autodive_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{AutoDive}+: {An} {Adaptive} {Model} {Enhanced} {Multimodal} {Online} {Annotation} {Tool}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715167},
	doi = {10.1145/3701716.3715167},
	abstract = {The demand for data by machine learning models, especially pre-trained large-scale models, has surged dramatically in recent times, resulting in an urgent need for efficient data annotation. Despite the prevalence of annotation tools, they grapple with challenges such as high data conversion costs, limited scalability, and inefficiencies in annotating multimodal data. To tackle these challenges, this paper introduces AutoDive+, an advanced data annotation tool empowered by active learning mechanisms and integrated automatic extraction models. Expanding upon the capabilities of its predecessor, AutoDive, which excels in direct text annotation within PDF documents, AutoDive+ undergoes extensive architectural enhancements. It not only exploits high-value resource ranking modules to boost annotation efficiency across entity and content annotation but also boasts scalable interfaces for seamless integration of new annotation modes and modalities. Furthermore, our novel model community concept facilitates adaptable integration of appropriate automatic extraction and annotation models. We substantiate the effectiveness of our proposed modules through user studies spanning diverse domains. Our vision entails the establishment of an annotation task-driven open community, bolstered by the support of AutoDive+. A live demo of Autodive+ is available at http://autodive.sciwiki.cn/, and a video demo http://autodive.sciwiki.cn/introVideo/introduce-v2.0.mp4. The source code is available at https://github.com/Autodive.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Wang, Ludi and Song, Dongze and Cui, Qiang and Chen, Xueqing and Zhou, Yuanchun and Cui, Wenjuan and Du, Yi},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {human in the loop, intelligent extraction, multimodal annotation tool},
	pages = {2919--2922},
}

@inproceedings{lutz_efficiently_2022,
	address = {New York, NY, USA},
	series = {{PODS} '22},
	title = {Efficiently {Enumerating} {Answers} to {Ontology}-{Mediated} {Queries}},
	isbn = {978-1-4503-9260-0},
	url = {https://doi.org/10.1145/3517804.3524166},
	doi = {10.1145/3517804.3524166},
	abstract = {We study the enumeration of answers to ontology-mediated queries (OMQs) where the ontology is a set of guarded TGDs or formulated in the description logic ELI and the query is a conjunctive query (CQ). In addition to the traditional notion of an answer, we propose and study two novel notions of partial answers that can take into account nulls generated by existential quantifiers in the ontology. Our main result is that enumeration of the traditional complete answers and of both kinds of partial answers is possible with linear-time preprocessing and constant delay for OMQs that are both acyclic and free-connex acyclic. We also provide partially matching lower bounds. Similar results are obtained for the related problems of testing a single answer in linear time and of testing multiple answers in constant time after linear time preprocessing. In both cases, the border between tractability and intractability is characterized by similar, but slightly different acyclicity properties.},
	booktitle = {Proceedings of the 41st {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lutz, Carsten and Przybylko, Marcin},
	year = {2022},
	note = {event-place: Philadelphia, PA, USA},
	keywords = {description logic, constant delay, enumeration, ontology-mediated queries, partial answers, tuple generating dependencies},
	pages = {277--289},
}

@inproceedings{deatcu_design_2018,
	address = {San Diego, CA, USA},
	series = {{Mod4Sim} '18},
	title = {Design patterns for variability modeling using {SES} ontology},
	isbn = {978-1-5108-6018-6},
	abstract = {The System Entity Structure (SES) is a high level approach for variability modeling, particularly in simulation engineering, which is under continuous development. In this context, an enhanced framework is introduced that supports dynamic variability evolution using the SES approach. However, the main focus is to start a discussion about a set of design patterns, which were developed to analyze the tree design and computing aspects of System Entity Structures. As development of our MATLAB-based SES toolbox for construction and pruning of SES trees proceeded, the necessity to have some generalized examples for testing and verification came more and more into awareness. We propose a set of design patterns that, if completely representable and computable by a certain tool, support all aspects of SES theory. In addition, the patterns give users substantial support for developing SES models for other applications.},
	booktitle = {Proceedings of the {Model}-{Driven} {Approaches} for {Simulation} {Engineering} {Symposium}},
	publisher = {Society for Computer Simulation International},
	author = {Deatcu, Christina and Folkerts, Hendrik and Pawletta, Thorsten and Durak, Umut},
	year = {2018},
	note = {event-place: Baltimore, Maryland},
	keywords = {MATLAB/simulink, model generation, SES, simulation engineering, system entity structure, variability modeling, versatile systems},
}

@inproceedings{winkels_component-based_2025,
	address = {Orlando, Florida, USA},
	series = {{WSC} '24},
	title = {Component-{Based} {Synthesis} of {Structural} {Variants} of {Simulation} {Models} for {Changeable} {Material} {Flow} {Systems}},
	isbn = {979-8-3315-3420-2},
	abstract = {Despite relevant research endeavors, modeling efforts related to the building of discrete-event simulation models for planning changeable material flow systems still limit their practical application. This is because simulation experts have to model many possible structural variants and compare them based on key performance indicators such as throughput, workload or investment costs, while also ensuring sufficient system changeability. This article presents a methodology for reducing efforts for structural variation during the experimental phase of a simulation study. Starting from a valid initial simulation model, structural variants of this simulation model are automatically generated by applying component-based software synthesis which uses combinatorial logic; thereby, a range of simulation models is provided for the user. This paper presents the outlined methodology using a case study and places it in the research context of reducing efforts associated with the design and execution of simulation experiments.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Winkels, Jan and Özkul, Felix and Sutherland, Robin and Löhn, Jannik and Wenzel, Sigrid and Rehof, Jakob},
	year = {2025},
	pages = {1657--1668},
}

@inproceedings{gala_towards_2021,
	address = {New York, NY, USA},
	series = {{SAC} '21},
	title = {Towards an ontology for urban tourism},
	isbn = {978-1-4503-8104-8},
	url = {https://doi.org/10.1145/3412841.3442142},
	doi = {10.1145/3412841.3442142},
	abstract = {Nowadays, diffusion and preservation of cultural heritage are being supported by technology on the Web. Thus, the online availability of urban tourism information, as part of cultural heritage, has been of enormous relevance to activate the tourism in many countries. The necessity of a well-defined and standard model for representing this knowledge is being managed by semantic web technologies, such as ontologies. However, current proposals represent partial knowledge of cultural heritage. In this context, this work proposes an ontology for indoor and outdoor environments of a city to represent the cultural heritage knowledge based on the UNESCO definitions. This ontology has a three-level architecture (Upper, Middle, and Lower ontologies) in accordance with a purpose of modularity and levels of specificity. To demonstrate the utility and suitability of our proposal, we have developed a parser to map and convert a museum repository (in CSV format) to RDF triples. With this case of study, we demonstrated that, by using our ontology, it is possible to represent the knowledge of urban tourism domains of a city.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Gala, Alexander Pinto-De la and Cardinale, Yudith and Dongo, Irvin and Ticona-Herrera, Regina},
	year = {2021},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {ontology, automatic population, urban tourism},
	pages = {1887--1890},
}

@inproceedings{gautam_leveraging_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23},
	title = {Leveraging {Existing} {Literature} on the {Web} and {Deep} {Neural} {Models} to {Build} a {Knowledge} {Graph} {Focused} on {Water} {Quality} and {Health} {Risks}},
	isbn = {978-1-4503-9416-1},
	url = {https://doi.org/10.1145/3543507.3584185},
	doi = {10.1145/3543507.3584185},
	abstract = {A knowledge graph focusing on water quality in relation to health risks posed by water activities (such as diving or swimming) is not currently available. To address this limitation, we first use existing resources to construct a knowledge graph relevant to water quality and health risks using KNowledge Acquisition and Representation Methodology (KNARM). Subsequently, we explore knowledge graph completion approaches for maintaining and updating the graph. Specifically, we manually identify a set of domain-specific UMLS concepts and use them to extract a graph of approximately 75,000 semantic triples from the Semantic MEDLINE database (which contains head-relation-tail triples extracted from PubMed). Using the resulting knowledge graph, we experiment with the KG-BERT approach for graph completion by employing pre-trained BERT/RoBERTa models and also models fine-tuned on a collection of water quality and health risks abstracts retrieved from the Web of Science. Experimental results show that KG-BERT with BERT/RoBERTa models fine-tuned on a domain-specific corpus improves the performance of KG-BERT with pre-trained models. Furthermore, KG-BERT gives better results than several translational distance or semantic matching baseline models.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Gautam, Nikita and Shumway, David and Kowalcyk, Megan and Khanal, Sarthak and Caragea, Doina and Caragea, Cornelia and Mcginty, Hande and Dorevitch, Samuel},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {Knowledge graph, BERT, knowledge graph completion, diving, health risks, water quality, water recreation},
	pages = {4161--4171},
}

@inproceedings{tong_detection_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Detection of anomalous modeling behavior: a goal-driven data mining approach},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3556509},
	doi = {10.1145/3550356.3556509},
	abstract = {Precise and timely detection of anomalous modeling behaviors is critical for both teaching and application of modeling methods. Existing methods usually focus on evaluating the modeling results rather than mining the knowledge hidden in the modeling process. In this paper, we propose to monitor and analyze the modeling process in order to timely detect anomalous modeling behaviors, potentially contributing to a comprehensive assessment of the modeling practice. Specifically, we propose to systematically build a goal model for characterizing the normal modeling behaviors, which establishes the connections between modelers' high-level modeling behaviors and low-level modeling operations. On top of such a goal model, we propose a data mining-based approach to semi-automatically validate the design of the goal model and explore other normal modeling behaviors. Then, we propose to automatically detect anomalous modeling behaviors by capturing normal modeling behaviors obtained from the goal model and actual modeling sequences. We have developed and deployed a data-flow diagram modeling platform, which implemented our proposed approach. We have conducted an experiment with 57 participants, the preliminary results of which show that our approach can effectively detect modelers' anomalous behaviors. The experiment results are beneficial for not only assessing the modelers' performance but also identifying the usability issues of the modeling tool.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Tong, Li and Yiting, Wang and Congkai, Geng},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {data mining, goal modeling, anomalous modeling behavior detection},
	pages = {142--145},
}

@article{mehmood_enml_2023,
	title = {{EnML}: {Multi}-label {Ensemble} {Learning} for {Urdu} {Text} {Classification}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3616111},
	doi = {10.1145/3616111},
	abstract = {Exponential growth of electronic data requires advanced multi-label classification approaches for the development of natural language processing (NLP) applications such as recommendation systems, drug reaction detection, hate speech detection, and opinion recognition/mining. To date, several machine and deep learning–based multi-label classification methodologies have been proposed for English, French, German, Chinese, Arabic, and other developed languages. Urdu is the 11th largest language in the world and has no computer-aided multi-label textual news classification approach. Unlike other languages, Urdu is lacking multi-label text classification datasets that can be used to benchmark the performance of existing machine and deep learning methodologies. With an aim to accelerate and expedite research for the development of Urdu multi-label text classification–based applications, this article provides multiple contributions as follows: First, it provides a manually annotated multi-label textual news classification dataset for the Urdu language. Second, it benchmarks the performance of traditional machine learning approaches particularly by adapting three data transformation approaches along with three top-performing machine learning classifiers and four algorithm adaptation-based approaches. Third, it benchmarks performance of 16 existing deep learning approaches and the four most widely used language models. Finally, it provides an ensemble approach that reaps the benefits of three different deep learning architectures to precisely predict different classes associated with a particular Urdu textual document. Experimental results reveal that proposed ensemble approach performance values (87\% accuracy, 92\% F1-score, and 8\% hamming loss) are significantly higher than adapted machine and deep learning–based approaches.},
	number = {9},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Mehmood, Faiza and Shahzadi, Rehab and Ghafoor, Hina and Asim, Muhammad Nabeel and Ghani, Muhammad Usman and Mahmood, Waqar and Dengel, Andreas},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deep learning, language models, data transformation methods, multi-label ensemble learning, multi-label Urdu news dataset, Multi-label Urdu text classification, traditional machine learning},
}

@inproceedings{fassbinder_towards_2021,
	address = {USA},
	series = {{PLoP} '19},
	title = {Towards an educational design pattern language to support the development of open educational resources in videos for the {MOOC} context},
	abstract = {The creation and adoption of Massive Open Online Courses (MOOCs) can bring many benefits and impact on education, such as put forward diversity in education; enhance student's learning by encouraging and engaging them for lifelong learning; connect with more individuals in informal contexts creating opportunities to transition to formal higher education or lifelong learning activities; force a re-conceptualization of higher education through the use of online study; enhance teachers' skills from developing Open Educational Resources (OERs) and adopting learner-centered pedagogical approaches and active learning strategies; among others. In such perspective, several studies have investigated the potential benefits about the use of videos as a support for the process of teaching in virtual learning environments and, particularly, in the context of MOOCs. However, video production for MOOCs still presents several challenges that need to be better investigated. It is because, in general, educators and MOOC teams (e.g. educators, learning designers, and educational technologists) are still using ad hoc decision-making procedures based on empirical knowledge obtained from their experiences with face-to-face courses or even traditional virtual courses. There are also gaps about what adaptations need to be performed to video formats and what are the attributes and steps needed to support the production and validation of videos for the MOOC context. In addition, since MOOCs are part of the Open Educational Movement, as well as the OERs, it is also important to reflect on the construction of OERs in the form of videos for the MOOC context. There is, therefore, a need for research that investigates the current theoretical panorama involving video construction for MOOCs, in order to propose strategies empirically validated and useful to support and guide MOOC teams during the development of videos more theoretically informed. Considering such context, the main objective of this paper is to present a set of patterns and move towards the development of an educational design pattern language able to support MOOC teams in the construction of OERs in the form of videos. The paper presents a life cycle for OERs production in the form of videos. From such cycle activities, we extracted nine patterns that are presented in the form of patlets (problem-solution pair). The patterns were also collected and refined from a literature review on guidelines about the production of MOOCs, OERs, and educational videos. The patterns are divided into five categories related to the life cycle: analysis, planning development, evaluation, and distribution. These patterns are expected to offer a compelling alternative to guide MOOC teams in designing OERs in the form of videos to enhance learning experiences, increase student engagement in the course, and emphasize self-directed learning, which are requirements for quality in MOOCs.},
	booktitle = {Proceedings of the 26th {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {The Hillside Group},
	author = {Fassbinder, Marcelo and Fassbinder, Aracele and Fioravanti, Maria Lydia and Barbosa, Ellen Francine},
	year = {2021},
	note = {event-place: Urbana, Illinois},
	keywords = {educational design patterns, educational videos, learning design, MOOCs, patterns},
}

@inproceedings{li_generative_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {Generative {Meta}-{Learning} for {Zero}-{Shot} {Relation} {Triplet} {Extraction}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3729988},
	doi = {10.1145/3726302.3729988},
	abstract = {Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation triplets from texts containing unseen relation types. This capability benefits various downstream information retrieval (IR) tasks. The primary challenge lies in enabling models to generalize effectively to unseen relation categories. Existing approaches typically leverage the knowledge embedded in pre-trained language models to accomplish the generalization process. However, these methods focus solely on fitting the training data during training, without specifically improving the model's generalization performance, resulting in limited generalization capability. For this reason, we explore the integration of bi-level optimization (BLO) with pre-trained language models for learning generalized knowledge directly from the training data, and propose a generative meta-learning framework which exploits the 'learning-to-learn' ability of meta-learning to boost the generalization capability of generative models.Specifically, we introduce a BLO approach that simultaneously addresses data fitting and generalization. This is achieved by constructing an upper-level loss to focus on generalization and a lower-level loss to ensure accurate data fitting. Building on this, we subsequently develop three generative meta-learning methods, each tailored to a distinct category of meta-learning. Extensive experimental results demonstrate that our framework performs well on the ZeroRTE task. Our code is available at https://github.com/leeworry/TGM-MetaLearning.},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Li, Wanli and Qian, Tieyun and Song, Yi and Zhang, Zeyu and Li, Jiawei and Chen, Zhuang and Zou, Lixin},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {pre-trained language model, meta-learning, zero-shot learning, relation triplet extraction},
	pages = {1371--1381},
}

@inproceedings{duan_knowledge-based_2025,
	address = {New York, NY, USA},
	series = {{AIIIP} '24},
	title = {Knowledge-based {Retrieval} {Methods} for {Enhancing} {Aerospace} {Model} {Software} {Documentation}},
	isbn = {979-8-4007-0730-8},
	url = {https://doi.org/10.1145/3707292.3707392},
	doi = {10.1145/3707292.3707392},
	abstract = {Addressing the critical challenges of poor structure, weak relevance, and limited reusability in aerospace model software development documents, we propose a knowledge-based retrieval method. This method constructs a knowledge base by analyzing key document features, including storage methods, file formats, content structure, inter-document associations, and naming conventions. The system implements structured document management and associates documents based on software configuration information. By partitioning the retrieval domain, constructing semantic vectors, and applying multi-dimensional weight configurations, the method enables efficient retrieval in domains characterized by dense information and frequent term repetition. An aerospace model software document retrieval system has been developed and preliminarily implemented in an aerospace enterprise, demonstrating its effectiveness in improving retrieval efficiency and enhancing the reuse of development knowledge.},
	booktitle = {Proceedings of the 2024 3rd {International} {Conference} on {Artificial} {Intelligence} and {Intelligent} {Information} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Duan, Jinjian and Wei, Deming and Wang, Meiqing and Chen, Gaohui and Zhang, Yang and Gao, Yanhua},
	year = {2025},
	keywords = {Intelligent Retrieval 2, Knowledge Base 4, Knowledge Reuse 3, Model Software Document 1},
	pages = {372--378},
}

@inproceedings{katyshev_using_2023,
	address = {New York, NY, USA},
	series = {{SIGCSE} 2023},
	title = {Using {Transformer} {Models} for {Knowledge} {Graph} {Construction} in {Computer} {Science} {Education}},
	isbn = {978-1-4503-9433-8},
	url = {https://doi.org/10.1145/3545947.3576365},
	doi = {10.1145/3545947.3576365},
	abstract = {The volume of information that can be used in the development of knowledge bases that can be used in education is constantly increasing. Also, this amount of data is very difficult to process and store. When designing a knowledge base to optimize the educational process, it is important to use ontologies. At the moment, the creation of an ontological knowledge model is the most promising option for storing and processing information. The article describes effective approaches for generating an ontological model using machine learning models based on the Transformer model.},
	booktitle = {Proceedings of the 54th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education} {V}. 2},
	publisher = {Association for Computing Machinery},
	author = {Katyshev, Alexander and Anikin, Anton and Sychev, Oleg},
	year = {2023},
	note = {event-place: Toronto ON, Canada},
	keywords = {machine learning, ontologies, semantics, transformers, neural networks, concepts, ontological graph, relations between concepts},
	pages = {1421},
}

@article{troncoso_ontology-based_2022,
	title = {Ontology-{Based} {Approach} to {Creating} {Semantic} {Wikis}},
	volume = {15},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3479012},
	doi = {10.1145/3479012},
	abstract = {Maintaining a semantic wiki is challenging. Coping with increasingly complex wikis led to the development of a methodical approach for simplifying the creation and maintenance of semantic wikis. The methodical approach used involves modeling the semantic relationships underlying the information in the wiki as an ontology, and then programmatically creating the wiki from the ontology constructs. A methodical approach for creating and maintaining wikis greatly simplifies the creation and maintenance of semantic wikis. Furthermore, reusing vocabularies and taxonomies throughout several projects becomes manageable. The approach was implemented as open source software that maps ontology constructs to the wiki artifacts, and practical applications are discussed.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Troncoso, Alvaro R. Ortiz},
	month = apr,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Semantic wikis, collaborative research environment, domain-driven software development},
}

@article{storey_conceptual_2023,
	title = {Conceptual {Modeling}: {Topics}, {Themes}, and {Technology} {Trends}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3589338},
	doi = {10.1145/3589338},
	abstract = {Conceptual modeling is an important part of information systems development and use that involves identifying and representing relevant aspects of reality. Although the past decades have experienced continuous digitalization of services and products that impact business and society, conceptual modeling efforts are still required to support new technologies as they emerge. This paper surveys research on conceptual modeling over the past five decades and shows how its topics and trends continue to evolve to accommodate emerging technologies, while remaining grounded in basic constructs. We survey over 5,300 papers that address conceptual modeling topics from the 1970s to the present, which are collected from 35 multidisciplinary journals and conferences, and use them as the basis from which to analyze the progression of conceptual modeling. The important role that conceptual modeling should play in our evolving digital world is discussed, and future research directions proposed.},
	number = {14s},
	journal = {ACM Comput. Surv.},
	author = {Storey, Veda C. and Lukyanenko, Roman and Castellanos, Arturo},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Conceptual modeling, information technology, database, information systems, clustering analysis, digital world, structured literature review},
}

@article{zorrilla_multilingual_2022,
	title = {A {Multilingual} {Neural} {Coaching} {Model} with {Enhanced} {Long}-term {Dialogue} {Structure}},
	volume = {12},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/3487066},
	doi = {10.1145/3487066},
	abstract = {In this work we develop a fully data driven conversational agent capable of carrying out motivational coaching sessions in Spanish, French, Norwegian, and English. Unlike the majority of coaching, and in general well-being related conversational agents that can be found in the literature, ours is not designed by hand-crafted rules. Instead, we directly model the coaching strategy of professionals with end users. To this end, we gather a set of virtual coaching sessions through a Wizard of Oz platform, and apply state of the art Natural Language Processing techniques. We employ a transfer learning approach, pretraining GPT2 neural language models and fine-tuning them on our corpus. However, since these only take as input a local dialogue history, a simple fine-tuning procedure is not capable of modeling the long-term dialogue strategies that appear in coaching sessions. To alleviate this issue, we first propose to learn dialogue phase and scenario embeddings in the fine-tuning stage. These indicate to the model at which part of the dialogue it is and which kind of coaching session it is carrying out. Second, we develop a global deep learning system which controls the long-term structure of the dialogue. We also show that this global module can be used to visualize and interpret the decisions taken by the the conversational agent, and that the learnt representations are comparable to dialogue acts. Automatic and human evaluation show that our proposals serve to improve the baseline models. Finally, interaction experiments with coaching experts indicate that the system is usable and gives rise to positive emotions in Spanish, French and English, while the results in Norwegian point out that there is still work to be done in fully data driven approaches with very low resource languages.},
	number = {2},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Zorrilla, Asier López and Torres, M. Inés},
	month = jul,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Dialogue system, explainable artificial intelligence, coaching, multilingual, transfer learning},
}

@inproceedings{atchison_topic_2018,
	address = {New York, NY, USA},
	series = {{ICSE} '18},
	title = {A topic analysis of the {R} programming language},
	isbn = {978-1-4503-5663-3},
	url = {https://doi.org/10.1145/3183440.3195087},
	doi = {10.1145/3183440.3195087},
	abstract = {We leverage Latent Dirichlet Allocation to analyze R source code from 10,051 R packages in order to better understand the topic space of scientific computing. Our method is able to identify several generic programming concepts and, more importantly, identify concepts that are highly specific to scientific and high performance computing applications.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceeedings}},
	publisher = {Association for Computing Machinery},
	author = {Atchison, Abigail and Anderson, Haley and Berardi, Christina and Best, Natalie and Firmani, Cristiano and German, Rene and Linstead, Erik},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {machine learning, topic modeling, R},
	pages = {183--184},
}

@inproceedings{helfer_tellus-onto_2021,
	address = {New York, NY, USA},
	series = {{SBSI} '21},
	title = {Tellus-{Onto}: uma ontologia para classificação e inferência de solos na agricultura de precisão: {Tellus}-{Onto}: an ontology for soil classification and inference in precision agriculture},
	isbn = {978-1-4503-8491-9},
	url = {https://doi.org/10.1145/3466933.3466946},
	doi = {10.1145/3466933.3466946},
	abstract = {Soil analysis laboratories demand large volumes of data used in precision agriculture. Among them, parameters that represent soil fertility such as texture and organic matter guide the fertilization process. However, this process can take time, thus limiting its usefulness. Therefore, this article proposes an ontology called Tellus-Onto that extends the state of the art in the classification of Brazilian soils according to the organic and textural composition. A series of axioms and semantic rules provided consultations and inferences about their instantiated basis. In order to test the ontology, we added 98 soil sample results and their classifications were inferred precisely and automatically. Laboratórios de análises de solos demandam volumes grandes de dados empregados na agricultura de precisão. Dentre eles, parâmetros que representam fertilidade de solos como textura e matéria orgânica orientam o processo de adubação. No entanto, este processo pode se tornar demorado, limitando assim sua utilidade. Sendo assim, este artigo propõe uma ontologia denominada Tellus-Onto que estende o estado da arte na classificação de solos brasileiros de acordo com a composição orgânica e textural. Uma série de axiomas e regras semânticas foram empregadas para proporcionar a realização de consultas e inferências sobre sua base instanciada. Para testar a ontologia foram instanciados 98 resultados de amostras de solos e inferidos suas classificações de modo preciso e automático.},
	booktitle = {Proceedings of the {XVII} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Helfer, Gilson Augusto and Costa, Adilson Ben da and Bavaresco, Rodrigo Simon and Barbosa, Jorge Luis Victória},
	year = {2021},
	note = {event-place: Uberlândia, Brazil},
	keywords = {ontology, agricultura de precisão, classificação de solos, precision agriculture ontologia, soil classification},
}

@inproceedings{rocha_syntactic_2021,
	address = {New York, NY, USA},
	series = {{iiWAS} '20},
	title = {A {Syntactic} and {Semantic} {Assessment} of a {Global} {Software} {Engineering} {Domain} {Ontology}},
	isbn = {978-1-4503-8922-8},
	url = {https://doi.org/10.1145/3428757.3429143},
	doi = {10.1145/3428757.3429143},
	abstract = {Globalization has allowed organizations to intensify the search for solutions that minimize challenges, reduce costs and optimize processes. In this way, global software development has emerged as an attempt to use the best resources for its limitations.In distributed environments, the use of Ontologies brings some benefits such as a uniform understanding of information among teams and ease of communication, as well as making for the lack of a reference model that can be applied in a distributed context.This work aims to propose a viable form of validation for DKDonto a domain ontology developed for Global Software Engineering. The validation allowed a broader and more targeted assessment, different from its original validation, which was carried out in a controlled environment, limited to answering questions already known by the knowledge base itself.The main result of this work is a satisfactory evaluation of the ontology, enabling it to be used and shared by companies or institutions, as well as the presentation of a set of methods and ways to evaluate and verify domain ontologies to be used in different domains.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Rocha, Rodrigo and Bion, Danillo and Azevedo, Ryan and Gomes, Arthur and Cordeiro, Diogo and Leandro, Renan and Silva, Israel and Freitas, Fred},
	year = {2021},
	note = {event-place: Chiang Mai, Thailand},
	keywords = {Ontology, Evaluation, Global Software Development},
	pages = {253--262},
}

@inproceedings{zaidi_learning_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {Learning {Custom} {Experience} {Ontologies} via {Embedding}-based {Feedback} {Loops}},
	isbn = {979-8-4007-0132-0},
	url = {https://doi.org/10.1145/3586183.3606715},
	doi = {10.1145/3586183.3606715},
	abstract = {Organizations increasingly rely on behavioral analytics tools like Google Analytics to monitor their digital experiences. Making sense of the data these tools capture, however, requires manual event tagging and filtering — often a tedious process. Prior approaches have trained machine learning models to automatically tag interaction data, but draw from fixed digital experience vocabularies which cannot be easily augmented or customized. This paper introduces a novel machine learning interaction pattern that generates customized tag predictions for organizations. The approach employs a general user experience word embedding to bootstrap an initial set of predictions, which can then be refined and customized by users to adapt the underlying vector space, iteratively improving the quality of future predictions. The paper presents a needfinding study that grounds the design choices of the system, and describes a real-world deployment as part of UserTesting.com that demonstrates the efficacy of the approach.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Zaidi, Ali and Turbeville, Kelsey and Ivančić, Kristijan and Moss, Jason and Gutierrez Villalobos, Jenny and Sagar, Aravind and Li, Huiying and Mehra, Charu and Li, Sixuan and Hutchins, Scott and Kumar, Ranjitha},
	year = {2023},
	note = {event-place: San Francisco, CA, USA},
	keywords = {clickstream analytics, Sankey diagrams, sequence alignment, usability testing, UX research},
}

@inproceedings{wang_gpt_2023,
	address = {New York, NY, USA},
	series = {{SIGSPATIAL} '23},
	title = {{GPT} {Applications} in {Relevance} {Model} {Training} in {Map} {Search}},
	isbn = {979-8-4007-0168-9},
	url = {https://doi.org/10.1145/3589132.3625618},
	doi = {10.1145/3589132.3625618},
	abstract = {Understanding map queries and retrieving correct entity results are the two main relevance tasks in Map search. They are usually performed by a set of task specific machine learning models. Collecting large amount of high quality labelled data for training such models is a time-consuming and labor-intensive process. Although various methods have been studied for producing pseudo data labels, they are limited in their effectiveness when applied across different languages or tasks. The recently released Large Language models (LLMs), including ChatGPT and GPT-4 (GPT for short), have demonstrated state-of-the-art performance in text understanding by using simple prompt instructions with only a handful of examples for in-context learning. In this paper, we explore GPT as a cost-effective alternative for both data labeling and synthetic data generation, where we subsequently use data obtained from this approach to train various task specific models such as maps intent detection, address detection, address parsing, geo-entity ranking, and rank scores calibration. GPT demonstrates strong potential in generating otherwise hard-to-synthesize data. We observe significant accuracy and relevance improvement across all task specific models when trained or fine-tuned on data generated by GPT. Lastly, we propose a general framework combining labeled data from GPT with other sources and a prompt fine-tune structure to guide GPT model in completing a given task.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Renzhong and Najafabadi, Maryam and Zhang, Chiqun and Chen, Long-Qi and Olenina, Tanya and Yankov, Dragomir},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {GPT, information retrieval, query processing, maps service},
}

@inproceedings{sharipbay_syntax_2019,
	address = {New York, NY, USA},
	series = {{DATA} '19},
	title = {Syntax parsing model of {Kazakh} simple sentences},
	isbn = {978-1-4503-7284-8},
	url = {https://doi.org/10.1145/3368691.3368745},
	doi = {10.1145/3368691.3368745},
	abstract = {This paper proposes a syntactic analysis of Kazakh simple sentences taking into account their semantics. To do this, first, the syntactic rules of sentences are described using formal grammar, then parsing trees and ontological models are built to determine the semantics of their components and the relationships between them. As a formal grammar used Chomsky's context-free grammar, and ontological models were built in the environment of Protege.},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Data} {Science}, {E}-{Learning} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sharipbay, Altynbek and Razakhova, Bibigul and Mukanova, Assel and Yergesh, Banu and Yelibayeva, Gaziza},
	year = {2019},
	note = {event-place: Dubai, United Arab Emirates},
	keywords = {ontological model, formal grammar, parsing of Kazakh language, parsing tree, simple sentences, syntactic rules of sentences},
}

@inproceedings{fierro_application-driven_2022,
	address = {New York, NY, USA},
	series = {{BuildSys} '22},
	title = {Application-driven creation of building metadata models with semantic sufficiency},
	isbn = {978-1-4503-9890-9},
	url = {https://doi.org/10.1145/3563357.3564083},
	doi = {10.1145/3563357.3564083},
	abstract = {Semantic metadata models such as Brick, RealEstateCore, Project Haystack, and BOT promise to simplify and lower the cost of developing software for smart buildings, enabling the widespread deployment of energy efficiency applications. However, creating these models remains a challenge. Despite recent advances in creating models from existing digital representations like point labels and architectural models, there is still no feedback mechanism to ensure that the human input to these methods results in a model that can actually support the desired software.In this paper, we introduce the notion of semantic sufficiency, a practical principle for semantic metadata model creation that asserts that a model is "finished" when it contains the metadata necessary to support a given set of applications. To support semantic sufficiency, we design a standard representation for capturing application metadata requirements and a templating system for generating common metadata model components with limited user input. We then construct an iterative model creation workflow that integrates metadata requirements to direct the model creation effort, and present several novel optimizations that increase the model utility while minimizing the effort by a human operator. These new abstractions for model creation and validation lower model development costs and ensure the utility of the resulting model, thus facilitating the adoption of intelligent building applications.},
	booktitle = {Proceedings of the 9th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {Fierro, Gabe and Saha, Avijit and Shapinsky, Tobias and Steen, Matthew and Eslinger, Hannah},
	year = {2022},
	note = {event-place: Boston, Massachusetts},
	keywords = {ontology, semantics, metadata, applications, brick},
	pages = {228--237},
}

@inproceedings{bekmanova_two_2021,
	address = {New York, NY, USA},
	series = {{ICEMIS}'21},
	title = {Two approaches of improving e-learning models qualities},
	isbn = {978-1-4503-9044-6},
	url = {https://doi.org/10.1145/3492547.3492680},
	doi = {10.1145/3492547.3492680},
	abstract = {ABSTRACTThe proposed two approaches of improving e-learning models qualities let us redesign existing models and e-learning systems.Model for organizing blended and distance learning involves the creation of an individual learning path, which makes it flexible. Ontological model used like a tool for optimization and redesign existing system modules.},
	booktitle = {The 7th {International} {Conference} on {Engineering} \&amp; {MIS} 2021},
	publisher = {Association for Computing Machinery},
	author = {Bekmanova, Gulmira and Nazyrova, Aizhan and Sharipbay, Altynbek and Suvorovsky, Oleg and Somzhurek, Baubek},
	year = {2021},
	note = {event-place: Almaty, Kazakhstan},
	keywords = {artificial intelligence, e-learning, ontological model, Blended learning, distance learning, e-learning system optimization, personalized training},
}

@inproceedings{xiang_creating_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Creating {Signature}-{Based} {Views} for {Description} {Logic} {Ontologies} with {Transitivity} and {Qualified} {Number} {Restrictions}},
	isbn = {978-1-4503-9096-5},
	url = {https://doi.org/10.1145/3485447.3511924},
	doi = {10.1145/3485447.3511924},
	abstract = {Developing ontologies for the Semantic Web is a time-consuming and error-prone task that typically requires the investment of considerable manpower and resources, as well as collaborative efforts. A potentially better idea is to reuse the “off-the-shelf” ontologies, whenever possible, somehow as per certain demands and requirements. A promising way to achieve ontology reuse is through creating views of ontologies, analogous to creating views of databases, with the resulting views focusing on specific topics and content of the original ontologies. This paper explores the problem of creating views of ontologies using a uniform interpolation approach. In particular, we develop a novel and practical uniform interpolation method for creating signature-based views for ontologies specified in the description logic , a very expressive description logic for which uniform interpolation has not been fully addressed. The method is terminating and sound, and computes uniform interpolants of -ontologies by eliminating from the input ontologies the names not used in the view using a forgetting procedure. This makes it the first and so far the only approach to eliminate both concept and (non-transitive) role names from -ontologies. Despite the inherent difficulty of uniform interpolation for this level of expressivity, an empirical evaluation with a prototypical implementation show very good success rates on a corpus of real-world ontologies, and demonstrates clear algorithmic advantage over the state-of-the-art system LETHE. This is extremely useful from the semantic web perspective, as it provides knowledge engineers with a powerful tool to create views of ontologies for ontology reuse.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Xiang, Yue and Wu, Xuan and Lu, Chang and Zhao, Yizheng},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {Ontologies, Description Logics, Forgetting, Knowledge Representation and Reasoning, The Semantic Web, Uniform Interpolation},
	pages = {808--817},
}

@inproceedings{fletcher_credal_2025,
	address = {New York, NY, USA},
	series = {{HILDA} '25},
	title = {{CREDAL}: {Close} {Reading} of {Data} {Models}},
	isbn = {979-8-4007-1959-2},
	url = {https://doi.org/10.1145/3736733.3736737},
	doi = {10.1145/3736733.3736737},
	abstract = {Data models are foundational to the creation of data and any data-driven system. Every algorithm, ML model, statistical model, and database depends on a data model to function. As such, data models are rich sites for examining the material, social, and political conditions shaping technical systems. Inspired by literary criticism, we propose close readings of data models—treating them as artifacts to be analyzed like texts. This practice highlights the materiality, genealogy, techne, closure, and design of data systems.While literary theory teaches that no single reading is "correct," systematic guidance is vital—especially for those in computing and data science, where sociopolitical dimensions are often overlooked. To address this gap, we introduce the CREDAL methodology for close readings of data models. We describe its iterative development and share results from a qualitative evaluation, demonstrating its usability and value for critical data studies.},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	publisher = {Association for Computing Machinery},
	author = {Fletcher, George and Nahurna, Olha and Prytula, Matvii and Stoyanovich, Julia},
	year = {2025},
	note = {event-place: Intercontinental Berlin, Berlin, Germany},
}

@article{aameri_reducible_2023,
	title = {Reducible {Theories} and {Amalgamations} of {Models}},
	volume = {24},
	issn = {1529-3785},
	url = {https://doi.org/10.1145/3565364},
	doi = {10.1145/3565364},
	abstract = {Within knowledge representation in artificial intelligence, a first-order ontology is a theory in first-order logic that axiomatizes the concepts in some domain. Ontology verification is concerned with the relationship between the intended models of an ontology and the models of the axiomatization of the ontology. In particular, we want to characterize the models of an ontology up to isomorphism and determine whether or not these models are equivalent to the intended models of the ontology. Unfortunately, it can be quite difficult to characterize the models of an ontology up to isomorphism. In the first half of this article, we review the different metalogical relationships between first-order theories and identify which relationship is needed for ontology verification. In particular, we will demonstrate that the notion of logical synonymy is needed to specify a representation theorem for the class of models of one first-order ontology with respect to another. In the second half of the article, we discuss the notion of reducible theories and show we can specify representation theorems by which models are constructed by amalgamating models of the constituent ontologies.},
	number = {1},
	journal = {ACM Trans. Comput. Logic},
	author = {Aameri, Bahar and Grüninger, Michael},
	month = jan,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {model theory, Amalgamations of models, first-order logic, reducible theories, relative interpretation, synonymous theories},
}

@inproceedings{chammard_assisted_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Assisted authoring of model-based systems engineering documents},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3421406},
	doi = {10.1145/3417990.3421406},
	abstract = {In systems engineering practices, system design and analysis have historically been performed using a document-centric approach where stakeholders produce a number of documents that represent their views on a system under development. Given the ad-hoc, disparate, and informal nature of natural language documents, these views become quickly inconsistent. Rigor in engineering work is also lost in the transition from model-based engineering design and analysis to engineering documents. Once the documents are delivered, the engineering portion of the work is disconnected. In the Open Model Based Engineering Environment (OpenMBEE), Cross-References (aka transclusions) synthesize relevant engineering information where model elements are not simply hyperlinked, but de-referenced in place in a document, upgrading a document-based process with model-based engineering technology. Those Cross-References are nowadays partially created manually, putting a burden on the engineer who is authoring the document. This paper presents an approach which can assist the engineer by providing machine-generated suggestions for Cross-References using language processing, graph analysis, and clustering technologies on model data managed by the OpenMBEE infrastructure.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Chammard, Thomas Boyer and Regalia, Blake and Karban, Robert and Gomes, Ivan},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {natural language processing, MBSE, clustering, SysML, entity linking, graph analysis, OpenMBEE},
}

@inproceedings{yuan_analysis_2021,
	address = {New York, NY, USA},
	series = {{EITCE} '20},
	title = {Analysis and {Research} on {Book} {Recommendation} {Model} {Based} on {Big} {Data}},
	isbn = {978-1-4503-8781-1},
	url = {https://doi.org/10.1145/3443467.3443723},
	doi = {10.1145/3443467.3443723},
	abstract = {In the context of big data, how to define user behavior models and provide personalized reading services for readers by mining large amounts of user data is a problem that the current reading platform needs to optimize urgently. First, we need to analyze the user behavior model to construct the research status and existing problems, in order to provide large data personalized services, targeted user behavior model based on reading platform construction strategies and construction methods, and designs a user logging library is utilized to extract the user interest in dominant and recessive demand ontology of personalized service plan. The user behavior model based on ontology can be technically seamlessly connected with the big data analysis platform, so as to provide real-time and accurate services, which can effectively deal with the challenges of "knowledge trek", "information overload" and "emotional loss" faced by the personalized service of the reading platform in the current big data environment.},
	booktitle = {Proceedings of the 2020 4th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Ming and Yang, Shulin and Gu, Mengdie and Gu, Huijie},
	year = {2021},
	note = {event-place: Xiamen, China},
	keywords = {Ontology, Big Data, Linked Data, Personalized Service, User Behavior Model},
	pages = {21--25},
}

@inproceedings{difallah_wikirag_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {{WikiRAG}: {Revisiting} {Wikidata} {KGC} {Datasets} with {Community} {Updates} and {Retrieval}-{Augmented} {Generation}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3737444},
	doi = {10.1145/3711896.3737444},
	abstract = {Link prediction is an important task for knowledge graph completion and curation, and it has received significant attention from the research community. However, researchers often train and evaluate new models on small or outdated datasets that do not reflect the current state of knowledge, thereby disregarding new information and the rich textual content often linked to knowledge graphs. As a result, many opportunities to leverage these dimensions are missed. We introduce WikiRAG, a framework for knowledge completion and evaluation derived from Wikidata and Wikipedia, which enables research integrating retrieval techniques and large language models. Our framework combines the following contributions: (i) We revisit the Wikidata5M dataset by updating it to reflect the current state of Wikidata and providing automated tools for its periodic maintenance. (ii) We enrich the dataset with long-form textual content sourced from Wikipedia, enabling research that goes beyond traditional graph structures and shallow text methods toward dense retrieval techniques. (iii) We propose a simple yet effective baseline that leverages retrieval-augmented generation, demonstrating the utility of the dataset and integrating language model capabilities for link prediction. The revised dataset, coined Wikidata5M-RE, shows that the original graph grew by roughly 50\% in the number of edges, while 10\% of the edges have been removed. A comparative analysis of classic methods demonstrates that these changes can impact downstream task evaluation. Finally, our evaluation of WikiRAG's KGC method shows an improvement of up to 9\% in link prediction accuracy over state-of-the-art baselines, setting the stage for a new avenue in knowledge completion that uses deep information extraction. The source code, data, and other artifacts have been made available on the project website: https://github.com/colab-nyuad/WikiRAG},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Difallah, Djellel},
	year = {2025},
	note = {event-place: Toronto ON, Canada},
	keywords = {large language models, retrieval augmented generation, knowledge graph completion, benchmarking, wikidata},
	pages = {5391--5401},
}

@inproceedings{martinez-gil_matching_2022,
	address = {New York, NY, USA},
	series = {{iiWAS2021}},
	title = {Matching {Large} {Biomedical} {Ontologies} {Using} {Symbolic} {Regression}},
	isbn = {978-1-4503-9556-4},
	url = {https://doi.org/10.1145/3487664.3487781},
	doi = {10.1145/3487664.3487781},
	abstract = {The problem of ontology matching consists of finding the semantic correspondences between two ontologies that, although belonging to the same domain, have been developed separately. Matching methods are of great importance since they allow us to find the pivot points from which an automatic data integration process can be established. Unlike the most recent developments based on deep learning, this study presents our research on the development of new methods for ontology matching that are accurate and interpretable at the same time. For this purpose, we rely on a symbolic regression model specifically trained to find the mathematical expression that can solve the ground truth accurately, with the possibility of being understood by a human operator and forcing the processor to consume as little energy as possible. The experimental evaluation results show that our approach seems to be promising.},
	booktitle = {The 23rd {International} {Conference} on {Information} {Integration} and {Web} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Martinez-Gil, Jorge and Yin, Shaoyi and Küng, Josef and Morvan, Franck},
	year = {2022},
	note = {event-place: Linz, Austria},
	keywords = {Information Integration, Ontology Matching, Similarity Measures},
	pages = {162--167},
}

@inproceedings{kohli_pentapen_2024,
	address = {New York, NY, USA},
	series = {{ICBBT} '24},
	title = {{PentaPen}: {Combining} {Penalized} {Models} to {Identify} {Important} {SNPs} on {Whole}-genome {Arabidopsis} thaliana {Data}},
	isbn = {979-8-4007-1766-6},
	url = {https://doi.org/10.1145/3674658.3674660},
	doi = {10.1145/3674658.3674660},
	abstract = {In the rapidly advancing field of genomics, the identification of Single Nucleotide Polymorphisms (SNPs) plays a crucial role in understanding complex phenotypic traits. This study introduces “PentaPen”, an innovative computational workflow which combines the strengths of five penalized models to achieve improved accuracy in SNP detection. We compare the performance of PentaPen with existing models, highlighting its advantages in solving problems arising from when the number of predictors exceeds the number of samples. Beyond model comparison, we provide insights into PentaPen’s effectiveness in utilizing all SNPs as input, streamlines data pre-processing, and leverages parallel computation, enabling the workflow a considerable stride in SNP detection. Furthermore, a thorough evaluation and comparison of computational complexities signifies competitive edge of the workflow over individual penalized models. As future research directions, we propose applications of PentaPen to plant-specific characteristics and suggest further explorations to assess the robustness of its findings. In summary, this manuscript presents the genomics community with a tool that combines computational efficiency with high-precision SNP detection, making a strong contribution to the field of genomic research.},
	booktitle = {Proceedings of the 2024 16th {International} {Conference} on {Bioinformatics} and {Biomedical} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Kohli, Nikita and Tomal, Jabed and Lin, Wenjun and Yan, Yan},
	year = {2024},
	keywords = {Machine Learning, Classification, Regression, Genome-Wide Association Study, High Dimensional Data, Single Nucleotide Polymorphism, SNP Identification},
	pages = {9--16},
}

@inproceedings{ben_chaaben_toward_2024,
	address = {New York, NY, USA},
	series = {{MODELS} '24},
	title = {Toward {Intelligent} {Generation} of {Tailored} {Graphical} {Concrete} {Syntax}},
	isbn = {979-8-4007-0504-5},
	url = {https://doi.org/10.1145/3640310.3674085},
	doi = {10.1145/3640310.3674085},
	abstract = {In model-driven engineering, the concrete syntax of a domain-specific modeling language (DSML) is fundamental as it constitutes the primary point of interaction between the user and the DSML. Nevertheless, the conventional one-size-fits-all approach to concrete syntax often undermines the effectiveness of DSMLs, as it fails to accommodate the diverse constraints and specific requirements inherent to diverse users and usage contexts. Such shortcomings can lead to a significant decline in the performance, usability, and efficiency of DSMLs. This vision paper proposes a conceptual framework to generate concrete syntax intelligently. Our framework considers multiple concerns of users and aims to align the concrete syntax with the context of the DSML usage. Additionally, we detail a baseline process to employ our framework in practice, leveraging large language models to expedite the generation of tailored concrete syntax. We illustrate the potential of our vision with two concrete examples and discuss the shortcomings and research challenges of current intelligent generation techniques.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ben Chaaben, Meriem and Ben Sghaier, Oussama and Dhaouadi, Mouna and Elrasheed, Nafisa and Darif, Ikram and Jaoua, Imen and Oakes, Bentley and Syriani, Eugene and Hamdaqa, Mohammad},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {Large Language Models, Artificial Intelligence, Concrete Syntax, Domain-specific Modeling Languages},
	pages = {160--171},
}

@inproceedings{anelli_sixth_2024,
	address = {New York, NY, USA},
	series = {{RecSys} '24},
	title = {Sixth {Knowledge}-aware and {Conversational} {Recommender} {Systems} {Workshop} ({KaRS})},
	isbn = {979-8-4007-0505-2},
	url = {https://doi.org/10.1145/3640457.3687114},
	doi = {10.1145/3640457.3687114},
	abstract = {Recommender systems, though widely used, often struggle to engage users effectively. While deep learning methods have enhanced connections between users and items, they often neglect the user’s perspective. Knowledge-based approaches, utilizing knowledge graphs, offer semantic insights and address issues like knowledge graph embeddings, hybrid recommendation, and interpretable recommendation. More recently, neural-symbolic systems, combining data-driven and symbolic techniques, show promise in recommendation systems, especially when used with knowledge graphs. Moreover, content features become vital in conversational recommender systems, which demand multi-turn dialogues. Recent literature highlights increasing interest in this area, particularly with the emergence of Large Language Models (LLMs), which excel in understanding user queries and generating recommendations in natural language. Sixth Knowledge-aware and Conversational Recommender Systems (KaRS) Workshop aims to disseminate advancements and discuss about challenges and opportunities.},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Anelli, Vito Walter and Ferrara, Antonio and Musto, Cataldo and Narducci, Fedelucio and Ragone, Azzurra and Zanker, Markus},
	year = {2024},
	note = {event-place: Bari, Italy},
	keywords = {knowledge graphs, large language models, natural language processing, recommender systems, conversational agents, neuro-symbolic},
	pages = {1245--1249},
}

@inproceedings{pei_construction_2020,
	address = {New York, NY, USA},
	series = {{RICAI} '20},
	title = {Construction of {Curriculum} {Knowledge} {Map} based on {Ontology}},
	isbn = {978-1-4503-8830-6},
	url = {https://doi.org/10.1145/3438872.3439091},
	doi = {10.1145/3438872.3439091},
	abstract = {With the rapid development of science and technology, knowledge update is accelerating, which puts forward higher requirements for teachers and students in Colleges and universities, and needs to grasp the latest development trends of curriculum knowledge related fields faster, more comprehensive and more accurate. Many schools have digitized their educational resources. However, the traditional sharing of educational resources lacks a unified knowledge representation structure, which makes the sharing and reuse of learning resources unsatisfactory. Curriculum is the core of school knowledge teaching. It evaluates the curriculum system and discipline system in order to achieve certain teaching objectives. Curriculum knowledge includes explicit knowledge and tacit knowledge.Knowledge map is a kind of model which can describe knowledge in semantic and knowledge level. Its purpose is to acquire, organize and present knowledge in a general and intuitive way, to search and match knowledge quickly, so as to improve the utilization of knowledge by learners and knowledge workers. Curriculum knowledge has obvious ontology characteristics, and there are many inconveniences and shortcomings in the presentation of traditional knowledge map. Using ontology technology to construct curriculum knowledge map can not only reflect the relationship between knowledge modules, but also realize the mining and representation of more knowledge relations and types such as tacit knowledge to a certain extent.},
	booktitle = {Proceedings of the 2020 2nd {International} {Conference} on {Robotics}, {Intelligent} {Control} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Pei, Pei and Xuejing, Ding and Deqing, Zhang},
	year = {2020},
	note = {event-place: Shanghai, China},
	keywords = {ontology, knowledge map, curriculum knowledge, ontology construction},
	pages = {259--265},
}

@inproceedings{franklin_ontology_2022,
	address = {New York, NY, USA},
	series = {{AIES} '22},
	title = {An {Ontology} for {Fairness} {Metrics}},
	isbn = {978-1-4503-9247-1},
	url = {https://doi.org/10.1145/3514094.3534137},
	doi = {10.1145/3514094.3534137},
	abstract = {Recent research has revealed that many machine-learning models and the datasets they are trained on suffer from various forms of bias, and a large number of different fairness metrics have been created to measure this bias. However, determining which metrics to use, as well as interpreting their results, is difficult for a non-expert due to a lack of clear guidance and issues of ambiguity or alternate naming schemes between different research papers. To address this knowledge gap, we present the Fairness Metrics Ontology (FMO), a comprehensive and extensible knowledge resource that defines each fairness metric, describes their use cases, and details the relationships between them. We include additional concepts related to fairness and machine learning models, enabling the representation of specific fairness information within a resource description framework (RDF) knowledge graph. We evaluate the ontology by examining the process of how reasoning-based queries to the ontology were used to guide the fairness metric-based evaluation of a synthetic data model.},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Franklin, Jade S. and Bhanot, Karan and Ghalwash, Mohamed and Bennett, Kristin P. and McCusker, Jamie and McGuinness, Deborah L.},
	year = {2022},
	note = {event-place: Oxford, United Kingdom},
	keywords = {bias, fairness metric, machine learning evaluation, rdf knowledge graph},
	pages = {265--275},
}

@inproceedings{ousmer_ontology_2019,
	address = {New York, NY, USA},
	series = {{EICS} '19},
	title = {An ontology for reasoning on body-based gestures},
	isbn = {978-1-4503-6745-5},
	url = {https://doi.org/10.1145/3319499.3328238},
	doi = {10.1145/3319499.3328238},
	abstract = {Body-based gestures, such as acquired by Kinect sensor, today benefit from efficient tools for their recognition and development, but less for automated reasoning. To facilitate this activity, an ontology for structuring body-based gestures, based on user, body and body parts, gestures, and environment, is designed and encoded in Ontology Web Language according to modelling triples (subject, predicate, object). As a proof-of-concept and to feed this ontology, a gesture elicitation study collected 24 participants X 19 referents for IoT tasks = 456 elicited body-based gestures, which were classified and expressed according to the ontology.},
	booktitle = {Proceedings of the {ACM} {SIGCHI} {Symposium} on {Engineering} {Interactive} {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ousmer, Mehdi and Vanderdonckt, Jean and Buraga, Sabin},
	year = {2019},
	note = {event-place: Valencia, Spain},
	keywords = {gesture interaction, Microsoft Kinect, natural gestures, ontology web language, resource description file},
}

@inproceedings{heng_building_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {Building {User} {Stories} and {Behavior} {Driven} {Development} {Scenarios} with a {Strict} {Set} of {Concepts}: {Ontology}, {Benefits} and {Primary} {Validation}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3577696},
	doi = {10.1145/3555776.3577696},
	abstract = {Behavior Driven Development (BDD) offers a way to express scenarios, written in structured natural language, on how the system should act to fulfill a requirement. Such a test scenario is written together with the requirement; this way these two can be conceived in unison, nested into each other. Lots of templates have been written to construct BDD scenarios and various practices were born out of usage. We fail to find documentation on templates. A strict set of templates with a clear definition of the used keywords aligned with the intends of BDD has been proposed recently in the form of an ontology. The present paper (i) evaluates the ontology on existing BDD scenarios found in the GitHub repository (exogenous validation) and (ii) merges an ontology for user story elements' representation with one for expressing BDD scenarios to evaluate its ability to guide the writing of BDD scenarios (endogenous validation). By linking both through strictly-identified concepts, we (i) provide guidance to the practitioner in the agile requirements engineering phase and (ii) with the adequate tagging of elements during the requirements engineering process we get meta-data allowing to suggest treatments to the BDD scenario (e.g. test automation or forward engineering into a software architecture or even code).},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Heng, Samedi and Tsilionis, Konstantinos and Wautelet, Yves},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {agile development, behavior driven development, scrum, test scenarios, user stories},
	pages = {1422--1429},
}

@inproceedings{chukkapalli_ontology_2021,
	address = {New York, NY, USA},
	series = {{SAT}-{CPS} '21},
	title = {Ontology driven {AI} and {Access} {Control} {Systems} for {Smart} {Fisheries}},
	isbn = {978-1-4503-8319-6},
	url = {https://doi.org/10.1145/3445969.3450429},
	doi = {10.1145/3445969.3450429},
	abstract = {Increasing number of internet connected devices has paved a path for smarter ecosystems in various sectors such as agriculture, aquaculture, manufacturing, healthcare, etc. Especially, integrating technologies like big data, artificial intelligence (AI), blockchain, etc. with internet connected devices has increased efficiency and productivity. Therefore, fishery farmers have started adopting smart fisheries technologies to better manage their fish farms. Despite their technological advancements smart fisheries are exposed and vulnerable to cyber-attacks that would cause a negative impact on the ecosystem both physically and economically.Therefore in this paper, we present a smart fisheries ecosystem where the architecture describes various interactions that happen between internet connected devices. We develop a smart fisheries ontology based on the architecture and implement Attribute Based Access Control System (ABAC) where access to resources of smart fisheries is granted by evaluating the requests. We also discuss how access control decisions are made in multiple use case scenarios of a smart fisheries ecosystem. Furthermore, we elaborate on some AI applications that would enhance the smart fisheries ecosystem.},
	booktitle = {Proceedings of the 2021 {ACM} {Workshop} on {Secure} and {Trustworthy} {Cyber}-{Physical} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chukkapalli, Sai Sree Laya and Aziz, Shaik Barakhat and Alotaibi, Nouran and Mittal, Sudip and Gupta, Maanak and Abdelsalam, Mahmoud},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {artificial intelligence, ontology, cybersecurity, access control, smart fisheries},
	pages = {59--68},
}

@inproceedings{vielhauer_towards_2025,
	address = {New York, NY, USA},
	series = {{IH}\&amp;{MMSEC} '25},
	title = {Towards {Modeling} {Hidden} \&amp; {Steganographic} {Malware} {Communication} based on {Images}},
	isbn = {979-8-4007-1887-8},
	url = {https://doi.org/10.1145/3733102.3733152},
	doi = {10.1145/3733102.3733152},
	abstract = {Recently, an increasing number of IT security incidents involving malware, which makes use of hidden and steganographic channels for malicious communication (a.k.a. as "stegomalware"), can be observed in the wild. Especially the use of images to hide malicious code is rising. In consideration of this shift, a new model is proposed in this paper, which aims to help security professionals to identify and analyze incidents revolving around steganographic malware in the future. The model focuses on practical aspects of steganalysis of communication data to elaborate linking properties to previous code analysis knowledge. The model features two distinct roles that interact with a knowledge base which stores malware features and helps building a context for the incident. For evaluation, two image steganography malware types are chosen from popular databases (malpedia and MITRE ATT\&amp;CK®), which are analyzed in multiple steps including steganalysis and code analysis. It is conceptually shown, how the extracted features can be stored in a knowledge base for later use to identify stegomalware from communication data without the need of a thorough code analysis. This allows to uncover previously hidden meta-information about the examined malicious programs, enrich the incident’s forensic context traces and thus allows for thorough forensic insights, including attribution and improved preventive security measures in the future.},
	booktitle = {Proceedings of the 2025 {ACM} {Workshop} on {Information} {Hiding} and {Multimedia} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Vielhauer, Claus and Loewe, Fabian and Pilgermann, Michael},
	year = {2025},
	keywords = {attribution, image steganography, Media forensics, stegomalware},
	pages = {52--63},
}

@article{feng_phenobert_2022,
	title = {{PhenoBERT}: {A} {Combined} {Deep} {Learning} {Method} for {Automated} {Recognition} of {Human} {Phenotype} {Ontology}},
	volume = {20},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2022.3170301},
	doi = {10.1109/TCBB.2022.3170301},
	abstract = {Automated recognition of Human Phenotype Ontology (HPO) terms from clinical texts is of significant interest to the field of clinical data mining. In this study, we develop a combined deep learning method named PhenoBERT for this purpose. PhenoBERT uses BERT, currently the state-of-the-art NLP model, as its core model for evaluating whether a clinically relevant text segment (CTS) could be represented by an HPO term. However, to avoid unnecessary comparison of a CTS with each of ∼14,000 HPO terms using BERT, we introduce a two-levels CNN module consisting of a series of CNN models organized at two levels in PhenoBERT. For a given CTS, the CNN module produces only a short list of candidate HPO terms for BERT to evaluate, significantly improving the computational efficiency. In addition, BERT is able to assign an ancestor HPO term to a CTS when recognition of the direct HPO term is not successful, mimicking the process of HPO term assignment by human. In two benchmarks, PhenoBERT outperforms four traditional dictionary-based methods and two recently developed deep learning-based methods in two benchmark tests, and its advantage is more obvious when the recognition task is more challenging. As such, PhenoBERT is of great use for assisting in the mining of clinical text data.},
	number = {2},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Feng, Yuhao and Qi, Lei and Tian, Weidong},
	month = apr,
	year = {2022},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {1269--1277},
}

@inproceedings{zhang_analysis_2024,
	address = {New York, NY, USA},
	series = {{ISAIMS} '23},
	title = {Analysis of the mechanism of antipsychotics induced abnormal {ECG} using {Medical} {Ontologies} and {Medical} {Knowledge} {Graphs}},
	isbn = {979-8-4007-0813-8},
	url = {https://doi.org/10.1145/3644116.3644311},
	doi = {10.1145/3644116.3644311},
	abstract = {Objective: Our aim is to understand the underlying mechanisms of antipsychotics-induced ECG abnormalities by using artificial intelligence (AI) method and information processing technique. Methods: This study employed the SNOMED CT (Systematized Nomenclature of Medicine - Clinical Terms) and the knowledge graph of brain science, to find relevant concepts associated with ECG abnormalities and antipsychotics usage. Results: Approximately 30 relevant publications were found, only 10 articles were included in this comprehensive analysis study. Our findings suggest that the underlying mechanisms of antipsychotic-induced ECG abnormalities include immediate, medium and lasting effects on the predisposing level, the autonomic nervous system, cardiomyocytes, and the blood vessel levels. Conclusions: The potential mechanisms underpinning antipsychotics-induced ECG abnormalities include predisposing, the autonomic nervous system, cardiomyocytes, and the blood vessel levels by use of the NOMED CT and the knowledge graph of brain science.},
	booktitle = {Proceedings of the 2023 4th {International} {Symposium} on {Artificial} {Intelligence} for {Medicine} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Xiyan and Hu, Chenping and Zhang, Lei and Huang, Zhisheng and Qin, Hongyun},
	year = {2024},
	note = {event-place: Chengdu, China},
	pages = {1146--1151},
}

@article{zhao_beyond_2025,
	title = {Beyond {Sequential} {Patterns}: {Rethinking} {Healthcare} {Predictions} with {Contextual} {Insights}},
	volume = {43},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3733234},
	doi = {10.1145/3733234},
	abstract = {Healthcare predictions, such as readmission prediction, stand as a cornerstone of societal well-being, exerting a profound influence on individual health outcomes and communal vitality. Existing research primarily employs advanced graph neural networks and sequential algorithms for patient modeling, with a focus on discerning the connections and sequential patterns inherent in Electronic Health Records (EHRs). However, the heterogeneity of entity interactions, the locality of EHR data, and the oversight of target relevance hinder further improvements. To address these limitations, we introduce a novel framework Beyond Sequential Patterns (BSP), which facilitates precise healthcare predictions by incorporating tri-contextual information. Specifically, we establish a symptom-driven hypergraph network with four semantic hyperedges tailored to the intricacies of the healthcare scenario, such as ontology. This serves as a global context, tracking the heterogeneous entity collaboration within and across patients. Moreover, we construct an extensive knowledge graph leveraging existing medical databases and large language models. By sampling and refining knowledge subgraphs as local context, we bolster the semantic associations of medical entities from closed-set EHR data to the open world. Finally, we introduce the candidate context, an explicit entity-relation loss. It enforces the neighbor consistency between the target and the representation during optimization, thus accounting for correlations among targets. Extensive experiments and rigorous robustness analysis on five tasks derived from four large medical datasets underscore the BSP’s superiority over the leading baselines, with improvements of 11\%, 3\%, 11\%, 3.5\%, and 2\% across five tasks, demonstrating the efficacy of incorporating diverse contexts.},
	number = {4},
	journal = {ACM Trans. Inf. Syst.},
	author = {Zhao, Chuang and Tang, Hui and Zhao, Hongke and Li, Xiaomeng},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge graph, Large language model, Language model, Semantics, Knowledge management, Health care, Electronic health record, Healthcare prediction, Medical informatics, Forecasting, Hypergraph learning, Medical computing, Electronic health, Health records, Health outcomes, Hyper graph, Sequential patterns, Well being},
	annote = {Cited by: 2},
}

@inproceedings{ukegbu_ontology-based_2023,
	address = {New York, NY, USA},
	series = {{EICC} '23},
	title = {Ontology-based {Framework} for {Boundary} {Verification} of {Safety} and {Security} {Properties} in {Industrial} {Control} {Systems}},
	isbn = {978-1-4503-9829-9},
	url = {https://doi.org/10.1145/3590777.3590785},
	doi = {10.1145/3590777.3590785},
	abstract = {As part of Industrial Control Systems (ICS), the control logic controls the physical processes of critical infrastructures such as power plants and water and gas distribution. The Programmable Logic Controller (PLC) commonly manages these processes through actuators based on information received from sensor readings. Therefore, boundary checking is essential in ICS because sensor readings and actuator values must be within the safe range to ensure safe and secure ICS operation. In this paper, we propose an ontology-based approach to provide the knowledge required to verify the boundaries of ICS components with respect to their safety and security specifications. For the proof of concept, the formal model of the Programmable Logic Controller (PLC) is created in UPPAAL and validated in UPPAAL-API. Then, the proposed boundary verification algorithm is used to import the required information from the safety/security ontology},
	booktitle = {Proceedings of the 2023 {European} {Interdisciplinary} {Cybersecurity} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Ukegbu, Chibuzo and Neupane, Ramesh and Mehrpouyan, Hoda},
	year = {2023},
	note = {event-place: Stavanger, Norway},
	keywords = {Control Systems, Formal Verification, Security Properties},
	pages = {47--52},
}

@inproceedings{wu_ontology_2020,
	address = {New York, NY, USA},
	series = {{CCIOT} '20},
	title = {Ontology {Matching} by {Jointly} {Encoding} {Terminological} {Description} and {Network} {Structure}},
	isbn = {978-1-4503-7527-6},
	url = {https://doi.org/10.1145/3429523.3429534},
	doi = {10.1145/3429523.3429534},
	abstract = {Ontology matching is usually performed to find semantic correspondences between the entity elements of different ontologies to enable interoperability. Current research on ontology matching has largely focused on representation learning. However, there still exist two limitations. Firstly, they are only used in the element level matching phase, ignoring relations of the entity. Secondly, the final alignment threshold is usually determined manually within these methods. It is difficult for an expert to adjust the threshold value and even more for non-expert user. To address these issues, we propose an alternative ontology matching framework, which models the matching process by embedding techniques with jointly encoding ontology terminological description and network structure. We further improve our iterative final alignment method by introducing an automatic adjustment of threshold method. Finally, we perform an experimental evaluation and compare it with state-of-the-art ontology matching systems on four Ontology Alignment Evaluation Initiative (OAEI) datasets. Our approach performs better than most of the systems and achieves a competitive performance. Moreover, we obtained F-measure values of 93.8\% and 90.8\% on the OAEI Large Biomedical Ontologies FMA-NCI and FMA-SNOMED subtasks.},
	booktitle = {Proceedings of the 2020 5th {International} {Conference} on {Cloud} {Computing} and {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Jifang and Lv, Jianghua and Guo, Haoming and Ma, Shilong},
	year = {2020},
	note = {event-place: Okinawa, Japan},
	keywords = {Ontology matching, semantic similarity, final alignment, graph attention-based autoencoder, network embedding},
	pages = {77--85},
}

@inproceedings{liu_intelligent_2024,
	address = {New York, NY, USA},
	series = {{ICMLC} '24},
	title = {An {Intelligent} {Risk} {Mining} {Method} by {Whole} {Process}-{Oriented} {Risk} {Analysis} {Model}},
	isbn = {979-8-4007-0923-4},
	url = {https://doi.org/10.1145/3651671.3651779},
	doi = {10.1145/3651671.3651779},
	abstract = {This paper proposes a risk intelligent analysis method oriented to the whole process of events and applications, it is used to solve the problem of difficult and incomplete identification of risks, to dig out the risks behind it, and to provide help to ensure the safety of the public. Specifically, through step-by-step mining of incidents and application usage risks, combined with the event evolutionary graph, the model is run to calculate the similarity of text risks for comprehensive analysis, the innovative point is to propose a risk weighting model for the subject and object entities. Experiment shows that the method can reflect the complete risk of events and applications, and has improved recall and precision. The advantage of this method is the integration of multiple perspectives and disciplines, which has guiding significance in practice.},
	booktitle = {Proceedings of the 2024 16th {International} {Conference} on {Machine} {Learning} and {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Lijuan and Shi, Li},
	year = {2024},
	note = {event-place: Shenzhen, China},
	keywords = {artificial intelligence, event evolutionary graph, risk mining, Whole Process-Oriented analysis},
	pages = {616--620},
}

@article{che_tagging_2024,
	title = {Tagging {Items} with {Emerging} {Tags}: {A} {Neural} {Topic} {Model} {Based} {Few}-{Shot} {Learning} {Approach}},
	volume = {42},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3641859},
	doi = {10.1145/3641859},
	abstract = {The tagging system has become a primary tool to organize information resources on the Internet, which benefits both users and the platforms. To build a successful tagging system, automatic tagging methods are desired. With the development of society, new tags keep emerging. The problem of tagging items with emerging tags is an open challenge for an automatic tagging system, and it has not been well studied in the literature. We define this problem as a tag-centered cold-start problem in this study and propose a novel neural topic model based few-shot learning method named NTFSL to solve the problem. In our proposed method, we innovatively fuse the topic modeling task with the few-shot learning task, endowing the model with the capability to infer effective topics to solve the tag-centered cold-start problem with the property of interpretability. Meanwhile, we propose a novel neural topic model for the topic modeling task to improve the quality of inferred topics, which helps enhance the tagging performance. Furthermore, we develop a novel inference method based on the variational auto-encoding framework for model inference. We conducted extensive experiments on two real-world datasets, and the results demonstrate the superior performance of our proposed model compared with state-of-the-art machine learning methods. Case studies also show the interpretability of the model.},
	number = {4},
	journal = {ACM Trans. Inf. Syst.},
	author = {Che, Shangkun and Liu, Hongyan and Liu, Shen},
	month = mar,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deep learning, classification, Few-shot learning, automatic tagging, generative probabilistic model, neural topic model},
}

@article{zeng_xlore_2024,
	title = {{XLORE} 3: {A} {Large}-{Scale} {Multilingual} {Knowledge} {Graph} from {Heterogeneous} {Wiki} {Knowledge} {Resources}},
	volume = {42},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3660521},
	doi = {10.1145/3660521},
	abstract = {In recent years, knowledge graph (KG) has attracted significant attention from academia and industry, resulting in the development of numerous technologies for KG construction, completion, and application. XLORE is one of the largest multilingual KGs built from Baidu Baike and Wikipedia via a series of knowledge modeling and acquisition methods. In this article, we utilize systematic methods to improve XLORE's data quality and present its latest version, XLORE 3, which enables the effective integration and management of heterogeneous knowledge from diverse resources. Compared with previous versions, XLORE 3 has three major advantages: (1) We design a comprehensive and reasonable schema, namely XLORE ontology, which can effectively organize and manage entities from various resources. (2) We merge equivalent entities in different languages to facilitate knowledge sharing. We provide a large-scale entity linking system to establish the associations between unstructured text and structured KG. (3) We design a multi-strategy knowledge completion framework, which leverages pre-trained language models and vast amounts of unstructured text to discover missing and new facts. The resulting KG contains 446 concepts, 2,608 properties, 66 million entities, and more than 2 billion facts. It is available and downloadable online at , providing a valuable resource for researchers and practitioners in various fields.},
	number = {6},
	journal = {ACM Trans. Inf. Syst.},
	author = {Zeng, Kaisheng and Jin, Hailong and Lv, Xin and Zhu, Fangwei and Hou, Lei and Zhang, Yi and Pang, Fan and Qi, Yu and Liu, Dingxiao and Li, Juanzi and Feng, Ling},
	month = aug,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge graphs, Knowledge graph, knowledge management, Knowledge fusion, Schema construction, Entity alignment, Entity linking, entity linking, Knowledge resource, Entity typing, entity alignment, entity typing, knowledge completion, knowledge fusion, schema construction, Large-scales, Unstructured texts, Knowledge completion},
	annote = {Cited by: 0; All Open Access; Bronze Open Access},
}

@inproceedings{jackermeier_dual_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Dual {Box} {Embeddings} for the {Description} {Logic} {EL}++},
	isbn = {979-8-4007-0171-9},
	url = {https://doi.org/10.1145/3589334.3645648},
	doi = {10.1145/3589334.3645648},
	abstract = {OWL ontologies, whose formal semantics are rooted in Description Logic (DL), have been widely used for knowledge representation. Similar to Knowledge Graphs (KGs), ontologies are often incomplete, and maintaining and constructing them has proved challenging. While classical deductive reasoning algorithms use the precise formal semantics of an ontology to predict missing facts, recent years have witnessed growing interest in inductive reasoning techniques that can derive probable facts from an ontology. Similar to KGs, a promising approach is to learn ontology embeddings in a latent vector space, while additionally ensuring they adhere to the semantics of the underlying DL. While a variety of approaches have been proposed, current ontology embedding methods suffer from several shortcomings, especially that they all fail to faithfully model one-to-many, many-to-one, and many-to-many relations and role inclusion axioms. To address this problem and improve ontology completion performance, we propose a novel ontology embedding method named Box2EL for the DL EL++, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles), and models inter-concept relationships using a bumping mechanism. We theoretically prove the soundness of Box2EL and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets on the tasks of subsumption prediction, role assertion prediction, and approximating deductive reasoning.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Jackermeier, Mathias and Chen, Jiaoyan and Horrocks, Ian},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {web ontology language, link prediction, description logic, ontology completion, ontology embedding},
	pages = {2250--2258},
}

@article{bounhas_usage_2019,
	title = {On the {Usage} of a {Classical} {Arabic} {Corpus} as a {Language} {Resource}: {Related} {Research} and {Key} {Challenges}},
	volume = {18},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3277591},
	doi = {10.1145/3277591},
	abstract = {This article presents a literature review of computer-science-related research applied on hadith, a kind of Arabic narration which appeared in the 7th century. We study and compare existent works in several fields of Natural Language Processing (NLP), Information Retrieval (IR), and Knowledge Extraction (KE). Thus, we illicit their main drawbacks and identify some perspectives, which may be considered by the research community. We also study the characteristics of these types of documents, by enumerating the advantages/limits of using hadith as a language resource. Moreover, our study shows that previous studies used different collections of hadiths, thus making it hard to compare their results objectively. Besides, many preprocessing steps are recurrent through these applications, thus wasting a lot of time. Consequently, the key issues for building generic language resources from hadiths are discussed, taking into account the relevance of related literature and the wide community of researchers that are interested in these narrations. The ultimate goal is to structure hadith books for multiple usages, thus building common collections which may be exploited in future applications.},
	number = {3},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Bounhas, Ibrahim},
	month = jan,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {hadith knowledge extraction, hadith mining, Hadith processing, hadith retrieval},
}

@inproceedings{mohan_low_2021,
	address = {New York, NY, USA},
	series = {{BCB} '21},
	title = {Low resource recognition and linking of biomedical concepts from a large ontology},
	isbn = {978-1-4503-8450-6},
	url = {https://doi.org/10.1145/3459930.3469524},
	doi = {10.1145/3459930.3469524},
	abstract = {Tools to explore scientific literature are essential for scientists, especially in biomedicine, where about a million new papers are published every year. Many such tools provide users the ability to search for specific entities (e.g. proteins, diseases) by tracking their mentions in papers. PubMed, the most well known database of biomedical papers, relies on human curators to add these annotations. This can take several weeks for new papers, and not all papers get tagged. Machine learning models have been developed to facilitate the semantic indexing of scientific papers. However their performance on the more comprehensive ontologies of biomedical concepts does not reach the levels of typical entity recognition problems studied in NLP. In large part this is due to their low resources, where the ontologies are large, there is a lack of descriptive text defining most entities, and labeled data can only cover a small portion of the ontology. In this paper, we develop a new model that overcomes these challenges by (1) generalizing to entities unseen at training time, and (2) incorporating linking predictions into the mention segmentation decisions. Our approach achieves new state-of-the-art results for the UMLS ontology in both traditional recognition/linking (+8 F1 pts) as well as semantic indexing-based evaluation (+10 F1 pts).},
	booktitle = {Proceedings of the 12th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Mohan, Sunil and Angell, Rico and Monath, Nicholas and McCallum, Andrew},
	year = {2021},
	note = {event-place: Gainesville, Florida},
	keywords = {deep learning, UMLS, biomedical concept recognition, named entity recognition and linking},
}

@article{abulaish_domain-specific_2022,
	title = {Domain-{Specific} {Keyword} {Extraction} {Using} {Joint} {Modeling} of {Local} and {Global} {Contextual} {Semantics}},
	volume = {16},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3494560},
	doi = {10.1145/3494560},
	abstract = {Domain-specific keyword extraction is a vital task in the field of text mining. There are various research tasks, such as spam e-mail classification, abusive language detection, sentiment analysis, and emotion mining, where a set of domain-specific keywords (aka lexicon) is highly effective. Existing works for keyword extraction list all keywords rather than domain-specific keywords from a document corpus. Moreover, most of the existing approaches perform well on formal document corpuses but fail on noisy and informal user-generated content in online social media. In this article, we present a hybrid approach by jointly modeling the local and global contextual semantics of words, utilizing the strength of distributional word representation and contrasting-domain corpus for domain-specific keyword extraction. Starting with a seed set of a few domain-specific keywords, we model the text corpus as a weighted word-graph. In this graph, the initial weight of a node (word) represents its semantic association with the target domain calculated as a linear combination of three semantic association metrics, and the weight of an edge connecting a pair of nodes represents the co-occurrence count of the respective words. Thereafter, a modified PageRank method is applied to the word-graph to identify the most relevant words for expanding the initial set of domain-specific keywords. We evaluate our method over both formal and informal text corpuses (comprising six datasets), and show that it performs significantly better in comparison to state-of-the-art methods. Furthermore, we generalize our approach to handle the language-agnostic case, and show that it outperforms existing language-agnostic approaches.},
	number = {4},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Abulaish, Muhammad and Fazil, Mohd and Zaki, Mohammed J.},
	month = jan,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Text mining, information extraction, domain-specific keyword extraction, language-agnostic keyword extraction},
}

@inproceedings{teixeira_de_castro_model-based_2024,
	address = {New York, NY, USA},
	series = {{ARES} '24},
	title = {A {Model}-based {Approach} for {Assessing} the {Security} of {Cyber}-{Physical} {Systems}},
	isbn = {979-8-4007-1718-5},
	url = {https://doi.org/10.1145/3664476.3670470},
	doi = {10.1145/3664476.3670470},
	abstract = {Cyber-Physical Systems (CPSs) complexity has been continuously increasing to support new life-impacting applications, such as Internet of Things (IoT) devices or Industrial Control Systems (ICSs). These characteristics introduce new critical security challenges to both industrial practitioners and academics. This work investigates how Model-Based System Engineering (MBSE) and attack graph approaches could be leveraged to model secure Cyber-Physical System solutions and identify high-impact attacks early in the system development life cycle. To achieve this, we propose a new framework that comprises (1) an easily adoptable modeling paradigm for Cyber-Physical System representation, (2) an attack-graph-based solution for Cyber-Physical System automatic quantitative security analysis, based on the MulVAL security tool, (3) a set of Model-To-Text (MTT) transformation rules to bridge the gap between SysML and MulVAL. We illustrated the validity of our proposed framework through an autonomous ventilation system example. A Denial of Service (DoS) attack targeting an industrial communication protocol was identified and displayed as attack graphs. In future work, we intend to connect the approach to dynamic security databases for automatic countermeasure selection.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Teixeira De Castro, Hugo and Hussain, Ahmed and Blanc, Gregory and El Hachem, Jamal and Blouin, Dominique and Leneutre, Jean and Papadimitratos, Panos},
	year = {2024},
	note = {event-place: Vienna, Austria},
	keywords = {Critical Infrastructures, Risk Analysis, Security and Privacy for Cyber-Physical Systems, Security by Design., Threats and Attack Modelling, Usable Security and Privacy},
}

@inproceedings{rodzman_domain_2019,
	address = {New York, NY, USA},
	series = {{ICSCA} '19},
	title = {Domain {Specific} {Classification} of {Malay} {Based} {Complaints} using the {Complaint} {Concept} {Ontologies}},
	isbn = {978-1-4503-6573-4},
	url = {https://doi.org/10.1145/3316615.3316682},
	doi = {10.1145/3316615.3316682},
	abstract = {The complaint from users is an effective method to identify the quality of services and facilities provided by an organization. The efficiency to respond to users' complaint also depends on an effective workflow. By having an effective method and workflow, the action taken by the management to improve the quality of services and facilities can be done immediately and effectively. One of the ways is by classifying the complaints that will isolate related complaints. This paper presents the implementation of the classification system that combines the application of Complaint Concept Ontologies in Malay language as classifier rules with the BM25 model of Information Retrieval system. Experiments showed the semantic based elements such as Malay ontology may bring the improvement of the classification of the Malay Complaint. The result yielded showed that the proposed classifier produced better result in four category compared to BM25 original score that only produced better result in one category. OBMCS also outperformed the LDA model in all eight categories on the Recall, Precision and F-measure metrics. The finding proven the proposed system is very useful, especially to the Malay complaint in regards of classification for documents in the domain area.},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Rodzman, Shaiful Bakhtiar bin and Suhaili, Siti Suhaima binti and Ismail, Normaly Kamal and Rahman, Nurazzah Abd and Aljunid, Syed Ahmad and Omar, Aslida binti},
	year = {2019},
	note = {event-place: Penang, Malaysia},
	keywords = {bm25 model, Malay complaint, ontology based classification, semantic classification},
	pages = {481--486},
}

@inproceedings{tramontana_ontology_2022,
	address = {New York, NY, USA},
	series = {{ICSIM} '22},
	title = {Ontology {Enrichment} with {Text} {Extracted} from {Wikipedia}},
	isbn = {978-1-4503-9551-9},
	url = {https://doi.org/10.1145/3520084.3520102},
	doi = {10.1145/3520084.3520102},
	abstract = {As biobanks require storing a large amount of data, the use of ontologies offer an effective solution to properly organise data and for data management. However, the specialised jargon embedded into an ontology, especially in the biomedical field, may constitute a difficulty for the people outside the proper domain. Our solution to this is to enhance ontology usability by automatically enriching an ontology. In this article we illustrate an enrichment process that allows us to expand in a controlled way the terms within an ontology. Our proposed enrichment process automatically adds in the ontological structure information extracted from external sources, in order to offer a more complete and clear knowledge of the domain and let users more easily query the ontology. Our enrichment process carefully selects from the wealth of information available in Wikipedia.},
	booktitle = {Proceedings of the 2022 5th {International} {Conference} on {Software} {Engineering} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Tramontana, Emiliano and Verga, Gabriella},
	year = {2022},
	note = {event-place: Yokohama, Japan},
	keywords = {Ontology enrichment, Wikipedia, Additional Key Words and Phrases: Ontologies, Biobanks, OBIB},
	pages = {113--117},
}

@article{gang_corpus_2025,
	title = {Corpus {Fusion} and {Text} {Summarization} {Extraction} for {Multi}-{Feature} {Enhanced} {Entity} {Alignment}},
	volume = {24},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3744558},
	doi = {10.1145/3744558},
	abstract = {Cross-lingual entity alignment endeavors to identify semantically similar entities within a knowledge graph, facilitating knowledge complementarity and enriching cross-lingual knowledge. In the context of knowledge-driven tasks such as cross-lingual question answering and knowledge recommendation, cross-lingual entity alignment can effectively enhancing the performance of these applications built upon cross-lingual knowledge graphs. However, the current methodologies exhibit constraints in efficiently extracting and combining features of multiple entities, rendering them unable to fully harness the wealth of extensive information provided by the knowledge graph. To address this challenge, we propose CFSE, a novel multi-feature enhanced fusion model, which includes deep extraction of complex entity relationship, name, and attribute features. Complex entity relationship features are extracted based on corpus fusion and RotatE model. Additionally, an algorithm based on BERT for multilingual text summarization was introduced to extract entity name and attribute features. Through comprehensive entity feature extraction, CFSE not only further improves the alignment accuracy, but also helps to maximize the depth mining of knowledge graph information. The effectiveness of CFSE in cross-lingual entity alignment applications was demonstrated through experimental results on the DBP15K dataset.},
	number = {9},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Gang, Liu and Wenli, Yang and Tongli, Wang and Zhihao, He},
	month = sep,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {BERT, feature fusion, Cross-lingual entity alignment, multilingual text summarization extraction, rotate},
}

@article{zou_knowledge-enhanced_2024,
	title = {Knowledge-{Enhanced} {Conversational} {Recommendation} via {Transformer}-{Based} {Sequential} {Modeling}},
	volume = {42},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3677376},
	doi = {10.1145/3677376},
	abstract = {In conversational recommender systems (CRSs), conversations usually involve a set of items and item-related entities or attributes, e.g., director is a related entity of a movie. These items and item-related entities are often mentioned along the development of a dialog, leading to potential sequential dependencies among them. However, most of existing CRSs neglect these potential sequential dependencies. In this article, we first propose a Transformer-based sequential conversational recommendation method, named TSCR, to model the sequential dependencies in the conversations to improve CRS. In TSCR, we represent conversations by items and the item-related entities, and construct user sequences to discover user preferences by considering both the mentioned items and item-related entities. Based on the constructed sequences, we deploy a Cloze task to predict the recommended items along a sequence. Meanwhile, in certain domains, knowledge graphs formed by the items and their related entities are readily available, which provide various different kinds of associations among them. Given that TSCR does not benefit from such knowledge graphs, we then propose a knowledge graph enhanced version of TSCR, called TSCRKG. In specific, we leverage the knowledge graph to offline initialize our model TSCRKG, and augment the user sequence of conversations (i.e., sequence of the mentioned items and item-related entities in the conversation) with multi-hop paths in the knowledge graph. Experimental results demonstrate that our TSCR model significantly outperforms state-of-the-art baselines, and the enhanced version TSCRKG further improves recommendation performance on top of TSCR.},
	number = {6},
	journal = {ACM Trans. Inf. Syst.},
	author = {Zou, Jie and Sun, Aixin and Long, Cheng and Kanoulas, Evangelos},
	month = oct,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {transformer, recommender system, Conversational recommendation, sequential recommendation},
}

@inproceedings{liu_retrieval_2022,
	address = {New York, NY, USA},
	series = {{ACAI} '21},
	title = {Retrieval and {Evaluation} of {Target} {Component} {Based} on {Ontology} {Knowledge}},
	isbn = {978-1-4503-8505-3},
	url = {https://doi.org/10.1145/3508546.3508644},
	doi = {10.1145/3508546.3508644},
	abstract = {Software reuse most focuses on component based software development (CBSD). However, it's not so accurate and efficient in the process, to solve this problem, this paper proposes an intelligent knowledge-driven method of target component retrieval and evaluation. This method is based on Ontology component description. With the help of the knowledge graph, a semantic mapping is formed between the component to be queried and the component description library, so the entity component is located. In order to measure the component matching performance, it gives multi-angle evaluation of queried candidate components by an evaluation index system. Based on component query information, it makes the searching target clearer, extends the semantic scope of components to be queried. In order to assembly components, a multi agent system (MAS) is also established. The result shows that this method not only makes the component retrieval process higher recall and precision, but also makes the component retrieval process more intelligent by meeting the assembly requirement.},
	booktitle = {Proceedings of the 2021 4th {International} {Conference} on {Algorithms}, {Computing} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Lijuan and Guo, Chengyu},
	year = {2022},
	note = {event-place: Sanya, China},
	keywords = {Ontology, artificial intelligence, knowledge graph, MAS, software reuse},
}

@inproceedings{ellerhold_enterprise_2023,
	address = {New York, NY, USA},
	series = {{CCSW} '23},
	title = {Enterprise {Cyber} {Threat} {Modeling} and {Simulation} of {Loss} {Events} for {Cyber} {Risk} {Quantification}},
	isbn = {979-8-4007-0259-4},
	url = {https://doi.org/10.1145/3605763.3625244},
	doi = {10.1145/3605763.3625244},
	abstract = {In today's enterprise landscape, effective risk management has emerged as a vital cornerstone. This importance has escalated significantly due to the widespread transition from traditional on-premise infrastructures to dynamic cloud environments. Many organizations rely on qualitative approaches for internal IT and cyber risk management; however, these approaches have notable drawbacks, such as a lack of accuracy and comparability. In this paper, we propose a novel approach to address these limitations by using the Factor Analysis of Information Risk (FAIR) methodology in conjunction with MITRE ATT\&amp;CK to model realistic cyberattacks on organizations and measure quantitative risk. We describe how this approach can be used to create an enterprise cyber threat model, providing a case study for a cloud scenario to demonstrate its usage and to illustrate its potential benefits. Our model has demonstrated its practical applicability in enterprise settings as we thoroughly evaluated its effectiveness within two prominent German companies. This allowed us to gain valuable insight into how our proposed approach can enhance an organization's risk management strategies. Our research demonstrates the value of using a quantitative approach like FAIR over qualitative risk assessment methods. Overall, our approach provides a more comprehensive understanding of the risks organizations are facing and offers guidance on implementing effective risk management strategies. This research can help organizations improve their risk management practices and reduce the potential negative impact of cyberattacks.},
	booktitle = {Proceedings of the 2023 on {Cloud} {Computing} {Security} {Workshop}},
	publisher = {Association for Computing Machinery},
	author = {Ellerhold, Christian and Schnagl, Johann and Schreck, Thomas},
	year = {2023},
	note = {event-place: Copenhagen, Denmark},
	keywords = {ck, cloud computing, cyber risk quantification, enterprise threat model, factor analysis of information risk (fair), mitre att\&amp, quantitative risk assessment, unified kill chain},
	pages = {17--29},
}

@inproceedings{uceda-sosa_domain_2022,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '22},
	title = {Domain specific ontologies from {Linked} {Open} {Data} ({LOD})},
	isbn = {978-1-4503-8582-4},
	url = {https://doi.org/10.1145/3493700.3493703},
	doi = {10.1145/3493700.3493703},
	abstract = {Logical and probabilistic reasoning tasks that require a deeper knowledge of semantics are increasingly relying on general purpose ontologies such as Wikidata and DBpedia. However, tasks such as entity disambiguation and linking may benefit from domain-specific knowledge graphs, which make it more efficient to consume the knowledge and easier to extend with proprietary content. We discuss our experience bootstrapping one such ontology for IT with a domain-agnostic pipeline, and extending it using domain-specific glossaries.},
	booktitle = {Proceedings of the 5th {Joint} {International} {Conference} on {Data} {Science} \&amp; {Management} of {Data} (9th {ACM} {IKDD} {CODS} and 27th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Uceda-Sosa, Rosario and Mihindukulasooriya, Nandana and Kumar, Atul and Bansal, Sahil and Nagar, Seema},
	year = {2022},
	note = {event-place: Bangalore, India},
	keywords = {Ontologies, Knowledge Graphs, IT Operations},
	pages = {105--109},
}

@inproceedings{aguiar_source_2021,
	address = {New York, NY, USA},
	series = {{SBSI} '21},
	title = {Source {Code} {Interoperability} based on {Ontology}},
	isbn = {978-1-4503-8491-9},
	url = {https://doi.org/10.1145/3466933.3466951},
	doi = {10.1145/3466933.3466951},
	abstract = {The different ways of representing a source code in different programming languages create a heterogeneous context. In addition, the use of multiple programming languages in a single source code (polyglot programming) brings a wide choice of terms from different languages, libraries and structures. These facts prevent the direct exchange of information between source codes of different programming languages, requiring specialized knowledge of the programming languages involved. In this article, we present an ontology-based method for source code interoperability that provides an alternative to mitigate heterogeneity problems, aiming to semantically represent the source code written in different programming languages and apply it from different perspectives in a unified way. In this sense, the method is applied in a lab experiment with the objective of validating its methodological aspects, instantiating their respective phases in different subdomains (object orientation and object/relational mapping) and programming languages (Java and Python) in the code smells detection perspective. In addition, the code smell detector produced is evaluated with a set of real-world software projects written in Java and Python.},
	booktitle = {Proceedings of the {XVII} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Aguiar, Camila Zacché de and Zanetti, Félix and Souza, Vitor E. Silva},
	year = {2021},
	note = {event-place: Uberlândia, Brazil},
	keywords = {ontology, applied ontology, code smell detection, interoperabity, source code},
}

@inproceedings{cerin_towards_2021,
	address = {New York, NY, USA},
	series = {{BiDEDE} '21},
	title = {Towards an emulation tool based on ontologies and data life cycles for studying smart buildings},
	isbn = {978-1-4503-8465-0},
	url = {https://doi.org/10.1145/3460866.3461772},
	doi = {10.1145/3460866.3461772},
	abstract = {In this paper, we share our vision to study a complex Information Technology (IT) system handling a massive amount of data in the context of 'smart buildings.' One technique for analyzing complex IT systems relies on emulation, where the final software system is fully deployed on real architectures, and is evaluated in considering "small" instances of situations the system is supposed to solve. We propose a software architecture for studying the ecosystem of 'smart buildings'. This software architecture is built: 1) on top of ontologies for the description of smart buildings; 2) on a special tool for mastering the life cycle of data produced by sensors and actuators inside the buildings.We assume that it is equally important to model both the building's components and the flow of data produced inside the building. We use existing software components for both goals and to make real our concerns. According to a translational methodology, we also discuss use cases for illustrating the potential of our approach and the particular challenges associated with making the two main components of our emulation tool inter-operate.Therefore, our main contribution is to propose a comprehensive, ambitious and realistic research plan to guide communities. The paper illustrates how computer scientists and smart buildings domain scientists may communicate to address and solve specific research problems related to Big Data in emergent distributed environments. We are also guessing that experimental results that can demonstrate the practicality of the proposed combination of tools could be devised in the future, based on our broad vision. The paper is, first and foremost, a visionary paper.},
	booktitle = {Proceedings of the {International} {Workshop} on {Big} {Data} in {Emergent} {Distributed} {Environments}},
	publisher = {Association for Computing Machinery},
	author = {Cérin, Christophe and Andres, Frédéric and Geldwerth-Feniger, Danielle},
	year = {2021},
	note = {event-place: Virtual Event, China},
	keywords = {ontology, big data tools, data life cycle, emulation principles, smart buildings, systems and methods},
}

@inproceedings{fei_formal_2020,
	address = {New York, NY, USA},
	series = {{WSSE} '20},
	title = {Formal {Description} of {Manufacturing} {Process} based on {Domain} {Ontology} {Construction}},
	isbn = {978-1-4503-8787-3},
	url = {https://doi.org/10.1145/3425329.3425377},
	doi = {10.1145/3425329.3425377},
	abstract = {In order to solve the problem of process knowledge sharing, integration and reuse in the field of machinery manufacturing due to the complexity, dispersion and diversity of process knowledge. Taking into account the advantages of ontology in knowledge representation, this paper proposes an ontology-based knowledge management framework in the production line. On the basis of inductively analyzing the attributes of the mechanical manufacturing process attributes and intra-process and interprocess relationships, an improved conceptual ontology expression model of the 4-tuple process is proposed.},
	booktitle = {Proceedings of the 2nd {World} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Fei, Huang and Youling, Chen and Dongsheng, Xu},
	year = {2020},
	note = {event-place: Chengdu, China},
	keywords = {domain ontology, Process knowledge, ontology modeling, semantic analysis},
	pages = {246--251},
}

@inproceedings{atkinson_misconceptions_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Misconceptions about {Potency}-{Based} {Deep} {Instantiation}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688213},
	doi = {10.1145/3652620.3688213},
	abstract = {Multi-level modeling languages differ in their approaches for controlling the properties of model elements over multiple modeling levels. Over the years the original approach for deeply characterizing model elements, the potency-based deep instantiation mechanism, has received a number of criticisms related to its flexibility, level stability, ontological soundness, type safety, and ability to reduce accidental complexity. However, some of these criticisms are founded on misconceptions and thus cannot usefully inform multi-level modeling language designs. In this paper we identify and clarify these misconceptions in order to help guide future considerations of language feature trade-offs and design choices.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Atkinson, Colin and Kühne, Thomas and Lange, Arne},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {ontologies, multi-level modeling, accidental complexity, deep instantiation, potency, type safety},
	pages = {810--817},
}

@inproceedings{yin_statistic_2024,
	address = {New York, NY, USA},
	series = {{PCCNT} '23},
	title = {The {Statistic} {Study} of {Thai} {Middle} {School} {Students}' {Acquisition} in {Chinese} {Negative} {Structures}},
	isbn = {978-1-4503-9995-1},
	url = {https://doi.org/10.1145/3630138.3630488},
	doi = {10.1145/3630138.3630488},
	abstract = {Among the large amounts of Chinese negatives, bu and mei are the most frequently-used with complicated syntactic function and differences on the aspects of semantics and pragmatics becoming the key and hard one in Chinese teaching. However, in Thailand language, the negative ไม่ is a quite simple syntactic structure representing almost all negative usage. This paper based on the former theoretical study takes two grades 72 students as the object of the study, making relative 20 questions in 10 syntactic conditions, conducting the operation of the average on the subjective effects and in 10 syntactic categories. Relative cross-contrast data analysis on the average accuracy between every two types of syntactic conditions included as well. Statistics show that students in Thai mother tongue always make biased errors about bu and mei for lacking enough correspondence in their mother tongue. According to the pairwise comparison inside of the category of simple and complicated syntactic condition, we observe that there contains a difficult level of acquisition about bu and mei from easy to difficult which is fixed collocation \&lt; simple syntactic condition \&lt; volitive and mental verbs \&lt; complicated syntactic condition.},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Power}, {Communication}, {Computing} and {Networking} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Yin, Xuefeng and Luo, Jianfei},
	year = {2024},
	note = {event-place: Wuhan, China},
}

@inproceedings{basse_ontology-based_2021,
	address = {New York, NY, USA},
	series = {{ICETC} '20},
	title = {Ontology-{Based} {Framework} {For} {Automatic} {Generation} {Of} {SQL} {Assessment}},
	isbn = {978-1-4503-8827-6},
	url = {https://doi.org/10.1145/3436756.3437037},
	doi = {10.1145/3436756.3437037},
	abstract = {Assessment measures learner progresses in acquisition of knowledge and skills. It helps teachers to refine their teaching strategies and better deliver knowledge and skills. Likewise, It makes it possible not only to simulate learners’ thinking but to measure the value or quality of their work. Creating effective assessment of learning is a difficult task for teachers. This article presents an automatic question generating system for individual assessment of learners’ practical knowledge.},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Education} {Technology} and {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Basse, Adrien and Diatta, Baboucar and Deme, Cherif Bachir and Ndiaye, Ndeye Massata},
	year = {2021},
	note = {event-place: London, United Kingdom},
	keywords = {ontology, assessment, question generation, SQL language},
	pages = {152--156},
}

@inproceedings{saada_modeling_2024,
	address = {New York, NY, USA},
	series = {{NISS} '24},
	title = {Modeling and {Conducting} {Security} {Risk} {Assessment} of {Smart} {Airport} {Infrastructures} with {SecRAM}},
	isbn = {979-8-4007-0929-6},
	url = {https://doi.org/10.1145/3659677.3659992},
	doi = {10.1145/3659677.3659992},
	abstract = {Despite the COVID-19 pandemic caused flights and routes cut back inverting an otherwise astonishing growth trend, air passenger traffic is increasing every year. As a result, airports are going through a continuous digital transformation enhancing their infrastructure to keep up their growth and to offer passengers an improved and seamless experience. On the other hand, a far-reaching digital infrastructure poses new cyber-security challenges. In response to such a novel and connected ecosystem adopted by airports, this paper presents a security risk assessment to identify and mitigate the consequences of attacks threatening the digital passenger process of smart airports. Also, we propose an extension of SecRAM, a risk assessment methodology proposed by the SESAR Joint Undertaking for the Air Traffic Management (ATM), to enable a continuous assessment as the threat landscape evolves.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Networking}, {Intelligent} {Systems} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Saada, Hajer and Orizio, Riccardo and Sebastio, Stefano},
	year = {2024},
	note = {event-place: Meknes, AA, Morocco},
	keywords = {Model-Based System Engineering, SecRAM, Security Risk Assessment, Smart Airports},
}

@article{pfannemuller_react-ion_2021,
	title = {{REACT}-{ION}: {A} {Model}-based {Runtime} {Environment} for {Situation}-aware {Adaptations}},
	volume = {15},
	issn = {1556-4665},
	url = {https://doi.org/10.1145/3487919},
	doi = {10.1145/3487919},
	abstract = {Trends such as the Internet of Things lead to a growing number of networked devices and to a variety of communication systems. Adding self-adaptive capabilities to these communication systems is one approach to reducing administrative effort and coping with changing execution contexts. Existing frameworks can help reducing development effort but are neither tailored toward the use in communication systems nor easily usable without knowledge in self-adaptive systems development. Accordingly, in previous work, we proposed REACT, a reusable, model-based runtime environment to complement communication systems with adaptive behavior. REACT addresses heterogeneity and distribution aspects of such systems and reduces development effort. In this article, we propose REACT-ION—an extension of REACT for situation awareness. REACT-ION offers a context management module that is able to acquire, store, disseminate, and reason on context data. The context management module is the basis for (i) proactive adaptation with REACT-ION and (ii) self-improvement of the underlying feedback loop. REACT-ION can be used to optimize adaptation decisions at runtime based on the current situation. Therefore, it can cope with uncertainty and situations that were not foreseeable at design time. We show and evaluate in two case studies how REACT-ION’s situation awareness enables proactive adaptation and self-improvement.},
	number = {4},
	journal = {ACM Trans. Auton. Adapt. Syst.},
	author = {Pfannemüller, Martin and Breitbach, Martin and Weckesser, Markus and Becker, Christian and Schmerl, Bradley and Schürr, Andy and Krupitzer, Christian},
	month = dec,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {framework, model-based, runtime environment, Self-adaptive systems, situation awareness},
}

@inproceedings{avgerinos_loutsaris_legal_2021,
	address = {New York, NY, USA},
	series = {dg.o '21},
	title = {Legal {Text} {Processing}: {Combing} two legal ontological approaches through text mining},
	isbn = {978-1-4503-8492-6},
	url = {https://doi.org/10.1145/3463677.3463730},
	doi = {10.1145/3463677.3463730},
	abstract = {The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the “legal informatics” research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.},
	booktitle = {Proceedings of the 22nd {Annual} {International} {Conference} on {Digital} {Government} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Avgerinos Loutsaris, Michalis and Lachana, Zoi and Alexopoulos, Charalampos and Charalabidis, Yannis},
	year = {2021},
	note = {event-place: Omaha, NE, USA},
	keywords = {big open legal data, legal information systems, legal ontologies},
	pages = {522--532},
}

@inproceedings{fang_research_2025,
	address = {New York, NY, USA},
	series = {{ICBAR} '24},
	title = {Research on the influencing factors of creation model of knowledge graphs for design under the perspective of graph workflow},
	isbn = {979-8-4007-0975-3},
	url = {https://doi.org/10.1145/3718751.3718912},
	doi = {10.1145/3718751.3718912},
	abstract = {Based on the analysis of SAPAD-AHP model, this study explores the influencing factors of graph structure in the construction of knowledge graph creation platform, and through the construction of the collaborative platform, the study is carried out in teaching and research practice. With the development of the interdisciplinary system and the establishment of many emerging disciplines, design as the intersection of science, engineering and liberal arts combined, its information grooming and knowledge production mode has also undergone an important transformation, and the processing capacity in the face of a variety of information intersection has greatly stimulated the demand for the construction of personal knowledge bases and knowledge graphs. The current solutions relying on tree structure and linear structure have considerable limitations. In this paper, we analyze the mapping of "Behavior-Product-Meaning" of the existing solutions through SAPAD method, and obtain the user's needs through cluster analysis, and filter the multi-layer core meaning clusters by calculating the weight of the general meaning clusters through the AHP method, so as to obtain the user's core needs. Based on the results of the requirements study, the study proposes a design scheme for personal knowledge graph construction based on graph structure, which for the first time connects graph structure and text content in tandem at the interaction aspect, integrates and simplifies the workflow, and examines its information combing and image editing capabilities in teaching and research practice, providing a valuable reference for the design of knowledge graph solutions for the design category.},
	booktitle = {Proceedings of the 2024 4th {International} {Conference} on {Big} {Data}, {Artificial} {Intelligence} and {Risk} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Fang, Yuxuan and Hu, Xiaochen and Chen, Shangyin},
	year = {2025},
	keywords = {knowledge graph, collaborative platform, graph workflow},
	pages = {971--984},
}

@inproceedings{druselmann_dataset_2025,
	address = {New York, NY, USA},
	series = {{NLPIR} '24},
	title = {A {Dataset} of {Semantically} {Related} {Multiword} {Terms} of the {Electrical} {Engineering} {Domain}},
	isbn = {979-8-4007-1738-3},
	url = {https://doi.org/10.1145/3711542.3711550},
	doi = {10.1145/3711542.3711550},
	abstract = {This paper presents the EEMWT dataset, a collection of 1873 triplets of co-hyponymic multiword terms of the electrical engineering domain. Each triplet combines an anchor term with closely and distantly related co-hyponymic terms. The degree of semantic relatedness is determined by the presence or absence of shared domain-specific semantic features. The primary purpose of this dataset is to serve as a tool for evaluating domain-specific language representation models and embedding vector pooling techniques by assessing the semantic relatedness of co-hyponymic multiword terms. The novelty of the dataset lies in the development approach, which replaces intuition-based scaling of relatedness with linguistically justified semantic-feature-based judgment. Furthermore, the traditional method of calculating interrater agreement rate is replaced with a statistical analysis of vector space distance between low and highly related terms. These innovations in dataset construction and evaluation have the potential to significantly reduce the costs associated with expert questionnaires in dataset development.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Druselmann, Maria and Harbusch, Karin},
	year = {2025},
	keywords = {semantic relatedness, dataset of semantically related terms, domain-specific language representation model, multiword term},
	pages = {258--264},
}

@inproceedings{van_den_berg_agent_2020,
	address = {Richland, SC},
	series = {{AAMAS} '20},
	title = {Agent {Ontology} {Alignment} {Repair} through {Dynamic} {Epistemic} {Logic}},
	isbn = {978-1-4503-7518-4},
	abstract = {Ontology alignments enable agents to communicate while preserving heterogeneity in their information. Alignments may not be provided as input and should be able to evolve when communication fails or when new information contradicting the alignment is acquired. In the Alignment Repair Game (ARG) this evolution is achieved via adaptation operators. ARG was evaluated experimentally and the experiments showed that agents converge towards successful communication and improve their alignments. However, whether the adaptation operators are formally correct, complete or redundant is still an open question. In this paper, we introduce a formal framework based on Dynamic Epistemic Logic that allows us to answer this question. This framework allows us (1) to express the ontologies and alignments used, (2) to model the ARG adaptation operators through announcements and conservative upgrades and (3) to formally establish the correctness, partial redundancy and incompleteness of the adaptation operators in ARG.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {van den Berg, Line and Atencia, Manuel and Euzenat, Jèrome},
	year = {2020},
	note = {event-place: Auckland, New Zealand},
	keywords = {ontology alignment, agent communication, alignment repair, dynamic epistemic logic},
	pages = {1422--1430},
}

@article{doan_technical_2024,
	title = {Technical {Perspective}: {Unicorn}: {A} {Unified} {Multi}-{Tasking} {Matching} {Model}},
	volume = {53},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/3665252.3665262},
	doi = {10.1145/3665252.3665262},
	abstract = {Data integration has been a long-standing challenge for data management. It has recently received significant attention due to at least three main reasons. First, many data science projects require integrating data from disparate sources before analysis can be carried out to extract insights. Second, many organizations want to build knowledge graphs, such as Customer 360s, Product 360s, and Supplier 360s, which capture all available information about the customers, products, and suppliers of an organization. Building such knowledge graphs often requires integrating data from multiple sources. Finally, there is also an increasing need to integrate a massive amount of data to create training data for AI models, such as large language models.},
	number = {1},
	journal = {SIGMOD Rec.},
	author = {Doan, AnHai},
	month = may,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {43},
}

@inproceedings{dimanidis_natural_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {A {Natural} {Language} {Driven} {Approach} for {Automated} {Web} {API} {Development}: {Gherkin2OAS}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3191654},
	doi = {10.1145/3184558.3191654},
	abstract = {Speeding up the development process of Web Services, while adhering to high quality software standards is a typical requirement in the software industry. This is why industry specialists usually suggest "driven by" development approaches to tackle this problem. In this paper, we propose such a methodology that employs Specification Driven Development and Behavior Driven Development in order to facilitate the phases of Web Service requirements elicitation and specification. Furthermore, we introduce gherkin2OAS, a software tool that aspires to bridge the aforementioned development approaches. Through the suggested methodology and tool, one may design and build RESTful services fast, while ensuring proper functionality.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Dimanidis, Anastasios and Chatzidimitriou, Kyriakos C. and Symeonidis, Andreas L.},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {restful API, behavior driven development, gherkin, open API specification},
	pages = {1869--1874},
}

@inproceedings{dalcin_recommendation_2023,
	address = {New York, NY, USA},
	series = {{SBES} '23},
	title = {Recommendation of {UML} {Model} {Conflicts}: {Unveiling} the {Biometric} {Lens} for {Conflict} {Resolution}},
	isbn = {979-8-4007-0787-2},
	url = {https://doi.org/10.1145/3613372.3613378},
	doi = {10.1145/3613372.3613378},
	abstract = {Model merging assumes a pivotal role in numerous model-centric software development tasks, e.g., evolving UML models to add new features or even reconciling UML models developed collaboratively by distributed development teams. Usually, UML model elements to-be-merged conflict with each other. Unfortunately, resolving conflicts remains a highly cognitive and error-prone task. Today, wearable devices capable of capturing biometric data are a reality. Recent studies indicate that the developer’s cognitive indicators may affect developers while performing development tasks. However, the current literature has neglected the recommendation of conflicts sensitive to the cognitive activities of software developers. This study, therefore, introduces BACR, a biometric-aware approach to recommend UML model conflicts using machine learning. BACR helps UML model merging to push a step forward, recommending model conflicts based on appropriate biometric indicators and using a behavior sequence transformer model. Our approach is based on four scientific institutions. It represents the first effort in supporting the prioritization of cognitively relevant UML model conflicts by developers, mitigating the risk of making incorrect decisions and preventing potential downstream issues.},
	booktitle = {Proceedings of the {XXXVII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Dalcin, Guilherme and Bolzan, Willian and Lazzari, Luan and Farias, Kleinner},
	year = {2023},
	note = {event-place: Campo Grande, Brazil},
	keywords = {Biometrics, Cognitive Load, Model Merging, Software Modeling},
	pages = {83--88},
}

@inproceedings{tian_aiai_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {aiai at the {FinSim}-2 task: {Finance} {Domain} {Terms} {Automatic} {Classification} {Via} {Word} {Ontology} and {Embedding}},
	isbn = {978-1-4503-8313-4},
	url = {https://doi.org/10.1145/3442442.3451388},
	doi = {10.1145/3442442.3451388},
	abstract = {This paper describes the method that we submitted to the FinSim-2 task on learning similarities for the financial domain. This task aims to automatically classify the Financial domain terms into the most relevant hypernym (or top-level) concept in an external ontology. This paper shows the result of experiments using the Catboost, Attention-LSTM, BERT, RoBERTa to develop an automatic finance domain classifier via word ontology and embedding. The experiment result demonstrates that each model could be an effective method to tackle the FinSim-2 task, respectively.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Tian, Ke and Chen, Hua},
	year = {2021},
	note = {event-place: Ljubljana, Slovenia},
	keywords = {Ontology, BERT, LSTM, Word2vec, Attention, RoBERTa, Catboost, FinSim-2 task},
	pages = {320--322},
}

@article{furst_versamatch_2023,
	title = {{VersaMatch}: {Ontology} {Matching} with {Weak} {Supervision}},
	volume = {16},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3583140.3583148},
	doi = {10.14778/3583140.3583148},
	abstract = {Ontology matching is crucial to data integration for across-silo data sharing and has been mainly addressed with heuristic and machine learning (ML) methods. While heuristic methods are often inflexible and hard to extend to new domains, ML methods rely on substantial and hard to obtain amounts of labeled training data. To overcome these limitations, we propose VersaMatch, a flexible, weakly-supervised ontology matching system. VersaMatch employs various weak supervision sources, such as heuristic rules, pattern matching, and external knowledge bases, to produce labels from a large amount of unlabeled data for training a discriminative ML model. For prediction, VersaMatch develops a novel ensemble model combining the weak supervision sources with the discriminative model to support generalization while retaining a high precision. Our ensemble method boosts end model performance by 4 points compared to a traditional weak-supervision baseline. In addition, compared to state-of-the-art ontology matchers, VersaMatch achieves an overall 4-point performance improvement in F1 score across 26 ontology combinations from different domains. For recently released, in-the-wild datasets, VersaMatch beats the next best matchers by 9 points in F1. Furthermore, its core weak-supervision logic can easily be improved by adding more knowledge sources and collecting more unlabeled data for training.},
	number = {6},
	journal = {Proc. VLDB Endow.},
	author = {Fürst, Jonathan and Argerich, Mauricio Fadel and Cheng, Bin},
	month = feb,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {1305--1318},
}

@inproceedings{sivertsen_exploring_2024,
	address = {New York, NY, USA},
	series = {{DIS} '24},
	title = {Exploring {Aesthetic} {Qualities} of {Deep} {Generative} {Models} through {Technological} ({Art}) {Mediation}},
	isbn = {979-8-4007-0583-0},
	url = {https://doi.org/10.1145/3643834.3661498},
	doi = {10.1145/3643834.3661498},
	abstract = {Deep Generative Models (DGM) have had a great impact both on visual art and broader visual culture. In this research-through-design project we investigate the use of a DGM for helping museum visitors explore the aesthetics of Edvard Munch’s art. We designed and built an interactive drawing table that allows a user to explore a StyleGAN model trained on sketches by Edvard Munch. The paper makes two novel contributions: 1. It presents a system that allows users to interact with a DGM by drawing on paper (rather than the typical text prompts used by most current systems). 2. We demonstrate how this mode and quality of interaction establish a unique perspective on Munch’s drawings as a practice. Through qualitative evaluation, we discuss how this setup led users towards a specific hermeneutic drawing strategy that enables building competency with the model and by proxy the data it is trained on. We suggest that the resulting interaction may contribute to an "education of attention" helping museum visitors to become attentive to certain visual qualities in Munch’s drawing practice. Finally, we discuss how the concepts of technological mediation and relationality are useful for designing how the output of a DGM is understood by its users.},
	booktitle = {Proceedings of the 2024 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Sivertsen, Christian and Løvlie, Anders Sundnes},
	year = {2024},
	note = {event-place: Copenhagen, Denmark},
	keywords = {machine learning, aesthetics, deep generative model, drawing, fine art, interaction design, postphenomenology, stylegan},
	pages = {2738--2752},
}

@inproceedings{rismani_what_2023,
	address = {New York, NY, USA},
	series = {{AIES} '23},
	title = {What does it mean to be a responsible {AI} practitioner: {An} ontology of roles and skills},
	isbn = {979-8-4007-0231-0},
	url = {https://doi.org/10.1145/3600211.3604702},
	doi = {10.1145/3600211.3604702},
	abstract = {With the growing need to regulate AI systems across a wide variety of application domains, a new set of occupations has emerged in the industry. The so-called responsible Artificial Intelligence (AI) practitioners or AI ethicists are generally tasked with interpreting and operationalizing best practices for ethical and safe design of AI systems. Due to the nascent nature of these roles, however, it is unclear to future employers and aspiring AI ethicists what specific function these roles serve and what skills are necessary to serve the functions. Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives. In this work, we examine what responsible AI practitioners do in the industry and what skills they employ on the job. We propose an ontology of existing roles alongside skills and competencies that serve each role. We created this ontology by examining the job postings for such roles over a two-year period (2020-2022) and conducting expert interviews with fourteen individuals who currently hold such a role in the industry. Our ontology contributes to business leaders looking to build responsible AI teams and provides educators with a set of competencies that an AI ethics curriculum can prioritize.},
	booktitle = {Proceedings of the 2023 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Rismani, Shalaleh and Moon, AJung},
	year = {2023},
	note = {event-place: Montréal, QC, Canada},
	keywords = {Education, Competency Framework, Responsible AI Practitioner},
	pages = {584--595},
}

@inproceedings{hviid_opm_2022,
	address = {New York, NY, USA},
	series = {{IoT} '21},
	title = {{OPM}: {An} {Ontology}-{Based} {Package} {Manager} for {Building} {Operating} {Systems}},
	isbn = {978-1-4503-8566-4},
	url = {https://doi.org/10.1145/3494322.3494338},
	doi = {10.1145/3494322.3494338},
	abstract = {The energy sector is experiencing new challenges with the move to green energy. One of these challenges is keeping a stable energy grid when transitioning the production to unpredictable energy generation from green sources. Demand Response (DR) can mitigate some of the lack of predictability by influencing the consumer’s load profile. Unfortunately, the cost of implementing DR, and the required infrastructure, vastly overshadows the benefits for the consumer, thereby negating the incentive to invest. Therefore, reducing the initial cost of investment is a critical factor for the success of DR. Building Operating Systems (BOS) is one possible avenue to achieve DR functionality in buildings. This paper seeks to reduce initial investment costs of BOSes, by introducing an ontology-based package manager (OPM), that dynamically resolves dependencies and installs services. An ontology-based approach to dependency resolution allows for loosely defined dependencies but also takes the context of the service into account, as well as requirements in terms of sensor availability and physical layout of the building. The OPM is evaluated by deploying a BOS and accompanying services for occupancy prediction. By significantly reducing deployment complexity, results show considerable time savings, and thereby cost reductions, on deployment and maintenance activities.},
	booktitle = {Proceedings of the 11th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Hviid, Jakob and Johansen, Aslak and Caleb Sangogboye, Fisayo and Kjærgaard, Mikkel Baun},
	year = {2022},
	note = {event-place: St.Gallen, Switzerland},
	keywords = {ontology, OWL, building operating systems, containerization, dependency resolution, deployment, Package manager},
	pages = {118--125},
}

@inproceedings{wang_selecting_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Selecting {Top}-k {Data} {Science} {Models} by {Example} {Dataset}},
	isbn = {979-8-4007-0124-5},
	url = {https://doi.org/10.1145/3583780.3615051},
	doi = {10.1145/3583780.3615051},
	abstract = {Data analytical pipelines routinely involve various domain-specific data science models. Such models require expensive manual or training effort and often incur expensive validation costs (e.g., via scientific simulation analysis). Meanwhile, high-value models remain to be ad-hocly created, isolated, and underutilized for a broad community. Searching and accessing proper models for data analysis pipelines is desirable yet challenging for users without domain knowledge. This paper introduces ModsNet, a novel MODel SelectioN framework that only requires an Example daTaset. (1) We investigate the following problem: Given a library of pre-trained models, a limited amount of historical observations of their performance, and an "example" dataset as a query, return k models that are expected to perform the best over the query dataset. (2) We formulate a regression problem and introduce a knowledge-enhanced framework using a model-data interaction graph. Unlike traditional methods, (1) ModsNet uses a dynamic, cost-bounded "probe-and-select" strategy to incrementally identify promising pre-trained models in a strict cold-start scenario (when a new dataset without any interaction with existing models is given). (2) To reduce the learning cost, we develop a clustering-based sparsification strategy to prune unpromising models and their interactions. (3) We showcase of ModsNet built on top of a crowdsourced materials knowledge base platform. Our experiments verified its effectiveness, efficiency, and applications over real-world analytical pipelines.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Mengying and Guan, Sheng and Ma, Hanchao and Bian, Yiyang and Che, Haolai and Daundkar, Abhishek and Sehirlioglu, Alp and Wu, Yinghui},
	year = {2023},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {knowledge graph, GNN-based recommendation, model selection},
	pages = {2686--2695},
}

@inproceedings{lin_cognitive_2024,
	address = {New York, NY, USA},
	series = {{CNIOT} '24},
	title = {Cognitive {Intelligence}: {Driven} by {Knowledge} {Graph} and {Big} {Model} {Collaboration}},
	isbn = {979-8-4007-1675-1},
	url = {https://doi.org/10.1145/3670105.3670139},
	doi = {10.1145/3670105.3670139},
	abstract = {Abstract: Cognitive intelligence is primarily characterized by the understanding, reasoning, cognition, and decision-making of complex things. It is a higher-order form of artificial intelligence development. This paper provides an in-depth analysis of two representative technologies, knowledge graph and big model, which promote the development of cognitive intelligence. Firstly, we systematically sorts out the characteristics, advantages, and shortcomings of these two technologies. Secondly, we proposes technical approaches and main methods for the mutual enhancement of knowledge graph and big model. Finally, we provides the main direction for the integrated development of knowledge graph and big model to promote the development of cognitive intelligence. We hope our work can provide reference and inspiration for relevant engineers and technical researchers.CCS Concepts: .Computing methodologies → Artificial intelligence; Knowledge representation and reasoning},
	booktitle = {Proceedings of the 2024 5th {International} {Conference} on {Computing}, {Networks} and {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Wangqun and Xu, Jing and Tian, Yu and Peng, Baoyun and Li, Yan and Ge, Yawei},
	year = {2024},
	note = {event-place: Tokyo, Japan},
	keywords = {artificial intelligence, knowledge graph, big model, cognitive intelligence},
	pages = {204--209},
}

@inproceedings{wang_micro-video_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {Micro-video {Tagging} via {Jointly} {Modeling} {Social} {Influence} and {Tag} {Relation}},
	isbn = {978-1-4503-9203-7},
	url = {https://doi.org/10.1145/3503161.3548098},
	doi = {10.1145/3503161.3548098},
	abstract = {The last decade has witnessed the proliferation of micro-videos on various user-generated content platforms. According to our statistics, around 85.7\% of micro-videos lack annotation. In this paper, we focus on annotating micro-videos with tags. Existing methods mostly focus on analyzing video content, neglecting users' social influence and tag relation. Meanwhile, existing tag relation construction methods suffer from either deficient performance or low tag coverage. To jointly model social influence and tag relation, we formulate micro-video tagging as a link prediction problem in a constructed heterogeneous network. Specifically, the tag relation (represented by tag ontology) is constructed in a semi-supervised manner. Then, we combine tag relation, video-tag annotation, and user follow relation to build the network. Afterward, a better video and tag representation are derived through Behavior Spread modeling and visual and linguistic knowledge aggregation. Finally, the semantic similarity between each micro-video and all candidate tags is calculated in this video-tag network. Extensive experiments on industrial datasets of three verticals verify the superiority of our model compared with several state-of-the-art baselines.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Xiao and Gan, Tian and Wei, Yinwei and Wu, Jianlong and Meng, Dai and Nie, Liqiang},
	year = {2022},
	note = {event-place: Lisboa, Portugal},
	keywords = {ontology construction, behavior spread, micro-video tagging},
	pages = {4478--4486},
}

@inproceedings{geng_ontozsl_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {{OntoZSL}: {Ontology}-enhanced {Zero}-shot {Learning}},
	isbn = {978-1-4503-8312-7},
	url = {https://doi.org/10.1145/3442381.3450042},
	doi = {10.1145/3442381.3450042},
	abstract = {Zero-shot Learning (ZSL), which aims to predict for those classes that have never appeared in the training data, has arisen hot research interests. The key of implementing ZSL is to leverage the prior knowledge of classes which builds the semantic relationship between classes and enables the transfer of the learned models (e.g., features) from training classes (i.e., seen classes) to unseen classes. However, the priors adopted by the existing methods are relatively limited with incomplete semantics. In this paper, we explore richer and more competitive prior knowledge to model the inter-class relationship for ZSL via ontology-based knowledge representation and semantic embedding. Meanwhile, to address the data imbalance between seen classes and unseen classes, we developed a generative ZSL framework with Generative Adversarial Networks (GANs). Our main findings include: (i) an ontology-enhanced ZSL framework that can be applied to different domains, such as image classification (IMGC) and knowledge graph completion (KGC); (ii) a comprehensive evaluation with multiple zero-shot datasets from different domains, where our method often achieves better performance than the state-of-the-art models. In particular, on four representative ZSL baselines of IMGC, the ontology-based class semantics outperform the previous priors e.g., the word embeddings of classes by an average of 12.4 accuracy points in the standard ZSL across two example datasets (see Figure\&nbsp;4).},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Geng, Yuxia and Chen, Jiaoyan and Chen, Zhuo and Pan, Jeff Z. and Ye, Zhiquan and Yuan, Zonggang and Jia, Yantao and Chen, Huajun},
	year = {2021},
	note = {event-place: Ljubljana, Slovenia},
	keywords = {Ontology, Generative Adversarial Networks, Knowledge Graph Completion, Image Classification, Zero-shot Learning},
	pages = {3325--3336},
}

@inproceedings{mohsen_scaled_2020,
	address = {New York, NY, USA},
	series = {{ICFET} '20},
	title = {Scaled {Scrum} {Framework} for {Cooperative} {Domain} {Ontology} {Evolution}},
	isbn = {978-1-4503-7533-7},
	url = {https://doi.org/10.1145/3404709.3404770},
	doi = {10.1145/3404709.3404770},
	abstract = {The field of research in ontology engineering appears to be mature, considering the vast number of contemporary methods and instruments for the formalization and application of knowledge representation models. However, the evolutionary aspects of ontologies are still little understood and supported. This is especially important in distributed and collaborative settings like the Semantic web, where ontologies naturally co-operate with their user communities. Various organizations and teams are building common ground in this context. Ontology is instrumental in this process through the formal description of shared knowledge. Such semanticity constitutes a sound basis for defining, sharing (business) objectives and interests and eventually developing useful collaborative services and systems. In this "complex" and dynamic environment, a collaborative model for process change requires more powerful methodologies for engineering, argumentation and negotiation. Software Engineering provides teamwork, team management, feedback management, versioning, merging, and evolving software artifacts with a wealth of techniques and tools. Many of these techniques can be used again in an ontology engineering environment. This paper examines how this problem can be resolved using Scrum and Nexus frameworks, which are among the most robust models for software development.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Frontiers} of {Educational} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
	year = {2020},
	note = {event-place: Tokyo, Japan},
	keywords = {Scrum, Ontology Evolution, Collaborative Evolution, Inter-organizational Ontology, Nexus},
	pages = {135--143},
}

@article{joy_ontology-based_2021,
	title = {Ontology-based {E}-learning {Content} {Recommender} {System} for {Addressing} the {Pure} {Cold}-start {Problem}},
	volume = {13},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3429251},
	doi = {10.1145/3429251},
	abstract = {E-learning recommender systems are gaining significance nowadays due to its ability to enhance the learning experience by providing tailor-made services based on learner preferences. A Personalized Learning Environment (PLE) that automatically adapts to learner characteristics such as learning styles and knowledge level can recommend appropriate learning resources that would favor the learning process and improve learning outcomes. The pure cold-start problem is a relevant issue in PLEs, which arises due to the lack of prior information about the new learner in the PLE to create appropriate recommendations. This article introduces a semantic framework based on ontology to address the pure cold-start problem in content recommenders. The ontology encapsulates the domain knowledge about the learners as well as Learning Objects (LOs). The semantic model that we built has been experimented with different combinations of the key learner parameters such as learning style, knowledge level, and background knowledge. The proposed framework utilizes these parameters to build natural learner groups from the learner ontology using SPARQL queries. The ontology holds 480 learners’ data, 468 annotated learning objects with 5,600 learner ratings. A multivariate k-means clustering algorithm, an unsupervised machine learning technique for grouping similar data, is used to evaluate the learner similarity computation accuracy. The learner satisfaction achieved with the proposed model is measured based on the ratings given by the 40 participants of the experiments. From the evaluation perspective, it is evident that 79\% of the learners are satisfied with the recommendations generated by the proposed model in pure cold-start condition.},
	number = {3},
	journal = {J. Data and Information Quality},
	author = {Joy, Jeevamol and Raj, Nisha S. and V. G., Renumol},
	month = apr,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, learning object, personalized learning environment, content recommenders, Learner profile, multivariate clustering, pure cold-start problem},
}

@inproceedings{wu_literature_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {From {Literature} to {Lab}: {Hardware}-{Independent} {Autonomous} {Chemical} {Synthesis} with {Reinforcement} {Learning}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715178},
	doi = {10.1145/3701716.3715178},
	abstract = {Chemical synthesis, a fundamental process in chemical engineering, traditionally requires extensive manual intervention and expertise, particularly in interpreting scientific literature and translating it into executable workflows. Autonomous systems offer the potential to revolutionize this field by enabling robots to autonomously read scientific literature and form general synthesis workflows. However, a key challenge lies in integrating diverse hardware components, requiring adaptable architectures that can seamlessly operate across various platforms. Furthermore, these systems must be robust, allowing for real-time error correction, and accessible to non-programmers. This paper presents Autonomous Chemical Synthesis System (ACSS), an adaptable architecture for chemical execution systems designed to address these challenges. ACSS enables robots to autonomously read scientific literature and form general synthesis workflows. We achieve hardware independence through protocol-integration reinforcement learning, enabling seamless integration across platforms. Chemical code and hardware descriptions are compiled and converted into robotic instructions for execution. Natural language error correction allows non-programmers to easily adjust the system. Our key contributions are: (1) Literature to Lab: directly extracting synthesis protocols from scientific articles; (2) Hardware Independence: developing an adaptive, reinforcement learning-based model for diverse hardware setups. ACSS has successfully synthesized 12 diverse compounds from literature, including lidocaine (a painkiller), Dess-Martin periodinane (an oxidizing agent), and alkyl fluoride (a fluorinating agent). This demonstration highlights the potential of NLP and reinforcement learning for efficient and accessible chemical research and production. Click here to find out more on YouTube.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Wu, Junfeng and He, Jing and Liu, Hai and Zheng, Zhaoqi and Cao, Yichen and Chen, Xingguo and Zou, Bingjie and Zou, Ruiping and Zhou, Guohua and Sturgess, David and van Zundert, André},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language model, natural language processing, reinforcement learning, robot chemist, task dependency graph},
	pages = {2923--2926},
}

@inproceedings{mezhuyev_expert_2024,
	address = {New York, NY, USA},
	series = {{ICCTA} '24},
	title = {Expert {System} for {Bainite} {Design}: the {Approach} to {Enrich} {Physical} {Models} with {Information} {Derived} from {Knowledge} {Models}},
	isbn = {979-8-4007-1638-6},
	url = {https://doi.org/10.1145/3674558.3674597},
	doi = {10.1145/3674558.3674597},
	abstract = {The development of a physical model begins with a knowledge model, initially existing as ideas in the mind of a researcher. A transition from knowledge models to strict mathematical formalisms is a challenging process, and may not always be feasible, particularly in the early stages of research. Another problem comes when many experts are participating in the development of new physical knowledge, which may result in inconsistency. To contribute to this domain, the paper presents the development of an expert system (ES), created to capture expert knowledge for the design of a new physical material, namely, the bainite steel. The ES combines physical properties and rules in a unique knowledge model and enriches them by derived from data probabilities. The proposed approach enables users to validate expert knowledge and find contradictions in the logical rules, giving the possibility of mapping them back to physical models.},
	booktitle = {Proceedings of the 2024 10th {International} {Conference} on {Computer} {Technology} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Mezhuyev, Vitaliy and Hofmann, Paul},
	year = {2024},
	note = {event-place: Vienna, Austria},
	keywords = {Expert system, Bainite steel, Material design, Physical modelling, Probabilistic programming},
	pages = {270--275},
}

@inproceedings{burattini_towards_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Towards {Linked} {Data} for {Ecosystems} of {Digital} {Twins}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688245},
	doi = {10.1145/3652620.3688245},
	abstract = {Due to either the inherent complexity of the domain or the evolving nature of systems, we can envision solutions that digitalize assets in a complex domain using an ecosystem of distributed Digital Twins instead of a single monolithic one. To effectively tackle interoperability in such ecosystems, this paper advocates for the introduction of a representation based on Semantic Web technologies enabling the discovery of both Digital Twin structure - i.e. the static information about the asset model and offered services - and state - i.e. the data and metrics collected at runtime - to support the management of ecosystems and the creation of application mashups. A review of the state of the art suggests that currently investigated ways to describe a Digital Twin are not sufficient to achieve this objective. A proposal of key requirements for a Digital Twin representation is outlined leading to the proposal of a core ontology and a Linked Data approach for state management.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Burattini, Samuele and Zimmermann, Antoine and Picone, Marco and Ricci, Alessandro},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {ontologies, semantic web, interoperability, digital twins},
	pages = {332--337},
}

@inproceedings{el_ghosh_application_2019,
	address = {New York, NY, USA},
	series = {{ICAIL} '19},
	title = {The {Application} of {ODCM} for {Building} {Well}-{Founded} {Legal} {Domain} {Ontologies}: {A} {Case} {Study} in the {Domain} of {Carriage} of {Goods} by {Sea}},
	isbn = {978-1-4503-6754-7},
	url = {https://doi.org/10.1145/3322640.3326725},
	doi = {10.1145/3322640.3326725},
	abstract = {The ontology engineering community is facing several key challenges about the development of domain ontologies. One major challenge is the building of well-founded domain ontologies. This concept has raised recently and it refers to ontologies that are grounded in validated foundational ontologies. This paper addresses the effective contribution of ontology-driven conceptual modeling process (ODCM) for developing such ontologies in the legal domain. A case study in the domain of carriage of goods by sea is presented.},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {El Ghosh, Mirna and Abdulrab, Habib},
	year = {2019},
	note = {event-place: Montreal, QC, Canada},
	keywords = {OntoUML, Ontology-Driven Conceptual Modeling, UFO, legal ontologies, well-founded ontologies},
	pages = {204--208},
}

@inproceedings{balloccu_greenfoodlens_2025,
	address = {New York, NY, USA},
	series = {{RecSys} '25},
	title = {{GreenFoodLens}: {Sustainability} {Labels} for {Food} {Recommendation}},
	isbn = {979-8-4007-1364-4},
	url = {https://doi.org/10.1145/3705328.3748165},
	doi = {10.1145/3705328.3748165},
	abstract = {Most food recommender systems aim to boost user engagement by analyzing recipe ingredients and users’ past choices. Even though consumers are paying more attention to sustainability, such as carbon and water footprints, there remains a notable lack of public corpora that combine detailed user–recipe interactions with reliable environmental impact data. This gap makes it hard to build recommendation tools that both match people’s tastes and help reduce ecological damage. To this end, we present GreenFoodLens, a resource that enriches HUMMUS, one of the largest corpora for food recommendation, with environmental impact estimates derived from the hierarchical taxonomy of the SU-EATABLE-LIFE project. We achieved this result through a multi-step process involving human annotations, iterative labeling assessments, knowledge refinement, and constrained generation techniques with large language models. Finally, we evaluate recommendation baselines on HUMMUS augmented with GreenFoodLens labels and find that models are driven by popularity signals, which may exacerbate the environmental impact of users’ recipe choices. These experiments demonstrate the practical benefit of GreenFoodLens for benchmarking and advancing sustainability-aware recommendation research. The resource is available at .},
	booktitle = {Proceedings of the {Nineteenth} {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Marras, Mirko and Medda, Giacomo and Murgia, Giovanni},
	year = {2025},
	keywords = {Sustainability, Large Language Model, Constrained Generation, Food Recommendation, Human Labeling., Recipe Recommendation},
	pages = {764--773},
}

@inproceedings{kayumova_systematic_2025,
	address = {New York, NY, USA},
	series = {{ICFNDS} '24},
	title = {Systematic {Mapping} of {Computational} {Linguistics} in {Distributed} {Knowledge} {Based} {Systems} and {Management}},
	isbn = {979-8-4007-1170-1},
	url = {https://doi.org/10.1145/3726122.3726228},
	doi = {10.1145/3726122.3726228},
	abstract = {Advancements in computational linguistics have enabled the formulation of multiple natural language processing frameworks with considerable semantic accuracy, knowledge representation, and real-time adaptability benefits. Emerging research on distributed knowledge-based systems is challenging traditional conceptions of language processing and data integration, and in the process, opening up windows of opportunity for enhancing the scalability associated with knowledge management in decentralized environments. As little is known about where computational linguistics integration is gaining momentum beyond academic research and software engineering, the purpose of this systematic mapping study is to map in what areas of distributed knowledge management it is perceived to gain traction. Drawing on data from 150 systematically selected research articles and trend analysis in computational linguistics applications, we identify a long tail of application domains and methodological approaches in which a total of 42 unique computational models operate, including techniques such as knowledge graph embeddings, transformer-based architectures, and multimodal language models. Our findings reveal a strong, positive correlation coefficient (r = 0.82) between natural language processing adoption and knowledge retrieval efficiency in distributed systems. However, existing frameworks do not passively comply. Rather, their linguistic adaptability and semantic interpretation mechanisms are integrated into the core functionality of distributed knowledge networks. The study concludes by identifying key research gaps, reflecting on the application of machine learning-enhanced linguistic models in the field of knowledge management, and proposing suggestions for future research directions in distributed data processing. The findings enrich understandings of the workings of computational linguistics methodologies in experiences of real-time knowledge extraction and intelligent information retrieval.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Future} {Networks} \&amp; {Distributed} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kayumova, Kamola and Akbarova, Shakhnoza and Bobokeldiyeva, Maftuna and Abdukarimova, Gulchehra and Khaydarova, Umida and Xasanova, Zarina},
	year = {2025},
	pages = {727--733},
}

@inproceedings{miskell_automated_2024,
	address = {New York, NY, USA},
	series = {{NLPIR} '23},
	title = {Automated {Framework} to {Extract} {Software} {Requirements} from {Source} {Code}},
	isbn = {979-8-4007-0922-7},
	url = {https://doi.org/10.1145/3639233.3639242},
	doi = {10.1145/3639233.3639242},
	abstract = {Software maintenance and innovation are constant challenges across industries, especially as programming languages evolve with technology. Similarly, poor lexicon quality degrades program comprehension, increasing the effort required by developers to improve existing software products. To address these challenges, we propose a novel automated framework that extracts software requirements directly from source code using a baseline AI language model applied to a Java code base. Leveraging natural language processing techniques, the framework validates programs and generates easily readable requirements by analyzing file contents. The framework enhances agility and flexibility by providing comprehensive documentation for existing software systems. It caters to both experienced and less-experienced developers, offering an intuitive graphical user interface and enabling efficient identification and resolution of errors. The resulting output facilitates natural interaction through language processing. By automating the extraction process, the framework allows developers to better understand software systems, make informed decisions, and adapt to evolving needs.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Miskell, Cameron and Diaz, Richard and Ganeriwala, Parth and Slhoub, Khaled and Nembhard, Fitzroy},
	year = {2024},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {Natural Language Processing, Software evolution, AI language model, Extracting functional requirements, Legacy code, Software verification},
	pages = {130--134},
}

@inproceedings{yang_design_2023,
	address = {New York, NY, USA},
	series = {{BDIOT} '23},
	title = {Design of {General} {Chronic} {Disease} {Retrieval} {Model} {Framework} {Based} on {Chinese} {Medical} {Knowledge} {Graph}},
	isbn = {979-8-4007-0801-5},
	url = {https://doi.org/10.1145/3617695.3617722},
	doi = {10.1145/3617695.3617722},
	abstract = {This article proposes a method for constructing a chronic disease retrieval model based on the Chinese medical knowledge graph. By combining the Chinese medical knowledge graph with classification retrieval, a chronic disease classification retrieval model based on the medical knowledge graph is constructed, which mainly includes three aspects: constructing medical knowledge graph for retrieval, designing hierarchical classification rules, scheming sorting strategies and display methods. The proposed hierarchical classification retrieval model mechanism and related strategies are conducive to the effective organization of health information, solving the current problems of multi-source heterogeneity and semantic ambiguity, improving the efficiency and quality of user retrieval, has a certain promoting effect on the development of theories and methods related to health information services.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Big} {Data} and {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Yi and Yu, Dekuang and Zhu, Yangyang and Qin, Yuxuan and Wang, Erhao},
	year = {2023},
	note = {event-place: Beijing, China},
	keywords = {Chinese medical knowledge graph, Chronic diseases, construction method, retrieval model framework},
	pages = {195--200},
}

@inproceedings{mohsen_blockchain_2020,
	address = {New York, NY, USA},
	series = {{ICFET} '20},
	title = {Blockchain as a {Platform} for {Collaborative} {Ontology} {Evolution}},
	isbn = {978-1-4503-7533-7},
	url = {https://doi.org/10.1145/3404709.3404769},
	doi = {10.1145/3404709.3404769},
	abstract = {The Semantic Web is an incomplete dream so far, but a revolutionary platform as Blockchain could be the solution to this, not optimal reality. There is still little understanding of, and support for, the evolutionary aspects of ontologies. This is particularly crucial in distributed and collaborative settings such as the Semantic Web, where ontologies naturally co-evolve with their communities of use. In this setting, different organizations and teams collaboratively build a common ground of the domain. In this "complex" and dynamic setting, a collaborative change process model requires more powerful engineering, argumentation and negotiation methodologies. Blockchain offers a robust framework for teams' collaboration and ontology versioning globally between an infinite number of teams. Blockchain is an example of a distributed computing system with high Byzantine fault tolerance. This makes blockchains potentially suitable for the recording of evolution events, ontology records, and other records management activities, such as ontology evolution, transaction processing and ontology documenting provenance. In this paper, after briefly summarizing the significant features of Blockchain, we describe blockchain-empowered solutions for building an evolution model based on blockchain technology and its artifacts.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Frontiers} of {Educational} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
	year = {2020},
	note = {event-place: Tokyo, Japan},
	keywords = {Blockchain, Ontology Evolution, Collaborative Evolution, Inter-organizational Ontology, Consensus, Distributed Computing, validation and Evaluation},
	pages = {183--190},
}

@article{zhang_conco-ernie_2023,
	title = {Conco-{ERNIE}: {Complex} {User} {Intent} {Detect} {Model} for {Smart} {Healthcare} {Cognitive} {Bot}},
	volume = {23},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3574135},
	doi = {10.1145/3574135},
	abstract = {The outbreak of Covid-19 has exposed the lack of medical resources, especially the lack of medical personnel. This results in time and space restrictions for medical services, and patients cannot obtain health information all the time and everywhere. Based on the medical knowledge graph, healthcare bots alleviate this burden effectively by providing patients with diagnosis guidance, pre-diagnosis, and post-diagnosis consultation services in the way of human-machine dialogue. However, the medical utterance is more complicated in language structure, and there are complex intention phenomena in semantics. It is a challenge to detect the single intent, multi-intent, and implicit intent of a patient’s utterance. To this end, we create a high-quality annotated Chinese Medical query (utterance) dataset, CMedQ (about 16.8k queries in medical domain which includes single, multiple, and implicit intents). It is hard to detect intent on such a complex dataset through traditional text classification models. Thus, we propose a novel detect model Conco-ERNIE, using concept co-occurrence patterns to enhance the representation of pre-trained model ERNIE. These patterns are mined using Apriori algorithm and will be embedded via Node2Vec. Their features will be aggregated with semantic features into Conco-ERNIE by using an attention module, which can catch user explicit intents and also predict user implicit intents. Experiments on CMedQ demonstrates that Conco-ERNIE achieves outstanding performance over baseline. Based on Conco-ERNIE, we develop an intelligent healthcare bot, MedicalBot. To provide knowledge support for MedicalBot, we also build a Chinese medical graph, CMedKG (about 45k entities and 283k relationships).},
	number = {1},
	journal = {ACM Trans. Internet Technol.},
	author = {Zhang, Bolin and Tu, Zhiying and Hang, Shaoshi and Chu, Dianhui and Xu, Xiaofei},
	month = feb,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Intent detection, cognitive service, healthcare bot, medical knowledge graph},
}

@inproceedings{wang_unified_2022,
	address = {New York, NY, USA},
	series = {{ICCIR} '22},
	title = {A unified modeling method of product demand and manufacturing capability in the textile industry {Internet}},
	isbn = {978-1-4503-9717-9},
	url = {https://doi.org/10.1145/3548608.3559289},
	doi = {10.1145/3548608.3559289},
	abstract = {Through the analysis of multiple cloud manufacturing models, we discussed the definition of the current textile industry Internet manufacturing model, and proposed the concept of the textile industry knowledge graph for the standardized expression and sharing of demand and manufacturing resources under the textile industry Internet manufacturing model; A unified modeling and packaging method for manufacturing requirements and manufacturing capabilities based on knowledge graph technology with standard process flow as the interface is proposed.},
	booktitle = {Proceedings of the 2022 2nd {International} {Conference} on {Control} and {Intelligent} {Robotics}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Guodong and Liu, Guohua},
	year = {2022},
	note = {event-place: Nanjing, China},
	keywords = {ontology, knowledge graph, meta-model, cloud manufacturing, Industrial Internet},
	pages = {690--694},
}

@inproceedings{li_improving_2020,
	address = {New York, NY, USA},
	series = {{CSAE} '20},
	title = {Improving {Biomedical} {Ontology} {Matching} {Using} {Domain}-specific {Word} {Embeddings}},
	isbn = {978-1-4503-7772-0},
	url = {https://doi.org/10.1145/3424978.3425102},
	doi = {10.1145/3424978.3425102},
	abstract = {Biomedical ontology is an effective carrier of biomedical knowledge. In real-world applications, many biomedical ontologies describe knowledge in the same field. In order to make full use of existing knowledge, it is necessary to carry out knowledge fusion to obtain a unified knowledge structure. For this reason, it becomes particularly important to find mapping entities that refer to the same object in different ontologies. At present, a large number of automatic matching systems engineers features and match entities by the name of the entity, the ontology structure and external resources. These methods have achieved encouraging results, but they ignore the semantic information of the entity labels. On the other hand, the representation learning method has already shined in many areas of natural language processing. The word vector obtained by the word embedding method contains semantic information of words. However, a separate representation learning method cannot fully capture the structural information of the ontology. In this paper, we propose a method which combines the representation learning method as a component with traditional feature engineering methods to improve the performance of the matching systems. We tested our method on real-world datasets. Experimental results show that our method can improve the recall and F1-measure of the existing matching systems.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Computer} {Science} and {Application} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Li, Guoxuan},
	year = {2020},
	note = {event-place: Sanya, China},
	keywords = {Artificial intelligence, Biomedical ontology matching, Word embedding, Feature engineering},
}

@inproceedings{rakib_thermoplastic_2023,
	address = {New York, NY, USA},
	series = {{DIS} '23},
	title = {Thermoplastic {Kilnforms}: {Extending} {Glass} {Kilnforming} {Techniques} to {Thermoplastic} {Materials} using {Ontology}-{Driven} {Design}},
	isbn = {978-1-4503-9893-0},
	url = {https://doi.org/10.1145/3563657.3596027},
	doi = {10.1145/3563657.3596027},
	abstract = {The ecology of thermoplastic materials is rapidly evolving, enabling an exciting landscape of functional, aesthetic, and interactive forms. Despite their utility in fused filament fabrication (FFF), an even larger and untapped design space exists for thermoplastics. In this work, we introduce a design method that leverages similarities with a more mature medium (glass) to guide a material-centered exploration of a new medium (thermoplastics). Through a collaboration between domain experts in thermoplastics and glass, we synthesized an ontology of kilnforming techniques and developed an annotated portfolio of thermoplastic kilnforms that capture generative design directions for altering the phenomenological qualities of plastic, prototyping metamaterials, and composite forms, and engaging with other material practices. We discuss how material parallels can continue to expand the role of thermoplastics as a design material and how ontology-driven design can serve as a means of localizing, questioning, and generating material knowledge.},
	booktitle = {Proceedings of the 2023 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Rakib, Mohammad Abu Nasir and Scidmore, Jeremy and Ginsberg, Justin and Torres, Cesar},
	year = {2023},
	note = {event-place: Pittsburgh, PA, USA},
	keywords = {composites, material exploration, thermoforming},
	pages = {263--281},
}

@inproceedings{aicher_self-imposed_2023,
	address = {New York, NY, USA},
	series = {{CUI} '23},
	title = {Self-imposed {Filter} {Bubble} {Model} for {Argumentative} {Dialogues}},
	isbn = {979-8-4007-0014-9},
	url = {https://doi.org/10.1145/3571884.3597131},
	doi = {10.1145/3571884.3597131},
	abstract = {During their information seeking people tend to filter out all the parts of the available information that do not fit their existing beliefs or opinions. In this paper we present a model for this “Self-imposed Filter Bubble” (SFB) consisting of four dimensions. Thereby, we aim to 1) estimate the probability of the user being caught in an SFB and consequently, 2) identify suitable clues to reduce this probability in the further course of a dialogue. Using an exemplary implementation in an argumentative dialogue system, we demonstrate the validity and applicability of this model in an online user study with 102 participants. These findings serve as a basis for developing a system strategy to break the user’s SFB and contribute to a sustainable and profound reflection on a topic from all viewpoints.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Aicher, Annalena Bea and Kornmüller, Daniel and Minker, Wolfgang and Ultes, Stefan},
	year = {2023},
	note = {event-place: Eindhoven, Netherlands},
	keywords = {User Modeling, Computational Argumentation, Confirmation Bias, Cooperative Argumentative Dialogue Systems (ADS), Echo Chambers},
}

@inproceedings{adhikari_towards_2022,
	address = {New York, NY, USA},
	series = {{PETRA} '22},
	title = {Towards {FAIR} {Explainable} {AI}: a standardized ontology for mapping {XAI} solutions to use cases, explanations, and {AI} systems},
	isbn = {978-1-4503-9631-8},
	url = {https://doi.org/10.1145/3529190.3535693},
	doi = {10.1145/3529190.3535693},
	abstract = {Several useful taxonomies have been published that survey the eXplainable AI (XAI) research field. However, these taxonomies typically do not show the relation between XAI solutions and several use case aspects, such as the explanation goal or the task context. In order to better connect the field of XAI research with concrete use cases and user needs, we designed the ASCENT (Ai System use Case Explanation oNTology) framework, which is a new ontology and corresponding metadata standard with three complementary modules for different aspects of an XAI solution: one for aspects of AI systems, another for use case aspects, and yet another for explanation properties. The descriptions of XAI solutions in this framework include whether the XAI solution has a positive, negative, inconclusive or unresearched relation with use case elements. Descriptions in ASCENT thus emphasize the (user) evaluation of XAI solutions in order to support finding validated practices for application in industry, as well as being helpful for identifying research gaps. Describing XAI solutions according to the proposed common metadata standard is an important step towards the FAIR (Findable, Accessible, Interoperable, Reusable) usage of XAI solutions.},
	booktitle = {Proceedings of the 15th {International} {Conference} on {PErvasive} {Technologies} {Related} to {Assistive} {Environments}},
	publisher = {Association for Computing Machinery},
	author = {Adhikari, Ajaya and Wenink, Edwin and van der Waa, Jasper and Bouter, Cornelis and Tolios, Ioannis and Raaijmakers, Stephan},
	year = {2022},
	note = {event-place: Corfu, Greece},
	keywords = {FAIR, ASCENT, user-centered, XAI ontology},
	pages = {562--568},
}

@inproceedings{rajbhoj_doctomodel_2023,
	address = {Melbourne, Australia},
	series = {{ICSE}-{SEIP} '23},
	title = {{DocToModel}: {Automated} {Authoring} of {Models} from {Diverse} {Requirements} {Specification} {Documents}},
	isbn = {979-8-3503-0037-6},
	url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00024},
	doi = {10.1109/ICSE-SEIP58684.2023.00024},
	abstract = {Early stages of Software Development Life Cycle (SDLC) namely requirement elicitation and requirements analysis have remained document-centric in the industry for market-driven, complex, large-scale business applications and products. The documentation typically runs into hundreds of Natural Language (NL) text documents which requirements engineers need to sift looking for the relevant information and also maintain these documents in-sync over time - a time-consuming and error-prone activity. Much of this difficulty can be overcome if the information is available in a structured form that is amenable to automated processing. Purposive models offer a way out. However, for easy adoption by industry practitioners, these models must be populated from NL text documents in a largely automated manner. This task is characterized by high variability with several documents containing different information conforming to different structures and styles. As a result, purposive information extractors need to be developed for each project/ product. Moreover, being an open-ended space there is no upper bound on the information extractors that need to be developed. To overcome this difficulty, we propose a document structure agnostic and meta-model agnostic tool, DocToModel, for the automated authoring of models from NL text documents. It provides a pattern mapping language to specify a mapping of structured and unstructured document information to meta-model elements, and a pattern interpreter to automate model authoring. The configurable and extensible architecture of DocToModel makes it generic and amenable to easy repurposing for other NL documents. This paper, describes the approach and illustrates its utility and efficacy on multiple real-world case studies.},
	booktitle = {Proceedings of the 45th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {IEEE Press},
	author = {Rajbhoj, Asha and Nistala, Padmalata and Kulkarni, Vinay and Soni, Shivani and Pathan, Ajim},
	year = {2023},
	keywords = {NLP, meta-model, automated model authoring, document parser, meta-model pattern, model extraction, pattern interpreter},
	pages = {199--210},
}

@inproceedings{bohm_deep_2025,
	address = {New York, NY, USA},
	series = {{ICMET} '24},
	title = {Deep and {Surface} {Representation} of {Competences} in {Academic} {Curricula}: {A} new approach to address human consumers and formal structures using {Generative} {AI}},
	isbn = {979-8-4007-1263-0},
	url = {https://doi.org/10.1145/3729434.3729456},
	doi = {10.1145/3729434.3729456},
	abstract = {The curricular design of programs in Higher Education is increasingly oriented towards competence-based learning goals in order to provide more specific and explicit qualifications. At the same time the descriptions of competences are mostly based on textual descriptions often in informal style targeted towards different audiences making descriptions vague and harder to compare. Formal models for the specification of competences exist for some time and even semantic models are being developed, e.g., the European Learning Model. These different approaches lead to a gap between formal and informal competence descriptions that require manual efforts of curriculum designer to maintain. This research borrows the concept of Deep and Surface representation models from the field of linguistics to unify the different approaches. Additionally, it focuses on the functionality of Generative Artificial Intelligence in the form of Large Language Models to close the aforementioned gap. It demonstrates the potential of the technology in both directions and discusses application potential of the concept.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Modern} {Educational} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Böhm, Karsten},
	year = {2025},
	keywords = {LLM, Semantic Web, Curricular Design, European Learning Model, Generative Artificial Intelligence, Higher Education},
	pages = {78--85},
}

@inproceedings{miranda_conceptual_2024,
	address = {New York, NY, USA},
	series = {{MO2RE} 2024},
	title = {A {Conceptual} {Model} {For} {Web} {Accessibility} {Requirements} {In} {Agile} {Development}},
	isbn = {979-8-4007-0569-4},
	url = {https://doi.org/10.1145/3643666.3648580},
	doi = {10.1145/3643666.3648580},
	abstract = {Accessibility is the practice of making content and functionality accessible to all users, regardless of their abilities. Although accessibility is a highly relevant quality attribute, it is often treated as an afterthought in software development, unfortunately excluding people with disabilities from using many web-based systems. Specifically in agile development, sprints focus on new features and quality attributes, such as accessibility, are often not considered sufficiently. In these cases, using conceptual models to understand and analyze requirements that developers have formulated as a set of related user stories is a research opportunity. To increase agile professionals' focus on accessibility, we built a conceptual model for web accessibility, identifying artifacts and concepts used in agile development to specify accessibility. We discuss how this model can be used as a guide to better integrate accessibility considerations into agile software development. Researchers can use the result to define resources that are not currently covered or improve underutilized practices. We plan to use the conceptual model in the next steps to adapt existing agile artifacts and create support tools for web accessibility in agile development.},
	booktitle = {Proceedings of the 1st {IEEE}/{ACM} {Workshop} on {Multi}-{Disciplinary}, {Open}, and {RElevant} {Requirements} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Miranda, Darliane and Araújo, João and Liebel, Grischa},
	year = {2024},
	note = {event-place: Lisbon, Portugal},
	keywords = {conceptual model, agile development, requirements engineering, accessibility requirements},
	pages = {15--21},
}

@inproceedings{partridge_implicit_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Implicit requirements for ontological multi-level types in the {UNICLASS} classification},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3421414},
	doi = {10.1145/3417990.3421414},
	abstract = {In the multi-level type modeling community, claims that most enterprise application systems use ontologically multi-level types are ubiquitous. To be able to empirically verify this claim one needs to be able to expose the (often underlying) ontological structure and show that it does, indeed, make a commitment to multi-level types. We have not been able to find any published data showing this being done. From a top-level ontology requirements perspective, checking this multi-level type claim is worthwhile. If the datasets for which the top-level ontology is required are ontologically committed to multi-level types, then this is a requirement for the top-level ontology. In this paper, we both present some empirical evidence that this ubiquitous claim is correct as well as describing the process we used to expose the underlying ontological commitments and examine them. We describe how we use the bCLEARer process to analyse the UNICLASS classifications making their implicit ontological commitments explicit. We show how this reveals the requirements for two general ontological commitments; higher-order types and first-class relations. This establishes a requirement for a top-level ontology that includes the UNICLASS classification to be able to accommodate these requirements. From a multi-level type perspective, we have established that the bCLEARer entification process can identify underlying ontological commitments to multi-level type that do not exist in the surface linguistic structure. So, we have a process that we can reuse on other datasets and application systems to help empirically verify the claim that ontological multi-level types are ubiquitous.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Partridge, Chris and Mitchell, Andrew and da Silva, Marco and Soto, Oscar Xiberta and West, Matthew and Khan, Mesbah and de Cesare, Sergio},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {top-level ontology, bCLEARer approach, first class relations, higher order types, UNICLASS},
}

@inproceedings{liu_crawlabel_2022,
	address = {New York, NY, USA},
	series = {{AST} '22},
	title = {{CrawLabel}: computing natural-language labels for {UI} test cases},
	isbn = {978-1-4503-9286-0},
	url = {https://doi.org/10.1145/3524481.3527229},
	doi = {10.1145/3524481.3527229},
	abstract = {End-to-end test cases that exercise the application under test via its user interface (UI) are known to be hard for developers to read and understand; consequently, diagnosing failures in these tests and maintaining them can be tedious. Techniques for computing natural-language descriptions of test cases can help increase test readability. However, so far, such techniques have been developed for unit test cases; they are not applicable to end-to-end test cases.In this paper, we focus on the problem of computing natural-language labels for the steps of end-to-end UI test cases for web applications. We present two techniques that apply natural-language processing to information available in the browser document object model (DOM). The first technique is an instance of a supervised approach in which labeling-relevant DOM attributes are ranked via manual analysis and fed into label computation. However, supervised approach requires a training dataset. So we propose the second technique, which is unsupervised: it leverages probabilistic context-free grammar learning to compute dominant DOM attributes automatically. We implemented these techniques, along with two simpler baseline techniques, in a tool called CrawLabel (available as a plugin to Crawljax, a state-of-the-art UI test-generation tool for web applications) and evaluated their effectiveness on open-source web applications. Our results indicate that the supervised approach can achieve precision, recall, and Fl-score of 83.38, 60.64, and 66.40, respectively. The unsupervised approach, although less effective, is competitive, achieving scores of 72.37, 58.12, and 59.77. We highlight key results and discuss the implications of our findings.},
	booktitle = {Proceedings of the 3rd {ACM}/{IEEE} {International} {Conference} on {Automation} of {Software} {Test}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yu and Yandrapally, Rahulkrishna and Kalia, Anup K. and Sinha, Saurabh and Tzoref-Brill, Rachel and Mesbah, Ali},
	year = {2022},
	note = {event-place: Pittsburgh, Pennsylvania},
	pages = {103--114},
}

@inproceedings{dong_reveal_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Reveal the {Unknown}: {Out}-of-{Knowledge}-{Base} {Mention} {Discovery} with {Entity} {Linking}},
	isbn = {979-8-4007-0124-5},
	url = {https://doi.org/10.1145/3583780.3615036},
	doi = {10.1145/3583780.3615036},
	abstract = {Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also apply KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT, and the general KB, WikiData.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Liu, Yinan and Horrocks, Ian},
	year = {2023},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {Ontology, Language model, Wikidata, language models, Biomedical ontologies, Knowledge based systems, biomedical ontologies, Entity linking, entity linking, Knowledge base enrichment, knowledge base enrichment, WikiData, 'current, Classification (of information), Matchings, Simple++, Feature-based classification, Knowledge base maintenance},
	pages = {452--462},
	annote = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{zhu_sosa-shacl_2022,
	address = {New York, NY, USA},
	series = {{IJCKG} '21},
	title = {{SOSA}-{SHACL}: {Shapes} {Constraint} for the {Sensor}, {Observation}, {Sample}, and {Actuator} {Ontology}},
	isbn = {978-1-4503-9565-6},
	url = {https://doi.org/10.1145/3502223.3502235},
	doi = {10.1145/3502223.3502235},
	abstract = {The explosive growth of the Linked Data on the Web has greatly facilitated collecting data from remote sensors, from air quality sensors spread out across a city, to seismograph stations spread across the entire world. Integrating these heterogeneous data can be quite challenging; however one can achieve this through the use of available W3C standards to create a knowledge graph. For this use case, the W3C also provides a standard, the Sensor, Observation, Sample, Actuator (SOSA) Ontology, that allows for the semantic encoding of sensors and their observations. However, even with the guidance of this standard, it may be difficult to produce a correct graph with high fidelity from heterogeneous sources. In this paper we present a set of (data) shape constraints, called SOSA-SHACL, for the SOSA ontology using a data validation language, namely the W3C standard SHACL (Shape Constraint Language). These constraints enable us to evaluate whether the modeled observations in our Knowledge Graph comply with the SOSA recommendations. Furthermore, we show through several case studies how the closed world assumption plays a role in the process of designing such shape constraints, especially as SOSA is based on the open world assumption.},
	booktitle = {Proceedings of the 10th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Rui and Shimizu, Cogan and Stephen, Shirly and Zhou, Lu and Cai, Ling and Mai, Gengchen and Janowicz, Krzysztof and Schildhauer, Mark and Hitzler, Pascal},
	year = {2022},
	note = {event-place: Virtual Event, Thailand},
	keywords = {knowledge graph quality assessment and refinement, RDF validation, sensors and observations},
	pages = {99--107},
}

@inproceedings{amiri_tool_2024,
	address = {New York, NY, USA},
	series = {{EuroPLoP} '23},
	title = {Tool {Support} for {Learning} {Architectural} {Guidance} {Models} from {Architectural} {Design} {Decision} {Models}},
	isbn = {979-8-4007-0040-8},
	url = {https://doi.org/10.1145/3628034.3628037},
	doi = {10.1145/3628034.3628037},
	abstract = {This paper presents an approach to architectural knowledge management that does not assume existing architectural design decisions or pattern applications are documented as architectural knowledge, but benefits from more existing data. We drew inspiration from manual qualitative research methods for mining patterns and architectural knowledge and created a guideline model of which the ADD models are instances. We evaluated our approach on 11 cases from the gray literature. We found that it can provide suitable recommendations after modeling only a single case and reaches theoretical saturation and recommendations with low to very low errors after only 6-8 cases. Our approach shows that creating a reusable architectural design space is possible based only on limited case data. Our approach not only provides a novel approach to architectural knowledge management but can also be used as a tool for pattern mining.},
	booktitle = {Proceedings of the 28th {European} {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {Association for Computing Machinery},
	author = {Amiri, Amirali and Ntentos, Evangelos and Zdun, Uwe and Geiger, Sebastian},
	year = {2024},
	note = {event-place: Irsee, Germany},
	keywords = {Architectural Design Decisions, Architectural Knowledge Management, Design Patterns},
}

@inproceedings{stang_data_2022,
	address = {New York, NY, USA},
	series = {{SEA4DQ} 2022},
	title = {Data quality as a microservice: an ontology and rule based approach for quality assurance of sensor data in manufacturing machines},
	isbn = {978-1-4503-9459-8},
	url = {https://doi.org/10.1145/3549037.3561272},
	doi = {10.1145/3549037.3561272},
	abstract = {The manufacturing industry is continuously looking for production improvements resulting in high quality production, reduced waste and competitive advantages. In this article, ontologies, semantic rule logic and microservices have been deployed to suggest a system for quality assurance of manufacturing machine data. The existing upper ontology for manufacturing service description has been used to define both the physical assets as well as the data quality requirements. The system is used to both operationalize data quality monitoring by semantic technology as well as enabling up-front modelling of data quality requirements. The approach is illustrated by a specific speed-feed case for manufacturing machines but could easily be extended to other manufacturing use-cases or even to other industries.},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Software} {Engineering} and {AI} for {Data} {Quality} in {Cyber}-{Physical} {Systems}/{Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Stang, Jørgen and Walther, Dirk and Myrseth, Per},
	year = {2022},
	note = {event-place: Singapore, Singapore},
	keywords = {IoT, Data Quality, Microservices, Manufacturing Machines, Ontolologies, Sensor Data},
	pages = {3--9},
}

@article{bromander_investigating_2021,
	title = {Investigating {Sharing} of {Cyber} {Threat} {Intelligence} and {Proposing} {A} {New} {Data} {Model} for {Enabling} {Automation} in {Knowledge} {Representation} and {Exchange}},
	volume = {3},
	url = {https://doi.org/10.1145/3458027},
	doi = {10.1145/3458027},
	abstract = {For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.},
	number = {1},
	journal = {Digital Threats},
	author = {Bromander, Siri and Swimmer, Morton and Muller, Lilly Pijnenburg and Jøsang, Audun and Eian, Martin and Skjøtskift, Geir and Borg, Fredrik},
	month = oct,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, knowledge graph, Cyber threat intelligence, security},
}

@inproceedings{chen_conceptualizing_2025,
	address = {New York, NY, USA},
	series = {L@{S} '25},
	title = {Conceptualizing {Online} {Feedback} {Engagement} from a {Sociomaterial} {Perspective}: {An} {Iceberg} {Model}},
	isbn = {979-8-4007-1291-3},
	url = {https://doi.org/10.1145/3698205.3733934},
	doi = {10.1145/3698205.3733934},
	abstract = {This study conceptualizes online feedback engagement through a sociomaterial lens, exploring how learners' feedback engagement is dynamically shaped by the entanglement of human and non-human actors in digital environments. While prior research has examined cognitive, behavioral, and affective dimensions of feedback engagement in face-to-face learning environments, few studies have explored how technological affordances and sociocultural values mediate these forms of engagement. Drawing on a sociomaterial perspective, this study proposes a multidimensional framework of feedback engagement comprising cognitive, behavioral, relational, and collaborative dimensions. By synthesizing existing literature and integrating insights from recent empirical studies involving digital feedback tools, the paper highlights how engagement is not solely a learner-driven phenomenon but is co-constructed through sociomaterial arrangements. The framework advances current understandings of online feedback engagement and offers implications for the design of pedagogically sound feedback practices in technology-mediated learning contexts.},
	booktitle = {Proceedings of the {Twelfth} {ACM} {Conference} on {Learning} @ {Scale}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Shijun (Cindy)},
	year = {2025},
	note = {event-place: Palermo, Italy},
	keywords = {feedback, feedback engagement, online learning, sociomaterialism},
	pages = {251--255},
}

@inproceedings{doughty_comparative_2024,
	address = {New York, NY, USA},
	series = {{ACE} '24},
	title = {A {Comparative} {Study} of {AI}-{Generated} ({GPT}-4) and {Human}-crafted {MCQs} in {Programming} {Education}},
	isbn = {979-8-4007-1619-5},
	url = {https://doi.org/10.1145/3636243.3636256},
	doi = {10.1145/3636243.3636256},
	abstract = {There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models\&nbsp;(LLMs) in generation and engagement with coding exercises, the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT-4 to produce multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs from high-level course context and module-level LOs. We evaluated 651 LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors. We also observed that the generated MCQs appeared to be well-aligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support MCQ authoring efforts.},
	booktitle = {Proceedings of the 26th {Australasian} {Computing} {Education} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Doughty, Jacob and Wan, Zipiao and Bompelli, Anishka and Qayum, Jubahed and Wang, Taozhi and Zhang, Juran and Zheng, Yujia and Doyle, Aidan and Sridhar, Pragnya and Agarwal, Arav and Bogart, Christopher and Keylor, Eric and Kultur, Can and Savelka, Jaromir and Sakr, Majd},
	year = {2024},
	note = {event-place: Sydney, NSW, Australia},
	keywords = {Large Language Models, LLMs, GPT-4, Automatic Generation, Automated Content Generation, Assessments, Learning Objectives, LOs, MCQs, Multiple-choice Questions},
	pages = {114--123},
}

@inproceedings{gena_ontologies_2022,
	address = {New York, NY, USA},
	series = {{UMAP} '22 {Adjunct}},
	title = {Ontologies and {Open} {Data} for {Enriching} {Personalized} {Social} {Moments} in {Human} {Robot} {Interaction}},
	isbn = {978-1-4503-9232-7},
	url = {https://doi.org/10.1145/3511047.3537690},
	doi = {10.1145/3511047.3537690},
	abstract = {This paper describes our proposal for enriching personalized social moments and dialogues between human and robot in the context of the Sugar, Salt \&amp; Pepper laboratory. The lab focused on the use of the Pepper robot in a therapeutic context to promote autonomies and functional acquisitions in highly functioning (Asperger) children with autism. This paper is focused on a post-hoc work aimed at improving the robot's autonomous dialogue strategies. In particular we are integrating the robot's dialogue with a knowledge base to have the robot able to move and reason on an ontology, and thus enriching its dialogue's strategies. For instance, the taxonomic structure of the ontology could allow Pepper to drive the focus of the conversation to related topics or to more general or specific topics, and, in general, it could improve its capability to manage the conversation and disambiguate the input from the user.},
	booktitle = {Adjunct {Proceedings} of the 30th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Gena, Cristina and Damiano, Rossana and Mattutino, Claudio and Mazzei, Alessandro and Brighenti, Stefania and Nazzario, Matteo and Meirone, Andrea and Quarato, Camilla and Miraglio, Elisabetta and Ricciardiello, Giulia and Petriglia, Francesco and Liscio, Federica and Piccinni, Giuseppe and Mazzotta, Loredana and Pecone, Cesare and Ricci, Valeria},
	year = {2022},
	note = {event-place: Barcelona, Spain},
	keywords = {Adaptivity, HRI, Human Behavior Understanding, Social Robots},
	pages = {151--154},
}

@inproceedings{mohsen_cooperative_2020,
	address = {New York, NY, USA},
	series = {{ICFET} '20},
	title = {Cooperative {Domain} {Ontology} {Reduction} {Based} on {Power} {Sets}},
	isbn = {978-1-4503-7533-7},
	url = {https://doi.org/10.1145/3404709.3404771},
	doi = {10.1145/3404709.3404771},
	abstract = {Ontology is widely used in the areas of knowledge engineering, web-based data mining, and others. The process of developing and evolving inter-organizational domain ontologies is easy to get much redundant information. PowerSets can be used to reduce the attributes of ontologies. In this paper, "Rule Finding Uniqueness," RFU is proposed for learning a set of rules in order to refine an ontology. The algorithm's primary goal is to generate unique rules that not only cover the initial set but also enhance reasoning. The claimed technique compresses Ontologies after it is already built or during the evolving process of the inter-organizational cooperative domain ontology. The proposed method can also be used to strengthen automatic and semi-automatic operations to develop and evolve ontologies. We can consider this approach as a maintenance operation that could be done periodically based on the ontology evolution frequency rate.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Frontiers} of {Educational} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
	year = {2020},
	note = {event-place: Tokyo, Japan},
	keywords = {Attributes, Inter-organizational domain ontology, Ontology Reductio, Power Sets},
	pages = {196--203},
}

@inproceedings{khiat_towards_2021,
	address = {New York, NY, USA},
	series = {{iiWAS} '20},
	title = {Towards an {Ontology} {Representing} {Characteristics} of {Inflammatory} {Bowel} {Disease}},
	isbn = {978-1-4503-8922-8},
	url = {https://doi.org/10.1145/3428757.3429110},
	doi = {10.1145/3428757.3429110},
	abstract = {Inflammatory bowel disease (IBD) is a chronic disease characterized by numerous, hard to predict periods of relapse and remission. "Digital twin" approaches, leveraging personalized predictive models, would significantly enhance therapeutic decision-making and cost-effectiveness. However, the associated computational and statistical methods require high quality data from a large population of patients. Such a comprehensive repository is very challenging to build, though, and none is available for IBD. To overcome this, a promising approach is to employ a knowledge graph, which is built from the available data and would help predicting IBD episodes and delivering more relevant personalized therapy at the lowest cost. In this research, we present a knowledge graph developed on the basis of patient records which are collected from one of the largest German gastroentologic outpatient clinic. First, we designed IBD ontology that encompasses the vocabulary, specifications and characteristics associated by physicians with IBD patients, such as disease classification schemas (e.g., Montreal Classification of IBD), status of the disease activity, and medications. Next, we defined the mappings between ontology entities and database variables. Physicians and project members participating in the Fraunhofer MED2ICIN project, validated the ontology and the knowledge graph. Furthermore, the knowledge graph has been validated against the competency questions compiled by physicians.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Khiat, Abderrahmane and Elias, Mirette and Foldenauer, Ann Christina and Koehm, Michaela and Blumenstein, Irina and Napolitano, Giulio},
	year = {2021},
	note = {event-place: Chiang Mai, Thailand},
	keywords = {Ontology, knowledge graph, Ontop, IBD, Mappings, VoCoReg},
	pages = {216--222},
}

@inproceedings{ribeiro_midas-owl_2021,
	address = {New York, NY, USA},
	series = {{SBSI} '21},
	title = {{MIDAS}-{OWL}: {An} {Ontology} for {Interoperability} between {Data} and {Service} {Cloud} {Layers}},
	isbn = {978-1-4503-8491-9},
	url = {https://doi.org/10.1145/3466933.3466953},
	doi = {10.1145/3466933.3466953},
	abstract = {As different cloud computing services have emerged over the years, the diversity of technologies and the lack of standardization has given rise to an interoperability problem in cloud computing. Cloud computing services include those such as Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), and Data as a Service (DaaS). In this context, interoperability enables a service to communicate with another service transparently. Among the solutions proposed in the literature, a middleware can be used to intermediate such communication and to mitigate the lack of interoperability in cloud computing. For instance, the middleware MIDAS (Middleware for DaaS and SaaS) provides transparent interoperability between SaaS and DaaS. Although MIDAS current version promotes syntactic interoperability, semantic interoperability is only superficially addressed. In collaboration with this project, we develop an OWL-based ontology to formally represent the communication between SaaS and DaaS, and discuss its strengths in providing semantic interoperability on MIDAS. We conduct a set of experiments to validate our ontology. We evaluate intrinsic (consistency, correctness, acceptance) and extrinsic (integration between ontology and MIDAS) issues. Results provide evidence that a semantic MIDAS interoperability can be enhanced by our ontology.},
	booktitle = {Proceedings of the {XVII} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ribeiro, Elivaldo Lozer Fracalossi and Souza, Marlo and Claro, Daniela Barreiro},
	year = {2021},
	note = {event-place: Uberlândia, Brazil},
	keywords = {ontology, semantic interoperability, cloud computing, cloud services},
}

@inproceedings{deng_joint_2024,
	address = {New York, NY, USA},
	series = {{BIC} '24},
	title = {A {Joint} {Framework} for {Predicting} {Disease}-{Gene} {Interactions} {Based} on {Pre}-trained {Models} and {Graph} {Attention} {Networks}},
	isbn = {979-8-4007-1664-5},
	url = {https://doi.org/10.1145/3665689.3665768},
	doi = {10.1145/3665689.3665768},
	abstract = {The study of disease-gene interactions is crucial in biomedical research. Identifying genes associated with diseases can provide critical insights into disease mechanisms, facilitate early diagnosis, and contribute to the development of targeted therapies. In this paper, we propose a novel framework for predicting disease-gene interactions called the PRGAT-DG, which utilizes pre-trained language models and graph attention networks to extract semantic and graph structure features respectively. Moreover, we introduce residual structure to alleviate the problem of excessive smoothing. Experimental results on a dataset released by Stanford University demonstrate the remarkable predictive accuracy of our framework, showcasing its superiority compared to other existing methods. This research holds significant implications for advancing our understanding of disease-gene interaction mechanisms and accelerating the development of relevant therapeutics.},
	booktitle = {Proceedings of the 2024 4th {International} {Conference} on {Bioinformatics} and {Intelligent} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Deng, Qiwen and Han, Yuexia and Sun, Jianfei},
	year = {2024},
	note = {event-place: Beijing, China},
	pages = {474--478},
}

@inproceedings{biglari_model_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Model validity and tolerance quantification for real-time adaptive approximation},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561604},
	doi = {10.1145/3550356.3561604},
	abstract = {Designing a Cyber-physical system (CPS) including modeling the control components and services is a challenging issue. Models and simulations at run-time play a crucial role to implement these control and prediction components.Real-time constraints raise the complexity of designing an efficient CPS system. Having detailed models in making decisions and/or numerous predictions in different contexts is computationally expensive and difficult to schedule on the computational infrastructure.Inspired by substitutability, one strategy for dealing with complex CPS and the contradiction of better real-time performance and reduced cost in CPS is to employ approximated models and switch to the most suited model adaptively at run-time.However, using an approximate model raises the uncertainty on the model's predictions. Nonetheless, the model is appropriate when the uncertainty is within bound. This bound is defined as tolerance which is the permitted amount of uncertainty.In this paper, we propose a method for quantifying the tolerance of cyber-physical systems, where we can switch between the original model and approximated models and how to identify more appropriate models.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Biglari, Raheleh and Denil, Joachim},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {cyber-physical systems, adaptation, uncertainty, approximation, model validity, real-time systems, tolerance quantification},
	pages = {668--676},
}

@inproceedings{munoz_imitating_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Imitating {Human} {Reasoning} to {Extract} {5W1H} in {News}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715532},
	doi = {10.1145/3701716.3715532},
	abstract = {Extracting key information from news articles is crucial for advancing search systems. Historically, the 5W1H framework, which organises information based on 'Who', 'What', 'When', 'Where', 'Why', and 'How', has been a predominant method in digital journalism empowering search tools. The rise of Large Language Models (LLMs) has sparked new research into their potential for performing such information extraction tasks effectively. Our study examines a novel approach to employing LLMs in the 5W1H extraction process, particularly focusing on their capacity to mimic human reasoning. We introduce two innovative Chain-of-Thought (COT) prompting techniques to extract 5W1H in news: extractive reasoning and question-level reasoning. The former directs the LLM to pinpoint and highlight essential details from texts, while the latter encourages the model to emulate human-like reasoning at the question-response level. Our research methodology includes experiments with leading LLMs using prompting strategies to ascertain the most effective approach. The results indicate that COT prompting significantly outperforms other methods. In addition, we show that the effectiveness of LLMs in such tasks depends greatly on the nature of the questions posed.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Muñoz, Carlos and Mendoza, Marcelo and Lobel, Hans and Keith, Brian},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {llm, 5w1h, imitative reasoning, news},
	pages = {1199--1203},
}

@inproceedings{diaconescu_multi-scale_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Multi-scale model-based explanations for cyber-physical systems: the urban traffic case},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561554},
	doi = {10.1145/3550356.3561554},
	abstract = {Automated control in Cyber-Physical Systems (CPS) generates behaviours that may surprise non-expert users. Relevant explanations are required to maintain user trust. Large CPS (e.g., autonomous car networks and smart grids) raise additional scaleability issues for the explanatory processes and complexity issues for generated explanations. We propose a multi-scale system modelling and explanation technique to address these concerns. The idea is to increase the scale, or abstraction level, of the modelled CPS, whenever possible without loss of salient information, so as to produce smaller system representations and hence to reduce the complexity of the explanatory process and of the generated explanations. We illustrate our proposal via an urban traffic case study, modelling traffic at two different scales (i.e., modelling individual cars at a lower-scale; and traffic jams at a higher-scale). We show how a multi-scale explanatory process can use the lower- and higher-scale models to generate either longer (more detailed) explanations, or shorter (more abstract) explanations, respectively. This proof-of-concept illustration offers a basis for further research towards a comprehensive multi-scale explanatory solution for CPS.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Diaconescu, Ada and Houze, Etienne and Dessalles, Jean-Louis and Vangheluwe, Hans and Franceschini, Romain},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {cyber-physical system, multi-scale model and explanation, traffic simulation},
	pages = {684--691},
}

@inproceedings{chetwyn_modelling_2024,
	address = {New York, NY, USA},
	series = {{EICC} '24},
	title = {Modelling {Indicators} of {Behaviour} for {Cyber} {Threat} {Hunting} via {Sysmon}},
	isbn = {979-8-4007-1651-5},
	url = {https://doi.org/10.1145/3655693.3655722},
	doi = {10.1145/3655693.3655722},
	abstract = {Hunting for threats is of capital importance for security teams. Establishing multifaceted contexts around the evolving behaviours of threat actors is paramount for enabling threat hunting teams to tell the malicious from the benign. The MITRE ATT\&amp;CK framework is the state-of-art knowledge base for referencing how threat actors conduct their tactics, techniques and procedures. Despite the abstract concepts of techniques being well defined, it is challenging to hunt from an abstract technique concept to security event data. In this work, we develop a data driven knowledge base of threat actor behaviours called Indicators of Behaviour, that use semantic reasoning to infer threat actor behaviours. Unlike generalised techniques in MITRE ATT\&amp;CK, these behaviours can be queried from a low level indicator and the behaviour itself. We use MITRE’s Caldera platform to emulate threat actor behaviours and Sysmon for capturing security events and defining the knowledge base’s semantics. By utilising this approach, the semantic reasoner aids threat hunting teams by inferring threat actor behaviour chains from individual interconnected events.},
	booktitle = {Proceedings of the 2024 {European} {Interdisciplinary} {Cybersecurity} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Chetwyn, Robert Andrew and Eian, Martin and Jøsang, Audun},
	year = {2024},
	note = {event-place: Xanthi, Greece},
	keywords = {Caldera, CK, MITRE ATT\&amp, Threat Actor Behaviour, Threat Hunting, TTP},
	pages = {95--104},
}

@inproceedings{zhang_conceptscope_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {{ConceptScope}: {Organizing} and {Visualizing} {Knowledge} in {Documents} based on {Domain} {Ontology}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445396},
	doi = {10.1145/3411764.3445396},
	abstract = {Current text visualization techniques typically provide overviews of document content and structure using intrinsic properties such as term frequencies, co-occurrences, and sentence structures. Such visualizations lack conceptual overviews incorporating domain-relevant knowledge, needed when examining documents such as research articles or technical reports. To address this shortcoming, we present ConceptScope, a technique that utilizes a domain ontology to represent the conceptual relationships in a document in the form of a Bubble Treemap visualization. Multiple coordinated views of document structure and concept hierarchy with text overviews further aid document analysis. ConceptScope facilitates exploration and comparison of single and multiple documents respectively. We demonstrate ConceptScope by visualizing research articles and transcripts of technical presentations in computer science. In a comparative study with DocuBurst, a popular document visualization tool, ConceptScope was found to be more informative in exploring and comparing domain-specific documents, but less so when it came to documents that spanned multiple disciplines.},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Xiaoyu and Chandrasegaran, Senthil and Ma, Kwan-Liu},
	year = {2021},
	note = {event-place: Yokohama, Japan},
	keywords = {Ontology, Knowledge Representation, Visualization},
}

@inproceedings{li_spatialnli_2019,
	address = {New York, NY, USA},
	series = {{SIGSPATIAL} '19},
	title = {{SpatialNLI}: {A} {Spatial} {Domain} {Natural} {Language} {Interface} to {Databases} {Using} {Spatial} {Comprehension}},
	isbn = {978-1-4503-6909-1},
	url = {https://doi.org/10.1145/3347146.3359069},
	doi = {10.1145/3347146.3359069},
	abstract = {A natural language interface (NLI) to databases is an interface that translates a natural language question to a structured query that is executable by database management systems (DBMS). However, an NLI that is trained in the general domain is hard to apply in the spatial domain due to the idiosyncrasy and expressiveness of the spatial questions. Inspired by the machine comprehension model, we propose a spatial comprehension model that is able to recognize the meaning of spatial entities based on the semantics of the context. The spatial semantics learned from the spatial comprehension model is then injected to the natural language question to ease the burden of capturing the spatial-specific semantics. With our spatial comprehension model and information injection, our NLI for the spatial domain, named SpatialNLI, is able to capture the semantic structure of the question and translate it to the corresponding syntax of an executable query accurately. We also experimentally ascertain that SpatialNLI outperforms state-of-the-art methods.},
	booktitle = {Proceedings of the 27th {ACM} {SIGSPATIAL} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Li, Jingjing and Wang, Wenlu and Ku, Wei-Shinn and Tian, Yingtao and Wang, Haixun},
	year = {2019},
	note = {event-place: Chicago, IL, USA},
	keywords = {Natural Language Interface, Spatial Data Science},
	pages = {339--348},
}

@inproceedings{guan_event_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Event {GDR}: {Event}-{Centric} {Generative} {Document} {Retrieval}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3651500},
	doi = {10.1145/3589335.3651500},
	abstract = {Generative document retrieval, an emerging paradigm in information retrieval, learns to build connections between documents and identifiers within a single model, garnering significant attention. However, there are still two challenges: (1) neglecting inner-content correlation during document representation; (2) lacking explicit semantic structure during identifier construction. Nonetheless, events have enriched relations and well-defined taxonomy, which could facilitate addressing the above two challenges. Inspired by this, we propose Event GDR, an event-centric generative document retrieval model, integrating event knowledge into this task. Specifically, we utilize an exchange-then-reflection method based on multi-agents for event knowledge extraction. For document representation, we employ events and relations to model the document to guarantee the comprehensiveness and inner-content correlation. For identifier construction, we map the events to well-defined event taxonomy to construct the identifiers with explicit semantic structure. Our method achieves significant improvement over the baselines on two datasets, and also hopes to provide insights for future research.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Guan, Yong and Liu, Dingxiao and Ma, Jinchen and Peng, Hao and Wang, Xiaozhi and Hou, Lei and Li, Ru},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {large language model, event knowledge, generative document retrieval},
	pages = {975--978},
}

@inproceedings{zou_smart_2019,
	address = {New York, NY, USA},
	series = {{AICS} 2019},
	title = {Smart {System} {Studied}: {New} {Approaches} to {Natural} {Language} {Understanding}},
	isbn = {978-1-4503-7150-6},
	url = {https://doi.org/10.1145/3349341.3349360},
	doi = {10.1145/3349341.3349360},
	abstract = {This paper aims to focus on the smart system as optimized expert knowledge acquisition system as new approaches to natural language understanding system. This method can finish fine processing for any text segment instantly. The module's precision machining can adopt big production method that combines on the line first, complete coverage and accurate grasp each language point and knowledge point and original point even their respective combination. Its characteristics are teachers and students can use the text analyzed method to do the fine processing of the same knowledge module, and only in Chinese or English, through the selection of keywords and terminology and knowledge modules that can be used as the menu to be selected as the way to achieve knowledge with the system. The result is the learning environment that enables human-computer collaboration system namely smart system to optimize the expert knowledge acquisition and the natural language understanding as a research field that has great significance to human beings. Its significance is that this learning environment software based on the National Excellent Courses by using the language chess with the feature of the introduction on the knowledge big production mode for the textual knowledge module finishing at Peking University.},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Zou, Xiaohui and Zou, Shunpeng and Wang, Xiaoqun},
	year = {2019},
	note = {event-place: Wuhan, Hubei, China},
	keywords = {Natural Language Understanding, Smart System Studied},
	pages = {1--6},
}

@inproceedings{he_energonql_2020,
	address = {New York, NY, USA},
	series = {{BuildSys} '20},
	title = {{EnergonQL}: {A} {Building} {Independent} {Acquisitional} {Query} {Language} for {Portable} {Building} {Analytics}},
	isbn = {978-1-4503-8061-4},
	url = {https://doi.org/10.1145/3408308.3427979},
	doi = {10.1145/3408308.3427979},
	abstract = {Emerging building analytics heavily rely on data-driven machine learning algorithms. However, writing these analytics is still challenging: developers not only need to know what data is required but also where this data is in each individual building when writing applications. To bridge this gap between analytics and the actual resources in buildings, we present EnergonQL, a building independent acquisitional data query language that extracts data for building analytics with a declarative query processor. EnergonQL provides logic views of building resources that universally apply to all buildings, thus allowing portable building analytics across buildings. We evaluate EnergonQL with four different building analytics and show that with EnergonQL the line-of-code and development efforts can be effectively reduced.},
	booktitle = {Proceedings of the 7th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {He, Fang and Xu, Cheng and Xu, Yanhui and Hong, Dezhi and Wang, Dan},
	year = {2020},
	note = {event-place: Virtual Event, Japan},
	keywords = {Smart buildings, data analytics, Declarative query language},
	pages = {266--269},
}

@inproceedings{mohseni_framework_2022,
	address = {New York, NY, USA},
	series = {{ICISDM} '22},
	title = {A {Framework} for {Exploring} {Computational} {Models} of {Novelty} in {Unstructured} {Text}},
	isbn = {978-1-4503-9625-7},
	url = {https://doi.org/10.1145/3546157.3546164},
	doi = {10.1145/3546157.3546164},
	abstract = {Novelty modeling in unstructured text data is a research topic within the Natural Language Processing (NLP) Community. Effective novelty models can play a key role in providing relevant and interesting content to the users which is the central goal in many applications including education and recommender systems. This paper presents a framework for comparing different approaches and applications of computational models of novelty in unstructured text data. We focus on computational models that apply methods such as natural language processing and information theory. The framework provides an ontology for computational novelty with respect to the source of text data, methods for representing the data, and models for measuring novelty. We explore the value of the framework by applying it to research on computational novelty in news articles, research publications, books, and recipes. This framework is independent of the type of data in the items and can be used as a tool for researchers to study, compare, and extend existing computational novelty models and applications.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Information} {System} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Mohseni, Maryam and Maher, Mary Lou},
	year = {2022},
	note = {event-place: Silicon Valley, CA, USA},
	keywords = {NLP, Recommender systems, Computational models of novelty, Surprise, Unstructured text},
	pages = {36--45},
}

@inproceedings{wilsdorf_exploiting_2022,
	address = {Phoenix, Arizona},
	series = {{WSC} '21},
	title = {Exploiting provenance and ontologies in supporting best practices for simulation experiments: a case study on sensitivity analysis},
	abstract = {Simulation studies are intricate processes and user support for conducting more consistent, systematic, and efficient simulation studies is needed. Simulation experiments as one crucial part of a simulation study can benefit from semi-automatic method selection, parameterization, and execution. However, this largely depends on the context in which the experiment is conducted. Context information about a simulation study can be provided in form of provenance that documents which artifacts contributed in developing a simulation model. We present an approach that exploits provenance to support best practices for simulation experiments. The approach relies on 1) explicitly specified provenance information, 2) an ontology of methods, 3) best practices rules, and 4) integration with a previously developed experiment generation pipeline. We demonstrate our approach by conducting a sensitivity analysis experiment within a cell biological simulation study.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Wilsdorf, Pia and Fischer, Nadine and Haack, Fiete and Uhrmacher, Adelinde M.},
	year = {2022},
}

@inproceedings{li_computing_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {Computing {Views} of {OWL} {Ontologies} for the {Semantic} {Web}},
	isbn = {978-1-4503-8312-7},
	url = {https://doi.org/10.1145/3442381.3449881},
	doi = {10.1145/3442381.3449881},
	abstract = {This paper tackles the problem of computing views of OWL ontologies using a forgetting-based approach. In traditional relational databases, a view is a subset of a database, whereas in ontologies, a view is more than a subset; it contains not only axioms contained in the original ontology, but may also contain newly-derived axioms entailed by the original ontology (implicitly contained in the original ontology). Specifically, given an ontology , the signature of is the set of all the names in , and a view of is a new ontology obtained from using only part of ’s signature, namely the target signature, while preserving all logical entailments up to the target signature. Computing views of OWL ontologies is useful for Semantic Web applications such as ontology-based query answering, in a way that the view can be used as a substitute of the original ontology to answer queries formulated with the target signature, and information hiding, in the sense that it restricts users from viewing certain information of an ontology. Forgetting is a form of non-standard reasoning concerned with eliminating from an ontology a subset of its signature, namely the forgetting signature, in such a way that all logical entailments are preserved up to the target signature. Forgetting can thus be used as a means for computing views of OWL ontologies — the solution of forgetting a set of names from an ontology is the view of for the target signature . In this paper, we present a forgetting-based method for computing views of OWL ontologies specified in the description logic , the basic extended with role hierarchy, nominals and inverse roles. The method is terminating and sound. Despite the method not being complete, an evaluation with a prototype implementation of the method on a corpus of real-world ontologies has shown very good success rates. This is very useful from the perspective of the Semantic Web, as it provides knowledge engineers with a powerful tool for creating views of OWL ontologies.},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Li, Jiaqi and Wu, Xuan and Lu, Chang and Deng, Wenxing and Zhao, Yizheng},
	year = {2021},
	note = {event-place: Ljubljana, Slovenia},
	keywords = {Ontology, Description Logics, Forgetting, Semantics Web},
	pages = {2624--2635},
}

@inproceedings{akgun_approach_2018,
	address = {New York, NY, USA},
	series = {{ICISS} '18},
	title = {An {Approach} for {Information} {Discovery} {Using} {Ontology} {In} {Semantic} {Web} {Content}},
	isbn = {978-1-4503-6421-8},
	url = {https://doi.org/10.1145/3209914.3209940},
	doi = {10.1145/3209914.3209940},
	abstract = {Information searching techniques are rapidly developing as the World Wide Web (WWW) evolves. Along with the development of information technologies, the need for acquiring domain knowledge bases, accessing data sources and discovering insights increases. The advancements in knowledge discovery, information management and artificial intelligence require faster data processing, storing more data and developing more intelligent applications. This study provides an information discovery and data integration approach for linked open data in the semantic web. Using semantics embedded in ontologies, data available in knowledge bases can be enhanced to better serve the information needs of users. The entity relationships between resources and resource hierarchies represented as linked open data in semantic web provide semantically rich insights about the data and facilitates knowledge discovery. Graph theory methods can be utilized to enrich the features of data sets in semantic web. In this study, we propose an approach for integrating isolated data sources with semantic web by using ontologies to make them available for information discovery and enhancing the features of semantic data by using graph theory techniques.},
	booktitle = {Proceedings of the 1st {International} {Conference} on {Information} {Science} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Akgün, Ayhan and Ayvaz, Serkan},
	year = {2018},
	note = {event-place: Jeju, Republic of Korea},
	keywords = {ontology, information retrieval, semantic web, Graph},
	pages = {250--255},
}

@article{maree_multi-modality_2021,
	title = {Multi-modality {Search} and {Recommendation} on {Palestinian} {Cultural} {Heritage} {Based} on the {Holy}-{Land} {Ontology} and {Extrinsic} {Semantic} {Resources}},
	volume = {14},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3447523},
	doi = {10.1145/3447523},
	abstract = {The Cultural Heritage (CH) sector and its associated tourism services have been affected notably by the advancement of the Internet as well as the explosive growth of smartphones and other handheld devices. These days, visitors can access reliable CH content using Web and mobile-based interfaces. However, conventional CH systems still lack the ability to provide meaningful semantically overt results that precisely meet user information needs in this domain. In addition, they often ignore the user search context and experience, which hinders their ability to adapt their behavior to the preferences, tasks, interests, and other user functionalities. In this article, we aim to address the issue of designing a precision-oriented multilingual and multi-criteria semantic-based mobile recommender system specifically targeting Palestine's CH, a country with great historical and cultural importance. We aim to better facilitate users’ access to CH content by providing them with multiple search functionalities. In this context, a user can search for relevant information using keywords (a.k.a. tags) or sentence-like queries and the system retrieves all relevant documents based on their semantic similarity. A second option is to search using current location information to retrieve correlated historical places and events. Finally, starting from a picture of interest, a third option makes it possible to extract captions describing its content that can be used to search for additional contextually relevant information. Additionally, the proposed system aims at personalizing users’ experience through progressively delivering output that meets their information needs based on a number of parameters such as users' logging data, interests, previous searches, and location-based information. A prototype of the proposed system has been developed and tested using Android smartphones and a manually constructed ontology enriched with CH links to the Art \&amp; Architecture Thesaurus (AAT) and DBpedia. By comparing our system with similar systems in this domain, findings demonstrate that it provides additional search features and functionalities to users. The proposed Holy-Land ontology is the first of its kind attempting to encode knowledge about Palestine's CH. It plays a crucial role in our proposal, serving as a pivotal entity in the combination of language-based, location-based, and visual-based retrieval strategies.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Maree, Mohammed and Rattrout, Amjad and Altawil, Muhanad and Belkhatir, Mohammed},
	month = jul,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cultural heritage, semantic similarity, content-based image retrieval, hybrid recommendation, knowledge-based search, manually constructed ontology},
}

@inproceedings{thinyane_smart_2021,
	address = {New York, NY, USA},
	series = {{SIN} 2020},
	title = {{SMART} {Citizen} {Cyber} {Resilience} ({SC2R}) {Ontology}},
	isbn = {978-1-4503-8751-4},
	url = {https://doi.org/10.1145/3433174.3433617},
	doi = {10.1145/3433174.3433617},
	abstract = {Adverse cyber incidents are some of the top risks currently facing the global community. Cybersecurity frameworks and models formulated to mitigate these risks are typically framed from the organizational perspective of governments and the private sector. Further, they are traditionally techno-centric solutions framed from a cybersecurity perspective towards prevention of risks; only recently are they incorporating resilience perspectives towards anticipation of risks and positive adaptation during adverse cyber incidents. This research makes advances on human-centric cyber resilience - from the perspective of citizens and centered on citizens’ multi-dimensional experience of adverse cyber incidents. It considers the goal of cyber resilience, from the capabilitarian perspective, as enhancing the cyber capabilities of individuals and achieving positive adaptation in the face of adverse cyber incidents. This paper presents an ontology that is formulated to formalize individuals’ cyber resilience. The paper motivates the need for such an ontology and discusses the constitutive concepts. Finally, it shows how this ontology can support the realization of cyber resilience for individual citizens.},
	booktitle = {13th {International} {Conference} on {Security} of {Information} and {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Thinyane, Mamello and Christine, Debora},
	year = {2021},
	note = {event-place: Merkez, Turkey},
	keywords = {Ontologies, Cybersecurity, Cyber resilience, Human-centric cybersecurity},
}

@inproceedings{bourahla_knowledge_2021,
	address = {Richland, SC},
	series = {{AAMAS} '21},
	title = {Knowledge {Improvement} and {Diversity} under {Interaction}-{Driven} {Adaptation} of {Learned} {Ontologies}},
	isbn = {978-1-4503-8307-3},
	abstract = {When agents independently learn knowledge, such as ontologies, about their environment, it may be diverse, incorrect or incomplete. This knowledge heterogeneity could lead agents to disagree, thus hindering their cooperation. Existing approaches usually deal with this interaction problem by relating ontologies, without modifying them, or, on the contrary, by focusing on building common knowledge. Here, we consider agents adapting ontologies learned from the environment in order to agree with each other when cooperating. In this scenario, fundamental questions arise: Do they achieve successful interaction? Can this process improve knowledge correctness? Do all agents end up with the same ontology? To answer these questions, we design a two-stage experiment. First, agents learn to take decisions about the environment by classifying objects and the learned classifiers are turned into ontologies. In the second stage, agents interact with each other to agree on the decisions to take and modify their ontologies accordingly. We show that agents indeed reduce interaction failure, most of the time they improve the accuracy of their knowledge about the environment, and they do not necessarily opt for the same ontology.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Bourahla, Yasser and Atencia, Manuel and Euzenat, Jérôme},
	year = {2021},
	note = {event-place: Virtual Event, United Kingdom},
	keywords = {ontologies, knowledge diversity, multi-agent learning, multi-agent social simulation},
	pages = {242--250},
}

@inproceedings{hussein_hybrid_2025,
	address = {New York, NY, USA},
	series = {{DocEng} '25},
	title = {A {Hybrid}, {Neuro}-symbolic {Approach} for {Scholarly} {Knowledge} {Organization}},
	isbn = {979-8-4007-1351-4},
	url = {https://doi.org/10.1145/3704268.3742700},
	doi = {10.1145/3704268.3742700},
	abstract = {The rapid development of generative AI leveraging neural models, particularly with the introduction of large language models (LLMs), has fundamentally advanced natural language processing and generation. However, such neural models are non-deterministic, opaque, and tend to confabulate. Knowledge Graphs (KGs) on the other hand contain factual information represented in a symbolic way for humans and machines following formal knowledge representation formalisms. However, the creation and curation of KGs is time-consuming, cumbersome, and resource-demanding. A key research challenge now is how to synergistically combine both formalisms with the human in the loop (Hybrid AI) to obtain structured and machine-processable knowledge in a scalable way. We introduce an approach for a tight integration of Humans, Neural Models (LLM), and Symbolic Representations (KG) for the semiautomatic creation and curation of Scholarly Knowledge Graphs. Our approach, while demonstrated in the scholarly context, establishes generalizable principles for neuro-symbolic integration that can be adapted to other domains. We implement and integrate our approach comprising an intelligent user interface and prompt templates for interaction with an LLM in the Open Research Knowledge Graph. We perform a thorough analysis of our approach and implementation with a user evaluation to assess the merits of the neuro-symbolic, hybrid approach for organizing scholarly knowledge.},
	booktitle = {Proceedings of the 2025 {ACM} {Symposium} on {Document} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Hussein, Hassan and Oelen, Allarad and Auer, Sören},
	year = {2025},
	note = {event-place: Nottingham, United Kingdom},
}

@article{bellomarini_exploiting_2022,
	title = {Exploiting the {Power} of {Equality}-{Generating} {Dependencies} in {Ontological} {Reasoning}},
	volume = {15},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3565838.3565850},
	doi = {10.14778/3565838.3565850},
	abstract = {Equality-generating dependencies (EGDs) allow to fully exploit the power of existential quantification in ontological reasoning settings modeled via Tuple-Generating Dependencies (TGDs), by enabling value-assignment or forcing the equivalence of fresh symbols. These capabilities are at the core of many common reasoning tasks, including graph traversals, clustering, data matching and data fusion, and many more related real-world scenarios.However, the interplay of TGDs and EGDs is known to lead to undecidability or intractability of query answering in tractable Datalog+/- fragments, like Warded Datalog+/-, for which, in the sole presence of TGDs, query answering is PTIME in data complexity. Restrictions of equality constraints, like separable EGDs, have been studied, but all achieve decidability at the cost of limited expressive power, which makes them unsuitable for the mentioned tasks.This paper introduces the class of "harmless" EGDs, that subsume separable EGDs and allow to model a very broad class of tasks. We contribute a sufficient syntactic condition for testing harmlessness, an undecidable task in general. We argue that in Warded Datalog+/- with harmless EGDs, ontological reasoning is decidable and PTIME. From such theoretical underpinnings, we develop novel chase-based techniques for reasoning with harmless EGDs and present an implementation within the Vadalog system, a state-of-the-art Datalog-based reasoner. We provide full-scale experimental evaluation and comparative analysis.},
	number = {13},
	journal = {Proc. VLDB Endow.},
	author = {Bellomarini, Luigi and Benedetto, Davide and Brandetti, Matteo and Sallinger, Emanuel},
	month = sep,
	year = {2022},
	note = {Publisher: VLDB Endowment},
	pages = {3976--3988},
}

@inproceedings{seifer_empirical_2019,
	address = {New York, NY, USA},
	series = {{SLE} 2019},
	title = {Empirical study on the usage of graph query languages in open source {Java} projects},
	isbn = {978-1-4503-6981-7},
	url = {https://doi.org/10.1145/3357766.3359541},
	doi = {10.1145/3357766.3359541},
	abstract = {Graph data models are interesting in various domains, in part because of the intuitiveness and flexibility they offer compared to relational models. Specialized query languages, such as Cypher for property graphs or SPARQL for RDF, facilitate their use. In this paper, we present an empirical study on the usage of graph-based query languages in open-source Java projects on GitHub. We investigate the usage of SPARQL, Cypher, Gremlin and GraphQL in terms of popularity and their development over time. We select repositories based on dependencies related to these technologies and employ various popularity and source-code based filters and ranking features for a targeted selection of projects. For the concrete languages SPARQL and Cypher, we analyze the activity of repositories over time. For SPARQL, we investigate common application domains, query use and existence of ontological data modeling in applications that query for concrete instance data. Our results show, that the usage of graph query languages in open-source projects increased over the last years, with SPARQL and Cypher being by far the most popular. SPARQL projects are more active in terms of query related artifact changes and unique developers involved, but Cypher is catching up. Relatively few applications use SPARQL to query for concrete instance data: A majority of those applications employ multiple different ontologies, including project and domain specific ones. Common application domains are management systems and data visualization tools.},
	booktitle = {Proceedings of the 12th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Seifer, Philipp and Härtel, Johannes and Leinberger, Martin and Lämmel, Ralf and Staab, Steffen},
	year = {2019},
	note = {event-place: Athens, Greece},
	keywords = {SPARQL, GraphQL, Empirical Study, Cypher, GitHub, Graphs, Gremlin, Query Languages},
	pages = {152--166},
}

@article{kirchhof_model-driven_2022,
	title = {Model-driven {Self}-adaptive {Deployment} of {Internet} of {Things} {Applications} with {Automated} {Modification} {Proposals}},
	volume = {3},
	url = {https://doi.org/10.1145/3549553},
	doi = {10.1145/3549553},
	abstract = {Today’s Internet of Things (IoT) applications are mostly developed as a bundle of hardware and associated software. Future cross-manufacturer app stores for IoT applications will require that the strong coupling of hardware and software is loosened. In the resulting IoT applications, a quintessential challenge is the effective and efficient deployment of IoT software components across variable networks of heterogeneous devices. Current research focuses on computing whether deployment requirements fit the intended target devices instead of assisting users in successfully deploying IoT applications by suggesting deployment requirement relaxations or hardware alternatives. This can make successfully deploying large-scale IoT applications a costly trial-and-error endeavor. To mitigate this, we have devised a method for providing such deployment suggestions based on search and backtracking. This can make deploying IoT applications more effective and more efficient, which, ultimately, eases reducing the complexity of deploying the software surrounding us.},
	number = {4},
	journal = {ACM Trans. Internet Things},
	author = {Kirchhof, Jörg Christian and Kleiss, Anno and Rumpe, Bernhard and Schmalzing, David and Schneider, Philipp and Wortmann, Andreas},
	month = sep,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Internet of Things, model-driven engineering, deployment, architecture description languages},
}

@inproceedings{vinogradov_information_2020,
	address = {New York, NY, USA},
	series = {{CSIS}'2019},
	title = {Information extraction tasks in public administration domain: {ISIDA}-{T} natural language processing system},
	isbn = {978-1-4503-7670-9},
	url = {https://doi.org/10.1145/3373722.3373782},
	doi = {10.1145/3373722.3373782},
	abstract = {This article represents approaches of the artificial intelligence methods, used in public administration. An overview of various technologies of artificial intelligence applications in the field of public administration and related fields is given. All of these research directions are particularly relevant to the task of digital technologies (including artificial intelligence) growth to create an efficient and competitive digital economics in Russia. Among the modern intellectual technologies that allow solving the widest range of tasks, an important role plays technologies related to natural language text processing - nonstructured NL-texts are essential segment of data, used for analysis and decision-making tasks (in terms of data volume, of course, video data have a leading position, however, they are usually suitable for solving tactical but not strategic management tasks). The article provides an overview of the existing methods of natural language processing and their practical application to the tasks of public administration. An integrated approach to the natural language processing tools using for solving practical problems in the field of public administration is considered on the example of the ISIDA-T system for extracting information from natural language texts, developed at the Artificial Intelligence Research Center PSI RAS. The system under consideration is distinguished by a modular approach to the pre-processing of unstructured text and the possibility of manual adjustment for a specific extraction task. This technological solution gives the necessary flexibility and ease of use. The system consists of configurable text preprocessing, linguistic analysis, target information retrieval and output of the results in user-friendly form modules. One of the important components of the system is an integrated knowledge resource that allows quickly and efficiently adjust the system to the specifics of the relevant subject area. An approach to the application of the ISIDA-T system to the task of fact information extraction (as the most important for analyzing the situation and subsequent management decisions) is proposed using the example of information extraction about resignations and appointments from news feeds.},
	booktitle = {Proceedings of the {XI} {International} {Scientific} {Conference} {Communicative} {Strategies} of the {Information} {Society}},
	publisher = {Association for Computing Machinery},
	author = {Vinogradov, Andrei and Kurshev, Evgeny and Vlasova, Natalia and Podobryaev, Alexey},
	year = {2020},
	note = {event-place: St. Petersburg, Russian Federation},
	keywords = {Information extraction, Natural language processing, Artificial intelligence, Public administration, Digital economy},
}

@inproceedings{mcgill_construction_2020,
	address = {New York, NY, USA},
	series = {{ICER} '20},
	title = {Construction of a {Taxonomy} for {Tools}, {Languages}, and {Environments} across {Computing} {Education}},
	isbn = {978-1-4503-7092-9},
	url = {https://doi.org/10.1145/3372782.3406258},
	doi = {10.1145/3372782.3406258},
	abstract = {The sheer number of tools, languages, and environments (TLEs) used in computing education has proliferated in the last few years as more tools are developed to meet new demands of the growing amount of K-12 computing education that has been undertaken. However, there is little formalized language at either the K-12 or post-secondary level that provides for a way to classify these TLEs for discussing research and for classifying in databases.In this research study, we step through a formal process for building a taxonomy for TLEs. As part of the supporting research, we first discuss the importance of taxonomies and classification systems in computing education, provide a formal method for building a taxonomy, and provide working definitions of TLEs based on previous literature. This is followed by a systematic literature review using a widely-accepted methodology for finding articles that have examined TLEs in primary, secondary, and post-secondary computing education. This literature review focuses on studies that looked at multiple TLEs and specifically attempted to classify or categorize them. We then propose a new taxonomy for classifying TLEs and provide definitions and samples for each category. This is followed by a discussion of the next steps in vetting the taxonomy and the challenges and issues that need to be considered when evaluating it for classifying TLEs in computing education.},
	booktitle = {Proceedings of the 2020 {ACM} {Conference} on {International} {Computing} {Education} {Research}},
	publisher = {Association for Computing Machinery},
	author = {McGill, Monica M. and Decker, Adrienne},
	year = {2020},
	note = {event-place: Virtual Event, New Zealand},
	keywords = {ontology, taxonomy, classification, education, tools, literature review, languages, computing, environments, k-12, post-secondary, primary, secondary},
	pages = {124--135},
}

@inproceedings{shuttleworth_towards_2022,
	address = {Phoenix, Arizona},
	series = {{WSC} '21},
	title = {Towards semi-automatic model specification},
	abstract = {This paper presents a natural language understanding (NLU) approach to transition a description of a phenomenon towards a simulation specification. As multidisciplinary endeavors using simulations increase, the need for teams to better communicate and make non-modelers active participants on the process increases. We focus on semi-automating the model conceptualization process towards the creation of a specification as it is one of the most challenging steps in collaborations. The approach relies on NLU processing of narratives, create a model that captures concepts and relationships, and finally provide a specification of a simulation implementation. An initial definition set and grammatical rules are proposed to formalize this process. These are followed by a Design of Experiments (DoE) to test the NLU model accuracy and a test case that generates Agent-Based Model (ABM) conceptualizations and specifications. We provide a discussion on the advantages and limitations of using NLUs for model conceptualization and specification processes.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Shuttleworth, David and Padilla, Jose J.},
	year = {2022},
}

@inproceedings{sore_towards_2024,
	address = {New York, NY, USA},
	series = {{ICMLC} '24},
	title = {Towards a {More} {Generic} and {Elastic} {Metadata} {Management} {Model} in a {Data} {Lake} {Environment}},
	isbn = {979-8-4007-0923-4},
	url = {https://doi.org/10.1145/3651671.3651773},
	doi = {10.1145/3651671.3651773},
	abstract = {The evolution of the vast amount of heterogeneous data sources is leading to the emergence of several new concepts. One of the best-known concepts that is emerging as a new and trending topic in the big data space is the data lake. This is a central repository that stores heterogeneous data sources in their native format, without any predefined schema. In the absence of an enforced schema, effective metadata management based on metadata models remains an active research topic to address the problems associated with the data lake: the "data swamp". The analysis of existing metadata models shows that there is no comprehensive model among them. In this paper, we present a generic and scalable metadata model, which refers to the ability to dynamically provision computing resources based on demand and to resize resources as needed during metadata integration. Our approach will be based on a functional architecture of the data lake, along with a set of features that promote the generality of the metadata model.CCS CONCEPTS: Information systems→ Data management systems→ Information integration → Entity resolution},
	booktitle = {Proceedings of the 2024 16th {International} {Conference} on {Machine} {Learning} and {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Sore, Safiatou S. and Ouedraogo, T. Frederic T.Frédric and Bikienga, Moustapha M. and Traore, Yaya Y.},
	year = {2024},
	note = {event-place: Shenzhen, China},
	keywords = {metadata, scalability, data lake, elasticity},
	pages = {44--51},
}

@inproceedings{zhang_fast_2021,
	address = {New York, NY, USA},
	series = {{SIGIR} '21},
	title = {Fast {Attention}-based {Learning}-{To}-{Rank} {Model} for {Structured} {Map} {Search}},
	isbn = {978-1-4503-8037-9},
	url = {https://doi.org/10.1145/3404835.3462904},
	doi = {10.1145/3404835.3462904},
	abstract = {Recent works show that Transformer-based learning-to-rank (LTR) approaches can outperform previous well-established ranking methods, such as gradient-boosted decision trees (GBDT), on document and passage re-ranking problems. A common assumption in these works is that the query and the result documents are comprised of purely textual information without explicit structure. In map search, the relevance of results is determined based on rich heterogeneous features - textual features derived from the query and the results, geospatial features such as proximity of a result to the user, structured features reflecting the address format of the result, and the perceived structure of the query. In this work, we propose a novel deep neural network LTR architecture, capable of seamlessly handling heterogeneous inputs, similar to GBDT-based methods. At the same time, unlike GBDT, the architecture does not require human input via (numerous) carefully-crafted features. Instead, features are inferred through a self-attention mechanism. Our model implements two lightweight attention layers optimized for ranking: the first layer computes query-result similarities, the second implements listwise ranking inference. We perform evaluation on several single language and one multilingual dataset. Our model outperforms by a wide margin other Transformer-based ranking architectures and has equal or better performance than GBDT models. Equally important, runtime inference is orders of magnitude faster than other Transformer architectures, significantly reducing hardware serving costs. The model is a low-cost alternative suitable to power ranking in industrial map search engines across a variety of languages and markets.},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Chiqun and Evans, Michael R. and Lepikhin, Max and Yankov, Dragomir},
	year = {2021},
	note = {event-place: Virtual Event, Canada},
	keywords = {information retrieval, geocoding, language understanding, map search, ranking system},
	pages = {942--951},
}

@inproceedings{donald_towards_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {Towards a {Semantic} {Approach} for {Linked} {Dataspace}, {Model} and {Data} {Cards}},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587659},
	doi = {10.1145/3543873.3587659},
	abstract = {The vast majority of artificial intelligence practitioners overlook the importance of documentation when building and publishing models and datasets. However, due to the recent trend in the explainability and fairness of AI models, several frameworks have been proposed such as Model Cards, and Data Cards, among others, to help in the appropriate re-usage of those models and datasets. In addition, because of the introduction of the dataspace concept for similar datasets in one place, there is potential that similar Model Cards, Data Cards, Service Cards, and Dataspace Cards can be linked to extract helpful information for better decision-making about which model and data can be used for a specific application. This paper reviews the case for considering a Semantic Web approach for exchanging Model/Data Cards as Linked Data or knowledge graphs in a dataspace, making them machine-readable. We discuss the basic concepts and propose a schema for linking Data Cards and Model Cards within a dataspace. In addition, we introduce the concept of a dataspace card which can be a starting point for extracting knowledge about models and datasets in a dataspace. This helps in building trust and reuse of models and data among companies and individuals participating as publishers or consumers of such assets.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Donald, Andy and Galanopoulos, Apostolos and Curry, Edward and Muñoz, Emir and Ullah, Ihsan and Waskow, M. A. and Dabrowski, Maciej and Kalra, Manan},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {Semantic Web, AI Documentation, Data Cards, Dataspace Cards, Model Cards, Service Cards},
	pages = {1468--1473},
}

@inproceedings{vanommeslaeghe_validation_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Validation and uncertainty in model-based design space exploration: an experience report},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561581},
	doi = {10.1145/3550356.3561581},
	abstract = {Model-based systems engineering (MBSE) techniques can help manage the growing complexity in the design and development of cyber-physical systems, and can even allow for the optimization of a system under design in simulation. However, models are always an abstraction of the real-world systems they represent. This introduces uncertainty at the model level, which affects the validity of simulation results, and thus also the results of the optimization. This, together with variations in real-world system parameters, significantly complicates the validation of simulation and optimization results. In this experience report, we first use a descriptive process model to describe our efforts to validate the results of a model-based design space exploration (DSE) process given this uncertainty. After this, we discuss lessons learned and insights gained, and identify future challenges. We present a possible prescriptive process model for future validation efforts, which specifically takes into account uncertainty.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Vanommeslaeghe, Yon and Ceulemans, David and van Acker, Bert and Denil, Joachim and Derammelaere, Stijn and De Meulenaere, Paul},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {cyber-physical systems, model-based systems engineering, validation, uncertainty, design space exploration},
	pages = {702--711},
}

@inproceedings{teixeira_interplay_2021,
	address = {New York, NY, USA},
	series = {{SAC} '21},
	title = {The interplay of a conversational ontology and {AI} planning for health dialogue management},
	isbn = {978-1-4503-8104-8},
	url = {https://doi.org/10.1145/3412841.3441942},
	doi = {10.1145/3412841.3441942},
	abstract = {Health dialogue systems are required to respect some special requirements such as predictability and reliability. While knowledge based approaches still seem to be the most appropriate for these systems, the automated generation of reliable policies remains an open problem. This work proposes an approach that integrates a conversational ontology (Convology) and Artificial Intelligence planning with the aim of automating the generation of a dialogue manager capable of handling goal-oriented dialogues for the health domain. The resulting dialogue manager is aimed to be integrated into a suitable architecture that provides the natural language components. We illustrate our approach by describing how it has been implemented into PuffBot, a multi-turn goal-oriented conversational agent for supporting patients affected by asthma.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Teixeira, Milene Santos and Maran, Vinícius and Dragoni, Mauro},
	year = {2021},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {automated planning, conversational ontology, dialogue management, health dialogue},
	pages = {611--619},
}

@inproceedings{jirkovsky_facilitation_2020,
	address = {New York, NY, USA},
	series = {{iiWAS2019}},
	title = {Facilitation of {Domain}-{Specific} {Data} {Models} {Design} using {Semantic} {Web} {Technologies} for {Manufacturing}},
	isbn = {978-1-4503-7179-7},
	url = {https://doi.org/10.1145/3366030.3366111},
	doi = {10.1145/3366030.3366111},
	abstract = {Modern manufacturing faces a challenge of integrating data models from various sources/domains which may differ both semantically and technically when particular domain specific data models are designed by different users and stored in different formats. This paper introduces an approach for facilitating the design of domain-specific data models using semantic web technologies. In this approach, all the information required for managing the production (including a description of a product, processes involved in the production, and existing resources and their specifications) is captured in an ontology. The proposed Product, Process, and Resource (PPR) ontology defines fundamental conceptualization of the production that can be easily applied to the arbitrary domain. Application of the PPR ontology is demonstrated in the case of simple truck assembling by means of robots. Capturing the knowledge in the form of ontology provides the advantage of employing supporting tools such as reasoners for consistency checking or query languages for information extraction. The paper demonstrates the utilization of SQWRL for searching resources suitable to manipulate given truck parts on the basis of semantic matching between properties of particular elements.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Jirkovský, Václav and Šebek, Ondřej and Kadera, Petr and Burget, Pavel and Knoch, Sönke and Becker, Tilman},
	year = {2020},
	note = {event-place: Munich, Germany},
	keywords = {Ontology, Data Model Design, Industrial Automation, Semantic Matchmaking},
	pages = {649--653},
}

@inproceedings{e_samaridi_designing_2021,
	address = {New York, NY, USA},
	series = {{PCI} '20},
	title = {Designing a {Greek} {Electronic} {Dictionary} based on {Ontology}},
	isbn = {978-1-4503-8897-9},
	url = {https://doi.org/10.1145/3437120.3437311},
	doi = {10.1145/3437120.3437311},
	abstract = {In this paper we examine the design of a conceptual dictionary of the modern Greek language with intelligent features, which will offer possibilities of semantic integration and interoperability in an automatic and secure way, connecting heterogeneous systems and approaches in the field of engineering and technologies under the light of a standard and standardized organization and coding of data that come from structured and semi-structured information sources.},
	booktitle = {Proceedings of the 24th {Pan}-{Hellenic} {Conference} on {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {E. Samaridi, Nikoletta and N. Karanikolas, Nikitas and C. Papakitsos, Evangelos and Papoutsidakis, Michail},
	year = {2021},
	note = {event-place: Athens, Greece},
	keywords = {ontology, taxonomy, interoperability, concepts, binary relations, dictionary, hierarchy, inheritance, semantic integration},
	pages = {223--225},
}

@article{cunningham_grounds_2023,
	title = {On the {Grounds} of {Solutionism}: {Ontologies} of {Blackness} and {HCI}},
	volume = {30},
	issn = {1073-0516},
	url = {https://doi.org/10.1145/3557890},
	doi = {10.1145/3557890},
	abstract = {Why is the solution the end point to a problem? While many in HCI and design have examined the impulse to solve problems–the solutionist or techno-solutionist mindset–we examine the logic that binds the solution and the problem together as a pair. Focusing on the timely and consequential problem of systemic racial injustice, we think through the paradoxical possibility that the pairing of the problem and solution (so often treated as the default in design and HCI) perpetuates the very conditions we seek to improve. With Calvin Warren’s profound Afro-pessimism, we recognize how the tools used to solve structural inequities around Black life are constructed with inequities themselves. The problem-solution, therefore, is a dead end. We use this paradox as an invitation to rethink ongoing efforts to seek equity and justice more broadly, setting out a fragile but hopeful path for HCI and design.},
	number = {2},
	journal = {ACM Trans. Comput.-Hum. Interact.},
	author = {Cunningham, Jay and Benabdallah, Gabrielle and Rosner, Daniela and Taylor, Alex},
	month = apr,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {theory, design problems, Solutionism, solutions},
}

@article{goy_building_2020,
	title = {Building {Semantic} {Metadata} for {Historical} {Archives} through an {Ontology}-driven {User} {Interface}},
	volume = {13},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3402440},
	doi = {10.1145/3402440},
	abstract = {Historical archives represent an immense wealth, the potential of which is endangered by the lack of effective management and access tools. We believe that this issue can be faced by providing archive catalogs with a semantic layer, containing rich semantic metadata, representing the content of documents in a full-fledged formal machine-readable format. In this article, we present the contribution offered in this direction by the PRiSMHA project, in which the conceptual vocabulary of the semantic layer is represented by computational ontologies. However, acquiring semantic knowledge represents a well-known bottleneck for knowledge-based systems; to solve this problem, PRiSMHA relies on a crowdsourcing collaborative model, i.e., an online community of users who collaborate in building semantic representations of the content of archival documents. In this perspective, this article aims at answering the following research question: Starting from the axioms characterizing concepts in the computational ontology underlying the system, how can we derive a user interface enabling users to formally represent the content of archival documents by exploiting the conceptual vocabulary provided by the ontology?Our solution includes the following steps: (a) a manually defined configuration, acting as a pre-filter, to hide “unsuited” classes, properties, and relations; (b) an algorithm, combining heuristics and reasoning, which extracts from the ontology all and only the “compatible” properties and relations, given an entity (event) type; and (c) a set of strategies to rank, group, and present the entity (event) properties and relations, based on the results of a study with users. This integrated solution enabled us to design an ontology-driven user interface enabling users to characterize entities, and in particular (historical) events, on the basis of the vocabulary provided by the ontology.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Goy, Annamaria and Colla, Davide and Magro, Diego and Accornero, Cristina and Loreto, Fabrizio and Radicioni, Daniele Paolo},
	month = aug,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {computational ontologies, crowdsourcing platform, historical archives, Ontology-driven user interfaces},
}

@inproceedings{zorgati_qoc_2020,
	address = {New York, NY, USA},
	series = {{IDEAS} '20},
	title = {{QoC} enhanced semantic {IoT} model},
	isbn = {978-1-4503-7503-0},
	url = {https://doi.org/10.1145/3410566.3410610},
	doi = {10.1145/3410566.3410610},
	abstract = {The miniaturization of computers, coupled with a constant increase in computing power, led to the emergence of new sources of context information. We are facing a new paradigm, the Internet of Things (IoT). Today, this latter improves the quality of life in multiple areas. However, the heterogeneity of objects used in such environments makes their interoperability difficult. In addition, the observations produced by context providers (connected objects) are generated with different vocabularies and data formats. This heterogeneity of technologies in the IoT world makes it necessary to adopt generic solutions. Therefore, it is important to transform the raw data from these context producers into knowledge and information based on ontologies. The use of ontologies solves the challenges of heterogeneity and interoperability of IoT systems. In this paper, we propose a semantic IoT model that aims to overcome the semantic interoperability challenges introduced by the variety of objects potentially used in IoT systems. Furthermore, we enhanced this ontology with quality of context meta-data. These meta-data helps in dealing with imperfection and inconsistency of the collected IoT data.},
	booktitle = {Proceedings of the 24th {Symposium} on {International} {Database} {Engineering} \&amp; {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Zorgati, Hela and Djemaa, Raoudha Ben and Amor, Ikram Amous Ben and Sedes, Florence},
	year = {2020},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {internet of things, semantic model, ontology reuse, QoC meta-data},
}

@inproceedings{finidori_configuring_2020,
	address = {USA},
	series = {{PLoP} '18},
	title = {Configuring patterns and pattern languages for systemic inquiry and design},
	abstract = {This paper builds on work relating to pattern languages for social change, such as in the papers titled Fourth generation pattern languages - patterns as epistemic threads for systemic orientation, and Pattern Literacy in support of Systems Literacy presented to the Systems Science and Pattern Language communities between 2015 and 2017.It is part of an endeavor to bring pattern thinking and systems thinking, or pattern science and systems science, closer to each other, in order to further introduce pattern thinking and pattern language in the design, assessment and orientation of our socio-technological and socioenvironmental systems, large or small, to better address the societal issues of our time. It complements several initiatives to put pattern languages at the service of sustainability and societal change, and to introduce pattern thinking and pattern language into systems thinking and systemic design.My broader aim is to enhance the innate patterning capability of human beings and thus an overall pattern literacy in support of systems literacy. Pattern literacy manifests our ability to grasp, learn, assemble, represent and mobilize patterns to make-sense of, converse about and shape our world(s). Systems literacy manifests our ability to interrogate and attempt to understand the relationships among systems wholes and parts, and the mechanisms that affect and shape our world(s), in part or as a whole.In this paper, I explore how a systemic approach to patterns and pattern language could support systemic inquiry and systemic design, and more generally the advancement of pattern language.In particular, I discuss the extension of the act of design to encompass the systemic inquiry that motivates a design and the on-going monitoring of the fitness of a design to its intended purpose. I examine the multiple facets and understandings of the concept of pattern and show how they can be reconciled to include both the inquiry or observational/informational aspects and the design aspects of patterns in a larger systems framework. In this light, I reexamine the appropriateness of the pattern expressed in problem-solution form in the context of complex systems, and the notion of generativity, and I propose ways forward for extended definitions and pattern forms.},
	booktitle = {Proceedings of the 25th {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {The Hillside Group},
	author = {Finidori, Helene},
	year = {2020},
	note = {event-place: Portland, Oregon},
	keywords = {action research, complex systems, semiotics, boundary objects, complex adaptive modeling, participatory inquiry, pattern language, pattern literacy},
}

@article{tseng_co-ml_2024,
	title = {Co-{ML}: {Collaborative} {Machine} {Learning} {Model} {Building} for {Developing} {Dataset} {Design} {Practices}},
	volume = {24},
	url = {https://doi.org/10.1145/3641552},
	doi = {10.1145/3641552},
	abstract = {Machine learning (ML) models are fundamentally shaped by data, and building inclusive ML systems requires significant considerations around how to design representative datasets. Yet, few novice-oriented ML modeling tools are designed to foster hands-on learning of dataset design practices, including how to design for data diversity and inspect for data quality. To this end, we outline a set of four data design practices (DDPs) for designing inclusive ML models and share how we designed a tablet-based application called Co-ML to foster learning of DDPs through a collaborative ML model building experience. With Co-ML, beginners can build image classifiers through a distributed experience where data is synchronized across multiple devices, enabling multiple users to iteratively refine ML datasets in discussion and coordination with their peers. We deployed Co-ML in a 2-week-long educational AIML Summer Camp, where youth ages 13–18 worked in groups to build custom ML-powered mobile applications. Our analysis reveals how multi-user model building with Co-ML, in the context of student-driven projects created during the summer camp, supported development of DDPs including incorporating data diversity, evaluating model performance, and inspecting for data quality. Additionally, we found that students’ attempts to improve model performance often prioritized learnability over class balance. Through this work, we highlight how the combination of collaboration, model testing interfaces, and student-driven projects can empower learners to actively engage in exploring the role of data in ML systems.},
	number = {2},
	journal = {ACM Trans. Comput. Educ.},
	author = {Tseng, Tiffany and Davidson, Matt J. and Morales-Navarro, Luis and Chen, Jennifer King and Delaney, Victoria and Leibowitz, Mark and Beason, Jazbo and Shapiro, R. Benjamin},
	month = apr,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Machine learning, collaboration, data science, computing education},
}

@inproceedings{renault_using_2018,
	address = {New York, NY, USA},
	series = {{SBQS} '18},
	title = {Using an {Ontology}-based {Approach} for {Integrating} {Applications} to support {Software} {Processes}},
	isbn = {978-1-4503-6565-9},
	url = {https://doi.org/10.1145/3275245.3275269},
	doi = {10.1145/3275245.3275269},
	abstract = {Software organizations use several applications to support their software processes. To properly support the software processes, applications should be integrated at different layers (data, service, and process). Moreover, the integration should cover semantic aspects. Therefore, an approach that provides guidelines on how to perform integration at different layers addressing semantic aspects can be helpful. This paper presents an extension of the Ontology-based Approach for Semantic Integration (OBA-SI), focusing on semantic integration at process layer. This extension establishes relationships between integration at data, service and process layers, and uses task ontologies and a process ontology to guide integration at process layer. It was used to provide an integrated solution involving applications supporting the Issue Management and Software Configuration Management processes.},
	booktitle = {Proceedings of the {XVII} {Brazilian} {Symposium} on {Software} {Quality}},
	publisher = {Association for Computing Machinery},
	author = {Renault, Laylla D.C. and Barcellos, Monalessa Perini and de Almeida Falbo, Ricardo},
	year = {2018},
	note = {event-place: Curitiba, Brazil},
	keywords = {Ontology, semantics, process integration, system integration},
	pages = {220--229},
}

@inproceedings{ma_geological_2024,
	address = {New York, NY, USA},
	series = {{CISAI} '24},
	title = {Geological {Disaster} {Named} {Entity} {Recognition} with {Small} {Samples} {Based} on {Data} {Augmentation} and {Prompt} {Engineering}},
	isbn = {979-8-4007-0725-4},
	url = {https://doi.org/10.1145/3703187.3703290},
	doi = {10.1145/3703187.3703290},
	abstract = {This paper uses a large language model to perform generative data enhancement on the original small sample data by performing random synonym replacement and random mask filling operations. In accordance with the reasoning logic of the large language model, three prompt templates are designed and the reasons are explored. Experiments show that when the parameters remain unchanged, the data enhanced by this method has been greatly improved under the three prompt templates, alleviating the difficulty of low resources of geological disaster data. And by comparing the performance of different instructions under different learning rates, the fine-tuning learning rate range suitable for the field of geological disasters is summarized. The limitation is that it is constrained by local computing resources, which reduces the parameter scale of LLM, and the recognition performance is low for extremely long or complex texts.},
	booktitle = {Proceedings of the 2024 7th {International} {Conference} on {Computer} {Information} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Xiangfei and Li, Lin},
	year = {2024},
	keywords = {Prompt Engineering, LLMs, Named Entity Recognition, Data Augmentation, Geological Disasters},
	pages = {613--617},
}

@inproceedings{lagrue_ontology_2019,
	address = {New York, NY, USA},
	series = {{SUMAC} '19},
	title = {An {Ontology} {Web} {Application}-based {Annotation} {Tool} for {Intangible} {Culture} {Heritage} {Dance} {Videos}},
	isbn = {978-1-4503-6910-7},
	url = {https://doi.org/10.1145/3347317.3357245},
	doi = {10.1145/3347317.3357245},
	abstract = {Collecting dance videos, preserving and promoting them after enriching the collected data has been significant actions in preserving Intangible culture heritage in South-East Asia. Whereas techniques for the conceptual modeling of the expressive semantics of dance videos are very complex, they are crucial to exploit effectively the video semantics. This paper proposes an ontology web-based dance video annotation system for representing the semantics of dance videos at different granularity levels. Especially, the system incorporates both syntactic and semantic features of pre-built dance ontology system in order to not only use the available semantic web system but also to create unity for users when annotating videos to minimize conflicts.},
	booktitle = {Proceedings of the 1st {Workshop} on {Structuring} and {Understanding} of {Multimedia} {HeritAge} {Contents}},
	publisher = {Association for Computing Machinery},
	author = {Lagrue, Sylvain and Chetcuti-Sperandio, Nathalie and Delorme, Fabien and Thi, Chau Ma and Thi, Duyen Ngo and Tabia, Karim and Benferhat, Salem},
	year = {2019},
	note = {event-place: Nice, France},
	keywords = {ontologies, knowledge representation, inconsistency-tolerant query answering, video annotation},
	pages = {75--81},
}

@inproceedings{skipanes_fast_2025,
	address = {New York, NY, USA},
	series = {{DFDS} '25},
	title = {Fast {Synthetic} {Data} {Generation} for {Case}-{Specific} {Entity} {Extraction} in {Criminal} {Investigations}},
	isbn = {979-8-4007-1076-6},
	url = {https://doi.org/10.1145/3712716.3712719},
	doi = {10.1145/3712716.3712719},
	abstract = {In major criminal investigations, the manual analysis of police reports for the categorization of entities is a resource-intensive task prone to human error. Recent advances in Named Entity Recognition (NER) models offer promising solutions for automating this process, potentially reducing both time and error rates.This paper demonstrates the effectiveness of fine-tuning a NER model using a publicly shared synthetic dataset inspired by real case files. Notably, we leverage a large language model (LLM) for generating both the synthetic data and the annotations used for training. This approach enables investigators to rapidly develop case-specific models tailored to ongoing investigations. To structure this effort, we propose an ontology for entity extraction in criminal cases, focusing on key entities, such as persons, seized items, communication profiles, vehicles, locations, organizations, and financial profiles. Our model achieves an average weighted F1-score of 94.2\% on the synthetic dataset.For further validation, we manually annotated a small dataset of confidential data from two homicide cases, achieving an average weighted F1-score of 81.6\%. Our results demonstrate that our approach can at times generalize well to real case files.},
	booktitle = {Proceedings of the {Digital} {Forensics} {Doctoral} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Skipanes, Mads and Pratama, Nardiena and Porter, Kyle and Demartini, Gianluca},
	year = {2025},
	keywords = {Large Language Models, Artificial Intelligence, Named Entity Recognition, Criminal Investigation, Data Management, Information Analysis., Synthetic Data},
}

@article{fan_unicorn_2024,
	title = {Unicorn: {A} {Unified} {Multi}-{Tasking} {Matching} {Model}},
	volume = {53},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/3665252.3665263},
	doi = {10.1145/3665252.3665263},
	abstract = {Data matching, which decides whether two data elements (e.g., string, tuple, column, or knowledge graph entity) are the "same" (a.k.a. a match), is a key concept in data integration. The widely used practice is to build task-specific or even dataset-specific solutions, which are hard to generalize and disable the opportunities of knowledge sharing that can be learned from different datasets and multiple tasks. In this paper, we propose Unicorn, a unified model for generally supporting common data matching tasks. Building such a unified model is challenging due to heterogeneous formats of input data elements and various matching semantics of multiple tasks. To address the challenges, Unicorn employs one generic Encoder that converts any pair of data elements (a, b) into a learned representation, and uses a Matcher, which is a binary classifier, to decide whether a matches b. To align matching semantics of multiple tasks, Unicorn adopts a mixture-of-experts model that enhances the learned representation into a better representation. We conduct extensive experiments using 20 datasets on 7 well-studied data matching tasks, and find that our unified model can achieve better performance on most tasks and on average, compared with the state-of-the-art specific models trained for ad-hoc tasks and datasets separately. Moreover, Unicorn can also well serve new matching tasks with zero-shot learning.},
	number = {1},
	journal = {SIGMOD Rec.},
	author = {Fan, Ju and Tu, Jianhong and Li, Guoliang and Wang, Peng and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song and Tang, Nan},
	month = may,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {44--53},
}

@article{kumar_neural_2022,
	title = {Neural {Topic} {Model} {Training} with the {REBAR} {Gradient} {Estimator}},
	volume = {21},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3517336},
	doi = {10.1145/3517336},
	abstract = {Topic modelling is an important approach of unsupervised machine learning that allows automatically extracting the main “topics” from large collections of documents. In addition, topic modelling is able to identify the topic proportions of each individual document, which can be helpful for organizing the collections. Many topic modelling algorithms have been proposed to date, including several that leverage advanced techniques such as variational inference and deep autoencoders. However, to date topic modelling has made limited use of reinforcement learning, a framework that has obtained vast success in many other unsupervised learning tasks. For this reason, in this article we propose training a neural topic model using a reinforcement learning objective and minimizing the objective with the recently-proposed REBAR gradient estimator. Experiments performed over two probing datasets have shown that the proposed model has achieved improvements over all the compared models in terms of both model perplexity and topic coherence, and produced topics that appear qualitatively informative and consistent.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Kumar, Amit and Esmaili, Nazanin and Piccardi, Massimo},
	month = nov,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deep neural networks, reinforcement learning, REBAR, Topic models, variational autoencoders, variational-autoencoder topic models},
}

@inproceedings{jiang_rcenr_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {{RCENR}: {A} {Reinforced} and {Contrastive} {Heterogeneous} {Network} {Reasoning} {Model} for {Explainable} {News} {Recommendation}},
	isbn = {978-1-4503-9408-6},
	url = {https://doi.org/10.1145/3539618.3591753},
	doi = {10.1145/3539618.3591753},
	abstract = {Existing news recommendation methods suffer from sparse and weak interaction data, leading to reduced effectiveness and explainability. Knowledge reasoning, which explores inferential trajectories in the knowledge graph, can alleviate data sparsity and provide explicitly recommended explanations. However, brute-force pre-processing approaches used in conventional methods are not suitable for fast-changing news recommendation. Therefore, we propose an explainable news recommendation model: the Reinforced and Contrastive Heterogeneous Network Reasoning Model for Explainable News Recommendation (RCENR), consisting of NHN-R2 and MR\&amp;CO frameworks. The NHN-R2 framework generates user/news subgraphs to enhance recommendation and extend the dimensions and diversity of reasoning. The MR\&amp;CO framework incorporates contrastive learning with a reinforcement-based strategy for self-supervised and efficient model training. Experiments on the MIND dataset show that RCENR is able to improve recommendation accuracy and provide diverse and credible explanations.},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Hao and Li, Chuanzhen and Cai, Juanjuan and Wang, Jingling},
	year = {2023},
	note = {event-place: Taipei, Taiwan},
	keywords = {knowledge reasoning, contrastive learning, explainable recommendation, markov decision process, news recommendation},
	pages = {1710--1720},
}

@article{piscopo_who_2018,
	title = {Who {Models} the {World}? {Collaborative} {Ontology} {Creation} and {User} {Roles} in {Wikidata}},
	volume = {2},
	url = {https://doi.org/10.1145/3274410},
	doi = {10.1145/3274410},
	abstract = {Wikidata is a collaborative knowledge graph which is central to many academic and industry IT projects. Its users are responsible for maintaining the schema that organises this knowledge into classes, properties, and attributes, which together form the Wikidata 'ontology'. In this paper, we study the relationship between different Wikidata user roles and the quality of the Wikidata ontology. To do so we first propose a framework to evaluate the ontology as it evolves. We then cluster editing activities to identify user roles in monthly time frames. Finally, we explore how each role impacts the ontology. Our analysis shows that the Wikidata ontology has uneven breadth and depth. We identified two user roles: contributors and leaders. The second category is positively associated to ontology depth, with no significant effect on other features. Further work should investigate other dimensions to define user profiles and their influence on the knowledge graph.},
	number = {CSCW},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Piscopo, Alessandro and Simperl, Elena},
	month = nov,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontologies, wikidata, collaborative knowledge engineering, user roles},
}

@inproceedings{meyer_zum_felde_extending_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {Extending {Actor} {Models} in {Data} {Spaces}},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587645},
	doi = {10.1145/3543873.3587645},
	abstract = {In today’s internet almost any party can share sets of data with each other. However, creating frameworks and regulated realms for the sharing of data is very complex when multiple parties are involved and complicated regulation comes into play. As solution data spaces were introduced to enable participating parties to share data among themselves in an organized, regulated and standardized way. However, contract data processors, acting as data space participants, are currently unable to execute data requests on behalf of their contract partners. Here we show that an on-behalf-of actor model can be easily added to existing data spaces. We demonstrate how this extension can be realized using verifiable credentials. We provide a sample use case, a detailed sequence diagram and discuss necessary architectural adaptations and additions to established protocols. Using the extensions explained in this work numerous real life use cases which previously could technically not be realized can now be covered. This enables future data spaces to provide more dynamic and complex real world use cases.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Meyer zum Felde, Hendrik and Kollenstart, Maarten and Bellebaum, Thomas and Dalmolen, Simon and Brost, Gerd},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {International Data Spaces, Actor Models, Contract Data Processing, Data Spaces, Gaia-X, On-behalf-of Model, Self-Sovereign Identities},
	pages = {1447--1451},
}

@article{rani_ttpxhunter_2024,
	title = {{TTPXHunter}: {Actionable} {Threat} {Intelligence} {Extraction} as {TTPs} from {Finished} {Cyber} {Threat} {Reports}},
	volume = {5},
	url = {https://doi.org/10.1145/3696427},
	doi = {10.1145/3696427},
	abstract = {Understanding the modus operandi of adversaries aids organizations to employ efficient defensive strategies and share intelligence in the community. This knowledge is often present in unstructured natural language text within threat analysis reports. A translation tool is needed to interpret the modus operandi explained in the sentences of the threat report and convert it into a structured format. This research introduces a methodology named TTPXHunter for automated extraction of threat intelligence in terms of Tactics, Techniques, and Procedures (TTPs) from finished cyber threat reports. It leverages cyber domain-specific state-of-the-art natural language model to augment sentences for minority class TTPs and refine pinpointing the TTPs in threat analysis reports significantly. We create two datasets: an augmented sentence-TTP dataset of (39,296) sentence samples and a (149) real-world cyber threat intelligence report-to-TTP dataset. Further, we evaluate TTPXHunter on the augmented sentence and report datasets. The TTPXHunter achieves the highest performance of (92.42\%) f1-score on the augmented dataset, and it also outperforms existing state-of-the-art TTP extraction method by achieving an f1-score of (97.09\%) when evaluated over the report dataset. TTPXHunter significantly improves cybersecurity threat intelligence by offering quick, actionable insights into attacker behaviors. This advancement automates threat intelligence analysis and provides a crucial tool for cybersecurity professionals to combat cyber threats.},
	number = {4},
	journal = {Digital Threats},
	author = {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla, Sandeep Kumar},
	month = dec,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cybersecurity, NLP, Natural Language Processing, CK, MITRE ATT\&amp, Cyber Security and AI, Cyber Security Threats, Threat Intelligence, Threat Intelligence Extraction, TTP Classification, TTP Extraction},
}

@article{thulke_task-oriented_2023,
	title = {Task-{Oriented} {Document}-{Grounded} {Dialog} {Systems} by {HLTPR}@{RWTH} for {DSTC9} and {DSTC10}},
	volume = {32},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2023.3267832},
	doi = {10.1109/TASLP.2023.3267832},
	abstract = {This paper summarizes our contributions to the document-grounded dialog tasks at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In both iterations the task consists of three subtasks: first detect whether the current turn is knowledge seeking, second select a relevant knowledge document, and third generate a response grounded on the selected document. For DSTC9 we proposed different approaches to make the selection task more efficient. The best method, Hierarchical Selection, actually improves the results compared to the original baseline and gives a speedup of 24x. In the DSTC10 iteration of the task, the challenge was to adapt systems trained on written dialogs to perform well on noisy automatic speech recognition transcripts. Therefore, we proposed data augmentation techniques to increase the robustness of the models as well as methods to adapt the style of generated responses to fit well into the proceeding dialog. Additionally, we proposed a noisy channel model that allows for increasing the factuality of the generated responses. In addition to summarizing our previous contributions, in this work, we also report on a few small improvements and reconsider the automatic evaluation metrics for the generation task which have shown a low correlation to human judgments.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Thulke, David and Daheim, Nico and Dugast, Christian and Ney, Hermann},
	month = apr,
	year = {2023},
	note = {Publisher: IEEE Press},
	pages = {733--741},
}

@inproceedings{pan_emotion_2019,
	address = {New York, NY, USA},
	series = {{ICDMML} 2019},
	title = {Emotion {Analysis} of {Tourists} {Based} on {Domain} {Ontology}},
	isbn = {978-1-4503-6090-6},
	url = {https://doi.org/10.1145/3335656.3335701},
	doi = {10.1145/3335656.3335701},
	abstract = {The big data of tourism has exploded with the rapid development of social media, providing a new data source for the emotion analysis of tourism. Based on online comments, this paper proposes an emotion analysis model that combines tourism domain ontology and semantic-based method to mine the fine-grained emotion of tourists and designs specific formulas to quantify the emotion of tourists. Finally, the Palace Museum is used as an example to verify the validity of the model. The analysis results show that: 1) Tourists pay more attention to the attributes such as "scenery", "tourist flow", "ticket", etc. in their travel activities. 2) The emotional score of the attributes such as "lodging environment", "scenery", "culture", "environment quality", etc. are higher, but the attributes such as "safety", "tourist flow", "toilet" and cost-related attributes are lower. The main reasons are: "low security", "massive tourists", "less and small toilets" and "high costs". 3) Due to the excessive number of tourists during the holiday, which leads poor travel experience to the tourists, the emotional score of tourists are lower in the 5th, 7th, 8th and 10th months. The analysis results can provide reference for tourists' travel decisions and the development and optimization of tourism.},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Data} {Mining} and {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Pan, Jiabin and Mou, Naixia and Liu, Wenbao},
	year = {2019},
	note = {event-place: Hong Kong, Hong Kong},
	keywords = {Domain ontology, Tourism, Emotion analysis, Online comment, the Palace Museum},
	pages = {146--150},
}

@inproceedings{le_sequence-based_2023,
	address = {New York, NY, USA},
	series = {{BCB} '23},
	title = {A {Sequence}-{Based} {Prediction} {Model} of {Vesicular} {Transport} {Proteins} {Using} {Ensemble} {Deep} {Learning}},
	isbn = {979-8-4007-0126-9},
	url = {https://doi.org/10.1145/3584371.3612950},
	doi = {10.1145/3584371.3612950},
	abstract = {This study aims to employ computational methods for the accurate identification of vesicular transport proteins. The identification of these proteins holds great significance in enhancing our understanding of their protein family structure, thereby enabling the design of more effective drug targets for individuals afflicted with endocrine disorders. In recent times, researchers in the field of biology have increasingly sought to leverage deep learning techniques to address this challenge. In order to further enhance the classification performance, we investigated the following models incorporating distinct features: (1) We devised a novel protein feature called AAC\_PSSM by amalgamating amino acid composition (AAC) and position-specific scoring matrix (PSSM) features. Subsequently, a gated recurrent unit (GRU) model was employed to learn such features; (2) An ensemble model was constructed by combining the existing GRU model with the model of a neural network featuring the AAC feature; (3) Random forest analysis was conducted using the pseudo-amino acid composition (PseAAC) feature; (4) Furthermore, we explored a natural language processing (NLP) approach by considering the protein sequence as a natural language and applying various neural network architectures. Upon analyzing the results obtained from the different models, it was observed that the ensemble model incorporating PSSM and AAC features exhibited the highest sensitivity of 81.03\% and accuracy of 82.43\%. Notably, our proposed model surpassed the performance of state-of-the-art models addressing the same problem and datasets, thus establishing its superiority.},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Le, Nguyen Quoc Khanh and Kha, Quang Hien},
	year = {2023},
	note = {event-place: Houston, TX, USA},
	keywords = {deep learning, gate recurrent unit, nesemble learning, position-specific scoring matrix, protein sequence, vesicular transport},
}

@inproceedings{kalbasi_physiomed_2023,
	address = {New York, NY, USA},
	series = {{BCB} '23},
	title = {{PhysioMed}: {A} {Semantic} {Web} based {Framework} for {Linking} {Healthcare} {Information} with {Computational} {Physiology} {Models}},
	isbn = {979-8-4007-0126-9},
	url = {https://doi.org/10.1145/3584371.3613056},
	doi = {10.1145/3584371.3613056},
	abstract = {This research focuses on the integration of computational physiology models and Electronic Health Records (EHR) to improve the accuracy and applicability of computational models in clinical settings. Computational physiology models provide quantified insights into biological processes, while EHRs store digital health information. By combining these two domains, the research aims to create a foundation for personalized and predictive clinical decision support systems.},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Kalbasi, Reza and Nickerson, David and Atalag, Koray},
	year = {2023},
	note = {event-place: Houston, TX, USA},
	keywords = {biomedical ontology, semantic web, health informatics, collaborative ontology development},
}

@inproceedings{vsesviatska_ardo_2021,
	address = {New York, NY, USA},
	series = {{SAC} '21},
	title = {{ArDO}: an ontology to describe the dynamics of multimedia archival records},
	isbn = {978-1-4503-8104-8},
	url = {https://doi.org/10.1145/3412841.3442057},
	doi = {10.1145/3412841.3442057},
	abstract = {Cultural heritage institutions store and digitize large amounts of multimedia data inside archives to make archival records findable by archivists, scientists, and general public. Cataloging standards vary from archive to archive and, therefore, the sharing and use of this data are limited. To solve this issue, linked open data (LOD) is rising as an essential paradigm to open and provide access to the archival resources. Archives which are opened to the world knowledge benefit from external connections by enabling the application of automated approaches to process archival records, helping all stakeholders to gain valuable insights. In this paper, we present the Archive Dynamics Ontology (ArDO) - an ontology designed for describing the hierarchical nature of archival multimedia data, as well as its application on the example of archival resources about the Weimar Republic. Furthermore, ArDO semantically organizes multimedia archival resources in form of texts, images, audios, and videos by representing the dynamics related to their classification over time. ArDO tracks the changes of a specific hierarchical classification schema referred to as systematics adopted to organize archival resources under semantically defined keywords.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Vsesviatska, Oleksandra and Tietz, Tabea and Hoppe, Fabian and Sprau, Mirjam and Meyer, Nils and Dessì, Danilo and Sack, Harald},
	year = {2021},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {multimedia archive, ontology design, ontology dynamics},
	pages = {1855--1863},
}

@inproceedings{mor_smart_2022,
	address = {New York, NY, USA},
	series = {{FIRE} '21},
	title = {Smart {City} {Umbrella} {Ontology} :{Context} -{Driven} {Framework} {For} {Traffic} {Planning}},
	isbn = {978-1-4503-9596-0},
	url = {https://doi.org/10.1145/3503162.3503170},
	doi = {10.1145/3503162.3503170},
	abstract = {Presently, huge public and private data sets are available from different government sources regarding transportation services in modern cities.The heterogeneous data-sets address day- to- day operations in transportation research,and these challenges can be effectively done with assistance of ontologies. Ontology is used for accessing and exploiting data from sensors to surveys in a semantically inter-operable way is demanding,in recent years. In this paper, an ontological framework is designed for smart cities by integration of entities such as surface transport network,road geometry, topology,and traffic monitoring sensors.The Smart City Umbrella Ontology (SCUO) is designed using standardized traffic guidelines in the context of Indian surface transportation.The paper focuses on ontology design by argumentation of road network nomenclature and relationship among entities.A mechanism for linking ontologies is implemented through Resource Description Framework(RDF) with a set of relations.The proposed framework can be used to provide services to commuters via specific applications in intelligent system of transportation by administration,and city stakeholders for design,policy-making purposes of traffic planning.},
	booktitle = {Proceedings of the 13th {Annual} {Meeting} of the {Forum} for {Information} {Retrieval} {Evaluation}},
	publisher = {Association for Computing Machinery},
	author = {Mor, Annu and Kumar, Mukesh and Chaudhury, Santanu},
	year = {2022},
	note = {event-place: Virtual Event, India},
	keywords = {resource description framework, linking ontologies, smart city umbrella ontology, Transportation ontologies},
	pages = {83--90},
}

@inproceedings{gogolla_teaching_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {On teaching descriptive and prescriptive modeling},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3418744},
	doi = {10.1145/3417990.3418744},
	abstract = {Models may be used for purposes relating (a) to understanding, predicting, and communicating model aspects, and (b) to implementing the model and capturing the design intent. Models that are primarily used for understanding, predicting and communicating are referred to as descriptive models, whereas models mainly used for implementation are called prescriptive models. This contribution focuses on teaching both the common and the distinguishing aspects of the two model categories. We start with an example for a general descriptive and prescriptive model, independent of particular software modeling languages, and continue to discuss an example demonstrating how UML and OCL can be applied for specifying both a descriptive and a prescriptive model. Finally, we discuss essentials to be learned from this teaching venture.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Gogolla, Martin and Selic, Bran},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {descriptive model, prescriptive model, UML and OCL model},
}

@inproceedings{manda_automated_2020,
	address = {New York, NY, USA},
	series = {{SBD} '20},
	title = {Automated ontology-based annotation of scientific literature using deep learning},
	isbn = {978-1-4503-7974-8},
	url = {https://doi.org/10.1145/3391274.3393636},
	doi = {10.1145/3391274.3393636},
	abstract = {Representing scientific knowledge using ontologies enables data integration, consistent machine-readable data representation, and allows for large-scale computational analyses. Text mining approaches that can automatically process and annotate scientific literature with ontology concepts are necessary to keep up with the rapid pace of scientific publishing. Here, we present deep learning models (Gated Recurrent Units (GRU) and Long Short Term Memory (LSTM)) combined with different input encoding formats for automated Named Entity Recognition (NER) of ontology concepts from text. The Colorado Richly Annotated Full Text (CRAFT) gold standard corpus was used to train and test our models. Precision, Recall, F-1, and Jaccard semantic similarity were used to evaluate the performance of the models. We found that GRU-based models outperform LSTM models across all evaluation metrics. Surprisingly, considering the top two probabilistic predictions of the model for each instance instead of the top one resulted in a substantial increase in accuracy. Inclusion of ontology semantics via subsumption reasoning yielded modest performance improvement.},
	booktitle = {Proceedings of {The} {International} {Workshop} on {Semantic} {Big} {Data}},
	publisher = {Association for Computing Machinery},
	author = {Manda, Prashanti and SayedAhmed, Saed and Mohanty, Somya D.},
	year = {2020},
	note = {event-place: Portland, Oregon},
	keywords = {ontologies, deep learning, named entity recognition, automated annotation},
}

@inproceedings{rajaonarivo_automatic_2022,
	address = {New York, NY, USA},
	series = {{WI}-{IAT} '21},
	title = {Automatic {Generation} of {Event} {Ontology} from {Social} {Network} and {Mobile} {Positioning} {Data}},
	isbn = {978-1-4503-9115-3},
	url = {https://doi.org/10.1145/3486622.3493933},
	doi = {10.1145/3486622.3493933},
	abstract = {The study of mobile positioning data makes it possible to detect whether an event has happened at a particular place during a given period. However, determining the nature and details of the event is a challenge, especially if the event is not widely known, as is the case for local events. We propose an approach to determining the nature of local events by generating an ontology in a completely automatic way from social network data and data on people’s movements and by querying this generated ontology. This approach uses entity discovery techniques, filtering systems and information enrichment via Open Data, as well as a system for matching discovered entities and ontology elements. Evaluation via a survey allowed us to validate approximately that the information presented in the ontology is reliable, makes sense and answers our questions.},
	booktitle = {{IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Rajaonarivo, Landy and Mine, Tsunenori and Arakawa, Yutaka},
	year = {2022},
	note = {event-place: Melbourne, VIC, Australia},
	keywords = {event ontology, data mining, automatic ontology generation, recommendation engine, social network data},
	pages = {87--94},
}

@inproceedings{ul_haq_ontology_2019,
	address = {New York, NY, USA},
	series = {{ICEIT} 2019},
	title = {Ontology {Based} {Test} {Case} {Generation} for {Black} {Box} {Testing}},
	isbn = {978-1-4503-6267-2},
	url = {https://doi.org/10.1145/3318396.3318442},
	doi = {10.1145/3318396.3318442},
	abstract = {Software systems are not considered complete unless properly tested and verified. In existing literature, a growing interest on establishment of automated testing techniques has been observed. However, tedious manual process of test case generation largely depends upon domain knowledge and formalized representation of user requirements. The advent of semantic web engineering has led the foundation for developing ontologies as a mean to express information and knowledge semantics regarding particular domain efficiently. In software testing, ontologies can be significantly helpful to automate testing phase as they encode domain knowledge in machine interpretable format. We have proposed automatic test case generation framework that involves ontology-based requirement specification and learning based methods for conducting black box testing. Our approach integrates knowledge-based system (ontology) with learning-based testing algorithm to automate: generation of test cases, test execution and test verdict construction. Proposed framework includes, requirement ontology to formalize requirement specification, Dialogue Manager that enables selection of available test cases and Learning Based Testing to generate counter examples of test cases through system learning. The contribution of this paper is to enable 1) requirement elicitation and specification using ontologies 2) test data selection from existing ontologies and 3) automatic test case generation from existing test cases. To represent the applicability of this research, ontology for requirement elicitation and specification is developed. Framework proposed in this research paper is an effort to provide software testing tools to save time, cost and efforts during test design phase.},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Educational} and {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Ul Haq, Sami and Qamar, Usman},
	year = {2019},
	note = {event-place: Cambridge, United Kingdom},
	keywords = {Ontologies, Test case generation, Black box testing, Learning based testing, Requirement Engineering},
	pages = {236--241},
}

@inproceedings{shi_flowxpert_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {{FlowXpert}: {Expertizing} {Troubleshooting} {Workflow} {Orchestration} with {Knowledge} {Base} and {Multi}-{Agent} {Coevolution}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3737221},
	doi = {10.1145/3711896.3737221},
	abstract = {Incident management remains a critical yet challenging task for large-scale cloud services. Most cloud service providers abstract troubleshooting into predefined workflows for different incidents, offering step-by-step guidance. However, manually crafting workflows is resource-consuming and knowledge-intensive, hindering large-scale deployment. Most automated techniques for workflow orchestration rely on large language models (LLMs) to handle complex tasks but overlook key aspects of troubleshooting, including complex expertise, domain requirements, and the reliability of AI feedback. These limitations undermine workflow quality. Therefore, we propose FlowXpert, a novel framework for troubleshooting workflow orchestration. Leveraging LLMs, it first builds a knowledge base centered on incident-aware nodes to precisely depict expertise. Then, fed into AI feedback and synthetic preference data, reinforcement learning is applied to refine the workflow generator and evaluator. To assess troubleshooting workflows, we introduce OpsFlowBench based on Huawei Cloud's datacenter switch operation documents. Benchmark tests under the tailored STEPScore metric validate its effectiveness. Furthermore, during a 10-week deployment in Huawei Cloud's datacenter network, FlowXpert provided valuable support to both on-call engineers and AI executors, as evidenced by empirical data and case study.},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Shi, Binpeng and Luo, Yu and Wang, Jingya and Zhao, Yongxin and Zhang, Shenglin and Hao, Bowen and Zhao, Chenyu and Sun, Yongqian and Zhang, Zhi and Sun, Ronghua and Li, Haihua and Song, Wei and Chen, Xiaolong and Miao, Jingbo and Pei, Dan},
	year = {2025},
	note = {event-place: Toronto ON, Canada},
	keywords = {large language model, incident management, troubleshooting, workflow orchestration},
	pages = {4839--4850},
}

@inproceedings{coulon_shape-diverse_2018,
	address = {New York, NY, USA},
	series = {{SLE} 2018},
	title = {Shape-diverse {DSLs}: languages without borders (vision paper)},
	isbn = {978-1-4503-6029-6},
	url = {https://doi.org/10.1145/3276604.3276623},
	doi = {10.1145/3276604.3276623},
	abstract = {Domain-Specific Languages (DSLs) manifest themselves in remarkably diverse shapes, ranging from internal DSLs embedded as a mere fluent API within a programming language, to external DSLs with dedicated syntax and tool support. Although different shapes have different pros and cons, combining them for a single language is problematic:\&nbsp;language designers usually commit to a particular shape early in the design process, and it is hard to reconsider this choice later. In this new ideas paper, we envision a language engineering approach enabling (i) language users to manipulate language constructs in the most appropriate shape according to the task at hand, and (ii) language designers to combine the strengths of different technologies for a single DSL. We report on early experiments and lessons learned building , our prototype approach to this problem. We illustrate its applicability in the engineering of a simple shape-diverse DSL implemented conjointly in Rascal, EMF, and Java. We hope that our initial contribution will raise the awareness of the community and encourage future research.},
	booktitle = {Proceedings of the 11th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Coulon, Fabien and Degueule, Thomas and van der Storm, Tijs and Combemale, Benoit},
	year = {2018},
	note = {event-place: Boston, MA, USA},
	keywords = {domain-specific language, shape-diverse dsl},
	pages = {215--219},
}

@inproceedings{hao_universal_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Universal {Representation} {Learning} of {Knowledge} {Bases} by {Jointly} {Embedding} {Instances} and {Ontological} {Concepts}},
	isbn = {978-1-4503-6201-6},
	url = {https://doi.org/10.1145/3292500.3330838},
	doi = {10.1145/3292500.3330838},
	abstract = {Many large-scale knowledge bases simultaneously represent two views of knowledge graphs (KGs): an ontology view for abstract and commonsense concepts, and an instance view for specific entities that are instantiated from ontological concepts. Existing KG embedding models, however, merely focus on representing one of the two views alone. In this paper, we propose a novel two-view KG embedding model, JOIE, with the goal to produce better knowledge embedding and enable new applications that rely on multi-view knowledge. JOIE employs both cross-view and intra-view modeling that learn on multiple facets of the knowledge base. The cross-view association model is learned to bridge the embeddings of ontological concepts and their corresponding instance-view entities. The intra-view models are trained to capture the structured knowledge of instance and ontology views in separate embedding spaces, with a hierarchy-aware encoding technique enabled for ontologies with hierarchies. We explore multiple representation techniques for the two model components and investigate with nine variants of JOIE. Our model is trained on large-scale knowledge bases that consist of massive instances and their corresponding ontological concepts connected via a (small) set of cross-view links. Experimental results on public datasets show that the best variant of JOIE significantly outperforms previous models on instance-view triple prediction task as well as ontology population on ontology-view KG. In addition, our model successfully extends the use of KG embeddings to entity typing with promising performance.},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \&amp; {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hao, Junheng and Chen, Muhao and Yu, Wenchao and Sun, Yizhou and Wang, Wei},
	year = {2019},
	note = {event-place: Anchorage, AK, USA},
	keywords = {knowledge graph, ontology learning, relational embeddings},
	pages = {1709--1719},
}

@inproceedings{sree-kumar_validating_2021,
	address = {New York, NY, USA},
	series = {{VaMoS} '21},
	title = {Validating {Feature} {Models} {With} {Respect} to {Textual} {Product} {Line} {Specifications}},
	isbn = {978-1-4503-8824-5},
	url = {https://doi.org/10.1145/3442391.3442407},
	doi = {10.1145/3442391.3442407},
	abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
	booktitle = {Proceedings of the 15th {International} {Working} {Conference} on {Variability} {Modelling} of {Software}-{Intensive} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sree-Kumar, Anjali and Planas, Elena and Clarisó, Robert},
	year = {2021},
	note = {event-place: Krems, Austria},
	keywords = {Natural Language Processing, Machine Learning, Requirements Engineering, Software Product Line, Feature Model Validation},
}

@inproceedings{sarmah_hybridrag_2024,
	address = {New York, NY, USA},
	series = {{ICAIF} '24},
	title = {{HybridRAG}: {Integrating} {Knowledge} {Graphs} and {Vector} {Retrieval} {Augmented} {Generation} for {Efficient} {Information} {Extraction}},
	isbn = {979-8-4007-1081-0},
	url = {https://doi.org/10.1145/3677052.3698671},
	doi = {10.1145/3677052.3698671},
	abstract = {Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q\&amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q\&amp;A format, and hence provide a natural set of pairs of ground-truth Q\&amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.},
	booktitle = {Proceedings of the 5th {ACM} {International} {Conference} on {AI} in {Finance}},
	publisher = {Association for Computing Machinery},
	author = {Sarmah, Bhaskarjit and Mehta, Dhagash and Hall, Benika and Rao, Rohan and Patel, Sunil and Pasquali, Stefano},
	year = {2024},
	note = {event-place: Brooklyn, NY, USA},
	pages = {608--616},
}

@inproceedings{zhang_prompt-based_2023,
	address = {New York, NY, USA},
	series = {{EITCE} '22},
	title = {A {Prompt}-based {Few}-shot {Machine} {Reading} {Comprehension} {Model} for {Intelligent} {Bridge} {Management}},
	isbn = {978-1-4503-9714-8},
	url = {https://doi.org/10.1145/3573428.3573599},
	doi = {10.1145/3573428.3573599},
	abstract = {Bridge inspection reports are an important data source in the bridge management process, and they contain a large amount of fine-grained information. However, the research on machine reading comprehension (MRC) methods for this field is insufficient, and annotating large scale domain-specific corpus is time-consuming. This paper presented a novel prompt-based few-shot MRC approach for intelligent bridge management. The proposed model uses the pretrained model MacBERT as backbone. The prompt templates are designed based on some domain-specific heuristic rules. The experimental results show that our model outperforms the baseline models in different few-shot settings. The proposed model can provide technical support for the construction of automatic question answering system in the field of bridge management.},
	booktitle = {Proceedings of the 2022 6th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Luyi and Li, Ren and Xiao, Qiao},
	year = {2023},
	note = {event-place: Xiamen, China},
	keywords = {Few-shot, Machine reading comprehension, Bridge inspection, Prompt},
	pages = {946--950},
}

@inproceedings{wu_exploring_2020,
	address = {Champaign, Illinois},
	series = {{JCDL} '19},
	title = {Exploring ontologies for collection protection in second sino-japanese war},
	isbn = {978-1-7281-1547-4},
	url = {https://doi.org/10.1109/JCDL.2019.00066},
	doi = {10.1109/JCDL.2019.00066},
	abstract = {The actions are usually trivial, implicit yet significant for the protection of collection during the wartime. Records of those actions, events, and particularly, the related persons, organizations may be scattered but deserve further attention since the records are precious to human knowledge protection efforts. Searching, identifying and collecting the related resources from the scattered data sets is not as complicated as developing an ontology for this domain of collection protection during the wartime period. This study has selected 1939–41, in the Second Sino-Japanese War during WWII when there were figures working on the collection protection projects that have been documented in the historical archives and many other resources, including oral history, journals, digital documentation, conference papers, literature reviews, etc. The preliminary ontology models for the protection of wartime period collection have been developed based on one of the oral history. The current research has further tuned and finalized the ontology of collection protection during the wartime period by analyzing the multi-sets of data resources. The implications of the research results are two folds: firstly, it could shed lights on the classification system for a digital library of this domain, and secondly, it could benefit the historical researchers for providing new clues in this line of collection protection during the wartime period research.},
	booktitle = {Proceedings of the 18th {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Wu, Mei Mei and Liu, Ying-Hsang},
	year = {2020},
	keywords = {ontologies, cultural heritage, collection protection, digital scholarship},
	pages = {351--352},
}

@article{sharma_neural_2024,
	title = {Neural {Methods} for {Data}-to-text {Generation}},
	volume = {15},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3660639},
	doi = {10.1145/3660639},
	abstract = {The neural boom that has sparked natural language processing (NLP) research throughout the last decade has similarly led to significant innovations in data-to-text (D2T) generation. This survey offers a consolidated view into the neural D2T paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating D2T from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for D2T research that focus not only on the design of linguistically capable systems but also on systems that exhibit fairness and accountability.},
	number = {5},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Sharma, Mandar and Gogineni, Ajay Kumar and Ramakrishnan, Naren},
	month = oct,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language generation, data-to-text generation, data-to-text, Narration},
}

@inproceedings{stenzer_query_2020,
	address = {New York, NY, USA},
	series = {{iiWAS2019}},
	title = {Query {Relaxation} using {Spreading}-{Activation} and {SKOS}-{Ontologies}},
	isbn = {978-1-4503-7179-7},
	url = {https://doi.org/10.1145/3366030.3366097},
	doi = {10.1145/3366030.3366097},
	abstract = {Digital libraries and archives adopting the Linked Open Data (LOD) approach add descriptive metadata to the objects stored in their inventory in order to facilitate searching and sorting. In many cases the available metadata terms are organized as a controlled vocabulary. Technically, the controlled vocabularies are often provided in the form of SKOS ontologies thus allowing to apply semantic web technologies to establish inter-vocabulary relations and ask queries.However, how can a search over the contents of different digital libraries each of which relies on their own vocabulary be performed without sacrificing recall or having to align the underlying ontologies beforehand?In this paper we present an approach based on query relaxation to solving this problem. Considering the graph nature of ontologies for controlled vocabularies we propose to use a spreading activation algorithm to relax and subsequently transform SPARQL queries in a way that makes them suitable for other vocabularies.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Stenzer, Alexander},
	year = {2020},
	note = {event-place: Munich, Germany},
	keywords = {SKOS, Query, Transformation, Relaxation, Spreading-Activation},
	pages = {330--334},
}

@inproceedings{sacenti_hybrid_2018,
	address = {New York, NY, USA},
	series = {{WebMedia} '18},
	title = {Hybrid {Recommender} {System} {Based} on {Multi}-{Hierarchical} {Ontologies}},
	isbn = {978-1-4503-5867-5},
	url = {https://doi.org/10.1145/3243082.3243106},
	doi = {10.1145/3243082.3243106},
	abstract = {Recommender Systems (RSs) are usually based in User Profiles (UP) to identify items of interest to a user, among the items of a usually vast collection. Traditional RSs are mostly based on ratings of items made by users and do not attempt to estimate the reasons that led the user to access these items. Furthermore, such systems may suffer from the lack of rating data, the so-called data sparsity. This paper proposes a hybrid recommender system that considers, besides the ratings of the users, a feature description analysis of the items accessed by the users. This analysis is based on ontological UP, described in accordance with a set of ontologies, one per feature. The use of ontologies provides a weak coupling between the proposed RS and the domain of the item to be recommended. The effectiveness of our proposal is demonstrated and evaluated in the movie domain using the MovieLens dataset. The experiments demonstrated an improvement in the quality of the recommendations and a greater tolerance to the data sparsity, compared to state-of-art systems.},
	booktitle = {Proceedings of the 24th {Brazilian} {Symposium} on {Multimedia} and the {Web}},
	publisher = {Association for Computing Machinery},
	author = {Sacenti, Juarez A. P. and Willrich, Roberto and Fileto, Renato},
	year = {2018},
	note = {event-place: Salvador, BA, Brazil},
	keywords = {Ontology, Recommender systems, Hibrid filtering},
	pages = {149--156},
}

@inproceedings{djebouri_exploitation_2021,
	address = {New York, NY, USA},
	series = {{ICIST} '20},
	title = {Exploitation of ontological approaches in {Big} {Data}: {A} {State} of the {Art}},
	isbn = {978-1-4503-7655-6},
	url = {https://doi.org/10.1145/3447568.3448553},
	doi = {10.1145/3447568.3448553},
	abstract = {The emergence of web technologies is generating a data deluge called Big Data. All this data is in fact a gold mine to be exploited. However, we are confronted with huge volumes of heterogeneous data (various formats) and varied data (various sources) and in continuous expansion. To deal with this, some research works have introduced ontologies: this is the purpose of this paper. We present the Big Data concept on the one hand and the ontology concept on the other. We first recalled the definitions of Big Data, its main dimensions known by the 3 V (volume, velocity, variety), the fields of application as well as the various problems related to it. We reviewed the different solutions proposed as well as the existing tools by using the NoSQL and the Map-Reduce paradigm implemented in Hadoop and Spark.We then looked at the concept of ontology, starting by recalling the definition of ontology, so an ontology is a conceptual model to represent reality and on which it is possible to develop systems that can be shared and reused. Ontologies are used to represent a domain and reason about its entities.Finally, we presented and discussed some research works that combined ontologies and Big Data. We have found that there is a very abundant literature that deals with big data and ontologies separately, but few studies combine the two concepts together. We will therefore focus on the latter in order to enrich the scientific literature in the domain.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Information} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Djebouri, Djamila and Keskes, Nabil},
	year = {2021},
	note = {event-place: Lecce, Italy},
	keywords = {Semantic Web, ontology, Big Data, Knowledge Base, Spark, HADOOP, Map-Reduce},
}

@inproceedings{kivrakidis_towards_2024,
	address = {New York, NY, USA},
	series = {{SETN} '24},
	title = {Towards {Enriching} the {Electric} {Vehicle} {Knowledge} {Graph} by {Linking} it to {DBpedia}},
	isbn = {979-8-4007-0982-1},
	url = {https://doi.org/10.1145/3688671.3688734},
	doi = {10.1145/3688671.3688734},
	abstract = {The automotive industry is focusing on Electric Vehicles (EVs) for their efficiency in reducing oil consumption and emissions. However, the EV market’s diversity in battery capacities, classifications, and connector types creates a lack of standardization. Researchers are exploring advanced data and knowledge management methods, with Knowledge Graphs (KGs) emerging as a promising solution. KGs represent data in a way that reflects human understanding, promoting natural human-machine interactions and enhancing AI’s insights. Their structure and constraints are defined by a vocabulary or ontology. This paper presents ongoing work towards enriching an existing Electric Vehicle Knowledge Graph (EVKG), which is specified by the Electric Vehicle Ontology (EVO). To achieve this, we utilized the Python programming language to create labels for each resource in the EVKG. These labels were then used to match and link these resources to their corresponding entries in DBpedia through the use of the owl:sameAs and rdfs:seeAlso properties. To ensure the matching was as accurate as possible, we developed two algorithms: one employing string matching and the other using word vectorization and distance techniques.},
	booktitle = {Proceedings of the 13th {Hellenic} {Conference} on {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Kivrakidis, Ioannis and Rigas, Emmanouil and Bassiliades, Nick},
	year = {2024},
	keywords = {Ontology, Knowledge Graph, Linked Data, Electric Vehicles, EVO Ontology},
}

@inproceedings{telli_ontology_2018,
	address = {New York, NY, USA},
	series = {{PRAI} 2018},
	title = {An {Ontology} for {Classifying} {Vietnamese} {Dance} {Movements}},
	isbn = {978-1-4503-6482-9},
	url = {https://doi.org/10.1145/3243250.3243253},
	doi = {10.1145/3243250.3243253},
	abstract = {This paper proposes an OWL ontology called "VDM" (Vietnamese Dance Movements), to define taxonomy of dance movement classes and their relationships for the traditional Vietnamese dances taking into account the semantics of its art and its cultural anthropologists. The "VDM" terminology can be used to describe elementary movements (poses) as a dataset ontology importing the ontology "VDM". These poses are results of dance sequences segmentation (using segmentation techniques). The ontology "VDM" is supported by classification rules, which are developed with the OWL complementary language SWRL (Semantic Web Rule Language) to entail movement phrases, which are basic movements with complete meaning. The dataset ontology containing pose descriptions can be queried using the query language SQWRL (Semantic Query Web-enhanced Rule Language).},
	booktitle = {Proceedings of the {International} {Conference} on {Pattern} {Recognition} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Telli, Abdelmoutia and Chau, Ma Thi and Bourahla, Mustapha and Tabia, Karim and Benferhat, Salem},
	year = {2018},
	note = {event-place: Union, NJ, USA},
	keywords = {Ontology, Semantic Web Technologies, Description Logics, Dance Labanotation, Traditional Vietnamese Dance},
	pages = {23--29},
}

@article{hernich_dichotomies_2020,
	title = {Dichotomies in {Ontology}-{Mediated} {Querying} with the {Guarded} {Fragment}},
	volume = {21},
	issn = {1529-3785},
	url = {https://doi.org/10.1145/3375628},
	doi = {10.1145/3375628},
	abstract = {We study ontology-mediated querying in the case where ontologies are formulated in the guarded fragment of first-order logic (GF) or extensions thereof with counting and where the actual queries are (unions of) conjunctive queries. Our aim is to classify the data complexity and Datalog rewritability of query evaluation depending on the ontology O, where query evaluation w.r.t. O is in PTime (resp. Datalog rewritable) if all queries can be evaluated in PTime w.r.t. O (resp. rewritten into Datalog under O), and coNP-hard if at least one query is coNP-hard w.r.t. O. We identify several fragments of GF that enjoy a dichotomy between Datalog-rewritability (which implies PTime) and coNP-hardness as well as several other fragments that enjoy a dichotomy between PTime and coNP-hardness, but for which PTime does not imply Datalog-rewritability. For the latter, we establish and exploit a connection to constraint satisfaction problems. We also identify fragments for which there is no dichotomy between PTime and coNP. To prove this, we establish a non-trivial variation of Ladner’s theorem on the existence of NP-intermediate problems. Finally, we study the decidability of whether a given ontology enjoys PTime query evaluation, presenting both positive and negative results, depending on the fragment.},
	number = {3},
	journal = {ACM Trans. Comput. Logic},
	author = {Hernich, André and Lutz, Carsten and Papacchini, Fabio and Wolter, Frank},
	month = feb,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology-based data access, dichotomies, query evaluation},
}

@inproceedings{ayimdji_tekemetieu_modeling_2021,
	address = {New York, NY, USA},
	series = {{HAI} '21},
	title = {Modeling an {Adaptive} {Resident}-{System} {Interaction} for {Cognitive} {Assistance} in {Ambient} {Assisted} {Living}},
	isbn = {978-1-4503-8620-3},
	url = {https://doi.org/10.1145/3472307.3484166},
	doi = {10.1145/3472307.3484166},
	abstract = {In the last decade, advances in the field of ambient assisted living (AAL) have changed the way services can be provided in smart homes. New possibilities are now offered for addressing complex interaction problems between the technology and users with special needs. Within this context, this study addresses human-computer interactions for cognitive assistance to people with Traumatic Brain Injury (TBI). Interdisciplinary research combining computer science and occupational therapy was conducted to model the interaction between an AAL system (AALS) and a person with cognitive impairments due to TBI residing in an AAL environment. Cognitive assistance is modelled as an interactive exchange where an AALS spreads assistance cues that should induce appropriate behaviours from an assisted person as responses. After an assistance cue is delivered, the AALS should evaluate the user reaction and, stop the interaction if the intended reaction is observed or resume by adapting the assistance. To do so, evidence-based cognitive rehabilitation and speech acts are used to model the interaction content i.e., assistive cues and user feedback, while an ontology formalizes the semantics of this knowledge in a computer readable format. A behavioural model based on behaviour trees informed by the ontological model is then used to enable the cognitive assistant to plan the sequence of cues to be delivered adaptively depending on the circumstances, the assistance goal and the user's behaviour. To show that the proposed interaction model can help an AALS to provide adaptive assistance to people with cognitive impairments, it is exemplified on a cooking safety assistant designed for people with TBI.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Human}-{Agent} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Ayimdji Tekemetieu, Armel and Pigot, Hélène and Bottari, Carolina and Gagnon-Roy, Mireille and Giroux, Sylvain},
	year = {2021},
	note = {event-place: Virtual Event, Japan},
	keywords = {Speech acts, Cognitive assistance, Ambient intelligence, interactive information retrieval, Ontology-based decision support, Traumatic Brain Injury},
	pages = {183--192},
}

@inproceedings{al-shareef_investigating_2023,
	address = {Istanbul, Turkey},
	series = {{ASONAM} '22},
	title = {Investigating {Community} {Detection} in {Arabic} {Scholarly} {Network} {Using} {Ontology}-{Based} {Semantic} {Expansion}},
	isbn = {978-1-6654-5661-6},
	url = {https://doi.org/10.1109/ASONAM55673.2022.10068618},
	doi = {10.1109/ASONAM55673.2022.10068618},
	abstract = {Clustering researchers in communities is an important task to support a range of techniques for analyzing and making sense of the research environment and helps researchers find people in the same field of interest to collaborate. In computer science, ontology is commonly used to capture knowledge about a particular area using relevant concepts and relations. This study investigates the use of overlapping community detection algorithms on a multilayered Arabic scholarly network to detect communities of researchers who share their research interests. Two researchers can share an interest if they co-authored a publication or share some keywords in their publications. The set of keywords is expanded via semantic search within a cross-domain ontology, e.g. DBpedia, allowing more researchers with indirect relationships to be connected. A 2-layer scholarly network was constructed by retrieving the scholarly data of faculty members from three colleges at Umm AlQura University (UQU) with rich Arabic publications. Four versions of this network were tested: unweighted, weighted, semantically expanded, and reduced semantically expanded. It was found that weights have an insignificant role in community detection within this study. In addition, a semantically expanded network does have better clustering potentials but only if was performed selectively. Otherwise, the expanded network might suffer from generic and non-discriminative keywords, making the community detection task more challenging. To our knowledge, this is the first investigation into detecting communities within an Arabic scholarly network.},
	booktitle = {Proceedings of the 2022 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {IEEE Press},
	author = {Al-Shareef, Sarah and Alharbi, Rahaf and Alharbi, Rawan and Almfarriji, Raghad and Alsharif, Maram and Alharthi, Rasha and Althaqafi, Lamia},
	year = {2023},
	keywords = {DBpedia, semantic annotation, community detection, social network analysis, Arabic scholarly data, multilayered complex network, overlapping community detection},
	pages = {96--103},
}

@inproceedings{ke_hierarchical_2024,
	address = {New York, NY, USA},
	series = {{ICBDDM} '24},
	title = {Hierarchical {Multi}-label {Classification} {Model} {Research} for {Scenic} {Area} {Tourism} {Resources}},
	isbn = {979-8-4007-1027-8},
	url = {https://doi.org/10.1145/3696500.3696562},
	doi = {10.1145/3696500.3696562},
	abstract = {Research on tourism resource demand offers critical decision support for the protection, development, and marketing of tourism resources. It also improves personalized experiences and satisfaction for tourists, thus fostering the advancement of the tourism industry into broader and higher realms. It also improves personalized experiences and satisfaction for tourists, thus fostering the advancement of the tourism industry into broader and higher realms. The objective of this study is to extract characteristics of 3A-level and above scenic spots nationwide, facilitating the integration and utilization of tourism resources in the country. The objective of this study is to extract characteristics of 3A-level and above scenic spots nationwide, facilitating the integration and utilization of tourism information. This facilitates the automation and efficiency of tourism resource classification and provides suggestions for improving services at these scenic areas. In response to the abundant resources of major scenic areas across the country, a new approach has been developed. In response to the abundant resources of major scenic areas across the country, this study introduces a hierarchical multi-label classification model for tourism resources. tourism resource classification theme system issued by the Ministry of Culture and Tourism, and leveraging FastText for pre-training on scenic area introductory texts, this research combines the advantages of the hierarchical multi-label classification model with the theme system of the Ministry of Culture and Tourism. introductory texts, this research combines the traditional LSTM model with an attention-based Transformer model. Additionally, a Graph-Convolutional Network (GCN) is used to classify tourism resources. Convolutional Network (GCN) is employed as a hierarchical structure-aware encoder to construct the MiLCT, a hierarchical multi-label classification model, enabling sophisticated multi-label classification to be applied to the text. model, enabling sophisticated multi-label classification of tourism scenic area resource data. Experimental comparisons demonstrate that, with increasing classification levels, the proposed model outperforms those lacking GCN and Transformer components in terms of micro-Precision, micro-Recision, micro-Recision, and micro-Recision. micro-Precision, micro-Recall, and micro-F1 scores, this indicates that the hierarchical structural information of the model can significantly enhance its performance. In comparison to the hierarchical multi-label correlation model, the proposed model demonstrates enhanced performance across the evaluated metrics, indicating its efficacy in integrating multimodal features and providing more comprehensive and accurate data characterization.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Big} {Data} and {Digital} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Ke, Lanxin and Jian, Li and Liao, Wang and Chen, Yibin and Cai, Yangqi and Ye, Linmei},
	year = {2024},
	note = {event-place: Shanghai, China},
	pages = {369--379},
}

@inproceedings{orozco_ontology-based_2020,
	address = {New York, NY, USA},
	series = {{MEDES} '19},
	title = {An {Ontology}-{Based} {Thermal} {Comfort} {Management} {System} {In} {Smart} {Buildings}},
	isbn = {978-1-4503-6238-2},
	url = {https://doi.org/10.1145/3297662.3365824},
	doi = {10.1145/3297662.3365824},
	abstract = {Achieving thermal comfort for occupants in buildings has been the main focus of several studies in recent years. The challenging issue of the building envelope is to save energy and achieve a high comfortable environment simultaneously. To calculate the thermal comfort level in a living space, environmental factors such as indoor air temperature, mean radiant temperature, air velocity, and humidity are needed. The latter parameters are aggregated through the well known PMV index. In this paper, we introduce a wireless sensor network (WSN)-based comfort measurement approach, called OnCom, using a dedicated ontology and the emotional state analysis of the occupant to reach the "adequate" indoor thermal comfort. The main thrust of OnCom stands on the smooth connection of human emotions with the thermal sensations. Carried out experiments showed that emotions, unveiled from tweets, have been efficiently used to mitigate user thermal discomfort.},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Orozco, Adrian Taboada and Mouakher, Amira and Ben Sassi, Imen and Nicolle, Christophe},
	year = {2020},
	note = {event-place: Limassol, Cyprus},
	keywords = {Ontology, Sentiment Analysis, Smart Buildings, Thermal Comfort, Wireless Sensor Networks (WSN)},
	pages = {300--307},
}

@article{silverman_nlp_2022,
	title = {{NLP} {Methods} for {Extraction} of {Symptoms} from {Unstructured} {Data} for {Use} in {Prognostic} {COVID}-19 {Analytic} {Models}},
	volume = {72},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.12631},
	doi = {10.1613/jair.1.12631},
	abstract = {Statistical modeling of outcomes based on a patient's presenting symptoms (symptomatology) can help deliver high quality care and allocate essential resources, which is especially important during the COVID-19 pandemic. Patient symptoms are typically found in unstructured notes, and thus not readily available for clinical decision making. In an attempt to fill this gap, this study compared two methods for symptom extraction from Emergency Department (ED) admission notes. Both methods utilized a lexicon derived by expanding The Center for Disease Control and Prevention's (CDC) Symptoms of Coronavirus list. The first method utilized a word2vec model to expand the lexicon using a dictionary mapping to the Uni ed Medical Language System (UMLS). The second method utilized the expanded lexicon as a rule-based gazetteer and the UMLS. These methods were evaluated against a manually annotated reference (f1-score of 0.87 for UMLS-based ensemble; and 0.85 for rule-based gazetteer with UMLS). Through analyses of associations of extracted symptoms used as features against various outcomes, salient risks among the population of COVID-19 patients, including increased risk of in-hospital mortality (OR 1.85, p-value \&lt; 0.001), were identified for patients presenting with dyspnea. Disparities between English and non-English speaking patients were also identified, the most salient being a concerning finding of opposing risk signals between fatigue and in-hospital mortality (non-English: OR 1.95, p-value = 0.02; English: OR 0.63, p-value = 0.01). While use of symptomatology for modeling of outcomes is not unique, unlike previous studies this study showed that models built using symptoms with the outcome of in-hospital mortality were not significantly different from models using data collected during an in-patient encounter (AUC of 0.9 with 95\% CI of [0.88, 0.91] using only vital signs; AUC of 0.87 with 95\% CI of [0.85, 0.88] using only symptoms). These findings indicate that prognostic models based on symptomatology could aid in extending COVID-19 patient care through telemedicine, replacing the need for in-person options. The methods presented in this study have potential for use in development of symptomatology-based models for other diseases, including for the study of Post-Acute Sequelae of COVID-19 (PASC).},
	journal = {J. Artif. Int. Res.},
	author = {Silverman, Greg M. and Sahoo, Himanshu S. and Ingraham, Nicholas E. and Lupei, Monica and Puskarich, Michael A. and Usher, Michael and Dries, James and Finzel, Raymond L. and Murray, Eric and Sartori, John and Simon, Gyorgy and Zhang, Rui and Melton, Genevieve B. and Tignanelli, Christopher J. and Pakhomov, Serguei VS},
	month = jan,
	year = {2022},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
	keywords = {Information extraction, Natural language processing, COVID-19, UMLS},
	pages = {429--474},
}

@inproceedings{mansour_eql-ce_2019,
	address = {New York, NY, USA},
	series = {{IDEAS} '19},
	title = {{EQL}-{CE}: an event query language for connected environments},
	isbn = {978-1-4503-6249-8},
	url = {https://doi.org/10.1145/3331076.3331103},
	doi = {10.1145/3331076.3331103},
	abstract = {Recent advances in sensor technology and information processing have allowed connected environments to impact various application domains. In order to detect events in these environments, existing works rely on the sensed data. However, these works are not re-usable since they statically define the targeted events (i.e., the definitions are hard to modify when needed). Here, we present a generic framework for event detection composed of (i) a representation of the environment; (ii) an event detection mechanism; and (iii) an Event Query Language (EQL) for user/framework interaction. This paper focuses on detailing the EQL which allows the definition of the data model components, handles instances of each component, protects the security/privacy of data/users, and defines/detects events. We also propose a query optimizer in order to handle the dynamicity of the environment and spatial/temporal constraints. We finally illustrate the EQL and conclude the paper with some future works.},
	booktitle = {Proceedings of the 23rd {International} {Database} {Applications} \&amp; {Engineering} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Mansour, Elio and Chbeir, Richard and Arnould, Philippe},
	year = {2019},
	note = {event-place: Athens, Greece},
	keywords = {internet of things, event query language, sensor networks},
}

@inproceedings{gandhi_guided_2019,
	address = {New York, NY, USA},
	series = {{FDG} '19},
	title = {Guided open story generation using probabilistic graphical models},
	isbn = {978-1-4503-7217-6},
	url = {https://doi.org/10.1145/3337722.3341871},
	doi = {10.1145/3337722.3341871},
	abstract = {In this work, we present an approach for performing computational storytelling in open domain based on Author Goals. Author Goals are constraints placed on a story event directed by the author of the system. There are two challenges present in this type of story generation: (1) automatically acquiring a model of story progression, and (2) guiding the progress of story progression in light of different goals. We propose a novel approach to story generation based on probabilistic graphical models and Loopy Belief Propagation (LBP) that addresses both of these problems. We show the applicability of our technique through a case study on the Visual Storytelling (VIST) 2017 dataset. We use image descriptions as author goals. This empirical analysis suggests that our approach is able to utilize goals information to better automatically generate stories.},
	booktitle = {Proceedings of the 14th {International} {Conference} on the {Foundations} of {Digital} {Games}},
	publisher = {Association for Computing Machinery},
	author = {Gandhi, Sagar and Harrison, Brent},
	year = {2019},
	note = {event-place: San Luis Obispo, California, USA},
	keywords = {natural language generation, belief propagation, computational storytelling, probabilistic graphical models},
}

@inproceedings{khemiri_learn2construct_2021,
	address = {New York, NY, USA},
	series = {{MEDES} '21},
	title = {{Learn2Construct}: {An} automatic ontology construction based on {LDA} from texual data},
	isbn = {978-1-4503-8314-1},
	url = {https://doi.org/10.1145/3444757.3485110},
	doi = {10.1145/3444757.3485110},
	abstract = {In recent years, the research on Ontology Learning has become a hot topic among researchers because of the exponential increase of the number of documents and textual data not only on the web but also in digital libraries. This has participated to the emergence of new computational tools and methods to deal with the automatic organization, representation, retrieval and exploration of large corpus in order to have a good way of organizing and managing huge volumes of data. LDA-based approaches have proven to provide the best result [18][16] [4]. However, they suffers to several limitations related to concept and relation extraction, as well as coping with the corpus evolution. In order to cope with these problems, we propose here a new solution named Learn2Construct which is an automatic ontology construction method based on topic modeling. Experiments have been conducted to measure the effectiveness of our solution and compare it to existing ones. The results obtained are more than satisfactory.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Khemiri, Ahmed and Drissi, Amani and Tissaoui, Anis and Sassi, Salma and Chbeir, Richard},
	year = {2021},
	note = {event-place: Virtual Event, Tunisia},
	keywords = {Topic modeling, LDA, Ontology Learning, Text classification, Latent Dirichlet Allocation, Ontology based on topic modeling},
	pages = {49--56},
}

@inproceedings{ismail_improve_2021,
	address = {New York, NY, USA},
	series = {{ICFNDS} '20},
	title = {Improve the {Firewall} {Accuracy} {By} using {Dynamic} {Ontology}},
	isbn = {978-1-4503-8886-3},
	url = {https://doi.org/10.1145/3440749.3442607},
	doi = {10.1145/3440749.3442607},
	abstract = {Data is considered an important asset for organizations, companies, and even people. Crucial decisions depend mainly on data. Exchanging data is essential in order to negotiate ideas, thoughts, and decisions. Networks are the communication channels of data exchange although data is exposed to different attacks, threats, and loss. Because of this, data security has become a key concern for different parties through their daily data manipulation. There are different ways to ensure data security. Paying attention to network threats, data encryption, and using strong passwords are all examples. However, a firewall represents the first defense line against malicious traffic throughout the network. Firewalls have a set of rules to be applied in the time of data exchange between inside and outside of data networks. Some of the firewalls apply such rules in a sequential manner, which degrades the performance of the firewall. In this work, we are utilizing a dynamic ontology of different firewall rules managed by SPARQL queries, so that the rules are applied faster, and thus, increasing the firewall performance. Experimental results show that our proposed methodology totally eliminates the anomalies in the firewall rules as a result of conducting longest matching with proper rules from the dynamically constructed ontology.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Future} {Networks} and {Distributed} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ismail, Qossay and Saleh, Osama and Hashayka, Mohammed and Awad, Ahmed and Hawash, Amjad and Othman, Othman},
	year = {2021},
	note = {event-place: St.Petersburg, Russian Federation},
	keywords = {Ontology, SPARQL, RDF, Redundancy, Rule, Correlation, Anomalies, DOM, False Negative, False Positive, Firewall, Generalization, IP, Jena, Port, Shadowing, Software Defined Network.},
}

@inproceedings{pellegrino_quality_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {From {Quality} {Reports} to {Knowledge} {Graphs}: a {Case} {Study} on {CSV}-to-{KG} {Transformation}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3717809},
	doi = {10.1145/3701716.3717809},
	abstract = {The construction of Knowledge Graphs (KGs) often demands substantial manual effort and domain expertise, especially when converting structured data formats like CSV files into KGs. Recent advancements in Large Language Models (LLMs) offer promising avenues to simplify this process through prompt engineering.This study investigates various prompting strategies-zero-shot, one-shot, prompt chaining, and a hybrid approach-to enable LLMs to automate the creation of KGs from CSV files. Using a dataset containing quality metrics for 2,026 KGs generated by KGHeartBeat, the paper assesses the performance of GPT-4o, GPT-o1 mini, Claude 3.5 Sonnet, and Gemini 1.5 pro, across different prompt configurations. The findings reveal that the hybrid approach consistently produces the most accurate and complete KGs, effectively addressing challenges related to scalability and complexity.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Pellegrino, Maria Angela and Tuozzo, Gabriele},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {knowledge graph, prompt engineering, data quality, quality assessment, llms, comparison, csv converter, empirical investigation},
	pages = {1626--1632},
}

@inproceedings{verspoor_why_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Why {Bother} {Enabling} {Biomedical} {Literature} {Analysis} with {Semantics}?},
	isbn = {978-1-4503-9130-6},
	url = {https://doi.org/10.1145/3487553.3527164},
	doi = {10.1145/3487553.3527164},
	abstract = {These days, ELMo\&nbsp;[3], BERT\&nbsp;[1], BART\&nbsp;[2] and other similarly cutely-named models appear to have dramatically advanced the state of the art in basically every problem in natural language processing and information retrieval. It can leave a researcher wondering whether there is more to language processing than deploying or fine-tuning contextual word embeddings. What of formal semantics and knowledge representation? What value do these bring to text analysis, either in modelling or in task definitions? In this talk, I will try to explore these questions, from the perspective of my long-running experiences in biomedical information extraction and literature exploration. Perhaps we can shift the academic conversation from a one-model-fits-all solution for individual tasks to a more nuanced consideration of complex, multi-faceted problems in which such models certainly can play a critical role but aren’t necessarily “all you need”\&nbsp;[4].},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Verspoor, Karin},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {ontologies, text mining, knowledge representation, semantics, language models, word embeddings},
	pages = {822},
}

@inproceedings{yoon_unsupervised_2025,
	address = {New York, NY, USA},
	series = {{WSDM} '25},
	title = {Unsupervised {Robust} {Cross}-{Lingual} {Entity} {Alignment} via {Neighbor} {Triple} {Matching} with {Entity} and {Relation} {Texts}},
	isbn = {979-8-4007-1329-3},
	url = {https://doi.org/10.1145/3701551.3703500},
	doi = {10.1145/3701551.3703500},
	abstract = {Cross-lingual entity alignment (EA) enables the integration of multiple knowledge graphs (KGs) across different languages, providing users with seamless access to diverse and comprehensive knowledge. Existing methods, mostly supervised, face challenges in obtaining labeled entity pairs. To address this, recent studies have shifted towards self-supervised and unsupervised frameworks. Despite their effectiveness, these approaches have limitations: (1) Relation passing: mainly focusing on the entity while neglecting the semantic information of relations, (2) Isomorphic assumption: assuming isomorphism between source and target graphs, which leads to noise and reduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise in the textual features, especially when encountering inconsistent translations or Out-of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an unsupervised and robust cross-lingual EA pipeline that jointly performs \&lt;u\&gt;E\&lt;/u\&gt;ntity-level and \&lt;u\&gt;R\&lt;/u\&gt;elation-level \&lt;u\&gt;Align\&lt;/u\&gt;ment by neighbor triple matching strategy using semantic textual features of relations and entities. Its refinement step iteratively enhances results by fusing entity-level and relation-level alignments based on neighbor triple matching. The additional verification step examines the entities' neighbor triples as the linearized text. This Align-then-Verify pipeline rigorously assesses alignment results, achieving near-perfect alignment even in the presence of noisy textual features of entities. Our extensive experiments demonstrate that the robustness and general applicability of ERAlign improved the accuracy and effectiveness of EA tasks, contributing significantly to knowledge-oriented applications.},
	booktitle = {Proceedings of the {Eighteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Yoon, Soojin and Ko, Sungho and Kim, Tongyoung and Kang, SeongKu and Yeo, Jinyoung and Lee, Dongha},
	year = {2025},
	note = {event-place: Hannover, Germany},
	keywords = {knowledge graph, pretrained language models, optimal transport, cross-lingual entity alignment, neighbor triple matching},
	pages = {184--193},
}

@inproceedings{sartini_marriage_2021,
	address = {New York, NY, USA},
	series = {K-{CAP} '21},
	title = {Marriage is a {Peach} and a {Chalice}: {Modelling} {Cultural} {Symbolism} on the {Semantic} {Web}},
	isbn = {978-1-4503-8457-5},
	url = {https://doi.org/10.1145/3460210.3493552},
	doi = {10.1145/3460210.3493552},
	abstract = {In this work, we fill the gap in the Semantic Web in the context of Cultural Symbolism. Building upon earlier work in citesartini\_towards\_2021, we introduce the Simulation Ontology, an ontology that models the background knowledge of symbolic meanings, developed by combining the concepts taken from the authoritative theory of Simulacra and Simulations of Jean Baudrillard with symbolic structures and content taken from "Symbolism: a Comprehensive Dictionary” by Steven Olderr. We re-engineered the symbolic knowledge already present in heterogeneous resources by converting it into our ontology schema to create HyperReal, the first knowledge graph completely dedicated to cultural symbolism. A first experiment run on the knowledge graph is presented to show the potential of quantitative research on symbolism.},
	booktitle = {Proceedings of the 11th {Knowledge} {Capture} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Sartini, Bruno and van Erp, Marieke and Gangemi, Aldo},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {ontology, semantic web, knowledge graph, linked data, symbolism},
	pages = {201--208},
}

@inproceedings{yang_encoding_2023,
	address = {New York, NY, USA},
	series = {{MM} '23},
	title = {Encoding and {Decoding} {Narratives}: {Datafication} and {Alternative} {Access} {Models} for {Audiovisual} {Archives}},
	isbn = {979-8-4007-0108-5},
	url = {https://doi.org/10.1145/3581783.3613434},
	doi = {10.1145/3581783.3613434},
	abstract = {Situated in the intersection of audiovisual archives, computational methods, and immersive interactions, this work probes the increasingly important accessibility issues from a two-fold approach. Firstly, the work proposes an ontological data model to handle complex descriptors (metadata, feature vectors, etc.) with regard to user interactions. Secondly, this work examines text-to-video retrieval from an implementation perspective by proposing a classifier-enhanced workflow to deal with complex and hybrid queries and a training data augmentation workflow to improve performance. This work serves as the foundation for experimenting with novel public-facing access models to large audiovisual archives.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Yuchen},
	year = {2023},
	note = {event-place: Ottawa ON, Canada},
	keywords = {audiovisual archive, computational archive, experimental museology, text-to-video encoding},
	pages = {9355--9359},
}

@inproceedings{bernard_modeling_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {Modeling changes in territorial partitions over time: ontologies {TSN} and {TSN}-change},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167227},
	doi = {10.1145/3167132.3167227},
	abstract = {Territories are governed, administered and observed from partitions of space into territorial units. All these territorial partitions change over time, for political or administrative reasons. In this paper, we present two innovative ontologies - Territorial Statistical Nomenclature (TSN) and TSN-Change - for the modeling of territorial partitions over time, adopting a Linked Data (LD) approach understandable by both humans and machines. These ontologies are innovative as they are generic, enabling the publication of any territorial partition into the LD Web. Above all, they allow for rich descriptions of changes, from one partition version to another, through Multi-Levels Change Graphs that link together changes that happen at different territorial levels (e.g., major regions or districts levels). These RDF graphs constitute a knowledge base to extract patterns of change and simulate scenarios.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Bernard, Camille and Villanova-Oliver, Marlène and Gensel, Jérôme and Dao, Hy},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {change ontology, space and time ontology, territorial statistical nomenclature, web of linked data},
	pages = {866--875},
}

@inproceedings{arruda_toward_2019,
	address = {New York, NY, USA},
	series = {{SAC} '19},
	title = {Toward a lightweight ontology for privacy protection in {IoT}},
	isbn = {978-1-4503-5933-7},
	url = {https://doi.org/10.1145/3297280.3297367},
	doi = {10.1145/3297280.3297367},
	abstract = {The literature asserts that the design of an ontology-based privacy model is an essential starting point to address privacy risks in IoT, where connected devices are increasingly capable of monitoring human activities. Due to the omnipresence of data privacy concerns in IoT, we highlight the need for privacy ontologies that combine an expressive vocabulary with extension points but that do not overload the processing of privacy policies data. This paper presents IoT-Priv as a lightweight privacy layer upon IoT basic concepts such as device, sensor, and service. We introduce privacy requirements guiding the IoT-Priv ontology design, match these requirements to the respective privacy terms modeled, and show how to use IoT-Priv through a usage scenario. Finally, we evaluate static metrics and response times of spatial and temporal query filters over instances of privacy policies. Results open the way for the creation of scalable, privacy-enabled systems.},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Arruda, Mayke Ferreira and Bulcão-Neto, Renato Freitas},
	year = {2019},
	note = {event-place: Limassol, Cyprus},
	keywords = {ontology, internet of things, evaluation, privacy, requirements},
	pages = {880--888},
}

@inproceedings{hsu_thought_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Thought {Graph}: {Generating} {Thought} {Process} for {Biological} {Reasoning}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3651572},
	doi = {10.1145/3589335.3651572},
	abstract = {We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28\% and LLM baselines by 5.38\% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Hsu, Chi-Yang and Cox, Kyle and Xu, Jiawei and Tan, Zhen and Zhai, Tianhua and Hu, Mengzhou and Pratt, Dexter and Chen, Tianlong and Hu, Ziniu and Ding, Ying},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {large language model, Large language model, Natural language processing, Language model, Semantic Web, Bioinformatics, natural language processing, Gene Ontology, semantic web, Gene ontology, bioinformatics, Language processing, Genes, gene ontology, biological process., Natural languages, Gene sets, Natural language processing systems, Biological process, Semantic-Web, Semantic web biological process},
	pages = {537--540},
	annote = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{hviid_service_2022,
	address = {New York, NY, USA},
	series = {{IoT} '21},
	title = {Service {Portability} and {Information} {Discovery} in {Building} {Operating} {Systems} using {Semantic} {Modeling}},
	isbn = {978-1-4503-8566-4},
	url = {https://doi.org/10.1145/3494322.3494337},
	doi = {10.1145/3494322.3494337},
	abstract = {To achieve cost-efficient IoT based Building Operating Systems (BOS), portable building services are needed. Most previous work has gone into hardware abstraction, while service abstraction has been neglected. This paper presents an information discovery mechanism for BOSs that is based on an ontology which integrates with other semantic models used in the space. The built environment is characterized by extreme heterogeneity; no buildings are entirely alike. Equipment is replaced or updated, control systems and building functionality evolve, as applications, models, forecasters, and controllers improve. For services deployed in such settings to operate at a scale, they must be robust to change. Describing service interfaces using a semantic model, together with the physical context in a building, enables applications to query for their service dependencies. Applications then depend on an abstract query, instead of specific services. For evaluation, nine services running on models of the service ecosystems of three concrete buildings, are implemented, demonstrated, and discussed. Results show services have successfully been made portable and adapts to the changing environments of buildings. Merging and splitting services without code changes to depending services also work as intended, as well as increasing system resilience, by arbitrating similar services.},
	booktitle = {Proceedings of the 11th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Hviid, Jakob and Johansen, Aslak and Fierro, Gabe and Kjærgaard, Mikkel Baun},
	year = {2022},
	note = {event-place: St.Gallen, Switzerland},
	keywords = {Ontology, Service Discovery, Dependency Resolution., Information Discovery, Service Abstraction},
	pages = {110--117},
}

@inproceedings{alaa_personalized_2020,
	address = {New York, NY, USA},
	series = {{ICCTA} '20},
	title = {Personalized {Recommendation} for {Online} {Retail} {Applications} {Based} on {Ontology} {Evolution}},
	isbn = {978-1-4503-7749-2},
	url = {https://doi.org/10.1145/3397125.3397134},
	doi = {10.1145/3397125.3397134},
	abstract = {The upcoming generation of World Wide Web is signified in semantic web technology that allows future applications to grasp and connect with numerous knowledge bases. Due to its exclusive function in modeling specific domain, Ontology has been playing an essential role in semantic web development. Recommender systems are an indispensable part of online site, which makes their use of high value in recommending items to users according to their interests. The semantic recommender systems recently aim to accomplish the website ontologies to generate semantic recommendations for users' profiles. Therefore, ontology-based semantic recommender systems are used to develop web recommendation. In this paper a recommendation system architecture based on ontology is proposed to give semantic recommendations for each user profile. The proposed system architecture applies two recommendation techniques, content-based filtering and collaborative filtering.},
	booktitle = {Proceedings of the 2020 6th {International} {Conference} on {Computer} and {Technology} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Alaa, Rana and Gawich, Mariam and Fernández-Veiga, Manuel},
	year = {2020},
	note = {event-place: Antalya, Turkey},
	keywords = {ontology, reasoning, ontology evolution, intelligent personalization, Semantic recommender system},
	pages = {12--16},
}

@inproceedings{ichida_modeling_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {Modeling a {Conversational} {Agent} using {BDI} {Framework}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3577657},
	doi = {10.1145/3555776.3577657},
	abstract = {Building conversational agents to help humans in domain-specific tasks is challenging since the agent needs to understand the natural language and act over it while accessing domain expert knowledge. Modern natural language processing techniques led to an expansion of conversational agents, with recent pretrained language models achieving increasingly accurate language recognition results using ever-larger open datasets. However, the black-box nature of such pretrained language models obscures the agent's reasoning and its motivations when responding, leading to unexplained dialogues. We develop a belief-desire-intention (BDI) agent as a task-oriented dialogue system to introduce mental attitudes similar to humans describing their behavior during a dialogue. We compare the resulting model with a pipeline dialogue model by leveraging existing components from dialogue systems and developing the agent's intention selection as a dialogue policy. We show that combining traditional agent modelling approaches, such as BDI, with more recent learning techniques can result in efficient and scrutable dialogue systems.},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Ichida, Alexandre Yukio and Meneguzzi, Felipe},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {machine learning, task-oriented dialogue systems, autonomous agent, belief-desire-intention},
	pages = {856--863},
}

@inproceedings{hong_reference_2023,
	address = {New York, NY, USA},
	series = {{ICCVIT} '23},
	title = {A {Reference} {Model} for {Information} {Security} {Applications} of {Generative} {Adversarial} {Network} {Variants}},
	isbn = {979-8-4007-0870-1},
	url = {https://doi.org/10.1145/3627341.3630381},
	doi = {10.1145/3627341.3630381},
	abstract = {Information security stemming from Generative Adversarial Network (GAN) variants has garnered significant attention. However, a complete reference model targeting this security problem has yet to be established. This paper selects several GAN variants as the research subject and proposes a reference model framework for the information security applications of adversarial generative network variants. The proposed framework is derived using the NIST information security reference model methodology. By conducting a comprehensive analysis of the structure and information security risks of GAN variants, this paper classifies information security attacks on information systems of GAN variants into three categories and maps them onto the security target reference model. The resulting security application reference model can serve as a basis and reference for improving system confidentiality, integrity, and availability, as well as facilitating the design, analysis, and verification of security against malicious attacks. Moreover, the research method employed in this paper is also applicable to information security research of other types of information systems. Therefore, the proposed reference model framework can serve as a valuable contribution to the field of information security and advance the development of effective countermeasures against adversarial generative network variants.},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Computer}, {Vision} and {Intelligent} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Hong, Sheng and Lai, Yizhong and Li, Yuzhou and You, Yang and Ou, Shuai and Han, Fei and Wu, Tiejun},
	year = {2023},
	note = {event-place: Chenzhou, China},
	keywords = {Information Security, Adversarial Generative Network Variant, Security Reference Model, System Security},
}

@article{musso_what_2024,
	title = {What {Is} in a \&lt;unittitle\&gt;? {Cross}-lingual {Topic} {Detection} \&amp; {Information} {Retrieval} in {Archives} {Portal} {Europe}},
	volume = {17},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3494572},
	doi = {10.1145/3494572},
	abstract = {Archives Portal Europe (APE, ) is the portal of European archives, an aggregator that connects on a single research point the catalogues and digitised archival material of all archives in and about Europe. It currently hosts material from more than 30 countries and from a variety of archival institutions (such as State archives, city archives, university and parish archives, private institutions, and more). It is maintained by the Archives Portal Europe Foundation, an international consortium of State archives and other archival institutions that aim to connect the archival material of single institutions into one digital repository to allow universal access to the archival heritage of Europe, promoting new forms of archival research beyond national or local boundaries. One of the research tools made available by Archives Portal Europe is by topics; however, these are currently maintained manually by the archivists, and the vast amount of archival material ingested in the portal makes it impossible to have a comprehensive body of topics that describe the whole of the APE repository. Archives are traditionally not organised by their subject content, but around the entity (person, organization, body) that created and/or collected the documents in the course of their activities. While this is an undisputed pillar of archival management, the availability of online digital repositories for archival research requires new tools for digital archival research, particularly when different archival traditions from different countries and different types of institutions are merged into a unique research portal. Topic detection becomes a fundamental tool to guide archival research and to allow archives to be accessible to potentially world-wide users in a situation where national and linguistics barriers blur or are re-defined. This article presents the preliminary results and plan for future iterations of an AI tool for automated topic detection in a multi- lingual environment, where human-created taxonomies act as bases for the algorithms to aggregate relevant material around a specific topic. The development is based on supervised machine learning, with a combination of human inputs in different languages, and of the usage of Wikipedia pages to model the relevant vocabulary and entities.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Musso, Marta and Arnold, Kerstin and Nanni, Federico and Cannelli, Beatrice},
	month = mar,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {search engines, Archives Portal Europe, Automated topic modelling, Digital Archives, Digital Research},
}

@inproceedings{laadhar_partitioning_2019,
	address = {New York, NY, USA},
	series = {{SAC} '19},
	title = {Partitioning and local matching learning of large biomedical ontologies},
	isbn = {978-1-4503-5933-7},
	url = {https://doi.org/10.1145/3297280.3297507},
	doi = {10.1145/3297280.3297507},
	abstract = {Conventional ontology matching systems are not well-tailored to ensure sufficient quality alignments for large ontology matching tasks. In this paper, we propose a local matching learning strategy to align large and complex biomedical ontologies. We define a novel partitioning approach that breakups large ontology alignment task into a set of local sub-matching tasks. We perform a machine learning approach for each local sub-matching task. We build a local machine learning model for each sub-matching task without any user involvement. Each local matching learning model automatically provides adequate matching settings for each local sub-matching task. Our results show that: (i) partitioning approach outperforms existing techniques, (ii) local matching while using a specific machine learning model for each sub-matching task yields to promising results and (iii) the combination between partitioning and machine learning increases the overall result.},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Laadhar, Amir and Ghozzi, Faiza and Megdiche, Imen and Ravat, Franck and Teste, Olivier and Gargouri, Faiez},
	year = {2019},
	note = {event-place: Limassol, Cyprus},
	keywords = {machine learning, semantic web, ontology matching, ontology partitioning},
	pages = {2285--2292},
}

@article{adamou_facets_2023,
	title = {The {Facets} of {Intangible} {Heritage} in {Southern} {Chinese} {Martial} {Arts}: {Applying} a {Knowledge}-driven {Cultural} {Contact} {Detection} {Approach}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3606702},
	doi = {10.1145/3606702},
	abstract = {Investigating the intangible nature of a cultural domain can take multiple forms, addressing, for example, the aesthetic, epistemic, and social dimensions of its phenomenology. The context of Southern Chinese martial arts is of particular significance, as it carries immaterial components of all these aspects: The technical and stylistic framework of a martial art system; the imagery associated to movements; and the transmission of knowledge orally, practically, or through influence, are but examples of intangible characteristics that can and should be captured, not unlike cultural artifacts. The latter case—the one of formalizing cultural influence through its various forms of evidenceis emblematic as well as largely untrodden ground. A previous attempt at detecting cultural influence computationally was made in the context of Roman archaeology, though the binding of that early effort with the domain model was tight; also, there has hardly been any prior dedicated effort to model the martial arts domain through ontologies. In this article, we present the realization of the full cycle of a computational approach to investigating cultural contact in Southern Chinese martial arts. The entire approach is predicated upon the usage of standards and techniques of the Semantic Web and formal knowledge. Starting from a modular domain ontology, which models martial arts independently of the goal of capturing cultural influence, we perform knowledge extraction from archival material from the Hong Kong Martial Arts Living Archive and generate a dataset of the results modeled after said ontology. Then, we combine the resulting knowledge base with a rule model that represents ways to infer knowledge of potential contact between cultures based on the evidence present in the knowledge base. The results offer an insight into how an inference-based computational model can be applied to detect interesting facts even in the as-yet underexplored domain of intangible cultural heritage. The implemented workflow shows that the full-cycle employment of semantic technologies can offer the ground truth required for largely different approaches, such as statistical and machine learning ones, to operate.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Adamou, Alessandro and Picca, Davide and Hou, Yumeng and Loreto Granados-García, Paula},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Semantic Web, ontologies, knowledge representation, Intangible cultural heritage, digitization, embodied knowledge, inferencing},
}

@article{chen_using_2022,
	title = {Using {Fuzzy} {Clustering} with {Deep} {Learning} {Models} for {Detection} of {COVID}-19 {Disinformation}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3548458},
	doi = {10.1145/3548458},
	abstract = {Since the beginning of 2020, the COVID-19 pandemic has killed millions of people around the world, leading to a worldwide panic that has fueled the rapid and widespread dissemination of COVID-19-related disinformation on social media. The phenomenon, described by the World Health Organization (WHO) as an "indodemic" presents a serious challenge to governments and public health authorities, but the spread of misinformation has made human detection less efficient than the rate of spread. While there have been many studies developing automated detection techniques for COVID-19 fake news, the results often refer to high accuracy but rarely to model detection time. This research uses fuzzy theory to extract features and uses multiple deep learning model frameworks to detect Chinese and English COVID-19 misinformation. With the reduction of text features, the detection time of the model is significantly reduced, and the model accuracy does not drop excessively. This study designs two different feature extraction methods based on fuzzy classification and compares the results with different deep learning models. BiLSTM was found to provide the best detection results for COVID-19 misinformation by directly using deep learning models, with 99\% accuracy in English and 86\% accuracy in Chinese. Applying fuzzy clustering to English COVID-19 fake news data features maintains 99\% accuracy while reducing detection time by 10\%. For Chinese misinformation, detection time is reduced by 15\% at the cost of an 8\% drop in accuracy.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Chen, Mu-Yen and Lai, Yi-Wei},
	month = jul,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {COVID-19, Fuzzy clustering, Deep learning model, Misinformation detection},
	annote = {Just Accepted},
}

@article{bienvenu_ontology-mediated_2018,
	title = {Ontology-{Mediated} {Queries}: {Combined} {Complexity} and {Succinctness} of {Rewritings} via {Circuit} {Complexity}},
	volume = {65},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/3191832},
	doi = {10.1145/3191832},
	abstract = {We give solutions to two fundamental computational problems in ontology-based data access with the W3C standard ontology language OWL\&nbsp;2\&nbsp;QL: the succinctness problem for first-order rewritings of ontology-mediated queries (OMQs) and the complexity problem for OMQ answering. We classify OMQs according to the shape of their conjunctive queries (treewidth, the number of leaves) and the existential depth of their ontologies. For each of these classes, we determine the combined complexity of OMQ answering and whether all OMQs in the class have polynomial-size first-order, positive existential, and nonrecursive datalog rewritings. We obtain the succinctness results using hypergraph programs, a new computational model for Boolean functions, which makes it possible to connect the size of OMQ rewritings and circuit complexity.},
	number = {5},
	journal = {J. ACM},
	author = {Bienvenu, Meghyn and Kikot, Stanislav and Kontchakov, Roman and Podolskii, Vladimir V. and Zakharyaschev, Michael},
	month = aug,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology-based data access, query rewriting, computational complexity, ontology-mediated query, succinctness},
}

@inproceedings{gu_ontology-based_2023,
	address = {New York, NY, USA},
	series = {{IJCKG} '22},
	title = {Ontology-based {Data} {Federation}},
	isbn = {978-1-4503-9987-6},
	url = {https://doi.org/10.1145/3579051.3579070},
	doi = {10.1145/3579051.3579070},
	abstract = {Ontology-based data access (OBDA) is a well-established approach to information management which facilitates the access to a (single) relational data source through the mediation of a high-level ontology, and the use of a declarative mapping linking the data layer to the ontology. We formally introduce here the notion of ontology-based data federation (OBDF) to denote a framework that combines OBDA with a data federation layer where multiple, possibly heterogeneous sources are virtually exposed as a single relational database. We discuss opportunities and challenges of OBDF, and provide techniques to deliver efficient query answering in an OBDF setting. Such techniques are validated through an extensive experimental evaluation based on the Berlin SPARQL Benchmark.},
	booktitle = {Proceedings of the 11th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Gu, Zhenzhen and Lanti, Davide and Mosca, Alessandro and Xiao, Guohui and Xiong, Jing and Calvanese, Diego},
	year = {2023},
	note = {event-place: Hangzhou, China},
	keywords = {Data federation, OBDA, Query optimization},
	pages = {10--19},
}

@inproceedings{paim_codesign_2018,
	address = {New York, NY, USA},
	series = {{IHC} '18},
	title = {{CoDesign} in the {Exploratory} {Phase} of an {Assistive} {Technology} product {Design} to support the {Teaching}-{Learning} {Process} of {Brazilian}-{Portuguese} {Language} for {Visual} {Persons}},
	isbn = {978-1-4503-6601-4},
	url = {https://doi.org/10.1145/3274192.3274204},
	doi = {10.1145/3274192.3274204},
	abstract = {Libras is a communication and expression language of many Visual Person (VP) in Brazil. In National High School Examination (ENEM), for candidates to a vacancy in higher education institutions, the essay must be written in Portuguese, not taking into account the first language of the candidate. This paper presents an adapted framework that groups concepts of CoDesign, technology adoption, HCI life cycle and Semantic Numbers, and also shows the results of first phase of research. This phase consists of a contextual analysis of an Assistive Technology project for Portuguese teaching as a second language for VP. The adapted framework aims to reach objectives, working with interested parts, to be accessible and potentially adopted by them. As results we have defined Interested Parts Diagram, discussed responses applied surveys and elaborated Personas. Also, it was possible to observe the relation of the PV with the Portuguese, their wishes and difficulties with ENEM.},
	booktitle = {Proceedings of the 17th {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Paim, Polianna and Prietch, Soraia and Duarte, Anderson},
	year = {2018},
	note = {event-place: Belém, Brazil},
	keywords = {Assistive Technology, CoDesign, Semantic Numbers, Visual People},
}

@inproceedings{chen_ontology_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {Ontology {Extraction} for {Large} {Ontologies} via {Modularity} and {Forgetting}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364424},
	doi = {10.1145/3360901.3364424},
	abstract = {We are interested in the computation of ontology extracts based on forgetting from large ontologies in real-world scenarios. Such scenarios require nearly all of the terms in the ontology to be forgotten, which poses a significant challenge to forgetting tools. In this paper we show that modularization and forgetting can be combined beneficially in order to compute ontology extracts. While a module is a subset of axioms of a given ontology, the solution of forgetting (also known as a uniform interpolant) is a compact representation of the ontology limited to a subset of the signature. The approach introduced in this paper uses an iterative workflow of four stages: (i) extension of the given signature and, if needed partitioning, (ii) modularization, (iii) forgetting, and (iv) evaluation by domain expert. For modularization we use three kinds of modules: locality-based, semantic and minimal subsumption modules. For forgetting three tools are used: NUI, LETHE and FAME. An evaluation on the SNOMED CT and NCIt ontologies for standard concept name lists showed that precomputing ontology modules reduces the number of terms that need to be forgotten. An advantage of the presented approach is high precision of the computed ontology extracts.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Jieying and Alghamdi, Ghadah and Schmidt, Renate A. and Walther, Dirk and Gao, Yongsheng},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {knowledge management, knowledge representation and reasoning, forgetting, uniform interpolation, description logics, knowledge abstraction, ontology abstraction, ontology modularity},
	pages = {45--52},
}

@inproceedings{jiang_digital_2023,
	address = {USA},
	series = {{CASCON} '23},
	title = {Digital {Twin} {Models} for {Resource} {Oriented} {Service} {Systems}},
	abstract = {Resource-oriented service computing has emerged as one of the major paradigms for building large scale distributed systems. These systems can grow very complex and require access to diverse re-sources. In order to shorten the development time of such systems, we can consider the use of digital twin models. We propose a frame-work whereby a resource-oriented service computing system can be represented as a collection of models that denote three major perspectives, namely structure, behavior and intent of stakeholders’ goals. Furthermore, in order for the digital twin model to be of practical use, the models denoting the different perspectives have to be integrated. In this respect, we propose an architecture which allows these models to exchange information using open source topic-based publish-subscribe systems, and discuss the elements of its associated run-time engine.},
	booktitle = {Proceedings of the 33rd {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Jiang, Hao and Tsiounis, Konstantinos and Kontogiannis, Kostas},
	year = {2023},
	note = {event-place: Las Vegas, NV, USA},
	keywords = {REST, DevOps, Digital Twin Models, Middleware., Resource Oriented Systems, Service Computing},
	pages = {226--229},
}

@inproceedings{glass_innovative_2024,
	address = {New York, NY, USA},
	series = {{CIIS} '23},
	title = {Innovative {Urban} {Design} {Simulation}: {Utilizing} {Agent}-{Based} {Modelling} through {Reinforcement} {Learning}},
	isbn = {979-8-4007-0906-7},
	url = {https://doi.org/10.1145/3638209.3638213},
	doi = {10.1145/3638209.3638213},
	abstract = {Data-driven design for cities is improving the quality of everyday life of citizens and optimizes the usage of resources. A new aspect is artificial intelligence, which Smart Cities could greatly benefit from. A central problem for urban designers is the unavailability of data to make relevant decisions. Agent-based simulations enable a view of the dynamic properties of the urban system, generating data in its course. However, the simulation must remain sufficiently simple to remain in the realm of computability. The research question of this paper is: How can we make agents behave more realistically to analyze citizens’ mobility behavior? To solve this problem, we first created a simulated virtual environment, where agents can move freely in a small part of a city, the harbor area in Hamburg, Germany. We assumed that happiness is a crucial motivating factor for the movement of citizens. A survey of 130 citizens provided the weights that govern the simulated environment and the happiness score assignation of places. As an AI method, we then used Reinforcement Learning as a general model and Q-learning as an algorithm to generate a baseline. Through randomly traversing the model environment a baseline was created. We are in the process of enhancing Reinforcement Learning with a Deep Q-Network to make the actors learn. Early experiments show a significant improvement over a tabular Q-learning approach. This paper contributes to the literature of urban planning, and data-driven architectural design. The main contribution is replacing the inefficient search for a global maximum of the happiness function, with an efficient local solution global maximum. This has implications for further research in the generation of synthetic data through simulations.},
	booktitle = {Proceedings of the 2023 6th {International} {Conference} on {Computational} {Intelligence} and {Intelligent} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Glass, Ayse and Noennig, Jorg R. and Bek, Burak and Glass, Roman and Menges, Eylul K. and Okhrin, Iryna and Baddam, Pramod and Sanchez, Mariela Rossana and Senthil, Gunalan and Jäkel, René},
	year = {2024},
	note = {event-place: Tokyo, Japan},
	keywords = {artificial intelligence, agent-based modeling, synthetic data, smart cities, city simulations, urban design},
	pages = {20--25},
}

@article{tian_gogcn_2022,
	title = {{GOGCN}: {Graph} {Convolutional} {Network} on {Gene} {Ontology} for {Functional} {Similarity} {Analysis} of {Genes}},
	volume = {20},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2022.3181300},
	doi = {10.1109/TCBB.2022.3181300},
	abstract = {The measurement of gene functional similarity plays a critical role in numerous biological applications, such as gene clustering, the construction of gene similarity networks. However, most existing approaches still rely heavily on traditional computational strategies, which are not guaranteed to achieve satisfactory performance. In this study, we propose a novel computational approach called \&lt;bold\&gt;GOGCN\&lt;/bold\&gt; to measure gene functional similarity by modeling the Gene Ontology (\&lt;bold\&gt;GO\&lt;/bold\&gt;) through Graph Convolutional Network (\&lt;bold\&gt;GCN\&lt;/bold\&gt;). GOGCN is a graph-based approach that performs sufficient representation learning for terms and relations in the GO graph. First, GOGCN employs the GCN-based knowledge graph embedding (KGE) model to learn vector representations (i.e., embeddings) for all entities (i.e., terms). Second, GOGCN calculates the semantic similarity between two terms based on their corresponding vector representations. Finally, GOGCN estimates gene functional similarity by making use of the pair-wise strategy. During the representation learning period, GOGCN promotes semantic interaction between terms through GCN, thereby capturing the rich structural information of the GO graph. Further experimental results on various datasets suggest that GOGCN is superior to the other state-of-the-art approaches, which shows its reliability and effectiveness.},
	number = {2},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Tian, Zhen and Fang, Haichuan and Teng, Zhixia and Ye, Yangdong},
	month = jun,
	year = {2022},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {1053--1064},
}

@inproceedings{li_study_2023,
	address = {New York, NY, USA},
	series = {{ICCMS} '23},
	title = {Study on the {Reference} {Architecture} and {Multi}-{Dimensional} {Fusion} {Framework} of {Production} {Equipment} {Models}},
	isbn = {979-8-4007-0791-9},
	url = {https://doi.org/10.1145/3608251.3608255},
	doi = {10.1145/3608251.3608255},
	abstract = {With the accelerated integration of the new generation information technology and manufacturing industry, the performance of production equipment continues to be optimized, evolved and upgraded iteratively. The production equipment model presents a multi-dimensional, multi-level, full life cycle integration development trend, which has become the key to improve the innovative capability and digital management level of production equipment. Aiming at the problems of multiple types of production equipment models, such as complex structure, difficult integration and low application efficiency, this study analyzed the relationship and interaction mechanism among production equipment models from the perspectives of life cycle, hierarchical structure and multi-dimension, and built a production equipment model architecture. Based on the model architecture of production equipment, a general multi-dimensional fusion framework of production equipment models was proposed, which takes the mechanism model as the core. This study can provide reference for the construction, integration and application of production equipment models.},
	booktitle = {Proceedings of the 2023 15th {International} {Conference} on {Computer} {Modeling} and {Simulation}},
	publisher = {Association for Computing Machinery},
	author = {Li, Jun and Zhou, Yong and Liu, Xin and Liu, Jinsong and Li, Qing and Kong, Xiangjun and Niu, Guankai},
	year = {2023},
	note = {event-place: Dalian, China},
	pages = {172--183},
}

@inproceedings{sebbaq_recommender_2020,
	address = {New York, NY, USA},
	series = {{SITA}'20},
	title = {Recommender {System} to {Support} {MOOCs} {Teachers}: {Framework} based on {Ontology} and {Linked} {Data}},
	isbn = {978-1-4503-7733-1},
	url = {https://doi.org/10.1145/3419604.3419619},
	doi = {10.1145/3419604.3419619},
	abstract = {The proliferation of Massive Open Online Courses (MOOCs) has generated conflicting opinions about their quality. In this paper, we aim at improving the quality of MOOCs through assisting teachers and designers from the initiation phase of MOOCs. For this purpose, we propose a recommendation system Framework based on the knowledge about teachers and MOOCs. Our approach aims to overcome the problems of traditional recommendation systems, by using and integrating different techniques: modeling via ontologies, semantic web technologies, extracting and integrating Linked Data from different sources, ontology mapping and semantic similarity measures.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Intelligent} {Systems}: {Theories} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Sebbaq, Hanane and el Faddouli, Nour-eddine and Bennani, Samir},
	year = {2020},
	note = {event-place: Rabat, Morocco},
	keywords = {Ontology, Ontology mapping, Semantic Web, Semantic similarity, Linked Data, MOOC, Recommender System},
}

@article{atrey_w4-groups_2024,
	title = {W4-{Groups}: {Modeling} the {Who}, {What}, {When} and {Where} of {Group} {Behavior} via {Mobility} {Sensing}},
	volume = {8},
	url = {https://doi.org/10.1145/3637427},
	doi = {10.1145/3637427},
	abstract = {Human social interactions occur in group settings of varying sizes and locations, depending on the type of social activity. The ability to distinguish group formations based on their purposes transforms how group detection mechanisms function. Not only should such tools support the effective detection of serendipitous encounters, but they can derive categories of relation types among users. Determining who is involved, what activity is performed, and when and where the activity occurs are critical to understanding group processes in greater depth, including supporting goal-oriented applications (e.g., performance, productivity, and mental health) that require sensing social factors. In this work, we propose W4-Groups that captures the functional perspective of variability and repeatability when automatically constructing short-term and long-term groups via multiple data sources (e.g., WiFi and location check-in data). We design and implement W4-Groups to detect and extract all four group features who-what-when-where from the user's daily mobility patterns. We empirically evaluate the framework using two real-world WiFi datasets and a location check-in dataset, yielding an average of 92\% overall accuracy, 96\% precision, and 94\% recall. Further, we supplement two case studies to demonstrate the application of W4-Groups for next-group activity prediction and analyzing changes in group behavior at a longitudinal scale, exemplifying short-term and long-term occurrences.},
	number = {CSCW1},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Atrey, Akanksha and Zakaria, Camellia and Balan, Rajesh and Shenoy, Prashant},
	month = apr,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {group modeling, next activity prediction, social interactions, user mobility},
}

@inproceedings{xiao_model_2023,
	address = {New York, NY, USA},
	series = {{ICCDA} '23},
	title = {Model construction and implementation of digital twin data for body workshop},
	isbn = {979-8-4007-0057-6},
	url = {https://doi.org/10.1145/3629264.3629273},
	doi = {10.1145/3629264.3629273},
	abstract = {Data is very important for Digital Twin Workshop (DTW). Data in DTW is not only to store it, but also needs to be associated with different physical information. Knowledge graph (KG) can acquire, integrate and utilize the information of the whole life of cycle process. This paper proposes the construction of digital twin data (DTD) by creation process and feature extraction methods of knowledge graph. The DTD combines the actual production data and the time-varying characteristics of geometry, motion and behavior model of the body workshop. It is able to realize the integration with DTW and graph database storage.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Computing} and {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Xiao, Zheng and Cao, Haowei and Zheng, Dongwei},
	year = {2023},
	note = {event-place: Guiyang, China},
	keywords = {Knowledge graph, Data model, Digital twin workshop},
	pages = {1--7},
}

@inproceedings{zarri_using_2021,
	address = {New York, NY, USA},
	series = {{ICIST} '20},
	title = {Using a {High}-{Level} {Conceptual} {Model} as a {Support} for the {Generalized} {World} {Entities} ({GWEs}) {Paradigm}},
	isbn = {978-1-4503-7655-6},
	url = {https://doi.org/10.1145/3447568.3448547},
	doi = {10.1145/3447568.3448547},
	abstract = {This paper concerns the Generalized World Entities (GWEs) paradigm, an extension of the ordinary IoT/WoT (Internet of Things/Web of Things) approach. GWEs offer a unified way to seamlessly model i) conceptual representations of physical objects, humans, robots and low-level events and ii) higher level of abstractions corresponding to structured situations/behaviors implying mutual relationships among low level entities. The GWEs approach is currently implemented in terms of NKRL, the "Narrative Knowledge Representation Language", which is both a Knowledge Representation language and a Computer Science environment. It is expected to represent a significant contribution with respect to bridge the gap between the recognition of entities at sensor level and their description/management at conceptual level.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Information} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Zarri, Gian Piero},
	year = {2021},
	note = {event-place: Lecce, Italy},
	keywords = {ontologies, semantic approach, Generalized World Entities, high-level abstraction entities, inferences, IoT/WoT, physical entities, seamless conceptual modelling},
}

@inproceedings{besbes_fuzzy_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {Fuzzy ontologies for search results diversification: application to medical data},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167343},
	doi = {10.1145/3167132.3167343},
	abstract = {Fuzzy ontologies offer an efficient representation of uncertain information in natural language and this representation allows a better interpretation of user queries and documents. Integrating fuzzy ontologies in a search results diversification process may improve the quality of returned documents since diversification helps covering the maximum of user's needs. In this context, we propose an ontology based diversification approach for search results applied to medical domain. The proposal first analyses the query in order to extract medical concepts. A contextual ontology fuzzification is then applied in order to offer an understanding of the user's information needs and finally a fuzzy search result diversification is performed in order to improve the ranking quality of returned documents. We perform a thorough experimental evaluation of our proposal with CLEF e-health 2016 topics. Evaluation results show a major improvement in precision and ranking.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Besbes, Ghada and Baazaoui-Zghal, Hajer},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {fuzzy ontology, information retrieval, medical data, search results diversification, semantic web},
	pages = {1968--1975},
}

@inproceedings{nast_meta-model_2022,
	address = {New York, NY, USA},
	series = {{AISS} '21},
	title = {Meta-{Model} and {Tool} {Support} for the {Organizational} {Aspects} of {Internet}-of-{Things} {Development} {Methods}: {Organizational} {Aspects} of {IoT} {Development} {Methods}},
	isbn = {978-1-4503-8586-2},
	url = {https://doi.org/10.1145/3503047.3503077},
	doi = {10.1145/3503047.3503077},
	abstract = {The Internet-of-Things (IoT) has long become reality and contributes to the digital transformation of many industrial domains. IoT technologies are at the core of industry 4.0 application scenarios, contribute to cyber-physical system implementation, smart connected products and new business models exploiting their potential. There is plenty of work on how to specify, design and implement IoT solutions, but a lot of enterprises struggle to create business value from IoT technology because they have difficulties to define the organizational integration. Methodologies for model-driven engineering (MDE) of IoT solutions should encompass both, organizational and system development and integration, but existing model-based approaches focus on the technical perspective. The paper proposes a modeling approach integrated into enterprise modeling techniques to compensate for this lack. The enterprise modeling language 4EM is extended by adding a method component for IoT modeling. The main contributions of this paper are (a) a summary of the state-of-research in the field, (b) an industrial case for model-based IoT development, and (c) a meta-model and tool support for IoT modelling.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Advanced} {Information} {Science} and {System}},
	publisher = {Association for Computing Machinery},
	author = {Nast, Benjamin and Sandkuhl, Kurt},
	year = {2022},
	note = {event-place: Sanya, China},
	keywords = {Internet-of-Things, Modelling methodologies},
}

@inproceedings{zhou_enslaved_2020,
	address = {New York, NY, USA},
	series = {{CIKM} '20},
	title = {The {Enslaved} {Dataset}: {A} {Real}-world {Complex} {Ontology} {Alignment} {Benchmark} using {Wikibase}},
	isbn = {978-1-4503-6859-9},
	url = {https://doi.org/10.1145/3340531.3412768},
	doi = {10.1145/3340531.3412768},
	abstract = {Ontology alignment has taken a critical place for helping heterogeneous resources to interoperate. It has been studied for over a decade, and over that time many alignment systems and methods have been developed by researchers to find simple 1:1 equivalence matches between two ontologies. However, very few alignment systems focus on finding complex correspondences. Even if the complex alignment systems are developed, the performance of finding complex relations still has a lot of room for improvement. One reason for this limitation may be that there are still few applicable alignment benchmarks that contain such complex relationships that can raise researchers' interests. In this paper, we propose a real-world dataset from the Enslaved project as a potential complex alignment benchmark. The benchmark consists of two resources, the Enslaved Ontology along with a Wikibase repository holding a large number of instance data from the Enslaved project, as well as a manually created reference alignment between them. The alignment was developed in consultation with domain experts in the digital humanities. The alignment not only includes simple 1:1 equivalence correspondences, but also more complex m:n equivalence and subsumption correspondences and are provided in both Expressive and Declarative Ontology Alignment Language (EDOAL) format and rule syntax. The Enslaved benchmark has been incorporated into the Ontology Alignment Evaluation Initiative (OAEI) 2020 and is completely free for public use to assist the researchers in developing and evaluating their complex alignment algorithms.},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Lu and Shimizu, Cogan and Hitzler, Pascal and Sheill, Alicia M. and Estrecha, Seila Gonzalez and Foley, Catherine and Tarr, Duncan and Rehberger, Dean},
	year = {2020},
	note = {event-place: Virtual Event, Ireland},
	keywords = {knowledge graph, ontology alignment, benchmark, wikibase},
	pages = {3197--3204},
}

@inproceedings{weyssow_better_2022,
	address = {New York, NY, USA},
	series = {{ICSE}-{NIER} '22},
	title = {Better modeling the programming world with code concept graphs-augmented multi-modal learning},
	isbn = {978-1-4503-9224-2},
	url = {https://doi.org/10.1145/3510455.3512771},
	doi = {10.1145/3510455.3512771},
	abstract = {The progress made in code modeling has been tremendous in recent years thanks to the design of natural language processing learning approaches based on state-of-the-art model architectures. Nevertheless, we believe that the current state-of-the-art does not focus enough on the full potential that data may bring to a learning process in software engineering. Our vision articulates on the idea of leveraging multi-modal learning approaches to modeling the programming world. In this paper, we investigate one of the underlying idea of our vision whose objective based on concept graphs of identifiers aims at leveraging high-level relationships between domain concepts manipulated through particular language constructs. In particular, we propose to enhance an existing pretrained language model of code by joint-learning it with a graph neural network based on our concept graphs. We conducted a preliminary evaluation that shows gain of effectiveness of the models for code search using a simple joint-learning method and prompts us to further investigate our research vision.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 44th {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results}},
	publisher = {Association for Computing Machinery},
	author = {Weyssow, Martin and Sahraoui, Houari and Liu, Bang},
	year = {2022},
	note = {event-place: Pittsburgh, Pennsylvania},
	keywords = {code modeling, code search, concept graphs, multi-modal learning},
	pages = {21--25},
}

@inproceedings{yeh_exploring_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Exploring {Empty} {Spaces}: {Human}-in-the-{Loop} {Data} {Augmentation}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713491},
	doi = {10.1145/3706598.3713491},
	abstract = {Data augmentation is crucial to make machine learning models more robust and safe. However, augmenting data can be challenging as it requires generating diverse data points to rigorously evaluate model behavior on edge cases and mitigate potential harms. Creating high-quality augmentations that cover these “unknown unknowns” is a time- and creativity-intensive task. In this work, we introduce Amplio, an interactive tool to help practitioners navigate “unknown unknowns” in unstructured text datasets and improve data diversity by systematically identifying empty data spaces to explore. Amplio includes three human-in-the-loop data augmentation techniques: Augment with Concepts, Augment by Interpolation, and Augment with Large Language Model. In a user study with 18 professional red teamers, we demonstrate the utility of our augmentation methods in helping generate high-quality, diverse, and relevant model safety prompts. We find that Amplio enabled red teamers to augment data quickly and creatively, highlighting the transformative potential of interactive augmentation workflows.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yeh, Catherine and Ren, Donghao and Assogba, Yannick and Moritz, Dominik and Hohman, Fred},
	year = {2025},
	keywords = {language models, data diversity, Human-in-the-loop data augmentation, interactive visualization, sparse autoencoders},
}

@inproceedings{zhou_eyes_2024,
	address = {New York, NY, USA},
	series = {{FDG} '24},
	title = {The {Eyes}, the {Hands} and the {Brain}: {What} can {Text}-to-{Image} {Models} {Offer} for {Game} {Design} and {Visual} {Creativity}?},
	isbn = {979-8-4007-0955-5},
	url = {https://doi.org/10.1145/3649921.3650001},
	doi = {10.1145/3649921.3650001},
	abstract = {Text-to-image models such as DALL-E, Stable Diffusion, and Midjourney have seen a boom in development and adoption in both commercial and hobbyist spaces. This paper is a theoretical analysis aimed at informing the development of games that help improve critical literacy around text-to-image models. It asks: what assumptions and perspectives do text-to-image models have on visual creativity, and how do we bring that out through games? We propose a theory to differentiate between seeing an image through the expression of color, shapes and lines, and seeing an image through the recognition of concepts and ideas. These two ways of seeing are two different ways of orienting the player/user to their visual creativity. While traditional painting mechanics emphasize the former, text-to-image interfaces emphasize the latter. We deploy this perspective to study games with traditional painting interactions and games with text-to-image interactions. This paper hopes to contribute to design both broadly for games about visual creativity, and narrowly for gameplay with text-to-image models — specifically, how the latter fosters a different type of visual creativity than traditional painting interactions.},
	booktitle = {Proceedings of the 19th {International} {Conference} on the {Foundations} of {Digital} {Games}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Hongwei and Zhu, Jichen and Mateas, Michael and Wardrip-Fruin, Noah},
	year = {2024},
	note = {event-place: Worcester, MA, USA},
	keywords = {Painting, Game Design, Stable Diffusion, Text-to-Image, Visual Creativity},
}

@inproceedings{boubekeur_automatic_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Automatic assessment of students' software models using a simple heuristic and machine learning},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3418741},
	doi = {10.1145/3417990.3418741},
	abstract = {Software models are increasingly popular. To educate the next generation of software engineers, it is important that they learn how to model software systems well, so that they can design them effectively in industry. It is also important that instructors have the tools that can help them assess students' models more effectively. In this paper, we investigate how a tool that combines a simple heuristic with machine learning techniques can be used to help assess student submissions in model-driven engineering courses. We apply our proposed technique to first identify submissions of high quality and second to predict approximate letter grades. The results are comparable to human grading and a complex rule-based technique for the former and surprisingly accurate for the latter.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Boubekeur, Younes and Mussbacher, Gunter and McIntosh, Shane},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {assessment, domain modeling, grading, heuristics, Umple},
}

@inproceedings{luz_conceptual_2023,
	address = {New York, NY, USA},
	series = {{SBES} '23},
	title = {A {Conceptual} {Model} to {Support} {Teaching} of {Software} {Engineering} {Controlled} ({Quasi}-){Experiments}},
	isbn = {979-8-4007-0787-2},
	url = {https://doi.org/10.1145/3613372.3614202},
	doi = {10.1145/3613372.3614202},
	abstract = {Throughout controlled experimentation, it is possible to provide evidence of the software being developed. In the academic environment, Experimentation in Software Engineering (ESE) is essential to understanding cause-effect relations, enabling a vision of the development process, and taking action on actual events in the software industry. As much as the experimentation processes have been used in industry and academia, there is a lack of formalization of the principles of ESE teaching and artifacts that can be useful to support it in higher education. One of the means to contribute to such a topic would be the design of a conceptual model, which is widely discussed in the literature, thus applying empirical methods for a better understanding of the context and representation of ESE teaching. Thus, in this paper, we developed a conceptual model to support the teaching of controlled experiments and quasi-experiments. To design the conceptual model, we carried out an analysis of metadata from controlled experiments and quasi-experiments in the literature and conducted a survey to collect data from instructors who teach ESE. We evaluated the model with the Technology Acceptance Model (TAM). Results consist of a feasible conceptual model aiming to standardize the basic concepts of ESE and further support the production and reuse of ESE materials.},
	booktitle = {Proceedings of the {XXXVII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Luz, Carlos and Oliveirajr, Edson and Steinmacher, Igor},
	year = {2023},
	note = {event-place: Campo Grande, Brazil},
	keywords = {Conceptual Modeling, Concepts, Controlled Experimentation, TAM Model, Teaching of Controlled Experimentation},
	pages = {236--245},
}

@inproceedings{levy_assessing_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Assessing the {Impact} of {Automated} {Suggestions} on {Decision} {Making}: {Domain} {Experts} {Mediate} {Model} {Errors} but {Take} {Less} {Initiative}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445522},
	doi = {10.1145/3411764.3445522},
	abstract = {Automated decision support can accelerate tedious tasks as users can focus their attention where it is needed most. However, a key concern is whether users overly trust or cede agency to automation. In this paper, we investigate the effects of introducing automation to annotating clinical texts\&nbsp;—\&nbsp;a multi-step, error-prone task of identifying clinical concepts (e.g., procedures) in medical notes, and mapping them to labels in a large ontology. We consider two forms of decision aid: recommending which labels to map concepts to, and pre-populating annotation suggestions. Through laboratory studies, we find that 18 clinicians generally build intuition of when to rely on automation and when to exercise their own judgement. However, when presented with fully pre-populated suggestions, these expert users exhibit less agency: accepting improper mentions, and taking less initiative in creating additional annotations. Our findings inform how systems and algorithms should be designed to mitigate the observed issues.},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Levy, Ariel and Agrawal, Monica and Satyanarayan, Arvind and Sontag, David},
	year = {2021},
	note = {event-place: Yokohama, Japan},
	keywords = {ontology, agency, text tagging, clinical annotation, human-AI teams, mental model},
}

@inproceedings{dammu_dynamic-kgqa_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {Dynamic-{KGQA}: {A} {Scalable} {Framework} for {Generating} {Adaptive} {Question} {Answering} {Datasets}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3730324},
	doi = {10.1145/3726302.3730324},
	abstract = {As question answering (QA) systems advance alongside the rapid evolution of foundation models, the need for robust, adaptable, and large-scale evaluation benchmarks becomes increasingly critical. Traditional QA benchmarks are often static and publicly available, making them susceptible to data contamination and memorization by large language models (LLMs). Consequently, static benchmarks may overestimate model generalization and hinder a reliable assessment of real-world performance. In this work, we introduce Dynamic-KGQA, a scalable framework for generating adaptive QA datasets from knowledge graphs (KGs), designed to mitigate memorization risks while maintaining statistical consistency across iterations. Unlike fixed benchmarks, Dynamic-KGQA generates a new dataset variant on every run while preserving the underlying distribution, enabling fair and reproducible evaluations. Furthermore, our framework provides fine-grained control over dataset characteristics, supporting domain-specific and topic-focused QA dataset generation. Additionally, Dynamic-KGQA produces compact, semantically coherent subgraphs that facilitate both training and evaluation of KGQA models, enhancing their ability to leverage structured knowledge effectively. To align with existing evaluation protocols, we also provide static large-scale train/test/validation splits, ensuring comparability with prior methods. By introducing a dynamic, customizable benchmarking paradigm, Dynamic-KGQA enables a more rigorous and adaptable evaluation of QA systems.},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Dammu, Preetam Prabhu Srikar and Naidu, Himanshu and Shah, Chirag},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {knowledge graphs, large language models, benchmark, dynamic evaluation, kgqa, question answering},
	pages = {3498--3508},
}

@article{xie_graph_2021,
	title = {Graph {Neural} {Collaborative} {Topic} {Model} for {Citation} {Recommendation}},
	volume = {40},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3473973},
	doi = {10.1145/3473973},
	abstract = {Due to the overload of published scientific articles, citation recommendation has long been a critical research problem for automatically recommending the most relevant citations of given articles. Relational topic models (RTMs) have shown promise on citation prediction via joint modeling of document contents and citations. However, existing RTMs can only capture pairwise or direct (first-order) citation relationships among documents. The indirect (high-order) citation links have been explored in graph neural network–based methods, but these methods suffer from the well-known explainability problem. In this article, we propose a model called Graph Neural Collaborative Topic Model that takes advantage of both relational topic models and graph neural networks to capture high-order citation relationships and to have higher explainability due to the latent topic semantic structure. Experiments on three real-world citation datasets show that our model outperforms several competitive baseline methods on citation recommendation. In addition, we show that our approach can learn better topics than the existing approaches. The recommendation results can be well explained by the underlying topics.},
	number = {3},
	journal = {ACM Trans. Inf. Syst.},
	author = {Xie, Qianqian and Zhu, Yutao and Huang, Jimin and Du, Pan and Nie, Jian-Yun},
	month = nov,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {graph neural networks, collaborative filtering, Explainable citation recommendation, relational topic models},
}

@inproceedings{panchal_aishe-onto_2021,
	address = {New York, NY, USA},
	series = {dg.o '21},
	title = {{AISHE}-{Onto}: {A} {Semantic} {Model} for {Public} {Higher} {Education} {Universities}},
	isbn = {978-1-4503-8492-6},
	url = {https://doi.org/10.1145/3463677.3463750},
	doi = {10.1145/3463677.3463750},
	abstract = {The Electronic Government is a challenging field for the Semantic Web and the ontologies play a key role in the development of the Semantic Web. This paper explains the terms of the university through university ontology. We will focus on creating a university ontology. Here an ontology-based case study has been implemented for Public Higher Education (AISHE-Onto). SPARQL queries have been applied to make reasoning with the proposed ontology. As a result, a successful query interface has been provided to search academic data by the AISHE-Onto semantic portal.},
	booktitle = {Proceedings of the 22nd {Annual} {International} {Conference} on {Digital} {Government} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Panchal, Ronak and Swaminarayan, Priya and Tiwari, Sanju and Ortiz-Rodriguez, Fernando},
	year = {2021},
	note = {event-place: Omaha, NE, USA},
	keywords = {Ontologies, Semantic Web, Egovernment, Public Administration},
	pages = {545--547},
}

@article{van_rozen_languages_2021,
	title = {Languages of {Games} and {Play}: {A} {Systematic} {Mapping} {Study}},
	volume = {53},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3412843},
	doi = {10.1145/3412843},
	abstract = {Digital games are a powerful means for creating enticing, beautiful, educational, and often highly addictive interactive experiences that impact the lives of billions of players worldwide. We explore what informs the design and construction of good games to learn how to speed-up game development. In particular, we study to what extent languages, notations, patterns, and tools, can offer experts theoretical foundations, systematic techniques, and practical solutions they need to raise their productivity and improve the quality of games and play. Despite the growing number of publications on this topic there is currently no overview describing the state-of-the-art that relates research areas, goals, and applications. As a result, efforts and successes are often one-off, lessons learned go overlooked, language reuse remains minimal, and opportunities for collaboration and synergy are lost. We present a systematic map that identifies relevant publications and gives an overview of research areas and publication venues. In addition, we categorize research perspectives along common objectives, techniques, and approaches, illustrated by summaries of selected languages. Finally, we distill challenges and opportunities for future research and development.},
	number = {6},
	journal = {ACM Comput. Surv.},
	author = {van Rozen, Riemer},
	month = dec,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {tools, languages, game design, patterns, Game development, notations, systematic map},
}

@inproceedings{fan_modeling_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Modeling {User} {Behavior} with {Graph} {Convolution} for {Personalized} {Product} {Search}},
	isbn = {978-1-4503-9096-5},
	url = {https://doi.org/10.1145/3485447.3511949},
	doi = {10.1145/3485447.3511949},
	abstract = {User preference modeling is a vital yet challenging problem in personalized product search. In recent years, latent space based methods have achieved state-of-the-art performance by jointly learning semantic representations of products, users, and text tokens. However, existing methods are limited in their ability to model user preferences. They typically represent users by the products they visited in a short span of time using attentive models and lack the ability to exploit relational information such as user-product interactions or item co-occurrence relations. In this work, we propose to address the limitations of prior arts by exploring local and global user behavior patterns on a user successive behavior graph, which is constructed by utilizing short-term actions of all users. To capture implicit user preference signals and collaborative patterns, we use an efficient jumping graph convolution to explore high-order relations to enrich product representations for user preference modeling. Our approach can be seamlessly integrated with existing latent space based methods and be potentially applied in any product retrieval method that uses purchase history to model user preferences. Extensive experiments on eight Amazon benchmarks demonstrate the effectiveness and potential of our approach. The source code is available at https://github.com/floatSDSDS/SBG .},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Fan, Lu and Li, Qimai and Liu, Bo and Wu, Xiao-Ming and Zhang, Xiaotong and Lv, Fuyu and Lin, Guli and Li, Sen and Jin, Taiwei and Yang, Keping},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {Graph Convolution, Personalized Product Search, User Preference Modeling},
	pages = {203--212},
}

@inproceedings{ferjaoui_conceptual_2021,
	address = {New York, NY, USA},
	series = {{TEEM}'20},
	title = {A {Conceptual} {Model} for {Personalized} {Learning} based on {Educational} {Robots}},
	isbn = {978-1-4503-8850-4},
	url = {https://doi.org/10.1145/3434780.3436609},
	doi = {10.1145/3434780.3436609},
	abstract = {Over a previous couple of years, Distance learning has successfully overcome the shortcomings of traditional methods of teaching and learning, likewise increases student interaction and diversities of opinion, online instructors could also be from any location across the world. So students have the chance to settle on a learning strategy most suited to their abilities, while education is streamlined to satisfy the requirements of the individual in question. Due to the on-going technological change, we are witnessing, Robots are getting an integral component of our society and have great potential in being utilized as an academic technology by providing students with a highly interactive and hands-on learning experience. Indeed, Robotics promises to inspire a replacement generation of learning. With the aim of understanding how students can use robots to review, we created and implemented a learning scenario through an ontological conceptual Model for Personalized Learning supported Educational Robots. This model enables us to supply inferences over learning data and supply personalized learning resources, adapted to the progress of the scholar within the learning process.},
	booktitle = {Eighth {International} {Conference} on {Technological} {Ecosystems} for {Enhancing} {Multiculturality}},
	publisher = {Association for Computing Machinery},
	author = {Ferjaoui, Dhekra and Cheniti Belcadhi, Lilia},
	year = {2021},
	note = {event-place: Salamanca, Spain},
	keywords = {Distance learning, personalized learning, ontological model, robots, learning scenario},
	pages = {29--33},
}

@inproceedings{cherian_design_2022,
	address = {New York, NY, USA},
	series = {{ICCBDC} '22},
	title = {Design and {Development} of {Alzheimer} {Disease} {Ontology}},
	isbn = {978-1-4503-9657-8},
	url = {https://doi.org/10.1145/3555962.3555966},
	doi = {10.1145/3555962.3555966},
	abstract = {Alzheimer is one of the common causes of dementia, which is affecting millions of people around the world. This research study proposes a framework for the knowledge representation of Alzheimer disease using Ontology. The case study is conducted in Sultanate of Oman. In Oman, it is suspected that around 5000 people are affected by this disease. The correct statistics is still unknown as public are not fully aware of the symptoms of this disease yet. So, the reported cases in the hospitals are less, even though it is believed that the actual number of patients is quite large. In this research study, Ontologies are used for knowledge representation of Alzheimer disease. Ontologies are best for semantically representing the knowledge. It is defined as a formal, explicit specification of a shared conceptualization. To develop efficient clinical decision support systems, Ontologies can be used to integrate the patient health record with data from different sources. Ontologies manage and specify the knowledge from the medical domain. Preliminary design of the ontology is given in this paper.},
	booktitle = {Proceedings of the 2022 6th {International} {Conference} on {Cloud} and {Big} {Data} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Cherian, Sherimon Puliprathu and Sherimon, Vinu and Agith, Align Elsa and Thomas, Anu Achankunju},
	year = {2022},
	note = {event-place: Birmingham, United Kingdom},
	pages = {19--23},
}

@inproceedings{koch_integration_2022,
	address = {New York, NY, USA},
	series = {{JCDL} '22},
	title = {Integration of models for linked data in cultural heritage and contributions to the {FAIR} principles},
	isbn = {978-1-4503-9345-4},
	url = {https://doi.org/10.1145/3529372.3530957},
	doi = {10.1145/3529372.3530957},
	abstract = {Incorporating linked data-based models into the process of describing cultural objects is increasingly important for cultural heritage. Communities such as libraries, archives, and museums have developed and adopted models specific to their contexts. Without a trivial solution, choosing models to support more general applications is challenging. This Ph.D. aims to analyze existing solutions and practices in these domains and propose validated solutions for the discovery, access, interoperability, and reuse of cultural objects, following the FAIR principles. Transversal to the base models used, this research intends to adopt solutions that balance the simplicity of the models with the satisfaction of the requirements.},
	booktitle = {Proceedings of the 22nd {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {Association for Computing Machinery},
	author = {Koch, Inês},
	year = {2022},
	note = {event-place: Cologne, Germany},
	keywords = {semantic web, data integration, FAIR principles, cultural heritage, linked open data},
}

@inproceedings{alrabbaa_explaining_2023,
	address = {New York, NY, USA},
	series = {{IJCKG} '22},
	title = {Explaining {Non}-{Entailment} by {Model} {Transformation} for the {Description} {Logic} {EL}},
	isbn = {978-1-4503-9987-6},
	url = {https://doi.org/10.1145/3579051.3579060},
	doi = {10.1145/3579051.3579060},
	abstract = {Reasoning results computed by description logic systems can be hard to comprehend. When an ontology does not entail an expected subsumption relationship, generating an explanation of this non-entailment becomes necessary. In this paper, we use countermodels to explain non-entailments. More precisely, we devise relevant parts of canonical models of ontologies that serve as explanations and discuss the computational complexity of extracting these parts by means of model transformations. Furthermore, we provide an implementation of these transformations and evaluate it using real ontologies.},
	booktitle = {Proceedings of the 11th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Alrabbaa, Christian and Hieke, Willi},
	year = {2023},
	note = {event-place: Hangzhou, China},
	keywords = {Model Transformation, Description Logics, Explainable AI},
	pages = {1--9},
}

@inproceedings{ivanova_ontology_2020,
	address = {New York, NY, USA},
	series = {{CompSysTech} '20},
	title = {Ontology {Evaluation} and {Multilingualism}},
	isbn = {978-1-4503-7768-3},
	url = {https://doi.org/10.1145/3407982.3407989},
	doi = {10.1145/3407982.3407989},
	abstract = {Many ontologies have been developed recently, and evaluation is important for researchers or users when searching ontologies needed in their work. Ontology evaluation also is important phase of ontology development, refinement or evolution process. Automatically-developed ontologies as a result of ontology learning also need from evaluation. So, ontology evaluation is important both for selecting ontologies, meeting some requirements, and as a part of ontology life cycle. In this paper we propose deep analysis of ontology evaluation and discuss how multilingualism affects ontology evaluation. We classify ontology evaluation approaches, methods and metrics according to several dimensions. Our analysis and conclusions will be helpful for development of good ontologies and for finding or selection of needed ontologies.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Computer} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Ivanova, Tatyana and Popov, Miroslav},
	year = {2020},
	note = {event-place: Ruse, Bulgaria},
	keywords = {Ontology, Ontology evaluation, Ontology assessment, Multilingualism, Ontology evaluation approaches},
	pages = {215--222},
}

@inproceedings{u_object_2019,
	address = {New York, NY, USA},
	series = {{DATA} '19},
	title = {Object driven semantic multi-video summarisation based on ontology},
	isbn = {978-1-4503-7284-8},
	url = {https://doi.org/10.1145/3368691.3368697},
	doi = {10.1145/3368691.3368697},
	abstract = {Multi-Video summarisation frameworks aim to extract representative frames from a collection of videos. In the proposed framework, a multi-video summarisation model is implemented that detects key frames from a collection of related videos on the basis of user query object and ontology inference approach. The framework also develops a novel large-scale ontology for video genre identification based on the characteristics of genre-specific videos. The presence of ontologies aids in generating semantically relevant summaries compared to traditional approaches. Quantitative evaluation ensures that maximum information from the video collection on the basis of query is retrieved. Additionally, the ontology-based query inference approach reduces the computation time significantly. Qualitative results prove that the summary generated is concise, informative and semantically relevant.},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Data} {Science}, {E}-{Learning} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {U., Sreeja M. and Kovoor, Binsu C.},
	year = {2019},
	note = {event-place: Dubai, United Arab Emirates},
	keywords = {ontology, query, semantic, genre-specific, multi-video},
}

@inproceedings{boubekeur_towards_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Towards a better understanding of interactions with a domain modeling assistant},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3418742},
	doi = {10.1145/3417990.3418742},
	abstract = {The enrolment of software engineering students has increased rapidly in the past few years following industry demand. At the same time, model-driven engineering (MDE) continues to become relevant to more domains like embedded systems and machine learning. It is therefore important to teach students MDE skills in an effective manner to prepare them for future careers in academia and industry. The use of interactive online tools can help instructors deliver course material to more students in a more efficient manner, allowing them to offload repetitive or tedious tasks to these systems and focus on other teaching activities that cannot be easily automated. Interactive online tools can provide students with a more engaging learning experience than static resources like books or written exercises. Domain modeling with class diagrams is a fundamental modeling activity in MDE. While there exist multiple modeling tools that allow students to build a domain model, none of them offer an interactive learning experience. In this paper, we explore the interactions between a student modeler and an interactive domain modeling assistant with the aim of better understanding the required interaction. We illustrate desired interactions with three examples and then formalize them in a metamodel. Based on the metamodel, we explain how to form a corpus of learning material that supports the assistant interactions.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Boubekeur, Younes and Mussbacher, Gunter},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {chatbot, class diagram, feedback, learning corpus, domain model},
}

@inproceedings{sotudeh_ontg-bart_2023,
	address = {New York, NY, USA},
	series = {{DocEng} '23},
	title = {{OntG}-{Bart}: {Ontology}-{Infused} {Clinical} {Abstractive} {Summarization}},
	isbn = {979-8-4007-0027-9},
	url = {https://doi.org/10.1145/3573128.3609346},
	doi = {10.1145/3573128.3609346},
	abstract = {Automating the process of clinical text summarization could save clinicians' reading time and reduce their fatigue, acknowledging the necessity of human professionals in the loop. This paper addresses clinical text summarization, aiming to incorporate ontology concept relationships via a Graph Neural Network (GNN) into the summarization process. Specifically, we propose a model, extending Bart's encoder-decoder framework with GNN encoder and multi-head attentional layers for decoder, producing ontology-aware summaries. This GNN interacts with the textual encoder, influencing their mutual representations. The model's effectiveness is validated on two real-world radiology datasets. We also present an ablation study to elucidate the impact of varied graph configurations and an error analysis aimed at pinpointing potential areas for future improvements.},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Document} {Engineering} 2023},
	publisher = {Association for Computing Machinery},
	author = {Sotudeh, Sajad and Goharian, Nazli},
	year = {2023},
	note = {event-place: Limerick, Ireland},
	keywords = {neural networks, abstractive summarization, clinical text summarization, text summarization},
}

@inproceedings{lembo_ontology_2020,
	address = {New York, NY, USA},
	series = {{DSMM} '20},
	title = {Ontology mediated information extraction in financial domain with {Mastro} {System}-{T}},
	isbn = {978-1-4503-8030-0},
	url = {https://doi.org/10.1145/3401832.3402681},
	doi = {10.1145/3401832.3402681},
	abstract = {Information extraction (IE) refers to the task of turning text documents into a structured form, in order to make the information contained therein automatically processable. Ontology Mediated Information Extraction (OMIE) is a new paradigm for IE that seeks to exploit the semantic knowledge expressed in ontologies to improve query answering over unstructured data (properly raw text). In this paper we present Mastro System-T, an OMIE tool born from a joint collaboration between the University of Rome "La Sapienza" and IBM Research Almaden and its first application in a financial domain, namely to facilitate the access to and the sharing of data extracted from the EDGAR system.},
	booktitle = {Proceedings of the {Sixth} {International} {Workshop} on {Data} {Science} for {Macro}-{Modeling}},
	publisher = {Association for Computing Machinery},
	author = {Lembo, Domenico and Li, Yunyao and Popa, Lucian and Scafoglieri, Federico Maria},
	year = {2020},
	note = {event-place: Portland, Oregon},
	keywords = {ontology, information extraction, financial domain, ontology based data access, ontology mediated information extraction},
}

@inproceedings{shah_ontological_2019,
	address = {New York, NY, USA},
	series = {{ICGDA} '19},
	title = {An {Ontological} {Approach} to {Specify} {Conflicts} among {Non}-{Functional} {Requirements}},
	isbn = {978-1-4503-6245-0},
	url = {https://doi.org/10.1145/3318236.3318257},
	doi = {10.1145/3318236.3318257},
	abstract = {It is a usual practice for a user to narrate the Non-Functional Requirements (NFRs) in natural language and the requirements engineers manually try to express the same, using semi-formal or formal language notations. However, inaccurate and the laborious manual approach may fail to detect all potential NFRs and conflicts among them. Existing solutions for specifying NFRs are based on a graphical representation that requires manual efforts. Furthermore, they do not take into account the classification of the types of conflicting NFRs that helps to prioritize NFRs. In addition, these approaches are not used in industrial practice due to three main reasons viz. 1) High manual inference 2) Sharing and reusing work can be difficult and 3) No support for machine understanding. Therefore, the aim of our research is to formally specify conflicting NFRs from available natural language NFRs by means of ontological representation that helps requirements analysts prioritize the NFRs at an early stage of requirements engineering.},
	booktitle = {Proceedings of the 2019 2nd {International} {Conference} on {Geoinformatics} and {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Shah, Unnati and Patel, Sankita and Jinwala, Devesh},
	year = {2019},
	note = {event-place: Prague, Czech Republic},
	keywords = {Ontology, Non-functional Requirements, Requirements Engineering, Conflict, Specification},
	pages = {145--149},
}

@inproceedings{blersch_semi-automatic_2018,
	address = {New York, NY, USA},
	series = {{RAISE} '18},
	title = {Semi-automatic generation of active ontologies from web forms for intelligent assistants},
	isbn = {978-1-4503-5723-4},
	url = {https://doi.org/10.1145/3194104.3194108},
	doi = {10.1145/3194104.3194108},
	abstract = {Intelligent assistants are becoming widespread. A popular method for creating intelligent assistants is modeling the domain (and thus the assistant's capabilities) as Active Ontology. Adding new functionality requires extending the ontology or building new ones; as of today, this process is manual.We describe an automated method for creating Active Ontologies for arbitrary web forms. Our approach leverages methods from natural language processing and data mining to synthesize the ontologies. Furthermore, our tool generates the code needed to process user input.We evaluate the generated Active Ontologies in three case studies using web forms from the UIUC Web Integration Repository, namely from the domains airfare, automobile, and book search. First, we examine how much of the generation process can be automated and how well the approach identifies domain concepts and their relations. Second, we test how well the generated Active Ontologies handle end-user input to perform the desired actions. In our evaluation, Easier automatically generates 65\% of the Active Ontologies' sensor nodes; the generated ontology for airfare search correctly answers 70\% of the queries.},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Realizing} {Artificial} {Intelligence} {Synergies} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Blersch, Martin and Landhäußer, Mathias and Mayer, Thomas},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	pages = {28--34},
}

@inproceedings{alnahdi_mobile_2021,
	address = {New York, NY, USA},
	series = {{ICIST} '20},
	title = {Mobile {Application} {Development} {Ontology}},
	isbn = {978-1-4503-7655-6},
	url = {https://doi.org/10.1145/3447568.3448533},
	doi = {10.1145/3447568.3448533},
	abstract = {Ontologies are structural knowledge components constructed to clarify concepts in a specific domain of knowledge. They are formed to represent the concepts and the relationships between these concepts in that domain. This paper proposes a Mobile Application Development Ontology that conceptualizes the knowledge in the domain of mobile application development. The use of mobile applications is flourishing as mobile devices use is pervasive. The proposed ontology aims to provide a terminology glossary reference for stakeholders in the domain of mobile application development; students, researchers, educators, mobile application analysts, and mobile application developers. The constructed ontology was visualized using a D3 library visualization tool. Applications used for the built ontology include educational purposes and glossaries for classifying concepts in the domain of mobile application development.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Information} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Alnahdi, Amany},
	year = {2021},
	note = {event-place: Lecce, Italy},
	keywords = {Ontology, Semantic Web, Pedagogy, Mobile Applications},
}

@inproceedings{khakzad_shahandashti_assessing_2024,
	address = {New York, NY, USA},
	series = {{FORGE} '24},
	title = {Assessing the {Impact} of {GPT}-4 {Turbo} in {Generating} {Defeaters} for {Assurance} {Cases}},
	isbn = {979-8-4007-0609-7},
	url = {https://doi.org/10.1145/3650105.3652291},
	doi = {10.1145/3650105.3652291},
	abstract = {Assurance cases (ACs) are structured arguments that allow verifying the correct implementation of the created systems' non-functional requirements (e.g., safety, security). This allows for preventing system failure. The latter may result in catastrophic outcomes (e.g., loss of lives). ACs support the certification of systems in compliance with industrial standards, e.g., DO-178C and ISO 26262. Identifying defeaters —arguments that challenge these ACs — is crucial for enhancing ACs' robustness and confidence. To automatically support that task, we propose a novel approach that explores the potential of GPT-4 Turbo, an advanced Large Language Model (LLM) developed by OpenAI, in identifying defeaters within ACs formalized using the Eliminative Argumentation (EA) notation. Our preliminary evaluation assesses the model's ability to comprehend and generate arguments in this context and the results show that GPT-4 turbo is very proficient in EA notation and can generate different types of defeaters.},
	booktitle = {Proceedings of the 2024 {IEEE}/{ACM} {First} {International} {Conference} on {AI} {Foundation} {Models} and {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Khakzad Shahandashti, Kimya and Sivakumar, Mithila and Mohajer, Mohammad Mahdi and Boaye Belle, Alvine and Wang, Song and Lethbridge, Timothy},
	year = {2024},
	note = {event-place: Lisbon, Portugal},
	keywords = {large language models, assurance cases, assurance defeaters, FM for Requirement Engineering, system certification},
	pages = {52--56},
}

@article{albilali_aker_2025,
	title = {{AKER}: {Arabic} {Knowledge}-enriched {Reader} for {Machine} {Reading} {Comprehension}},
	volume = {24},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3736164},
	doi = {10.1145/3736164},
	abstract = {Machine reading comprehension aims at understanding a passage and answer a given question by selecting a span from the passage. Recently, pre-trained language models achieved state-of-the-art results on Arabic machine reading comprehension, yet a broad body of works suggests that BERT-based variant models fail to encode and associate common sense facts and world knowledge. To alleviate this weakness, we propose an Arabic knowledge enriched reader model, which fuses external knowledge into contextual representation using an attention and gating mechanism. We learn and generate Arabic knowledge graph embeddings that represent information from Arabic Wikidata and utilize this representation when fusing knowledge. We adopted a knowledge graph embedding scoring function to select the most relevant concepts to the context from the knowledge graph. We evaluated our approach on multiple Arabic machine reading comprehension datasets. Despite leveraging a comparatively smaller pre-trained language model, our approach significantly outperforms large language models in Arabic machine reading comprehension across multiple benchmark datasets, achieving substantial gains in both EM and F1 scores.},
	number = {6},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Albilali, Eman and Al-Twairesh, Nora and Hosny, Manar},
	month = jun,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {large language model, knowledge base, Machine reading comprehension, question answering},
}

@article{yang_diffsound_2023,
	title = {Diffsound: {Discrete} {Diffusion} {Model} for {Text}-to-{Sound} {Generation}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2023.3268730},
	doi = {10.1109/TASLP.2023.3268730},
	abstract = {Generating sound effects that people want is an important topic. However, there are limited studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a token-decoder, and a vocoder. The framework first uses the token-decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the token-decoder significantly influences the generation performance. Thus, we focus on designing a good token-decoder in this study. We begin with the traditional autoregressive (AR) token-decoder. However, the AR token-decoder always predicts the mel-spectrogram tokens one by one in order, which may introduce the unidirectional bias and accumulation of errors problems. Moreover, with the AR token-decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR token-decoders, we propose a non-autoregressive token-decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound model predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained by iteration. Our experiments show that our proposed Diffsound model not only produces better generation results when compared with the AR token-decoder but also has a faster generation speed, \&lt;italic\&gt;i.e.\&lt;/italic\&gt;, MOS: 3.56 \&lt;italic\&gt;v.s\&lt;/italic\&gt; 2.786.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
	month = apr,
	year = {2023},
	note = {Publisher: IEEE Press},
	pages = {1720--1733},
}

@inproceedings{lathouwers_modelling_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Modelling program verification tools for software engineers},
	isbn = {978-1-4503-9466-6},
	url = {https://doi.org/10.1145/3550355.3552426},
	doi = {10.1145/3550355.3552426},
	abstract = {In software engineering, models are used for many different things. In this paper, we focus on program verification, where we use models to reason about the correctness of systems. There are many different types of program verification techniques which provide different correctness guarantees. We investigate the domain of program verification tools, and present a concise megamodel to distinguish these tools. We also present a data set of almost 400 program verification tools. This data set includes the category of verification tool according to our megamodel, practical information such as input/output format, repository links, and more. The categorisation enables software engineers to find suitable tools, investigate similar alternatives and compare them. We also identify trends for each level in our megamodel based on the categorisation. Our data set, publicly available at https://doi.org/10.4121/20347950, can be used by software engineers to enter the world of program verification and find a verification tool based on their requirements.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lathouwers, Sophie and Zaytsev, Vadim},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {megamodelling, formal methods, program verification},
	pages = {98--108},
}

@article{ronzino_documenting_2022,
	title = {Documenting the {Structure} and {Adaptive} {Reuse} of {Roman} {Amphitheatres} through the {CIDOC} {CRMba} {Model}},
	volume = {15},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3485466},
	doi = {10.1145/3485466},
	abstract = {This article addresses an important aspect of the built heritage documentation, which concerns encoding information about a building in a formal way, making it available for reuse by the research community. Formal ontologies allow structuring and integrating information from heterogeneous sources without loss of semantic information. In the field of Cultural Heritage (CH), the CIDOC Conceptual Reference Model (CRM) ontology is well known and widely accepted as it provides definitions and a formal structure to describe the implicit and explicit concepts and relationships used in the CH documentation. One of its extensions, the CRMba model, has been specifically designed to document information on a built structure and its components. In this work, we have applied the CRMba model to the documentation of Roman architectures, in particular, Roman amphitheatres, demonstrating how the semantic model allows encoding information about the structure of the building and its evolution over time and space, stressing on the concepts of “empty spaces” and “functional spaces” defined by form, and focusing on the relationship between form and function. The aim of the work is to explore the potentiality of the model and to provide, through a series of examples supported by graphs, standard encoding procedures to be reused by scholars dealing with similar case studies.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Ronzino, Paola and Toth, Anna and Falcidieno, Bianca},
	month = apr,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {CIDOC CRM, amphitheatre, Buildings archaeology},
}

@inproceedings{hadj-mbarek_towards_2021,
	address = {New York, NY, USA},
	series = {{ICIST} '20},
	title = {Towards a {Collection} {Data} {Approach} to {Crisis} {Situation} {Based} on {Ontologies}},
	isbn = {978-1-4503-7655-6},
	url = {https://doi.org/10.1145/3447568.3448526},
	doi = {10.1145/3447568.3448526},
	abstract = {Disasters bring unforeseen situations to the public, requiring a quick and immediate response from official organizations. During these periods, access to rapidly changing information plays an important role in decision-making. However, the transmissions of information hinder this task and ultimately delay response operations. Social networks such as Twitter and Facebook create new opportunities to address this type of real-world problems. The involved actors use these channels at the time of crisis to share various information, whether they are requesting assistance, or describing an event. In this article, we will present the first steps of validation of a collect data approach from Twitter. This approach is based on a domain ontology.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Information} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Hadj-Mbarek, Asma and Maalel, Ahmed},
	year = {2021},
	note = {event-place: Lecce, Italy},
	keywords = {Ontology, Twitter, Data collection, Crisis situation},
}

@article{gil_artificial_2021,
	title = {Artificial {Intelligence} for {Modeling} {Complex} {Systems}: {Taming} the {Complexity} of {Expert} {Models} to {Improve} {Decision} {Making}},
	volume = {11},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/3453172},
	doi = {10.1145/3453172},
	abstract = {Major societal and environmental challenges involve complex systems that have diverse multi-scale interacting processes. Consider, for example, how droughts and water reserves affect crop production and how agriculture and industrial needs affect water quality and availability. Preventive measures, such as delaying planting dates and adopting new agricultural practices in response to changing weather patterns, can reduce the damage caused by natural processes. Understanding how these natural and human processes affect one another allows forecasting the effects of undesirable situations and study interventions to take preventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system's response to a diversity of conditions. A major challenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide variety of data sources available only in formats that require complex conversions. Using expert models for particular problems requires integration of models with third-party data as well as integration of models across disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotemporal, and execution mismatches, which are largely done by hand today and may take more than 2 years of effort.We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce modeling effort while ensuring utility for decision making. Our work to date makes several innovative contributions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assists them by suggesting relevant choices and automating steps along the way; (2) semantic metadata for models, including their modeling variables and constraints, that ensures model relevance and proper use for a given decision-making problem; and (3) semantic representations of datasets in terms of modeling variables that enable automated data selection and data transformations. This framework is implemented in the MINT (Model INTegration) framework, and currently includes data and models to analyze the interactions between natural and human systems involving climate, water availability, agricultural production, and markets. Our work to date demonstrates the utility of AI techniques to accelerate modeling to support decision-making and uncovers several challenging directions for future work.},
	number = {2},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Gil, Yolanda and Garijo, Daniel and Khider, Deborah and Knoblock, Craig A. and Ratnakar, Varun and Osorio, Maximiliano and Vargas, Hernán and Pham, Minh and Pujara, Jay and Shbita, Basel and Vu, Binh and Chiang, Yao-Yi and Feldman, Dan and Lin, Yijun and Song, Hayley and Kumar, Vipin and Khandelwal, Ankush and Steinbach, Michael and Tayal, Kshitij and Xu, Shaoming and Pierce, Suzanne A. and Pearson, Lissa and Hardesty-Lewis, Daniel and Deelman, Ewa and Silva, Rafael Ferreira Da and Mayani, Rajiv and Kemanian, Armen R. and Shi, Yuning and Leonard, Lorne and Peckham, Scott and Stoica, Maria and Cobourn, Kelly and Zhang, Zeya and Duffy, Christopher and Shu, Lele},
	month = jul,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {integrated modeling, Intelligent user interfaces, model metadata, regional-level decision-making, remote sensing data},
}

@inproceedings{jeusfeld_deductive_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Deductive reconstruction of {MLT}* for multi-level modeling},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3421410},
	doi = {10.1145/3417990.3421410},
	abstract = {In the last two decades, about a dozen proposals were made to extend object-oriented modeling by multiple abstraction levels. One group of proposals designates explicit levels to objects and classes. The second group uses the powertype pattern to implicitly establish levels. From this group, we consider two proposals, DeepTelos and MLT*. Both have been defined via axioms and both give a central role to the powertype pattern. In this paper, we reconstruct MLT* with the deductive axiomatization style used for DeepTelos. The resulting specification is executed in a deductive database to check MLT* multi-level models for errors and complete them with derived facts that do not have to be explicitly asserted by modelers. This leverages the rich rules of MLT* with the deductive approach underlying DeepTelos. The effort also allows us to clearly establish the relation between DeepTelos and MLT*, in an attempt to clarify the relations between approaches in this research domain. As a byproduct, we supply MLT-Telos as a fully operational deductive implementation of MLT* to the research community.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Jeusfeld, Manfred A. and Almeida, João Paulo A. and Carvalho, Victorio A. and Fonseca, Claudenir M. and Neumayr, Bernd},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {multi-level modeling, conceptbase, datalog, deeptelos, MLT*, object-oriented modeling, powertype},
}

@article{angermeier_security_2023,
	title = {Security {Risk} {Assessments}: {Modeling} and {Risk} {Level} {Propagation}},
	volume = {7},
	issn = {2378-962X},
	url = {https://doi.org/10.1145/3569458},
	doi = {10.1145/3569458},
	abstract = {Security risk assessment is an important task in systems engineering. It is used to derive security requirements for a secure system design and to evaluate design alternatives as well as vulnerabilities. Security risk assessment is also a complex and interdisciplinary task, where experts from the application domain and the security domain have to collaborate and understand each other. Automated and tool-supported approaches are desired to help manage the complexity. However, the models used for system engineering usually focus on functional behavior and lack security-related aspects. Therefore, we present our modeling approach that alleviates communication between the involved experts and features steps of computer-aided modeling to achieve consistency and avoid omission errors. We demonstrate our approach with an example. We also describe how to model impact rating and attack feasibility estimation in a modular fashion, along with the propagation and aggregation of these estimations through the model. As a result, experts can make local decisions or changes in the model, which in turn provides the impact of these decisions or changes on the overall risk profile. Finally, we discuss the advantages of our model-based method.},
	number = {1},
	journal = {ACM Trans. Cyber-Phys. Syst.},
	author = {Angermeier, Daniel and Wester, Hannah and Beilke, Kristian and Hansch, Gerhard and Eichler, Jörn},
	month = feb,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {risk analysis, threat modeling, model-based, secure design, security engineering, Security risk assessment},
}

@article{boovaraghavan_tao_2023,
	title = {{TAO}: {Context} {Detection} from {Daily} {Activity} {Patterns} {Using} {Temporal} {Analysis} and {Ontology}},
	volume = {7},
	url = {https://doi.org/10.1145/3610896},
	doi = {10.1145/3610896},
	abstract = {Translating fine-grained activity detection (e.g., phone ring, talking interspersed with silence and walking) into semantically meaningful and richer contextual information (e.g., on a phone call for 20 minutes while exercising) is essential towards enabling a range of healthcare and human-computer interaction applications. Prior work has proposed building ontologies or temporal analysis of activity patterns with limited success in capturing complex real-world context patterns. We present TAO, a hybrid system that leverages OWL-based ontologies and temporal clustering approaches to detect high-level contexts from human activities. TAO can characterize sequential activities that happen one after the other and activities that are interleaved or occur in parallel to detect a richer set of contexts more accurately than prior work. We evaluate TAO on real-world activity datasets (Casas and Extrasensory) and show that our system achieves, on average, 87\% and 80\% accuracy for context detection, respectively. We deploy and evaluate TAO in a real-world setting with eight participants using our system for three hours each, demonstrating TAO's ability to capture semantically meaningful contexts in the real world. Finally, to showcase the usefulness of contexts, we prototype wellness applications that assess productivity and stress and show that the wellness metrics calculated using contexts provided by TAO are much closer to the ground truth (on average within 1.1\%), as compared to the baseline approach (on average within 30\%).},
	number = {3},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	author = {Boovaraghavan, Sudershan and Patidar, Prasoon and Agarwal, Yuvraj},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {activity recognition, ontology, Behavioral context recognition, deep Learning},
}

@inproceedings{liu_approach_2018,
	address = {New York, NY, USA},
	series = {{ACAI} '18},
	title = {An {Approach} for {Learning} {Ontology} from {Relational} {Database}},
	isbn = {978-1-4503-6625-0},
	url = {https://doi.org/10.1145/3302425.3302495},
	doi = {10.1145/3302425.3302495},
	abstract = {As a conceptual model and modeling tool, ontology can describe knowledge systems at the semantic level, it can also effectively solve the problem of information integration and sharing. In the research of information integration based on ontology, how to transform relational database data into ontology is an important research direction. Since relational databases are widely used to store data, this paper proposes a new method(WN\_Graph) for learning ontology from relational database. Compared with the existing methods, WN\_Graph combines intermediate conceptual graph model with WordNet to get more hierarchical relationships of concepts. Therefore, more rich semantic relationships of relational databases are extracted. We use data set from the medical field to verify and analyze the proposed method.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Algorithms}, {Computing} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Xiao and Gao, Feng},
	year = {2018},
	note = {event-place: Sanya, China},
	keywords = {Ontology, WordNet, Relational Database, Graph Model},
}

@article{gao_end--end_2022,
	title = {End-to-{End} {Task}-{Oriented} {Dialog} {Modeling} {With} {Semi}-{Structured} {Knowledge} {Management}},
	volume = {30},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2022.3153255},
	doi = {10.1109/TASLP.2022.3153255},
	abstract = {Current task-oriented dialog (TOD) systems mostly manage structured knowledge (e.g. databases and tables) to guide the goal-oriented conversations. However, they fall short of handling dialogs which also involve unstructured knowledge (e.g. reviews and documents). In this article, we formulate a task of modeling TOD grounded on a fusion of structured and unstructured knowledge. To address this task, we propose a TOD system with semi-structured knowledge management, SeKnow, which extends the belief state to manage knowledge with both structured and unstructured contents. Furthermore, we introduce two implementations of SeKnow based on a non-pretrained sequence-to-sequence model and a pretrained language model, respectively. Both implementations use the end-to-end manner to jointly optimize dialog modeling grounded on structured and unstructured knowledge. We conduct experiments on a modified version of MultiWOZ 2.1 dataset, Mod-MultiWOZ 2.1, where dialogs are processed to involve semi-structured knowledge. Experimental results show that SeKnow has strong performances in both end-to-end dialog and intermediate knowledge management, compared to existing TOD systems and their extensions with pipeline knowledge management schemes.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Gao, Silin and Takanobu, Ryuichi and Bosselut, Antoine and Huang, Minlie},
	month = feb,
	year = {2022},
	note = {Publisher: IEEE Press},
	pages = {2173--2187},
}

@inproceedings{lu_learning_2019,
	address = {New York, NY, USA},
	series = {{BCB} '19},
	title = {Learning {Electronic} {Health} {Records} through {Hyperbolic} {Embedding} of {Medical} {Ontologies}},
	isbn = {978-1-4503-6666-3},
	url = {https://doi.org/10.1145/3307339.3342148},
	doi = {10.1145/3307339.3342148},
	abstract = {Unplanned intensive care units (ICU) readmissions and in-hospital mortality of patients are two important metrics for evaluating the quality of hospital care. Identifying patients with higher risk of readmission to ICU or of mortality can not only protect those patients from potential dangers, but also reduce the high costs of healthcare. In this work, we propose a new method to incorporate information from the Electronic Health Records (EHRs) of patients and utilize hyperbolic embeddings of a medical ontology (i.e., ICD-9) in the prediction model. The results prove the effectiveness of our method and show that hyperbolic embeddings of ontological concepts give promising performance.},
	booktitle = {Proceedings of the 10th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Lu, Qiuhao and de Silva, Nisansa and Kafle, Sabin and Cao, Jiazhen and Dou, Dejing and Nguyen, Thien Huu and Sen, Prithviraj and Hailpern, Brent and Reinwald, Berthold and Li, Yunyao},
	year = {2019},
	note = {event-place: Niagara Falls, NY, USA},
	keywords = {graph embedding, medical ontology, mortality prediction, readmission prediction},
	pages = {338--346},
}

@inproceedings{fathalla_scientific_2019,
	address = {New York, NY, USA},
	series = {{SAC} '19},
	title = {The scientific events ontology of the {OpenResearch}.org curation platform},
	isbn = {978-1-4503-5933-7},
	url = {https://doi.org/10.1145/3297280.3297631},
	doi = {10.1145/3297280.3297631},
	abstract = {Scholarly events, such as conferences play a key role in scholarly communication from many research fields, such as computer science. We describe a systematic redesign of the OpenResearch Scientific Events Ontology (OR-SEO) that is used as a schema for the event pages on OpenResearch.org curation platform. OR-SEO is now in use in thousands of event pages on OpenResearch, which enables users to create events wiki-pages without going into the details of the implementation of the ontology. We syntactically and semantically validated OR-SEO to conform to the W3C standards. It has been published through a persistent URL following W3C best practices for publishing Linked data and has been registered at Linked Open Vocabularies.},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Fathalla, Said and Vahdati, Sahar and Auer, Sören and Lange, Christoph},
	year = {2019},
	note = {event-place: Limassol, Cyprus},
	keywords = {knowledge engineering, linked data, scholarly communication, scientific events modeling, semantic MediaWiki},
	pages = {2311--2313},
}

@article{lano_model_2021,
	title = {Model {Transformation} {Development} {Using} {Automated} {Requirements} {Analysis}, {Metamodel} {Matching}, and {Transformation} by {Example}},
	volume = {31},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3471907},
	doi = {10.1145/3471907},
	abstract = {In this article, we address how the production of model transformations (MT) can be accelerated by automation of transformation synthesis from requirements, examples, and metamodels. We introduce a synthesis process based on metamodel matching, correspondence patterns between metamodels, and completeness and consistency analysis of matches. We describe how the limitations of metamodel matching can be addressed by combining matching with automated requirements analysis and model transformation by example (MTBE) techniques.We show that in practical examples a large percentage of required transformation functionality can usually be constructed automatically, thus potentially reducing development effort. We also evaluate the efficiency of synthesised transformations.Our novel contributions are: The concept of correspondence patterns between metamodels of a transformation.Requirements analysis of transformations using natural language processing (NLP) and machine learning (ML).Symbolic MTBE using “predictive specification” to infer transformations from examples.Transformation generation in multiple MT languages and in Java, from an abstract intermediate language.},
	number = {2},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Lano, K. and Kolahdouz-Rahimi, S. and Fang, S.},
	month = nov,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {model-driven engineering, requirements engineering, automated software engineering, Model transformations},
}

@inproceedings{frattini_second_2024,
	address = {New York, NY, USA},
	series = {{WSESE} '24},
	title = {A {Second} {Look} at the {Impact} of {Passive} {Voice} {Requirements} on {Domain} {Modeling}: {Bayesian} {Reanalysis} of an {Experiment}},
	isbn = {979-8-4007-0567-0},
	url = {https://doi.org/10.1145/3643664.3648211},
	doi = {10.1145/3643664.3648211},
	abstract = {The quality of requirements specifications may impact subsequent, dependent software engineering (SE) activities. However, empirical evidence of this impact remains scarce and too often superficial as studies abstract from the phenomena under investigation too much. Two of these abstractions are caused by the lack of frameworks for causal inference and frequentist methods which reduce complex data to binary results. In this study, we aim to demonstrate (1) the use of a causal framework and (2) contrast frequentist methods with more sophisticated Bayesian statistics for causal inference. To this end, we reanalyze the only known controlled experiment investigating the impact of passive voice on the subsequent activity of domain modeling. We follow a framework for statistical causal inference and employ Bayesian data analysis methods to re-investigate the hypotheses of the original study. Our results reveal that the effects observed by the original authors turned out to be much less significant than previously assumed. This study supports the recent call to action in SE research to adopt Bayesian data analysis, including causal frameworks and Bayesian statistics, for more sophisticated causal inference.},
	booktitle = {Proceedings of the 1st {IEEE}/{ACM} {International} {Workshop} on {Methodological} {Issues} with {Empirical} {Studies} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Frattini, Julian and Fucci, Davide and Torkar, Richard and Mendez, Daniel},
	year = {2024},
	note = {event-place: Lisbon, Portugal},
	keywords = {requirements engineering, bayesian data analysis, controlled experiment, requirements quality},
	pages = {27--33},
}

@article{wang_stft-domain_2022,
	title = {{STFT}-{Domain} {Neural} {Speech} {Enhancement} {With} {Very} {Low} {Algorithmic} {Latency}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2022.3224285},
	doi = {10.1109/TASLP.2022.3224285},
	abstract = {Deep learning based speech enhancement in the short-time Fourier transform (STFT) domain typically uses a large window length such as 32 ms. A larger window can lead to higher frequency resolution and potentially better enhancement. This however incurs an algorithmic latency of 32 ms in an online setup, because the overlap-add algorithm used in the inverse STFT (iSTFT) is also performed using the same window size. To reduce this inherent latency, we adapt a conventional dual-window-size approach, where a regular input window size is used for STFT but a shorter output window is used for overlap-add, for STFT-domain deep learning based frame-online speech enhancement. Based on this STFT-iSTFT configuration, we employ complex spectral mapping for frame-online enhancement, where a deep neural network (DNN) is trained to predict the real and imaginary (RI) components of target speech from the mixture RI components. In addition, we use the DNN-predicted RI components to conduct frame-online beamforming, the results of which are used as extra features for a second DNN to perform frame-online postfiltering. The frequency-domain beamformer can be easily integrated with our DNNs and is designed to not incur any algorithmic latency. Additionally, we propose a future-frame prediction technique to further reduce the algorithmic latency. Evaluation on noisy-reverberant speech enhancement shows the effectiveness of the proposed algorithms. Compared with Conv-TasNet, our STFT-domain system can achieve better enhancement performance for a comparable amount of computation, or comparable performance with less computation, maintaining strong performance at an algorithmic latency as low as 2 ms.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Wang, Zhong-Qiu and Wichern, Gordon and Watanabe, Shinji and Le Roux, Jonathan},
	month = nov,
	year = {2022},
	note = {Publisher: IEEE Press},
	pages = {397--410},
}

@inproceedings{belmonte_program_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {Program understanding using ontologies and dynamic analysis},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167298},
	doi = {10.1145/3167132.3167298},
	abstract = {No maintenance activity can be performed without understanding at least the part of the program that needs to be modified. Therefore, considering its cost, helping developers to understand programs is a must. Consequently, our research aims at building a business-related model of the program semantics, which is grounded in Perkinsfi research in psychology. After a short reminder of our model, whose performance in helping developers to understand programs has been presented elsewhere, this paper presents the automatic instantiation of the model. This rests on the ontology technology as well as on an innovative dynamic analysis technique. We present a use case to evaluate the performance of our technique.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Belmonte, Javier and Dugerdil, Philippe},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {ontology, reverse engineering, dynamic analysis, program understanding},
	pages = {1552--1559},
}

@inproceedings{acosta_uncertainty_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Uncertainty in coupled models of cyber-physical systems},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561539},
	doi = {10.1145/3550356.3561539},
	abstract = {The development of cyber-physical systems typically involves the association between multiple coupled models that capture different aspects of the system and the environment where it operates. Due to the dynamic aspect of the environment, unexpected conditions and uncertainty may impact the system. In this work, we tackle this problem and propose a taxonomy for characterizing uncertainty in coupled models. Our taxonomy extends existing proposals to cope with the particularities of coupled models in cyber-physical systems. In addition, our taxonomy discusses the notion of uncertainty propagation to other parts of the system. This allows for studying and (in some cases) quantifying the effects of uncertainty on other models in a system even at design time. We show the applicability of our uncertainty taxonomy in real use cases motivated by our envisioned scenario of automotive development.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Acosta, Maribel and Hahner, Sebastian and Koziolek, Anne and Kühn, Thomas and Mirandola, Raffaela and Reussner, Ralf},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	pages = {569--578},
}

@inproceedings{kejriwal_commonsense_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Commonsense {AI} in the {History} of the {Web}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3716841},
	doi = {10.1145/3701716.3716841},
	abstract = {Machine common sense (MCS)-the challenge of enabling computers to grasp everyday human knowledge-has been a grand challenge in Artificial Intelligence (AI) since the 1950s. While recent advances in large language models have led to impressive progress, there is still no consensus on how much common sense today's AI actually possesses. In this brief review, we revisit the historical development of MCS in the context of the Web, examining how the Web's evolution-from early knowledge representation efforts to knowledge graphs, the Semantic Web, and crowdsourcing-has shaped MCS research. We argue that key breakthroughs in Web technologies were instrumental in addressing longstanding challenges of scale and coverage in commonsense reasoning. At the same time, MCS research has influenced the development of core Web applications, including intelligent agents, plausibility-based reasoning, and robust evaluation of black-box AI systems.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Kejriwal, Mayank and McGuinness, Deborah L. and Lieberman, Henry},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {llms, conceptnet, cyc, machine common sense},
	pages = {837--840},
}

@inproceedings{benarab_ontology_2019,
	address = {New York, NY, USA},
	series = {{ICMLC} '19},
	title = {An {Ontology} {Embedding} {Approach} {Based} on {Multiple} {Neural} {Networks}},
	isbn = {978-1-4503-6600-7},
	url = {https://doi.org/10.1145/3318299.3318365},
	doi = {10.1145/3318299.3318365},
	abstract = {In this paper, we present a low-dimensional vector representation method for the concepts and instances of an ontology. The main idea is to transform the ontological entities into digestible data for machine learning and deep learning algorithms that only use digital inputs. The generated vectors will represent the semantics contained in the source ontology. We use the semantic relationships connecting the concepts as a landmark to train expert neural networks using the noise contrastive estimation technique to project them into a vector space specific to this relationship with weightings dependent on their frequency. The resulting vectors are then combined and fed into an autoencoder to generate a denser representation. The generated representation vectors can be used to find the semantically similar ontology entities, allowing creating a semantic network automatically. Thus, semantically similar ontology entities will have relatively close corresponding vector representations in the projection space.},
	booktitle = {Proceedings of the 2019 11th {International} {Conference} on {Machine} {Learning} and {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Benarab, Achref and Rafique, Fahad and Sun, Jianguo},
	year = {2019},
	note = {event-place: Zhuhai, China},
	keywords = {autoencoders, concept embeddings, continuous vector representations, feature representation, multiple neural networks, Ontology embeddings},
	pages = {186--190},
}

@inproceedings{jiang_short-text_2023,
	address = {New York, NY, USA},
	series = {{icWCSN} '23},
	title = {Short-{Text} {Semantic} {Similarity} {Model} of {BERT}-{Based} {Siamese} {Network}},
	isbn = {978-1-4503-9846-6},
	url = {https://doi.org/10.1145/3585967.3585994},
	doi = {10.1145/3585967.3585994},
	abstract = {People convey their emotions and thoughts through words, the medium of human thoughts. Up against the vigorous development of streaming media, the calculation of text similarity is imperative in the field of natural language processing. Any text-related field is inseparable from text semantic similarity. The calculation of text semantic similarity plays a key role in document management, document classification, and document relevance. Besides, popular natural language processing tasks in some trendy fields, such as artificial intelligence, human-machine translation, problem system, intelligent chat system, and nomenclature recognition, are intertwined with text semantic similarity calculation. In recent years, many excellent researchers have studied the algorithms and models of text semantic similarity from different dimensions. In this paper, a new short-text cosine similarity calculation model of the BERT-based Siamese network is proposed.},
	booktitle = {Proceedings of the 2023 10th {International} {Conference} on {Wireless} {Communication} and {Sensor} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Haoyu},
	year = {2023},
	note = {event-place: Chengdu, China},
	keywords = {BERT, Cosine Similarity, Short-Text Semantic Similarity, Siamese Network},
	pages = {145--149},
}

@inproceedings{jearanaiwongkul_ontology-based_2018,
	address = {New York, NY, USA},
	series = {{IAIT} '18},
	title = {An {Ontology}-based {Approach} to {Plant} {Disease} {Identification} {System}},
	isbn = {978-1-4503-6568-0},
	url = {https://doi.org/10.1145/3291280.3291786},
	doi = {10.1145/3291280.3291786},
	abstract = {Disease identification in plants is an important issue for farmers in terms of plant production and the reduction of losses in crop field. To deal with this issue, a number of systems and techniques for identifying plant diseases have been proposed. However, most of them mainly concentrate on image-based pattern recognition rather than focusing on the observed abnormalities of plant diseases. In other words, they have not employed ontology for semantic detection of plant disease. Current systems do not support farmers to find disease names w.r.t. the semantics of an infected plant. In this work, we proposed an ontology-based approach to modeling plant diseases and demonstrate our approach by developing a rice disease ontology. The ontology helps identifying plant diseases from existing symptoms on plants. To construct the ontology, we reused the domain knowledge related to symptoms of plant diseases from reliable sources. The proposed ontology is modeled in Web Ontology Language (OWL). We also develope a system architecture compatible with the modeled ontology. Finally, we illustrate the usage of our ontology in DL Query for disease retrieval, given that a farmer's observation has already been transformed into an OWL concept. The retrieved results of DL Query make use of subsumption relationship among concepts and properties defined in the ontology.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Advances} in {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Jearanaiwongkul, Watanee and Anutariya, Chutiporn and Andres, Frederic},
	year = {2018},
	note = {event-place: Bangkok, Thailand},
	keywords = {Ontology, Knowledge representation, Plant disease identification},
}

@inproceedings{wang_evaluation_2024,
	address = {New York, NY, USA},
	series = {{ICCSMT} '23},
	title = {Evaluation {Index} {System} for {SysML} {System} {Design} {Model}},
	isbn = {979-8-4007-0951-7},
	url = {https://doi.org/10.1145/3644523.3644561},
	doi = {10.1145/3644523.3644561},
	abstract = {Model-based Systems Engineering (MBSE) is an advanced approach to complex product design and development. The fundamental concept of MBSE is to develop and use consistent models of the system under development, referred to as named system models, as a cross-sectional and cross-discipline platform for information transfer. With the implementation and popularization of MBSE, the industry has put forward higher requirements for the quality of system models. Due to the lack of system model evaluation theory, the quality of the model is opaque, and different stakeholders have different evaluation results for the same model product. This defeats the purpose of using models instead of documents, which is to ensure a consistent exchange of information, and then seriously hinders the use and delivery of the model. Therefore, this study analyzes the characteristics of the system and model and the application of the system model, forming an evaluation index system for SysML system models. It provides practitioners using SysML with systematic theory and practical general methods to solve these problems, and provides a reference for practitioners using different languages to evaluate the quality of MBSE models.},
	booktitle = {Proceedings of the 2023 4th {International} {Conference} on {Computer} {Science} and {Management} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Linyao and Zhao, Yan and Mao, Yinxuan and Lu, Zhiang},
	year = {2024},
	note = {event-place: Xi'an, China},
	pages = {200--206},
}

@inproceedings{tapia-leon_extension_2019,
	address = {New York, NY, USA},
	series = {{ICEIT} 2019},
	title = {Extension of the {BiDO} {Ontology} to {Represent} {Scientific} {Production}},
	isbn = {978-1-4503-6267-2},
	url = {https://doi.org/10.1145/3318396.3318422},
	doi = {10.1145/3318396.3318422},
	abstract = {The SPAR Ontology Network is a suite of complementary ontology modules to describe the scholarly publishing domain. BiDO Standard Bibliometric Measures is part of its set of ontologies. It allows describing of numerical and categorical bibliometric data such as h-index, author citation count, journal impact factor. These measures may be used to evaluate scientific production of researchers. However, they are not enough. In a previous study, we determined the lack of some terms to provide a more complete representation of scientific production. Hence, we have built an extension using the NeOn Methodology to restructure the BiDO ontology. With this extension, it is possible to represent and measure the number of documents from research, the number of citations from a paper and the number of publications in high impact journals according to its area and discipline.},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Educational} and {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Tapia-Leon, Mariela and Santana-Perez, Idafen and Poveda-Villalón, María and Espinoza-Arias, Paola and Chicaiza, Janneth and Corcho, Oscar},
	year = {2019},
	note = {event-place: Cambridge, United Kingdom},
	keywords = {Ontology, SPARQL, RDF, BiDO, Scholarly Publishing, Scientific Production, SPAR Ontology Network},
	pages = {166--172},
}

@inproceedings{ivanova_collaborative_2023,
	address = {New York, NY, USA},
	series = {{CompSysTech} '23},
	title = {Collaborative methodology for semantic modeling of learning domain knowledge},
	isbn = {979-8-4007-0047-7},
	url = {https://doi.org/10.1145/3606305.3606320},
	doi = {10.1145/3606305.3606320},
	abstract = {Technologies for structured representation of knowledge are very useful for learning and for organization of personalized tutoring. Concept maps (CMs) have been used in many different ways to support learning. They are good tool for regular note-taking, visualization of the structure of learning content, or assisting in discussions and problem-solving activities. On the other hand, ontologies can benefit personalization and adaption of the tutoring process to the needs of specific learners or groups. This research discusses benefits of joint usage of both concept maps and ontologies in e-learning. We propose a collaborative methodology for semantic modeling of learning domain knowledge, showing how to use both CMs and ontologies to improve learning.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Computer} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Ivanova, Tatyana Ivanova},
	year = {2023},
	note = {event-place: Ruse, Bulgaria},
	pages = {174--179},
}

@inproceedings{brecher_multi-level_2018,
	address = {New York, NY, USA},
	series = {{ISCSIC} '18},
	title = {Multi-{Level} {Modeling} {Framework} for {Machine} as a {Service} {Applications} {Based} on {Product} {Process} {Resource} {Models}},
	isbn = {978-1-4503-6628-1},
	url = {https://doi.org/10.1145/3284557.3284714},
	doi = {10.1145/3284557.3284714},
	abstract = {At present, manufacturing processes are highly tailored to a specific product. Changes in product requirements therefore lead to big manual efforts for adapting the manufacturing process and reconfiguring production resources accordingly. Existing approaches do not cope well with this complexity. This hinders agile, customer-oriented manufacturing. A promising approach for automated assembling processes is the Machine as a Service paradigm, which aims for providing production resources on demand. This requires a consistent and pervasive formalization of product specifications, the corresponding manufacturing resources and their interdependencies. Thus, our first contribution is a generic and extensible multi-level and modular modeling framework to formalize products and available resources. Our framework is scalable for large companies and enables reuse for cross-company collaboration and supplier integration. Thereby, the static relationship between product, process and resource is avoided by describing product features and resource skills in separate models. Our framework uses the standardized SysML/UML. Our second contribution is the ability of our framework to integrate different standards. For demonstration, we apply our multi-level approach to a flexible assembly of terminal boxes for transmission gears and show the integration of standards by embedding the eCl@ss classification.},
	booktitle = {Proceedings of the 2nd {International} {Symposium} on {Computer} {Science} and {Intelligent} {Control}},
	publisher = {Association for Computing Machinery},
	author = {Brecher, Christian and Kusmenko, Evgeny and Lindt, Achim and Rumpe, Bernhard and Storms, Simon and Wein, Stephan and von Wenckstern, Michael and Wortmann, Andreas},
	year = {2018},
	note = {event-place: Stockholm, Sweden},
	keywords = {Asset Administration Shell, Industry 4.0 Components, Integration of Ontologies, Language Adaption and Aggregation, Machine as a Service, Multi-level Modeling, PPR Models},
}

@inproceedings{pikus_semi-automatic_2019,
	address = {New York, NY, USA},
	series = {{SAC} '19},
	title = {Semi-automatic ontology-driven development documentation: generating documents from {RDF} data and {DITA} templates},
	isbn = {978-1-4503-5933-7},
	url = {https://doi.org/10.1145/3297280.3297508},
	doi = {10.1145/3297280.3297508},
	abstract = {For a data-driven economy, digitization of product information throughout the entire product lifecycle is key to agility and efficiency of product-related processes. Documenting products and their development, e.g., creating requirement specifications, is an indispensable, time-consuming and resource-intensive activity in large organizations. A vast amount of related information often emerges across several siloing lifecycle tools, and only a portion of it is available in the post-hoc documentation. Additionally, numerous product lines and versions additionally increase the documentation effort. To tackle these issues in a research project, we developed a semi-automatic end-to-end documentation system, able to generate documents based on templates and structured data. As a use case for document generation, we employ the RDF-based lifecycle tool integration standard OSLC and add extended publishing information. In order to generate target documents, we leverage DITA, an established digital publishing standard. A pilot implementation demonstrates that the approach is able to extract distributed lifecycle data and to generate several types of documents in multiple formats. Since the method can also be used to generate documents from arbitrary RDF graphs, the results can be generalized to other domains beyond software development. We believe that the results support the change from a document-driven to a data-driven documentation paradigm in large organizations.},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Pikus, Yevgen and Weißenberg, Norbert and Holtkamp, Bernhard and Otto, Boris},
	year = {2019},
	note = {event-place: Limassol, Cyprus},
	keywords = {ontology, linked data, digital publishing, product documentation, tool integration},
	pages = {2293--2302},
}

@inproceedings{ojino_towards_2020,
	address = {New York, NY, USA},
	series = {{SAC} '20},
	title = {Towards an ontology for personalized hotel room recommendation: student research abstract},
	isbn = {978-1-4503-6866-7},
	url = {https://doi.org/10.1145/3341105.3374230},
	doi = {10.1145/3341105.3374230},
	abstract = {This paper presents the design of an ontology based on user profile that allows personalizing guests' hotel rooms and services. The ontology being developed using NeON methodology, takes into consideration the maximum number of concepts associated with hotel guest profile and hotel room. The ontology will also provide a sound representation of comfort metrics for hotel rooms to support recommendation.},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Ojino, Ronald Ochieng},
	year = {2020},
	note = {event-place: Brno, Czech Republic},
	keywords = {ontology, semantics, comfort metrics, NeON methodology},
	pages = {2060--2063},
}

@inproceedings{stewart_development_2019,
	address = {New York, NY, USA},
	series = {{BCI}'19},
	title = {Development of base ontology for a digital library of the {Bulgarian} museums' collections},
	isbn = {978-1-4503-7193-3},
	url = {https://doi.org/10.1145/3351556.3351581},
	doi = {10.1145/3351556.3351581},
	abstract = {This paper gives a further look into the process of ontology engineering for the needs of the Bulgarian museums' digital collections. The representation of the data model and a skeleton for a digital library offers a universal solution that can be used for the digitalization of movable cultural heritage ensuring its compatibility with the existing legislation in the domain. The main purpose of the base ontology is to unify and extend the usability of accumulated knowledge stored in the museum collection as well as information retrieval and query processing. The development of its structure has been proceeded following the bottom-up model due to the requirements of the front and backend users forming an important step for the standardization of I.T. solutions in the work of Bulgarian museums.},
	booktitle = {Proceedings of the 9th {Balkan} {Conference} on {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Stewart, Radovesta and Simeonov, Stanislav and Pavlov, Radoslav},
	year = {2019},
	note = {event-place: Sofia, Bulgaria},
	keywords = {Ontologies, Digital libraries, Digitalization, Museum database},
}

@inproceedings{ehl_supporting_2025,
	address = {New York, NY, USA},
	series = {{SAC} '25},
	title = {Supporting {Software} {Engineers} in {IT} {Security} and {Privacy} through {Automated} {Knowledge} {Discovery}},
	isbn = {979-8-4007-0629-5},
	url = {https://doi.org/10.1145/3672608.3707798},
	doi = {10.1145/3672608.3707798},
	abstract = {Security and privacy are increasingly essential concepts in software engineering. New threats and corresponding countermeasures are continuously discovered. Concurrently, projects are becoming more complex and are exposed to a greater number of threats. This presents a significant challenge for software engineers. As a result, security and privacy are often neglected due to a lack of knowledge, limited time, and financial constraints. While systematic literature reviews exist to address the increasing volume of publications, software engineers still require up-to-date knowledge of current threats and measures. This paper presents an automated, time-efficient, and cost-effective method for discovering knowledge from state-of-the-art literature and project artifacts, such as design documents. The presented method utilizes Large Language Models (LLMs) for data extraction and is demonstrated through a prototypical implementation and evaluation. This evaluation involves security and privacy in open-access scientific publications and project documentation from European Union research and development projects. The extracted knowledge is used to populate a quality model that is specifically designed to provide software engineers with information that helps them apply the findings. This quality model offers software engineers valuable, up-to-date insights into security and privacy, bridging the gap between scientific research and practical applications.},
	booktitle = {Proceedings of the 40th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Ehl, Marco and Ahmadian, Amir Shayan and Großer, Katharina and Elsofi, Duaa Adel Ali and Herrmann, Marc and Specht, Alexander and Schneider, Kurt and Jürjens, Jan},
	year = {2025},
	note = {event-place: Catania International Airport, Catania, Italy},
	keywords = {large language model, security, privacy, knowledge discovery, quality model},
	pages = {1647--1656},
}

@inproceedings{meng_deconstructing_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Deconstructing {Depression} {Stigma}: {Integrating} {AI}-driven {Data} {Collection} and {Analysis} with {Causal} {Knowledge} {Graphs}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3714255},
	doi = {10.1145/3706598.3714255},
	abstract = {Mental-illness stigma is a persistent social problem, hampering both treatment-seeking and recovery. Accordingly, there is a pressing need to understand it more clearly, but analyzing the relevant data is highly labor-intensive. Therefore, we designed a chatbot to engage participants in conversations; coded those conversations qualitatively with AI assistance; and, based on those coding results, built causal knowledge graphs to decode stigma. The results we obtained from 1,002 participants demonstrate that conversation with our chatbot can elicit rich information about people’s attitudes toward depression, while our AI-assisted coding was strongly consistent with human-expert coding. Our novel approach combining large language models (LLMs) and causal knowledge graphs uncovered patterns in individual responses and illustrated the interrelationships of psychological constructs in the dataset as a whole. The paper also discusses these findings’ implications for HCI researchers in developing digital interventions, decomposing human psychological constructs, and fostering inclusive attitudes.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Meng, Han and Zhang, Renwen and Wang, Ganyi and Yang, Yitian and Qin, Peinuan and Lee, Jungup and Lee, Yi-Chieh},
	year = {2025},
	keywords = {Large Language Model, Depression, Chatbot, AI-assisted Coding, Causal Knowledge Graph, Social Stigma},
}

@inproceedings{todorov_beyond_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Beyond {Facts}: 4th {International} {Workshop} on {Computational} {Methods} for {Online} {Discourse} {Analysis}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3641296},
	doi = {10.1145/3589335.3641296},
	abstract = {Expressing opinions and interacting with others on the Web has led to the production of an abundance of online discourse data, such as claims and viewpoints on controversial topics, their sources and contexts (events, entities). This data constitutes a valuable source of insights for studies into misinformation spread, bias reinforcement, echo chambers or political agenda setting. Computational methods, mostly from the field of NLP, have emerged that tackle a wide range of tasks in this context, including argument and opinion mining, claim detection, checkworthiness detection, stance detection or fact verification. However, computational models require robust definitions of classes and concepts under investigation. Thus, these computational tasks require a strong interdisciplinary and epistemological foundation, specifically with respect to the underlying definitions of key concepts such as claims, arguments, stances, check-worthiness or veracity. This requires a highly interdisciplinary approach combining expertise from fields such as communication studies, computational linguistics and computer science. As opposed to facts, claims are inherently more complex. Their interpretation strongly depends on the context and a variety of intentional or unintended meanings, where terminology and conceptual understandings strongly diverge across communities. From a computational perspective, in order to address this complexity, the synergy of multiple approaches, coming both from symbolic (knowledge representation) and statistical AI seem to be promising to tackle such challenges. This workshop aims at strengthening the relations between these communities, providing a forum for shared works on the modeling, extraction and analysis of discourse on the Web. It will address the need for a shared understanding and structured knowledge about discourse data in order to enable machine-interpretation, discoverability and reuse, in support of scientific or journalistic studies into the analysis of societal debates on the Web. Beyond research into information and knowledge extraction, data consolidation and modeling for knowledge graphs building, the workshop targets communities focusing on the analysis of online discourse, relying on methods from machine learning, natural language processing, large language models and Web data mining.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Todorov, Konstantin and Fafalios, Pavlos and Dietze, Stefan and Dimitrov, Dimitar},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {knowledge graphs, language models, computational journalism, computational fact-checking, intent detection, mis- and dis-information, online discourse analysis, social web mining, stance and viewpoint discovery},
	pages = {1418--1421},
}

@inproceedings{polovina_move_2020,
	address = {New York, NY, USA},
	series = {{CSCW} '20 {Companion}},
	title = {{MOVE}: {Measuring} {Ontologies} in {Value}-seeking {Environments}: {CSCW} for {Human} {Adaptation}},
	isbn = {978-1-4503-8059-1},
	url = {https://doi.org/10.1145/3406865.3418595},
	doi = {10.1145/3406865.3418595},
	abstract = {The interest in sharing the Data-Information-Knowledge-Wisdom (DIKW) continuum has been amplified by the latest multi-scale social changes including but not limited to pandemics, economic crises, climate change, and racial issues. This workshop aims to inspire research and discussion on measuring sharing of the DIKW continuum, including through computer-mediated methods, represented by its ontologies. The implied suggestion is that there are ways to improve human adaptation by social technologies that enable rapidly finding solutions for complex global situations. We therefore invite research on (1) ontologies as a medium that enables comparing and measuring the DIKW continuum, (2) ontologies and their convergence or divergence with the values that motivate and determine DIKW sharing, (3) properties and dynamics of ontologies shared via social technologies in their relation to human adaptation.},
	booktitle = {Companion {Publication} of the 2020 {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Polovina, Simon and Polovina, Rubina and Kemp, Neil and Pu, Ken},
	year = {2020},
	note = {event-place: Virtual Event, USA},
	keywords = {ontology, knowledge, knowledge sharing, human adaptation},
	pages = {475--482},
}

@inproceedings{lohia_design_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Design diagrams as ontological source},
	isbn = {978-1-4503-5572-8},
	url = {https://doi.org/10.1145/3338906.3340446},
	doi = {10.1145/3338906.3340446},
	abstract = {beginabstract In custom software development projects, it is frequently the case that the same type of software is being built for different customers. The deliverables are similar because they address the same market (e.g., Telecom, Banking) or have similar functions or both. However, most organisations do not take advantage of this similarity and conduct each project from scratch leading to lesser margins and lower quality. Our key observation is that the similarity among the projects alludes to the existence of a veritable domain of discourse whose ontology, if created, would make the similarity across the projects explicit. Design diagrams are an integral part of any commercial software project deliverables as they document crucial facets of the software solution. We propose an approach to extract ontological information from UML design diagrams (class and sequence diagrams) and represent it as domain ontology in a convenient representation. This ontology not only helps in developing a better understanding of the domain but also fosters software reuse for future software projects in that domain. Initial results on extracting ontology from thousands of model from public repository show that the created ontologies are accurate and help in better software reuse for new solutions. endabstract},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Lohia, Pranay and Kannan, Kalapriya and Srivastava, Biplav and Mehta, Sameep},
	year = {2019},
	note = {event-place: Tallinn, Estonia},
	keywords = {Semantic Representation, Ontology Extraction, Software Re-use},
	pages = {863--873},
}

@article{balsebre_mining_2023,
	title = {Mining {Geospatial} {Relationships} from {Text}},
	volume = {1},
	url = {https://doi.org/10.1145/3588947},
	doi = {10.1145/3588947},
	abstract = {A geospatial Knowledge Graph (KG) is a heterogeneous information network, capable of representing relationships between spatial entities in a machine-interpretable format, and has tremendous applications in logistics and social networks. Existing efforts to build a geospatial KG, have mainly used sparse spatial relationships, e.g., a district located inside a city, which provide only marginal benefits compared to a traditional database. In spite of the substantial advances in the tasks of link prediction and knowledge graph completion, identifying geospatial relationships remains challenging, particularly due to the fact that spatial entities are represented with single-point geometries, and textual attributes are frequently missing. In this study, we present GTMiner, a novel framework capable of jointly modeling Geospatial and Textual information to construct a knowledge graph, by mining three useful spatial relationships from a geospatial database, in an end-to-end fashion. The system is divided into three components: (1) a Candidate Selection module, to efficiently select a small number of candidate pairs; (2) a Relation Prediction component to predict spatial relationships between the entities; (3) a KG Refinement procedure, to improve both coverage and correctness of a geospatial knowledge graph. We carry out experiments on four cities' geospatial databases, from publicly-available sources and compare with existing algorithms for link prediction and geospatial data integration. Finally, we conduct an ablation study to motivate our design choices and an efficiency analysis to show that the time required by GTMiner for training and inference is comparable, or even shorter, than existing solutions.},
	number = {1},
	journal = {Proc. ACM Manag. Data},
	author = {Balsebre, Pasquale and Yao, Dezhong and Cong, Gao and Huang, Weiming and Hai, Zhen},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, model, knowledge, learning, information, prediction, text, representation, language, area, geokg, geometry, geospatial, graph, interest, kg, of, poi, point, pois, relation, relationship, relationships, spatial},
}

@inproceedings{kumar_using_2023,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '23},
	title = {Using domain ontology to identify consistent and inconsistent cases from {LSTM}-generated transfer type {AWPs}},
	isbn = {978-1-4503-9797-1},
	url = {https://doi.org/10.1145/3570991.3571028},
	doi = {10.1145/3570991.3571028},
	booktitle = {Proceedings of the 6th {Joint} {International} {Conference} on {Data} {Science} \&amp; {Management} of {Data} (10th {ACM} {IKDD} {CODS} and 28th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Kumar, Suresh and Kumar P, Sreenivasa},
	year = {2023},
	note = {event-place: Mumbai, India},
	pages = {289--290},
}

@inproceedings{chen_semantic-based_2020,
	address = {New York, NY, USA},
	series = {{WSSE} '20},
	title = {A {Semantic}-based {Multi}-agent {Dynamic} {Interaction} {Model}},
	isbn = {978-1-4503-8787-3},
	url = {https://doi.org/10.1145/3425329.3425332},
	doi = {10.1145/3425329.3425332},
	abstract = {Due to the autonomy of agents and their ability to perceive the environment, multi-agent systems have been widely used in many fields. The design of multi-agent systems requires the support of interactive models. The traditional multi-agent interaction model has certain feasibility in solving specific tasks. However, in a distributed environment, a relatively static multi-agent interaction model is not sufficient to support a dynamically changing interaction process. Frequent data interactions will also consume resources of multi-agent systems, thereby reducing agent performance. In this study, we propose a semantic-based multiagent dynamic interaction model (MADIM). MADIM uses semantic ontology to map the objects in the interaction model, and defines the interaction protocol through the rule description language. This model is attached with dynamically configurable semantic templates and interaction rule base. We added a reusable dynamic resolution engine component to MADIM to provide dynamic resolution services for the semantic information in the model. MADIM supports dynamic interactive behavior and has good interoperability and interpretability. Our model provides a flexible solution to the multi-agent interaction process. Finally, we verified the feasibility of the model design scheme through a simple example.},
	booktitle = {Proceedings of the 2nd {World} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Siming and Xiao, Liang and Cheng, Mo},
	year = {2020},
	note = {event-place: Chengdu, China},
	keywords = {Semantic rules, Multi-agent system, Protocol, Grouping, Interaction model},
	pages = {101--108},
}

@inproceedings{cambo_model_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {Model {Positionality} and {Computational} {Reflexivity}: {Promoting} {Reflexivity} in {Data} {Science}},
	isbn = {978-1-4503-9157-3},
	url = {https://doi.org/10.1145/3491102.3501998},
	doi = {10.1145/3491102.3501998},
	abstract = {Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model’s development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cambo, Scott Allen and Gergle, Darren},
	year = {2022},
	note = {event-place: New Orleans, LA, USA},
	keywords = {data science, annotator fingerprinting, Computational reflexivity, critical data studies, human-centered data science, human-centered machine learning, model positionality, position mining},
}

@inproceedings{iman_study_2023,
	address = {New York, NY, USA},
	series = {{NISS} '23},
	title = {A study of extending {BPMN} for {IoT}-aware process modeling},
	isbn = {979-8-4007-0019-4},
	url = {https://doi.org/10.1145/3607720.3607797},
	doi = {10.1145/3607720.3607797},
	abstract = {A fundamental obstacle to automatic business process detection is the lack of modeling concepts that explicitly express Internet of Things elements as components of a business process model. In order to present a framework for discovering business process models from sensor data, we have studied and compared in our previous publications associated with the modeling of processes in an intelligent environment via the application of process mining. The application of this technique is associated with process modeling, which does not include IoT elements, namely IoT information and devices as output. In this paper, we presented a semantic framework built on our extended BPMN ontology model for IoT.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Networking}, {Intelligent} {Systems} \&amp; {Security}},
	publisher = {Association for Computing Machinery},
	author = {Iman, el kodssi and Hanae, Sbai},
	year = {2023},
	note = {event-place: Larache, Morocco},
}

@inproceedings{wen_membership_2024,
	address = {New York, NY, USA},
	series = {{CCS} '24},
	title = {Membership {Inference} {Attacks} {Against} {In}-{Context} {Learning}},
	isbn = {979-8-4007-0636-3},
	url = {https://doi.org/10.1145/3658644.3690306},
	doi = {10.1145/3658644.3690306},
	abstract = {Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. Empirical results show that our attacks can accurately determine membership status in most cases, e.g., 95\% accuracy advantage against LLaMA, indicating that the associated risks are much higher than those shown by existing probability-based attacks. Additionally, we propose a hybrid attack that synthesizes the strengths of the aforementioned strategies, achieving an accuracy advantage of over 95\% in most cases. Furthermore, we investigate three potential defenses targeting data, instruction, and output. Results demonstrate combining defenses from orthogonal dimensions significantly reduces privacy leakage and offers enhanced privacy assurances.},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Rui and Li, Zheng and Backes, Michael and Zhang, Yang},
	year = {2024},
	note = {event-place: Salt Lake City, UT, USA},
	keywords = {large language models, in-context learning, membership inference attacks},
	pages = {3481--3495},
}

@inproceedings{shen_ideationweb_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {{IdeationWeb}: {Tracking} the {Evolution} of {Design} {Ideas} in {Human}-{AI} {Co}-{Creation}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713375},
	doi = {10.1145/3706598.3713375},
	abstract = {Due to the remarkable content generation capabilities, large language models (LLMs) have demonstrated potential in supporting early-stage conceptual design. However, current interaction paradigms often struggle to effectively facilitate multi-round idea exploration and selection, leading to random outputs, unclear iterations, and cognitive overload. To address these challenges, we propose a human-AI co-ideation framework aimed at tracking the evolution of design ideas. This framework leverages a structured idea representation, an analogy-based reasoning mechanism and interactive visualization techniques. It guides both designers and AI to systematically explore design spaces. We also develop a prototype system, IdeationWeb, which integrates an intuitive, mind map-like visual interface and interactive methods to support co-ideation. Our user study validates the framework’s feasibility, demonstrating enhanced collaboration and creativity between humans and AI. Furthermore, we identified collaborative design patterns from user behaviors, providing valuable insights for future human-AI interaction design.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Shen, Hanshu and Shen, Lyukesheng and Wu, Wenqi and Zhang, Kejun},
	year = {2025},
	keywords = {Large language models, Human-AI interaction, Creativity support, Design space, Human-AI co-ideation},
}

@inproceedings{elstermann_semantic_2018,
	address = {New York, NY, USA},
	series = {S-{BPM} {One} '18},
	title = {The {Semantic} {Exchange} {Standard} for {Subject}-{Oriented} {Process} {Models}},
	isbn = {978-1-4503-5360-1},
	url = {https://doi.org/10.1145/3178248.3178257},
	doi = {10.1145/3178248.3178257},
	abstract = {After the first general concept about using ontologies for the exchange of subject-oriented business process models in 2014, a concrete proposal for using the Web Ontology Language (OWL) and the semantic web technology framework as a concrete and direct means was made in 2017 including a proof of concept and proposal for a first standard. Based upon this work a standardization committee has formed and further developed the concept, brought in or reduced definitions, and discussed and agreed on specific controversial topics. In this paper we describe the progress made in 2017 and the current state of the OWL-Standard for the Subject-Oriented Parallel Activity Specification Schema (PASS) modeling language. We present the current state and argue for the made conventions.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Subject}-{Oriented} {Business} {Process} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Elstermann, Matthes and Krenn, Florian},
	year = {2018},
	note = {event-place: Linz, Austria},
	keywords = {ontologies, OWL, PASS, subject-oriented process models},
}

@article{cimino_sigfrid_2025,
	title = {{SIGFRID}: {Unsupervised}, {Platform}-{Agnostic} {Interference} {Detection} in {IoT} {Automation} {Rules}},
	volume = {6},
	url = {https://doi.org/10.1145/3722231},
	doi = {10.1145/3722231},
	abstract = {Smart home technology has profoundly changed modern living by interconnecting devices, services, dataflows, and user interactions into integrated, automated environments. Homeowners can easily program smart devices using conditional IF-THEN rules, where triggers prompt corresponding actions. However, as smart homes incorporate more multifunctional devices, conflicting trigger-action rules can simultaneously control devices in inconsistent ways, causing unexpected and potentially unsafe interference situations. This article introduces Sigfrid, a novel interference detection approach using scene interaction graphs constructed through Large Language Models (LLMs). To enhance LLM reasoning, we propose a new prompt engineering methodology that integrates automated and manual editing techniques to formulate queries for deriving causal insights in the smart home domain. Interferences are identified through efficient exploration of the graph constructed from the extracted relations. We evaluate Sigfrid on real-world If-This-Then-That (IFTTT) and SmartThings rule sets, demonstrating its superiority over state-of-the-art methods by more than 21\% in F1-score.},
	number = {2},
	journal = {ACM Trans. Internet Things},
	author = {Cimino, Gaetano and Deufemia, Vincenzo},
	month = apr,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {IoT, smart home, behavioral modeling, interference detection, trigger-action platforms},
}

@inproceedings{dupuy_anchor_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Anchor {Prediction}: {A} {Topic} {Modeling} {Approach}},
	isbn = {978-1-4503-9130-6},
	url = {https://doi.org/10.1145/3487553.3524927},
	doi = {10.1145/3487553.3524927},
	abstract = {Networks of documents connected by hyperlinks, such as Wikipedia, are ubiquitous. Hyperlinks are inserted by the authors to enrich the text and facilitate the navigation through the network. However, authors tend to insert only a fraction of the relevant hyperlinks, mainly because this is a time consuming task. In this paper we address an annotation, which we refer to as anchor prediction. Even though it is conceptually close to link prediction or entity linking, it is a different task that require developing a specific method to solve it. Given a source document and a target document, this task consists in automatically identifying anchors in the source document, i.e words or terms that should carry a hyperlink pointing towards the target document. We propose a contextualized relational topic model, CRTM, that models directed links between documents as a function of the local context of the anchor in the source document and the whole content of the target document. The model can be used to predict anchors in a source document, given the target document, without relying on a dictionary of previously seen mention or title, nor any external knowledge graph. Authors can benefit from CRTM, by letting it automatically suggest hyperlinks, given a new document and the set of target document to connect to. It can also benefit to readers, by dynamically inserting hyperlinks between the documents they’re reading. Experiments conducted on several Wikipedia corpora (in English, Italian and German) highlight the practical usefulness of anchor prediction and demonstrate the relevancy of our approach.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Dupuy, Jean and Guille, Adrien and Jacques, Julien},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {Topic modeling, Annotation, Anchor prediction, Document network},
	pages = {1310--1318},
}

@inproceedings{proenca_using_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {Using enterprise architecture model analysis and description logics for maturity assessment},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167140},
	doi = {10.1145/3167132.3167140},
	abstract = {A Maturity Model represents a path towards an increasingly organized and systematic way of doing business. It is therefore a widely used technique valuable to assess certain aspects of organizations, as for example business processes. A maturity assessment can enable stakeholders to clearly identify strengths and improvement points, and prioritize actions in order to reach higher maturity levels. Doing maturity assessments can range from simple self-assessment questionnaires to full-blown assessment methods, such as those recommended by the ISO/IEC 15504 or the SEI CMMI. A main caveat of these assessments is the resources they encompass. In addition, many times the lack of automation renders benchmarks not possible. Assuming that the wide spread of Enterprise Architecture practices is making the modeling of business domains a fact, and considering the recent state of the art on the representation of those models as ontologies, this paper proposes how existing semantic technology can be used to automate maturity models assessment methods.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Proença, Diogo and Borbinha, José},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {ontology, OWL, maturity model, description logics, archimate, enterprise architecture},
	pages = {102--109},
}

@inproceedings{islam_ontology-based_2022,
	address = {New York, NY, USA},
	series = {{SAC} '22},
	title = {Ontology-based user privacy management in smart grid},
	isbn = {978-1-4503-8713-2},
	url = {https://doi.org/10.1145/3477314.3508383},
	doi = {10.1145/3477314.3508383},
	abstract = {A smart grid system is one of the most complex cyber-physical systems consisting of power generation, distribution, consumption, customer domains, and millions of connected end devices. The widespread implementation of the smart grid raises concerns about the privacy of the data it collects. Since users' personal and non-personal data are going to be accessed by different entities involved in the smart grid and by third parties, the privacy concern can be a big obstacle in the adoption of the smart grid among people. Hence, there is an urgent need to provide the user with a privacy solution to support selective sharing of their usage data with different entities. In this paper, we propose an ontology-based user privacy management approach that will enable the user to release their data based on sensitivity and privacy factors, thus making an informed privacy decision on their usage data sharing. Green Button Initiative is a smart grid application that allows users to download and share their energy usage data with third parties. We present a proof-of-concept implementation extending the Green Button Initiative to test the feasibility of the proposed approach. Lastly, we discuss the results of a user study to investigate the effectiveness of our proposed approach.},
	booktitle = {Proceedings of the 37th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Islam, Raisa and Cerny, Tomas and Shin, Dongwan},
	year = {2022},
	note = {event-place: Virtual Event},
	pages = {174--182},
}

@inproceedings{obeid_ontology-based_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {Ontology-based {Recommender} {System} in {Higher} {Education}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3191533},
	doi = {10.1145/3184558.3191533},
	abstract = {Academic advising is limited in its ability to assist students in identifying academic pathways. Selecting a major and a university is a challenging process rife with anxiety. Students at high school are not sure how to match their interests with their working future or major. Therefore, high school students need guidance and support. Moreover, students need to filter, prioritize and efficiently get appropriate information from the web in order to solve the problem of information overload. This paper represents an approach for developing ontology-based recommender system improved with machine learning techniques to orient students in higher education. The proposed recommender system is an assessment tool for students' vocational strengths and weaknesses, interests and capabilities. The main objective of our ontology-based recommender system is to identify the student requirements, interests, preferences and capabilities to recommend the appropriate major and university for each one.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Obeid, Charbel and Lahoud, Inaya and El Khoury, Hicham and Champin, Pierre-Antoine},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {ontology-based, education, recommender system},
	pages = {1031--1034},
}

@inproceedings{rawsthorne_automatic_2023,
	address = {New York, NY, USA},
	series = {{GeoHumanities} '23},
	title = {Automatic {Nested} {Spatial} {Entity} and {Spatial} {Relation} {Extraction} {From} {Text} for {Knowledge} {Graph} {Creation}: {A} {Baseline} {Approach} and a {Benchmark} {Dataset}},
	isbn = {979-8-4007-0349-2},
	url = {https://doi.org/10.1145/3615887.3627754},
	doi = {10.1145/3615887.3627754},
	abstract = {Automatically extracting geographic information from text is the key to harnessing the vast amount of spatial knowledge that only exists in this unstructured form. The fundamental elements of spatial knowledge include spatial entities, their types and the spatial relations between them. Structuring the spatial knowledge contained within text as a geospatial knowledge graph, and disambiguating the spatial entities, significantly facilitates its reuse. The automatic extraction of geographic information from text also allows the creation or enrichment of gazetteers. We propose a baseline approach for nested spatial entity and binary spatial relation extraction from text, a new annotated French-language benchmark dataset on the maritime domain that can be used to train algorithms for both extraction tasks, and benchmark results for the two tasks carried out individually and end-to-end. Our approach involves applying the Princeton University Relation Extraction system (PURE), made for flat, generic entity extraction and generic binary relation extraction, to the extraction of nested, spatial entities and spatial binary relations. By extracting nested spatial entities and the spatial relations between them, we have more information to aid entity disambiguation. In our experiments we compare the performance of a pretrained monolingual French BERT language model with that of a pretrained multilingual BERT language model, and study the effect of including cross-sentence context. Our results reveal very similar results for both models, although the multilingual model performs slightly better in entity extraction, and the monolingual model has slightly better relation extraction and end-to-end performances. We observe that increasing the amount of cross-sentence context improves the results for entity extraction whereas it has the opposite effect on relation extraction.},
	booktitle = {Proceedings of the 7th {ACM} {SIGSPATIAL} {International} {Workshop} on {Geospatial} {Humanities}},
	publisher = {Association for Computing Machinery},
	author = {Rawsthorne, Helen Mair and Abadie, Nathalie and Kergosien, Eric and Duchêne, Cécile and Saux, Éric},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {deep learning, neural network, language model, binary spatial relation, geographic information, maritime data, nested spatial entity, spatial knowledge},
	pages = {21--30},
}

@inproceedings{su_case_2018,
	address = {New York, NY, USA},
	series = {{ICMHI} '18},
	title = {Case {Based} {Reasoning} {Driven} {Ontological} {Intelligent} {Health} {Projection} {System}},
	isbn = {978-1-4503-6389-1},
	url = {https://doi.org/10.1145/3239438.3239470},
	doi = {10.1145/3239438.3239470},
	abstract = {For the past few years, health-related issues are getting more and more attention. With regular health screening, disease detection and subsequent medical attention can proceed with far lesser chance of missing the critical period. The projection of health related issues from health screening has been studied by using statistical analysis methods. However, the results generally fail to alert the subjects regarding the potential risks involved due to lack of certainty. The potential health issues derived from statistical analysis of health screening data and medical general guidelines can only be presented to the subjects with probability and lack of supportive hard evidence. The efficacy of health screening consequently fails to be realized. In order to confront the dilemma, we have developed an Intelligent Health Projection System (IHPS) for providing an evidential health status projection and a more motivated health plan to the subjects. This study focuses on modelling Type 2 Diabetes Mellitus (T2DM) and associated factors as an example. Other cases can be analogously implemented. The IHPS adaptively provides the projection of potential risk of getting T2DM by exploring the similarity between the subject's health screening data and previous T2DM patients' cases. The main building blocks, the cases that serve as a knowledge base for IHPS are modelled using ontology technology. As the main functionality of IHPS, the T2DM projection uses the traces left by previous T2DM patients and works on top of case-based reasoning mechanism. The proposed IHPS aims to promote better self-health management by enhancing a subject's comprehension on risks revealed in health screening result. The cases retrieved can not only being used for risk projection of a subject but also serving as evidences for physicians to provide more accurate and convincing health advice.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Medical} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Su, Chuan-Jun and Huang, Shi-Feng and Li, Yi},
	year = {2018},
	note = {event-place: Tsukuba, Japan},
	keywords = {Ontology, Entropy, Case-based Reasoning, Analytic Hierarchy Process, Health Projection, Type 2 Diabetes Mellitus},
	pages = {185--194},
}

@inproceedings{liang_towards_2019,
	address = {New York, NY, USA},
	series = {{ICBDT} '19},
	title = {Towards {A} {Goal}-driven {Dynamic} {Business} {Process} {Ontology}},
	isbn = {978-1-4503-7192-6},
	url = {https://doi.org/10.1145/3358528.3358550},
	doi = {10.1145/3358528.3358550},
	abstract = {Business process Modelling plays an important role in supporting the whole life-cycle management of business processes. With the frequent changes of the environment and the urgent requirements of business collaboration, sharing, inter-operation, there is an increasing need to dynamically form business process models based on business goals and the underlying criteria. The traditional modelling methods, which focus on the activities with a fixed execution order, are not suitable for the situation. In this paper, a goal-driven dynamic business process ontology (GDBPO) is introduced. The ontology consists of four parts, business process ontology, goal ontology, business rule ontology and decision-making ontology. It can support decomposing the highlevel business goals into the refined operational level activity goals which are aligned with business rules and the available activities to dynamically construct the business process. Moreover, the proposed ontology can explicitly represent the knowledge within processes and be employed as a knowledge base for reasoning and decision-making.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Big} {Data} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Liang, Yan and Wen, Zepeng and Liu, Li and Li, Gongliang and Guo, Bing},
	year = {2019},
	note = {event-place: Jinan, China},
	keywords = {Ontology, Decision-making, Dynamic business process, Goal-driven},
	pages = {295--299},
}

@inproceedings{oelen_tinygenius_2022,
	address = {New York, NY, USA},
	series = {{JCDL} '22},
	title = {{TinyGenius}: intertwining natural language processing with microtask crowdsourcing for scholarly knowledge graph creation},
	isbn = {978-1-4503-9345-4},
	url = {https://doi.org/10.1145/3529372.3533285},
	doi = {10.1145/3529372.3533285},
	abstract = {As the number of published scholarly articles grows steadily each year, new methods are needed to organize scholarly knowledge so that it can be more efficiently discovered and used. Natural Language Processing (NLP) techniques are able to autonomously process scholarly articles at scale and to create machine readable representations of the article content. However, autonomous NLP methods are by far not sufficiently accurate to create a high-quality knowledge graph. Yet quality is crucial for the graph to be useful in practice. We present TinyGenius, a methodology to validate NLP-extracted scholarly knowledge statements using microtasks performed with crowdsourcing. The scholarly context in which the crowd workers operate has multiple challenges. The explainability of the employed NLP methods is crucial to provide context in order to support the decision process of crowd workers. We employed TinyGenius to populate a paper-centric knowledge graph, using five distinct NLP methods. In the end, the resulting knowledge graph serves as a digital library for scholarly articles.},
	booktitle = {Proceedings of the 22nd {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {Association for Computing Machinery},
	author = {Oelen, Allard and Stocker, Markus and Auer, Sören},
	year = {2022},
	note = {event-place: Cologne, Germany},
	keywords = {crowdsourcing microtasks, intelligent user interfaces, knowledge graph validation, scholarly knowledge graphs},
}

@inproceedings{nguyen_context-enriched_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Context-{Enriched} {Learning} {Models} for {Aligning} {Biomedical} {Vocabularies} at {Scale} in the {UMLS} {Metathesaurus}},
	isbn = {978-1-4503-9096-5},
	url = {https://doi.org/10.1145/3485447.3511946},
	doi = {10.1145/3485447.3511946},
	abstract = {The Unified Medical Language System (UMLS) Metathesaurus construction process mainly relies on lexical algorithms and manual expert curation for integrating over 200 biomedical vocabularies. A lexical-based learning model (LexLM) was developed to predict synonymy among Metathesaurus terms and largely outperforms a rule-based approach (RBA) that approximates the current construction process. However, the LexLM has the potential for being improved further because it only uses lexical information from the source vocabularies, while the RBA also takes advantage of contextual information. We investigate the role of multiple types of contextual information available to the UMLS editors, namely source synonymy (SS), source semantic group (SG), and source hierarchical relations (HR), for the UMLS vocabulary alignment (UVA) problem. In this paper, we develop multiple variants of context-enriched learning models (ConLMs) by adding to the LexLM the types of contextual information listed above. We represent these context types in context-enriched knowledge graphs (ConKGs) with four variants ConSS, ConSG, ConHR, and ConAll. We train these ConKG embeddings using seven KG embedding techniques. We create the ConLMs by concatenating the ConKG embedding vectors with the word embedding vectors from the LexLM. We evaluate the performance of the ConLMs using the UVA generalization test datasets with hundreds of millions of pairs. Our extensive experiments show a significant performance improvement from the ConLMs over the LexLM, namely +5.0\% in precision (93.75\%), +0.69\% in recall (93.23\%), +2.88\% in F1 (93.49\%) for the best ConLM. Our experiments also show that the ConAll variant including the three context types takes more time, but does not always perform better than other variants with a single context type. Finally, our experiments show that the pairs of terms with high lexical similarity benefit most from adding contextual information, namely +6.56\% in precision (94.97\%), +2.13\% in recall (93.23\%), +4.35\% in F1 (94.09\%) for the best ConLM. The pairs with lower degrees of lexical similarity also show performance improvement with +0.85\% in F1 (96\%) for low similarity and +1.31\% in F1 (96.34\%) for no similarity. These results demonstrate the importance of using contextual information in the UVA problem.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Vinh and Yip, Hong Yung and Bajaj, Goonmeet and Wijesiriwardene, Thilini and Javangula, Vishesh and Parthasarathy, Srinivasan and Sheth, Amit and Bodenreider, Olivier},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {neural networks, scalability, knowledge graph embeddings., supervised learning, UMLS Metathesaurus, vocabulary alignment},
	pages = {1037--1046},
}

@article{schmid_dynamic_2024,
	title = {Dynamic {Convolutional} {Neural} {Networks} as {Efficient} {Pre}-{Trained} {Audio} {Models}},
	volume = {32},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2024.3376984},
	doi = {10.1109/TASLP.2024.3376984},
	abstract = {The introduction of large-scale audio datasets, such as AudioSet, paved the way for Transformers to conquer the audio domain and replace CNNs as the state-of-the-art neural network architecture for many tasks. Audio Spectrogram Transformers are excellent at exploiting large datasets, creating powerful pre-trained models that surpass CNNs when fine-tuned on downstream tasks. However, current popular Audio Spectrogram Transformers are demanding in terms of computational complexity compared to CNNs. Recently, we have shown that, by employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch up with and even outperform Transformers on large datasets. In this work, we extend this line of research and increase the capacity of efficient CNNs by introducing dynamic CNN blocks constructed of dynamic convolutions, a dynamic ReLU activation function, and Coordinate Attention. We show that these dynamic CNNs outperform traditional efficient CNNs, such as MobileNets, in terms of the performance–complexity trade-off at the task of audio tagging on the large-scale AudioSet. Our experiments further indicate that the proposed dynamic CNNs achieve competitive performance with Transformer-based models for end-to-end fine-tuning on downstream tasks while being much more computationally efficient.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Schmid, Florian and Koutini, Khaled and Widmer, Gerhard},
	month = mar,
	year = {2024},
	note = {Publisher: IEEE Press},
	pages = {2227--2241},
}

@inproceedings{schroder_ontology-based_2018,
	address = {New York, NY, USA},
	series = {{ECSA} '18},
	title = {An ontology-based approach for documenting and validating architecture rules},
	isbn = {978-1-4503-6483-6},
	url = {https://doi.org/10.1145/3241403.3241457},
	doi = {10.1145/3241403.3241457},
	abstract = {Architecture conformance checking is an important activity of architecture enforcement where the architect ensures that all architecture concepts are implemented correctly in the source code. In order to support the architect, a lot of tools for conformance checking are available that allow to formalize the architecture in order to perform an automated verification. Typically, the formalization uses a rigid, tool-specific architecture concept language that may strongly deviate from the project-specific architecture concept language. In addition, a high level of formal expertise is required in order to comprehend the created formalization. We present an approach that uses a controlled natural language for the formalization of architecture concepts. This language allows to flexibly express architecture rules directly with project-specific concepts. Consequently, the resulting formalization is easy to understand and might also be used as an architecture documentation at the same time. Nevertheless, the documentation can be automatically verified, since the approach is based on powerful means of the semantic web, i.e., ontologies and description logics. For the evaluation of the approach, we use the real-world software system TEAMMATES and show that architecture rules and concepts can be flexibly designed and checked for conformance in order to detect crucial architecture violations.},
	booktitle = {Proceedings of the 12th {European} {Conference} on {Software} {Architecture}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Schröder, Sandra and Riebisch, Matthias},
	year = {2018},
	note = {event-place: Madrid, Spain},
	keywords = {ontologies, description logics, architecture conformance checking, architecture documentation, architecture erosion},
}

@article{lin_introduction_2023,
	title = {Introduction to the {Special} {Issue} of {Recent} {Advances} in {Computational} {Linguistics} for {Asian} {Languages}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3588316},
	doi = {10.1145/3588316},
	number = {3},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Lin, Jerry Chun-Wei and DÍaz, Vicente GarcÍa and Molinera, Juan Antonio Morente},
	month = apr,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
}

@article{zabokrtsky_sentence_2020,
	title = {Sentence {Meaning} {Representations} {Across} {Languages}: {What} {Can} {We} {Learn} from {Existing} {Frameworks}?},
	volume = {46},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00385},
	doi = {10.1162/coli_a_00385},
	abstract = {This article gives an overview of how sentence meaning is represented in eleven deep-syntactic frameworks, ranging from those based on linguistic theories elaborated for decades to rather lightweight NLP-motivated approaches. We outline the most important characteristics of each framework and then discuss how particular language phenomena are treated across those frameworks, while trying to shed light on commonalities as well as differences.},
	number = {3},
	journal = {Comput. Linguist.},
	author = {Žabokrtský, Zdeněk and Zeman, Daniel and Ševčíková, Magda},
	month = nov,
	year = {2020},
	note = {Place: Cambridge, MA, USA
Publisher: MIT Press},
	pages = {605--665},
}

@inproceedings{zhang_enriching_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {Enriching {Ontology} with {Temporal} {Commonsense} for {Low}-{Resource} {Audio} {Tagging}},
	isbn = {978-1-4503-8446-9},
	url = {https://doi.org/10.1145/3459637.3482097},
	doi = {10.1145/3459637.3482097},
	abstract = {Audio tagging aims at predicting sound events occurred in a recording. Traditional models require enormous laborious annotations, otherwise performance degeneration will be the norm. Therefore, we investigate robust audio tagging models in low-resource scenarios with the enhancement of knowledge graphs. Besides existing ontological knowledge, we further propose a semi-automatic approach that can construct temporal knowledge graphs on diverse domain-specific label sets. Moreover, we leverage a variant of relation-aware graph neural network, D-GCN, to combine the strength of the two knowledge types. Experiments on AudioSet and SONYC urban sound tagging datasets suggest the effectiveness of the introduced temporal knowledge, and the advantage of the combined KGs with D-GCN over single knowledge source.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Zhiling and Zhou, Zelin and Tang, Haifeng and Li, Guangwei and Wu, Mengyue and Zhu, Kenny Q.},
	year = {2021},
	note = {event-place: Virtual Event, Queensland, Australia},
	keywords = {knowledge graph, graph neural network, audio tagging, low-resource},
	pages = {3652--3656},
}

@inproceedings{bifis_hierarchical_2021,
	address = {New York, NY, USA},
	series = {{PETRA} '21},
	title = {A {Hierarchical} {Ontology} for {Dialogue} {Acts} in {Psychiatric} {Interviews}},
	isbn = {978-1-4503-8792-7},
	url = {https://doi.org/10.1145/3453892.3461349},
	doi = {10.1145/3453892.3461349},
	abstract = {We present our work on modeling the context of an interview during diagnostic sessions for patients with mental health problems. The results are to be exploited by translation system for telehealth services. More specifically, we plan to use the context of the psychiatric interview in order to set informative priors over the vocabulary of the speaker. Therefore we have modelled the context with a hierarchical ontology, and we use it to classify the current state of the interview. The state is extracted after the doctor asks a question, and allow us to select a non-uniform prior regarding the vocabulary of the patient.},
	booktitle = {Proceedings of the 14th {PErvasive} {Technologies} {Related} to {Assistive} {Environments} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Bifis, Aristeidis and Trigka, Maria and Dedegkika, Sofia and Goula, Panagiota and Constantinopoulos, Constantinos and Kosmopoulos, Dimitrios},
	year = {2021},
	note = {event-place: Corfu, Greece},
	keywords = {depression, dialogue context, hierarchical classification, stress},
	pages = {330--337},
}

@inproceedings{wang_spatial-temporal_2019,
	address = {New York, NY, USA},
	series = {{BDE} '19},
	title = {Spatial-temporal {Data} {Association} {Based} {Ontology} {Alignment} {Research} in {High} {Education} {Context}},
	isbn = {978-1-4503-6091-3},
	url = {https://doi.org/10.1145/3341620.3341640},
	doi = {10.1145/3341620.3341640},
	abstract = {In the process of practicing smart campus, student behavior is highly concerned. Student activities produce spatial-temporal data, which records the daily life of students and contains the potential regulations of student behavior. The complexity of these data brings challenges for data collection and data analysis. The key to solve these problems is data fusion. In addition, ontology alignment is an important method for exploring the association between different ontology in different fields. It can solve the problem of data fusion in practice and maximize the value of data. At present, the research methods of ontology alignment are mostly mathematical similarity algorithms, and not considering the uncertainty of spatial-temporal data. Ontology is better way to deal with spatial-temporal data. In order to solve this problem effectively, this paper proposes a method based on spatial-temporal data association, which establishes fuzzy ontology and formulates a series of fuzzy rules for fuzzy reasoning, and mines the relationship between data; then it connects the different concepts between ontologies to realize the alignment of ontologies. Finally, the fuzzy ontology modeling and fuzzy reasoning are implemented and tested by using high education context. The rationality and effectiveness of the method are verified.},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Big} {Data} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Wei and Mu, Wenxin and Gou, Juanqiong},
	year = {2019},
	note = {event-place: Hong Kong, Hong Kong},
	keywords = {fuzzy ontology, ontology alignment, Data association, fuzzy reasoning},
	pages = {125--130},
}

@article{ferrier-barbut_learning_2023,
	title = {Learning {With} {Pedagogical} {Models}: {Videos} {As} {Adjuncts} to {Apprenticeship} for {Surgical} {Training}},
	volume = {7},
	url = {https://doi.org/10.1145/3579615},
	doi = {10.1145/3579615},
	abstract = {Videos are a powerful media to learn activities through guided physical training such as surgery, especially when they are produced following human learning models and not as "how-to" videos. However, their success greatly depends on how they are integrated into the extensive curricula of domains where learning occurs through guided practice. In this work, we investigate the impact of integrating video as a learning tool into the learning curricula of surgery. We created a pedagogical video on surgical hysterectomy through a model based on the Conceptual Fields theory (Vergnaud) and performed two rounds of interviews with seven medical residents, who watched the video freely during their residency in gynecology-obstetrics as they trained with experts. We find that videos can complement guided physical training, as they can provide the rationale behind expert action, something that is difficult to explicit during guided training. Still, their linear and static nature limits their integration as true adjuncts. We discuss our vision of moving towards interactive videos created with an ontological approach, developed in a workshop with four expert surgeons, which involves the ability to navigate through levels of information and layers of representations, so that experts can represent information to learners according to pedagogical models that complement their complex and extensive learning curricula.},
	number = {CSCW1},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Ferrier-Barbut, Eléonore and Avellino, Ignacio and Canlorbe, Geoffroy and Vitrani, Marie-Aude and Luengo, Vanda},
	month = apr,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {surgery, conceptual fields theory, guided practice, video-based learning},
}

@inproceedings{ahmetaj_common_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Common {Foundations} for {SHACL}, {ShEx}, and {PG}-{Schema}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714694},
	doi = {10.1145/3696410.3714694},
	abstract = {Graphs have emerged as a foundation for a variety of applications, including capturing factual knowledge, semantic data integration, social networks, and informing machine learning algorithms. Formalising properties of the data and ensuring data quality requires describing schemas of such graphs. Driven by diverse applications, the Semantic Web and database communities developed not only different graph data models-RDF and property graphs-but also different graph schema languages-SHACL, ShEx, and PG-Schema. Each language has its unique approach to defining constraints and validating graph data, leaving potential users in the dark about their commonalities and differences. In this paper, we provide concise formal definitions of the core components of these languages, employ a uniform framework to facilitate a comprehensive comparison between them, and identify a common set of functionalities, shedding light on both overlapping and distinctive features.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Ahmetaj, Shqiponja and Boneva, Iovka and Hidders, Jan and Hose, Katja and Jakubowski, Maxime and Labra Gayo, Jose Emilio and Martens, Wim and Mogavero, Fabio and Murlak, Filip and Okulmus, Cem and Polleres, Axel and Savković, Ognjen and Šimkus, Mantas and Tomaszuk, Dominik},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {RDF, SHACL, common data model, ShEx, graph databases, graph schema languages, PG-schema, property graphs},
	pages = {8--21},
}

@inproceedings{chen_developing_2018,
	address = {New York, NY, USA},
	series = {{ICETC} '18},
	title = {Developing educational ontology: a case study in physics},
	isbn = {978-1-4503-6517-8},
	url = {https://doi.org/10.1145/3290511.3290546},
	doi = {10.1145/3290511.3290546},
	abstract = {Nowadays, e-learning systems are widely developed. With e-learning, students can learn different subjects remotely, and teachers can edit online teaching scripts and issue online courseware. But complain is ceaseless, that is either students or teachers are not satisfied with the existing state of affairs. The reason is that they expect that students can master knowledge architecture of required subjects, but current scattered courseware lacks systematicness. How to describe knowledge architectures of subjects, e.g. in K12, and help students to master them? Ontology can be used to efficiently present knowledge architecture of different subjects, such as Physics. A big challenge is how to construct educational ontology to describe systematic knowledge of subjects automatically. In this paper educational ontology is as a new topic discussed. An approach to automatically constructing educational ontology is proposed to convert textbook into a corresponding Ontology, with High School Physics as an example.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Education} {Technology} and {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Jizhi and Gu, Junzhong},
	year = {2018},
	note = {event-place: Tokyo, Japan},
	keywords = {ontology, e-learning, ontology development, educational ontology},
	pages = {201--206},
}

@inproceedings{ratsamano_knowledge-based_2020,
	address = {New York, NY, USA},
	series = {{ICIT} '19},
	title = {Knowledge-{Based} {Ontology} {Development} for {Folk} {Medicine}},
	isbn = {978-1-4503-7663-1},
	url = {https://doi.org/10.1145/3377170.3377201},
	doi = {10.1145/3377170.3377201},
	abstract = {This research sought to develop a semantic ontology of knowledge about the introduction of folk medicine. In this research, the content of folk medicine is presented comprehensively and the research goals or procedures are defined as follows: (1) determine the purpose and scope of the ontology; (2) develop the ontology by using the Hozo Ontology Editor program (Osaka University, Osaka, Japan); and (3) have an expert evaluate the ontology that is developed. The purpose of this research is to clarify the system-recommended treatment involving folk medicine that can guide patients well. Ideally, the treatment selection process will be accurate 100\% of the time and can be used in related research.},
	booktitle = {Proceedings of the 2019 7th {International} {Conference} on {Information} {Technology}: {IoT} and {Smart} {City}},
	publisher = {Association for Computing Machinery},
	author = {Ratsamano, Salintip and Chairungsee, Supaporn},
	year = {2020},
	note = {event-place: Shanghai, China},
	keywords = {Ontology, semantic web, folk medicine, Thai medicine},
	pages = {70--74},
}

@inproceedings{zhang_study_2023,
	address = {New York, NY, USA},
	series = {{KDD} '23},
	title = {A {Study} of {Situational} {Reasoning} for {Traffic} {Understanding}},
	isbn = {979-8-4007-0103-0},
	url = {https://doi.org/10.1145/3580305.3599246},
	doi = {10.1145/3580305.3599246},
	abstract = {Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work, based on natural language inference, commonsense knowledge-graph self-supervision, multi-QA joint training, and dense retrieval of domain information. We associate each method with a relevant knowledge source, including knowledge graphs, relevant benchmarks, and driving manuals. In extensive experiments, we benchmark various knowledge-aware methods against the three datasets, under zero-shot evaluation; we provide in-depth analyses of model performance on data partitions and examine model predictions categorically, to yield useful insights on traffic understanding, given different background knowledge and reasoning strategies.},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Jiarui and Ilievski, Filip and Ma, Kaixin and Kollaa, Aravinda and Francis, Jonathan and Oltramari, Alessandro},
	year = {2023},
	note = {event-place: Long Beach, CA, USA},
	keywords = {language models, question answering, traffic understanding, zero-shot evaluation},
	pages = {3262--3272},
}

@inproceedings{fong_ontology-powered_2019,
	address = {New York, NY, USA},
	series = {{ITCC} '19},
	title = {Ontology-{Powered} {Hybrid} {Extensional}-{Intensional} {Learning}},
	isbn = {978-1-4503-7228-2},
	url = {https://doi.org/10.1145/3355402.3355406},
	doi = {10.1145/3355402.3355406},
	abstract = {Deep learning has made headlines in the past few years due to successes in tasks, such as self-driving vehicles and board games, which were previously thought difficult or impossible. The successes have generated much interest in artificial intelligence among researchers and members of the public. However, deep learning algorithms generally require very large labelled data sets to work well and large labelled data sets are not always readily available. In addition, most machine learning techniques, including deep learning, often perform well statistically but can fail miserably when, for example, data are deliberately perturbed in an adversarial attack. Another criticism of deep learning techniques is a relative lack of explainability. This paper proposes the use of intentional learning to simultaneously address these issues. Preliminary evaluation on the MNIST data set has shown promising results. Specifically, by combing extensional and intensional learning, it is possible to achieve similar accuracy result as extensional learning only using only one-sixth of the original training data set.},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Information} {Technology} and {Computer} {Communications}},
	publisher = {Association for Computing Machinery},
	author = {Fong, A. C.M. and Hong, Guanyue},
	year = {2019},
	note = {event-place: Singapore, Singapore},
	keywords = {ontologies, neural networks, intension and extension of concepts, Machine learning paradigms},
	pages = {18--23},
}

@inproceedings{fischer_generative_2023,
	address = {New York, NY, USA},
	series = {{CUI} '23},
	title = {Generative {AI} {Considered} {Harmful}},
	isbn = {979-8-4007-0014-9},
	url = {https://doi.org/10.1145/3571884.3603756},
	doi = {10.1145/3571884.3603756},
	abstract = {The recent months have seen an explosion of interest, hype, and concern about generative AI, driven by the release of ChatGPT. In this article I seek to explicate some potential and actual harms of the engineering and use of generative AI such as ChatGPT. With this I also suggest a reframing for researchers with an interest in interaction. With this reframing I seek to provoke researchers to consider studying the settings of ChatGPT development and use as active sites of production. Research should focus on the organisational, technological and interactional practices and contexts in and through which generative AI and its outputs—harmful and otherwise—are produced, by whom, to what end, and with what consequences on societies.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Fischer, Joel E},
	year = {2023},
	note = {event-place: Eindhoven, Netherlands},
	keywords = {ChatGPT, Large Language Models, LLM, NLP, generative AI, GPT-3, GPT-4, natural language, text generation, NLG},
}

@inproceedings{song_model-based_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Model-based fleet deployment of edge computing applications},
	isbn = {978-1-4503-7019-6},
	url = {https://doi.org/10.1145/3365438.3410951},
	doi = {10.1145/3365438.3410951},
	abstract = {Edge computing brings software in close proximity to end users and IoT devices. Given the increasing number of distributed Edge devices with various contexts, as well as the widely adopted continuous delivery practices, software developers need to maintain multiple application versions and frequently (re-)deploy them to a fleet of many devices with respect to their contexts. Doing this correctly and efficiently goes beyond manual capabilities and requires employing an intelligent and reliable automated approach. Accordingly this paper describes a joint research with a Smart Healthcare application provider on a model-based approach to automatically assigning multiple software deployments to hundreds of Edge gateways. From a Platform-Specific Model obtained from the existing Edge computing platform, we extract a Platform-Independent Model that describes a list of target devices and a pool of available deployments. Next, we use constraint solving to automatically assign deployments to devices at once, given their specific contexts. The resulting solution is transformed back to the PSM as to proceed with software deployment accordingly. We validate the approach with a Fleet Deployment prototype integrated into the DevOps toolchain currently used by the application provider. Initial experiments demonstrate the viability of the approach and its usefulness in supporting DevOps in Edge computing applications.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Song, Hui and Dautov, Rustem and Ferry, Nicolas and Solberg, Arnor and Fleurey, Franck},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {IoT, DevOps, device fleet, model-based software engineering, software deployment},
	pages = {132--142},
}

@inproceedings{voit_converting_2020,
	address = {New York, NY, USA},
	series = {{ICCTA} '20},
	title = {Converting {Diagram} to a {Timeline} {Ontology}},
	isbn = {978-1-4503-7749-2},
	url = {https://doi.org/10.1145/3397125.3397151},
	doi = {10.1145/3397125.3397151},
	abstract = {CAD systems design and development is not a simple process. It consists of large amount of works, most of them are interconnected and should be performed either simultaneously or sequentially, some of them depend on success of previous works etc. Design workflows allow to describe works in visual form and calculate their quantitative and qualitative parameters. They significantly increase the design process efficiency and the product quality due to the usage of participants interaction language unification. However, modern workflow management tools lack of some important functions especially in part of temporal analysis and ontology-based timeline diagrams.In this paper, we describe the novel method to convert any diagram to a timeline structure like an ontology. The method includes converting algorithm and allows engineers to get the issue of workflows in which they are involved thus giving them a help to design complex CAD systems. It is shown that any diagram describing complex system behavior may be converted into simple view as a timeline ontology. An illustrated example is given in the article.},
	booktitle = {Proceedings of the 2020 6th {International} {Conference} on {Computer} and {Technology} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Voit, Nikolay and Kirillov, Sergey and Bochkov, Semen},
	year = {2020},
	note = {event-place: Antalya, Turkey},
	keywords = {workflows, analysis, business process, computer-aided design},
	pages = {80--86},
}

@inproceedings{combi_enriching_2021,
	address = {New York, NY, USA},
	series = {{SAC} '21},
	title = {Enriching surgical process models by {BPMN} extensions for temporal durations},
	isbn = {978-1-4503-8104-8},
	url = {https://doi.org/10.1145/3412841.3441939},
	doi = {10.1145/3412841.3441939},
	abstract = {Many surgical interventions are finding new techniques in robot-assisted surgery, which allows surgeons to perform surgery with the help of robotic arms. A formal representation of robot-assisted surgery can provide surgeons with an overview of the main stages of surgical intervention and a detailed description of the different steps, including all the possible emergencies that may occur. Formalizing such kinds of interventions could also help to train new surgeons. However, literature does not consider formal representations and properties of robot-assisted surgery properly.The Business Process Model and Notation (BPMN) is a standard language allowing to represent processes in a graphical and semi-formal way. In this paper, we propose to use BPMN for representing the processes and the guidelines underlying robot-assisted surgery, considering the explicit modeling of temporal and informational aspects: in detail, guidelines aim at providing surgeons with high-level recommendations based on the operational knowledge of expert users, delivering hints on how to execute exploration tasks. As a real-world application domain, we consider here the Robot-Assisted Partial Nephrectomy (RAPN), which is the partial surgical removal of a kidney to treat severe kidney diseases such as cancer.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Combi, Carlo and Galetto, Francesca and Nakawala, Hirenkumar Chandrakant and Pozzi, Giuseppe and Zerbato, Francesca},
	year = {2021},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {nephrectomy, process modelling (BPMN), surgical processes, temporal durations},
	pages = {586--593},
}

@inproceedings{palma_machine_2021,
	address = {New York, NY, USA},
	series = {dg.o '21},
	title = {Machine {Learning} {Predictive} {Model} for the {Passive} {Transparency} at the {Brazilian} {Ministry} of {Mines} and {Energy}},
	isbn = {978-1-4503-8492-6},
	url = {https://doi.org/10.1145/3463677.3463715},
	doi = {10.1145/3463677.3463715},
	abstract = {This paper presents a case study based on the CRISP-DM Model and the use of Text Mining tools and techniques to automate the Passive Transparency process at the Brazilian Ministry of Mines and Energy. Thus, a Machine Learning Model is proposed to predict the class of the technical unit responsible for the data/information requested by citizens. Through the application of the algorithm LDA and TF-IDF it was possible to map the topics of the most relevant subjects for society. The stability of the model was tested from the comparative analysis between 5 known classification algorithms (Random Forest, Multinomial NB, Linear SVC, Logistic Regression, XGBoost and Gradient Boosting). XGBoost presented better performance and precision in multiclass learning outcomes.},
	booktitle = {Proceedings of the 22nd {Annual} {International} {Conference} on {Digital} {Government} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Palma, Ingrid and Ladeira, Marcelo and Reis, Ana Carla Bittencourt},
	year = {2021},
	note = {event-place: Omaha, NE, USA},
	keywords = {Topic Modeling, Machine Learning Algorithms, Multicriteria Decision Making, Passive Transparency, Predictive Analisys and XGBoost.},
	pages = {76--81},
}

@inproceedings{souza_experience_2021,
	address = {New York, NY, USA},
	series = {{SBQS} '20},
	title = {Experience {Report} on {Developing} an {Ontology}-based {Approach} for {Knowledge} {Management} in {Software} {Testing}},
	isbn = {978-1-4503-8923-5},
	url = {https://doi.org/10.1145/3439961.3439993},
	doi = {10.1145/3439961.3439993},
	abstract = {Software testing is a knowledge intensive process. Thus, Knowledge Management (KM) emerges as a means to manage testing knowledge, and, consequently, to improve software quality. However, there are only a few KM solutions supporting software testing. This paper reports experiences from the development of an approach, Ontology-based Testing Knowledge Management (OntoT-KM), to assist in launching KM initiatives in the software testing domain with the support of Knowledge Management Systems (KMSs). OntoT-KM provides a process guiding how to start applying KM in software testing. OntoT-KM is based on the findings of a systematic mapping on KM in software testing and the results of a survey with testing practitioners. Moreover, OntoT-KM considers the conceptualization established by a Reference Ontology on Software Testing (ROoST). As a proof of concept, OntoT-KM was applied to develop a KMS called Testing KM Portal (TKMP), which was evaluated in terms of usefulness, usability and functional correctness. Results show that the developed KMS from OntoT-KM is a potential system for managing knowledge in software testing, so, the approach is able to guide KM initiatives in software testing.},
	booktitle = {Proceedings of the {XIX} {Brazilian} {Symposium} on {Software} {Quality}},
	publisher = {Association for Computing Machinery},
	author = {Souza, Érica Ferreira de and Falbo, Ricardo de Almeida and Specimille, Marcos S. and Coelho, Alexandre G. N. and Vijaykumar, Nandamudi L. and Felizardo, Katia Romero and Meinerz, Giovani Volnei},
	year = {2021},
	note = {event-place: São Luís, Brazil},
	keywords = {Knowledge Management, Knowledge Management System, Software Testing, Testing Ontology},
}

@inproceedings{zhang_distributed_2024,
	address = {New York, NY, USA},
	series = {{ICBAR} '23},
	title = {A {Distributed} {Logistics} {Data} {Security} {Sharing} {Model} {Based} on {Semantics} and {CP}-{ABE}},
	isbn = {979-8-4007-1647-8},
	url = {https://doi.org/10.1145/3656766.3656776},
	doi = {10.1145/3656766.3656776},
	abstract = {Effective sharing of logistics data in a secure manner is vital for the growth and progress of the logistics industry. With the development of logistics digitalization, the demand for data sharing among logistics enterprises is fiercely increasing. However, ensuring the safety and security of logistics business data is a challenging task, especially considering the importance of data sharing and exchange for business collaboration. To address this issue, we propose a distributed logistics data security sharing model that adopts semantics and CP-ABE technology to solve the semantic heterogeneity problem between logistics enterprises and ensure data security. In addition, we propose a semantic based access policy generation method, which is integrated into CP-ABE to simplify the creation process of access policies and improve the user friendliness and practicality of the system.},
	booktitle = {Proceedings of the 2023 3rd {International} {Conference} on {Big} {Data}, {Artificial} {Intelligence} and {Risk} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Xue and Wang, Li and Xu, Lianzheng and Fu, Deqian},
	year = {2024},
	note = {event-place: Chengdu, China},
	pages = {51--56},
}

@article{rizzo_ranking_2022,
	title = {Ranking {Models} for the {Temporal} {Dimension} of {Text}},
	volume = {41},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3565481},
	doi = {10.1145/3565481},
	abstract = {Temporal features of text have been shown to improve clustering and organization of documents, text classification, visualization, and ranking. Temporal ranking models consider the temporal expressions found in text (e.g., “in 2021” or “last year”) as time units, rather than as keywords, to define a temporal relevance and improve ranking. This article introduces a new class of ranking models called Temporal Metric Space Models (TMSM), based on a new domain for representing temporal information found in documents and queries, where each temporal expression is represented as a time interval. Furthermore, we introduce a new frequency-based baseline called Temporal BM25 (TBM25). We evaluate the effectiveness of each proposed metric against a purely textual baseline, as well as several variations of the metrics themselves, where we change the aggregate function, the time granularity and the combination weight. Our extensive experiments on five test collections show statistically significant improvements of TMSM and TBM25 over state-of-the-art temporal ranking models. Combining the temporal similarity scores with the text similarity scores always improves the results, when the combination weight is between 2\% and 6\% for the temporal scores. This is true also for test collections where only 5\% of queries contain explicit temporal expressions.},
	number = {2},
	journal = {ACM Trans. Inf. Syst.},
	author = {Rizzo, Stefano Giovanni and Brucato, Matteo and Montesi, Danilo},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Temporal information retrieval, Temporal Metric Space, temporal ranking, texto-temporal relevance, time similarity, timexes},
}

@inproceedings{motara_structural_2020,
	address = {New York, NY, USA},
	series = {{SAC} '20},
	title = {A structural modeling notation for the typed functional paradigm},
	isbn = {978-1-4503-6866-7},
	url = {https://doi.org/10.1145/3341105.3373863},
	doi = {10.1145/3341105.3373863},
	abstract = {Although typed functional programming is becoming increasingly important for practical software development, it remains inaccessible from a modeling perspective. This paper develops and theoretically justifies an initial best-practices notation for the typed functional paradigm. A small case study explores how the same scenario is modeled differently in the object-oriented and typed functional paradigms, and it is argued that the notation developed is a necessary step on the path to a more comprehensive notation for modeling within the paradigm.},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Motara, Yusuf Moosa},
	year = {2020},
	note = {event-place: Brno, Czech Republic},
	keywords = {fml, functional programming, modeling languages, notation},
	pages = {1515--1522},
}

@article{di_persia_exp2go_2022,
	title = {{exp2GO}: {Improving} {Prediction} of {Functions} in the {Gene} {Ontology} {With} {Expression} {Data}},
	volume = {20},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2022.3167245},
	doi = {10.1109/TCBB.2022.3167245},
	abstract = {The computational methods for the prediction of gene function annotations aim to automatically find associations between a gene and a set of Gene Ontology (GO) terms describing its functions. Since the hand-made curation process of novel annotations and the corresponding wet experiments validations are very time-consuming and costly procedures, there is a need for computational tools that can reliably predict likely annotations and boost the discovery of new gene functions. This work proposes a novel method for predicting annotations based on the inference of GO similarities from expression similarities. The novel method was benchmarked against other methods on several public biological datasets, obtaining the best comparative results. exp2GO effectively improved the prediction of GO annotations in comparison to state-of-the-art methods. Furthermore, the proposal was validated with a full genome case where it was capable of predicting relevant and accurate biological functions. The repository of this project withh full data and code is available at \&lt;uri\&gt;https://github.com/sinc-lab/exp2GO\&lt;/uri\&gt;.},
	number = {2},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Di Persia, Leandro and Lopez, Tiago and Arce, Agustin and Milone, Diego H. and Stegmayer, Georgina},
	month = apr,
	year = {2022},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {999--1008},
}

@inproceedings{yang_towards_2025,
	address = {New York, NY, USA},
	series = {{JCDL} '24},
	title = {Towards {Comprehensive} {Artwork} {Representation}: {Motivations} and {Challenges} in {Capturing} {Multi}-{Dimensional} {Art} {Descriptions}},
	isbn = {979-8-4007-1093-3},
	url = {https://doi.org/10.1145/3677389.3702517},
	doi = {10.1145/3677389.3702517},
	abstract = {This paper explores the motivations and challenges in developing a comprehensive framework for artwork representation. Current artwork-related ontologies and description methods often fall short in capturing the multi-faceted nature of artworks, focusing primarily on basic metadata or specific aspects while neglecting others. We propose a conceptual framework for an ontology that integrates descriptive, contextual, and interpretive aspects of artworks, addressing the limitations of existing models. The paper discusses the potential benefits of this holistic approach for art education, public appreciation, and digital accessibility. Key challenges are identified, including capturing complex visual elements, balancing objectivity with subjectivity in interpretation, and representing the multiple layers of meaning in artworks. The proposed framework aims to enhance the quality and depth of artwork representation, potentially facilitating the development of automated systems for artwork analysis and description.},
	booktitle = {Proceedings of the 24th {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Can and Pereira Nunes, Bernardo and Rodríguez Méndez, Sergio and Chen, Yige and Manrique, Rubén and Casanova, Marco Antonio},
	year = {2025},
	note = {event-place: Hong Kong, China},
	keywords = {ontology, artworks, captioning, contextual object model, descriptive object model},
}

@inproceedings{wagner_business_2022,
	address = {Phoenix, Arizona},
	series = {{WSC} '21},
	title = {Business process modeling and simulation with {DPMN}: processing activities},
	abstract = {The Business Process Modeling Notation (BPMN) has been established as a modeling standard in Business Process (BP) Management. However, BPMN lacks several important elements needed for BP simulation and is not well-aligned with the Queueing Network paradigm of Operations Research and the related BP simulation paradigm pioneered by the Discrete Event Simulation (DES) languages/tools GPSS and SIMAN/Arena. The Discrete Event Process Modeling Notation (DPMN) proposed by Wagner (2018) is based on Event Graphs (Schruben 1983), which capture the DES paradigm of Event-Based Simulation. By allowing to make flowchart models of queueing/processing networks with a precise semantics, DPMN reconciles (the flowchart approach of) BPMN with DES. DPMN is the first visual modeling language that supports all important DES approaches: event-based simulation, activity-based DES and Processing Network models, providing a foundation for harmonizing and unifying the many different terminologies/concepts and diagram languages of established DES tools.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Wagner, Gerd},
	year = {2022},
}

@inproceedings{xian_construction_2018,
	address = {New York, NY, USA},
	series = {{CSAE} '18},
	title = {Construction and {Application} of {Upper} {Country} {Ontology} {Based} on {OWL} and {SKOS}},
	isbn = {978-1-4503-6512-3},
	url = {https://doi.org/10.1145/3207677.3278056},
	doi = {10.1145/3207677.3278056},
	abstract = {The concept1 of country has been widely used in all kinds of databases or systems. A commonly understood and unified schema for country entity is needed to knowledge organization, semantic association and deeply integration of the multi-source and heterogeneous data resources about country concepts. This paper proposes a computer understandable and computable Upper Country Ontology based on OWL and SKOS, with instances of 195 countries, 7 continents, 38 regions and more than 10 international organizations. The proposed ontology is expected to be used as knowledge middleware, supports identifying and indexing country entities, publishing open linked dataset, referencing country instance, SPARQL endpoint query and semantic reasoning and so on.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Computer} {Science} and {Application} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Xian, Guojian and Li, Jiao and Kou, Yuantao and Luo, Tingting and Huang, Yongwen},
	year = {2018},
	note = {event-place: Hohhot, China},
	keywords = {SPARQL, OWL, SKOS, Protégé, Jena, Upper Country Ontology, WebVOWL},
}

@inproceedings{van_bremen_ontology-mediated_2019,
	address = {New York, NY, USA},
	series = {{CIKM} '19},
	title = {Ontology-{Mediated} {Queries} over {Probabilistic} {Data} via {Probabilistic} {Logic} {Programming}},
	isbn = {978-1-4503-6976-3},
	url = {https://doi.org/10.1145/3357384.3358168},
	doi = {10.1145/3357384.3358168},
	abstract = {We study ontology-mediated querying over probabilistic data for the case when the ontology is formulated in EL(hdr), an expressive member of the EL family of description logics. We leverage techniques that have been developed (i) for classical ontology-mediated querying and (ii) for probabilistic logic programming and provide an implementation based on our findings. We include both theoretical considerations and an experimental evaluation of our approach.},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {van Bremen, Timothy and Dries, Anton and Jung, Jean Christoph},
	year = {2019},
	note = {event-place: Beijing, China},
	keywords = {probabilistic logic programming, uncertainty, ontology-mediated querying},
	pages = {2437--2440},
}

@inproceedings{paweloszek_merging_2018,
	address = {New York, NY, USA},
	series = {{ICIME} 2018},
	title = {Merging of {Ontologies} - {Conceptual} {Design} {Issues}},
	isbn = {978-1-4503-6489-8},
	url = {https://doi.org/10.1145/3285957.3285987},
	doi = {10.1145/3285957.3285987},
	abstract = {An approach to merging ontologies is presented that integrates financial knowledge in Decision Support Systems. The process of ontology merging is supported by an editor Protégé, and illustrated by examples extracted from financial information systems. It is shown how the correspondences between concepts, properties, and relations in the various ontologies can be processed. However not all problems of ontology integration may be resolved automatically. Therefore a number of cases of manager involvement in the merging process are considered.},
	booktitle = {Proceedings of the 2018 10th {International} {Conference} on {Information} {Management} and {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Pawełoszek, Ilona and Korczak, Jerzy},
	year = {2018},
	note = {event-place: Salford, United Kingdom},
	keywords = {Ontology integration, Ontology merging, Financial ontology, Decision Support Systems},
	pages = {59--63},
}

@inproceedings{lin_traceability_2021,
	address = {Madrid, Spain},
	series = {{ICSE} '21},
	title = {Traceability {Transformed}: {Generating} more {Accurate} {Links} with {Pre}-{Trained} {BERT} {Models}},
	isbn = {978-1-4503-9085-9},
	url = {https://doi.org/10.1109/ICSE43902.2021.00040},
	doi = {10.1109/ICSE43902.2021.00040},
	abstract = {Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31\% measured using Mean Average Precision (MAP). RNN severely underper-formed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning.},
	booktitle = {Proceedings of the 43rd {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang, Meng and Cleland-Huang, Jane},
	year = {2021},
	keywords = {deep learning, language models, Software traceability},
	pages = {324--335},
}

@inproceedings{de_kok_using_2020,
	address = {New York, NY, USA},
	series = {{SAC} '20},
	title = {Using word embeddings for ontology-driven aspect-based sentiment analysis},
	isbn = {978-1-4503-6866-7},
	url = {https://doi.org/10.1145/3341105.3373848},
	doi = {10.1145/3341105.3373848},
	abstract = {Nowadays, the Web is the main platform to gather information. The growing amount of freely available unstructured data has increased the interest in sentiment analysis, where the goal is to extract opinions from text. In this paper we focus on review-level aspect-based sentiment analysis, where we predict the sentiment of a certain aspect in a review. We propose a two-stage sentiment analysis algorithm. In the first stage a domain ontology is utilized to predict the sentiment. If the domain ontology stage is inconclusive, a back-up stage based on an SVM bag-of-words model is employed. Furthermore, the use of word embeddings to improve the domain ontology coverage in the first stage by finding semantically similar words is investigated. We find that the two-stage approach significantly outperforms two baseline methods and achieves competitive results for the SemEval-2016 data. Furthermore, by not employing the back-up stage, we still perform significantly better than the baselines. Lastly, we find that employing word embeddings improves the accuracy when the domain ontology size is relatively small.},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {de Kok, Sophie and Frasincar, Flavius},
	year = {2020},
	note = {event-place: Brno, Czech Republic},
	keywords = {domain ontology, word embeddings, aspect-based sentiment analysis, review-level sentiment analysis},
	pages = {834--842},
}

@inproceedings{bekmanova_intelligent_2024,
	address = {New York, NY, USA},
	series = {{ICSLT} '24},
	title = {Intelligent question-answering system based on the public political discourse knowledge},
	isbn = {979-8-4007-1679-9},
	url = {https://doi.org/10.1145/3678610.3678613},
	doi = {10.1145/3678610.3678613},
	abstract = {The article describes the implementation of a question-answering system based on knowledge of public political discourse. An ontology of the subject area was developed, a program was developed to publish an answer to a question on the topic of political discourse in the Kazakh language.},
	booktitle = {Proceedings of the 2024 10th {International} {Conference} on {E}-{Society}, e-{Learning} and e-{Technologies} ({ICSLT})},
	publisher = {Association for Computing Machinery},
	author = {Bekmanova, Gulmira and Sairanbekova, Ayaulym and Ongarbayev, Yerkin and Mukanova, Assel and Zulkhazhav, Altanbek and Omarbekova, Assel and Ukenova, Aru},
	year = {2024},
	keywords = {Artificial intelligence, ontology, OWL, knowledge base, sparql, discourse, formalization, owlready, python},
	pages = {14--19},
}

@article{doerr_identifiable_2025,
	title = {Identifiable {Individuals} and {Reality}: {Describing} the {Past} by\&nbsp;{Formal}\&nbsp;{Propositions}},
	volume = {18},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3715011},
	doi = {10.1145/3715011},
	abstract = {Data of historical and ideographic sciences, such as cultural heritage studies, geography, geological evolution, biodiversity, but also experimental data of nomothetic natural sciences, are increasingly documented and published in information systems compatible with predicate-logic that refer to things in reality by unique identifiers (or “keys” most likely to be unique in some context). This can only work as a method of sharing and integrating knowledge beyond the spatial and temporal context of local projects, if the referred features or phenomena of reality are distinct and can diachronically be identified by independent observers in the same way and without a clarifying dialogue between them. In this article, we argue that only a smaller part of the features in our environment are sufficiently distinct over a useful time-span in order to form such “identifiable individuals”. Ontological categories should each provide specific criteria for the so-called ontological individuation, i.e., about how parts of reality can be subdivided into identifiable individuals that are useful for modelling their behaviour and interactions in reality for answering specific scientific questions, by obeying evidential constraints as a result of applying these criteria in observations. We motivate by several examples that there are always cases in which the individuality of such an instance may be undecidable basically within all such ontological categories and that all ontological categories are more or less effective approximations of reality. We argue that effective knowledge sharing by information systems using formal ontologies is only possible if these limitations of applicability and precision to real world phenomena are well understood and taken into account.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Doerr, Martin},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Information integration, Scientific realism, Epistemic Individuation, Historical Data, Historical Sciences, Ontological Individuation},
}

@inproceedings{ihab_ontology-based_2019,
	address = {New York, NY, USA},
	series = {{SCA} '19},
	title = {Ontology-based sentiment analysis and community detection on social media: application to {Brexit}},
	isbn = {978-1-4503-6289-4},
	url = {https://doi.org/10.1145/3368756.3369090},
	doi = {10.1145/3368756.3369090},
	abstract = {Sentiment Analysis and Community Detection are two of the main methods used to analyze and comprehend human interactions on social media. These domains expanded immensely with the rise of social media, as it provided a free and ever-increasing quantity of data. Domain ontologies are of great assistance in collecting specific data, as it describes the domain's features and their existing relationships. Therefore, we utilize them in collecting subject-specific data on social media. This paper describes the framework we've designed in order to understand, in depth, the impact of a subject on social media users, and also to evaluate the difference between the Lexicon Approach and the Machine Learning Approach, by assessing the strengths and weaknesses of each. This framework also aims to deeply understand the connections that exist between users, depending on their point of view on a particular subject. The resulting framework not only analyzes textual data (by taking into account the negation and sentence POS tags), but also visual one, such as images. In order to test the framework, we chose to analyze the Brexit phenomenon by collecting ontology-based data from Twitter and Reddit, and it had some promising results.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Smart} {City} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Ihab, Moudhich and Soumaya, Loukili and Mohamed, Bahra and Haytam, Hmami and Abdelhadi, Fennan},
	year = {2019},
	note = {event-place: Casablanca, Morocco},
	keywords = {machine learning, ontology, classification, sentiment analysis, social media analysis, community detection, opinion mining, lexicon},
}

@inproceedings{khannat_towards_2020,
	address = {New York, NY, USA},
	series = {{SITA}'20},
	title = {Towards {Mining} {Semantically} {Enriched} {Configurable} {Process} {Models}},
	isbn = {978-1-4503-7733-1},
	url = {https://doi.org/10.1145/3419604.3419797},
	doi = {10.1145/3419604.3419797},
	abstract = {Providing configurable process model with high quality is a primary objective to derive process variants with better accuracy and facilitate process model reuse. For this purpose, many research works have been interested in configurable process mining techniques to discover and configure processes from event logs. Moreover, to use the knowledge captured by event logs when mining processes, the concept of semantic process mining is introduced. It allows for combining semantic technologies with process mining. Despite the diversity of works in mining and customizing configurable process models, the application of these techniques is still limited to use semantics in minimizing the complexity of discovered processes. However, it seems to be pertinent to discover semantically enriched configurable process models directly from event logs. Consequently, this can facilitate using semantic in configuring, verifying conformance or enhancing discovered configurable processes. In this paper, we present a comparative study of existing works that focus on mining configurable process models with respect to semantic technologies. Our aim is to propose a new framework to automatically discover semantically enriched configurable processes.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Intelligent} {Systems}: {Theories} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Khannat, Aicha and Sbai, Hanae and Kjiri, Laila},
	year = {2020},
	note = {event-place: Rabat, Morocco},
	keywords = {Ontology, Semantic Technologies, Variability, Configurable Process Model, Process Mining},
}

@inproceedings{olabe_modern_2020,
	address = {New York, NY, USA},
	series = {{ICEEL} '19},
	title = {Modern {Education} with a {Computational} {Model} of the {Mind}},
	isbn = {978-1-4503-7225-1},
	url = {https://doi.org/10.1145/3371647.3371666},
	doi = {10.1145/3371647.3371666},
	abstract = {We are witnessing a great effort on the part of the educational systems of the world in modernizing the curricular content of primary and secondary schools. One example of these educational initiatives is the effort of integrating aspects of engineering and technology with the existing core subjects of sciences and mathematic in K-12 education. These efforts are often labeled as STEM or STEAM (Science, Technology, Engineering, Mathematics, and Arts.) These subjects, studied in an integral form, are considered essential in the education of the citizens of a modern society. A common obstacle encountered in the implementation of these projects or initiatives is the lack of consensus on the specific topics to be included in the curriculum, the pedagogical methodology selected for the classroom, and the means, computer-based or otherwise, to be used by the teachers and the students. Often the lack of consensus among the different constituencies in charge of these projects finds its roots in the different assumptions made by their participants. This paper addresses one of the most acute set of differences present in these projects: the teaching methods used in class, knowing the resources and limitations of the human mind. In the last few decades we have learned much of how the mind works; what tasks are intrinsically easy or difficult for the human mind. Also, with the extensive access to computing power in our society, it is important to determine if a traditional task was studied in school for its practical use or for its value in developing the potential qualities of the mind. In this paper we will use the word computation in its traditional meaning of symbol manipulation. In that sense, all processes of thinking, solving problems, and endeavors of creation are processes of symbol manipulation, or computation. In this paper we present a computational model of the mind in order to provide a standard reference that will help in finding answers to questions such as: when is a task complex, what are the cognitive capabilities and limitations of the mind, what teaching methodologies are optimal, and why.},
	booktitle = {Proceedings of the 2019 3rd {International} {Conference} on {Education} and {E}-{Learning}},
	publisher = {Association for Computing Machinery},
	author = {Olabe, Juan Carlos and Basogain, Xabier and Olabe, Miguel Ángel},
	year = {2020},
	note = {event-place: Barcelona, Spain},
	keywords = {Ontology, Computing, Complex Systems, Computational Theory of the Mind, Model of the Mind, New approaches to problem solving, STEM Education},
	pages = {41--45},
}

@inproceedings{sultan_ai-driven_2024,
	address = {New York, NY, USA},
	series = {{MODELS} '24},
	title = {{AI}-{Driven} {Consistency} of {SysML} {Diagrams}},
	isbn = {979-8-4007-0504-5},
	url = {https://doi.org/10.1145/3640310.3674079},
	doi = {10.1145/3640310.3674079},
	abstract = {Graphical modeling languages, expected to simplify systems analysis and design, present a challenge in maintaining consistency across their varied views. Traditional rule-based methods for ensuring consistency in languages like UML often fall short in addressing complex semantic dimensions. Moreover, the integration of Large Language Models (LLMs) into Model Driven Engineering (MDE) introduces additional consistency challenges, as LLM's limited output contexts requires the integration of responses. This paper presents a new framework that automates the detection and correction of inconsistencies across different views, leveraging formally defined rules and incorporating OpenAI's GPT, as implemented in TTool. Focusing on the consistency between use case and block diagrams, the framework is evaluated through its application to three case studies, highlighting its potential to significantly enhance consistency management in graphical modeling.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sultan, Bastien and Apvrille, Ludovic},
	year = {2024},
	note = {event-place: Linz, Austria},
	pages = {149--159},
}

@inproceedings{herding_design_2022,
	address = {Phoenix, Arizona},
	series = {{WSC} '21},
	title = {Design and application of an ontology for demand fulfillment in semiconductor supply chains},
	abstract = {Ensuring interoperability of different information systems for planning and control is a challenging task in semiconductor supply chains. This is partially caused by the sheer size of the involved production facilities and the supply chains in the semiconductor domain, the permanent appearance of uncertainty, and the rapid technological changes which lead to sophisticated planning and control systems in this domain. Ontologies are a promising approach to support interoperability among such systems. Demand fulfillment is an important function in semiconductor supply chains. However, at the same time, it is a planning function that is not very well understood. In the present paper, a domain- and task ontology for demand fulfillment is designed based on a domain analysis. The usage of the proposed ontology is illustrated by means of an example.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Herding, Raphael and Mönch, Lars and Ehm, Hans},
	year = {2022},
}

@inproceedings{hammoudeh_bringing_2022,
	address = {New York, NY, USA},
	series = {{ICFNDS} '21},
	title = {Bringing {Coordination} {Languages} {Back} to the {Future} {Using} {Blockchain} {Smart} {Contracts}},
	isbn = {978-1-4503-8734-7},
	url = {https://doi.org/10.1145/3508072.3508117},
	doi = {10.1145/3508072.3508117},
	abstract = {This paper presents a blockchain extension of the run-time Sensing as a Service SOA (3SOA) approach presented in\&nbsp;[5]. 3SOA defines a practical approach for implementing service-oriented Internet of Things (IoT) using coordination languages to integrate and program individual IoT objects to compose into full IoT system. We believe that the modularity, reuse, interoperability and portability of this model has much to offer, but that there exist some challenges in overcoming the performance issues inherent in the approach, and extending the range of applications to which it is suited. We are particularly interested in applying the coordination languages to decentralized systems. To this end, blockchain smart contracts are proposed to offer a decentralized trustable method to automatically verify compliance with pre-defined conditions before executing a transaction involving multiple parties. To validate our proposal, we demonstrate a healthcare functional prototypes as a proof of concept.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Future} {Networks} and {Distributed} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hammoudeh, Mohammad and Adebisi, Bamidele and Unal, Devrim and Laouid, Abdelkader},
	year = {2022},
	note = {event-place: Dubai, United Arab Emirates},
	keywords = {Internet of Things, blockchain, smart contracts, service-oriented},
	pages = {299--304},
}

@inproceedings{gaur_kil_2023,
	address = {New York, NY, USA},
	series = {{KDD} '23},
	title = {{KiL} 2023 : 3rd {International} {Workshop} on {Knowledge}-infused {Learning}},
	isbn = {979-8-4007-0103-0},
	url = {https://doi.org/10.1145/3580305.3599199},
	doi = {10.1145/3580305.3599199},
	abstract = {Recent prolific advances in artificial intelligence through the incorporation of domain knowledge have constituted a new paradigm for AI and data mining communities. For example, the human feedback-based language generation in ChatGPT (a large language model (LLM)), the use of Protein Bank in DeepMind's AlphaFold, and the use of 23 rules of safety in DeepMind's Sparrow have demonstrated the success of teaming human knowledge and AI. In addition, the knowledge retrieval-guided language modeling methods have strengthened the association between knowledge and AI. However, translating research methods and resources into practice presents a new challenge for the machine learning and data/knowledge mining communities. For example, in DARPA's Explainable AI seminar, the need for explainable contextual adaptation is seen as the 3rd phase of AI, facilitating the interplay between data and knowledge for explainability, safety, and, eventually, trust. However, policymakers and practitioners assert serious usability and privacy concerns that constrain adoption, notably in high-consequence domains, such as cybersecurity, healthcare, and other social good domains. In addition, limitations in output quality, measurement, and interactive ability, including both the provision of explanations and the acceptance of user preferences, result in low adoption rates in such domains. This workshop aims to accelerate our pace towards creating innovative methods for integrating knowledge into contemporary AI and data science methods and develop metrics for assessing performance in various applications.},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Gaur, Manas and Tsamoura, Efthymia and Sreedharan, Sarath and Mittal, Sudip},
	year = {2023},
	note = {event-place: Long Beach, CA, USA},
	keywords = {language models, programming languages, explainable ai, games, knowledge-infused learning, neurosymbolic ai, safe ai},
	pages = {5857--5858},
}

@inproceedings{savelka_lex_2021,
	address = {New York, NY, USA},
	series = {{ICAIL} '21},
	title = {Lex {Rosetta}: transfer of predictive models across languages, jurisdictions, and legal domains},
	isbn = {978-1-4503-8526-8},
	url = {https://doi.org/10.1145/3462757.3466149},
	doi = {10.1145/3462757.3466149},
	abstract = {In this paper, we examine the use of multi-lingual sentence embeddings to transfer predictive models for functional segmentation of adjudicatory decisions across jurisdictions, legal systems (common and civil law), languages, and domains (i.e. contexts). Mechanisms for utilizing linguistic resources outside of their original context have significant potential benefits in AI \&amp; Law because differences between legal systems, languages, or traditions often block wider adoption of research outcomes. We analyze the use of Language-Agnostic Sentence Representations in sequence labeling models using Gated Recurrent Units (GRUs) that are transferable across languages. To investigate transfer between different contexts we developed an annotation scheme for functional segmentation of adjudicatory decisions. We found that models generalize beyond the contexts on which they were trained (e.g., a model trained on administrative decisions from the US can be applied to criminal law decisions from Italy). Further, we found that training the models on multiple contexts increases robustness and improves overall performance when evaluating on previously unseen contexts. Finally, we found that pooling the training data from all the contexts enhances the models' in-context performance.},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Savelka, Jaromir and Westermann, Hannes and Benyekhlef, Karim and Alexander, Charlotte S. and Grant, Jayla C. and Amariles, David Restrepo and Hamdani, Rajaa El and Meeùs, Sébastien and Troussel, Aurore and Araszkiewicz, Michał and Ashley, Kevin D. and Ashley, Alexandra and Branting, Karl and Falduti, Mattia and Grabmair, Matthias and Harašta, Jakub and Novotná, Tereza and Tippett, Elizabeth and Johnson, Shiwanni},
	year = {2021},
	note = {event-place: São Paulo, Brazil},
	keywords = {annotation, transfer learning, adjudicatory decisions, document segmentation, domain adaptation, multi-lingual sentence embeddings},
	pages = {129--138},
}

@inproceedings{kim_realtime_2024,
	address = {New York, NY, USA},
	series = {{ASONAM} '23},
	title = {Realtime {Disaster} {Detection} {Through} {GNN} {Models} {Using} {Disaster} {Knowledge} {Graphs}},
	isbn = {979-8-4007-0409-3},
	url = {https://doi.org/10.1145/3625007.3627514},
	doi = {10.1145/3625007.3627514},
	abstract = {In the context of the increasing scale and complexity of disasters caused by rapid climate change, a comprehensive understanding of disaster big data is essential for effective detection and response. The disaster knowledge graph proposed in this paper fills this gap by capturing the connections between various disaster-related data sources and their potential for growth across heterogeneous datasets. We generate time-series disaster graphs every minute using SNS data (e.g., Twitter) and public data, specifically focusing on disasters. Then, we create disaster knowledge graphs to represent the relationships between various data sources and try to predict their potential developments. We label and annotate knowledge graphs and then detect sudden changes in time-series disaster knowledge graphs for disaster detection. To that end, we assess the effectiveness of three state-of-the-art GNN models for graph-based event classification using Graph Convolutional Network (GCN), Graph Attention Network (GAT), and SageConv. In addition, we evaluate a simple clustering model, K-means, for comparison. Our experiments show promising results with approximately 87\% precision in detecting disaster events using structural data and connectivity patterns within disaster graphs. Finally, we measure the result of disaster detection time with an unseen dataset, showing positive results that about 70\% detect a disaster in less than 3 minutes. To comprehensively analyze real-time social media data and understand the patterns of disaster to enhance disaster management and response strategies, our approach combines the strength of GNNs with a designed disaster knowledge graph.},
	booktitle = {Proceedings of the 2023 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Seonhyeong and Khan, Irshad and Kwon, Young-Woo},
	year = {2024},
	note = {event-place: Kusadasi, Turkiye},
	keywords = {knowledge graphs, graph neural networks, disaster detection},
	pages = {221--228},
}

@inproceedings{li_research_2025,
	address = {New York, NY, USA},
	series = {{AIIIP} '24},
	title = {Research on the {Construction} of {Digital} {Knowledge} {Graphs} {Based} on {Resources} of {National} {First}-{Class} {Undergraduate} {Programs}},
	isbn = {979-8-4007-0730-8},
	url = {https://doi.org/10.1145/3707292.3707389},
	doi = {10.1145/3707292.3707389},
	abstract = {[Purpose/Significance]: The digitalization of education is an essential path to advancing higher education. The construction of knowledge graphs is a key approach to achieving the digitalization and intelligence of education. [Method/Process]: This paper leverages the rich video resources of existing national first-class undergraduate programs and, based on the teaching orientations of different universities, independently designs customized ontologies and extraction principles. These are then integrated into the LLM knowledge graph builder to ensure the hierarchical structure of the overall course framework. The course video content is transformed into text form, and large language models (LLMS) and word segmentation tools are used for core content extraction, text cleaning, and lexical analysis. The structured text is then converted into SPO (Subject-Predicate-Object) triplets database. [Results/Conclusions]: Finally, the database is imported into the LLM knowledge graph builder, which is pre-configured with extraction rules. It will automatically generate the knowledge graph. After the text is imported into the LLM knowledge graph builder, it will be manually checked to ensure it better meets the actual needs of the students. [Innovation/Limitations]: The research team plans to apply the knowledge graph to train a specialized knowledge-based Q\&amp;A assistant. This will support students' understanding and self-assessment of knowledge points in an online learning community. Student feedback will be used to improve and enrich the knowledge graph. Compared to existing methods, this approach better aligns with the constantly evolving digital teaching resources available online, offering more comprehensive and higher-level automation.},
	booktitle = {Proceedings of the 2024 3rd {International} {Conference} on {Artificial} {Intelligence} and {Intelligent} {Information} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Li, Yanjun and Yang, Ruiting and Guo, Donghao and Song, Yu},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Ontology, Ontology construction, Federated learning, Students, personalized learning, A, ontology construction, course resources, intelligent Q\&amp, Hierarchical structures, Graph-based, Ontology's, Course resource, Curricula, High educations, Intelligent Q\&A, Personalized learning, Teaching, Undergraduate projects},
	pages = {353--359},
	annote = {Cited by: 0},
}

@article{mohammadi_simulated_2019,
	title = {Simulated {Annealing}-based {Ontology} {Matching}},
	volume = {10},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3314948},
	doi = {10.1145/3314948},
	abstract = {Ontology alignment is a fundamental task to reconcile the heterogeneity among various information systems using distinct information sources. The evolutionary algorithms (EAs) have been already considered as the primary strategy to develop an ontology alignment system. However, such systems have two significant drawbacks: they either need a ground truth that is often unavailable, or they utilize the population-based EAs in a way that they require massive computation and memory. This article presents a new ontology alignment system, called SANOM, which uses the well-known simulated annealing as the principal technique to find the mappings between two given ontologies while no ground truth is available. In contrast to population-based EAs, the simulated annealing need not generate populations, which makes it significantly swift and memory-efficient for the ontology alignment problem. This article models the ontology alignment problem as optimizing the fitness of a state whose optimum is obtained by using the simulated annealing. A complex fitness function is developed that takes advantage of various similarity metrics including string, linguistic, and structural similarities. A randomized warm initialization is specially tailored for the simulated annealing to expedite its convergence. The experiments illustrate that SANOM is competitive with the state-of-the-art and is significantly superior to other EA-based systems.},
	number = {1},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Mohammadi, Majid and Hofman, Wout and Tan, Yao-Hua},
	month = may,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology alignment, OAEI, SANOM, simulated annealing},
}

@inproceedings{hamim_student_2019,
	address = {New York, NY, USA},
	series = {{SCA} '19},
	title = {Student profile modeling: an overview model},
	isbn = {978-1-4503-6289-4},
	url = {https://doi.org/10.1145/3368756.3369075},
	doi = {10.1145/3368756.3369075},
	abstract = {Today, provide a customized, personalized and adaptive service is a big challenge that improve the services and functionalities of systems. The knowledge of the user profile is a vital phase in this process. Profile modeling is an important field that aims to give an abstract representation of some aspects related to the user features. In the educational field, student profile modeling can be a support for decision-making at different levels like learning, orientation, and recommendation. It can mainly offer the most exact description of students in order to: be able to act in case of problems such as failure, drop out; offer students the most appropriate orientation and recommendation; and define the most adaptive learning resources depending on their profile. In this paper, we present an analytical and statistical study on student profile modeling to propose a detailed description of the student's profile obtained with different techniques in different contexts of the educational field. We aim to extract the different categories of student's features that can be used alone or combined for decision making in different fields.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Smart} {City} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Hamim, Touria and Benabbou, Faouzia and Sael, Nawal},
	year = {2019},
	note = {event-place: Casablanca, Morocco},
	keywords = {machine learning, ontologies, learner profile, e-portfolio, profile modeling},
}

@inproceedings{liu_research_2022,
	address = {New York, NY, USA},
	series = {{WI}-{IAT} '21},
	title = {Research and {Construction} of {Classical} {Formulas} {Knowledge} {Graph} {Based} on {Ontology}},
	isbn = {978-1-4503-9187-0},
	url = {https://doi.org/10.1145/3498851.3498942},
	doi = {10.1145/3498851.3498942},
	abstract = {Classical formula is an important part and basis of traditional Chinese prescriptions. The effective storage and expression of classical formula knowledge in ancient books and modern documents is a key issue for studying and using classical formulas. This paper standardized and normalized the contents of the classic formula in "Treatise on Exogenous Febrile Disease" and "The Synopsis of the Golden Chamber", collected and organized classical formulas related documents from China National Knowledge Infrastructure (CNKI). Ontology construction tool Protégé is used to establish the domain ontology of classical formulas, and Neo4j graph database is used to construct the knowledge graph of ancient books and modern CNKI documents. 296 ancient classical formula items and 11175 modern documents from CNKI are collected, normalized and stored in database as the data source of this research. On the basis of these work, constructed the ontology and knowledge graph of classical formulas and built an application system for knowledge query. Constructing knowledge graph from top to bottom based on ontology can express and visualize the related knowledge of classical formulas accurately and efficiently. The construction strategy mentioned in this paper has got a good result and showed great potential in traditional Chinese medicine knowledge domain.},
	booktitle = {{IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Li and Li, Xuebo},
	year = {2022},
	note = {event-place: Melbourne, VIC, Australia},
	keywords = {Knowledge Graph, TCM, Classical Formula},
	pages = {140--143},
}

@inproceedings{jose_simulating_2021,
	address = {Orlando, Florida},
	series = {{WSC} '20},
	title = {Simulating re-configurable multi-rovers for planetary exploration using behavior-based ontology},
	isbn = {978-1-7281-9499-8},
	abstract = {For planetary explorations, the space agencies have usually sent single robotic rovers to complete missions. An alternative approach is to send multiple rovers, which can insure against failure of one or more rovers. Planning for a multi-rover mission has its own challenges, and simulations can aid in identifying and addressing such challenges. In this paper, we present an ontology-based approach to simulate a multi-rover planetary exploration mission, with a focus on resilience, adaptation, heterogeneity, and reconfigurability. We present an ontology that describes multiple rovers along with an inventory of their parts shipped with a lander. Our approach shows that having the ontology-based simulations help in complex scenarios such as to loan parts from inventory, and salvaging a damaged rover for good parts.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Jose, Justin and Singh, Divye and Patel, Amit and Hayatnagarkar, Harshal G.},
	year = {2021},
	pages = {254--265},
}

@inproceedings{hemam_multi-viewpoints_2018,
	address = {New York, NY, USA},
	series = {{ICEMIS} '18},
	title = {Multi-{Viewpoints} {Ontological} {Knowledge} {Representation}: {A} {Fuzzy} {Description} {Logics} {Based} {Approach}},
	isbn = {978-1-4503-6392-1},
	url = {https://doi.org/10.1145/3234698.3234761},
	doi = {10.1145/3234698.3234761},
	abstract = {Description Logics (DLs) play a key role in the design of ontologies. An ontology is a formal description of important concepts in a particular domain. In practice, a set of a specific-domain applications use different representations of the same real world entity due to various viewpoints, context, and specific interest. In this paper, we are interested in the problem of representing an ontology in a heterogeneous domain by taking into consideration different viewpoints and different terminologies of various users, groups or even communities in the organisation. This type of ontology, called multi-viewpoints ontology, confers to the same universe of discourse, several partial descriptions, where each one is relative to a particular viewpoint. Moreover, these partial descriptions share at global level, fuzzy ontological elements allowing the representation of vague/imprecise knowledge between the various viewpoints. So, our goal is to propose a fuzzy multi-viewpoints ontology Web language, which is an extension of OWL-DL language, to allow the multi-viewpoints ontologies representation.},
	booktitle = {Proceedings of the {Fourth} {International} {Conference} on {Engineering} \&amp; {MIS} 2018},
	publisher = {Association for Computing Machinery},
	author = {Hemam, Mounir and Djezzar, Meriem and Seghir, Zianou Ahmed},
	year = {2018},
	note = {event-place: Istanbul, Turkey},
	keywords = {Ontology, Semantic Web, Knowledge Engineering, Description Logics, Fuzziness, Viewpoint},
}

@inproceedings{reynolds_towards_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {Towards model-driven self-explanation for autonomous decision-making systems},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00095},
	doi = {10.1109/MODELS-C.2019.00095},
	abstract = {The ability for systems to make decisions by themselves is increasing with advances in different areas of AI such as machine learning and optimisation techniques for autonomous systems among other. Humans are handing over more decisions to systems that provide no explanations for their judgements unless they are enabled explicitly in their design. Trust based on a program being well written and tested correctly is not appropriate for AI-based autonomous systems. Unlike traditional software, this new software increasingly exhibit emergent behaviours making it unpredictable due to unexpected situations. Self-explanation is sometimes implemented, tracking decisions to give explanations to users. A more consistent, proven approach to self-explanation would be needed for making trustable systems.The paper proposes a research agenda to define an architecture to enable self-explanation for autonomous decision-making systems. The approach will be model-driven to facilitate reuse, the rapid development of tools and suitable abstractions for demonstrating concepts. The architecture will be informed by existing research in provenance ontology and model version research. The evaluation of the architecture is expected to be done using two case studies. The first will implement self-explanation as a primary concern in the building of a system. The second case will attempt to fit self-explanation to an existing system.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Reynolds, Owen J.},
	year = {2021},
	keywords = {decision-making, model-driven, autonomous, self-explanation},
	pages = {624--628},
}

@inproceedings{verkijk_you_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {Do you catch my drift? {On} the usage of embedding methods to measure concept shift in knowledge graphs},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627555},
	doi = {10.1145/3587259.3627555},
	abstract = {Automatically detecting and measuring differences between evolving Knowledge Graphs (KGs) has been a topic of investigation for years. With the rising popularity of embedding methods, we investigate the possibility of using embeddings to detect Concept Shift in evolving KGs. Specifically, we go deeper into the usage of nearest neighbour set comparison as the basis for a similarity measure, and show why this approach is conceptually problematic. As an alternative, we explore the possibility of using clustering methods. This paper serves to (i) inform the community about the challenges that arise when using KG embeddings for the comparison of different versions of a KG specifically, (ii) investigate how this is supported by theories on knowledge representation and semantic representation in NLP and (iii) take the first steps into the direction of valuable representation of semantics within KGs for comparison.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Verkijk, Stella and Roothaert, Ritten and Pernisch, Romana and Schlobach, Stefan},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {Semantics, NLP, Concept Shift, Knowledge Graph Embeddings},
	pages = {70--74},
}

@inproceedings{reynolds_towards_2020,
	address = {New York, NY, USA},
	series = {{SAM} '20},
	title = {Towards automated provenance collection for runtime models to record system history},
	isbn = {978-1-4503-8140-6},
	url = {https://doi.org/10.1145/3419804.3420262},
	doi = {10.1145/3419804.3420262},
	abstract = {In highly dynamic environments, systems are expected to make decisions on the fly based on their observations that are bound to be partial. As such, the reasons for its runtime behaviour may be difficult to understand. In these cases, accountability is crucial, and decisions by the system need to be traceable. Logging is essential to support explanations of behaviour, but it poses challenges. Concerns about analysing massive logs have motivated the introduction of structured logging, however, knowing what to log and which details to include is still a challenge. Structured logs still do not necessarily relate events to each other, or indicate time intervals. We argue that logging changes to a runtime model in a provenance graph can mitigate some of these problems. The runtime model keeps only relevant details, therefore reducing the volume of the logs, while the provenance graph records causal connections between the changes and the activities performed by the agents in the system that have introduced them. In this paper, we demonstrate a first version towards a reusable infrastructure for the automated construction of such a provenance graph. We apply it to a multithreaded traffic simulation case study, with multiple concurrent agents managing different parts of the simulation. We show how the provenance graphs can support validating the system behaviour, and how a seeded fault is reflected in the provenance graphs.},
	booktitle = {Proceedings of the 12th {System} {Analysis} and {Modelling} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Reynolds, Owen and García-Domínguez, Antonio and Bencomo, Nelly},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {Provenance, self-explanation, multi-threading, runtime models, self-adaptation},
	pages = {12--21},
}

@article{stunkel_comprehensive_2021,
	title = {Comprehensive {Systems}: {A} formal foundation for {Multi}-{Model} {Consistency} {Management}},
	volume = {33},
	issn = {0934-5043},
	url = {https://doi.org/10.1007/s00165-021-00555-2},
	doi = {10.1007/s00165-021-00555-2},
	abstract = {Model management is a central activity in Software Engineering. The most challenging aspect of model management is to keep inter-related models consistent with each other while they evolve. As a consequence, there is a lot of scientific activity in this area, which has produced an extensive body of knowledge, methods, results and tools. The majority of these approaches, however, are limited to binary inter-model relations; i.e. the synchronisation of exactly two models. Yet, not every multi-ary relation can be factored into a family of binary relations. In this paper, we propose and investigate a novel comprehensive system construction, which is able to represent multi-ary relations among multiple models in an integrated manner and thus serves as a formal foundation for artefacts used in consistency management activities involving multiple models. The construction is based on the definition of partial commonalities among a set of models using the same language, which is used to denote the (local) models. The main theoretical results of this paper are proofs of the facts that comprehensive systems are an admissible environment for (i) applying formal means of consistency verification (diagrammatic predicate framework), (ii) performing algebraic graph transformation (weak adhesive HLR category), and (iii) that they generalise the underlying setting of graph diagrams and triple graph grammars.},
	number = {6},
	journal = {Form. Asp. Comput.},
	author = {Stünkel, Patrick and König, Harald and Lamo, Yngve and Rutle, Adrian},
	month = dec,
	year = {2021},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Category theory, Adhesive categories, Consistency restoration, Consistency verification, Graph diagrams, Inter-model consistency, Model merging, Model synchronisation, Model weaving, Multi-directional transformations (MX), Multi-modelling, Triple graph grammars},
	pages = {1067--1114},
}

@inproceedings{zhao_developing_2025,
	address = {New York, NY, USA},
	series = {{ICAIE} '24},
	title = {Developing a {Knowledge} {Graph} for {Intelligent} {Structural} {Design} {Course}},
	isbn = {979-8-4007-1269-2},
	url = {https://doi.org/10.1145/3722237.3722259},
	doi = {10.1145/3722237.3722259},
	abstract = {Knowledge graphs provide concept visualization and context information across many applications. However, the process of building a knowledge graph by transforming large and intricate unstructured data into a new domain presents numerous challenges. Learning from an Intelligent Structural Design (ISD) course requires comprehension of structural concepts, design techniques, AI technologies, and the tackling of multifaceted real-project problems, which require adaptive learning strategies and cognitive engagement. This study introduces a bottom-up approach to constructing a knowledge graph specifically designed for the course. Furthermore, by employing a deep learning model, a personalized learning path could be generated based on a student's submitted assignments. To evaluate the effectiveness of this novel educational paradigm, an experiment was conducted involving two groups. The results indicate that participants who used the Course Knowledge Graph (CKG) attained higher scores than those instructed through conventional teaching methods.},
	booktitle = {Proceedings of the 2024 3rd {International} {Conference} on {Artificial} {Intelligence} and {Education}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Linlin and Liu, Zhansheng and Zhao, Xuefeng},
	year = {2025},
	keywords = {ontology, natural language processing (NLP), Course Knowledge Graph, Intelligent Structural Design, Smart Construction},
	pages = {123--128},
}

@inproceedings{almeida_preface_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {Preface to the 6th international workshop on multi-level modelling ({MULTI} 2019)},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00015},
	doi = {10.1109/MODELS-C.2019.00015},
	abstract = {Multi-level modeling (MLM) represents a significant extension to the traditional two-level object-oriented paradigm with the potential to dramatically improve upon the utility, reliability and complexity of models. Different from conventional approaches, they allow for an arbitrary number of classification levels and introduce other concepts that foster expressiveness, reuse and adaptability. A key aspect of the MLM paradigm is the use of entities that are simultaneously types and instances, a feature which has consequences for conceptual modeling, language engineering and for the development of model-based software systems.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Almeida, João Paulo A. and Rutle, Adrian and Wimmer, Manuel},
	year = {2021},
	pages = {64--65},
}

@inproceedings{flisar_document_2018,
	address = {New York, NY, USA},
	series = {{WIMS} '18},
	title = {Document {Enrichment} using {DBPedia} {Ontology} for {Short} {Text} {Classification}},
	isbn = {978-1-4503-5489-9},
	url = {https://doi.org/10.1145/3227609.3227649},
	doi = {10.1145/3227609.3227649},
	abstract = {Every day, millions of short-texts are generated for which effective tools for organization and retrieval are required. Because of the short length of these documents and of their extremely sparse representations, the traditional text classification methods are not effective. We propose a new approach that uses DBpedia Spotlight annotation tools, to identify relevant entities in text and enrich short text documents with concepts derived from those entities, represented in DBpedia ontology. Our experiments show that the proposed document enrichment approach is beneficial for classification of short texts, and is robust with respect to concept drifts and input sources. We report experimental results in three challenging collections, using a variety of classification methods. The results show that the use of DBpedia ontology significantly improves the classification performance of classifiers in short-text classification.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Web} {Intelligence}, {Mining} and {Semantics}},
	publisher = {Association for Computing Machinery},
	author = {Flisar, Jernej and Podgorelec, Vili},
	year = {2018},
	note = {event-place: Novi Sad, Serbia},
	keywords = {ontology, DBPedia, short text classification, text enrichment},
}

@article{gottlob_stable_2021,
	title = {Stable {Model} {Semantics} for {Guarded} {Existential} {Rules} and {Description} {Logics}: {Decidability} and {Complexity}},
	volume = {68},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/3447508},
	doi = {10.1145/3447508},
	abstract = {This work investigates the decidability and complexity of database query answering under guarded existential rules with nonmonotonic negation according to the classical stable model semantics. In this setting, existential quantification is interpreted via Skolem functions, and the unique name assumption is adopted. As a first result, we show the decidability of answering first-order queries based on such rules by a translation into the satisfiability problem for guarded second-order formulas having the tree-model property. To obtain precise complexity results for unions of conjunctive queries, we transform the original problem in polynomial time into an intermediate problem that is easier to analyze: query answering for guarded disjunctive existential rules with stratified negation. We obtain precise bounds for the general setting and for various restricted settings. We also consider extensions of the original formalism with negative constraints, keys, and the possibility of negated atoms in queries. Finally, we show how the above results can be used to provide decidability and complexity results for a natural adaptation of the stable model semantics to description logics such as\&nbsp;ELHI and the DL-Lite\&nbsp;family.},
	number = {5},
	journal = {J. ACM},
	author = {Gottlob, Georg and Hernich, André and Kupke, Clemens and Lukasiewicz, Thomas},
	month = oct,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Answer set programming, complexity, decidability, stable models},
}

@inproceedings{doynikova_ontology_2019,
	address = {New York, NY, USA},
	series = {{ARES} '19},
	title = {Ontology of {Metrics} for {Cyber} {Security} {Assessment}},
	isbn = {978-1-4503-7164-3},
	url = {https://doi.org/10.1145/3339252.3341496},
	doi = {10.1145/3339252.3341496},
	abstract = {Development of metrics that are valuable for assessing security and decision making is an important element of efficient counteraction to cyber threats. The paper proposes an ontology of metrics for cyber security assessment. The developed ontology is based on determining the concepts and relations between primary features of initial security data and forming a set of hierarchically interconnected security metrics. The paper describes the main classes of the proposed ontology, the revealed relations, the involved security metrics, and the used data sources. The publicly available sources of security data are analyzed to get primary security metrics. Application of the approach is shown on a case study. The main feature of the proposed ontology is representation of security metrics as separate instances of ontology. It allows using the relations between the concepts of ontology for calculating integral metrics reflecting the security state.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Doynikova, Elena and Fedorchenko, Andrey and Kotenko, Igor},
	year = {2019},
	note = {event-place: Canterbury, CA, United Kingdom},
	keywords = {ontology, semantics, countering cyber attacks, intelligent data analysis, security assessment, Security metrics},
}

@inproceedings{tsoutsa_towards_2020,
	address = {New York, NY, USA},
	series = {{ICACS} '20},
	title = {Towards an {Ontology} for {Teamwork} {Enabled} {Services}},
	isbn = {978-1-4503-7732-4},
	url = {https://doi.org/10.1145/3423390.3423395},
	doi = {10.1145/3423390.3423395},
	abstract = {Teamwork ability is a crucial aspect of humans, agents and other intelligent systems. This study is dedicated to the conceptual formal modeling of service teamworking by considering concepts defined within a vast range of models and theories of human and agent team development. We make the conceptual based modeling by using ontologies to define teams, roles, services and their modeling domains abstracting the most important concepts of team development whilst providing a decent level of formality and unambiguity. An ontology named TrEWSOnto is proposed as a model to assist the development process of teamwork enabled services in tandem with the representation of their composition process activity. This includes (i) the definition of concepts that are used to improve the teamwork ability of peer services, (ii) the description of the main concepts are used for the role modeling composition, and (iii) the description of situations that teamwork roles should monitor to catch potential "unhealthy" behavior happen during the service activity in order to run proactively and avoid obstacles or collisions. The result is a structure of five ontology sections together forming a representation for TeamwoRk Enabled Web (TrEW) Services and the environment they live.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Algorithms}, {Computing} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tsoutsa, Paraskevi and Ragos, Omiros},
	year = {2020},
	note = {event-place: Rabat, Morocco},
	keywords = {Microservices, Teamwork, Composition, Web Service, Teamwork Ontology},
	pages = {69--75},
}

@inproceedings{dassereto_how_2020,
	address = {New York, NY, USA},
	series = {{LocalRec}'20},
	title = {How to {Tune} {Parameters} in {Geographical} {Ontologies} {Embedding}},
	isbn = {978-1-4503-8160-4},
	url = {https://doi.org/10.1145/3423334.3431448},
	doi = {10.1145/3423334.3431448},
	abstract = {Many Natural Language Processing (NLP) tasks, like question answering or analyzing verbatim comments, have started to use word embeddings due to their ability to capture semantic relations between words. Recently, embeddings have been also applied in the geospatial context to represent geospatial ontologies, thanks to their ability to capture semantic similarity. In this paper, we present an analysis of a promising embedding technique particularly suitable for representing hierarchical structures. We conduct a deep technical evaluation of many parameters and their impact on the quality of the representation.},
	booktitle = {Proceedings of the 4th {ACM} {SIGSPATIAL} {Workshop} on {Location}-{Based} {Recommendations}, {Geosocial} {Networks}, and {Geoadvertising}},
	publisher = {Association for Computing Machinery},
	author = {Dassereto, Federico and Rocco, Laura Di and Shaw, Shanley and Guerrini, Giovanna and Bertolotto, Michela},
	year = {2020},
	note = {event-place: Seattle, WA, USA},
	keywords = {Embeddings, Knowledge Bases, Geographic Information Retrieval, Geotagging},
}

@inproceedings{jia_agentir_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {{AgentIR}: 2nd {Workshop} on {Agent}-based {Information} {Retrieval}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3730365},
	doi = {10.1145/3726302.3730365},
	abstract = {Information retrieval (IR) systems are essential in modern society, aiding users to efficiently locate relevant information through query expansion, document retrieval, ranking, and re-ranking. User feedback from ranked outputs forms a dynamic interaction loop with IR systems, which can be modeled as either one-time or sequential decision-making problems. Over the past decade, deep reinforcement learning (DRL) has emerged as a promising approach to decision-making, leveraging the high model capacity of deep learning for complex tasks. While significant research has explored the application of DRL to IR tasks, several fundamental challenges remain underexplored, including the underlying information theory in DRL settings, the limitations of reinforcement learning methods for industrial IR applications, and the simulation of DRL-based IR systems. Concurrently, the advent of large language models (LLMs) has introduced new opportunities for optimizing and simulating IR systems. Building on the success of the Agent-based IR Workshop at SIGIR 2024, we propose hosting the second Agent-based IR Workshop at SIGIR 2025. This workshop will continue to provide a platform for researchers and practitioners from academia and industry to present cutting-edge advances in DRL-based and LLM-based IR systems from an agent-based perspective. By building on the foundation laid in the first workshop, the 2025 edition aims to delve deeper into emerging research challenges, foster collaborations, and explore innovative applications. Through engaging discussions and insightful presentations, the workshop seeks to further expand the boundaries of IR research and solidify its role as a premier venue for advancing agent-based IR systems.},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Jia, Pengyue and Cai, Qingpeng and Zhao, Xiangyu and Pan, Ling and Xin, Xin and Huang, Jin and Zhang, Weinan and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {llm, agent-based information retrieval, drl},
	pages = {4180--4183},
}

@inproceedings{antunes_eeg_2021,
	address = {New York, NY, USA},
	series = {{IVA} '21},
	title = {{EEG} {Model}: {Emotional} {Episode} {Generation} for {Social} {Sharing} of {Emotions}},
	isbn = {978-1-4503-8619-7},
	url = {https://doi.org/10.1145/3472306.3478342},
	doi = {10.1145/3472306.3478342},
	abstract = {Social sharing of emotions (SSE) occurs when one communicates their feelings and reactions to a certain event in the course of a social interaction. The phenomenon is part of our social fabric and plays an important role in creating empathetic responses and establishing rapport. Intelligent social agents capable of SSE will have a mechanism to create and build long-term interaction with humans. In this paper, we present the Emotional Episode Generation (EEG) model, a fine-tuned GPT-2 model capable of generating emotional social talk regarding multiple event tuples in a human-like manner. Human evaluation results show that the model successfully translates one or more event-tuples into emotional episodes, reaching quality levels close to human performance. Furthermore, the model clearly expresses one emotion in each episode as well as humans. To train this model we used a public dataset and built upon it using event extraction techniques1.},
	booktitle = {Proceedings of the 21st {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author = {Antunes, Ana and Campos, Joana and Dias, João and Santos, Pedro A. and Prada, Rui},
	year = {2021},
	note = {event-place: Virtual Event, Japan},
	keywords = {emotional text generation, event-to-text generation, social agents},
	pages = {1--8},
}

@inproceedings{mondal_modelling_2022,
	address = {New York, NY, USA},
	series = {{iiWAS2021}},
	title = {Modelling {IoT} {Application} {Requirements} for {Benchmarking} {IoT} {Middleware} {Platforms}},
	isbn = {978-1-4503-9556-4},
	url = {https://doi.org/10.1145/3487664.3487742},
	doi = {10.1145/3487664.3487742},
	abstract = {The significant advances in the Internet of Things (IoT) have led to IoT applications being widely used in various scenarios ranging from smart city, smart farming, to Industrial IoT (IIoT) solutions. With the explosion of IoT application development, IoT middleware platforms are increasingly being used for hosting such IoT applications. This has given rise to the need for developing benchmarking solutions to analyze and test the performance of different middleware platforms that host these IoT applications. To develop such benchmarks, there are a number of key components that are needed. One of these components is an IoT dataset. To generate such datasets, representing IoT application requirements in a general and formal way is important. In this paper, we propose a framework to model the IoT Applications Requirements and enable Data Generation(ARDG-IoT). The framework supports a formal way to capture IoT application requirements and use these requirements to generate IoT data that can be used to create benchmarks for different IoT middleware platforms. ARDG-IoT consists of our proposed model, IoTSySML, which captures the application requirements, and an IoT data simulator tool, which is used to generate IoT data. We present an evaluation of the framework using a real world Industrial IoT application case study.},
	booktitle = {The 23rd {International} {Conference} on {Information} {Integration} and {Web} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Mondal, Shalmoly and Hassani, Alireza and Jayaraman, Prem Prakash and Delir Haghighi, Pari and Georgakopoulos, Dimitrios},
	year = {2022},
	note = {event-place: Linz, Austria},
	keywords = {Benchmarking, Requirements Engineering, IoT Application Requirements, IoT Middleware, Requirements Modelling},
	pages = {553--561},
}

@article{amaral_assessing_2021,
	title = {Assessing the {Quality} of {Sources} in {Wikidata} {Across} {Languages}: {A} {Hybrid} {Approach}},
	volume = {13},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3484828},
	doi = {10.1145/3484828},
	abstract = {Wikidata is one of the most important sources of structured data on the web, built by a worldwide community of volunteers. As a secondary source, its contents must be backed by credible references; this is particularly important, as Wikidata explicitly encourages editors to add claims for which there is no broad consensus, as long as they are corroborated by references. Nevertheless, despite this essential link between content and references, Wikidata's ability to systematically assess and assure the quality of its references remains limited. To this end, we carry out a mixed-methods study to determine the relevance, ease of access, and authoritativeness of Wikidata references, at scale and in different languages, using online crowdsourcing, descriptive statistics, and machine learning. Building on previous work of ours, we run a series of microtasks experiments to evaluate a large corpus of references, sampled from Wikidata triples with labels in several languages. We use a consolidated, curated version of the crowdsourced assessments to train several machine learning models to scale up the analysis to the whole of Wikidata. The findings help us ascertain the quality of references in Wikidata and identify common challenges in defining and capturing the quality of user-generated multilingual structured data on the web. We also discuss ongoing editorial practices, which could encourage the use of higher-quality references in a more immediate way. All data and code used in the study are available on GitHub for feedback and further improvement and deployment by the research community.},
	number = {4},
	journal = {J. Data and Information Quality},
	author = {Amaral, Gabriel and Piscopo, Alessandro and Kaffee, Lucie-aimée and Rodrigues, Odinaldo and Simperl, Elena},
	month = oct,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {knowledge graphs, Wikidata, data quality, crowdsourcing, verifiability},
}

@inproceedings{dew_skill_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {A {Skill} {Proficiency} {Framework} for {Workforce} {Learning} and {Development}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679228},
	doi = {10.1145/3627673.3679228},
	abstract = {Understanding the skills and proficiency levels required for various roles is crucial for effective workforce planning, learning and development. In this paper, we propose a robust skill proficiency modeling framework that offers a structured method to help describe, assess and develop proficiency in key skills, facilitating individuals' career pathways and aiding organizations in talent management and adaptability. We first design a skill proficiency description pipeline, which generates statements describing the requirements at each proficiency level of a skill. Following this, we build a skill proficiency by occupation model using large-scale job ad data to help organizations and individuals understand the skill proficiency requirements for different roles. Finally,we design a visual analytics system, based on a real-world career pathway scenario, to demonstrate the practical usefulness and effectiveness of our framework. A demo video is available at www.dropbox.com/scl/fi/nd0f3vi03n12g4y0sluaw/cikm24\_demo.mp4?rlkey=55vya144q5ftai1uqqaubr5u5.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Dew, Rebecca and Li, Mingzhao and Baratha Raj, Sandya},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {large language model, GPT, visual analytics, skill proficiency},
	pages = {5210--5214},
}

@inproceedings{abdullahi_k-paths_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {K-{Paths}: {Reasoning} over {Graph} {Paths} for {Drug} {Repurposing} and {Drug} {Interaction} {Prediction}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3737011},
	doi = {10.1145/3711896.3737011},
	abstract = {Biomedical knowledge graphs (KGs) encode rich, structured information critical for drug discovery tasks, but extracting meaningful insights from large-scale KGs remains challenging due to their complex structure. Existing biomedical subgraph retrieval methods are tailored for graph neural networks (GNNs), limiting compatibility with other paradigms, including large language models (LLMs). We introduce K-Paths, a model-agnostic retrieval framework that extracts structured, diverse, and biologically meaningful multi-hop paths from dense biomedical KGs. These paths enable prediction of unobserved drug-drug and drug-disease interactions, including those involving entities not seen during training, thus supporting inductive reasoning. K-Paths is training-free and employs a diversity-aware adaptation of Yen's algorithm to extract the K shortest loopless paths between entities in a query, prioritizing biologically relevant and relationally diverse connections. These paths serve as concise, interpretable reasoning chains that can be directly integrated with LLMs or GNNs to improve generalization, accuracy, and enable explainable inference. Experiments on benchmark datasets show that K-Paths improves zero-shot reasoning across state-of-the-art LLMs. For instance, Tx-Gemma 27B improves by 19.8 and 4.0 F1 points on interaction severity prediction and drug repurposing tasks, respectively. Llama 70B achieves gains of 8.5 and 6.2 points on the same tasks. K-Paths also boosts the training efficiency of EmerGNN, a state-of-the-art GNN, by reducing the KG size by 90\% while maintaining predictive performance. Beyond efficiency, K-Paths bridges the gap between KGs and LLMs, enabling scalable and explainable LLM-augmented scientific discovery. We release our code and the retrieved paths as a benchmark for inductive reasoning.},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Abdullahi, Tassallah and Gemou, Ioanna and Nayak, Nihal V. and Murtaza, Ghulam and Bach, Stephen H. and Eickhoff, Carsten and Singh, Ritambhara},
	year = {2025},
	note = {event-place: Toronto ON, Canada},
	keywords = {drug discovery, explainability, llms, gnns, inductive reasoning, knowledge graph reasoning},
	pages = {5--16},
}

@inproceedings{fan_ontology_2018,
	address = {New York, NY, USA},
	series = {{DSMM}'18},
	title = {An {Ontology} of {Ownership} and {Control} {Relations} for {Bank} {Holding} {Companies}},
	isbn = {978-1-4503-5883-5},
	url = {https://doi.org/10.1145/3220547.3220551},
	doi = {10.1145/3220547.3220551},
	abstract = {We consider the challenges and benefits of ontologies for information management for regulatory reporting from bank holding companies (BHCs). Many BHCs, especially the largest and most complex firms, have multiple federal supervisors who oversee a diverse array of subsidiaries. This creates a federated data management problem that disperses information across many firms and regulators. We prototype an ontology for the Federal Reserve's public National Information Center (NIC) database. The NIC identifies all BHCs, their subsidiaries, and the ownership and control relationships among them. It is a basic official source on the structure of the industry. A formal ontology can capture this expert-curated knowledge in a coherent, structured format. This could assure data integrity and enable non-experts to more readily integrate and analyze data about complex organizations. We test the design and development of federated prototype ontologies in OWL/RDF to provide and integrate the NIC data with precise semantics for transparency and consistency. Our preliminary results indicate that this is feasible in practice for data search and analysis, and that the ontologies can facilitate semantic integration and improve the integrity of data and metadata.},
	booktitle = {Proceedings of the {Fourth} {International} {Workshop} on {Data} {Science} for {Macro}-{Modeling} with {Financial} and {Economic} {Datasets}},
	publisher = {Association for Computing Machinery},
	author = {Fan, Liju and Flood, Mark D.},
	year = {2018},
	note = {event-place: Houston, TX, USA},
	keywords = {ontologies, knowledge representation, data integration, bank holding companies, data integrity, Financial regulation},
}

@inproceedings{meskelundefined_aldona_2019,
	address = {New York, NY, USA},
	series = {{SAC} '19},
	title = {{ALDONA}: a hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalised domain ontology and a neural attention model},
	isbn = {978-1-4503-5933-7},
	url = {https://doi.org/10.1145/3297280.3297525},
	doi = {10.1145/3297280.3297525},
	abstract = {Sentences containing several different polarity aspects cause one of the main problems in sentiment analysis. Depending on an aspect, the same context words can have different effects on its sentiment value. Additionally, the polarity can be influenced by the domain-specific knowledge, showing the necessity to incorporate it into the sentiment classification. In this paper we present a hybrid solution for sentence-level aspect-based sentiment analysis using A Lexicalised Domain Ontology and Neural Attention (ALDONA) model to handle the problems mentioned above. To measure the influence of each word in a given sentence on an aspect's polarity, we introduce the bidirectional context attention mechanism. Moreover, the classification module is designed to handle the sentence's complex structure. Finally, the manually created lexicalised domain ontology (represented in OWL) is integrated to exploit the field-specific knowledge. Computational results obtained on a benchmark data set based on Web reviews have shown ALDONA's ability to outperform several state-of-the-art models and stress its contribution to aspect-based sentiment classification.},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Meškelundefined, Donatas and Frasincar, Flavius},
	year = {2019},
	note = {event-place: Limassol, Cyprus},
	keywords = {aspect-based sentiment classification, bidirectional gated neural network, hybrid model, lexicalised domain ontology},
	pages = {2489--2496},
}

@inproceedings{wang_interpretable_2022,
	address = {New York, NY, USA},
	series = {{CIKM} '22},
	title = {Interpretable {Emotion} {Analysis} {Based} on {Knowledge} {Graph} and {OCC} {Model}},
	isbn = {978-1-4503-9236-5},
	url = {https://doi.org/10.1145/3511808.3557365},
	doi = {10.1145/3511808.3557365},
	abstract = {Sentiment analysis or opinion mining has been significant for information extraction from the text. At the same time, emotion psychology also proposed many appraisal theories for emotional evaluations and concrete predictions. While sentiment analysis focuses on identifying the polarity, appraisal theories of emotion can define different emotions and view emotions as process rather than states. In real life, the mechanism of emotional generations and interactions is complicated. Only plausible polarity can't provide enough explanations for the emotional mechanism. Hence an explainable model is in demand during emotion inference and dynamical analysis. In this paper, an analysis framework is constructed for interpreting casual association based on the emotional logic. Knowledge graph is introduced into the appraisal theories for inferring the emotions and predicting the action tendency. The emotion knowledge graph levels: concept level and case level. The concept level can be built manually as an abstract based on the appraisal model of Ortony, Clore \&amp; Collins (OCC model). The inference and predictions can be implemented at this level. The case level includes entities, objects, events and cognitive relations between them that extract from the text through the modular functions. The elements in the case level can be linked to the abstract types in the concept level for the emotional inference. We test this emotional analysis framework on several datasets from the appraisal theory and the text of drama works. The results demonstrate that our framework can make better inferences on emotions and good interpretability for human beings.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Shuo and Zhang, Yifei and Lin, Bochen and Li, Boxun},
	year = {2022},
	note = {event-place: Atlanta, GA, USA},
	keywords = {knowledge graph, analysis framework, appraisal theories, emotional logic, occ model},
	pages = {2038--2045},
}

@inproceedings{anuradha_nanomi_arachchige_enhancing_2023,
	address = {New York, NY, USA},
	series = {{HIP} '23},
	title = {Enhancing {Named} {Entity} {Recognition} for {Holocaust} {Testimonies} through {Pseudo} {Labelling} and {Transformer}-based {Models}},
	isbn = {979-8-4007-0841-1},
	url = {https://doi.org/10.1145/3604951.3605514},
	doi = {10.1145/3604951.3605514},
	abstract = {The Holocaust was a tragic and catastrophic event in World War II (WWII) history that resulted in the loss of millions of lives. In recent years, the emergence of the field of digital humanities has made the study of Holocaust testimonies an important area of research for historians, Holocaust educators, social scientists, and linguists. One of the challenges in analysing Holocaust testimonies is the recognition and categorisation of named entities such as concentration camps, military officers, ships, and ghettos, due to the scarcity of annotated data. This paper presents a research study on a domain-specific hybrid named-entity recognition model, which focuses on developing NER models specifically tailored for the Holocaust domain. To overcome the problem of data scarcity, we employed hybrid annotation approach to training different transformer model architectures in order to recognise the named entities. Results show transformer models to have good performance compared to other approaches.},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Historical} {Document} {Imaging} and {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Anuradha Nanomi Arachchige, Isuri and Ha, Le and Mitkov, Ruslan and Steinert, Johannes-Dieter},
	year = {2023},
	note = {event-place: San Jose, CA, USA},
	keywords = {Transformers, NER, Holocaust Testimonies, Pseudo Labelling},
	pages = {85--90},
}

@article{casadei_macroprogramming_2023,
	title = {Macroprogramming: {Concepts}, {State} of the {Art}, and {Opportunities} of {Macroscopic} {Behaviour} {Modelling}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3579353},
	doi = {10.1145/3579353},
	abstract = {Macroprogramming refers to the theory and practice of expressing the macro(scopic) behaviour of a collective system using a single program. Macroprogramming approaches are motivated by the need of effectively capturing global/system-level aspects and the collective behaviour of multiple computational components, while abstracting over low-level details. Previously, this programming style had been primarily adopted to describe the data-processing logic in sensor networks; recently, research forums on spatial computing, collective systems, and the Internet of Things have provided renewed interest in macro approaches. However, related contributions are still fragmented and lack conceptual consistency. Therefore, to foster principled research, an integrated view of the field is provided, together with opportunities and challenges.},
	number = {13s},
	journal = {ACM Comput. Surv.},
	author = {Casadei, Roberto},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {collective intelligence, Macro programming, system-level programming},
}

@article{sen_athena_2020,
	title = {{ATHENA}++: natural language querying for complex nested {SQL} queries},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3407790.3407858},
	doi = {10.14778/3407790.3407858},
	abstract = {Natural Language Interfaces to Databases (NLIDB) systems eliminate the requirement for an end user to use complex query languages like SQL, by translating the input natural language (NL) queries to SQL automatically. Although a significant volume of research has focused on this space, most state-of-the-art systems can at best handle simple select-project-join queries. There has been little to no research on extending the capabilities of NLIDB systems to handle complex business intelligence (BI) queries that often involve nesting as well as aggregation. In this paper, we present Athena++, an end-to-end system that can answer such complex queries in natural language by translating them into nested SQL queries. In particular, Athena++ combines linguistic patterns from NL queries with deep domain reasoning using ontologies to enable nested query detection and generation. We also introduce a new benchmark data set (FIBEN), which consists of 300 NL queries, corresponding to 237 distinct complex SQL queries on a database with 152 tables, conforming to an ontology derived from standard financial ontologies (FIBO and FRO). We conducted extensive experiments comparing Athena++ with two state-of-the-art NLIDB systems, using both FIBEN and the prominent Spider benchmark. Athena++ consistently outperforms both systems across all benchmark data sets with a wide variety of complex queries, achieving 88.33\% accuracy on FIBEN benchmark, and 78.89\% accuracy on Spider benchmark, beating the best reported accuracy results on the dev set by 8\%.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Sen, Jaydeep and Lei, Chuan and Quamar, Abdul and Özcan, Fatma and Efthymiou, Vasilis and Dalmia, Ayushi and Stager, Greg and Mittal, Ashish and Saha, Diptikalyan and Sankaranarayanan, Karthik},
	month = jul,
	year = {2020},
	note = {Publisher: VLDB Endowment},
	pages = {2747--2759},
}

@inproceedings{kurte_semantics-enabled_2019,
	address = {New York, NY, USA},
	series = {{ARIC}'19},
	title = {Semantics-enabled {Spatio}-{Temporal} {Modeling} of {Earth} {Observation} {Data}: {An} application to {Flood} {Monitoring}},
	isbn = {978-1-4503-6954-1},
	url = {https://doi.org/10.1145/3356395.3365545},
	doi = {10.1145/3356395.3365545},
	abstract = {Extreme events such as urban floods are dynamic in nature, i.e. they evolve with time. The spatiotemporal analysis of such disastrous events is important for understanding the resiliency of an urban system during these events. Remote Sensing (RS) data is one of the crucial earth observation (EO) data sources that can facilitate such spatiotemporal analysis due to its wide spatial coverage and high temporal availability. In this paper, we propose a discrete mereotopology (DM) based approach to enable representation and querying of spatiotemporal information from a series of multitemporal RS images that are acquired during a flood disaster event. We represent this spatiotemporal information using a semantic model called Dynamic Flood Ontology (DFO). To establish the effectiveness and applicability of the proposed approach, spatiotemporal queries relevant during an urban flood scenario such as, show me road segments that were partially flooded during the time interval t1 have been demonstrated with promising results.},
	booktitle = {Proceedings of the 2nd {ACM} {SIGSPATIAL} {International} {Workshop} on {Advances} on {Resilient} and {Intelligent} {Cities}},
	publisher = {Association for Computing Machinery},
	author = {Kurte, Kuldeep and Potnis, Abhishek and Durbha, Surya},
	year = {2019},
	note = {event-place: Chicago, IL, USA},
	keywords = {ontology, semantics, discrete mereotopology, flood disaster, spatial relations, spatiotemporal},
	pages = {41--50},
}

@inproceedings{shaaban_ontology-based_2019,
	address = {New York, NY, USA},
	series = {{SPLC} '19},
	title = {Ontology-{Based} {Security} {Tool} for {Critical} {Cyber}-{Physical} {Systems}},
	isbn = {978-1-4503-6668-7},
	url = {https://doi.org/10.1145/3307630.3342397},
	doi = {10.1145/3307630.3342397},
	abstract = {Industry 4.0 considers as a new advancement concept of the industrial revolution, which introduces a full utilization of Internet technologies. This concept aims to combine diverse technological resources into the industry field, which enables the communication between two worlds: the physical and the cyber one. Cyber-physical Systems are one of the special forces that integrate and build a variety of existing technologies and components. The diversity of components and technologies creates new security threats that can exploit vulnerabilities to attack a critical system. This work introduces an ontology-based security tool-chain able to be integrated with the initial stages of the development process of critical systems. The tool detects the potential threats, and apply the suitable security requirements which can address these threats. Eventually, it uses the ontology approach to ensure that the security requirements are fulfilled.},
	booktitle = {Proceedings of the 23rd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} {B}},
	publisher = {Association for Computing Machinery},
	author = {Shaaban, Abdelkader Magdy and Gruber, Thomas and Schmittner, Christoph},
	year = {2019},
	note = {event-place: Paris, France},
	keywords = {ontology, security, cyber-physical system, threats},
	pages = {207--210},
}

@article{hasan_gene_2023,
	title = {Gene {Expression} and {Metadata} {Based} {Identification} of {Key} {Genes} for {Hepatocellular} {Carcinoma} {Using} {Machine} {Learning} and {Statistical} {Models}},
	volume = {20},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2023.3322753},
	doi = {10.1109/TCBB.2023.3322753},
	abstract = {Biomarkers associated with hepatocellular carcinoma (HCC) are of great importance to better understand biological response mechanisms to internal or external intervention. The study aimed to identify key candidate genes for HCC using machine learning (ML) and statistics-based bioinformatics models. Differentially expressed genes (DEGs) were identified using limma and then selected their common genes among DEGs identified from four datasets. After that, protein-protein interaction networks were constructed using STRING and then Cytoscape was used to determine hub genes, significant modules, and their associated genes. Simultaneously, three ML-based techniques such as support vector machine (SVM), least absolute shrinkage and selection operator-logistic regression (LASSO-LR), and partial least squares-discriminant analysis (PLS-DA) were implemented to determine the discriminative genes of HCC from common DEGs. Moreover, metadata of hub genes were formed by listing all hub genes from existing studies to incorporate other findings in our analysis. Finally, seven key candidate genes (ASPM, CCNB1, CDK1, DLGAP5, KIF20 A, MT1X, and TOP2A) were identified by intersecting common genes among hub genes, significant modules genes, discriminative genes from SVM, LASSO-LR, and PLS-DA, and meta hub genes from existing studies. Another three independent test datasets were also used to validate these seven key candidate genes using AUC, computed from ROC.},
	number = {6},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Hasan, Md. Al Mehedi and Maniruzzaman, Md. and Shin, Jungpil},
	month = oct,
	year = {2023},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {3786--3799},
}

@inproceedings{enea_how_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {How to {Support} {Human} {Operator} in "{Uncertainty}" {Managing} during the {Ontology} {Learning} {Process}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3191548},
	doi = {10.1145/3184558.3191548},
	abstract = {Creating ontologies is an essential while challenging task to be performed by either a human or a system: on one hand it is excessively burdensome for a human operator, on the other it is very complex also for a machine due to the not negligible amount of "uncertainty" that it must be able to manage. In the last years, some attempts have been made to automate this process, but at present, due to the large number of aspects to be covered in the automatic creation of an ontology (such as Domain terminology extraction, Concept discovery, Concept hierarchy derivation, ") satisfactory solutions have not been reached yet. In order to produce efficient tools for both creation and enrichment of ontologies, the participation of the human in such a process still seems necessary. Our approach, that foresees a broader framework for ontology learning, is based by first on the automatic extraction of triples from heterogeneous sources, then on the presentation of the most reliable triples to the human operator for validation purposes. The system provides the user with a series of graphical representations that can give him an overview of the level of uncertainty of the automatically generated ontology. Then provides the user with the possibility to perform SPARQL what-if queries, (i.e. assuming as true the triples filtered according to the level of confidence, the source and the structure of the triples).},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Enea, Roberto and Pazienza, Maria Teresa and Turbati, Andrea and Colantonio, Alessandro},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {ontology learning, human computer interaction, reasoning on uncertain knowledge},
	pages = {1147--1154},
}

@inproceedings{prinz_abstraction_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {On abstraction in the {OMG} hierarchy: systems, models, and descriptions},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561573},
	doi = {10.1145/3550356.3561573},
	abstract = {The Model-Driven Architecture (MDA) uses a metadata hierarchy with several layers that are placed on top of each other. The traditional view is that the layers provide abstractions related to models in languages defined by meta-models. Over the years, it has been difficult to define a consistent understanding of the layers. In this paper, we propose such a consistent understanding by clarifying the relations between the different elements in the hierarchy. This is done based on the Scandinavian approach to modelling that distinguishes between systems and system descriptions. Systems can be physical, digital, or even mental, while descriptions can be programs, language descriptions, specifications, and diagrams. We relate descriptions and systems by explaining where semantics of objects originate and how they apply in the hierarchy.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Prinz, Andreas and Xanthopoulou, Themis Dimitra and Gjøsæter, Terje and Møller-Pedersen, Birger},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {semantics, model, abstraction, instantiation, description, system},
	pages = {322--330},
}

@inproceedings{kechagioglou_sharing_2019,
	address = {New York, NY, USA},
	series = {{ICGDA} '19},
	title = {Sharing {Geoprocessing} {Workflows} with {Business} {Process} {Model} and {Notation} ({BPMN})},
	isbn = {978-1-4503-6245-0},
	url = {https://doi.org/10.1145/3318236.3318239},
	doi = {10.1145/3318236.3318239},
	abstract = {Graphical geoprocessing workflows are often built visually on interactive canvases of GIS software. Such workflows cannot be shared among different software, due to structural and semantical differences. This study experiments with a workflow created for ILWIS software and transforms it into a BPMN process model, exploiting XML serialisations of the two workflows. Ultimately, it aims at contributing to interoperability of geoprocessing workflows, through an extended approach serving as a frame around workflow conversion.},
	booktitle = {Proceedings of the 2019 2nd {International} {Conference} on {Geoinformatics} and {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Kechagioglou, Xeni and Lemmens, Rob and Retsios, Vasilios},
	year = {2019},
	note = {event-place: Prague, Czech Republic},
	keywords = {Interoperability, Workflow, Business Process Model and Notation (BPMN), eXtensible Stylesheet Language Transformations (XSLT), Geoinformatics, Geoprocessing, ILWIS},
	pages = {56--60},
}

@inproceedings{barret_abstra_2022,
	address = {New York, NY, USA},
	series = {{CIKM} '22},
	title = {Abstra: {Toward} {Generic} {Abstractions} for {Data} of {Any} {Model}},
	isbn = {978-1-4503-9236-5},
	url = {https://doi.org/10.1145/3511808.3557179},
	doi = {10.1145/3511808.3557179},
	abstract = {Digital data sharing leads to unprecedented opportunities to develop data-driven systems for supporting economic activities, the social and political life, and science. Many open-access datasets are RDF (Linked Data) graphs, but others are JSON or XML documents, CSV files, Neo4J property graphs, etc.Potential users need to understand a dataset in order to decide if it is useful for their goal. While some published datasets come with a schema and/or documentation, this is not always the case.We demonstrate Abstra, a dataset abstraction system, which applies on a large variety of data models. Abstra computes a description meant for humans, and integrates Information Extraction to classify dataset content among a set of categories of interest to the user. Our abstractions are conceptually close to Entity-Relationship diagrams, but our entities can have deeply nested structure.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Barret, Nelly and Manolescu, Ioana and Upadhyay, Prajna},
	year = {2022},
	note = {event-place: Atlanta, GA, USA},
	keywords = {data integration, dataset discovery, E-R schema},
	pages = {4803--4807},
}

@inproceedings{peng_data_2024,
	address = {New York, NY, USA},
	series = {{AIPR} '23},
	title = {Data fusing driven graph embedding model for multi-source heterogeneous knowledge graph},
	isbn = {979-8-4007-0767-4},
	url = {https://doi.org/10.1145/3641584.3641803},
	doi = {10.1145/3641584.3641803},
	abstract = {Graph embedding has been suggested as an efficient approach for discovering substantial and valuable knowledge in knowledge graphs. However, due to the vast amount and complexity of knowledge graphs, it is challenging to obtain reliable embeddings, which poses challenges in scalability and property selection. Previous research has given little attention to these challenges. In addressing this issue, a knowledge graph embedding model known as the data fusing driven graph embedding model (DFGE) is proposed, which comprises three modules: node embedding module, attribute embedding module, and merge module. Within DFGE, node attribute information is filtered and combined with graph structure information to calculate embeddings. Furthermore, multi-source heterogeneous data fusion technology (MHDF) is also proposed within this model to capture attribute features, embed and fuse nodes' attributes, and enhance DFGE's effectiveness. Extensive experiments are conducted on both Cora and a new proposed Buildings Seismic Damage dataset. Results confirm that DFGE delivers significant performance improvements over state-of-the-art models.},
	booktitle = {Proceedings of the 2023 6th {International} {Conference} on {Artificial} {Intelligence} and {Pattern} {Recognition}},
	publisher = {Association for Computing Machinery},
	author = {Peng, Xiangyu and Zhang, Yu and Hu, Wang},
	year = {2024},
	note = {event-place: Xiamen, China},
	keywords = {knowledge graph, heterogeneous data, graph embedding},
	pages = {1452--1457},
}

@inproceedings{jimenez-macias_model_2021,
	address = {New York, NY, USA},
	series = {{TEEM}'21},
	title = {A model to characterize exercises using probabilistic methods},
	isbn = {978-1-4503-9066-8},
	url = {https://doi.org/10.1145/3486011.3486523},
	doi = {10.1145/3486011.3486523},
	abstract = {Many studies have been conducted on modeling learners in education using probabilistic methods to infer different indicators. However, little research has been done on modeling content in education by estimating different content characteristics using probabilistic methods. Based on the existing gap in this area, this paper presents a model for exercises using probabilistic methods with interactions carried out by students to obtain content characteristics and their relationship such as the number of attempts, grade, and time spent related with student efficiency. Simulations were performed to train the model and find the different curves that best characterize the exercises based on programmatically generated interactions. This model allows the teacher to redesign the exercises to improve the student’s learning process.},
	booktitle = {Ninth {International} {Conference} on {Technological} {Ecosystems} for {Enhancing} {Multiculturality} ({TEEM}'21)},
	publisher = {Association for Computing Machinery},
	author = {Jiménez-Macías, Alberto and Muñoz-Merino, Pedro J. and Delgado Kloos, Carlos},
	year = {2021},
	note = {event-place: Barcelona, Spain},
	keywords = {learning analytics, content modeling, smart content, smart learning environments},
	pages = {594--599},
}

@inproceedings{chakraborty_ontoconnect_2021,
	address = {New York, NY, USA},
	series = {{SAC} '21},
	title = {{OntoConnect}: unsupervised ontology alignment with recursive neural network},
	isbn = {978-1-4503-8104-8},
	url = {https://doi.org/10.1145/3412841.3442059},
	doi = {10.1145/3412841.3442059},
	abstract = {Ontology alignment is performed to combine or integrate multiple knowledge bases at the elemental and structural levels. The current state-of-the-art systems use many different approaches to match semantics, syntax, and terminologies of different ontological entities. However, most of the ontology alignment systems depend on domain knowledge, which makes the alignment process domain-specific. To address this challenge, we aim at developing an ontology alignment approach that is independent of domain knowledge. To achieve this goal, an ontology alignment approach is proposed which exploits an unsupervised learning method using a recursive neural network to align classes between different ontologies. In particular, the proposed approach extracts structural information of the classes in ontology to train the unsupervised model. The proposed approach is tested against a reference gold copy of the Anatomy data set in the Ontology Alignment Evaluation Initiative. Our evaluation results show that the proposed unsupervised neural network approach using the meta information of ontological classes yields satisfactory results with a precision of 95.66\% and F-measure of 80.26\% for a similarity threshold of 0.96 with the 100-dimension input vector. Increasing the input vector dimension to 300 results in improved precision of 97.71\% and F-measure of 80.38\% with a 0.96 threshold. The significance of the proposed approach is that it can be used for ontology alignment independent of domain expertise and without the need for human intervention.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Chakraborty, Jaydeep and Bansal, Srividya K. and Virgili, Luca and Konar, Krishanu and Yaman, Beyza},
	year = {2021},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {LSTM, unsupervised learning, ontology schema alignment, ontology schema matching, recursive neural network},
	pages = {1874--1882},
}

@inproceedings{barcelo_when_2021,
	address = {Vancouver, Canada},
	series = {{LICS} '19},
	title = {When is ontology-mediated querying efficient?},
	abstract = {In ontology-mediated querying, description logic (DL) ontologies are used to enrich incomplete data with domain knowledge which results in more complete answers to queries. However, the evaluation of ontology-mediated queries (OMQs) over relational databases is computationally hard. This raises the question when OMQ evaluation is efficient, in the sense of being tractable in combined complexity or fixed-parameter tractable. We study this question for a range of ontology-mediated query languages based on several important and widely-used DLs, using unions of conjunctive queries as the actual queries. For the DL ELHI⊥, we provide a characterization of the classes of OMQs that are fixed-parameter tractable. For its fragment ELHdr⊥, which restricts the use of inverse roles, we provide a characterization of the classes of OMQs that are tractable in combined complexity. Both results are in terms of equivalence to OMQs of bounded tree width and rest on a reasonable assumption from parameterized complexity theory. They are similar in spirit to Grohe's seminal characterization of the tractable classes of conjunctive queries over relational databases. We further study the complexity of the meta problem of deciding whether a given OMQ is equivalent to an OMQ of bounded tree width, providing several completeness results that range from NP to 2ExpTime, depending on the DL used. We also consider the DL-Lite family of DLs, including members that, unlike ELHI⊥, admit functional roles.},
	booktitle = {Proceedings of the 34th {Annual} {ACM}/{IEEE} {Symposium} on {Logic} in {Computer} {Science}},
	publisher = {IEEE Press},
	author = {Barceló, Pablo and Feier, Cristina and Lutz, Carsten and Pieris, Andreas},
	year = {2021},
}

@inproceedings{zeli_establishment_2023,
	address = {New York, NY, USA},
	series = {{ICIIP} '22},
	title = {The {Establishment} and {Research} of an {Evaluation} {Model} {Based} on {Network} {Analytic} {Hierarchy} {Process}},
	isbn = {978-1-4503-9671-4},
	url = {https://doi.org/10.1145/3570236.3570256},
	doi = {10.1145/3570236.3570256},
	abstract = {AHP is a decision-making method that decomposes the elements that are always related to decision-making into the levels of goals, criteria, and plans, and conducts qualitative and quantitative analysis on this basis. By analyzing a series of factors that affect the target, comparing their relative importance, and finally selecting the scheme with the highest score is the optimal scheme. This paper makes use of this characteristic of its analysis, establishes a set of evaluation model system, and applies the evaluation model to Wuhan Business School. Finally, the model is scientific and reasonable, which provides a reference for other researches.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Intelligent} {Information} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Zeli, Cen},
	year = {2023},
	note = {event-place: Bucharest, Romania},
	keywords = {AHP, evaluation model, evaluation system, model analysis},
}

@inproceedings{yang_design_2025,
	address = {New York, NY, USA},
	series = {{CUI} '25},
	title = {Design {Activity} {Simulation}: {Opportunities} and {Challenges} in {Using} {Multiple} {Communicative} {AI} {Agents} to {Tackle} {Design} {Problems}},
	isbn = {979-8-4007-1527-3},
	url = {https://doi.org/10.1145/3719160.3736609},
	doi = {10.1145/3719160.3736609},
	abstract = {Large Language Models (LLMs) can enhance structured design thinking, yet existing copilot approaches integrate them into human workflows rather than exploring their autonomous potential. This paper investigates how LLM-based communicative AI agents can independently tackle open-ended design problems and how their strengths and limitations inform human-AI collaboration. We iteratively design a system where AI agents play different roles and simulate human design activity through conversational turns. The agents investigate user needs, identify design constraints, and explore the design space, with useful insights emerging from their interactions. To assess reasoning quality, we conducted a human jury evaluation with five HCI researchers and explored potential applications through a contextual inquiry with seven professionals. Our findings demonstrate that integrating human design thinking techniques enhances AI reasoning. AI agents effectively tackle design problems, generating low-novelty yet well-grounded and practical solutions that meet key design requirements.},
	booktitle = {Proceedings of the 7th {ACM} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Boyin and Dudley, John J and Kristensson, Per Ola},
	year = {2025},
	keywords = {Generative AI, End-user interaction with LLMs and Multimodal models},
}

@inproceedings{babaalla_towards_2024,
	address = {New York, NY, USA},
	series = {{NISS} '24},
	title = {Towards an {Automatic} {Extracting} {UML} {Class} {Diagram} from {System}'s {Textual} {Specification}},
	isbn = {979-8-4007-0929-6},
	url = {https://doi.org/10.1145/3659677.3659742},
	doi = {10.1145/3659677.3659742},
	abstract = {Developing a software system from natural language requirements is a complex and delicate task that requires a high level of design and programming expertise. Increasing the level of abstraction used to describe these requirements is the most natural solution. Model-Driven Engineering (MDE) also takes this route, using abstract models as primary entities to generate source code automatically or semi-automatically. Among these models, the UML class diagram occupies a privileged place in object-oriented systems because it not only serves as a basis for communication between developers but also provides a closely aligned static representation of the system implementation. However, creating a UML class diagram from a textual system specification poses a significant challenge due to the inherent imprecision and ambiguity commonly found in natural language expressions. In this paper, we propose a model-centric approach based on deep learning for the automatic extraction of UML class diagrams from textual requirements.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Networking}, {Intelligent} {Systems} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Babaalla, Zakaria and Jakimi, Abdeslam and Oualla, Mohamed and Saadane, Rachid and Chehri, Abdellah},
	year = {2024},
	note = {event-place: Meknes, AA, Morocco},
	keywords = {Machine learning, Deep learning, Natural Language Processing, UML, Class diagram},
}

@inproceedings{olufisayo_dahunsi_ontology-based_2021,
	address = {New York, NY, USA},
	series = {{RecSys} '21},
	title = {An {Ontology}-based {Knowledgebase} for {User} {Profile} and {Garment} {Features} in {Apparel} {Recommender} {Systems}},
	isbn = {978-1-4503-8458-2},
	url = {https://doi.org/10.1145/3460231.3473901},
	doi = {10.1145/3460231.3473901},
	abstract = {This research proposes the development of an ontology-based knowledgebase for Apparel recommender systems. User and garment attribute definitions and expert style rules will be extracted from expert literature and the most important accuracy-driving user and garment features will be parsed from it. The features will then be used to define the classes and slots of the knowledgebase with constraints defined by the style rules from the experts. This knowledgebase will be made publicly available for modification and reuse in future apparel recommender systems design.},
	booktitle = {Proceedings of the 15th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Olufisayo Dahunsi, Bolanle},
	year = {2021},
	note = {event-place: Amsterdam, Netherlands},
	pages = {851--854},
}

@inproceedings{rinaldi_semantic-based_2018,
	address = {New York, NY, USA},
	series = {{MEDES} '18},
	title = {A semantic-based model to represent multimedia big data},
	isbn = {978-1-4503-5622-0},
	url = {https://doi.org/10.1145/3281375.3281386},
	doi = {10.1145/3281375.3281386},
	abstract = {The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Rinaldi, Antonio M. and Russo, Cristiano},
	year = {2018},
	note = {event-place: Tokyo, Japan},
	keywords = {semantics, multimedia ontologies, semantic bigdata},
	pages = {31--38},
}

@inproceedings{wang_spot_2023,
	address = {New York, NY, USA},
	series = {{BCB} '23},
	title = {{SPOT}: {Sequential} {Predictive} {Modeling} of {Clinical} {Trial} {Outcome} with {Meta}-{Learning}},
	isbn = {979-8-4007-0126-9},
	url = {https://doi.org/10.1145/3584371.3613001},
	doi = {10.1145/3584371.3613001},
	abstract = {Clinical trials are essential to drug development but time-consuming, costly, and prone to failure. Accurate trial outcome prediction based on historical trial data promises better trial investment decisions and more trial success. Existing trial outcome prediction models were not designed to model the relations among similar trials, capture the progression of features and designs of similar trials, or address the skewness of trial data which causes inferior performance for less common trials.To fill the gap and provide accurate trial outcome prediction, we propose Sequential Predictive mOdeling of clinical Trial outcome (SPOT) that first identifies trial topics to cluster the multisourced trial data into relevant trial topics. It then generates trial embeddings and organizes them by topic and time to create clinical trial sequences. With the consideration of each trial sequence as a task, it uses a meta-learning strategy to achieve a point where the model can rapidly adapt to new tasks with minimal updates. In particular, the topic discovery module enables a deeper understanding of the underlying structure of the data, while sequential learning captures the evolution of trial designs and outcomes. This results in predictions that are not only more accurate but also more interpretable, taking into account the temporal patterns and unique characteristics of each trial topic. We demonstrate that SPOT wins over the prior methods by a significant margin on trial outcome benchmark data: with a 21.5\% lift on phase I, an 8.9\% lift on phase II, and a 5.5\% lift on phase III trials in the metric of the area under precision-recall curve (PR-AUC). Code is available at https://github.com/RyanWangZf/PyTrial.},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Zifeng and Xiao, Cao and Sun, Jimeng},
	year = {2023},
	note = {event-place: Houston, TX, USA},
}

@inproceedings{motara_functional_2018,
	address = {New York, NY, USA},
	series = {{SAICSIT} '18},
	title = {A functional ontology},
	isbn = {978-1-4503-6647-2},
	url = {https://doi.org/10.1145/3278681.3278687},
	doi = {10.1145/3278681.3278687},
	abstract = {The ontology of information systems — the way in which knowledge claims, and thus theories, are conceptualised and represented — is of particular importance in the information systems field, due to its reliance on relations between entities. This work proposes, demonstrates, and evaluates an alternative ontology for theory description which is arguably more powerful and more expressive than the dominant ontological model.},
	booktitle = {Proceedings of the {Annual} {Conference} of the {South} {African} {Institute} of {Computer} {Scientists} and {Information} {Technologists}},
	publisher = {Association for Computing Machinery},
	author = {Motara, Yusuf Moosa and van der Schyff, Karl},
	year = {2018},
	note = {event-place: Port Elizabeth, South Africa},
	keywords = {ontology, functional design, theory building, theory of planned behaviour},
	pages = {49--54},
}

@inproceedings{el_yamami_ontological_2018,
	address = {New York, NY, USA},
	series = {{SCA} '18},
	title = {An {Ontological} {Representation} of {PMBOK} {Framework} {Knowledge} {Areas}},
	isbn = {978-1-4503-6562-8},
	url = {https://doi.org/10.1145/3286606.3286825},
	doi = {10.1145/3286606.3286825},
	abstract = {Considering the problematic of IT Governance frameworks implementation, since it is difficult to apply a common framework to all organizations, this paper intended to address issues related to the adoption of IT project governance practices in organizations. It provides a common representation of its knowledge areas (Scope management, Schedule management, Cost management, Quality management and Risk Management) through ontological approach, relying on PMBOK framework best practice. The goal is to provide a machine-readable document for modeling IT projects governance domain. The results of this paper constitute a considerable contribution for practitioners, by participating in the development of IT governance approaches, to be used by non-specialists PMBOK in organizations, and by limiting as much as possible the bureaucracy of the information systems frameworks.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Smart} {City} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {El Yamami, Abir and Mansouri, Khalifa and Qbadou, Mohammed and Illoussamen, Elhossein and Laaziri, Majida and Benmoussa, Khaoula},
	year = {2018},
	note = {event-place: Tetouan, Morocco},
	keywords = {Ontology, Protégé, PMBOK, IT Governance, IT Project management},
}

@inproceedings{rayzhekov_between_2024,
	address = {New York, NY, USA},
	series = {{TEI} '24},
	title = {Between {This} and {That} is {It}: {Embodied} {Semantic} {Space} at the {Edge}},
	isbn = {979-8-4007-0402-4},
	url = {https://doi.org/10.1145/3623509.3635326},
	doi = {10.1145/3623509.3635326},
	abstract = {This paper describes the interactive artwork “Between This and That is It”: An AI-augmented typewriter that blends several decades of computerized optimization of text production. The artwork employs an offline processing machine learning-based language model embedded in a typical office typewriter from the 1980s. Deliberately diverting from the pervasive conversational user interface, the interaction style is based on a well-defined minimalist pattern of complementing two user-supplied words with a third word that – in the model – lies in the middle of the other two. This constraint interaction invites to explore the limits of the semantic space of language models and poses questions related to the topology of meaning, with respect to truthfulness, biases, and cliches, by creating a semi-intelligent poetic co-performance involving the audience. This project seeks to foster a discussion on the creative collaboration between humans and AI within the constraints of machine learning technologies embedded into objects from the near past. The experience and the perception of the interaction are shaped by a hybrid space shared between the audience members and the AI, the mechanical limitations of the typewriter, the embedded mini-computer’s computational capacity, and the language model itself.},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Tangible}, {Embedded}, and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Rayzhekov, Antoni and Murer, Martin},
	year = {2024},
	note = {event-place: Cork, Ireland},
	keywords = {AI, Language models, AI edge computing, graspable AI, Interactive Art},
}

@inproceedings{samimi_visual-conversational_2025,
	address = {New York, NY, USA},
	series = {{CUI} '25},
	title = {Visual-{Conversational} {Interface} for {Evidence}-{Based} {Explanation} of {Diabetes} {Risk} {Prediction}},
	isbn = {979-8-4007-1527-3},
	url = {https://doi.org/10.1145/3719160.3736616},
	doi = {10.1145/3719160.3736616},
	abstract = {Healthcare professionals need effective ways to use, understand, and validate AI-driven clinical decision support systems. Existing systems face two key limitations: complex visualizations and lack of grounding in scientific evidence. We present an integrated Decision Support System that combines interactive visualizations with a conversational agent for explaining diabetes risk assessments. We propose a hybrid prompt handling approach combining fine-tuned language models for analytical queries with general Large Language Models (LLMs) for broader medical questions, a methodology for grounding AI explanations in scientific evidence and a feature range analysis technique to support deeper understanding of feature contributions. We conducted a mixed-methods study with 30 healthcare professionals and found that the conversational interactions helped healthcare professionals build a clear understanding of model assessments, while the integration of scientific evidence calibrated trust in the system’s decisions. Most participants reported that the system supported both patient risk evaluation and recommendation.},
	booktitle = {Proceedings of the 7th {ACM} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Samimi, Reza and Bhattacharya, Aditya and Gosak, Lucija and Stiglic, Gregor and Verbert, Katrien},
	year = {2025},
	keywords = {Explainable AI, Human-centered AI, Clinical Decision Support Systems, Conversational AI},
}

@article{liu_jointly_2020,
	title = {Jointly {Integrating} {VCF}-{Based} {Variants} and {OWL}-{Based} {Biomedical} {Ontologies} in {MongoDB}},
	volume = {17},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2019.2951137},
	doi = {10.1109/TCBB.2019.2951137},
	abstract = {The development of the next-generation sequencing (NGS) technologies has led to massive amounts of VCF (Variant Call Format) files, which have been the standard formats developed with 1000 Genomes Project. At the same time, with the widespread use of biomedical ontologies in the biomedical community, more and more applications have accepted the Web Ontology Language (OWL) as the dominant data format for the specifications of biomedical ontology descriptions, leading to the rapid growth of OWL-based biomedical ontology scale. In this paper, we seek to explore an effective method for the management of VCF-based genetic variants and OWL-based biological ontologies using the MongoDB database. Considering many current applications (such as the short genetic variations database dbSNP, etc.) are transitioning to the new design by using JSON (JavaScript Object Notation) to support future massive data expansion and interchanges. We firstly propose a series of rules for the mapping from VCF and OWL files to JSON files, and then present rule-based algorithms for transforming VCF-based genetic variants and OWL-based biological ontologies into JSON objects. On this basis, we introduce effective approaches of integrating the mapped JSON files in MongoDB. Finally, we complement this work with a set of experiments to show the performance of our proposed approaches. The source code of the proposed approaches could be freely available at https://github.com/lyotvincent/AJIA.},
	number = {5},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Liu, Jian and Qu, Zhi and Yang, Mo and Sun, Jialiang and Su, Shuhui and Zhang, Lei},
	month = oct,
	year = {2020},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {1504--1515},
}

@inproceedings{goel_tcs_witm_2021_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {{TCS}\_WITM\_2021 @{FinSim}-2: {Transformer} based {Models} for {Automatic} {Classification} of {Financial} {Terms}},
	isbn = {978-1-4503-8313-4},
	url = {https://doi.org/10.1145/3442442.3451386},
	doi = {10.1145/3442442.3451386},
	abstract = {Recent advancement in neural network architectures has provided several opportunities to develop systems to automatically extract and represent information from domain specific unstructured text sources. The Finsim-2021 shared task, collocated with the FinNLP workshop, offered the challenge to automatically learn effective and precise semantic models of financial domain concepts. Building such semantic representations of domain concepts requires knowledge about the specific domain. Such a thorough knowledge can be obtained through the contextual information available in raw text documents on those domains. In this paper, we proposed a transformer-based BERT architecture that captures such contextual information from a set of domain specific raw documents and then perform a classification task to segregate domain terms into fixed number of class labels. The proposed model not only considers the contextual BERT embeddings but also incorporates a TF-IDF vectorizer that gives a word level importance to the model. The performance of the model has been evaluated against several baseline architectures.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Goel, Tushar and Chauhan, Vipul and Verma, Ishan and Dasgupta, Tirthankar and Dey, Lipika},
	year = {2021},
	note = {event-place: Ljubljana, Slovenia},
	keywords = {Ontology, Transformers, Text Classification, Automatic Classification of Financial Term, TFIDF Vectors},
	pages = {311--315},
}

@inproceedings{wen_preliminary_2019,
	address = {New York, NY, USA},
	series = {{EASE} '19},
	title = {Preliminary {Evaluation} of an {Ontology}-{Based} {Contextualized} {Learning} {System} for {Software} {Security}},
	isbn = {978-1-4503-7145-2},
	url = {https://doi.org/10.1145/3319008.3319017},
	doi = {10.1145/3319008.3319017},
	abstract = {Learning software security is a big challenging task in the information technology sector due to the vast amount of security knowledge and the difficulties in understanding the practical applications. The traditional teaching and learning materials, which are usually organized topically and security-centric, have fewer linkages with learners' experience and prior knowledge that they bring to the learning sessions. Learners often do not associate vulnerabilities or coding practices with programs similar to what they were writing in their previous time. Consequently, their motivation for learning is not touched by conventional methods. The aim of this paper is the presentation of an ontology-based learning system for software security with contextualized learning approaches, and of the results of an initial evaluation using a controlled quasi-experiment in a university learning environment. This system facilitates the contextual learning process by providing contextualized access to security knowledge via real software application scenarios, in which learners can explore and relate the security knowledge to the context they are already familiar with. The experiment results show that the prototyped system with the proposed learning approach not only yields significant knowledge gain compared to the conventional learning approach but also gains better learning satisfaction of students.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Shao-Fang and Katt, Basel},
	year = {2019},
	note = {event-place: Copenhagen, Denmark},
	keywords = {ontology, Software security, context-based knowledge, learning system},
	pages = {90--99},
}

@article{arenas_expressive_2018,
	title = {Expressive {Languages} for {Querying} the {Semantic} {Web}},
	volume = {43},
	issn = {0362-5915},
	url = {https://doi.org/10.1145/3238304},
	doi = {10.1145/3238304},
	abstract = {The problem of querying RDF data is a central issue for the development of the Semantic Web. The query language SPARQL has become the standard language for querying RDF since its W3C standardization in 2008. However, the 2008 version of this language missed some important functionalities: reasoning capabilities to deal with RDFS and OWL vocabularies, navigational capabilities to exploit the graph structure of RDF data, and a general form of recursion much needed to express some natural queries. To overcome these limitations, a new version of SPARQL, called SPARQL 1.1, was released in 2013, which includes entailment regimes for RDFS and OWL vocabularies, and a mechanism to express navigation patterns through regular expressions. Unfortunately, there are a number of useful navigation patterns that cannot be expressed in SPARQL 1.1, and the language lacks a general mechanism to express recursive queries. To the best of our knowledge, no efficient RDF query language that combines the above functionalities is known. It is the aim of this work to fill this gap. To this end, we focus on a core fragment of the OWL 2 QL profile of OWL 2 and show that every SPARQL query enriched with the above features can be naturally translated into a query expressed in a language that is based on an extension of Datalog, which allows for value invention and stratified negation. However, the query evaluation problem for this language is highly intractable, which is not surprising since it is expressive enough to encode some inherently hard queries. We identify a natural fragment of it, and we show it to be tractable and powerful enough to define SPARQL queries enhanced with the desired functionalities.},
	number = {3},
	journal = {ACM Trans. Database Syst.},
	author = {Arenas, Marcelo and Gottlob, Georg and Pieris, Andreas},
	month = nov,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Semantic web, SPARQL, RDF, query answering, Datalog-based languages},
}

@inproceedings{pahal_ontology-based_2018,
	address = {New York, NY, USA},
	series = {{SCA} '18},
	title = {An {Ontology}-based {Context}-aware {IoT} {Framework} for {Smart} {Surveillance}},
	isbn = {978-1-4503-6562-8},
	url = {https://doi.org/10.1145/3286606.3286846},
	doi = {10.1145/3286606.3286846},
	abstract = {In this paper, we have proposed an ontology-based context-aware framework for providing intelligent services such as smart surveillance, which employ IoT technologies to ensure better quality of life in a smart city. An IoT network such as a smart surveillance system combines the working of Closed-circuit television (CCTV) cameras and various sensors to perform real-time computation for identifying threats and critical situations with the help of valuable context information. This information is perceptual in nature and needs to be converted into higher-level abstractions that can further be used for reasoning to recognize situations. Semantic abstractions for perceptual inputs are possible with the use of a multimedia ontology encoded using Multimedia Web Ontology Language (MOWL) that helps to define concepts, properties and structure of a possible environment. MOWL also allows for a dynamic modeling of real-time situations by employing Dynamic Bayesian networks (DBN), which suits the requirements of a intelligent IoT system. In this paper, we show the application of this framework in a smart surveillance system. Surveillance is enhanced by not only helping to analyze past events, but by predicting dangerous situations for which preventive actions can be taken. In our proposed approach, continuous video stream of data captured by CCTV cameras can be processed on the fly to give real-time alerts to concerned authorities. These alerts can be disseminated using e-mail, text messaging, on-screen alerts and alarms.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Smart} {City} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Pahal, Nisha and Mallik, Anupama and Chaudhury, Santanu},
	year = {2018},
	note = {event-place: Tetouan, Morocco},
	keywords = {Internet of Things (IoT), Dynamic Bayesian Network (DBN), Multimedia ontology, Smart Surveillance},
}

@inproceedings{barcelo_containment_2018,
	address = {New York, NY, USA},
	series = {{PODS} '18},
	title = {Containment for {Rule}-{Based} {Ontology}-{Mediated} {Queries}},
	isbn = {978-1-4503-4706-8},
	url = {https://doi.org/10.1145/3196959.3196963},
	doi = {10.1145/3196959.3196963},
	abstract = {Many efforts have been dedicated to identifying restrictions on ontologies expressed as tuple-generating dependencies (tgds), a.k.a. existential rules, that lead to the decidability of answering ontology-mediated queries (OMQs). This has given rise to three families of formalisms: guarded, non-recursive, and sticky sets of tgds. We study the containment problem for OMQs expressed in such formalisms, which is a key ingredient for solving static analysis tasks associated with them. Our main contribution is the development of specially tailored techniques for OMQ containment under the classes of tgds stated above. This enables us to obtain sharp complexity bounds for the problems at hand.},
	booktitle = {Proceedings of the 37th {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Barcelo, Pablo and Berger, Gerald and Pieris, Andreas},
	year = {2018},
	note = {event-place: Houston, TX, USA},
	keywords = {tuple-generating dependencies, ontology-mediated queries, computational complexity, conjunctive queries, query containment},
	pages = {267--279},
}

@inproceedings{kraus_create_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {"{Create} a {Fear} of {Missing} {Out}" - {ChatGPT} {Implements} {Unsolicited} {Deceptive} {Designs} in {Generated} {Websites} {Without} {Warning}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713083},
	doi = {10.1145/3706598.3713083},
	abstract = {With the recent advancements in Large Language Models (LLMs), web developers increasingly apply their code-generation capabilities to website design. However, since these models are trained on existing designerly knowledge, they may inadvertently replicate bad or even illegal practices, especially deceptive designs (DD). This paper examines whether users can accidentally create DD for a fictitious webshop using GPT-4. We recruited 20 participants, asking them to use ChatGPT to generate functionalities (product overview or checkout) and then modify these using neutral prompts to meet a business goal (e.g., “increase the likelihood of us selling our product”). We found that all 20 generated websites contained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no warnings. When reflecting on the designs, only 4 participants expressed concerns, while most considered the outcomes satisfactory and not morally problematic, despite the potential ethical and legal implications for end-users and those adopting ChatGPT’s recommendations.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Krauß, Veronika and McGill, Mark and Kosch, Thomas and Thiel, Yolanda Maira and Schön, Dominik and Gugenheimer, Jan},
	year = {2025},
	keywords = {ChatGPT, LLM, Dark Patterns, Deceptive Design, Design Inspiration},
}

@inproceedings{alenezi_comparison_2020,
	address = {New York, NY, USA},
	series = {{EASE} '20},
	title = {A {Comparison} {Study} of {Available} {Sofware} {Security} {Ontologies}},
	isbn = {978-1-4503-7731-7},
	url = {https://doi.org/10.1145/3383219.3383292},
	doi = {10.1145/3383219.3383292},
	abstract = {A rising number of software and services malfunctioning due to security flaws has increased the importance of software security and resulted in numerous knowledge sources of the domain. Building secure software systems require the understanding and extraction of the available knowledge, and a standard knowledge management platform is needed. Ontologies form an integral part of knowledge management platforms as they capture and structure the given knowledge. Various software security ontologies have been proposed previously, either stand-alone or as part of some bigger ontology like a computer or information security. However, these ontologies do not cover the entire domain and cannot be used as a standard ontology for software security in its current form. In this paper, we have identified and evaluated the existing ontologies that specifically capture software security knowledge, both qualitatively and quantitatively with the help of ontology evaluation tools, in order to select the best ontology that can be extended to prepare the standard ontology for the software security domain.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Alenezi, Mamdouh and Basit, Hamid Abdul and Khan, Faraz Idris and Beg, Maham Anwar},
	year = {2020},
	note = {event-place: Trondheim, Norway},
	keywords = {Security Ontology, Software Quality Assurance, Software Quality Management, Software Security, Software Security Knowledge Management},
	pages = {499--504},
}

@inproceedings{yeh_towards_2018,
	address = {New York, NY, USA},
	series = {{ICIIT} '18},
	title = {Towards a {Biographic} {Knowledge}-based {Story} {Ontology} {System}},
	isbn = {978-1-4503-6378-5},
	url = {https://doi.org/10.1145/3193063.3193065},
	doi = {10.1145/3193063.3193065},
	abstract = {In this article, we illustrate some of the semantic web-related technologies and design a set of ontology knowledge structures based on biographical history, using the OWL markup language, which we call BKOnto. This is an official framework for processing biographical history-related messages on the semantic web, including biographical events, time and space relationships, related personal messages, and more. We elaborate on this ontology knowledge architecture and explain how to use BKOnto as a basis for more domain-specific knowledge representation. In BKOnto, we use the OWL language to define the main components of the cognitive structure of the historical body of biography, namely the Storyline of the biography and the historical event of the biography. The so-called biographical story line, which is used to organize the history of multiple biographical superstructure, can be used to describe the biography of a particular person. The so-called biographical historical events, based on the historical data can be based on the description of the content and related space-time factor description of the basic unit. BKOnto's design was based on the StoryLine and Event infrastructure, and then we developed the ontology knowledge building system based on this ontology awareness architecture. Therefore, we also developed a set of ontology knowledge building system based on BKOnto, which is called StoryTeller. The StoryTeller system can be used to construct relevant knowledge of human things in the history of the biography and form a complete biographical story. StoryTeller system, mainly based on the story line organized by the timeline, which contains a number of types and events related to multiple human things as the basic unit to build the story line. The event unit not only describes the description of related human affairs, but also contains the description of time factor and space factor, which is used to construct the space-time information of the unit in the story line. As a result, in a story line with multiple event units, you will be able to present a wealth of information about people and things with their associated spatiotemporal features. In addition, based on the idea of supporting the digital collection system, we also linked up individual event units with the digital collection system of their information sources so that more diverse digital collections could be presented in the future. The empirical study also uses the Mackay Digital Archives Project (http://dlm.csie.au.edu.tw/) as a source of information to demonstrate the ontology knowledge building process of Mackay's biographical stories, as well as related Digital collection of information.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Intelligent} {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Yeh, Jian-hua},
	year = {2018},
	note = {event-place: Ha Noi, Viet Nam},
	keywords = {ontology, OWL, semantic web, Biographical knowledge, ontology composition system, temporal event},
	pages = {33--38},
}

@inproceedings{deng_unified_2023,
	address = {New York, NY, USA},
	series = {{ICMR} '23},
	title = {A {Unified} {Model} for {Video} {Understanding} and {Knowledge} {Embedding} with {Heterogeneous} {Knowledge} {Graph} {Dataset}},
	isbn = {979-8-4007-0178-8},
	url = {https://doi.org/10.1145/3591106.3592258},
	doi = {10.1145/3591106.3592258},
	abstract = {Video understanding is an important task in short video business platforms and it has a wide application in video recommendation and classification. Most of the existing video understanding works only focus on the information that appeared within the video content, including the video frames, audio and text. However, introducing common sense knowledge from the external Knowledge Graph (KG) dataset is essential for video understanding when referring to the content which is less relevant to the video. Owing to the lack of video knowledge graph dataset, the work which integrates video understanding and KG is rare. In this paper, we propose a heterogeneous dataset that contains the multi-modal video entity and fruitful common sense relations. This dataset also provides multiple novel video inference tasks like the Video-Relation-Tag (VRT) and Video-Relation-Video (VRV) tasks. Furthermore, based on this dataset, we propose an end-to-end model that jointly optimizes the video understanding objective with knowledge graph embedding, which can not only better inject factual knowledge into video understanding but also generate effective multi-modal entity embedding for KG. Comprehensive experiments indicate that combining video understanding embedding with factual knowledge benefits the content-based video retrieval performance. Moreover, it also helps the model generate better knowledge graph embedding which outperforms traditional KGE-based methods on VRT and VRV tasks with at least 42.36\% and 17.73\% improvement in HITS@10.},
	booktitle = {Proceedings of the 2023 {ACM} {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Deng, Jiaxin and Shen, Dong and Pan, Haojie and Wu, Xiangyu and Liu, Ximan and Meng, Gaofeng and Yang, Fan and Gao, Tingting and Fu, Ruiji and Wang, Zhongyuan},
	year = {2023},
	note = {event-place: Thessaloniki, Greece},
	keywords = {knowledge graph, multi-modal learning, video inference, video understanding},
	pages = {95--104},
}

@inproceedings{abri_classification_2021,
	address = {New York, NY, USA},
	series = {{NLPIR} '20},
	title = {A {Classification} on {Different} {Aspects} of {User} {Modelling} in {Personalized} {Web} {Search}},
	isbn = {978-1-4503-7760-7},
	url = {https://doi.org/10.1145/3443279.3443291},
	doi = {10.1145/3443279.3443291},
	abstract = {In the context of personalization has recently been doing a lot of researches and applications. A common component of all research in the field of personalization is user modeling that also called user profiling. The main work of the user modeling in the field of personalization in the first step is capturing information about users and in the next step is to identify the user's preferences and interests and efficient use this information for increasing the retrieval performance. How to collect information about the user, user model structures and the used techniques to create a user model is different in each of personalized applications. In the previous studies, there was not a complete classification on the major dimensions of user models. In this research, we present an appropriate classification on the major dimensions of user models. We aim to present a survey on applications and techniques of user modeling and make a classification of user modeling by considering the existing literature and research and we hope can help to researchers in better-developing on the area.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Abri, Sara and Abri, Rayan and Cetin, Salih},
	year = {2021},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {User Profiling, Recommendation Systems, Personalized Search},
	pages = {194--199},
}

@inproceedings{exelmans_virtual_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {A {Virtual} {Global} {Monorepo} of {Immutable} {Linked} {Data}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688222},
	doi = {10.1145/3652620.3688222},
	abstract = {The data layer of today's model management solutions often is either centralized or Git-based. We point out a number of limitations of current approaches, such as poor replicability, manually configured access control, centralization, hard-coded 'meta-data', and inflexible encodings. We argue for a set of fundamental features / restrictions (most importantly immutability and capability-based security) for decentralized model management systems to adapt, to solve these problems at their root. We distinguish a fundamental core from non-fundamental applications (such as versioning), that can be built on top.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Exelmans, Joeri and Pietron, Jakob and Raschke, Alexander and Vangheluwe, Hans},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {model management, capability-based security, versioning},
	pages = {1000--1004},
}

@inproceedings{wehnert_legal_2021,
	address = {New York, NY, USA},
	series = {{ICAIL} '21},
	title = {Legal norm retrieval with variations of the bert model combined with {TF}-{IDF} vectorization},
	isbn = {978-1-4503-8526-8},
	url = {https://doi.org/10.1145/3462757.3466104},
	doi = {10.1145/3462757.3466104},
	abstract = {In this work, we examine variations of the BERT model on the statute law retrieval task of the COLIEE competition. This includes approaches to leverage BERT's contextual word embeddings, fine-tuning the model, combining it with TF-IDF vectorization, adding external knowledge to the statutes and data augmentation. Our ensemble of Sentence-BERT with two different TF-IDF representations and document enrichment exhibits the best performance on this task regarding the F2 score. This is followed by a fine-tuned LEGAL-BERT with TF-IDF and data augmentation and our third approach with the BERTScore. As a result, we show that there are significant differences between the chosen BERT approaches and discuss several design decisions in the context of statute law retrieval.},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Wehnert, Sabine and Sudhi, Viju and Dureja, Shipra and Kutty, Libin and Shahania, Saijal and De Luca, Ernesto W.},
	year = {2021},
	note = {event-place: São Paulo, Brazil},
	keywords = {data augmentation, contextual word embeddings, document enrichment, legal information retrieval},
	pages = {285--294},
}

@inproceedings{hormozdiari_session_2021,
	address = {New York, NY, USA},
	series = {{BCB} '21},
	title = {Session details: {Ontologies} \&amp; databases},
	isbn = {978-1-4503-8450-6},
	url = {https://doi.org/10.1145/3478676},
	doi = {10.1145/3478676},
	booktitle = {Proceedings of the 12th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Hormozdiari, Fereydoun},
	year = {2021},
	note = {event-place: Gainesville, Florida},
}

@inproceedings{choi_genpara_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {{GenPara}: {Enhancing} the {3D} {Design} {Editing} {Process} by {Inferring} {Users}' {Regions} of {Interest} with {Text}-{Conditional} {Shape} {Parameters}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713502},
	doi = {10.1145/3706598.3713502},
	abstract = {In 3D design, specifying design objectives and visualizing complex shapes through text alone proves to be a significant challenge. Although advancements in 3D GenAI have significantly enhanced part assembly and the creation of high-quality 3D designs, many systems still to dynamically generate and edit design elements based on the shape parameters. To bridge this gap, we propose GenPara, an interactive 3D design editing system that leverages text-conditional shape parameters of part-aware 3D designs and visualizes design space within the Exploration Map and Design Versioning Tree. Additionally, among the various shape parameters generated by LLM, the system extracts and provides design outcomes within the user’s regions of interest based on Bayesian inference. A user study (N = 16) revealed that GenPara enhanced the comprehension and management of designers with text-conditional shape parameters, streamlining design exploration and concretization. This improvement boosted efficiency and creativity of the 3D design process.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Choi, Jiin and Lee, Seung Won and Hyun, Kyung Hoon},
	year = {2025},
	keywords = {Large Language Models (LLMs), Human-AI Interaction, 3D Generative AI, Bayesian Inference, Design Space},
}

@article{tu_unicorn_2023,
	title = {Unicorn: {A} {Unified} {Multi}-tasking {Model} for {Supporting} {Matching} {Tasks} in {Data} {Integration}},
	volume = {1},
	url = {https://doi.org/10.1145/3588938},
	doi = {10.1145/3588938},
	abstract = {Data matching - which decides whether two data elements (e.g., string, tuple, column, or knowledge graph entity) are the "same" (a.k.a. a match) - is a key concept in data integration, such as entity matching and schema matching. The widely used practice is to build task-specific or even dataset-specific solutions, which are hard to generalize and disable the opportunities of knowledge sharing that can be learned from different datasets and multiple tasks. In this paper, we propose Unicorn, a unified model for generally supporting common data matching tasks. Unicorn can enable knowledge sharing by learning from multiple tasks and multiple datasets, and can also support zero-shot prediction for new tasks with zero labeled matching/non-matching pairs. However, building such a unified model is challenging due to heterogeneous formats of input data elements and various matching semantics of multiple tasks. To address the challenges, Unicorn employs one generic Encoder that converts any pair of data elements (a, b) into a learned representation, and uses a Matcher, which is a binary classifier, to decide whether a matches b. To align matching semantics of multiple tasks, Unicorn adopts a mixture-of-experts model that enhances the learned representation into a better representation. We conduct extensive experiments using 20 datasets on seven well-studied data matching tasks, and find that our unified model can achieve better performance on most tasks and on average, compared with the state-of-the-art specific models trained for ad-hoc tasks and datasets separately. Moreover, Unicorn can also well serve new matching tasks with zero-shot learning.},
	number = {1},
	journal = {Proc. ACM Manag. Data},
	author = {Tu, Jianhong and Fan, Ju and Tang, Nan and Wang, Peng and Li, Guoliang and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {data integration, multi-task learning, data matching},
}

@inproceedings{papoutsoglou_mapping_2024,
	address = {New York, NY, USA},
	series = {{SETN} '24},
	title = {Mapping the {Current} {Status} of {CTI} {Knowledge} {Graphs} through a {Bibliometric} {Analysis}},
	isbn = {979-8-4007-0982-1},
	url = {https://doi.org/10.1145/3688671.3688738},
	doi = {10.1145/3688671.3688738},
	abstract = {Bibliometric analysis in the field of cybersecurity and Cyber Threat Intelligence (CTI) is crucial for identifying research trends, key themes, and collaborative networks, which can guide future research directions and policy decisions. This paper presents a comprehensive bibliometric analysis of the current status of research on knowledge graphs in cybersecurity, highlighting significant trends and thematic clusters. The analysis reveals a rapidly growing interest in integrating knowledge graphs with advanced machine learning and AI techniques, such as deep learning and neural networks, to enhance cyber threat intelligence and response strategies. Key findings include the prominence of natural language processing, entity recognition, and relation extraction as critical methodologies in this field. Thematic evolution analysis shows the adoption of large language models (LLMs) and an ongoing focus on structured knowledge representation. The study underscores the potential of knowledge graphs to improve cybersecurity through better data organization, threat detection, and intelligence extraction.},
	booktitle = {Proceedings of the 13th {Hellenic} {Conference} on {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Papoutsoglou, Maria and Meditskos, Georgios and Bassiliades, Nick and Kontopoulos, Efstratios and Vrochidis, Stefanos},
	year = {2024},
	keywords = {Knowledge Graphs, Cybersecurity, Bibliometric Analysis, CTI},
}

@inproceedings{lewis_model_2022,
	address = {New York, NY, USA},
	series = {{DLfM} '22},
	title = {A model for annotating musical versions and arrangements across multiple documents and media},
	isbn = {978-1-4503-9668-4},
	url = {https://doi.org/10.1145/3543882.3543891},
	doi = {10.1145/3543882.3543891},
	abstract = {We present a model for the annotation of musical works, where the annotations are created with respect to a conceptual abstraction of the music instead of directly to concrete encodings. This supports musicologists in constructing arguments about musical elements that occur in multiple digital library sources (or other web resources), that recur across a work, or that appear in different forms in different arrangements. It provides a way of discussing musical content without tying that discourse to the location, notation or medium of the content, allowing evidence from multiple libraries and in different formats to be brought together to support musicological assertions. This model is implemented in Linked Data and illustrated in a prototype application in which musicologists annotate vocal arrangements of the Allegretto from Beethoven’s Seventh Symphony from multiple sources.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {Association for Computing Machinery},
	author = {Lewis, David and Shibata, Elisabete and Saccomano, Mark and Rosendahl, Lisa and Kepper, Johannes and Hankinson, Andrew and Siegert, Christine and Page, Kevin},
	year = {2022},
	note = {event-place: Prague, Czech Republic},
	keywords = {linked data, FRBR, conceptual modelling, digital musicology, music arrangements},
	pages = {10--18},
}

@inproceedings{wang_research_2019,
	address = {New York, NY, USA},
	series = {{ICSCA} '19},
	title = {Research on {Domain} {Ontology} {Automation} {Construction} {Based} on {Chinese} {Texts}},
	isbn = {978-1-4503-6573-4},
	url = {https://doi.org/10.1145/3316615.3316685},
	doi = {10.1145/3316615.3316685},
	abstract = {The main construction method of the current ontology is to rely on ontology experts for manual construction. Because manual construction requires a lot of manual participation, manual construction has great limitations. Text data as one of the main forms of data source, how to construct domain ontology automatically from texts and how to provide semantic retrieval support to text quickly by ontology is the hotspot of ontology research at present. Aiming at the above problems, an automatic construction method of domain ontology based on knowledge graph and association rule mining is presented, and it can extract the concepts, hierarchies and non-hierarchies of domain ontology from text, and finally form ontology by Jena. It also provides semantic retrieval of text by associating text and concepts in the process of ontology construction. Finally, the effect of automatic ontology construction is verified by the effect of text retrieval.},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Bo and Luo, Jun and Zhu, Shuyuan},
	year = {2019},
	note = {event-place: Penang, Malaysia},
	keywords = {RDF, Knowledge Graph, Ontology Construction, Jena, Association Rule Mining},
	pages = {425--430},
}

@article{florence_pop-pl_2018,
	title = {{POP}-{PL}: {A} {Patient}-{Oriented} {Prescription} {Programming} {Language}},
	volume = {40},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/3210256},
	doi = {10.1145/3210256},
	abstract = {A medical prescription is a set of health care instructions that govern the plan of care for an individual patient, which may include orders for drug therapy, diet, clinical assessment, and laboratory testing. Clinicians have long used algorithmic thinking to describe and implement prescriptions but without the benefit of a formal programming language. Instead, medical algorithms are expressed using a natural language patois, flowcharts, or as structured data in an electronic medical record system. The lack of a prescription programming language inhibits expressiveness; results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse; and increases the risk of fatal medical error.This article reports on the design and evaluation of Patient-Oriented Prescription Programming Language (POP-PL), a domain-specific programming language designed for expressing prescriptions. The language is based around the idea that programs and humans have complementary strengths that, when combined properly, can make for safer, more accurate performance of prescriptions. Use of POP-PL facilitates automation of certain low-level vigilance tasks, freeing up human cognition for abstract thinking, compassion, and human communication.We implemented this language and evaluated its design attempting to write prescriptions in the new language and evaluated its usability by assessing whether clinicians can understand and modify prescriptions written in the language. We found that some medical prescriptions can be expressed in a formal domain-specific programming language, and we determined that medical professionals can understand and correctly modify programs written in POP-PL. We also discuss opportunities for refining and further developing POP-PL.},
	number = {3},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Florence, Spencer P. and Fetscher, Burke and Flatt, Matthew and Temps, William H. and St-Amour, Vincent and Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte and Yarnold, Paul R. and Findler, Robert Bruce and Belknap, Steven M.},
	month = jul,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {DSL design, empirical evaluation, medical prescriptions, medical programming languages},
}

@inproceedings{bekkali_arabic_2019,
	address = {New York, NY, USA},
	series = {{SMC} '19},
	title = {Arabic {Sentiment} {Analysis} based on {Topic} {Modeling}},
	isbn = {978-1-4503-6129-3},
	url = {https://doi.org/10.1145/3314074.3314091},
	doi = {10.1145/3314074.3314091},
	abstract = {Users of social media generate a huge volume of reviews and comments. These reviews and comments express user's opinions about different topics. As a result, there is a great need to understand and classify these reviews. Sentiment Analysis Systems is a good way to overcome this problem. Reviews are considered as short texts and they are different from traditional documents without enough contextual information. To address this issue, we propose an efficient representation for short text based on concepts instead of terms, which transforms the data representation into a shorter, more compact, and more predictive one. However, for the Arabic language, the majority of semantic resources are incomplete projects; this may presents a serious problem about the coverage ratio of the Arabic language compared with other Languages. To overcome this problem and starting with the assumption that terms belonging to same topic share many semantic links in the same dataset, their corresponding concepts will share the same semantics links in the same dataset. We suggest integrating Topic Modeling as a tool to bring together terms with the same semantic links. The proposed method has been tested and evaluated using the Large Scale Arabic Book Reviews Dataset and the obtained results illustrate the interest and efficiency of our contribution.},
	booktitle = {Proceedings of the {New} {Challenges} in {Data} {Sciences}: {Acts} of the {Second} {Conference} of the {Moroccan} {Classification} {Society}},
	publisher = {Association for Computing Machinery},
	author = {Bekkali, Mohammed and Lachkar, Abdelmonaime},
	year = {2019},
	note = {event-place: Kenitra, Morocco},
	keywords = {Arabic Language, LDA, Sentiment Analysis, Conceptualization, Topic Modeling, Short Text Representation},
}

@inproceedings{selvaraj_study_2019,
	address = {New York, NY, USA},
	series = {{ICSIM} '19},
	title = {A {Study} on {Traditional} {Medicine} {Ontology}},
	isbn = {978-1-4503-6642-7},
	url = {https://doi.org/10.1145/3305160.3305212},
	doi = {10.1145/3305160.3305212},
	abstract = {The traditional medical field needs to be studied more for applying compound reasoning using ontology because traditional medicine treats the patients by finding root cause of the symptoms rather than treating for the symptoms directly. Many countries have their own traditional medicines like traditional Chinese medicine, traditional Korean medicine, Siddha, Ayurveda and Unani. Already many projects are started to work on the ontology development for traditional medicines. In this paper, we analyze and summarize the existing medical ontology researches. This study is useful to understand the existing medical ontology system and also provide an idea to develop and enhance the traditional medicine ontology by reusing existing resources. In this study, we also propose the ontology-based medical support system to predict the seasonal diseases by combining medical ontology with Big data analysis.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Software} {Engineering} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Selvaraj, Suganya and Choi, Eunmi},
	year = {2019},
	note = {event-place: Bali, Indonesia},
	keywords = {Medical ontology study, Ontology-based medical support system, Traditional medicine ontology},
	pages = {235--239},
}

@inproceedings{de_lara_facet-oriented_2018,
	address = {New York, NY, USA},
	series = {{SLE} 2018},
	title = {Facet-oriented modelling: open objects for model-driven engineering},
	isbn = {978-1-4503-6029-6},
	url = {https://doi.org/10.1145/3276604.3276610},
	doi = {10.1145/3276604.3276610},
	abstract = {Model-driven engineering (MDE) promotes models as the principal assets in software projects. Models are built using a modelling language whose syntax is defined by a metamodel. Hence, objects in models are typed by a metamodel class, and this typing relation is static as it is established at creation time and cannot be changed later. This way, objects in MDE are closed and fixed with respect to the type they conform to, the slots/properties they have, and the constraints they should obey. This hampers the reuse of model-related artefacts like model transformations, as well as the opportunistic or dynamic combination of metamodels. To alleviate this rigidity, we propose making model objects open so that they can acquire or drop so-called facets, each one contributing a type, slots and constraints to the object. Facets are defined by regular metamodels, hence being a lightweight extension of standard metamodelling. Facet metamodels may declare usage interfaces, and it is possible to specify laws that govern how facets are to be assigned to the instances of a metamodel. In this paper, we describe our proposal, report on an implementation, and illustrate scenarios where facets have advantages over other techniques.},
	booktitle = {Proceedings of the 11th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {de Lara, Juan and Guerra, Esther and Kienzle, Jörg and Hattab, Yanis},
	year = {2018},
	note = {event-place: Boston, MA, USA},
	keywords = {Reuse, Model-Driven Engineering, MetaDepth, Flexible Modelling, Metamodelling, Role-Based Modelling},
	pages = {147--159},
}

@inproceedings{kuhne_meaningful_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Meaningful metrics for multi-level modelling},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3421412},
	doi = {10.1145/3417990.3421412},
	abstract = {One of the key enablers of further growth of multi-level modeling will be the development of objective ways to allow multi-level modeling approaches to be compared to one another and to two-level modeling approaches. While significant strides have been made regarding qualitative comparisons, there is currently no adequate way to quantitatively assess to what extent a multi-level model may be preferable over another model with respect to high-level qualities such as understandability, maintainability, and control capacity. In this paper, we propose deep metrics, as an approach to quantitatively measure high-level model concerns of multi-level models that are of interest to certain stakeholders. Beyond the stated goals, we see deep metrics as furthermore supporting the comparison of modeling styles and aiding modelers in making individual design decisions. We discuss what makes a metric "depth-aware" so that it can appropriately capture multi-level model properties, and present two concrete proposals for metrics that measure high-level multi-level model qualities.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Kühne, Thomas and Lange, Arne},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {metrics, multi-level modeling, model comparison},
}

@inproceedings{sowah_edgebase_2018,
	address = {New York, NY, USA},
	series = {{ACAI} '18},
	title = {Edgebase: {A} {Cooperative} {Query} {Answering} {Database} {System} {With} {A} {Natural} {Language} {Interface}},
	isbn = {978-1-4503-6625-0},
	url = {https://doi.org/10.1145/3302425.3302482},
	doi = {10.1145/3302425.3302482},
	abstract = {Traditional Database Management Systems (DBMS) require users to meticulously construct and submit queries to generate answers. The lack of query syntax flexibility in traditional database systems make results in simple and direct answers - queries retrieve precisely matched elements stated in the given Boolean query. In this paper, we propose a Cooperative Query Answering Database System (CDBS) that provide answers to user queries in the same manner as humans do, and not as machines.The method of "Cooperative Query Answering (CQA)", emanated from the perception that to provide adequate and "complete" answers to queries, recognition of users' intentions is vital. Most database systems require users to submit their queries using SQL syntax. In addition to presenting answers to queries in human-like manner, we present a cooperative approach to query submission. By this, we present an architecture that combines the rich features of html, Natural Language (NL) with Query-By-Form (QBF) method and MySQL to enable our proposed system accept user queries in plain English language.To authenticate our approach and proposed system, a set of thorough experiments were conducted on two database systems using mysqlslap benchmark and a comparative study with other methods is done.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Algorithms}, {Computing} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Sowah, Edmund and Xu, Jianqiu},
	year = {2018},
	note = {event-place: Sanya, China},
	keywords = {Cooperative Query Answering, Natural Language, Query Language, Query syntax, Query-By-Form},
}

@inproceedings{pattar_soco-its_2020,
	address = {New York, NY, USA},
	series = {{ICIT} '19},
	title = {{SoCo}-{ITS}: {Service} {Oriented} {Context} {Ontology} for {Intelligent} {Transport} {System}},
	isbn = {978-1-4503-7663-1},
	url = {https://doi.org/10.1145/3377170.3377274},
	doi = {10.1145/3377170.3377274},
	abstract = {Intelligent Transport System (ITS) is a culmination of technological and application systems that are contrived to improve the performance of road transportation and upgrade the commuter's experience. The integration of Internet of Things (IoT) with the transport system has contributed to the development of ITS. In this paper, we concentrate on the commercial servitization standpoint of the application. We structure and formulate an ontology called Service-Oriented Context Ontology for Intelligent Transport System: SoCo-ITS. This ontological framework abets in identifying appropriate services required by the commuters in transit based on their situation, predilection and ITS environmental information. We discuss the detailed implementation description and also accentuate its role in ITS through a use case scenario and an exemplar application portraying the importance of the proposed ontological model.},
	booktitle = {Proceedings of the 2019 7th {International} {Conference} on {Information} {Technology}: {IoT} and {Smart} {City}},
	publisher = {Association for Computing Machinery},
	author = {Pattar, Santosh and Sandhya, C. R. and Vala, Darshil and Buyya, Rajkumar and Venugopal, K. R. and Iyenger, S. S. and Patnaik, L. M.},
	year = {2020},
	note = {event-place: Shanghai, China},
	keywords = {Ontology, Internet of Things, Service Discovery, Intelligent Transport Systems, User Centric Services},
	pages = {503--508},
}

@inproceedings{alfaifi_ontology_2018,
	address = {New York, NY, USA},
	series = {{DH} '18},
	title = {An {Ontology} of {Psychological} {Barriers} to {Support} {Behaviour} {Change}},
	isbn = {978-1-4503-6493-5},
	url = {https://doi.org/10.1145/3194658.3194680},
	doi = {10.1145/3194658.3194680},
	abstract = {Helping people to adopt and maintain healthier lifestyles is a primary goal of behaviour change interventions. Successful interventions need to account for different barriers (informational, environmental, or psychological) that prevent people from engaging in healthy behaviours. Computational approaches to modelling these interventions focus primarily on informational needs, or on persuasive techniques. The study presented in this paper is specifically aimed at creating a formal conceptual model of the psychological notion of barriers to healthy behaviour, by means of an ontology,i.e. an explicit and machine readable specification of a conceptualisation shared by all the stakeholders citeStuder-et-al98. The model accounts for other related patient concepts to understand patient behaviour better. This machine-readable knowledge can function as a background to finding the right interventions for behaviour change. Whilst the model is generic and expandable to include other diseases and behaviours, our study uses type 2 diabetes to contextualise the problem of behaviour change.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Digital} {Health}},
	publisher = {Association for Computing Machinery},
	author = {Alfaifi, Yousef and Grasso, Floriana and Tamma, Valentina},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {behaviour change ontology, behaviour ontology, physical activity behaviour, type 2 diabetes},
	pages = {11--15},
}

@inproceedings{wilczynski_concept-centric_2023,
	address = {New York, NY, USA},
	series = {Onward! 2023},
	title = {Concept-{Centric} {Software} {Development}: {An} {Experience} {Report}},
	isbn = {979-8-4007-0388-1},
	url = {https://doi.org/10.1145/3622758.3622894},
	doi = {10.1145/3622758.3622894},
	abstract = {Developers have long recognized the importance of the concepts underlying the systems they build, and the primary role that concepts play in shaping user experience. To date, however, concepts have tended to be only implicit in software design with development being organized instead around more concrete artifacts (such as wireframes and code modules). Palantir, a software company whose data analytics products are widely used by major corporations, recently reworked the internal representation of its software development process to bring concepts to the fore, making explicit the concepts underlying its products, including how they are clustered together, used in applications, and governed by teams. With a centralized repository of concepts, Palantir engineers are able to align products more closely based on shared concepts, evolve concepts in response to user needs, and communicate more effectively with non-engineering groups within the company. This paper reports on Palantir's experiences to date, analyzing both successes and challenges, and offers advice to other organizations considering adopting a concept-centric approach to software development.},
	booktitle = {Proceedings of the 2023 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {Association for Computing Machinery},
	author = {Wilczynski, Peter and Gregoire-Wright, Taylor and Jackson, Daniel},
	year = {2023},
	note = {event-place: Cascais, Portugal},
	keywords = {ontology, concepts, software design},
	pages = {120--135},
}

@article{zen_quality_2023,
	title = {A {Quality} {Model}-based {Approach} for {Measuring} {User} {Interface} {Aesthetics} with {Grace}},
	volume = {7},
	url = {https://doi.org/10.1145/3593224},
	doi = {10.1145/3593224},
	abstract = {User interface aesthetics, a particular sub-characteristic of the ISO 25010 software quality model, is correlated to the perceived or actual usability of a graphical user interface, its user experience, and trust. While many measures, such as balance, symmetry, proportion, alignment, regularity, and simplicity, can be computed, no consensus exists today on which measure to adopt, which formula to compute for each measure, and which interpretation to give for each computed formula. To accommodate these variations and to make the assessment explicit and interpretable, we present a quality model-based approach for measuring aesthetics that defines a quality function in terms of the formula of the measures, their weights, their composition, and their overall computation. This quality model is transformed into a configuration used by GRACE, a web application developed for this purpose. When the measures, their formula, their weights, or the quality function change, the quality model changes, which is re-computed to compare it with any other model, thus making the measurement process explicit and interpretable. We apply this approach to a large dataset "Lab in the Wild” to investigate the correlations between aesthetic measures and perceived visual complexity and colorfulness. We discuss limitations through threats to validity.},
	number = {EICS},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Zen, Mathieu and Burny, Nicolas and Vanderdonckt, Jean},
	month = jun,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {aesthetics, graphical user interfaces, software measurement, usability engineering, user interface evaluation, visual measures, visual techniques},
}

@inproceedings{blin_okg_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {{OKG}: {A} {Knowledge} {Graph} for {Fine}-grained {Understanding} of {Social} {Media} {Discourse} on {Inequality}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627557},
	doi = {10.1145/3587259.3627557},
	abstract = {In recent years, social media platforms such as Twitter have allowed people to voice their opinions by engaging in online discussions. The availability of such discussions has garnered interest amongst researchers in analyzing the dynamics on critical topics, such as inequality. Most of the current strategies are, however, limited with respect to conveying the fine-grained opinions of users, focusing on tasks such as sentiment analysis or topic modeling that extract coarse categorizations. In this work, we address this challenge by integrating a Twitter corpus with the output of finer-grained semantic parsing for the analysis of social media discourse. To do so, we first introduce the OBservatory Integrated Ontology (OBIO) that integrates social media metadata with various types of linguistic knowledge. We then present the Observatory Knowledge Graph (OKG), a knowledge graph in terms of the ontology, populated with tweets on inequality. We lastly provide use cases showing how the knowledge graph can be used as the backbone of a social media observatory, to facilitate a deeper understanding of social media discourse.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Blin, Inès and Stork, Lise and Spillner, Laura and Santagiustina, Carlo},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {Ontology Engineering and Population, Social Media Discourse},
	pages = {166--174},
}

@inproceedings{jradeh_graph_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Graph {Embeddings} {Meet} {Link} {Keys} {Discovery} for {Entity} {Matching}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714581},
	doi = {10.1145/3696410.3714581},
	abstract = {Entity Matching (EM) automates the discovery of identity links between entities within different Knowledge Graphs (KGs). Link keys are crucial for EM, serving as rules allowing to identify identity links across different KGs, possibly described using different ontologies. However, the approach for extracting link keys struggles to scale on large KGs. While embedding-based EM methods efficiently handle large KGs they lack explainability. This paper proposes a novel hybrid EM approach to guarantee the scalability link key extraction approach and improve the explainability of embedding-based EM methods. First, embedding-based EM approaches are used to sample the KGs based on the identity links they generate, thereby reducing the search space to relevant sub-graphs for link key extraction. Second, rules (in the form of link keys) are extracted to explain the generation of identity links by the embedding-based methods. Experimental results demonstrate that the proposed approach allows link key extraction to scale on large KGs, preserving the quality of the extracted link keys. Additionally, it shows that link keys can improve the explainability of the identity links generated by embedding-methods, allowing for the regeneration of 77\% of the identity links produced for a specific EM task, thereby providing an approximation of the reasons behind their generation.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Jradeh, Chloé Khadija and Raoufi, Ensiyeh and David, Jérôme and Larmande, Pierre and Scharffe, François and Todorov, Konstantin and Trojahn, Cassia},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {Knowledge graphs, Language model, knowledge graphs, language models, Hybrid AI, Embeddings, Entity matching, Graph embeddings, entity matching, embedding-based em, graph embeddings, hybrid ai, link keys, symbolic em, Embedding-based entity matching, Link key, Symbolic entity matching},
	pages = {3344--3353},
	annote = {Cited by: 0; All Open Access; Green Final Open Access; Green Open Access},
}

@article{murano_describing_2023,
	title = {Describing {Inscriptions} of {Ancient} {Italy}. {The} {ItAnt} {Project} and {Its} {Information} {Encoding} {Process}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3606703},
	doi = {10.1145/3606703},
	abstract = {This article discusses the challenges addressed in the digital scholarly encoding of the fragmentary texts of the languages of Ancient Italy according to the TEI/EpiDoc Guidelines in XML format. It describes the solutions and customisations that have been adopted for dealing with the peculiarities of our epigraphical documentation and with the formalisation of epigraphical information deemed interesting for data retrieval in a historical linguistic perspective. The making of a digital corpus consisting of new critical editions of selected inscriptions is a work carried out in the context of the project “Languages and Cultures of Ancient Italy. Historical Linguistics and Digital Models”, which aims to investigate the languages of Ancient Italy by combining the traditional methods, proper to historical linguistics, with methods and technology proper to the digital humanities and computational lexicography. More specifically, the purpose of the project is to create a set of interrelated digital language resources which comprise: (1) a digital corpus of texts editions; (2) a computational lexicon compliant with the Web Semantic requirements; (3) a relevant bibliographic reference dataset encoded according to the FRBRoo/LRMoo specifications. Additionally, selected textual data and scientific interpretations will be encoded using CIDOC CRM and its extensions, namely CRMtex and CRMinf. The present contribution thus tackles one of the main aspects of the project, and proposes significant innovations in the encoding of critical editions for epigraphic texts of fragmentary languages, which will hopefully foster future interoperability and integration with other external datasets, a paramount concern of the project.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Murano, Francesca and Quochi, Valeria and Del Grosso, Angelo Mario and Rigobianco, Luca and Zinzi, Mariarosaria},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Text encoding, ancient languages, digital epigraphy, TEI/EpiDoc},
}

@inproceedings{liu_effective_2022,
	address = {New York, NY, USA},
	series = {{SAC} '22},
	title = {Effective use of {BERT} in graph embeddings for sparse knowledge graph completion},
	isbn = {978-1-4503-8713-2},
	url = {https://doi.org/10.1145/3477314.3507031},
	doi = {10.1145/3477314.3507031},
	abstract = {Graph embedding methods have emerged as effective solutions for knowledge graph completion. However, such methods are typically tested on benchmark datasets such as Freebase, but show limited performance when applied on sparse knowledge graphs with orders of magnitude lower density. To compensate for the lack of structure in a sparse graph, low dimensional representations of textual information such as word2vec or BERT embeddings have been used. This paper proposes a BERT-based method (BERT-ConvE), to exploit transfer learning of BERT in combination with a convolutional network model ConvE. Comparing to existing text-aware approaches, we effectively make use of the context dependency of BERT embeddings through optimizing the features extraction strategies. Experiments on ConceptNet show that the proposed method outperforms strong baselines by 50\% on knowledge graph completion tasks. The proposed method is suitable for sparse graphs as also demonstrated by empirical studies on ATOMIC and sparsified-FB15k-237 datasets. Its effectiveness and simplicity make it appealing for industrial applications.},
	booktitle = {Proceedings of the 37th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Xinglan and Hussain, Hussain and Razouk, Houssam and Kern, Roman},
	year = {2022},
	note = {event-place: Virtual Event},
	keywords = {BERT, language model, knowledge graph embedding, context aware embedding, sparse knowledge graph},
	pages = {799--802},
}

@article{paul_new_2021,
	title = {A {New} {Family} of {Similarity} {Measures} for {Scoring} {Confidence} of {Protein} {Interactions} {Using} {Gene} {Ontology}},
	volume = {19},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2021.3083150},
	doi = {10.1109/TCBB.2021.3083150},
	abstract = {The large-scale protein-protein interaction (PPI) data has the potential to play a significant role in the endeavor of understanding cellular processes. However, the presence of a considerable fraction of false positives is a bottleneck in realizing this potential. There have been continuous efforts to utilize complementary resources for scoring confidence of PPIs in a manner that false positive interactions get a low confidence score. Gene Ontology (GO), a taxonomy of biological terms to represent the properties of gene products and their relations, has been widely used for this purpose. We utilize GO to introduce a new set of specificity measures: Relative Depth Specificity (RDS), Relative Node-based Specificity (RNS), and Relative Edge-based Specificity (RES), leading to a new family of similarity measures. We use these similarity measures to obtain a confidence score for each PPI. We evaluate the new measures using four different benchmarks. We show that all the three measures are quite effective. Notably, RNS and RES more effectively distinguish true PPIs from false positives than the existing alternatives. RES also shows a robust set-discriminating power and can be useful for protein functional clustering as well.},
	number = {1},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Paul, Madhusudan and Anand, Ashish},
	month = may,
	year = {2021},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {19--30},
}

@inproceedings{el-ansari_improved_2020,
	address = {New York, NY, USA},
	series = {{NISS} '20},
	title = {An improved modeling method for profile-based personalized search},
	isbn = {978-1-4503-7634-1},
	url = {https://doi.org/10.1145/3386723.3387874},
	doi = {10.1145/3386723.3387874},
	abstract = {A Personalized search system aims to provide tailor-made results for each user's query according to his preferences. Building such a system depends mainly on creating profiles that represent real user interests. In this paper, we present a method for modeling users and creating accurate profiles by implicitly tracking and collecting user-browsing data. Furthermore, we examine techniques to enhance profile accuracy through combining multiple browsing data sources, distinguishing important concepts from irrelevant ones in a user profile, and the concept levels required from a reference ontology to describe user's interests.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Networking}, {Information} {Systems} \&amp; {Security}},
	publisher = {Association for Computing Machinery},
	author = {El-Ansari, Anas and Beni-Hssane, Abderrahim and Saadi, Mostafa},
	year = {2020},
	note = {event-place: Marrakech, Morocco},
	keywords = {Ontology, User profile, Accuracy, Personalized Search},
}

@article{liao_improving_2023,
	title = {Improving {Readability} for {Automatic} {Speech} {Recognition} {Transcription}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3557894},
	doi = {10.1145/3557894},
	abstract = {Modern Automatic Speech Recognition (ASR) systems can achieve high performance in terms of recognition accuracy. However, a perfectly accurate transcript still can be challenging to read due to grammatical errors, disfluency, and other noises common in spoken communication. These readable issues introduced by speakers and ASR systems will impair the performance of downstream tasks and the understanding of human readers. In this work, we present a task called ASR post-processing for readability (APR) and formulate it as a sequence-to-sequence text generation problem. The APR task aims to transform the noisy ASR output into a readable text for humans and downstream tasks while maintaining the semantic meaning of speakers. We further study the APR task from the benchmark dataset, evaluation metrics, and baseline models: First, to address the lack of task-specific data, we propose a method to construct a dataset for the APR task by using the data collected for grammatical error correction. Second, we utilize metrics adapted or borrowed from similar tasks to evaluate model performance on the APR task. Lastly, we use several typical or adapted pre-trained models as the baseline models for the APR task. Furthermore, we fine-tune the baseline models on the constructed dataset and compare their performance with a traditional pipeline method in terms of proposed evaluation metrics. Experimental results show that all the fine-tuned baseline models perform better than the traditional pipeline method, and our adapted RoBERTa model outperforms the pipeline method by 4.95 and 6.63 BLEU points on two test sets, respectively. The human evaluation and case study further reveal the ability of the proposed model to improve the readability of ASR transcripts.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Liao, Junwei and Eskimez, Sefik and Lu, Liyang and Shi, Yu and Gong, Ming and Shou, Linjun and Qu, Hong and Zeng, Michael},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Automatic speech recognition, data synthesis, post-processing for readability, pre-trained model},
}

@inproceedings{guerra_quest_2018,
	address = {New York, NY, USA},
	series = {{MODELS} '18},
	title = {On the {Quest} for {Flexible} {Modelling}},
	isbn = {978-1-4503-4949-9},
	url = {https://doi.org/10.1145/3239372.3239376},
	doi = {10.1145/3239372.3239376},
	abstract = {Modelling is a fundamental activity in Software Engineering, and central to model-based engineering approaches. It is used for different purposes, and so its nature can range from informal (e.g., as a casual mechanism for problem discussion and understanding) to fully formal (e.g., to enable the automated processing of models by model transformations). However, existing modelling tools only serve one of these two extreme purposes: either to create informal drawings or diagrams, or to build models fully conformant to their modelling language. This lack of reconciliation is hampering the adoption of model-based techniques in practice, as they are deemed too imprecise in the former case, and too rigid in the latter.In this new ideas paper, we claim that modelling tools need further flexibility covering different stages, purposes and approaches to modelling. We detail requirements for such a new generation of modelling tools, describe our first steps towards their realization in the Kite metamodelling tool, and showcase application scenarios.},
	booktitle = {Proceedings of the 21th {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Guerra, Esther and de Lara, Juan},
	year = {2018},
	note = {event-place: Copenhagen, Denmark},
	keywords = {Model-driven engineering, Modelling process, Flexible modelling},
	pages = {23--33},
}

@inproceedings{reynolds_automated_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Automated provenance graphs for models@run.time},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3419503},
	doi = {10.1145/3417990.3419503},
	abstract = {Software systems are increasingly making decisions autonomously by incorporating AI and machine learning capabilities. These systems are known as self-adaptive and autonomous systems (SAS). Some of these decisions can have a life-changing impact on the people involved and therefore, they need to be appropriately tracked and justified: the system should not be taken as a black box. It is required to be able to have knowledge about past events and records of history of the decision making. However, tracking everything that was going on in the system at the time a decision was made may be unfeasible, due to resource constraints and complexity. In this paper, we propose an approach that combines the abstraction and reasoning support offered by models used at runtime with provenance graphs that capture the key decisions made by a system through its execution. Provenance graphs relate the entities, actors and activities that take place in the system over time, allowing for tracing the reasons why the system reached its current state. We introduce activity scopes, which highlight the high-level activities taking place for each decision, and reduce the cost of instrumenting a system to automatically produce provenance graphs of these decisions. We demonstrate a proof of concept implementation of our proposal across two case studies, and present a roadmap towards a reusable provenance layer based on the experiments.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Reynolds, Owen and García-Domínguez, Antonio and Bencomo, Nelly},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {provenance, self-explanation, runtime models, autonomous decision-making, PROV-DM},
}

@inproceedings{gao_improving_2018,
	address = {New York, NY, USA},
	series = {{ICBCB} 2018},
	title = {Improving {Medical} {Ontology} {Based} on {Word} {Embedding}},
	isbn = {978-1-4503-6348-8},
	url = {https://doi.org/10.1145/3194480.3194490},
	doi = {10.1145/3194480.3194490},
	abstract = {Medical ontology learning or improving is automatically learning the knowledge in ontology format from medical data, mainly text data. With the rise of the word vector space, improving ontology using word embedding has become a hot spot. Most of previous studies have focused on how to acquire different ontological elements using all kinds of learning technologies. Few studies focus on the prior knowledge in a given ontology. In essence, ontology learning or improving is still a learning process based on existing samples. So, the type and number of knowledge acquired is limited by existing samples in a given ontology. This paper firstly formalizes several kinds of prior knowledge for classes in a given ontology. Then we propose a method, named improving medical ontology based on word embeddings (IMO-WE), to enrich different types of knowledge from medical text according to characteristics of different types of prior knowledge. At last, the paper collects the PubMed Central (PMC) data and the PHARE ontology, and finishes a series of experiments to evaluate the IMO-WE. The experimental results yield the following conclusions. The first one is that the data-rich model can achieve higher accuracy for the IMO-WE under same setting in training progress. So, collecting and training big medical data is a viable way to learn more useful knowledge. The second one is that the IMO-WE can be used to improving ontology knowledge when medical data is sufficiently abundant and the ontology has appropriate prior knowledge. Moreover, in the task of improving synonymous labels through similarity distance, the accuracy of IMO-WE is significantly better than that of the Random indexing method.},
	booktitle = {Proceedings of the 2018 6th {International} {Conference} on {Bioinformatics} and {Computational} {Biology}},
	publisher = {Association for Computing Machinery},
	author = {Gao, Mingxia and Chen, Furong and Wang, Rifeng},
	year = {2018},
	note = {event-place: Chengdu, China},
	keywords = {word embedding, medical ontology improving, prior knowledge},
	pages = {121--127},
}

@inproceedings{bouihi_moodles_2018,
	address = {New York, NY, USA},
	series = {{LOPAL} '18},
	title = {Moodle's {Ontology} {Development} from {UML} for {Social} {Learning} {Network} {Analysis}},
	isbn = {978-1-4503-5304-5},
	url = {https://doi.org/10.1145/3230905.3230933},
	doi = {10.1145/3230905.3230933},
	abstract = {The online learning called e-learning is a new learning path that offers to learners to study at their own pace and at the moments that suit them. It is in this perspective that the semantic web has known its emergence in the field of e-learning to offer platforms content more personalized and more adapted to student's and teacher's needs. Since Moodle is the most popular e-learning platform, we propose in this paper to build its OWL ontology by exploring the representative data that we collected from its UML class diagram. The choice of UML class diagram as a basis for data collection for the development of the ontology is justified by the fact that the transition from UML to OWL ontology brings ontology development process closer to the wider software engineering population. The built ontology brings also great benefit in the field of the Social Learning Network Analysis. Because it gives the opportunity to study the behavior of the platform users by giving meaning to their relationships instead of modelling them only as knots and edges.},
	booktitle = {Proceedings of the {International} {Conference} on {Learning} and {Optimization} {Algorithms}: {Theory} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Bouihi, Bouchra and Bahaj, Mohamed},
	year = {2018},
	note = {event-place: Rabat, Morocco},
	keywords = {Ontology, Semantic web, OWL, E-learning, Moodle, Social Network Analysis, UML},
}

@article{de_giacomo_instance-level_2021,
	title = {Instance-{Level} {Update} in {DL}-{Lite} {Ontologies} through {First}-{Order} {Rewriting}},
	volume = {70},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.12414},
	doi = {10.1613/jair.1.12414},
	abstract = {In this paper we study instance-level update in DL-LiteA , a well-known description logic that influenced the OWL 2 QL standard. Instance-level update regards insertions and deletions in the ABox of an ontology. In particular we focus on formula-based approaches to instance-level update. We show that DL-LiteA , which is well-known for enjoying first-order rewritability of query answering, enjoys a first-order rewritability property also for instance-level update. That is, every update can be reformulated into a set of insertion and deletion instructions computable through a non-recursive Datalog program with negation. Such a program is readily translatable into a first-order query over the ABox considered as a database, and hence into SQL. By exploiting this result, we implement an update component for DL-LiteA-based systems and perform some experiments showing that the approach works in practice.},
	journal = {J. Artif. Int. Res.},
	author = {De Giacomo, Giuseppe and Oriol, Xavier and Rosati, Riccardo and Savo, Domenico Fabio},
	month = may,
	year = {2021},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
	pages = {1335--1371},
}

@inproceedings{corradini_x-iot_2022,
	address = {New York, NY, USA},
	series = {{SAC} '22},
	title = {X-{IoT}: a model-driven approach for cross-platform {IoT} applications development},
	isbn = {978-1-4503-8713-2},
	url = {https://doi.org/10.1145/3477314.3507164},
	doi = {10.1145/3477314.3507164},
	abstract = {Several heterogeneous IoT platforms have been proposed and regularly used by enterprises and academies to support and facilitate IoT software applications development. However, IoT applications strongly depend on the functionalities supported by the specific platform used. This affects the development and portability of the developed applications that may require significant changes, or a complete re-design, for being migrated between platforms. This paper presents X-IoT, an MDE approach for developing cross-platform IoT applications. The approach implements a Domain-Specific Modelling Language (DSML) based on emerging IoT application requirements. A meta-model that incorporates the main IoT platform characteristics has been developed within the ADOxx platform, together with a graphical notation. Through the DSML, it is possible to model a platform-independent model of IoT applications that are refined and deployed on specific IoT platforms.},
	booktitle = {Proceedings of the 37th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Corradini, Flavio and Fedeli, Arianna and Fornari, Fabrizio and Polini, Andrea and Re, Barbara},
	year = {2022},
	note = {event-place: Virtual Event},
	keywords = {internet of things, IoT platform, model driven engineering, cross-platform, IoT application development},
	pages = {1448--1451},
}

@inproceedings{el_orche_approach_2019,
	address = {New York, NY, USA},
	series = {{NISS} '19},
	title = {Approach to use ontology based on electronic payment system and machine learning to prevent {Fraud}},
	isbn = {978-1-4503-6645-8},
	url = {https://doi.org/10.1145/3320326.3320369},
	doi = {10.1145/3320326.3320369},
	abstract = {Machine learning is a field of study of artificial intelligence that is based on statistical approaches to give computers the ability to learn from data. An ontology is the structured set of terms and concepts representing the meaning of a field of information, whether by the metadata of a namespace, or the elements of a domain of knowledge. in this paper we propose an approach to combine a machine learning with an ontology based on an electronic payment system and another ontology generated automatically to prevent and fight the risks of fraud.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Networking}, {Information} {Systems} \&amp; {Security}},
	publisher = {Association for Computing Machinery},
	author = {El Orche, Ahmed and Bahaj, Mohamed},
	year = {2019},
	note = {event-place: Rabat, Morocco},
	keywords = {Ontology, Machine learning, Electronic payment system, Fraud},
}

@inproceedings{kaoutar_global_2019,
	address = {New York, NY, USA},
	series = {{SCA} '19},
	title = {Global similarity based ontology to enhance the quality of big and distributed {RDF} data},
	isbn = {978-1-4503-6289-4},
	url = {https://doi.org/10.1145/3368756.3369092},
	doi = {10.1145/3368756.3369092},
	abstract = {Nowadays, the web content of e-commerce data is increasing rapidly, which make the traditional techniques to querying this resources not efficient, for that the researches focus to how using the new technologies to provide a relevant and complete answers to user query. Using Technologies of big data and web semantic are two new fields that can be exploiting to processes data semantically and to handle with storage of this hug data.In recent works [1, 2], we have proposed the techniques using in big data and we are proposed in architecture that integrate the big RDF (Resources Description Framework) data semantically by exploiting HDFS (Hadoop Distributed File System) to store Global RDF schema and Map Reduce to process the query, in aims to give an infrastructure who give a complete and pertinent answers to user query. In this paper we are proposed a simple scenario to have a complete and pertinent response to user query.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Smart} {City} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Kaoutar, Lamrani and Abderrahim, Ghadi},
	year = {2019},
	note = {event-place: Casablanca, Morocco},
	keywords = {ontology, semantic web, big data},
}

@inproceedings{turki_data_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Data {Models} for {Annotating} {Biomedical} {Scholarly} {Publications}: the {Case} of {CORD}-19},
	isbn = {978-1-4503-9130-6},
	url = {https://doi.org/10.1145/3487553.3524675},
	doi = {10.1145/3487553.3524675},
	abstract = {Semantic text annotations have been a key factor for supporting computer applications ranging from knowledge graph construction to biomedical question answering. In this systematic review, we provide an analysis of the data models that have been applied to semantic annotation projects for the scholarly publications available in the CORD-19 dataset, an open database of the full texts of scholarly publications about COVID-19. Based on Google Scholar and the screening of specific research venues, we retrieve seventeen publications on the topic mostly from the United States of America. Subsequently, we outline and explain the inline semantic annotation models currently applied on the full texts of biomedical scholarly publications. Then, we discuss the data models currently used with reference to semantic annotation projects on the CORD-19 dataset to provide interesting directions for the development of semantic annotation models and projects.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Turki, Houcemeddine and Hadj Taieb, Mohamed Ali and Piad-Morffis, Alejandro and Ben Aouicha, Mohamed and Bile, René Fabrice},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {CORD-19, Semantic relations, Semantic annotations, Annotation models, Named entity annotation, Semantic relation annotation},
	pages = {740--750},
}

@inproceedings{selvaraj_tkm_2020,
	address = {New York, NY, USA},
	series = {{ICSIM} '20},
	title = {{TKM} {Ontology} {Integration} and {Visualization}},
	isbn = {978-1-4503-7690-7},
	url = {https://doi.org/10.1145/3378936.3378976},
	doi = {10.1145/3378936.3378976},
	abstract = {Ontology is the most efficient way of representing knowledge and influence relationship about diseases, symptoms, medications, and diagnosis in the traditional medical field. Since integrating traditional medicine ontologies with modern medical ontologies can benefit to the treatment process for effectively using traditional medicines, there is a need for integration and visualization of traditional medicine ontology. Furthermore, an effective ontology visualization is useful to design, manage, and browse the traditional medicine ontology successfully. In this paper, we construct the traditional Korean medicine (TKM) ontology using TKM domain knowledge with a few imported classes from modern medicine ontology and traditional Chinese medicine ontology (TCM) and also visualize the TKM using Web-based Visualization of Ontologies (WebVowl).},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Software} {Engineering} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Selvaraj, Suganya and Choi, Eunmi},
	year = {2020},
	note = {event-place: Sydney, NSW, Australia},
	keywords = {medical ontology virtualization, TKM ontology integration, Traditional Korean medicine ontology},
	pages = {146--149},
}

@inproceedings{kaar_resilient_2018,
	address = {New York, NY, USA},
	series = {S-{BPM} {One} '18},
	title = {Resilient {Ontology} {Support} {Facilitating} {Multi}-{Perspective} {Process} {Integration} in {Industry} 4.0},
	isbn = {978-1-4503-5360-1},
	url = {https://doi.org/10.1145/3178248.3178253},
	doi = {10.1145/3178248.3178253},
	abstract = {A major challenge for Industry 4.0 organizations is the mutual alignment of automation and information technology while increasing effectiveness and agility of processes. From a technological view, it requires architectures and systems coupling heterogeneous technologies, from an operations perspective, it requires context-sensitive representations. Ontologies do not only support alignment, but also integration and development processes. For the introduced ontology we utilize the multi-perspective RAMI4.0 framework, as it provides several layers and perspectives, including production and business processes. We suggest using Subject-oriented Business Process Management (S-BPM) models to represent executable processes, as they allow encapsulating industry standard-conform as well as stakeholder behavior. Thereby, the ontology backs perspective specific knowledge, and can be adapted as semantic baseline in a flexible way.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Subject}-{Oriented} {Business} {Process} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Kaar, Claudia and Frysak, Josef and Stary, Christian and Kannengiesser, Udo and Müller, Harald},
	year = {2018},
	note = {event-place: Linz, Austria},
	keywords = {Ontology engineering, Industry 4.0, semantic interoperability, resilience, process integration, RAMI4.0},
}

@inproceedings{demchenko_designing_2019,
	address = {New York, NY, USA},
	series = {{ICBDE} '19},
	title = {Designing {Customisable} {Data} {Science} {Curriculum} {Using} {Ontology} for {Data} {Science} {Competences} and {Body} of {Knowledge}},
	isbn = {978-1-4503-6186-6},
	url = {https://doi.org/10.1145/3322134.3322143},
	doi = {10.1145/3322134.3322143},
	abstract = {Importance of Data Science education and training is growing with the emergence of data driven technologies and organisational culture that intend to derive actionable value for improving research process or enterprise business using variety of enterprise data and widely available open and social media data. Modern data driven research and industry require new types of specialists that are capable to support all stages of the data lifecycle from data production and input to data processing and actionable results delivery, visualisation and reporting, which can be jointly defined as the Data Science professions family. The education and training of Data Scientists requires multi-disciplinary approach combining wide view of the Data Science and Analytics foundation with deep practical knowledge in domain specific areas. In modern conditions with the fast technology change and strong skills demand, the Data Science education and training should be customizable and delivered in multiple form, also providing sufficient data labs facilities for practical training. This paper discusses approach to building customizable Data Science curriculum for different types of learners based on using the ontology of the EDISON Data Science Framework (EDSF) developed in the EU funded Project EDISON and widely used by universities and professional training organisations.},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Big} {Data} and {Education}},
	publisher = {Association for Computing Machinery},
	author = {Demchenko, Yuri and Comminiello, Luca and Reali, Gianluca},
	year = {2019},
	note = {event-place: London, United Kingdom},
	keywords = {Big Data, Data Science, Data Science Body of Knowledge, Data Science Competences Framework, Data Science Model Curriculum, Data Science Ontology, Data Scientist Professional, EDISON Data Science Framework (EDSF)},
	pages = {124--128},
}

@inproceedings{zulfiya_model_2019,
	address = {New York, NY, USA},
	series = {{DATA} '19},
	title = {A model and a method for assessing students' competencies in e-learning system},
	isbn = {978-1-4503-7284-8},
	url = {https://doi.org/10.1145/3368691.3372391},
	doi = {10.1145/3368691.3372391},
	abstract = {This article discusses a model and a method for assessing students' competencies in e-learning system, verification of fulfillment of the educational program goals and formation of a graduate with competencies set as a goal at the entrance. It also offers assessment at the level of a discipline, a module and verification of the achievement of the educational program goals.},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Data} {Science}, {E}-{Learning} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zulfiya, Kaderkeyeva and Gulmira, Bekmanova and Altynbek, Sharipbay and Assel, Omarbekova},
	year = {2019},
	note = {event-place: Dubai, United Arab Emirates},
	keywords = {artificial intelligence, ontology, knowledge, knowledge base, evaluation, e-learning, knowledge models, logic, sets, competencies},
}

@inproceedings{cuffy_exploring_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Exploring {Representations} for {Singular} and {Multi}-{Concept} {Relations} for {Biomedical} {Named} {Entity} {Normalization}},
	isbn = {978-1-4503-9130-6},
	url = {https://doi.org/10.1145/3487553.3524701},
	doi = {10.1145/3487553.3524701},
	abstract = {Since the rise of the COVID-19 pandemic, peer-reviewed biomedical repositories have experienced a surge in chemical and disease related queries. These queries have a wide variety of naming conventions and nomenclatures from trademark and generic, to chemical composition mentions. Normalizing or disambiguating these mentions within texts provides researchers and data-curators with more relevant articles returned by their search query. Named entity normalization aims to automate this disambiguation process by linking entity mentions onto their appropriate candidate concepts within a biomedical knowledge base or ontology. We explore several term embedding aggregation techniques in addition to how the term’s context affects evaluation performance. We also evaluate our embedding approaches for normalizing term instances containing one or many relations within unstructured texts.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Cuffy, Clint and French, Evan and Fehrmann, Sophia and McInnes, Bridget T.},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {transformer, word embeddings, named entity normalization, entity linking, entity normalization, neural networks, datasets, concept linking, concept mapping, concept normalization, concept unique identifier, MeSH identifier, named entity disambiguation, named entity linking},
	pages = {823--832},
}

@article{zhao_are_2022,
	title = {Are {Topics} {Interesting} or {Not}? {An} {LDA}-based {Topic}-graph {Probabilistic} {Model} for {Web} {Search} {Personalization}},
	volume = {40},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3476106},
	doi = {10.1145/3476106},
	abstract = {In this article, we propose a Latent Dirichlet Allocation– (LDA) based topic-graph probabilistic personalization model for Web search. This model represents a user graph in a latent topic graph and simultaneously estimates the probabilities that the user is interested in the topics, as well as the probabilities that the user is not interested in the topics. For a given query issued by the user, the webpages that have higher relevancy to the interested topics are promoted, and the webpages more relevant to the non-interesting topics are penalized. In particular, we simulate a user’s search intent by building two profiles: A positive user profile for the probabilities of the user is interested in the topics and a corresponding negative user profile for the probabilities of being not interested in the the topics. The profiles are estimated based on the user’s search logs. A clicked webpage is assumed to include interesting topics. A skipped (viewed but not clicked) webpage is assumed to cover some non-interesting topics to the user. Such estimations are performed in the latent topic space generated by LDA. Moreover, a new approach is proposed to estimate the correlation between a given query and the user’s search history so as to determine how much personalization should be considered for the query. We compare our proposed models with several strong baselines including state-of-the-art personalization approaches. Experiments conducted on a large-scale real user search log collection illustrate the effectiveness of the proposed models.},
	number = {3},
	journal = {ACM Trans. Inf. Syst.},
	author = {Zhao, Jiashu and Huang, Jimmy Xiangji and Deng, Hongbo and Chang, Yi and Xia, Long},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Personalization, Latent Dirichlet Allocation (LDA), probabilistic model, topic-graph, Web search},
}

@inproceedings{yang_toward_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Toward {Real}-life {Dialogue} {State} {Tracking} {Involving} {Negative} {Feedback} {Utterances}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539385},
	doi = {10.1145/3534678.3539385},
	abstract = {Recently, the research of dialogue systems has been widely concerned, especially task-oriented dialogue systems, which have received increased attention due to their wide application prospect. As a core component, dialogue state tracking (DST) plays a key role in task-oriented dialogue systems, and its function is to parse natural language dialogues into dialogue state formed by slot-value pairs. It is well known that dialogue state tracking has been well studied and explored on current benchmark datasets such as the MultiWOZ. However, almost all current research completely ignores the user negative feedback utterances that exist in real-life conversations when a system error occurs, which often contains user-provided corrective information for the system error. Obviously, user negative feedback utterances can be used to correct the inevitable errors in automatic speech recognition and model generalization. Thus, in this paper, we will explore the role of negative feedback utterances in dialogue state tracking in detail through simulated negative feedback utterances. Specifically, due to the lack of dataset involving negative feedback utterances, first, we have to define the schema of user negative feedback utterances and propose a joint modeling method for feedback utterance generation and filtering. Then, we explore three aspects of interaction mechanism that should be considered in real-life conversations involving negative feedback utterances and propose evaluation metrics related to negative feedback utterances. Finally, on WOZ2.0 and MultiWOZ2.1 datasets, by constructing simulated negative feedback utterances in training and testing, we not only verify the important role of negative feedback utterances in dialogue state tracking, but also analyze the advantages and disadvantages of different interaction mechanisms involving negative feedback utterances, lighting future research on negative feedback utterances.},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Puhai and Huang, Heyan and Wei, Wei and Mao, Xian-Ling},
	year = {2022},
	note = {event-place: Washington DC, USA},
	keywords = {dialogue state tracking, negative feedback, real-life dialogue},
	pages = {2222--2232},
}

@inproceedings{xiao_event_2022,
	address = {New York, NY, USA},
	series = {{ICETT} '22},
	title = {Event {Graph} {Construction} {Based} on {Disciplinary} {Procedural} {Knowledge}: {Concept} {Model} and {Application}},
	isbn = {978-1-4503-9697-4},
	url = {https://doi.org/10.1145/3535756.3535770},
	doi = {10.1145/3535756.3535770},
	abstract = {Abstract: At present, procedural knowledge is mainly represented by production rules. Although this representation is concise, problems exist on lack of semantic elements and single semantic relationship, which is not conducive to the representation of procedural knowledge and hinders the teaching and application of procedural knowledge. Therefore, this paper firstly analyzes the semantic elements that procedural knowledge should have, and the lack of time and space elements in traditional representation methods. Combining with the cognitive background of procedural knowledge and introducing event cognition, the representation method of procedural knowledge graph and the construction process model are obtained. The verification proves that the event graph is a procedural knowledge representation method with complete semantic roles and rich semantic relations.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Education} and {Training} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Xiao, Yao and Zhan, Zehui and Yuan, Man},
	year = {2022},
	note = {event-place: Macau, China},
	keywords = {Knowledge Representation, Procedural knowledge, Event Graph, Knowledge Model},
	pages = {85--91},
}

@inproceedings{guo_mental_2021,
	address = {New York, NY, USA},
	series = {{ISAIMS} '21},
	title = {Mental {Health} {Question} and {Answering} {System} {Based} on {Bert} {Model} and {Knowledge} {Graph} {Technology}},
	isbn = {978-1-4503-9558-8},
	url = {https://doi.org/10.1145/3500931.3501011},
	doi = {10.1145/3500931.3501011},
	abstract = {With the development and progress of society, people are facing increasing pressure. The emergence of this phenomenon has led to a rapid increase in the incidence of mental illness. In order to deal with this phenomenon, this paper proposes a system of question and answering on the basic knowledge of mental health (MHQ\&amp;A) by using deep learning retrieval technology and knowledge graph technology. The system MHQ\&amp;A is designed mainly for the general public, to answer the basic knowledge of mental health, especially the field of depression. First of all, the basic and the professional question and answer data about mental health were respectively obtained by the reptilian bot from the "IASK" website knowledge and the "Dr. Dingxiang" website. Then, the questions and answers obtained through the crawler are made into a Question and Answering Knowledge Graph of Basic Health Knowledge in the mental health field, which is combined with semantic data of antidepressants and the semantic data of depression papers. Finally, a set of template matching rules is designed to determine the type of problem of users. If the questions are about the professional knowledge of medicine or thesis, the reasoning template will be used to reason and search the answer in the "Question and Answering Knowledge Graph of Basic Health Knowledge in the Mental Health Field". If the questions are about other basic knowledge in the field of mental health, the BERT model is used to vectorize the questions of users, and the matching questions and corresponding answers in the MHQ\&amp;A are found through cosine similarity calculation. Through the test of system accuracy, it is proved that the system can effectively combine deep learning technology and knowledge.},
	booktitle = {Proceedings of the 2nd {International} {Symposium} on {Artificial} {Intelligence} for {Medicine} {Sciences}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Chaohui and Lin, Shaofu and Huang, Zhisheng and Yao, Yahong},
	year = {2021},
	note = {event-place: Beijing, China},
	keywords = {Deep learning, Knowledge Graph, Mental illness, Question and answering system},
	pages = {472--476},
}

@inproceedings{hoser_modeling_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {Modeling adaptive learning agents for domain knowledge transfer},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00101},
	doi = {10.1109/MODELS-C.2019.00101},
	abstract = {The implementation of intelligent agents in industrial applications is often prevented by the high cost of adopting such a system to a particular problem domain. This paper states the thesis that when learning agents are applied to work environments that require domain-specific experience, the agent benefits if it can be further adapted by a supervising domain expert. Closely interacting with the agent, a domain expert should be able to understand its decisions and update the underlying knowledge base as needed.The result would be an agent with individualized knowledge that comes in part from the domain experts. The model of such an adaptive learning agent must take into account the problem domain, the design of the learning agent and the perception of the domain user. Therefore, already in the modeling phase, more attention must be paid to make the learning element of the agent adaptable by an operator. Domain modeling and meta-modeling methods could help to make inner processes of the agent more accessible. In addition, the knowledge gained should be made reusable for future agents in similar environments.To begin with, the existing methods for modeling agent systems and the underlying concepts will be evaluated, based on the requirements for different industrial scenarios. The methods are then compiled into a framework that allows for the description and modeling of such systems in terms of adaptability to a problem domain. Where necessary, new methods or tools will be introduced to close the gap between inconsistent modeling artifacts.The framework shall then be used to build learning agents for real-life scenarios and observe their application in a case study. The results will be used to assess the quality of the adapted knowledge base and compare it to a manual knowledge modeling process.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Höser, Moritz},
	year = {2021},
	keywords = {knowledge engineering, domain modeling, adaptive learning agents, multi-modeling},
	pages = {660--665},
}

@inproceedings{bahrani_fdcm_2020,
	address = {New York, NY, USA},
	series = {{CIKM} '20},
	title = {{FDCM}: {Towards} {Balanced} and {Generalizable} {Concept}-based {Models} for {Effective} {Medical} {Ranking}},
	isbn = {978-1-4503-6859-9},
	url = {https://doi.org/10.1145/3340531.3412151},
	doi = {10.1145/3340531.3412151},
	abstract = {Concept-based IR is expected to improve the quality of medical ranking since it captures more semantics than BOW representations. However, bringing concepts and BOW together into a transparent IR framework is challenging. We propose a new aggregation parameter to combine conceptual and term-based Dirichlet Compound Model scores effectively. The determination of this linear parameter is the result of exploring to what degree the difference of the conceptual and term-based sum of IDFs is influential to the integration. Instead of employing heuristics to find combined models, this paper aims to build the grounds for establishing reasonable aggregation standards based on semantic query performance predictors.},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Bahrani, Mohammad and Roelleke, Thomas},
	year = {2020},
	note = {event-place: Virtual Event, Ireland},
	keywords = {query formulation, concept-based ir, dirichlet compound language modelling, semantic ir},
	pages = {1957--1960},
}

@inproceedings{belkaroui_towards_2018,
	address = {New York, NY, USA},
	series = {{IOT} '18},
	title = {Towards events ontology based on data sensors network for viticulture domain},
	isbn = {978-1-4503-6564-2},
	url = {https://doi.org/10.1145/3277593.3277619},
	doi = {10.1145/3277593.3277619},
	abstract = {Wine Cloud project is the first "Big Data" platform on the french viticulture value chain. The aim of this platform is to provide a complete traceability of the life cycle of the wine, from the wine-grower to the consumer. In particular, Wine Cloud may qualify as an agricultural decision platform that will be used for vine life cycle management in order to predict the occurrence of major risks (vine diseases, grape vine pests, physiological risks, fermentation stoppage, oxidation of vine, etc...). Also to make wine production more rational by offering winegrower a set of recommendation regarding their strategy's of production development.The proposed platform "Wine Cloud" is based on heterogeneous sensors network (agricultural machines, plant sensors and measuring stations) deployed throughout a vineyard. These sensors allow for capturing data from the agricultural process and remote monitoring vineyards in the Internet of Things (IoT) era. However, the sensors data from different source is hard to work together for lack of semantic. Therefore, the task of coherently combining heterogeneous sensors data becomes very challenging. The integration of heterogeneous data from sensors can be achieved by data mining algorithms able to build correlations. Nevertheless, the meaning and the value of these correlations is difficult to perceive without highlighting the meaning of the data and the semantic description of the measured environment.In order to bridge this gap and build causality relationships form heterogeneous sensor data, we propose an ontology-based approach, that consists in exploring heterogeneous sensor data (light, temperature, atmospheric pressure, etc) in terms of ontologies enriched with semantic meta-data describing the life cycle of the monitored environment.},
	booktitle = {Proceedings of the 8th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Belkaroui, Rami and Bertaux, Aurélie and Labbani, Ouassila and Hugol-Gential, Clémentine and Nicolle, Christophe},
	year = {2018},
	note = {event-place: Santa Barbara, California, USA},
	keywords = {ontologies, IoT, event ontology, big data, semantic sensor data, smart viticulture},
}

@inproceedings{el_hajjamy_semantic_2018,
	address = {New York, NY, USA},
	series = {{LOPAL} '18},
	title = {Semantic integration of heterogeneous classical data sources in ontological data warehouse},
	isbn = {978-1-4503-5304-5},
	url = {https://doi.org/10.1145/3230905.3230929},
	doi = {10.1145/3230905.3230929},
	abstract = {The development of semantic web technologies and the expansion of the amount of data managed within companies databases lead to an enormous quantity of heterogeneous, distributed and autonomous data sources. However, this growth of information will give rise to real obstacles if we cannot maintain the pace with these changes and meet the needs of users. To succeed, it is necessary to find a solution for integrating data from traditional information systems into richer systems. In this perspective, Ontologies are a key component in solving the problem of semantic heterogeneity, thus enabling semantic interoperability between different web applications and services. In this paper, we provide and develop a semi-automatic approach for designing an ontological data warehouse from various sources. Our approach is to convert the different classical data sources (UML, XML, RDB) to local ontologies (OWL2), then merge these ontologies into a global ontological model based on syntactic, structural and semantic similarity measurement techniques to identify similar concepts and avoid their redundancy in the merge result. Our study is proven by a developed prototype that demonstrates the efficiency and power of our strategy and validates the theoretical concept.},
	booktitle = {Proceedings of the {International} {Conference} on {Learning} and {Optimization} {Algorithms}: {Theory} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {El Hajjamy, Oussama and Alaoui, Larbi and Bahaj, Mohamed},
	year = {2018},
	note = {event-place: Rabat, Morocco},
	keywords = {Semantic web, ontologies, UML, RDB, XML, data warehouse, Integrating data, OWL2},
}

@inproceedings{dagenais_driving_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Driving {Requirements} {Evolution} by {Engineers}' {Opinions}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688566},
	doi = {10.1145/3652620.3688566},
	abstract = {Requirements are often incomplete or imprecise. Especially when innovation is a pronounced aspect of product development, sufficiently refined requirements can only be obtained by leveraging engineering knowledge gained through the exploration of innovative designs. However, such innovative designs often contradict prevalent requirements and might be deemed incorrect unless requirements evolve. In this paper, we present a method to drive requirements evolution by engineering opinions—early indicators of emergent engineering knowledge. Opinions about the suitability of a new design emerge earlier than hard evidence can be produced, potentially accelerating the evolution of requirements and saving time and costs. In this work, we develop a sound formal framework to inform requirements engineers about the potential need for requirements evolution based on engineering opinions. We formalize engineering opinions in terms of subjective logic and unify them with key concepts of model-driven engineering.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dagenais, Kyanna and David, Istvan},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {ontologies, model-driven engineering, collaboration, uncertainty, consistency, design space exploration, parallel engineering},
	pages = {920--929},
}

@inproceedings{barbalho_investigation_2018,
	address = {New York, NY, USA},
	series = {{EATIS} '18},
	title = {An {Investigation} on the use of {Ontologies} for pattern classification - {Study} applied to the monitoring of food intake},
	isbn = {978-1-4503-6572-7},
	url = {https://doi.org/10.1145/3293614.3293627},
	doi = {10.1145/3293614.3293627},
	abstract = {Several tools are developed with the purpose of solving problems and exposing results similar to human reasoning. For this, various artificial intelligence techniques are being implemented to improve these applications. For the poorly structured and high volume data, the ontology presents itself as a technique capable of structuring this data and exposing representative results. In this way, this work describes the use of an ontology as the data classification technique and pattern recognition. The objective is to develop an ontological structure capable of analyzing and classifying the movements and sound signals of the chewing and swallowing process in solids or liquids. To validate the ontology, the tests were performed in real environments. The results obtained, based on the realized experiments, point to the viability of the use of ontologies for the problem in question.},
	booktitle = {Proceedings of the {Euro} {American} {Conference} on {Telematics} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Barbalho, Ingridy M. P. and Silva, Patricio de A. and Fernandes, Felipe R. dos S. and Neto, Francisco M. M. and Leite, Cicilia R. M.},
	year = {2018},
	note = {event-place: Fortaleza, Brazil},
	keywords = {Ontologies, Artificial Intelligence, Data classification, Food Intake},
}

@inproceedings{dib_bigraphical_2021,
	address = {New York, NY, USA},
	series = {{ICCAI} '21},
	title = {Bigraphical {Modelling} and {Design} of {Multi}-{Agent} {Systems}},
	isbn = {978-1-4503-8950-1},
	url = {https://doi.org/10.1145/3467707.3467762},
	doi = {10.1145/3467707.3467762},
	abstract = {Multi-agent systems are recognized as a major area of distributed artificial intelligence. In fact, MAS have found multiple applications, including the design and development of complex, hierarchical and critical systems. However, ensuring the accuracy of complex interactions and the correct execution of activities of a MAS is becoming a tedious task. In this work, we focus on the formal specification of interaction, holonic and sociotechnical concepts to the BRS-MAS model. The proposed approach, is based on Bigraphical reactive systems. Bigraphs, provide means to specify at same time locality and connectivity of different type of system ranging from soft systems to cyber physical systems. In addition, to its intuitive graphical representation, it provides algebraic definition. This, makes the resulted specifications more precise. Further, it enables the verification of the specified system at the design time (before the implementation) using verification tools.},
	booktitle = {Proceedings of the 2021 7th {International} {Conference} on {Computing} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {DIB, Ahmed Taki Eddine and MAAMRI, Ramdane},
	year = {2021},
	note = {event-place: Tianjin, China},
	keywords = {Multi-agent system, Computing methodologies, Formal specification, Algebraic language theory, Holonic, Theory of computation},
	pages = {365--371},
}

@inproceedings{zhang_research_2022,
	address = {New York, NY, USA},
	series = {{ICCPR} '21},
	title = {Research on {General} {Text} {Classification} {Model} {Integrating} {Character}-{Level} {Attention} and {Multi}-{Scale} {Features}},
	isbn = {978-1-4503-9043-9},
	url = {https://doi.org/10.1145/3497623.3497652},
	doi = {10.1145/3497623.3497652},
	abstract = {To solve the problem of poor interpretability of the model caused by the random initialization of convolution kernel in convolution neural network,and the problem of local and single feature extraction scale, a general character-level classification model is designed by referring to the method in CV. Firstly,a multi-scale feature extraction module is added to the network embedding layer to extract rich context information, and the problem of matrix sparsity is solved to some extent by setting different void rates. Then, the network depth is controlled by the number of blocks. Different depths have different grasps of global information and can adapt to tasks with different complexity. Next, add a modified attention mechanism module after the block to enhance the attention of the model to key parts. Finally, the full connection layer of the network is replaced by the full convolution layer to simplify the model. The block is a compressed version of deep separable convolution, and the overall model parameters are reduced to about one-tenth of the original, but the accuracy and performance are improved. The results show that the model is very effective.},
	booktitle = {Proceedings of the 2021 10th {International} {Conference} on {Computing} and {Pattern} {Recognition}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Congcong and Zhao, Haifeng and Cao, Mingwei},
	year = {2022},
	note = {event-place: Shanghai, China},
	keywords = {attention mechanism, compression, full convolution, multi-scale features},
	pages = {183--187},
}

@inproceedings{halilaj_effte_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {{EffTE}: a dependency-aware approach for test-driven ontology development},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167344},
	doi = {10.1145/3167132.3167344},
	abstract = {The development of domain-specific ontologies requires joint efforts among different groups of stakeholders, such as ontology engineers and domain experts. By following a test-driven development technique, a set of test cases ensures that ontology changes do not violate predefined requirements. However, since the number of test cases can be large and their evaluation time may be high, the ontology development process can be negatively impacted. We propose EffTE, an approach for efficient test-driven ontology development relying on a graph-based model of dependencies between test cases. It enables prioritization and selection of test cases to be evaluated. Traversing the dependency graph is realized using breadth-first search along with a mechanism that tracks tabu test cases, i.e., test cases to be ignored for further evaluation due to faulty parent test cases. As a result, the number of evaluated test cases is minimized, thus reducing the time required for validating the ontology after each modification. We conducted an empirical evaluation to determine the efficiency of our approach. The evaluation results suggest that our approach is more efficient than an exhaustive evaluation of the test cases; in particular with an increasing ontology size and number of test cases.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Halilaj, Lavdim and Grangel-González, Irlán and Lohmann, Steffen and Vidal, Maria-Esther and Auer, Sören},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {ontology engineering, dependency graph, test cases, test-driven ontology development},
	pages = {1976--1983},
}

@inproceedings{tseng_collaborative_2023,
	address = {New York, NY, USA},
	series = {{IDC} '23},
	title = {Collaborative {Machine} {Learning} {Model} {Building} with {Families} {Using} {Co}-{ML}},
	isbn = {979-8-4007-0131-3},
	url = {https://doi.org/10.1145/3585088.3589356},
	doi = {10.1145/3585088.3589356},
	abstract = {Existing novice-friendly machine learning (ML) modeling tools center around a solo user experience, where a single user collects only their own data to build a model. However, solo modeling experiences limit valuable opportunities for encountering alternative ideas and approaches that can arise when learners work together; consequently, it often precludes encountering critical issues in ML around data representation and diversity that can surface when different perspectives are manifested in a group-constructed data set. To address this issue, we created Co-ML – a tablet-based app for learners to collaboratively build ML image classifiers through an end-to-end, iterative model-building process. In this paper, we illustrate the feasibility and potential richness of collaborative modeling by presenting an in-depth case study of a family (two children 11 and 14-years-old working with their parents) using Co-ML in a facilitated introductory ML activity at home. We share the Co-ML system design and contribute a discussion of how using Co-ML in a collaborative activity enabled beginners to collectively engage with dataset design considerations underrepresented in prior work such as data diversity, class imbalance, and data quality. We discuss how a distributed collaborative process, in which individuals can take on different model-building responsibilities, provides a rich context for children and adults to learn ML dataset design.},
	booktitle = {Proceedings of the 22nd {Annual} {ACM} {Interaction} {Design} and {Children} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Tseng, Tiffany and King Chen, Jennifer and Abdelrahman, Mona and Kery, Mary Beth and Hohman, Fred and Hilliard, Adriana and Shapiro, R. Benjamin},
	year = {2023},
	note = {event-place: Chicago, IL, USA},
	keywords = {machine learning, learning, collaboration, children, families},
	pages = {40--51},
}

@inproceedings{maresca_ontoaugment_2021,
	address = {New York, NY, USA},
	series = {{SenSys} '21},
	title = {{OntoAugment}: {Ontology} {Matching} through {Weakly}-{Supervised} {Label} {Augmentation}},
	isbn = {978-1-4503-9097-2},
	url = {https://doi.org/10.1145/3485730.3493445},
	doi = {10.1145/3485730.3493445},
	abstract = {Ontology matching enables harmonizing heterogeneous data models. Existing ontology matching approaches include machine learning. In particular, recent works leverage weak supervision (WS) through programmatic labeling to avoid the intensive hand-labeling for large ontologies. Programmatic labeling relies on heuristics and rules, called Labeling Functions (LFs), that generate noisy and incomplete labels. However, to cover a reasonable portion of the dataset, programmatic labeling might require a significant number of LFs that might be time expensive and not always straightforward to program.This paper proposes a novel system, namely OntoAugment, that augments LF labels for the ontology matching problem, starting from outcomes of the LFs. Our solution leverages the "similarity of similarities" between ontology concept bipairs that are two pairs of concepts. OntoAugment projects a label yielded by an LF for a concept pair to a similar pair that the same LF does not label. Thus, a wider portion of the dataset is covered even with a limited set of LFs. Experimentation results show that OntoAugment provides significant improvements (up to 11 F1 points) compared to the state-of-the-art WS approach when fewer LFs are used, whereas it maintains the performance without creating additional noise when a higher number of LFs already achieves high performance.},
	booktitle = {Proceedings of the 19th {ACM} {Conference} on {Embedded} {Networked} {Sensor} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Maresca, Fabio and Solmaz, Gürkan and Cirillo, Flavio},
	year = {2021},
	note = {event-place: Coimbra, Portugal},
	keywords = {ontology matching, semantic, weak supervision, data programming},
	pages = {420--425},
}

@inproceedings{schleipen_modeling_2023,
	address = {New York, NY, USA},
	series = {{IoT} '22},
	title = {A modeling approach for integration and contextualization of simulation-based digital services in {IIoT}},
	isbn = {978-1-4503-9665-3},
	url = {https://doi.org/10.1145/3567445.3571109},
	doi = {10.1145/3567445.3571109},
	abstract = {In the context of the Industrial Internet of Things (IIoT) production plants and components are increasingly growing together with information technologies. This is often realized by means of digital twins. They collect data in real time and learn from this data; they control processes automatically or support human decisions; and they communicate and interact through the internet. This is more and more evolving to intercompany interactions based on digital services. In addition to data of isolated assets (e.g. production resources), new capabilities for standard-based data integration and orchestration are necessary to contextualize the interaction of multiple digital twins and services. This paper suggests an approach to use common standards in the industrial context such as AutomationML, FMI, and OPC UA as basis for integration and contextualization of simulation-based digital services on IIoT platforms.},
	booktitle = {Proceedings of the 12th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Schleipen, Miriam and Schubert, Viktor and Dzidic, Samir and Penner, Dimitri and Spieckermann, Sven},
	year = {2023},
	note = {event-place: Delft, Netherlands},
	keywords = {simulation, AutomationML, capability, FMI, skill, smart service},
	pages = {205--210},
}

@article{zhao_sphinteract_2025,
	title = {Sphinteract: {Resolving} {Ambiguities} in {NL2SQL} through {User} {Interaction}},
	volume = {18},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3717755.3717772},
	doi = {10.14778/3717755.3717772},
	abstract = {Translating natural language questions into SQL queries (NL2SQL) is a challenging task of great practical importance. Prior work has extensively studied how to address NL2SQL using Large Language Models (LLMs) with solutions ranging from careful prompt engineering, to fine-tuning existing LLMs, or even training custom models. However, a remaining challenging problem in NL2SQL is the inherent ambiguity in the natural language questions asked by users. In this paper, we introduce Sphinteract, a framework designed to assist LLMs in generating high-quality SQL answers that accurately reflect the user intent. Our key insight to resolve ambiguity is to take into account minimal user feedback interactively. We introduce the Summarize, Review, Ask (SRA) paradigm, which guides LLMs in identifying ambiguities in NL2SQL tasks and generates targeted questions for the user to answer. We propose three different methods of how to process user feedback and generate SQL queries based on user input. Our experiments on the challenging KaggleDBQA and BIRD benchmarks demonstrate that by means of asking clarification questions to the user, LLMs can efficiently incorporate the feedback, resulting in accuracy improvements of up to 42\%.},
	number = {4},
	journal = {Proc. VLDB Endow.},
	author = {Zhao, Fuheng and Deep, Shaleen and Psallidas, Fotis and Floratou, Avrilia and Agrawal, Divyakant and Abbadi, Amr El},
	month = may,
	year = {2025},
	note = {Publisher: VLDB Endowment},
	pages = {1145--1158},
}

@inproceedings{korol_fear_2025,
	address = {New York, NY, USA},
	series = {{DFDS} '25},
	title = {{FEAR}: {A} {Novel} {Framework} for {Representing} {Digital} {Forensic} {Artifacts} in {Knowledge} {Graphs}},
	isbn = {979-8-4007-1076-6},
	url = {https://doi.org/10.1145/3712716.3712726},
	doi = {10.1145/3712716.3712726},
	abstract = {In digital forensics, knowledge graphs have demonstrated significant potential via software agent automation and knowledge discovery using encoded expert knowledge in, for example, the form of Semantic Web rules. These advancements have been limited in terms of efficiently encoding extracted digital artifacts into graph representation, the associated overhead of implementing frameworks to handle digital forensic evidence, and the lack of sharing of such code that is often a preamble to other research. This paper introduces a digital forensic framework, Forensic Extraction and Representation (FEAR), that enables a simplified process of accessing and encoding extracted digital forensic artifacts in semantic knowledge graphs. The adoption of such a framework can facilitate the sharing of expert knowledge and reduce the burden of development for researchers exploring the application of knowledge graphs, software agents, and automated reasoning in the field of digital forensics, while accelerating the adoption of emerging research by practitioners.},
	booktitle = {Proceedings of the {Digital} {Forensics} {Doctoral} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Korol, Allan and Sikos, Leslie F.},
	year = {2025},
	keywords = {declarative language, digital forensic artifacts, digital forensic framework, digital forensic software agent, semantic knowledge graph},
}

@inproceedings{atarashi_software_2018,
	address = {New York, NY, USA},
	series = {{SAAM} '18},
	title = {The {Software} {Defined} {Media} {Ontology} for {Music} {Events}},
	isbn = {978-1-4503-6495-9},
	url = {https://doi.org/10.1145/3243907.3243915},
	doi = {10.1145/3243907.3243915},
	abstract = {With the advent of viewing services based on the Internet, the importance of object-based viewing services for interpreting objects existing in space and utilizing them as content is increasing. Since 2014, the Software Defined Media Consortium has been researching object-based media and Internet-based viewing spaces. This paper defineds a framework in event participants and professional recorders each freely share recorded data, and a third party can creates an application based on the data. This study aims to provide an SDM ontology-based contents management mechanism with a detailed description of the object-based audio and video data and the recording environment. The data can be shared via the Internet and is highly reusable. We implemented this management mechanism and have developed and validated applications that are capable of interactively playing 3D content from any viewpoints freely.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Semantic} {Applications} for {Audio} and {Music}},
	publisher = {Association for Computing Machinery},
	author = {Atarashi, Ray and Sone, Takuro and Komohara, Yu and Tsukada, Manabu and Kasuya, Takashi and Okumura, Hiraku and Ikeda, Masahiro and Esaki, Hiroshi},
	year = {2018},
	note = {event-place: Monterey, CA, USA},
	keywords = {Ontology, RDF, Content management},
	pages = {15--23},
}

@article{tan_bert-based_2021,
	title = {A {BERT}-{Based} {Two}-{Stage} {Model} for {Chinese} {Chengyu} {Recommendation}},
	volume = {20},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3453185},
	doi = {10.1145/3453185},
	abstract = {In Chinese, Chengyu are fixed phrases consisting of four characters. As a type of idioms, their meanings usually cannot be derived from their component characters. In this article, we study the task of recommending a Chengyu given a textual context. Observing some of the limitations with existing work, we propose a two-stage model, where during the first stage we re-train a Chinese BERT model by masking out Chengyu from a large Chinese corpus with a wide coverage of Chengyu. During the second stage, we fine-tune the re-trained, Chengyu-oriented BERT on a specific Chengyu recommendation dataset. We evaluate this method on ChID and CCT datasets and find that it can achieve the state of the art on both datasets. Ablation studies show that both stages of training are critical for the performance gain.},
	number = {6},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Tan, Minghuan and Jiang, Jing and Dai, Bing Tian},
	month = aug,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Question answering, Chengyu recommendation, idiom understanding},
}

@inproceedings{zhao_medrag_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{MedRAG}: {Enhancing} {Retrieval}-augmented {Generation} with {Knowledge} {Graph}-{Elicited} {Reasoning} for {Healthcare} {Copilot}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714782},
	doi = {10.1145/3696410.3714782},
	abstract = {Retrieval-augmented generation (RAG) is a well-suited technique for retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a key module of the healthcare copilot, helping reduce misdiagnosis for healthcare practitioners and patients. However, the diagnostic accuracy and specificity of existing heuristic-based RAG models used in the medical domain are inadequate, particularly for diseases with similar manifestations. This paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited reasoning for the medical domain that retrieves diagnosis and treatment recommendations based on manifestations. MedRAG systematically constructs a comprehensive four-tier hierarchical diagnostic KG encompassing critical diagnostic differences of various diseases. These differences are dynamically integrated with similar EHRs retrieved from an EHR database, and reasoned within a large language model. This process enables more accurate and specific decision support, while also proactively providing follow-up questions to enhance personalized medical decision-making. MedRAG is evaluated on both a public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD) collected from Tan Tock Seng Hospital, and its performance is compared against various existing RAG methods. Experimental results show that, leveraging the information integration and relational abilities of the KG, our MedRAG provides more specific diagnostic insights and outperforms state-of-the-art models in reducing misdiagnosis rates. Our code will be available at https://github.com/SNOWTEAM2023/MedRAG},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Xuejiao and Liu, Siyan and Yang, Su-Yin and Miao, Chunyan},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {large language models, knowledge graph, decision support, retrieval-augmented generation, healthcare copilot},
	pages = {4442--4457},
}

@inproceedings{istrate_models_2021,
	address = {Richland, SC},
	series = {{AAMAS} '21},
	title = {Models {We} {Can} {Trust}: {Toward} a {Systematic} {Discipline} of ({Agent}-{Based}) {Model} {Interpretation} and {Validation}},
	isbn = {978-1-4503-8307-3},
	abstract = {We advocate the development of a discipline of interacting with and extracting information from models, both mathematical (e.g. game-theoretic ones) and computational (e.g. agent-based models). We outline some directions for the development of a such a discipline: - the development of logical frameworks for the systematic formal specification of stylized facts and social mechanisms in (mathematical and computational) social science. Such frameworks would bring to attention new issues, such as phase transitions, i.e. dramatical changes in the validity of the stylized facts beyond some critical values in parameter space. We argue that such statements are useful for those logical frameworks describing properties of ABM. - the adaptation of tools from the theory of reactive systems (such as bisimulation) to obtain practically relevant notions of two systems "having the same behavior". - the systematic development of an adversarial theory of model perturbations, that investigates the robustness of conclusions derived from models of social behavior to variations in several features of the social dynamics. These may include: activation order, the underlying social network, individual agent behavior.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Istrate, Gabriel},
	year = {2021},
	note = {event-place: Virtual Event, United Kingdom},
	keywords = {adversarial perturbations, agent-based simulation, bisimulation, game-theoretic models, logical frameworks, robustness},
	pages = {6--11},
}

@inproceedings{tissaoui_leonto_2020,
	address = {New York, NY, USA},
	series = {{MEDES} '20},
	title = {{LEOnto}: {New} {Approach} for {Ontology} {Enrichment} using {LDA}},
	isbn = {978-1-4503-8115-4},
	url = {https://doi.org/10.1145/3415958.3433076},
	doi = {10.1145/3415958.3433076},
	abstract = {The Latent Dirichlet Allocation (LDA) model [18] was originally developed and utilised for document modeling and topic extraction in Information Retrieval. To design high quality domain ontologies, effective and usable methodologies are needed to facilitate their building process. In this paper, we propose a new approach for semi-automatic ontology enriching from textual corpus based on LDA model. In our approach, LDA is adopted to provide efficient dimension reduction, able to capture semantic relationships between word-topic and topic-document in terms of probability distributions with minimum human intervention. We conducted several experiments with different model parameters and the corresponding behavior of the enriching technique was evaluated by domain experts. We also compared the results of our method with two existing learning methods using the same dataset. The study showed that our method outperforms the other methods in terms of recall and precision measures.},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Tissaoui, Anis and Sassi, Salma and Chbeir, Richard},
	year = {2020},
	note = {event-place: Virtual Event, United Arab Emirates},
	keywords = {Knowledge acquisition, Ontology learning, Ontology enrichment, LDA, Probabilistic topic models},
	pages = {132--139},
}

@inproceedings{voutos_semantic_2018,
	address = {New York, NY, USA},
	series = {{PCI} '18},
	title = {A semantic data model for sensory spatio-temporal environmental concepts},
	isbn = {978-1-4503-6610-6},
	url = {https://doi.org/10.1145/3291533.3291571},
	doi = {10.1145/3291533.3291571},
	abstract = {Nowadays, the well-known Resource Description Framework (RDF) forms a rather general method for web resources' conceptual description or even for generic information modeling. However, RDF's capabilities are challenged once used to effectively represent non-thematic metadata, e.g, in the form of spatial and temporal objects deriving primarily from sensor information. In addition, Wireless Sensor Network (WSN) is considered today to be a widely adopted platform, related to environmental monitoring and decision making applications. Specifically, exclusive subjects, such as environmental degradation and optimized agriculture, provide a scope of applied research on the basis of multilevel semantic data analysis. Observations and sensors are the core of empirical science and their implementation (i.e., the increasing volume of data, heterogeneity of devices, data formats and measurement procedures) produce a large volume of unsupervised data. Thus, the prevailing growth of sensing systems has currently led to the development of defined interoperability among standards on web semantics. In particular, Semantic Sensor Network (SSN) ontologies prospect on modeling the capabilities and properties of sensors, monitoring procedures and observations. Furthermore, the dynamically evolving natural phenomena require proper conceptualization of environmental change and monitoring agents. Consequently, this paper describes an inaugural attempt to create a conceptual framework of spatially and temporally-enabled environmental variables for sensing systems.},
	booktitle = {Proceedings of the 22nd {Pan}-{Hellenic} {Conference} on {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Voutos, Yorghos and Mylonas, Phivos},
	year = {2018},
	note = {event-place: Athens, Greece},
	keywords = {semantics, ontology model, environmental monitoring, sensors},
	pages = {180--185},
}

@inproceedings{cartealy_metabolic_2020,
	address = {New York, NY, USA},
	series = {{ICBBS} '19},
	title = {Metabolic {Pathway} {Membership} {Inference} using an {Ontology}-based {Similarity} {Approach}},
	isbn = {978-1-4503-7251-0},
	url = {https://doi.org/10.1145/3369166.3369174},
	doi = {10.1145/3369166.3369174},
	abstract = {Determining whether a protein belongs to a metabolic pathway is an important annotation task, can provide context to the basic functional annotation, and aid reconstruction of incomplete pathways. In this work, we develop a method for pathway membership inference based gene ontology (GO) similarity between a query protein and proteins that are known to be members of a given pathway. By comparing with various existing measures of GO term semantic similarity, we develop an effective and efficient way to take into account of both information content of individual GO terms and the whole GO hierarchy. We test the classifier using 10-fold cross validation for all metabolic pathways reported in KEGG database and demonstrate that our method outperforms with statistical significance in comparison to a suite of existing semantic similarity measures, as evaluated using ROC score. And our method outperforms other methods in running time by multiple orders of magnitude for long pathways.},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Bioinformatics} and {Biomedical} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Cartealy, Imam and Liao, Li},
	year = {2020},
	note = {event-place: Beijing, China},
	keywords = {Gene Ontology, Semantic Similarity, Membership Prediction, Metabolic Pathway},
	pages = {97--102},
}

@article{hadan_motivating_2024,
	title = {From {Motivating} to {Manipulative}: {The} {Use} of {Deceptive} {Design} in a {Game}'s {Free}-to-{Play} {Transition}},
	volume = {8},
	url = {https://doi.org/10.1145/3677074},
	doi = {10.1145/3677074},
	abstract = {Over the last decade, the free-to-play (F2P) game business model has gained popularity in the games industry. We examine the role of deceptive design during a game's transition to F2P and its impacts on players. Our analysis focuses on game mechanics and a Reddit analysis of the Overwatch (OW) series after it transitioned to an F2P model. Our study identifies nine game mechanics that use deceptive design patterns. We also identify factors contributing to a negative gameplay experience. Business model transitions in games present possibilities for problematic practices. Our findings identify the need for game developers and publishers to balance player investments and fairness of rewards. A game's successful transition depends on maintaining fundamental components of player motivation and ensuring transparent communication. Compared to existing taxonomies in other media, games need a comprehensive classification of deceptive design. We emphasize the importance of understanding player perceptions and the impact of deceptive practices in future research.},
	number = {CHI PLAY},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Hadan, Hilda and Sgandurra, Sabrina Alicia and Zhang-Kennedy, Leah and Nacke, Lennart E.},
	month = oct,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deceptive design, free-to-play, game model transition, game player perception, Overwatch},
}

@inproceedings{bogdanova_use_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {Use of personalized feedback reports in a blended conceptual modelling course},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00103},
	doi = {10.1109/MODELS-C.2019.00103},
	abstract = {Despite the substantial number of existing publications on conceptual modelling education and feedback, in particular, the perfect balance between the effectiveness of feedback and the costs of the feedback design tailored to the field-specific needs of conceptual modelling remains an unanswered scientific and pedagogical question. The existing educational literature and online courses on conceptual modelling tend to overlook the essential aspects of metacognition and self-regulation in the learning process. The problem of providing feedback is exacerbated by the time-consuming nature of manual feedback provision and the difficulties of automating the provision of personalized and elaborated feedback. This paper presents an experience report on designing and implementing a learning ontology-based personalized feedback report aimed at raising student self-awareness and self-regulated learning in a university level conceptual modelling course, while the design aims at automation of the feedback provisioning in the near future. It describes the stages of learning report development and provides directions for adapting this type of feedback for various learning settings in conceptual modelling education, in view of potential future automation of report provision.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Bogdanova, Daria and Snoeck, Monique},
	year = {2021},
	keywords = {education, conceptual modelling, blended learning, data modelling, formative assessment, formative feedback, learning report},
	pages = {672--679},
}

@inproceedings{casale_integrated_2021,
	address = {Orlando, Florida},
	series = {{WSC} '20},
	title = {Integrated performance evaluation of extended queueing network models with line},
	isbn = {978-1-7281-9499-8},
	abstract = {Despite the large literature on queueing theory and its applications, tool support to analyze these models is mostly focused on discrete-event simulation and mean-value analysis (MVA). This circumstance diminishes the applicability of other types of advanced queueing analysis methods to practical engineering problems, for example analytical methods to extract probability measures useful in learning and inference. In this tool paper, we present LINE 2.0, an integrated software package to specify and analyze extended queueing network models. This new version of the tool is underpinned by an object-oriented language to declare a fairly broad class of extended queueing networks. These abstractions have been used to integrate in a coherent setting over 40 different simulation-based and analytical solution methods, facilitating their use in applications.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Casale, Giuliano},
	year = {2021},
	pages = {2377--2388},
}

@inproceedings{nikolov_uncovering_2020,
	address = {New York, NY, USA},
	series = {{CIKM} '20},
	title = {Uncovering {Semantic} {Bias} in {Neural} {Network} {Models} {Using} a {Knowledge} {Graph}},
	isbn = {978-1-4503-6859-9},
	url = {https://doi.org/10.1145/3340531.3412009},
	doi = {10.1145/3340531.3412009},
	abstract = {While neural networks models have shown impressive performance in many NLP tasks, lack of interpretability is often seen as a disadvantage. Individual relevance scores assigned by post-hoc explanation methods are not sufficient to show deeper systematic preferences and potential biases of the model that apply consistently across examples. In this paper we apply rule mining using knowledge graphs in combination with neural network explanation methods to uncover such systematic preferences of trained neural models and capture them in the form of conjunctive rules. We test our approach in the context of text classification tasks and show that such rules are able to explain a substantial part of the model behaviour as well as indicate potential causes of misclassifications when the model is applied outside of the initial training context.},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Nikolov, Andriy and d'Aquin, Mathieu},
	year = {2020},
	note = {event-place: Virtual Event, Ireland},
	keywords = {knowledge graphs, neural networks, explainable AI, rule mining},
	pages = {1175--1184},
}

@inproceedings{caesar_engineering_2020,
	address = {New York, NY, USA},
	series = {{VaMoS} '20},
	title = {Engineering support for variability modeling for context-sensitive reconfiguration of collaborative manufacturing systems},
	isbn = {978-1-4503-7501-6},
	url = {https://doi.org/10.1145/3377024.3377027},
	doi = {10.1145/3377024.3377027},
	abstract = {The manufacturing domain faces new challenges due to market changes. One of these changes affects consumer behavior, i.e. customers demand individualized products in small batches. Varying production requests require different configurations of manufacturing systems. But until today most of these systems are designed for single purpose usage, therefore new manufacturing systems are required. One solution are modular manufacturing systems, which can be composed of diverse modules from different vendors providing varying capabilities [1]. Modular manufacturing systems are re-configurable at two different levels. First, on the system group level, where modules with the required capabilities are selected and orchestrated. Second, on the module level, where a configuration has to be selected that provides the capability with the right manufacturing parameters. To be able to select and orchestrate the modules on the system group level, each module has to provide a self-description, which covers the current configuration [2].},
	booktitle = {Proceedings of the 14th {International} {Working} {Conference} on {Variability} {Modelling} of {Software}-{Intensive} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Caesar, Birte},
	year = {2020},
	note = {event-place: Magdeburg, Germany},
	keywords = {reverse engineering, variability modeling, context-sensitive reconfiguration, variability mining},
}

@inproceedings{seaborn_crossing_2021,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '21},
	title = {Crossing the {Tepper} {Line}: {An} {Emerging} {Ontology} for {Describing} the {Dynamic} {Sociality} of {Embodied} {AI}},
	isbn = {978-1-4503-8095-9},
	url = {https://doi.org/10.1145/3411763.3451783},
	doi = {10.1145/3411763.3451783},
	abstract = {Artificial intelligences (AI) are increasingly being embodied and embedded in the world to carry out tasks and support decision-making with and for people. Robots, recommender systems, voice assistants, virtual humans—do these disparate types of embodied AI have something in common? Here we show how they can manifest as “socially embodied AI.” We define this as the state that embodied AI “circumstantially” take on within interactive contexts when perceived as both social and agentic by people. We offer a working ontology that describes how embodied AI can dynamically transition into socially embodied AI. We propose an ontological heuristic for describing the threshold: the Tepper line. We reinforce our theoretical work with expert insights from a card sort workshop. We end with two case studies to illustrate the dynamic and contextual nature of this heuristic.},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Seaborn, Katie and Pennefather, Peter and Miyake, Norihisa and Otake-Matsuura, Mihoko},
	year = {2021},
	note = {event-place: Yokohama, Japan},
	keywords = {Ontology, AI, Artificial agents, Social embodiment, Social perceptions},
}

@inproceedings{li_improved_2021,
	address = {New York, NY, USA},
	series = {{CCRIS} '21},
	title = {Improved non-autoregressive dialog state tracking model},
	isbn = {978-1-4503-9045-3},
	url = {https://doi.org/10.1145/3483845.3483880},
	doi = {10.1145/3483845.3483880},
	abstract = {Dialogue systems, a powerful tool of human-machine interaction, are widely applied in e-commerce, online education, and cellphone assistant, etc. Dialogue state tracking (DST), updating the state of user goals during dialogue, is a core part of task-oriented dialogue systems. Recent research has made progress in low-latency and good-performance DST neural network models, i.e., non-autoregressive dialogue state tracking model (NADST). However, there are still some rooms for improvement in dialogue state tracking. In this paper, we propose following ways to improve the efficiency of NADST: (1) adding shrinkage residual network into fertility prediction; (2) constructing residual connection between different hierarchical attentions; (3) inserting a relative position encoding into state decoder for improving the performance of state prediction. The results of analysis and experiments indicate that the proposed model is the SOTA non-autoregressive method of dialog state tracking.},
	booktitle = {Proceedings of the 2021 2nd {International} {Conference} on {Control}, {Robotics} and {Intelligent} {System}},
	publisher = {Association for Computing Machinery},
	author = {Li, Baizhen and Zhan, Yibin and Wei, Zhihua and Huang, Shi and Sun, Lijun},
	year = {2021},
	note = {event-place: Qingdao, China},
	pages = {199--203},
}

@inproceedings{grisstte_daily_2020,
	address = {New York, NY, USA},
	series = {{ASONAM} '19},
	title = {Daily life patients sentiment analysis model based on well-encoded embedding vocabulary for related-medication text},
	isbn = {978-1-4503-6868-1},
	url = {https://doi.org/10.1145/3341161.3343854},
	doi = {10.1145/3341161.3343854},
	abstract = {Millions of health-related messages and fresh communications can reveal important public health issues. New Drugs, Diseases, Adverse Drug Reactions (ADRs) keep appearing on social media in new Unicode versions. In particular, generative Model for both Sentiment analysis (SA) and Naturel Language Understanding (NLU) requires medical human labeled data or making use of resources for weak supervision that operates with the ignorance and the inability to define related-medication targets, and results in inaccurate sentiment prediction performance. The frequent use of informal medical language, nonstandard format and abbreviation forms, as well as typos in social media messages has to be taken into account. We probe the transition-based approach between patients language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology[21] to be formal input of our neural network model. At this end, we propose daily life patients Sentiment Analysis model based on hybrid embedding vocabulary for related-medication text under distributed dependency, and concepts translation methodology by incorporating medical knowledge from social media and real life medical science systems. The proposed neural network layers is shared between medical concept Normalization model and sentiment prediction model in order to understand and leverage related-sentiment information behind conceptualized features in Multiple context. The experiments were performed on various real world scenarios where limited resources in this case.},
	booktitle = {Proceedings of the 2019 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Grisstte, Hanane and Nfaoui, ELhabib},
	year = {2020},
	note = {event-place: Vancouver, British Columbia, Canada},
	keywords = {social media, sentiment analysis, BiLSTM model, medical concepts normalization, patient self-reports},
	pages = {921--928},
}

@inproceedings{hu_evolving_2020,
	address = {Barcelona, Spain},
	series = {{ASONAM} '18},
	title = {Evolving medical ontologies based on causal inference},
	isbn = {978-1-5386-6051-5},
	abstract = {Causal inference and analytics plays a critical role in public health and disease prevention. Through mining of large patient datasets, it is possible to identify opportunities for intervention and to determine the effectiveness of treatment. There are currently many methods to analyze and learn causal relationships in large patient datasets, as well as specific causal studies in epidemiology that define specific relationships among symptoms and treatments. This paper introduces a novel methodology to utilize causal knowledge to extend and improve a standard hierarchical medical ontology. First, we will obtain the hierarchical structure of the patient symptom variables based on the Medical Dictionary for Regulatory Activities Terminology (MedDRA). Then, we will learn a Causal Bayesian Network (CBN) using Max-Min Hill-Climbing (MMHC), a hybrid constraint and score-based learning algorithm, on the pre-existing National Institutes of Mental Health (NIMH) study on Sequenced Treatment Alternatives to Relieve Depression (STAR*D) patient dataset. Finally, we will use the causal links discovered in the CBN to evolve the ontology and its hierarchy.},
	booktitle = {Proceedings of the 2018 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {IEEE Press},
	author = {Hu, Hengyi and Kerschberg, Larry},
	year = {2020},
	keywords = {ontology, data mining, healthcare data, data management, patient data, ontology evolution, causal inference, causal networks, causality, healthcare information technology, bayesian networks},
	pages = {954--957},
}

@article{li_neighborhood_2022,
	title = {A {Neighborhood} {Re}-{Ranking} {Model} {With} {Relation} {Constraint} for {Knowledge} {Graph} {Completion}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2022.3225537},
	doi = {10.1109/TASLP.2022.3225537},
	abstract = {Knowledge graph completion (KGC) aims to predict missing links based on observed triples. However, current KGC models are still limited by the following two aspects. (1) the entity semantics is implicitly learned by neural network and merely depends on existing facts, which mostly suffers from less additional specific knowledge. Although previous studies have noticed that entity type information can effectively improve KGC task, most of them rely on labeled type-specific data. (2) the recent graph-based models mainly concentrate on Graph Neural Network (GNN) to update source entity representation, regardless of the separate role that neighborhood information plays and may mix noisy neighbor features for target prediction. To address the above two issues, we propose a neighborhood re-ranking model with relation constraint for KGC task. We suggest that both relation constraint and structured information located in triples can boost the model performance. More importantly, we automatically generate explicit constraints as additional type feature to enrich entity representation instead of depending on human annotated labels. Meanwhile, we construct a neighborhood completion module to re-rank candidate entities for full use of the neighbor structure rather than traditional GNN updating manner. Extensive experiments on seven benchmarks demonstrate that our model achieves the competitive results in comparison to the recent advanced baselines.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Li, Yu and Hu, Bojie and Liu, Jian and Chen, Yufeng and Xu, Jinan},
	month = nov,
	year = {2022},
	note = {Publisher: IEEE Press},
	pages = {411--425},
}

@article{ochieng_large-scale_2018,
	title = {Large-{Scale} {Ontology} {Matching}: {State}-of-the-{Art} {Analysis}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3211871},
	doi = {10.1145/3211871},
	abstract = {Ontologies have become a popular means of knowledge sharing and reuse. This has motivated the development of large-sized independent ontologies within the same or different domains with some overlapping information among them. To integrate such large ontologies, automatic matchers become an inevitable solution. However, the process of matching large ontologies has high space and time complexities. Therefore, for a tool to efficiently and accurately match these large ontologies within the limited computing resources, it must have techniques that can significantly reduce the high space and time complexities associated with the ontology matching process. This article provides a review of the state-of-the-art techniques being applied by ontology matching tools to achieve scalability and produce high-quality mappings when matching large ontologies. In addition, we provide a direct comparison of the techniques to gauge their effectiveness in achieving scalability. A review of the state-of-the-art ontology matching tools that employ each strategy is also provided. We also evaluate the state-of-the-art tools to gauge the progress they have made over the years in improving alignment’s quality and reduction of execution time when matching large ontologies.},
	number = {4},
	journal = {ACM Comput. Surv.},
	author = {Ochieng, Peter and Kyanda, Swaib},
	month = jul,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Survey, scalability, ontology mapping, mapping repair, repair},
}

@inproceedings{islam_ontology-driven_2019,
	address = {Montreal, Quebec, Canada},
	series = {{ICSSP} '19},
	title = {An ontology-driven approach to automating the process of integrating security software systems},
	url = {https://doi.org/10.1109/ICSSP.2019.00017},
	doi = {10.1109/ICSSP.2019.00017},
	abstract = {A wide variety of security software systems need to be integrated into a Security Orchestration Platform (SecOrP) to streamline the processes of defending against and responding to cybersecurity attacks. Lack of interpretability and interoperability among security systems are considered the key challenges to fully leverage the potential of the collective capabilities of different security systems. The processes of integrating security systems are repetitive, time-consuming and error-prone; these processes are carried out manually by human experts or using ad-hoc methods. To help automate security systems integration processes, we propose an \&lt;u\&gt;On\&lt;/u\&gt;tology-driven approach for \&lt;u\&gt;S\&lt;/u\&gt;ecurity OrchestrAtion Platform (OnSOAP). The developed solution enables interpretability, and interoperability among security systems, which may exist in operational silos. We demonstrate OnSOAP's support for automated integration of security systems to execute the incident response process with three security systems (Splunk, Limacharlie, and Snort) for a Distributed Denial of Service (DDoS) attack. The evaluation results show that OnSOAP enables SecOrP to interpret the input and output of different security systems, produce error-free integration details, and make security systems interoperable with each other to automate and accelerate an incident response process.},
	booktitle = {Proceedings of the {International} {Conference} on {Software} and {System} {Processes}},
	publisher = {IEEE Press},
	author = {Islam, Chadni and Babar, Muhammad Ali and Nepal, Surya},
	year = {2019},
	keywords = {ontology, automated integration process, incident response process, security orchestration, security system},
	pages = {54--63},
}

@inproceedings{arif_building_2019,
	address = {New York, NY, USA},
	series = {{ICCCM} '19},
	title = {Building a {Biomedical} {Ontology} for {Respiratory} {Tract} {Infection}},
	isbn = {978-1-4503-7195-7},
	url = {https://doi.org/10.1145/3348445.3348461},
	doi = {10.1145/3348445.3348461},
	abstract = {Respiratory tract infections are most common disease which can affect any human during any part of their age. According to sources almost 60\% of all antibiotic prescription is due to Respiratory tract infection. The concepts and their relations related to Respiratory tract infection are need to be explained with the help of biomedical literature as well as historical records. But these literature or records cannot be efficiently managed by users due to their unstructured representation. Biomedical Ontologies are best way to identify the concepts and their respective relations from huge amount of unstructured data. Our research aimed to create a biomedical Ontology for the domain of Respiratory tract infection using UMLS as a data source, which contains concepts, subtypes, their relationships, and semantic types. As a result ontology contains 26 main and sub types of Respiratory tract infections, also 234 broad relations with 107325 relation counts and 1151 narrow relation with 34580 relation counts. The ontology created is evaluated by domain experts and results are formulated.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Computer} and {Communications} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Arif, Khawaja Sarmad and Qamar, Usman and Wahab, Kanwal and Riaz, Muhammad Qasim},
	year = {2019},
	note = {event-place: Bangkok, Thailand},
	keywords = {Semantic web, Biomedical Ontology, UMLS, Respiratory Tract Infection, RTIs},
	pages = {8--12},
}

@inproceedings{pelzetter_using_2018,
	address = {New York, NY, USA},
	series = {{W4A} '18},
	title = {Using {Ontologies} as a {Foundation} for {Web} {Accessibility} {Tools}},
	isbn = {978-1-4503-5651-0},
	url = {https://doi.org/10.1145/3192714.3196316},
	doi = {10.1145/3192714.3196316},
	abstract = {Creating web sites has become quite a complex task. One of most important aspects of a modern web site is accessibility. However, despite extensive standards many web sites have accessibility issues. This paper presents a new approach for creating tools to improve the accessibility of web sites using ontologies.},
	booktitle = {Proceedings of the 15th {International} {Web} for {All} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Pelzetter, Jens},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {Accessibility Web Ontologies},
}

@article{jackson_alloy_2019,
	title = {Alloy: a language and tool for exploring software designs},
	volume = {62},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3338843},
	doi = {10.1145/3338843},
	abstract = {Exploiting a simple, expressive logic based on relations to describe designs and automate their analysis.},
	number = {9},
	journal = {Commun. ACM},
	author = {Jackson, Daniel},
	month = aug,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {66--76},
}

@inproceedings{yuan_modeling_2022,
	address = {New York, NY, USA},
	series = {{IPEC} '22},
	title = {Modeling {Analysis} of the {Influence} of {Seoul} {City} {Image} on {Tourists}' {Willingness} to {Revisit} {Based} on {Parallel} {Computing}},
	isbn = {978-1-4503-9578-6},
	url = {https://doi.org/10.1145/3544109.3544138},
	doi = {10.1145/3544109.3544138},
	abstract = {Tourism is closely related to people's lives today, and the development level of tourism informatization is an important indicator to measure the modern tourism industry. At present, more and more data on the Internet is released in the form of linked data, which reduces the complexity of the integration of distributed, heterogeneous or autonomous data, and at the same time promotes the application of linked data. The purpose of this article is to model the influence of Seoul's city image on tourists' willingness to revisit based on parallel computing. This paper studies the similarity calculation efficiency in the data set of passenger-related passenger revisiting intention resources. According to the established tourist tourism ontology, this paper adopts the MapReduce parallel computing framework to design a parallel computing method of related data similarity to improve the discovery efficiency of the related data model of the willingness of large-scale tourists to revisit. Experimental research shows that this article analyzes the difference in perceptions of various image factors by tourists of different ages, and finds that the Р value in the single-factor analysis of variance table is greater than 0.05, indicating that tourists of different ages do not have significant perceptions of each image factor.},
	booktitle = {Proceedings of the 3rd {Asia}-{Pacific} {Conference} on {Image} {Processing}, {Electronics} and {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Min},
	year = {2022},
	note = {event-place: Dalian, China},
	pages = {164--168},
}

@inproceedings{wen_toward_2021,
	address = {New York, NY, USA},
	series = {{HRI} '21 {Companion}},
	title = {Toward {Hybrid} {Relational}-{Normative} {Models} of {Robot} {Cognition}},
	isbn = {978-1-4503-8290-8},
	url = {https://doi.org/10.1145/3434074.3446353},
	doi = {10.1145/3434074.3446353},
	abstract = {Most previous work on enabling robots' moral competence has used norm-based systems of moral reasoning. However, a number of limitations to norm-based ethical theories have been widely acknowledged. These limitations may be addressed by role-based ethical theories, which have been extensively discussed in the philosophy of technology literature but have received little attention within robotics. My work proposes a hybrid role/norm-based model of robot cognitive processes including moral cognition.},
	booktitle = {Companion of the 2021 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Ruchen},
	year = {2021},
	note = {event-place: Boulder, CO, USA},
	keywords = {human-robot interaction, robot ethics, robotic moral competence, role ethics},
	pages = {568--570},
}

@inproceedings{khemiri_towards_2022,
	address = {Phoenix, Arizona},
	series = {{WSC} '21},
	title = {Towards a generic semiconductor manufacturing simulation model},
	abstract = {Simulation is one of the most used approaches to analyze semiconductor manufacturing systems. However, most simulation models are designed for single-use applications and study a limited set of problems that are not reusable afterwards. Recently, in order to overcome this problem, the idea to develop a generic wafer fab simulation model has emerged. Nonetheless, few papers address the development of a generic wafer fab simulation. This paper proposes a generic, data-driven simulation model to evaluate and analyze a wide range of problems arising in modern semiconductor manufacturing systems. We discuss the issues related to the genericity of such a simulation model and the data and semantic integration issues faced by users.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Khemiri, Abdelhak and Yugma, Claude and Dauzère-Pérès, Stéphane},
	year = {2022},
}

@inproceedings{popov_knowledge_2020,
	address = {New York, NY, USA},
	series = {{CompSysTech} '20},
	title = {Knowledge {Model} for {Developing}, {Searching} and {Using} {Personalized} {Learning} {Content} for {Learners}, {Having} {Dyslexia} {Disability}},
	isbn = {978-1-4503-7768-3},
	url = {https://doi.org/10.1145/3407982.3407997},
	doi = {10.1145/3407982.3407997},
	abstract = {Much research in the area of e-learning systems today is focused on personalization and adaptivity, but only a few of the proposed systems are intended for students, having learning disabilities. These learners are significant amount of all learners (children, having some dyslexia symptoms are between 10\% and 20\% of all children). Dyslexia symptoms can be specific for different learners and can require specific organization of learning. All the experts in learning say that suitable learning is a key for achieving excellent results for these learners.We made extensive study on the relevant theories intended for better understanding of the requirements of an e-learning for people, having learning disabilities and existing frameworks or tools for people with dyslexia. As a result we propose a knowledge model for developing, describing, searching and recommending personalized learning resources in the web for learners, having dyslexia learning disability. We also propose agent-based software architecture, that manage and use implemented knowledge for supporting leaning resource development, searching, ranking according to specified criteria and recommendation of these resources to dyslexic learners.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Computer} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Popov, Miroslav and Ivanova, Tatyana},
	year = {2020},
	note = {event-place: Ruse, Bulgaria},
	keywords = {Ontology, knowledge, E-Learning, Dyslexia, Web-based learning},
	pages = {258--265},
}

@inproceedings{chen_gap_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {From {Gap} to {Synergy}: {Enhancing} {Contextual} {Understanding} through {Human}-{Machine} {Collaboration} in {Personalized} {Systems}},
	isbn = {979-8-4007-0132-0},
	url = {https://doi.org/10.1145/3586183.3606741},
	doi = {10.1145/3586183.3606741},
	abstract = {This paper presents LangAware, a collaborative approach for constructing personalized context for context-aware applications. The need for personalization arises due to significant variations in context between individuals based on scenarios, devices, and preferences. However, there is often a notable gap between humans and machines in the understanding of how contexts are constructed, as observed in trigger-action programming studies such as IFTTT. LangAware enables end-users to participate in establishing contextual rules in-situ using natural language. The system leverages large language models (LLMs) to semantically connect low-level sensor detectors to high-level contexts and provide understandable natural language feedback for effective user involvement. We conducted a user study with 16 participants in real-life settings, which revealed an average success rate of 87.50\% for defining contextual rules in a variety of 12 campus scenarios, typically accomplished within just two modifications. Furthermore, users reported a better understanding of the machine’s capabilities by interacting with LangAware.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Weihao and Yu, Chun and Wang, Huadong and Wang, Zheng and Yang, Lichen and Wang, Yukun and Shi, Weinan and Shi, Yuanchun},
	year = {2023},
	note = {event-place: San Francisco, CA, USA},
	keywords = {Large Language Models, Personalization, Context-Aware Systems, End User Context Construction},
}

@article{malmi_development_2022,
	title = {Development and {Use} of {Domain}-specific {Learning} {Theories}, {Models}, and {Instruments} in {Computing} {Education}},
	volume = {23},
	url = {https://doi.org/10.1145/3530221},
	doi = {10.1145/3530221},
	abstract = {Use of theory within a field of research provides the foundation for designing effective research programs and establishing a deeper understanding of the results obtained. This, together with the emergence of domain-specific theory, is often taken as an indicator of the maturity of any research area. This article explores the development and subsequent usage of domain-specific theories and theoretical constructs (TCs) in computing education research (CER). All TCs found in 878 papers published in three major CER publication venues over the period 2005–2020 were identified and assessed to determine the nature and purpose of the constructs found. We focused more closely on areas related to learning, studying, and progression, where our analysis found 80 new TCs that had been developed, based on multiple epistemological perspectives. Several existing frameworks were used to categorize the areas of CER focus in which TCs were found, the methodology by which they were developed, and the nature and purpose of the TCs. A citation analysis was undertaken, with 1,727 citing papers accessed to determine to what extent and in what ways TCs had been used and developed to inform subsequent work, also considering whether these aspects vary according to different focus areas within computing education. We noted which TCs were used most often and least often, and we present several brief case studies that demonstrate progressive development of domain-specific theory. The exploration provides insights into trends in theory development and suggests areas in which further work might be called for. Our findings indicate a general interest in the development of TCs during the period studied, and we show examples of how different approaches to theory development have been used. We present a framework suggesting how strategies for developing new TCs in CER might be structured and discuss the nature of theory development in relation to the field of CER.},
	number = {1},
	journal = {ACM Trans. Comput. Educ.},
	author = {Malmi, Lauri and Sheard, Judy and Kinnunen, Päivi and {Simon} and Sinclair, Jane},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {theory, literature, Computing education, instrument, research, theoretical construct},
}

@inproceedings{da_silva_quirino_visual_2018,
	address = {New York, NY, USA},
	series = {{SBES} '18},
	title = {Visual notations for software pattern languages: a mapping study},
	isbn = {978-1-4503-6503-1},
	url = {https://doi.org/10.1145/3266237.3266266},
	doi = {10.1145/3266237.3266266},
	abstract = {Reuse has been recognized as an important practice in software engineering. The use of patterns makes it easier to reuse successful solutions, speeds up the development process, and promotes the application of good practices. Related patterns can be organized in a Pattern Language (PL), which represents the patterns and their relations, and provides guidance on how to select, reuse and integrate them. Visual notations are often used to provide a graphical representation to PLs. Aiming to investigate how PLs related to software have been visually represented, we carried out a systematic mapping. We identified and analyzed 64 PLs. As a result, we noticed a lack of consensus on the elements that should be represented in a PL and the symbols used to represent them. Moreover, most PLs have ambiguous or inexpressive visual representations.},
	booktitle = {Proceedings of the {XXXII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {da Silva Quirino, Glaice Kelly and Barcellos, Monalessa Perini and de Almeida Falbo, Ricardo},
	year = {2018},
	note = {event-place: Sao Carlos, Brazil},
	keywords = {visual notation, pattern language, mapping study},
	pages = {72--81},
}

@inproceedings{bocciarelli_resource_2023,
	address = {Singapore, Singapore},
	series = {{WSC} '22},
	title = {Resource {Modeling} in {Business} {Process} {Simulation}},
	abstract = {Business Process (BP) models address the specification of the flow of events and activities, along with the dependencies of activities on resources. BP models are often analyzed by using simulation-based approaches. This paper focuses on resource modeling for BP modeling and simulation, by first introducing the most important concepts and discussing how resources are modeled in the standard BP modeling notation (i.e., BPMN) and in the area of Discrete Event Simulation. Then, the paper presents two newer BP modeling and simulation approaches, namely the Object Event Modeling and Simulation with the Discrete Event Process Modeling Notation (DPMN) based on the JavaScript-based simulation framework OESjs, and Performability-enabled BPMN (PyBPMN), with the Java-based simulation framework eBPMN. A simple but effective case study dealing with a pizza service process is used to illustrate the main features of the presented approaches.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Bocciarelli, Paolo and D'Ambrogio, Andrea and Wagner, Gerd},
	year = {2023},
	pages = {1296--1310},
}

@inproceedings{cho_fishnet_2024,
	address = {New York, NY, USA},
	series = {{ICAIF} '24},
	title = {{FISHNET}: {Financial} {Intelligence} from {Sub}-querying, {Harmonizing}, {Neural}-{Conditioning}, {Expert} {Swarms}, and {Task} {Planning}},
	isbn = {979-8-4007-1081-0},
	url = {https://doi.org/10.1145/3677052.3698597},
	doi = {10.1145/3677052.3698597},
	abstract = {Financial intelligence generation from vast data sources has typically relied on traditional methods of knowledge-graph construction or database engineering. Recently, fine-tuned financial domain-specific Large Language Models (LLMs), have emerged. While these advancements are promising, limitations such as high inference costs, hallucinations, and the complexity of concurrently analyzing high-dimensional financial data, emerge. This motivates our invention FISHNET (Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert swarming, and Task planning), an agentic architecture that accomplishes highly complex analytical tasks for more than 98,000 regulatory filings that vary immensely in terms of semantics, data hierarchy, or format. FISHNET shows remarkable performance for financial insight generation (61.8\% success rate over 5.0\% Routing, 45.6\% RAG R-Precision). We conduct rigorous ablations to empirically prove the success of FISHNET, each agent’s importance, and the optimized performance of assembling all agents. Our modular architecture can be leveraged for a myriad of use-cases, enabling scalability, flexibility, and data integrity that are critical for financial tasks.},
	booktitle = {Proceedings of the 5th {ACM} {International} {Conference} on {AI} in {Finance}},
	publisher = {Association for Computing Machinery},
	author = {Cho, Nicole and Srishankar, Nishan and Cecchi, Lucas and Watson, William},
	year = {2024},
	note = {event-place: Brooklyn, NY, USA},
	keywords = {Planning, Harmonizing, LLM Agents, Sub-querying, Swarming},
	pages = {591--599},
}

@inproceedings{bernard_ontology-based_2018,
	address = {New York, NY, USA},
	series = {{SIGSPATIAL} '18},
	title = {An ontology-based algorithm for managing the evolution of multi-level territorial partitions},
	isbn = {978-1-4503-5889-7},
	url = {https://doi.org/10.1145/3274895.3274944},
	doi = {10.1145/3274895.3274944},
	abstract = {Through times, regions all over the world are very often subject to change (their names, their belonging, their composition, and their geometries). In this paper, we present a Semantic Matching Algorithm for automatically detecting, describing and publishing in the Linked Open Data Web, rich descriptions of changes occurring in multi-level territorial partitions (e.g., partitions made of major regions, regions and districts levels). We adopt a Linked Data (LD) approach for the semantic descriptions of the changes they undergo, relying on two existing generic ontologies, TSN-Ontology and TSN-Change Ontology. The created RDF graphs draw the lineage of each region over time (horizontal reading of the graphs), as well as the propagation of a change event through the partition levels (vertical reading).},
	booktitle = {Proceedings of the 26th {ACM} {SIGSPATIAL} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bernard, Camille and Plumejeaud-Perreau, Christine and Villanova-Oliver, Marlène and Gensel, Jérôme and Dao, Hy},
	year = {2018},
	note = {event-place: Seattle, Washington},
	keywords = {semantic web, linked open data, versioning, change detection, evolutive multi-level territorial partition, geospatial data matching, semantic matching algorithm, spatio-temporal ontology},
	pages = {456--459},
}

@inproceedings{li_reader-aware_2024,
	address = {New York, NY, USA},
	series = {{HT} '24},
	title = {Reader-aware {Writing} {Assistance} through {Reader} {Profiles}},
	isbn = {979-8-4007-0595-3},
	url = {https://doi.org/10.1145/3648188.3675152},
	doi = {10.1145/3648188.3675152},
	abstract = {Establishing rapport between authors and readers of scientific texts is essential for supporting readers in understanding texts as intended, facilitating socio-discursive practices within disciplinary communities, and helping in identifying interdisciplinary links among scientific writings. We propose a Reader-aware Congruence Assistant (RaCA), which supports writers to create texts that are adapted to target readers. Similar to user-centered design which is based on user profiles, RaCA features reader-centered writing through reader profiles that are dynamically computed from information discovered through academic search engines. Our assistant then leverages large language models to measure the congruence of a written text with a given reader profile, and provides feedback to the writer. We demonstrate our approach with an implemented prototype that illustrates how RaCA exploits information available on the Web to construct reader profiles, assesses writer-reader congruence and offers writers color-coded visual feedback accordingly. We argue that our approach to reader-oriented scientific writing paves the way towards the more personalized interaction of readers and writers with scientific content, and discuss how integration with Semantic Web technologies and Adaptive User Interface design can help materialize this vision within an ever-growing Web of scientific ideas, proof, and discourse.},
	booktitle = {Proceedings of the 35th {ACM} {Conference} on {Hypertext} and {Social} {Media}},
	publisher = {Association for Computing Machinery},
	author = {Li, Ge and Vachtsevanou, Danai and Lemée, Jérémy and Mayer, Simon and Strecker, Jannis},
	year = {2024},
	note = {event-place: Poznan, Poland},
	keywords = {Natural Language Processing, Personalized Text Adaptation, Reader Profile, Text Congruence},
	pages = {344--350},
}

@inproceedings{hanikova_towards_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Towards {Fact}-check {Summarization} {Leveraging} on {Argumentation} {Elements} {Tied} to {Entity} {Graphs}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3651914},
	doi = {10.1145/3589335.3651914},
	abstract = {Fact-check consumers can have different preferences regarding the amount of text being used for explaining the claim veracity verdict. Dynamically adapting the size of a fact-check report is thus an important functionality for systems designed to convey claim verification explainability. Recent works have experimented with applying transformers-based or LLM-based text summarization methods in a zero-shot or few-shot manner, making use of some existing texts available in the summary parts of fact-check reports (e.g., called "justification” in PolitiFact). However, for complex fact-checks, the purely sub-symbolic summarizers tend to either omit some elements of the fact-checker's argumentation chains or include contextual statements that may not be essential at the given level of granularity. In this paper, we propose a new method for enhancing fact-check summarization with the aim of injecting elements of structured fact-checker argumentation. This argumentation is, in turn, not only captured at the discourse level but tied to an entity graph representing the fact-check, for which we employ the PURO diagrammatic language. We have empirically performed a manual analysis of fact-check reports from two fact-checker websites, yielding (1) textual snippets containing the argumentation essence of the fact-check report and (2) categorized argumentation elements tied to entity graphs. These snippets are then fed to a state-of-the-art hybrid summarizer which has previously produced accurate fact-check summaries, as an additional input. We observe mild improvements on various ROUGE metrics, even if the validity of the results is limited given the small size of the dataset. We also compare the human-provided argumentation element categories with those returned, for the given fact-check ground truth summary, using a pre-trained language model upon both basic and augmented prompting. This yields a moderate accuracy as the model often fails to comply with the explicit given instructions.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Haniková, Kateřina and Chudán, David and Svátek, Vojtěch and Vajdečka, Peter and Troncy, Raphaël and Vencovský, Filip and Syrovátková, Jana},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {text summarization, argumentation, entity graph, fact-checking},
	pages = {1473--1481},
}

@inproceedings{dharuman_mprot-dpo_2024,
	address = {Atlanta, GA, USA},
	series = {{SC} '24},
	title = {{MProt}-{DPO}: {Breaking} the {ExaFLOPS} {Barrier} for {Multimodal} {Protein} {Design} {Workflows} with {Direct} {Preference} {Optimization}},
	isbn = {979-8-3503-5291-7},
	url = {https://doi.org/10.1109/SC41406.2024.00013},
	doi = {10.1109/SC41406.2024.00013},
	abstract = {We present a scalable, end-to-end workflow for protein design. By augmenting protein sequences with natural language descriptions of their biochemical properties, we train generative models that can be preferentially aligned with protein fitness landscapes. Through complex experimental- and simulation-based observations, we integrate these measures as preferred parameters for generating new protein variants and demonstrate our workflow on five diverse supercomputers. We achieve \&gt;1 ExaFLOPS sustained performance in mixed precision on each supercomputer and a maximum sustained performance of 4.11 ExaFLOPS and peak performance of 5.57 ExaFLOPS. We establish the scientific performance of our model on two tasks: (1) across a predetermined benchmark dataset of deep mutational scanning experiments to optimize the fitness-determining mutations in the yeast protein HIS7, and (2) in optimizing the design of the enzyme malate dehydrogenase to achieve lower activation barriers (and therefore increased catalytic rates) using simulation data. Our implementation thus sets high watermarks for multimodal protein design workflows.},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage}, and {Analysis}},
	publisher = {IEEE Press},
	author = {Dharuman, Gautham and Hippe, Kyle and Brace, Alexander and Foreman, Sam and Hatanpää, Väinö and Sastry, Varuni K. and Zheng, Huihuo and Ward, Logan and Muralidharan, Servesh and Vasan, Archit and Kale, Bharat and Mann, Carla M. and Ma, Heng and Cheng, Yun-Hsuan and Zamora, Yuliana and Liu, Shengchao and Xiao, Chaowei and Emani, Murali and Gibbs, Tom and Tatineni, Mahidhar and Canchi, Deepak and Mitchell, Jerome and Yamada, Koichi and Garzaran, Maria and Papka, Michael E. and Foster, Ian and Stevens, Rick and Anandkumar, Anima and Vishwanath, Venkatram and Ramanathan, Arvind},
	year = {2024},
	keywords = {Large language models, AI, HPC, protein design},
}

@inproceedings{gutlein_hide_2022,
	address = {Phoenix, Arizona},
	series = {{WSC} '21},
	title = {Hide your model! layer abstractions for data-driven co-simulations},
	abstract = {Modeling and simulating of problems that span across multiple domains can be tricky. Often, the need for a co-simulation arises, for example because the modeling cannot be done with a single tool. Domain experts may face a barrier when it comes to the implementation of such a co-simulation. In addition, the demand for integrating data from various sources into simulation models seems to be growing. Therefore, we propose an abstraction concept that hides simulators and models behind generalized interfaces that are derived from prototypical classes. The data-driven abstraction concept facilitates having an assembly kit with predefined simulator building blocks that can be easily plugged together. Furthermore, data streams can be seamlessly ingested into such a composed model. Likewise, the co-simulation can be accessed via the resulting interfaces for further processing and interactions.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Gütlein, Moritz and German, Reinhard and Djanatliev, Anatoli},
	year = {2022},
}

@inproceedings{li_new_2021,
	address = {New York, NY, USA},
	series = {{ICAIIS} 2021},
	title = {A {New} query method for the temporal {RDF} {Model} {RDFMT} {Based} on {SPARQL}},
	isbn = {978-1-4503-9020-0},
	url = {https://doi.org/10.1145/3469213.3470224},
	doi = {10.1145/3469213.3470224},
	abstract = {With the explosion of real-time data, the representation and query of temporal data has become a hot research topic. Many researchers have proposed various temporal representation models and query methods. On the basis of the proposed temporal model RDFMT, we put forward the query language SPARQLMT for RDFMT. SPARQLMT is expanded based on SPARQL language, adding a syntax and semantics that is convenient for querying temporal information. As we all know, SPARQL is the official query language of the standard RDF model. SPARQLMT is based on SPARQL, which is also conducive to using the SPARQL query engine. In this paper we mainly illustrate a query method SPARQLMT for the RDFMT by extending SPARQL and give the semantics and syntax of SPARQLMT.},
	booktitle = {2021 2nd {International} {Conference} on {Artificial} {Intelligence} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Li, Haixia},
	year = {2021},
	note = {event-place: Chongqing, China},
}

@article{liang_multizoo_2023,
	title = {{MULTIZOO} \&amp; {MULTIBENCH}: a standardized toolkit for multimodal deep learning},
	volume = {24},
	issn = {1532-4435},
	abstract = {Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MULTIZOO, a public toolkit consisting of standardized implementations of \&gt; 20 core multimodal algorithms and MULTIBENCH, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MULTIBENCH paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community. Code: https://github.com/pliang279/MultiBench Documentation: https://multibench.readthedocs.io/en/latest/},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Agarwal, Aravind and Cheng, Yun and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
	month = jan,
	year = {2023},
	note = {Publisher: JMLR.org},
	keywords = {representation learning, multimodal learning, open source software, benchmarks},
}

@inproceedings{shi_visualizing_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {Visualizing the {Flow} of {Discourse} with a {Concept} {Ontology}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3186943},
	doi = {10.1145/3184558.3186943},
	abstract = {Understanding and visualizing human discourse has long being a challenging task. Although recent work on argument mining have shown success in classifying the role of various sentences, the task of recognizing concepts and understanding the ways in which they are discussed remains challenging. Given an email thread or a transcript of a group discussion, our task is to extract the relevant concepts and understand how they are referenced and re-referenced throughout the discussion. In the present work, we present a preliminary approach for extracting and visualizing group discourse by adapting Wikipedia's category hierarchy to be an external concept ontology. From a user study, we found that our method achieved better results than 4 strong alternative approaches, and we illustrate our visualization method based on the extracted discourse flows.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Shi, Baoxu and Weninger, Tim},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {concept ontology, discourse visualization},
	pages = {89--90},
}

@inproceedings{takhom_knowledge_2022,
	address = {New York, NY, USA},
	series = {{IJCKG} '21},
	title = {Knowledge {Graph} {Enhanced} {Community} {Consensus}: a {Scenario}-based {Knowledge} {Construction} on {Buddha} {Images}},
	isbn = {978-1-4503-9565-6},
	url = {https://doi.org/10.1145/3502223.3502744},
	doi = {10.1145/3502223.3502744},
	booktitle = {Proceedings of the 10th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Takhom, Akkharawoot and Utasri, Tharathon and Leenoi, Dhanon and Soomjinda, Pitchaya and Boonkwan, Prachya and Supnithi, Thepchai},
	year = {2022},
	note = {event-place: Virtual Event, Thailand},
	keywords = {Knowledge graph, Semantic Web, Ontology development, Knowledge engineering, Community consensus, Community-driven approach, Knowledge construction},
	pages = {191--194},
}

@inproceedings{cao_clinical_2020,
	address = {New York, NY, USA},
	series = {{SPML} '20},
	title = {Clinical {Decision} {Support} {System} {Based} on {KNN}/{Ontology} {Extraction} {Method}},
	isbn = {978-1-4503-7573-3},
	url = {https://doi.org/10.1145/3432291.3432305},
	doi = {10.1145/3432291.3432305},
	abstract = {The complexity of the knowledge structure in the clinical cases, involving a wide range of attributes, results in making its case similarity calculation more complex. The existing medical ontologies, due to different expressions of the same concepts in computer information retrieval, causes difficulties in terms of sharing useful information in different database systems. This paper constructs a new decision support system based on KNN/ontology method was proposed. The detail of the methods and processes of common clinical case knowledge acquisition in combination with the method of obtaining structured information has been presented. The clinical case data similarity calculation method based on various types such as symptom information, medical history information, complications, surgical information, diagnostic results and other information, for record of a clinical diagnosis and treatment process. The validity of the similarity calculation method and the weight calculation method is verified by the clinical case data. The proposed methods can be effective for improving the quality and level of clinical services for medical service organizations.},
	booktitle = {Proceedings of the 2020 3rd {International} {Conference} on {Signal} {Processing} and {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Suqun and Lingao, Wang and Ji, Rendong and Wang, Chao and Yao, Liu and Kai, Lin and Abdalla, Ahmed N. and k., Sujatha},
	year = {2020},
	note = {event-place: Beijing, China},
	keywords = {Similarity, Neural Network, clinical medicine, extraction method},
	pages = {56--62},
}

@inproceedings{signe_substring_2025,
	address = {New York, NY, USA},
	series = {{ICTIR} '25},
	title = {A {Substring} {Extraction}-{Based} {RAG} {Method} for {Minimising} {Hallucinations} in {Aircraft} {Maintenance} {Question} {Answering}},
	isbn = {979-8-4007-1861-8},
	url = {https://doi.org/10.1145/3731120.3744624},
	doi = {10.1145/3731120.3744624},
	abstract = {Hallucination occurs when a language model generates plausible yet nonfactual information. In particular, faithfulness hallucinations (inconsistency with a given context) cannot be tolerated in critical domains such as aircraft maintenance due to the potentially severe consequences. To mitigate this issue, Retrieval Augmented Generation (RAG) methods have been introduced. These approaches are relevant for reducing the risks of hallucination but do not eliminate them, as the generator may still produce content unfaithful to the retrieved context. This paper proposes a novel RAG approach that leverages a substring extraction tool from retrieved documents to minimise hallucinations. Experiments performed on real aircraft maintenance documentation revealed that, despite the lower accuracy of the answers compared to traditional RAG methods, the proposed approach demonstrates an improved control over hallucination risks. This highlights the potential of our method in highly technical use cases where accuracy and reliability are key.},
	booktitle = {Proceedings of the 2025 {International} {ACM} {SIGIR} {Conference} on {Innovative} {Concepts} and {Theories} in {Information} {Retrieval} ({ICTIR})},
	publisher = {Association for Computing Machinery},
	author = {Signé, Quentin and Boughanem, Mohand and Moreno, Jose G. and Belkacem, Thiziri},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {retrieval augmented generation, question answering, technical maintenance},
	pages = {513--521},
}

@inproceedings{kaindl_modelgenguis_2021,
	address = {New York, NY, USA},
	series = {{IUI} '21 {Companion}},
	title = {{ModelGenGUIs} – {High}-level {Interaction} {Design} with {Discourse} {Models} for {Automated} {GUI} {Generation}},
	isbn = {978-1-4503-8018-8},
	url = {https://doi.org/10.1145/3397482.3450619},
	doi = {10.1145/3397482.3450619},
	abstract = {Since manual creation of user interfaces is hard and expensive, automated generation may become more and more important in the future. Instead of generating UIs from simple abstractions, transforming them from high-level models should be more attractive. In particular, we let an interaction designer model discourses in the sense of dialogues (supported by a tool), inspired by human-human communication. This tutorial informs about our approach, both about its advantages and its challenges (e.g., in terms of usability of generated UIs). In particular, our unique approach to optimization for a given device (e.g., a Smartphone) that applies Artificial Intelligence (AI) techniques will be high-lighted, as well as the techniques based on ontologies for automated GUI generation and customization. We also address low-vision accessibility of Web-pages, by combining automated design-time generation of Web-pages with responsive design for improving accessibility.},
	booktitle = {Companion {Proceedings} of the 26th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Kaindl, Hermann},
	year = {2021},
	note = {event-place: College Station, TX, USA},
	keywords = {customization, automated GUI generation, discourse models, Interaction design, low-vision accessibility of Web-pages, task models},
	pages = {3--4},
}

@inproceedings{anelli_interpretability_2022,
	address = {New York, NY, USA},
	series = {{CIKM} '22},
	title = {Interpretability of {BERT} {Latent} {Space} through {Knowledge} {Graphs}},
	isbn = {978-1-4503-9236-5},
	url = {https://doi.org/10.1145/3511808.3557617},
	doi = {10.1145/3511808.3557617},
	abstract = {The advent of pretrained language have renovated the ways of handling natural languages, improving the quality of systems that rely on them. BERT played a crucial role in revolutionizing the Natural Language Processing (NLP) area. However, the deep learning framework it implements lacks interpretability. Thus, recent research efforts aimed to explain what BERT learns from the text sources exploited to pre-train its linguistic model. In this paper, we analyze the latent vector space resulting from the BERT context-aware word embeddings. We focus on assessing whether regions of the BERT vector space hold an explicit meaning attributable to a Knowledge Graph (KG). First, we prove the existence of explicitly meaningful areas through the Link Prediction (LP) task. Then, we demonstrate these regions being linked to explicit ontology concepts of a KG by learning classification patterns. To the best of our knowledge, this is the first attempt at interpreting the BERT learned linguistic knowledge through a KG relying on its pretrained context-aware word embeddings.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Anelli, Vito Walter and Biancofiore, Giovanni Maria and De Bellis, Alessandro and Di Noia, Tommaso and Di Sciascio, Eugenio},
	year = {2022},
	note = {event-place: Atlanta, GA, USA},
	keywords = {knowledge graphs, natural language processing, deep learning},
	pages = {3806--3810},
}

@inproceedings{kici_text_2021,
	address = {USA},
	series = {{CASCON} '21},
	title = {Text classification on software requirements specifications using transformer models},
	abstract = {Text classification in Software Requirements Specifications (SRS) documents is an essential task for various purposes including automatically extracting requirements and their types as well as identification of duplicate or conflicting information, which all contribute to avoiding potential issues in the later stages of the software development life cycle. While a variety of machine learning approaches have been considered for text classification over SRS documents, many of these fail to provide adequate performance as they often ignore the meaning of software artifacts or integrate domain knowledge for the classification task. Recent advances in deep learning methodology have significantly contributed to Natural Language Processing (NLP) and text classification. One of the main challenges in using deep learning models for various NLP tasks in the software engineering domain is the scarcity of labeled textual data. In addition, even with sufficient data, training from the scratch still requires significant training time and computational resources. Transfer learning is a novel approach that proposes a solution to such reservations by providing pre-trained models that enable fine-tuning with the customized data. In this research, we conduct an empirical analysis on multi-class text classification over SRS documents using different pre-trained transformer models including BERT, DistilBERT, Roberta, AlBERT, and XLNet, and compare their performance. We test the performance of these models using three SRS datasets: DOORS, NFR-PROMISE, and PURE. Our numerical study shows that the transformer models are able to generate highly accurate results to classify all categories except Priority of the requirements. While all models provide a 80\% or higher accuracy for other classification tasks, the accuracy of the models to classify the Priority does not exceed 60\%.},
	booktitle = {Proceedings of the 31st {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Kici, Derya and Bozanta, Aysun and Cevik, Mucahit and Parikh, Devang and Başar, Ayşe},
	year = {2021},
	note = {event-place: Toronto, Canada},
	keywords = {BERT, NLP, text classification, transfer learning, software requirement specifications},
	pages = {163--172},
}

@inproceedings{refoufi_robust_2018,
	address = {New York, NY, USA},
	series = {{MedPRAI} '18},
	title = {A {Robust} {Approach} to the {Ontology} {Matching} {Problem}},
	isbn = {978-1-4503-5290-1},
	url = {https://doi.org/10.1145/3177148.3180086},
	doi = {10.1145/3177148.3180086},
	abstract = {Ontology matching is the process that identifies correspondences between similar concepts in two different ontologies of the same domain of discourse to solve knowledge heterogeneous problems. We propose an automatic similarity based matching algorithm that exploits almost all types of entities descriptions as well as their relations to effectively compute the correspondences between the two to be matched ontologies. The iterative algorithm computes each measure of similarity separately and then aggregates them in a linear combination to compose the final similarity score. The measures used deal with linguistic, semantic, and structural as well as many other measures to gain efficiency. We also include a new similarity measure based on dynamic programming in conjunction with known measures to refine the similarity process. Finally, we provide comparative experimental results in support of our method on several well-known ontology benchmarks recommended by the OAEI1. The results obtained are shown to be quite superior compared to the state-of-the-art ontology matching systems.},
	booktitle = {Proceedings of the 2nd {Mediterranean} {Conference} on {Pattern} {Recognition} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Refoufi, Allaoua and Benarab, Achref},
	year = {2018},
	note = {event-place: Rabat, Morocco},
	keywords = {Ontology matching, WordNet, ontology alignment, OWL/XML, similarity measure},
	pages = {76--83},
}

@inproceedings{li_mixed_2024,
	address = {New York, NY, USA},
	series = {{CECCT} '23},
	title = {Mixed {Knowledge}-enhance {Empathetic} {Dialogue} {Generation}},
	isbn = {979-8-4007-1630-0},
	url = {https://doi.org/10.1145/3637494.3637508},
	doi = {10.1145/3637494.3637508},
	abstract = {Empathy plays a pivotal role in human communication, and thus, it is an essential capability that any human-centered dialogue system should possess. Early research on empathetic response generation often focused on directly capturing the emotional state of the context using fixed emotion labels. However, the logical aspects exhibited in human conversations heavily rely on experiential and knowledge-based resources within the brain. This implies that whether the aim is to acquire more nuanced emotional states or to generate responses enriched with comprehensive information, the incorporation of external knowledge as supplementary information in empathetic dialogue systems is imperative. In response to this challenge, we propose a novel approach for extracting external knowledge. This is achieved by designing two components: a fine-grained knowledge graph constructed using the context and an external knowledge base, and coarse-grained knowledge acquisition based on COMET. These two scales of knowledge are then integrated with the context using methods like context refinement. This not only make the model to gain a deeper understanding of the user's context but also enhances the expression of empathy in the dialogue system. We conducted extensive experiments on the EMPATHETICDIALOGUES dataset and demonstrated the superiority of our approach over the baseline model.CCS CONCEPTS • Computing methodologies∼Artificial intelligence∼Natural language processing∼Discourse, dialogue and pragmatics},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Electronics}, {Computers} and {Communication} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Li, Xuxin and Wang, Ge and Wang, Yue and Zhou, Qirui},
	year = {2024},
	note = {event-place: Guilin, China},
	keywords = {Natural Language Processing, deep learning, dialog system, empathetic dialogue generation, external knowledge},
	pages = {77--81},
}

@inproceedings{guermah_using_2018,
	address = {New York, NY, USA},
	series = {{LOPAL} '18},
	title = {Using {Context} {Ontology} and {Linear} {SVM} for {Chronic} {Kidney} {Disease} {Prediction}},
	isbn = {978-1-4503-5304-5},
	url = {https://doi.org/10.1145/3230905.3230941},
	doi = {10.1145/3230905.3230941},
	abstract = {In the e-Health learning area, the use of chronic patient context has become very important given the increase in the number of individuals who suffer from these diseases and the unavailability of medications. Specifically, chronic kidney failure is one of the diseases that goes undetected and undiagnosed until it is well advanced. The need for preventive prediction remains an essential task for the well-being of patients at risk. In this paper, we aim to explore the added value of using ontology-based prediction, focusing on Linear SVM, to deal with Chronic Kidney Disease Problems.},
	booktitle = {Proceedings of the {International} {Conference} on {Learning} and {Optimization} {Algorithms}: {Theory} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Guermah, Hatim and Fissaa, Tarik and Guermah, Bassma and Hafiddi, Hatim and Nassar, Mahmoud},
	year = {2018},
	note = {event-place: Rabat, Morocco},
	keywords = {Ontologies, Context-Awareness, Chronic Kidney Disease, Linear SVM},
}

@inproceedings{rossi_multilevel_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {A multilevel modelling approach for tourism flows detection},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00020},
	doi = {10.1109/MODELS-C.2019.00020},
	abstract = {Application development in the Internet of Things (IoT) faces various issues such as lack of separation of concerns and lack of high-level abstraction to address its large scale and heterogeneity. MDE supports the management of this heterogeneity raising the level of abstraction and thanks to its core operations. Multilevel modelling makes it possible to extend MDE techniques to more than two meta-levels permitting model elements to have a dual type-instance dimension, making it particularly suitable for this application domain. People flow monitoring and detection is one of the hot topics in smart cities projects. In this paper, we exploit MDE techniques, through multilevel modelling approaches, to design the infrastructure supporting a solution part of a comprehensive project related to urban informatics. Moreover, even if we target the people flow monitoring and detection scenario, the provided multilevel approach is open and extensible to further IoT scenarios, to specifically manage the evolutionary nature of the IoT.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Rossi, Maria Teresa and De Sanctis, Martina and Iovino, Ludovico and Rutle, Adrian},
	year = {2021},
	pages = {103--112},
}

@article{ke_efficient_2024,
	title = {Efficient {Validation} of {SHACL} {Shapes} with {Reasoning}},
	volume = {17},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3681954.3682023},
	doi = {10.14778/3681954.3682023},
	abstract = {As the usage of knowledge graphs (KGs) becomes more pervasive in practical applications, there is a burgeoning need for high-quality data. The SHApes Constraint Language (SHACL) allows for expressing certain types of quality constraints that define sub-structures and correct values in KGs modelled with RDF. Nevertheless, performing SHACL validation without entailment often yields onesided outcomes, as it falls short of validating crucial implicit data encoded in the KG ontology. Current solutions that incorporate entailment into SHACL validation are inefficient, due to the time-intensive process of applying inference rules to the entire dataset. Moreover, applying entailment for SHACL validation can generate large amounts of redundant triples, exacerbating the validation workload and resulting in erroneous or redundant validation results. In light of these challenges, we propose Re-SHACL, an approach that combines targeted reasoning and entity merging techniques to generate a concise, consolidated RDF graph devoid of redundancy. Re-SHACL significantly reduces execution time and improves the accuracy of the validation reports. Our experiments demonstrate that Re-SHACL can be combined with state-of-the-art validators to deliver accurate validation reports efficiently.},
	number = {11},
	journal = {Proc. VLDB Endow.},
	author = {Ke, Jin and Zacouris, Zenon and Acosta, Maribel},
	month = jul,
	year = {2024},
	note = {Publisher: VLDB Endowment},
	pages = {3589--3601},
}

@inproceedings{schroeder_towards_2023,
	address = {Singapore, Singapore},
	series = {{WSC} '22},
	title = {Towards {Reusable} {Building} {Blocks} to {Develop} {Covid}-19 {Simulation} {Models}},
	abstract = {Modeling \&amp; Simulation has played an essential role in supporting the decision-making activities of policymakers for COVID-19. However, a proliferation of models has been noted in the literature, and new models are only more likely to emerge given the shift to long-term management of the disease and the call for highly tailored tools. Having a multiplicity of models can have benefits, for example when contributing to ensembles of models. However, if each model is created from scratch, there is significant redundancy in efforts hence time inefficiency and a heightened risk of bugs. Our study examines the naturally occurring practices of modelers who wrote COVID-19 models in NetLogo to identify redundancy in code and thus suggest reusable 'building blocks' that would speed-up the process of model development as well as improving code quality. Based on 28 models, we identified five themes and discussed their transformation into potential building blocks for simulation.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Schroeder, Shane A. and Vendome, Christopher and Giabbanelli, Philippe J. and Montfort, Alan M.},
	year = {2023},
	pages = {569--580},
}

@inproceedings{dave_3rd_2024,
	address = {New York, NY, USA},
	series = {{WSDM} '24},
	title = {The 3rd {International} {Workshop} on {Interactive} and {Scalable} {Information} {Retrieval} {Methods} for {eCommerce} ({ISIR}-{eCom} 2024)},
	isbn = {979-8-4007-0371-3},
	url = {https://doi.org/10.1145/3616855.3635724},
	doi = {10.1145/3616855.3635724},
	abstract = {Over the past few years, consumer behavior has shifted from traditional in-store shopping to online shopping. For example, eCommerce sales have grown from around 5\% of total US sales in 2012 to around 15.4\% in year 2023. This rapid growth of eCommerce has created new challenges and vital new requirements for intelligent information retrieval systems. Which lead to the primary motivations of this workshop:(1) Since the pandemic hit, eCommerce became an important part of people's routine and they started using online shop- ping for smallest grocery items to big electronics as well as cars. With such a large assortment of products and millions of users, achieving higher scalability without losing accuracy is a leading concern for information retrieval systems for eCommerce.(2) The diverse buyers make the relevance of the results highly subjective, because relevance varies for different buyers. The most suitable and intuitive solution to this problem is to make the system interactive and provide correct relevance for different users. Hence, interactive information retrieval systems are becoming necessity in eCommerce.(3) To handle sudden change in buyers' behavior, industries adopted existing sub-optimal information retrieval techniques for various eCommerce tasks. Parallelly, they also started exploring/researching for better solutions and in dire need of help from research community.This workshop will provide a forum to discuss and learn the latest trends for interactive and scalable information retrieval approaches for eCommerce. It will provide academic and industrial researchers a platform to present their latest works, share research ideas, present and discuss various challenges, and identify the areas where further research is needed. It will foster the development of a strong research community focused on solving eCommerce-related information retrieval problems that provide superior eCommerce experience to all users.},
	booktitle = {Proceedings of the 17th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Dave, Vachik S. and Pang, Linsey and Cui, Xiquan and Luo, Chen and Zamani, Hamed and Wu, Lingfei and Karypis, George},
	year = {2024},
	note = {event-place: Merida, Mexico},
	keywords = {information retrieval, recommender systems, interactive systems, ecommerce search, large language models (llms) in ecommerce, natural language processing (nlp) for ecommerce, ranking models},
	pages = {1208--1209},
}

@article{abraham_semantic_2025,
	title = {Semantic {Foundations} for {Precision} {Medicine}},
	url = {https://doi.org/10.1145/3745789},
	doi = {10.1145/3745789},
	abstract = {Precision medicine, which aims to optimize medical care at the individual level, remains a significant challenge and aspiration in oncology. The pathway to a successful implementation requires methods that can work with a vast heterogeneity of cancer, and consider the interplay of environmental, societal, biological, and clinical factors. To support decision-making in this context, computational frameworks must integrate large-scale, diverse, and noisy data, discover fine-grained patient subgroups with shared underlying characteristics, and characterize the imperfect preclinical spaces where novel therapies are tested. We propose an integrated digital-twin framework in which machine learning and semantic models collaboratively represent and reason with diverse patient data and medical domain knowledge to generate treatment recommendations. Clinical and molecular characteristics are used to discover subtypes of brain cancers, which are represented as ontologies with associated rules to determine a patient’s membership in a given subtype. Similarly, preclinical models used for therapeutic testing are characterized and assessed for their similarity to patient cancer models. By semantically discovering links between these preclinical models and patient cancer subtypes, novel therapeutics tested on preclinical models can be prioritized and hypothesized for individual patients. This approach, which requires empirical testing, demonstrates how cross-domain reasoning can be used to propose individualized treatment plans.},
	journal = {ACM Trans. Comput. Healthcare},
	author = {Abraham, Joel and Austin, Mark and Gilbert, Mark R. and Celiku, Orieta},
	month = jun,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontologies, Machine Learning, Digital Twins, Semantic Models},
	annote = {Just Accepted},
}

@inproceedings{saleme_towards_2018,
	address = {New York, NY, USA},
	series = {{MEDES} '18},
	title = {Towards a reference ontology on mulsemedia systems},
	isbn = {978-1-4503-5622-0},
	url = {https://doi.org/10.1145/3281375.3281378},
	doi = {10.1145/3281375.3281378},
	abstract = {The use of multiple senses in interactive applications has become increasingly feasible due to the upsurge of commercial, off-the-shelf devices to produce sensory effects. Creating Multiple Sensorial Media (MulSeMedia) immersive systems requires understanding their digital ecosystem. Mulsemedia systems encompass a set of applications, and devices of different types assembled to communicate or express feelings from the virtual world to the real world. Despite existing standards, tools, and recent research devoted to them, there is still a lack of formal and explicit representation of what mulse-media is. Misconceptions could eventually lead to the construction of solutions that might not take into account reuse, integration, standardization, among other design features. In this paper, we propose to establish a common conceptualization about mulsemedia systems through a reference ontology, named MulseOnto, covering their main notions. To evaluate it, we applied ontology verification and validation techniques, including assessment by humans and a data-driven approach. The results showed that MulseOnto can be used as a consensual conceptual model for exploring the knowledge about the whole chain of mulsemedia systems.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Saleme, Estêvão B. and Santos, Celso A. S. and Falbo, Ricardo A. and Ghinea, Gheorghita and Andres, Frederic},
	year = {2018},
	note = {event-place: Tokyo, Japan},
	keywords = {mulsemedia systems, multimedia, reference ontology},
	pages = {23--30},
}

@inproceedings{esterhuyse_cooperative_2024,
	address = {New York, NY, USA},
	series = {{SLE} '24},
	title = {Cooperative {Specification} via {Composition} {Control}},
	isbn = {979-8-4007-1180-0},
	url = {https://doi.org/10.1145/3687997.3695635},
	doi = {10.1145/3687997.3695635},
	abstract = {High-level, declarative specification languages are typically highly modular: specifications are comprised of fragments that are themselves meaningful. As such, complex specifications are built from incrementally composed fragments. In a cooperative specification, different fragments are contributed by different agents, usually capturing requirements on different facets of the system. For example, legal regulators and system administrators cooperate to specify the behaviour of a data exchange system. In practice, cooperative specification is difficult, as different contributors' requirements are difficult to elicit, express, and compose. In this work, we characterise cooperative specification and adopt an approach that leverages language features specifically introduced for controlling specification composition. In our approach, specifications model the domain as usual, but also specify how specifications may change. For example, a legal regulator defines 'consent to process data' and specifies which agents may consent, and which relaxations of the requirement are permitted. We propose and demonstrate generic language extensions that improve composition control in three case study languages: Datalog, Alloy, and eFLINT. We reflect on how these extensions improve composition control, and afford new data exchange scenarios. Finally, we relate our contributions to existing works, and to the greater vision of multi-agent data exchange to the satisfaction of their shared, complex, dynamic requirements.},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Esterhuyse, Christopher A. and van Binsbergen, L. Thomas},
	year = {2024},
	note = {event-place: Pasadena, CA, USA},
	keywords = {Data Exchange, Meta-Programming, Program Composition, Program Refinement, Specification Languages},
	pages = {2--15},
}

@article{rizvi_efficient_2020,
	title = {Efficient {Authorization} of {Graph}-database {Queries} in an {Attribute}-supporting {ReBAC} {Model}},
	volume = {23},
	issn = {2471-2566},
	url = {https://doi.org/10.1145/3401027},
	doi = {10.1145/3401027},
	abstract = {Neo4j is a popular graph database that offers two versions: an enterprise edition and a community edition. The enterprise edition offers customizable Role-based Access Control features through custom developed procedures, while the community edition does not offer any access control support. Being a graph database, Neo4j appears to be a natural application for Relationship-Based Access Control (ReBAC), an access control paradigm where authorization decisions are based on relationships between subjects and resources in the system (i.e., an authorization graph). In this article, we present AReBAC, an attribute-supporting ReBAC model for Neo4j that provides finer-grained access control by operating over resources instead of procedures. AReBAC\&nbsp;employs Nano-Cypher, a declarative policy language based on Neo4j’s Cypher query language, the result of which allows us to weave database queries with access control policies and evaluate both simultaneously. Evaluating the combined query and policy produces a result that (i) matches the search criteria, and (ii) the requesting subject is authorized to access. AReBAC\&nbsp;is accompanied by the algorithms and their implementation required for the realization of the presented ideas, including GP-Eval, a query evaluation algorithm. We also introduce Live-End Backjumping (LBJ), a backtracking scheme that provides a significant performance boost over conflict-directed backjumping for evaluating queries. As demonstrated in our previous work, the original version of GP-Eval already performs significantly faster than the Neo4j’s Cypher evaluation engine. The optimized version of GP-Eval, which employs LBJ, further improves the performance significantly, thereby demonstrating the capabilities of the technique.},
	number = {4},
	journal = {ACM Trans. Priv. Secur.},
	author = {Rizvi, Syed Zain Raza and Fong, Philip W. L.},
	month = jul,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Neo4j, graph database, attributes, graph patterns, live-end backjumping, nano-cypher, Relationship-based access control},
}

@inproceedings{rusmawati_automated_2022,
	address = {New York, NY, USA},
	series = {{IJCKG} '21},
	title = {Automated {Reasoning} on {Machine} {Learning} {Model} of {Legislative} {Election} {Prediction}},
	isbn = {978-1-4503-9565-6},
	url = {https://doi.org/10.1145/3502223.3502746},
	doi = {10.1145/3502223.3502746},
	abstract = {Prediction models using machine learning have been utilized in various fields, including the general election prediction model. However, we still need more insight into the model result through explainable AI. To reason the result, in this in-use paper, here we compare two approaches: using ontology reasoner Protégé and Silas (a machine learning tool empowered with automated reasoning). Using the data set of the Indonesia legislative election in 2019, we build the prediction model, followed by extracting the formula from the decision tree then reasoning the model predicates. The result shows that to some extent we can have a better understanding of the reasonable result from the machine learning prediction model.},
	booktitle = {Proceedings of the 10th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Rusmawati, Yanti},
	year = {2022},
	note = {event-place: Virtual Event, Thailand},
	keywords = {machine learning, explainable AI, automated reasoning},
	pages = {200--204},
}

@inproceedings{andresel_stable_2020,
	address = {New York, NY, USA},
	series = {{WWW} '20},
	title = {Stable {Model} {Semantics} for {Recursive} {SHACL}},
	isbn = {978-1-4503-7023-3},
	url = {https://doi.org/10.1145/3366423.3380229},
	doi = {10.1145/3366423.3380229},
	abstract = {SHACL (SHape Constraint Language) is a W3C recommendation for validating graph-based data against a set of constraints (called shapes). Importantly, SHACL allows to define recursive shapes, i.e. a shape may refer to itself, directly of indirectly. The recommendation left open the semantics of recursive shapes, but proposals have emerged recently to extend the official semantics to support recursion. These proposals are based on the principle of possibility (or non-contradiction): a graph is considered valid against a schema if one can assign shapes to nodes in such a way that all constraints are satisfied. This semantics is not constructive, as it does not provide guidelines about how to obtain such an assignment, and it may lead to unfounded assignments, where the only reason to assign a shape to a node is that it allows validating the graph. In contrast, we propose in this paper a stricter, more constructive semantics for SHACL, based on stable models, which are well-known in Answer Set Programming (ASP). This semantics additionally requires a shape assignment to be properly justified by the input constraints. We further exploit the connection to logic programming, and show that SHACL constraints can be naturally represented as logic programs, and that the validation problem for a graph and a SHACL schema can be encoded as an ASP reasoning task. The proposed semantics also enjoys computationally tractable validation in the presence of constraints with stratified negation (as opposed to the previous semantics). We also extend our semantics to 3-valued stable models, which yields a more relaxed notion of validation, tolerant to certain faults in the schema or data. By exploiting a connection between 3-valued stable model semantics and the well-founded semantics for logic programs, we can use our translation into ASP to show another tractability result. Finally, we provide a preliminary evaluation of the approach, which leverages an ASP solver to perform graph validation.},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {Association for Computing Machinery},
	author = {Andresel, Medina and Corman, Julien and Ortiz, Magdalena and Reutter, Juan L. and Savkovic, Ognjen and Simkus, Mantas},
	year = {2020},
	note = {event-place: Taipei, Taiwan},
	keywords = {SHACL, answer set programming, graph-structured data},
	pages = {1570--1580},
}

@article{harikrishna_childrens_2019,
	title = {Children’s {Story} {Classification} in {Indian} {Languages} {Using} {Linguistic} and {Keyword}-based {Features}},
	volume = {19},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3342356},
	doi = {10.1145/3342356},
	abstract = {The primary objective of this work is to classify Hindi and Telugu stories into three genres: fable, folk-tale, and legend. In this work, we are proposing a framework for story classification (SC) using keyword and part-of-speech (POS) features. For improving the performance of SC system, feature reduction techniques and combinations of various POS tags are explored. Further, we investigated the performance of SC by dividing the story into parts depending on its semantic structure. In this work, stories are (i) manually divided into parts based on their semantics as introduction, main, and climax; and (ii) automatically divided into equal parts based on number of sentences in a story as initial, middle, and end. We have also examined sentence increment model, which aims at determining an optimum number of sentences required to identify story genre by incremental selection of sentences in a story. Experiments are conducted on Hindi and Telugu story corpora consisting of 300 and 150 short stories, respectively. The performance of SC system is evaluated using different combinations of keyword and POS-based features, with three well-established machine learning classifiers: (i) Naive Bayes (NB), (ii) k-Nearest Neighbour (KNN), and (iii) Support Vector Machine (SVM). Performance of the classifier is evaluated using 10-fold cross-validation and effectiveness of classifier is measured using precision, recall, and F-measure. From the classification results, it is observed that adding linguistic information boosts the performance of story classification. In view of the structure of the story, main, and initial parts of the story have shown comparatively better performance. The results from the sentence incremental model have indicated that the first nine and seven sentences in Hindi and Telugu stories, respectively, are sufficient for better classification of stories. In most of the studies, SVM models outperformed the other models in classification accuracy.},
	number = {2},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Harikrishna, D. M. and Rao, K. Sreenivasa},
	month = nov,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {support vector machines, K-nearest neighbour, keyword features, latent semantic analysis, linear discriminant analysis, linguistic features, naive Bayes, sparse representation, story classification, text-to-speech, vector space model},
}

@article{feng_cost_2022,
	title = {A {Cost} {Estimation} {Model} for {Scrum} {Projects}},
	volume = {38},
	issn = {1937-4771},
	abstract = {One of the daunting tasks of software developers is to estimate the development cost of a new software product. Most cost estimation models use the set of requirements for the product as the starting point. Function-point, COCOMO and use case-based cost estimation models belong to this category. These models assume that the requirements of the product are fairly rigid. However, with the advent of agile-based software development methods, the requirements keep changing during the development process. Therefore, traditional cost estimation models need to be refined to accommodate changes in requirements. The refinements should reflect the changes in cost when the requirements change. In this paper, we describe a cost estimation model for projects that use scrum, a popular agile method. The model uses the requirements of the new software product, written in the form of user stories, as the primary source. The cost is adjusted every time the requirements are changed or new requirements are introduced. We have also developed a project tracking tool for scrum projects in which this model has been implemented. The model was applied to academic projects developed by graduate students; the results indicate that estimations are fairly reasonable.},
	number = {4},
	journal = {J. Comput. Sci. Coll.},
	author = {Feng, Xingzhan and Periyasamy, Kasi},
	month = nov,
	year = {2022},
	note = {Place: Evansville, IN, USA
Publisher: Consortium for Computing Sciences in Colleges},
	pages = {30--37},
}

@article{dang_uie-based_2025,
	title = {{UIE}-{Based} {Relational} {Extraction} {Task} for {Mine} {Hoist} {Fault} {Data}},
	volume = {24},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3705313},
	doi = {10.1145/3705313},
	abstract = {Information extraction is pivotal in natural language processing, where the goal is to convert unstructured text into structured information. A significant challenge in this domain is the diversity and specific needs of various processing tasks. Traditional approaches typically utilize separate frameworks for different information extraction tasks, such as named entity recognition and relationship extraction, which hampers their uniformity and scalability. In this study, this study introduce a Universal Information Extraction (UIE) framework combined with a cue learning strategy, significantly improving the efficiency and accuracy of extracting mine hoist fault data. Initially, domain-specific data is manually labeled to fine-tune the model, and the accuracy is further enhanced by constructing negative examples during this fine-tuning process. The model then focuses on faults using the Structured Extraction Language (SEL) and a schema-based prompt syntax, the Structural Schema Instructor (SSI), which targets and extracts key information from the fault data to meet specific domain requirements. Experimental results show that UIE substantially improves the processing efficiency and the F1 accuracy of the extracted mine hoist fault data, with the fine-tuned F1 score increasing from 23.59\% to 92.51\%.},
	number = {1},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Dang, Xiaochao and Ding, Guozhen and Dong, Xiaohui and Li, Fenfang and Gao, Shiwei and Wang, Yue},
	month = jan,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Joint extraction, prompt learning, mechanical problem, mining sector},
}

@inproceedings{shi_construction_2020,
	address = {New York, NY, USA},
	series = {{CSAE} '20},
	title = {Construction of {Neurosurgery} {Knowledge} {Graph} {Based} on {Bi}-{LSTM}-{CRF} {Model}},
	isbn = {978-1-4503-7772-0},
	url = {https://doi.org/10.1145/3424978.3425095},
	doi = {10.1145/3424978.3425095},
	abstract = {Medical knowledge is complex. The knowledge graph provides an efficient solution for integrating medical knowledge and analyzing medical data. In this paper, the neurosurgery knowledge graph is constructed. First, the ontology pattern library is constructed, and then named entity recognition is performed based on the Bi-LSTM-CRF model. The entities are extracted from the text and the entity relationships are defined. The knowledge graph of the symptom-disease-department is integrated, and finally stored in Neo4j Graph database. As the basis of information retrieval, intelligent question answering, diagnosis and treatment system, knowledge graph can effectively transfer medical knowledge and provide intelligent medical assistance for doctors and patients.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Computer} {Science} and {Application} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shi, Yu and Qin, Qiuli},
	year = {2020},
	note = {event-place: Sanya, China},
	keywords = {Knowledge graph, Bi-LSTM-CRF model, Neurosurgery},
}

@inproceedings{wilsdorf_creating_2023,
	address = {Singapore, Singapore},
	series = {{WSC} '22},
	title = {Creating {PROV}-{DM} {Graphs} from {Model} {Databases}},
	abstract = {Documenting the provenance of the main products of a simulation study plays a crucial role in improving the understanding of mechanistic, biological models as well as their reproducibility and credibility. With model databases already an ample collection of simulation models, including metainformation and source files, exists. In this paper, we bridge the gap between the information contained in model databases and the PROV-DM provenance standard, which allows making the diverse products and their relationships formally explicit. We present a procedure for creating PROV-DM graphs from model database entries, and illustrate the approach based on ten different models from the BioModels database. These case studies demonstrate the advantages of having a standardized provenance view in addition to the regular database entries, i.e., enhanced means for visualizing the structure of the simulation study and the curation process.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Wilsdorf, Pia and Uhrmacher, Adelinde M.},
	year = {2023},
	pages = {2118--2129},
}

@inproceedings{koleva_wiki-tabner_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {Wiki-{TabNER}: {Integrating} {Named} {Entity} {Recognition} into {Wikipedia} {Tables}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3730344},
	doi = {10.1145/3726302.3730344},
	abstract = {Interest in solving table interpretation tasks has grown over the years, yet it still relies on existing datasets that may be overly simplified. This is potentially reducing the effectiveness of the dataset for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To enrich the existing benchmark datasets, we extract and annotate a new, more challenging dataset. The proposed Wiki-TabNER dataset features complex tables containing several entities per cell, with named entities labeled using DBpedia classes. This dataset is specifically designed to address named entity recognition (NER) task within tables, but it can also be used as a more challenging dataset for evaluating the entity linking task. In this paper we describe the distinguishing features of the Wiki-TabNER dataset and the labeling process. In addition, we propose a prompting framework for evaluating the new large language models on the within tables NER task. Finally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed dataset.},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Koleva, Aneta and Ringsquandl, Martin and Hatem, Ahmed and Runkler, Thomas and Tresp, Volker},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {named entity recognition, table interpretation},
	pages = {3812--3820},
}

@article{chantas_probabilistic_2018,
	title = {A {Probabilistic}, {Ontological} {Framework} for {Safeguarding} the {Intangible} {Cultural} {Heritage}},
	volume = {11},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3131610},
	doi = {10.1145/3131610},
	abstract = {In this article, we propose Multi-Entity Bayesian Networks (MEBNs) as the probabilistic ontological framework for the analysis of the Tsamiko and Salsa dances. More specifically, our analysis has the objective of the dancer assessment with respect to both choreography execution accuracy and the synchronization of the dance movements with the musical rhythm. For this task, we make use of the explicit, expert-provided knowledge on dance movements and their relations to the musical beat. Due to the complexity of this knowledge, the MEBNs were used as the probabilistic ontological framework in which the knowledge is formalized. The reason we opt for MEBNs for this task is that they combine Bayesian and formal (first-order) logic into a single model. In this way, the Bayesian probabilistic part of MEBNs was used to capture, using example data and training, the implicit part of the expert knowledge about dances, i.e., this part of the knowledge that cannot be formalized and explicitly defined accurately enough, while the logical maintains the explicit knowledge representation in the same way ontologies do. Moreover, we present in detail the MEBN models we built for Tsamiko and Salsa, using expert-provided explicit knowledge. Last, we conduct experiments that demonstrate the effectiveness of the proposed MEBN-based methodology we employ to achieve our analysis objectives. The results of the experiments demonstrate the superiority of MEBNs to conventional models, such as BNs, in terms of the dancer assessment accuracy.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Chantas, Giannis and Karavarsamis, Sotiris and Nikolopoulos, Spiros and Kompatsiaris, Ioannis},
	month = aug,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Intangible cultural heritage, multi-entity bayesian networks, multimodal semantic analysis},
}

@inproceedings{kaffee_ranking_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {Ranking {Knowledge} {Graphs} {By} {Capturing} {Knowledge} about {Languages} and {Labels}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364443},
	doi = {10.1145/3360901.3364443},
	abstract = {Capturing knowledge about the mulitilinguality of a knowledge graph is of supreme importance to understand its applicability across multiple languages. Several metrics have been proposed for describing mulitilinguality at the level of a whole knowledge graph. Albeit enabling the understanding of the ecosystem of knowledge graphs in terms of the utilized languages, they are unable to capture a fine-grained description of the languages in which the different entities and properties of the knowledge graph are represented. This lack of representation prevents the comparison of existing knowledge graphs in order to decide which are the most appropriate for a multilingual application.In this work, we approach the problem of ranking knowledge graphs based on their language features and propose LINGVO, a framework able to capture mulitilinguality at different levels of granularity. Grounded in knowledge graph descriptions, LINGVO is, additionally, able to solve the problem of ranking knowledge graphs according to a degree of mulitilinguality of the represented entities. We have empirically studied the effectiveness of LINGVO in a benchmark of queries to be executed against existing knowledge graphs. The observed results provide evidence that LINGVO captures the mulitilinguality of the studied knowledge graphs similarly than a crowd-sourced gold standard.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Kaffee, Lucie-Aimée and Endris, Kemele M. and Simperl, Elena and Vidal, Maria-Esther},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {knowledge graph, question answering, multilinguality, ranking},
	pages = {21--28},
}

@inproceedings{flotynski_semantic_2018,
	address = {New York, NY, USA},
	series = {{Web3D} '18},
	title = {Semantic 4-dimensional modeling of {VR} content in a heterogeneous collaborative environment},
	isbn = {978-1-4503-5800-2},
	url = {https://doi.org/10.1145/3208806.3208830},
	doi = {10.1145/3208806.3208830},
	abstract = {Interactive 3D content gains increasing use in VR/AR applications in different domains, such as education, training, engineering, spatial and urban planning as well as architectural and interior design. While modeling and presenting interactive 3D scenes in collaborative VR/AR environments, different 3D objects are added, modified and removed by different users, which leads to the evolution of the scenes over time. Representation of VR content covering temporal knowledge is essential to enable exploration of such time-dependent VR/AR content. However, the available approaches do not enable exploration of VR content with regards to its time-dependent components and properties, which limits their usage in web-based systems. The main contribution of this paper is 4–dimensional representation of VR content, which encompasses time being the fourth dimension. The representation is based on the semantic web standards and ontologies, which enable the use of domain knowledge for collaborative creation and exploration of content. This could improve the availability of VR/AR applications to domain specialists without expertise in 3D graphics and animation, thus improving the overall dissemination of VR/AR on the web. The representation has been implemented in a heterogeneous collaborative VR environment for urban design.},
	booktitle = {Proceedings of the 23rd {International} {ACM} {Conference} on {3D} {Web} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Flotyński, Jakub and Sobociński, Paweł},
	year = {2018},
	note = {event-place: Poznań, Poland},
	keywords = {ontologies, semantic web, 3D content, collaborative design, web 3D/VR/AR},
}

@inproceedings{wang_medication_2022,
	address = {New York, NY, USA},
	series = {{WI}-{IAT} '21},
	title = {Medication {Recommendation} {Based} on a {Knowledge}-enhanced {Pre}-training {Model}},
	isbn = {978-1-4503-9187-0},
	url = {https://doi.org/10.1145/3498851.3498968},
	doi = {10.1145/3498851.3498968},
	abstract = {More and more attention has been paid to electronic medical record (EMR)-based auxiliary diagnosis and treatment, in which medication recommendation is an important research direction. The existing medication recommendation models mainly depend on the data of patients, diagnosis and medications. However, the insufficient amount of clinical data with temporal dependencies becomes a major obstacle. This paper proposes a new knowledge-enhanced pre-training model for medication recommendation. On the one hand, the classification knowledge in diagnostic codes and drug codes is encoded by Graph Attention Network and fused into the clinical data for expanding the data content. On the other hand, a large number of single visit data of EMR are used to create the pre-trained visit model by a modified BERT for expanding the data scale. The experimental results on EMR data from more than 2,000 medical and health institutions in Hainan, China show that the fusion of classification knowledge and pre-training model can effectively improve the accuracy of medication recommendation.},
	booktitle = {{IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Mengzhen and Chen, Jianhui and Lin, Shaofu},
	year = {2022},
	note = {event-place: Melbourne, VIC, Australia},
	keywords = {Electronic medical record, Graph Attention Network, Medication recommendation, Pre-training model},
	pages = {290--294},
}

@inproceedings{gowhar_imbalanced_2025,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '24},
	title = {Imbalanced {Multi}-{Class} {Research} {Article} {Classification} using {Sentence} {Transformers} and {Machine} {Learning} {Algorithms}},
	isbn = {979-8-4007-1124-4},
	url = {https://doi.org/10.1145/3703323.3703698},
	doi = {10.1145/3703323.3703698},
	abstract = {Categorizing scientific articles into specific research fields is a challenging problem, considering the volume and variety of published literature. However, existing classification systems often suffer from limitations regarding taxonomy or the models used for classification. This article explores approaches built on Sentence Transformer embeddings combined with Machine Learning algorithms to classify articles into 123 predefined classes, with the dataset being heavily imbalanced in nature. The effectiveness of Large Language Models (LLMs) for generating synthetic data is also experimented with, along with synonym augmentation and SMOTE. The best-performing model, the One vs Rest classifier trained on MP-Net sentence embeddings with SMOTE, achieved an accuracy of 77\%, and outperformed all the other models.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Data} {Science} and {Management} of {Data} (12th {ACM} {IKDD} {CODS} and 30th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Gowhar, Saliq and Kempaiah, Praveen and Kamath S, Sowmya and Sugumaran, Vijayan},
	year = {2025},
	keywords = {Natural Language Processing, Machine Learning, Document classification, Sentence Transformers},
	pages = {309--310},
}

@inproceedings{shi_ontologies_2018,
	address = {New York, NY, USA},
	series = {{WIMS} '18},
	title = {Ontologies for the {Real} {Property} {Domain}},
	isbn = {978-1-4503-5489-9},
	url = {https://doi.org/10.1145/3227609.3227661},
	doi = {10.1145/3227609.3227661},
	abstract = {Real property, also known as real estate, realty or immovable property, is one of the most important assets for the world economy. Real property data is valuable input for decision makers in various domains. Real property data has temporal and spatial characteristics and is distributed across multiple systems. Integration of real property data from legal and business systems (possibly from different countries) with contextual data in related domains is a challenging task that requires cross-domain knowledge. Real property ontologies, capturing relevant domain knowledge in a structured way, are essential in the process of integrating real property data. This paper identifies key aspects of the real property domain from a data integration perspective, and surveys ontologies for real property and its related domains. It analyzes geospatial standards for representing geospatial concepts and attributes relevant to real properties, the Land Administration Domain Model and its implementations, and ontologies for real property transactions and for real property data integration. This survey aims to collect and compare existing real property ontologies and conceptual models, serving as a reference point for ontologies and conceptual models in the real property domain.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Web} {Intelligence}, {Mining} and {Semantics}},
	publisher = {Association for Computing Machinery},
	author = {Shi, Ling and Roman, Dumitru},
	year = {2018},
	note = {event-place: Novi Sad, Serbia},
	keywords = {geospatial data, cadaster, LADM, Real property ontology},
}

@article{alshammari_combining_2022,
	title = {Combining a {Novel} {Scoring} {Approach} with {Arabic} {Stemming} {Techniques} for {Arabic} {Chatbots} {Conversation} {Engine}},
	volume = {21},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3511215},
	doi = {10.1145/3511215},
	abstract = {Arabic is recognized as one of the main languages around the world. Many attempts and efforts have been done to provide computing solutions to support the language. Developing Arabic chatbots is still an evolving research field and requires extra efforts due to the nature of the language. One of the common tasks of any natural language processing application is the stemming step. It is important for developing chatbots, since it helps with pre-processing the input data and it can be involved with different phases of the chatbot development process. The aim of this article is to combine a scoring approach with Arabic stemming techniques for developing an Arabic chatbot conversation engine. Two experiments are conducted to evaluate the proposed solution. The first experiment is to select which stemmer is more accurate when applying our solution, since our algorithm can support various stemmers. The second experiment was conducted to evaluate our proposed approach against various machine learning models. The results show that the ISRIS stemming algorithm is the best fit for our solution with accuracy 78.06\%. The results also indicate that our novel solution achieved an F1 score of 65.5\%, while the other machine learning models achieved slightly lower scores. Our study presents a novel technique by combining scoring mechanisms with stemming processes to produce the best answer for every query sent by chatbots users compared to other approaches. This can be helpful for developing Arabic chatbot and can support many domains such as education, business, and health. This technique is among the first techniques that developed purposefully to serve the development of Arabic chatbots conversation engine.},
	number = {4},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Alshammari, Nasser O. and Alharbi, Fawaz D.},
	month = jan,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural language processing, machine learning, chatbot, Arabic language, stemming},
}

@inproceedings{laadidi_simplification_2018,
	address = {New York, NY, USA},
	series = {{ICSIM} '18},
	title = {Simplification of {OWL} {Ontology} {Sources} for {Data} {Warehousing}},
	isbn = {978-1-4503-5438-7},
	url = {https://doi.org/10.1145/3178461.3178483},
	doi = {10.1145/3178461.3178483},
	abstract = {Nowadays, with the emergence of new web technologies, no one could deny the necessity of including such external data sources in the analysis process in order to provide the necessary knowledge for companies to improve their services and increase their profits. However, processing data in an open environment such as the web has become too difficult due to the diversity of distributed data sources and incapability of machines to 'understand' the real semantic of web resources. The Semantic Web (SW) provides the semantic annotations to describe and link scattered information over the web and facilitate inference mechanisms using ontologies. Web Ontology Language (OWL) is the W3C recommendation. A Data warehouse (DW) is used in decision making processes to store multidimensional (MD) information from heterogeneous data sources using ETL (Extract, Transform and Load) techniques. In this paper, we introduce firstly a simplification method of OWL inputs and then we define the related MD schema. Transformation rules are applied for defining multidimensional concepts over the OWL graph.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Software} {Engineering} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Laadidi, Yassine and Bahaj, Mohamed},
	year = {2018},
	note = {event-place: Casablanca, Morocco},
	keywords = {OWL ontology, semantic web, data warehousing},
	pages = {77--81},
}

@inproceedings{kalogirou_linked_2020,
	address = {New York, NY, USA},
	series = {{ICEGOV} '20},
	title = {Linked government data hub, an ontology agnostic data harvester and {API}},
	isbn = {978-1-4503-7674-7},
	url = {https://doi.org/10.1145/3428502.3428619},
	doi = {10.1145/3428502.3428619},
	abstract = {Openness and Transparency are important principles for citizens and therefore for eGovernment. Most government portals are designed in a way that finding relevant and up-to-date information requires time and effort. Data and information are scattered in different platforms in a non-collaborative way, leading to a labyrinth of redirections. Sharing of data and information is hindered by the lack of interoperability between the multiple vocabularies and formats (mostly proprietary). This paper attempts to bridge those systems by proposing at a national level a data hub model that aggregates and categorizes data from various services. Linked data and standardized vocabularies are used to gather information from multiple public sectors and services. The case study reuses some basic components of Joinup collaborative platform to provide aggregated up-to-date information on the portal of the Greek Public Administration Di@vgeia. The model has the potential of a wider application in different business domains and multiple Government levels.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Theory} and {Practice} of {Electronic} {Governance}},
	publisher = {Association for Computing Machinery},
	author = {Kalogirou, Victoria and van Dooren, Sander and Dimopoulos, Ilias and Charalabidis, Yannis and De-Baets, Jean-Paul and Lobo, Georges},
	year = {2020},
	note = {event-place: Athens, Greece},
	keywords = {Application Program Interface (API), Data Hub, eGovernment (eGov), Interoperability (IoP), Linked Data (LD), public services},
	pages = {779--782},
}

@inproceedings{wibowo_requirements_2020,
	address = {New York, NY, USA},
	series = {{ACSW} '20},
	title = {Requirements {Traceability} {Ontology} to {Support} {Requirements} {Management}},
	isbn = {978-1-4503-7697-6},
	url = {https://doi.org/10.1145/3373017.3373038},
	doi = {10.1145/3373017.3373038},
	booktitle = {Proceedings of the {Australasian} {Computer} {Science} {Week} {Multiconference}},
	publisher = {Association for Computing Machinery},
	author = {Wibowo, Adi and Davis, Joseph},
	year = {2020},
	note = {event-place: Melbourne, VIC, Australia},
	keywords = {Requirement traceability, requirements management, traceability ontology},
}

@inproceedings{li_temporal_2021,
	address = {New York, NY, USA},
	series = {{DSIT} 2021},
	title = {A {Temporal} {RDF} {Model} for {Multi}-grained {Time} {Information} {Modeling}},
	isbn = {978-1-4503-9024-8},
	url = {https://doi.org/10.1145/3478905.3478908},
	doi = {10.1145/3478905.3478908},
	abstract = {With the rapid increase of temporal data, how to represent and manage temporal data has become a research issue worth digging in. To better represent temporal data, there have been many works on adding the dimension to RDF or other data representations such as relational databases. However, few works pay attention to the problem of updating time information in the form of triple elements in RDF. Note that this not only makes it easy to express that the relationship between entities is effective over a period of time, but also makes it easy to express that the entities themselves are effective in the time. A model of temporal data representation based on RDF is proposed in this paper which not only considering the validity of triples, but also considering the temporal validity of the entities themselves within the triples.},
	booktitle = {2021 4th {International} {Conference} on {Data} {Science} and {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Li, Haixia and Yan, Li},
	year = {2021},
	note = {event-place: Shanghai, China},
	keywords = {modeling, RDF, Temporal data},
	pages = {9--14},
}

@article{sangsavate_experiments_2023,
	title = {Experiments of {Supervised} {Learning} and {Semi}-{Supervised} {Learning} in {Thai} {Financial} {News} {Sentiment}: {A} {Comparative} {Study}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3603499},
	doi = {10.1145/3603499},
	abstract = {Sentiment classification is an instrument of natural language processing tasks in text analysis to measure customer feedback from given documents such as product reviews, news, and texts. This research aims to experiment with Thai financial news sentiment classification and evaluate sentiment classification performance. In this research, we show financial news sentiment classification experimental results when comparing supervised and semi-supervised methods. In the research methodology, we use PyThaiNLP to tokenize and remove stopwords and split datasets into 85\% of the training set and 15\% of the testing set. Next, we classify sentiment using machine learning and deep learning approaches with feature extraction such as bag-of-words, term frequency–inverse document frequency, and word embedding (Word2Vec and Bidirectional Encoder Representations from Transformers (BERT)) in given texts. The results show that support vector machine with the BERT model yields the best performance at 83.38\%; in contrast, the random forest classifier with bag-of-words yields the worst performance at 54.10\% in the machine learning approach. Another experiment reveals that long short-term memory with the BERT model yields the best performance at 84.07\% in contrast to the convolutional neural network with bag-of-words, which yields the worst performance at 69.80\% in the deep learning approach. The results imply that support vector machine, convolutional neural network, and long short-term memory are suitable for classifying sentiment in complex structure language. From this study, we observe the importance of sentiment classification tools between supervised and semi-supervised learning, and we look forward to furthering this work.},
	number = {7},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Sangsavate, Suntarin and Sinthupinyo, Sukree and Chandrachai, Achara},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural language processing, semi-supervised learning, sentiment classification, supervised learning, Thai language},
}

@inproceedings{liu_information_2021,
	address = {New York, NY, USA},
	series = {{ICAIIS} 2021},
	title = {Information system development method for domain ontology reuse},
	isbn = {978-1-4503-9020-0},
	url = {https://doi.org/10.1145/3469213.3470316},
	doi = {10.1145/3469213.3470316},
	booktitle = {2021 2nd {International} {Conference} on {Artificial} {Intelligence} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Bin and Wu, Baojun and Huang, Xinxin},
	year = {2021},
	note = {event-place: Chongqing, China},
}

@article{goncales_comparison_2019,
	title = {Comparison of {Software} {Design} {Models}: {An} {Extended} {Systematic} {Mapping} {Study}},
	volume = {52},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3313801},
	doi = {10.1145/3313801},
	abstract = {Model comparison has been widely used to support many tasks in model-driven software development. For this reason, many techniques of comparing them have been proposed in the last few decades. However, academia and industry have overlooked a classification of currently available approaches to the comparison of design models. Hence, a thorough understanding of state-of-the-art techniques remains limited and inconclusive. This article, therefore, focuses on providing a classification and a thematic analysis of studies on the comparison of software design models. We carried out a systematic mapping study following well-established guidelines to answer nine research questions. In total, 56 primary studies (out of 4,132) were selected from 10 widely recognized electronic databases after a careful filtering process. The main results are that a majority of the primary studies (1) provide coarse-grained techniques of the comparison of general-purpose diagrams, (2) adopt graphs as principal data structure and compare software design models considering structural properties only, (3) pinpoint commonalities and differences between software design models rather than assess their similarity, and (4) propose new techniques while neglecting the production of empirical knowledge from experimental studies. Finally, this article highlights some challenges and directions that can be explored in upcoming studies.},
	number = {3},
	journal = {ACM Comput. Surv.},
	author = {Gonçales, Lucian José and Farias, Kleinner and Oliveira, Toacy Cavalcante De and Scholl, Murilo},
	month = jul,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {UML, model comparison, model similarity, software design models},
}

@inproceedings{putze_model-based_2021,
	address = {New York, NY, USA},
	series = {{ICMI} '20 {Companion}},
	title = {Model-based {Prediction} of {Exogeneous} and {Endogeneous} {Attention} {Shifts} {During} an {Everyday} {Activity}},
	isbn = {978-1-4503-8002-7},
	url = {https://doi.org/10.1145/3395035.3425206},
	doi = {10.1145/3395035.3425206},
	abstract = {Human attention determines to a large degree how users interact with technical devices and how technical artifacts can support them optimally during their tasks. Attention shifts between different targets, triggered through changing requirements of an ongoing task or through salient distractions in the environment. Such shifts mark important transition points which an intelligent system needs to predict and attribute to an endogenous or exogenous cause for an appropriate reaction. In this paper, we describe a model which performs this task through a combination of bottom-up and topdown modeling components. We evaluate the model in a scenario with a dynamic task in a rich environment and show that the model is able to predict attention future switches with a robust classification performance.},
	booktitle = {Companion {Publication} of the 2020 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Putze, Felix and Burri, Merlin and Vortmann, Lisa-Marie and Schultz, Tanja},
	year = {2021},
	note = {event-place: Virtual Event, Netherlands},
	keywords = {attention shifts, exogenous and endogenous attention, top-down and bottom-up modeling},
	pages = {417--425},
}

@inproceedings{wu_automatic_2024,
	address = {New York, NY, USA},
	series = {{ICITEE} '23},
	title = {Automatic {Generation} of {GIM} {Data} {Audit} {Rules} {Based} on {Sentence} {Embedding} {Vectors}},
	isbn = {979-8-4007-0829-9},
	url = {https://doi.org/10.1145/3640115.3640224},
	doi = {10.1145/3640115.3640224},
	abstract = {The digital design results of power substations establish the foundation for their running and maintaining. Currently, the digital design of substations is delivered in the format of Grid Information Model (GIM), an alternative Building Information Modeling (BIM) format for describing power grid infrastructure in China. Since the correctness, compliance, and consistency of GIM data are necessary conditions for information sharing and business decision support, the GIM data must be audited before sharing among the stakeholders. The traditional manual review of GIM data is too inefficient and costly to execute, and thus the power grid industry seeks automatic review approaches. However, one challenge for automated auditing of GIM data is the lack of auditing rules. In order to establish such a rule base for GIM data auditing, this study first categorizes the audit rules, and proposes an XML encoding method for the audit rules. Meanwhile, the methodology of converting the rules described in natural language into XML is also rules proposed using the SBERT model. The application of the developed tool is demonstrated and verified through case studies.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Information} {Technologies} and {Electrical} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Bing and Song, Yuanbin and Cao, Jinhao},
	year = {2024},
	note = {event-place: Changde, Hunan, China},
	keywords = {Natural language processing, Controlled natural language, Rule representation, Grid information model, Model audit, Sentence BERT},
	pages = {668--673},
}

@inproceedings{holubova_multi-model_2021,
	address = {New York, NY, USA},
	series = {{IDEAS} '21},
	title = {Multi-{Model} {Data} {Modeling} and {Representation}: {State} of the {Art} and {Research} {Challenges}},
	isbn = {978-1-4503-8991-4},
	url = {https://doi.org/10.1145/3472163.3472267},
	doi = {10.1145/3472163.3472267},
	abstract = {Following the current trend, most of the well-known database systems, being relational, NoSQL, or NewSQL, denote themselves as multi-model. This industry-driven approach, however, lacks plenty of important features of the traditional DBMSs. The primary problem is a design of an optimal multi-model schema and its sufficiently general and efficient representation. In this paper, we provide an overview and discussion of the promising approaches that could potentially be capable of solving these issues, along with a summary of the remaining open problems.},
	booktitle = {Proceedings of the 25th {International} {Database} {Engineering} \&amp; {Applications} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Holubova, Irena and Contos, Pavel and Svoboda, Martin},
	year = {2021},
	note = {event-place: Montreal, QC, Canada},
	keywords = {Conceptual modeling, Category theory, Inter-model relationships, Logical models, Multi-model data},
	pages = {242--251},
}

@inproceedings{ben_ellefi_cultural_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {Cultural {Heritage} {Resources} {Profiling}: {Ontology}-based {Approach}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3191598},
	doi = {10.1145/3184558.3191598},
	abstract = {Cultural heritage (CH) resources are very heterogeneous since the information was collected from vast diversity of cultural sites and digitally recorded in different formats. With the progress of 3D technologies, photogrammetry techniques become the adopted solution for representing CH artifacts by turning photos from small finds, to entire landscapes, into accurate 3D models. To meet knowledge representation with cultural heritage photogrammetry, this paper proposes an ontology-profiling method for modeling a real case of archaeological amphorae. The ontological profile consists of all needed information to represent a CH resource including typology attributes, geo-spatial information and photogrammetry process. An example illustrating the applicability of this profiling method to the problem of CH resources conceptualization is presented. We also outline our perspectives for using ontologies in data-driven science, in particular on modeling a complete pipeline that manages both the photogrammetric process and the archaeological knowledge.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Ben Ellefi, Mohamed and Papini, Odile and Merad, Djamal and Boi, Jean-Marc and Royer, Jean-Philip and Pasquet, Jérôme and Sourisseau, Jean-Christophe and Castro, Filipe and Nawaf, Mohammad Motasem and Drap, Pierre},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {ontology, cultural heritage, archaeology, photogrammetry, profiles},
	pages = {1489--1496},
}

@inproceedings{tsitseklis_recbot_2023,
	address = {New York, NY, USA},
	series = {{UMAP} '23 {Adjunct}},
	title = {{RECBOT}: {Virtual} {Museum} navigation through a {Chatbot} assistant and personalized {Recommendations}},
	isbn = {978-1-4503-9891-6},
	url = {https://doi.org/10.1145/3563359.3596661},
	doi = {10.1145/3563359.3596661},
	abstract = {The trend for digitalization of museums has been on the rise in recent years, as museums seek to make their collections and exhibitions more accessible to a wider audience. This has involved the use of technologies such as virtual and augmented reality, online exhibits, and digital archives. These digital initiatives have allowed museums to reach new audiences and provide immersive experiences that enhance visitors’ engagement with the exhibits. Following this trend, in the current work, we propose a conversational agent that assists remote visitors in accessing a museum’s collection. The proposed architecture includes a chatbot for user interaction that employs Natural Language Processing techniques for understanding the user’s input. To increase visitor engagement, a hybrid recommender system is developed that combines content-based and collaborative-filtering components. The available data is modeled in the form of a Knowledge Graph, which allows for useful insights to be extracted from it.},
	booktitle = {Adjunct {Proceedings} of the 31st {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Tsitseklis, Konstantinos and Stavropoulou, Georgia and Zafeiropoulos, Anastasios and Thanou, Athina and Papavassiliou, Symeon},
	year = {2023},
	note = {event-place: Limassol, Cyprus},
	keywords = {Natural Language Processing, chatbot, conversational agent, recommender system, online museum, virtual tour},
	pages = {388--396},
}

@inproceedings{agarwal_comparative_2021,
	address = {New York, NY, USA},
	series = {{IC3}-2021},
	title = {Comparative {Study} of {Topic} {Modeling} and {Word} {Embedding} {Approaches} for {Web} {Service} {Clustering}},
	isbn = {978-1-4503-8920-4},
	url = {https://doi.org/10.1145/3474124.3474169},
	doi = {10.1145/3474124.3474169},
	abstract = {Vector space representation of web services plays a prominent role in enhancing the performance of different web service-based processes like clustering, recommendation, ranking, discovery, etc. Generally, Term Frequency - Inverse Document Frequency (TF-IDF) and topic modeling methods are widely used for service representation. In recent years, word embedding techniques have attracted researchers a lot because they can map services or documents based on semantic similarity. This paper provides a comparative analysis of two topic modeling techniques, i.e., Latent Dirichlet Allocation (LDA) and Gibbs Sampling algorithm for Dirichlet Multinomial Mixture (GSDMM) \&amp; two word embedding techniques, i.e., word2vec and fastText. These topic modeling and word embedding techniques are applied to a dataset of web service documents for vector space representation. K-Means clustering is used to analyze the performance, and results are evaluated based on standard evaluation criteria. Results demonstrate that word2vec model outperforms other techniques and provides a satisfactory improvement on clustering.},
	booktitle = {Proceedings of the 2021 {Thirteenth} {International} {Conference} on {Contemporary} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Agarwal, Neha and Sikka, Geeta and Awasthi, Lalit Kumar},
	year = {2021},
	note = {event-place: Noida, India},
	keywords = {Word Embedding, K-Means Clustering, Topic Models, Web Services},
	pages = {309--313},
}

@inproceedings{li_efficient_2018,
	address = {New York, NY, USA},
	series = {{ICCIP} '18},
	title = {Efficient and density adaptive edge weight model for measuring semantic similarity},
	isbn = {978-1-4503-6534-5},
	url = {https://doi.org/10.1145/3290420.3290459},
	doi = {10.1145/3290420.3290459},
	abstract = {The measurement of semantic similarity between concepts is an important research topic in natural language processing. However, previous efforts suffered from the mismatch of the accuracy and efficiency. In this paper, we propose an edge weight model for improving the accuracy of edge-based measures that have an inherent high efficiency. It combines the edge counting model with the information theory and deduces a function of edge weight based on the number of direct hyponyms of the subsumer in the edge. This model doesn't require any additional parameter and can adapt the effect of different densities to edges. Extensive experiments on four test datasets for WordNet and SNOMED-CT demonstrate that the proposed edge weight model can significantly improve the accuracy of various edge-based similarity measures and has a wide coverage over different ontologies. Compared with IC-based measures, our model has a remarkable advantage in efficiency and is comparable to it in accuracy.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Communication} and {Information} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Li, Fei and Liao, Lejian and Li, Chunyi and He, Sixing},
	year = {2018},
	note = {event-place: Qingdao, China},
	keywords = {semantic similarity, SNOMED-CT, information theory, edge-weight, wordnet},
	pages = {127--134},
}

@inproceedings{ranade_fabula_2024,
	address = {New York, NY, USA},
	series = {{ASONAM} '23},
	title = {{FABULA}: {Intelligence} {Report} {Generation} {Using} {Retrieval}-{Augmented} {Narrative} {Construction}},
	isbn = {979-8-4007-0409-3},
	url = {https://doi.org/10.1145/3625007.3627505},
	doi = {10.1145/3625007.3627505},
	abstract = {Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports.We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.},
	booktitle = {Proceedings of the 2023 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ranade, Priyanka and Joshi, Anupam},
	year = {2024},
	note = {event-place: Kusadasi, Turkiye},
	keywords = {knowledge graphs, large language models, retrieval augmented generation, narratives},
	pages = {603--610},
}

@inproceedings{aka_measuring_2021,
	address = {New York, NY, USA},
	series = {{AIES} '21},
	title = {Measuring {Model} {Biases} in the {Absence} of {Ground} {Truth}},
	isbn = {978-1-4503-8473-5},
	url = {https://doi.org/10.1145/3461702.3462557},
	doi = {10.1145/3461702.3462557},
	abstract = {The measurement of bias in machine learning often focuses on model performance across identity subgroups (such as man and woman) with respect to groundtruth labels. However, these methods do not directly measure the associations that a model may have learned, for example between labels and identity subgroups. Further, measuring a model's bias requires a fully annotated evaluation dataset which may not be easily available in practice.We present an elegant mathematical solution that tackles both issues simultaneously, using image classification as a working example. By treating a classification model's predictions for a given image as a set of labels analogous to a "bag of words", we rank the biases that a model has learned with respect to different identity labels. We use man, woman as a concrete example of an identity label set (although this set need not be binary), and present rankings for the labels that are most biased towards one identity or the other. We demonstrate how the statistical properties of different association metrics can lead to different rankings of the most "gender biased" labels, and conclude that normalized pointwise mutual information (nPMI) is most useful in practice. Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Aka, Osman and Burke, Ken and Bauerle, Alex and Greer, Christina and Mitchell, Margaret},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {information extraction, bias, fairness, datasets, model analysis, image tagging, stereotypes},
	pages = {327--335},
}

@inproceedings{barboza_automatic_2018,
	address = {New York, NY, USA},
	series = {{SBSI} '18},
	title = {Automatic {Validation} of {Knowledge}-intensive {Process} {Models} through {Alloy}},
	isbn = {978-1-4503-6559-8},
	url = {https://doi.org/10.1145/3229345.3229405},
	doi = {10.1145/3229345.3229405},
	abstract = {Knowledge-intensive Processes (KiP) are poorly structured, dynamic and highly complex. The Knowledge Intensive Process Ontology (KiPO) constitutes a semantically rich conceptualization (encompassing a set of logical rules) about the domain of KiP that may serve as a basis to understand, identify and manage KiP effectively. However, applying KiPO in real scenarios requires its instantiation, validation and simulation in an application level, which are complex tasks for users that typically are not experts in non-trivial issues on conceptual modeling. This work proposes a rule-based strategy to validate or simulate KiP models. The proposed strategy transforms the KiPO rules into the existing specifications in the Alloy logic-based language, using the Alloy Analyzer model analyzer. The main contribution of this research is to show the applicability of the Alloy tool to this context in a case study with four different scenarios. A process modeler can directly benefit from these results.},
	booktitle = {Proceedings of the {XIV} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Barboza, Tatiana and Santoro, Flávia Maria and Baião, Fernanda},
	year = {2018},
	note = {event-place: Caxias do Sul, Brazil},
	keywords = {Conceptual Modeling, Model validation, BPM, Knowledge-intensive Process},
}

@article{bi_bi-directional_2021,
	title = {Bi-directional {Long} {Short}-{Term} {Memory} {Model} with {Semantic} {Positional} {Attention} for the {Question} {Answering} {System}},
	volume = {20},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3439800},
	doi = {10.1145/3439800},
	abstract = {The intelligent question answering system aims to provide quick and concise feedback on the questions of users. Although the performance of phrase-level and numerous attention models have been improved, the sentence components and position information are not emphasized enough. This article combines Ci-Lin and word2vec to divide all of the words in the question-answer pairs into groups according to the semantics and select one kernel word in each group. The remaining words are common words and realize the semantic mapping mechanism between kernel words and common words. With this Chinese semantic mapping mechanism, the common words in all questions and answers are replaced by the semantic kernel words to realize the normalization of the semantic representation. Meanwhile, based on the bi-directional LSTM model, this article introduces a method of the combination of semantic role labeling and positional context, dividing the sentence into multiple semantic segments according to semantic logic. The weight is given to the neighboring words in the same semantic segment and propose semantic role labeling position attention based on the bi-directional LSTM model (BLSTM-SRLP). The good performance of the BLSTM-SRLP model has been demonstrated in comparative experiments on the food safety field dataset (FS-QA).},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Bi, Mingwen and Zhang, Qingchuan and Zuo, Min and Xu, Zelong and Jin, Qingyu},
	month = jun,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Question answering, BLSTM model, Chinese semantic mapping mechanism, semantic positional-based attention},
}

@article{williams_metacogs_2022,
	title = {{MetaCogs}: {Mitigating} {Executive} {Dysfunction} via {Agent}-based {Modeling} for {Metacognitive} {Strategy} {Development}},
	volume = {15},
	issn = {1936-7228},
	url = {https://doi.org/10.1145/3514254},
	doi = {10.1145/3514254},
	abstract = {Executive functions (EF) are a collection of cognitive domains governing task initiation, motor planning, attention, and goal-oriented action. Difficulties with EF have marked impacts on adaptive living skills, learning outcomes, and quality of life for people with cognitive and psychosocial disabilities, as well as the broader population. While there is considerable research interest in EF training intervention for disabled populations, very few studies explore metacognitive intervention for people with cognitive disabilities. Metacognition comprises conscious beliefs and strategies around task management and goal setting. Metacognitive awareness has been shown to mediate the effects of executive function on self-regulated learning. Metacognitive interventions have also shown promise in general education, military training, and medical practice. We present a virtual reality experience deploying agent-based modeling to support explicit metacognitive strategy instruction for undergraduate students of all neurotypes. Our results support that explicit instructional material explaining executive function and metacognition in relation to problem-solving experiences influenced participant self-concept and awareness of personal traits and cognitive processes.},
	number = {3},
	journal = {ACM Trans. Access. Comput.},
	author = {Williams, Rua M. and Alikhademi, Kiana and Munyaka, Imani N. S. and Gilbert, Juan E.},
	month = jul,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {autism, virtual reality, ADHD, executive function, Metacognition},
}

@inproceedings{ortiz_semantic_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {A semantic model to fight social exclusion},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00110},
	doi = {10.1109/MODELS-C.2019.00110},
	abstract = {This work presents a semantic model meant to help with the identification and prediction of individuals at risk of social exclusion. The model is based on the self-sufficiency matrix, a tool that evaluates a person's self-sufficiency in different areas, and that is used by Barcelona's City Council. Existing data sources can then be mapped to this model, in order to analyze, query, and visualize the data.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Ortiz, Victor-Alejandro and Estañol, Montserrat and Marinescu, Maria-Cristina and Sancho, Maria-Ribera and Teniente, Ernest and Rueda, Carmen},
	year = {2021},
	keywords = {modeling, semantic technologies, self-suffiency matrix, social exclusion},
	pages = {730--731},
}

@inproceedings{bozic_validation_2018,
	address = {New York, NY, USA},
	series = {{iiWAS2018}},
	title = {Validation of {Tagging} {Suggestion} {Models} for a {Hotel} {Ticketing} {Corpus}},
	isbn = {978-1-4503-6479-9},
	url = {https://doi.org/10.1145/3282373.3282386},
	doi = {10.1145/3282373.3282386},
	abstract = {This paper investigates methods for the prediction of tags on a textual corpus that describes hotel staff inputs in a ticketing system. The aim is to improve the tagging process and find the most suitable method for suggesting tags for a new text entry. The paper consists of two parts: (i) exploration of existing sample data, which includes statistical analysis and visualisation of the data to provide an overview, and (ii) evaluation of tag prediction approaches. We have included different approaches from different research fields in order to cover a broad spectrum of possible solutions. As a result, we have tested a machine learning model for multi-label classification (using gradient boosting), a statistical approach (using frequency heuristics), and two simple similarity-based classification approaches (Nearest Centroid and k-Nearest Neighbours). The experiment which compares the approaches uses recall to measure the quality of results. Finally, we provide a recommendation of the modelling approach which produces the best accuracy in terms of tag prediction on the sample data.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Božić, Bojan and Ríos, André and Delany, Sarah Jane},
	year = {2018},
	note = {event-place: Yogyakarta, Indonesia},
	keywords = {Multi-label Classification, Natural Language Processing, k-Nearest Neighbour, Tag Prediction},
	pages = {15--23},
}

@inproceedings{schoberl_certgraph_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {{CertGraph}: {Towards} a {Comprehensive} {Knowledge} {Graph} for {Cloud} {Security} {Certifications}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3687795},
	doi = {10.1145/3652620.3687795},
	abstract = {This paper introduces CertGraph, a knowledge graph-based approach designed to streamline security certification which integrates evidence from multiple sources. Unlike existing approaches, we consider the complete stack from software to policies, and enable the fusion of evidence from different views and sources. Its extensible ontology is designed to accommodate multiple domains, including cloud security, AI models, and source code. By providing an automated and systematic approach to build an ontology, CertGraph aims to facilitate more effective security certification and compliance verification.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Schöberl, Stefan and Banse, Christian and Geist, Verena and Kunz, Immanuel and Pinzger, Martin},
	year = {2024},
	note = {event-place: Linz, Austria},
	pages = {76--77},
}

@inproceedings{zhang_multi-modal_2023,
	address = {New York, NY, USA},
	series = {{ICCIP} '22},
	title = {Multi-modal {Variational} {Auto}-{Encoder} {Model} for {Micro}-video {Popularity} {Prediction}},
	isbn = {978-1-4503-9710-0},
	url = {https://doi.org/10.1145/3571662.3571664},
	doi = {10.1145/3571662.3571664},
	abstract = {Popularity prediction of micro videos on multimedia is a hotly studied topic due to the widespread use of video upload sharing services. It’s also a challenging task because popular pattern is affected by multiple factors and is hard to be modeled. The goal of this paper is to use feature extraction techniques and variation auto-encoder (VAE) framework to predict the popularity of online micro-videos. First, we identify four declarable modalities that are important for adaptability and expansibility. Then, we design a multi-modal based VAE regression model (MASSL) to exploit the domestic and foreign information extracted from heterogeneous features. The model can be applied to large-scale multimedia platforms, even the modality absence scenarios. With extensive experiments conducted on the dataset, which was originally generated from the most popular video-sharing website in China, the result demonstrates the effectiveness of our proposed model by comparing with baseline approaches.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Communication} and {Information} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Zhuoran and Xu, Shibiao and Guo, Li and Lian, Wenke},
	year = {2023},
	note = {event-place: Beijing, China},
	keywords = {deep learning, social media, popularity prediction},
	pages = {9--16},
}

@inproceedings{yordanova_text2rlab_2025,
	address = {New York, NY, USA},
	series = {{IUI} '25 {Companion}},
	title = {{Text2RLab}: {No}-{Code} {Methodology} for {Robotic} {Programming} and {Interaction} in {Laboratory} {Tasks}},
	isbn = {979-8-4007-1409-2},
	url = {https://doi.org/10.1145/3708557.3716350},
	doi = {10.1145/3708557.3716350},
	abstract = {Using robotic systems in laboratory settings increases the quality and reproducibility of laboratory experiments. One challenge laboratory personal faces is the need of programming knowledge to set up the robotic system. To address this problem, in this work we propose a no-code methodology for robotic programming in laboratory tasks. The methodology takes as an input instructions in natural language and generates machine readable models of execution steps. The final result is a model with a probabilistic structure that allows connecting sensor observations to the model’s states, thus providing a mechanism for robotic action execution and state estimation. The proposed methodology has the potential to increase the applicability of robotic systems and thus to improve the quality and reproducibility of laboratory experiments.},
	booktitle = {Companion {Proceedings} of the 30th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Yordanova, Kristina Y. and Stoev, Teodor and Rebl, Henrike and Hahn, Olga and Peters, Kirsten},
	year = {2025},
	keywords = {model learning, plan generation, probabilistic modelling, robotic programming},
	pages = {96--100},
}

@article{ye_knowledge_2022,
	title = {A {Knowledge} {Graph}-{Enhanced} {Tensor} {Factorisation} {Model} for {Discovering} {Drug} {Targets}},
	volume = {19},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2022.3197320},
	doi = {10.1109/TCBB.2022.3197320},
	abstract = {The drug discovery and development process is a long and expensive one, costing over 1 billion USD on average per drug and taking 10-15 years. To reduce the high levels of attrition throughout the process, there has been a growing interest in applying machine learning methodologies to various stages of drug discovery and development in the recent decade, especially at the earliest stage – identification of druggable disease genes. In this paper, we have developed a new tensor factorisation model to predict potential drug targets (genes or proteins) for treating diseases. We created a three-dimensional data tensor consisting of 1,048 gene targets, 860 diseases and 230,011 evidence attributes and clinical outcomes connecting them, using data extracted from the Open Targets and PharmaProjects databases. We enriched the data with gene target representations learned from a drug discovery-oriented knowledge graph and applied our proposed method to predict the clinical outcomes for unseen gene target and disease pairs. We designed three evaluation strategies to measure the prediction performance and benchmarked several commonly used machine learning classifiers together with Bayesian matrix and tensor factorisation methods. The result shows that incorporating knowledge graph embeddings significantly improves the prediction accuracy and that training tensor factorisation alongside a dense neural network outperforms all other baselines. In summary, our framework combines two actively studied machine learning approaches to disease target identification, namely tensor factorisation and knowledge graph representation learning, which could be a promising avenue for further exploration in data-driven drug discovery.},
	number = {6},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Ye, Cheng and Swiers, Rowan and Bonner, Stephen and Barrett, Ian},
	month = aug,
	year = {2022},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {3070--3080},
}

@inproceedings{daun_teaching_2021,
	address = {Virtual Event, Spain},
	series = {{ICSE}-{JSEET} '21},
	title = {Teaching model-based requirements engineering to industry professionals: an experience report},
	isbn = {978-0-7381-3320-1},
	url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00013},
	doi = {10.1109/ICSE-SEET52601.2021.00013},
	abstract = {The use of conceptual models to foster requirements engineering has been proposed and evaluated as beneficial for several decades. For instance, goal-oriented requirements engineering or the specification of scenarios are commonly done using conceptual models. Bringing such model-based requirements engineering approaches into industrial practice typically requires industrial training. In this paper, we report lessons learned from a training program for teaching industry professionals modelbased requirements engineering. Particularly, we as educators and learners report experiences from designing the training program, conducting the actual training, and applying the instructed material in our day-to-day work. From these findings we provide guidelines for educators designing requirements engineering courses for industry professionals.},
	booktitle = {Proceedings of the 43rd {International} {Conference} on {Software} {Engineering}: {Joint} {Track} on {Software} {Engineering} {Education} and {Training}},
	publisher = {IEEE Press},
	author = {Daun, Marian and Brings, Jennifer and Goger, Marcel and Koch, Walter and Weyer, Thorsten},
	year = {2021},
	keywords = {requirements engineering, conceptual modeling, industrial training},
	pages = {40--49},
}

@inproceedings{jayasimha_deep_2020,
	address = {New York, NY, USA},
	series = {{CoDS} {COMAD} 2020},
	title = {Deep {Neural} {Learning} for {Automated} {Diagnostic} {Code} {Group} {Prediction} {Using} {Unstructured} {Nursing} {Notes}},
	isbn = {978-1-4503-7738-6},
	url = {https://doi.org/10.1145/3371158.3371176},
	doi = {10.1145/3371158.3371176},
	abstract = {Disease prediction, a central problem in clinical care and management, has gained much significance over the last decade. Nursing notes documented by caregivers contain valuable information concerning a patient's state, which can aid in the development of intelligent clinical prediction systems. Moreover, due to the limited adaptation of structured electronic health records in developing countries, the need for disease prediction from such clinical text has garnered substantial interest from the research community. The availability of large, publicly available databases such as MIMIC-III, and advancements in machine and deep learning models with high predictive capabilities have further facilitated research in this direction. In this work, we model the latent knowledge embedded in the unstructured clinical nursing notes, to address the clinical task of disease prediction as a multi-label classification of ICD-9 code groups. We present EnTAGS, which facilitates aggregation of the data in the clinical nursing notes of a patient, by modeling them independent of one another. To handle the sparsity and high dimensionality of clinical nursing notes effectively, our proposed EnTAGS is built on the topics extracted using Non-negative matrix factorization. Furthermore, we explore the applicability of deep learning models for the clinical task of disease prediction, and assess the reliability of the proposed models using standard evaluation metrics. Our experimental evaluation revealed that the proposed approach consistently exceeded the state-of-the-art prediction model by 1.87\% in accuracy, 12.68\% in AUPRC, and 11.64\% in MCC score.},
	booktitle = {Proceedings of the 7th {ACM} {IKDD} {CoDS} and 25th {COMAD}},
	publisher = {Association for Computing Machinery},
	author = {Jayasimha, Aditya and Gangavarapu, Tushaar and Kamath, S. Sowmya and Krishnan, Gokul S.},
	year = {2020},
	note = {event-place: Hyderabad, India},
	keywords = {Deep Learning, Multi-label Classification, Natural Language Processing, Clinical Decision Support Systems, Disease Prediction, Healthcare Analytics},
	pages = {152--160},
}

@article{howison_extracting_2025,
	title = {Extracting {Structured} {Labor} {Market} {Information} from {Job} {Postings} with {Generative} {AI}},
	volume = {6},
	url = {https://doi.org/10.1145/3674847},
	doi = {10.1145/3674847},
	abstract = {Labor market information is an important input to labor, workforce, education, and macroeconomic policy. However, granular and real-time data on labor market trends are lacking; publicly available data from survey samples are released with significant lags and miss critical information such as skills and benefits. We use generative Artificial Intelligence to automatically extract structured labor market information from unstructured online job postings for the entire U.S. labor market. To demonstrate our methodology, we construct a sample of 6,800 job postings stratified by 68 major occupational groups, extract structured information on educational requirements, remote-work flexibility, full-time availability, and benefits, and show how these job characteristics vary across occupations. As a validation, we compare frequencies of educational requirements by occupation from our sample to survey data and find no statistically significant difference. Finally, we discuss the scalability to collections of millions of job postings. Our results establish the feasibility of measuring labor market trends at scale from online job postings thanks to advances in generative AI techniques. Improved access to such insights at scale and in real-time could transform the ability of policy leaders, including federal and state agencies and education providers, to make data-informed decisions that better support the American workforce.},
	number = {1},
	journal = {Digit. Gov.: Res. Pract.},
	author = {Howison, Mark and Ensor, William O. and Maharjan, Suraj and Parikh, Rahil and Sengamedu, Srinivasan H. and Daniels, Paul and Gaither, Amber and Yeats, Carrie and Reddy, Chandan K. and Hastings, Justine S.},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {large language models, education, Amazon Bedrock, policy, Workforce},
}

@inproceedings{meijer_explaining_2021,
	address = {New York, NY, USA},
	series = {{SAC} '21},
	title = {Explaining a neural attention model for aspect-based sentiment classification using diagnostic classification},
	isbn = {978-1-4503-8104-8},
	url = {https://doi.org/10.1145/3412841.3441957},
	doi = {10.1145/3412841.3441957},
	abstract = {Many high performance machine learning models for Aspect-Based Sentiment Classification (ABSC) produce black box models, and therefore barely explain how they classify a certain sentiment value towards an aspect. In this paper, we propose explanation models, that inspect the internal dynamics of a state-of-the-art neural attention model, the LCR-Rot-hop, by using a technique called Diagnostic Classification. Our diagnostic classifier is a simple neural network, which evaluates whether the internal layers of the LCR-Rot-hop model encode useful word information for classification, i.e., the part of speech, the sentiment value, the presence of aspect relation, and the aspect-related sentiment value of words. We conclude that the lower layers in the LCR-Rot-hop model encode the part of speech and the sentiment value, whereas the higher layers represent the presence of a relation with the aspect and the aspect-related sentiment value of words.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Meijer, Lisa and Frasincar, Flavius and Truşcă, Maria Mihaela},
	year = {2021},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {aspect-based sentiment classification, diagnostic classification, neural rotatory attention model},
	pages = {821--827},
}

@inproceedings{zhang_sustainable_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {Sustainable {Grain} {Transportation} in {Ukraine} {Amidst} {War} {Utilizing} {KNARM} and {KnowWhereGraph}},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587618},
	doi = {10.1145/3543873.3587618},
	abstract = {In this work, we propose a sustainable path-finding application for grain transportation during the ongoing Russian military invasion in Ukraine. This application is to build a suite of algorithms to find possible optimal paths for transporting grain that remains in Ukraine. The application uses the KNowledge Acquisition and Representation Methodology(KNARM) and the KnowWhereGraph to achieve this goal. Currently, we are working towards creating an ontology that will allow for a more effective heuristic approach by incorporating the lessons learned from the KnowWhereGraph. The aim is to enhance the path-finding process and provide more accurate and efficient results. In the future, we will continue exploring and implementing new techniques that can further improve the sustainability of the path-finding applications with a knowledge graph backend for grain transportation through hazardous and adversarial environments. The code is available upon reviewer’s request. It can not be made public due to the sensitive nature of the data.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yinglun and Broyaka, Antonina and Kastens, Jude and Featherstone, Allen M. and Shimizu, Cogan and Hitzler, Pascal and Mcginty, Hande Küçük},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {knowledge graphs, ontology engineering, global food systems, path-finding},
	pages = {742--745},
}

@article{smith_generalisable_2023,
	title = {Generalisable {Dialogue}-based {Approach} for {Active} {Learning} of {Activities} of {Daily} {Living}},
	volume = {13},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/3616017},
	doi = {10.1145/3616017},
	abstract = {While Human Activity Recognition systems may benefit from Active Learning by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in activity recognition systems, which utilises semantic similarity measures and a dataset of natural language descriptions of common activities (which we make publicly available). Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply this approach to two active learning scenarios: (i) using an existing CASAS dataset, demonstrating long-term usage; and (ii) using an online activity recognition system, which tackles the issue of online segmentation and labelling. We demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) quantitatively, as an efficient way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to compare our approach and an established method. Results show the potential of our approach as a hands-free interface for annotation of sensor data as part of an active learning system. We provide insights into the challenges of active learning for activity recognition under real-world conditions and identify potential ways to address them.},
	number = {3},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Smith, Ronnie and Dragone, Mauro},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {semantic similarity, natural language, Active Learning (AL), Human Activity Recognition (HAR) labelling, Human-in-the-Loop (HITL) annotation},
}

@article{tang_queryartisan_2024,
	title = {{QueryArtisan}: {Generating} {Data} {Manipulation} {Codes} for {Ad}-hoc {Analysis} in {Data} {Lakes}},
	volume = {18},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3705829.3705832},
	doi = {10.14778/3705829.3705832},
	abstract = {Query processing over data lakes is a challenging task, often requiring extensive data pre-processing activities such as data cleaning, transformation, and loading. However, the advent of Large Language Models (LLMs) has illuminated a new pathway to address these complexities by offering a unified approach to understanding the diverse datasets submerged in data lakes. In this paper, we introduce QueryArtisan, a novel LLM-powered analytic tool specifically designed for data lakes. QueryArtisan transcends traditional ETL (Extract, Transform, Load) processes by generating just-intime code for dataset-specific queries. It eliminates the need for an intermediary schema, enabling users to query the data lake directly using natural language. To achieve this, we have developed a suite of heterogeneous operators capable of processing data across various modalities. Additionally, QueryArtisan incorporates a cost model-based query optimization technique, significantly enhancing its code generation capabilities for efficient query resolution. Our extensive experimental evaluations, conducted with real-life datasets, demonstrate that QueryArtisan markedly outperforms existing solutions in terms of effectiveness, efficiency and usability.},
	number = {2},
	journal = {Proc. VLDB Endow.},
	author = {Tang, Xiu and Liu, Wenhao and Wu, Sai and Yao, Chang and Yuan, Gongsheng and Ying, Shanshan and Chen, Gang},
	month = oct,
	year = {2024},
	note = {Publisher: VLDB Endowment},
	pages = {108--116},
}

@inproceedings{gbenro_hmmeta_2020,
	address = {New York, NY, USA},
	series = {{BCB} '20},
	title = {{HMMeta}: {Protein} {Function} {Prediction} using {Hidden} {Markov} {Models}},
	isbn = {978-1-4503-7964-9},
	url = {https://doi.org/10.1145/3388440.3414702},
	doi = {10.1145/3388440.3414702},
	abstract = {As the body of genomic product data increases at a much faster rate than can be annotated, computational analysis of protein function has never been more important. In this research, we introduce a novel protein function prediction method HMMeta, which is based on the prominent natural language prediction technique Hidden Markov Models (HMM). With a new representation of protein sequence as a language, we trained a unique HMM for each Gene Ontology (GO) term taken from the UniProt database, which in total has 27,451 unique GO IDs leading to the creation of 27,451 Hidden Markov Models. We employed data augmentation to artificially inflate the number of protein sequences associated with GO terms that have a limited amount in the database, and this helped to balance the number of protein sequences associated with each GO term. Predictions are made by running the sequence against each model created. The models within eighty percent of the top scoring model, or 75 models with the highest scores, whichever is less, represent the functions that are most associated with the given sequence. We benchmarked our method in the latest Critical Assessment of protein Function Annotation (CAFA 4) experiment as CaoLab2, and we also evaluated HMMeta against several other protein function prediction methods against a subset of the UniProt database. HMMeta achieved favorable results as a sequence-based method, and outperforms a few notable methods in some categories through our evaluation, which shows great potential for automated protein function prediction. The tool is available at https://github.com/KPHippe/HMM-For-Protein-Prediction.},
	booktitle = {Proceedings of the 11th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Gbenro, Sola and Hippe, Kyle and Cao, Renzhi},
	year = {2020},
	note = {event-place: Virtual Event, USA},
	keywords = {Machine Learning, Hidden Markov Model, Protein Function Prediction},
}

@inproceedings{acuna_towards_2021,
	address = {New York, NY, USA},
	series = {{ICFET} '21},
	title = {Towards the {Development} of an {Adaptive} {E}-learning {System} with {Chatbot} {Using} {Personalized} {E}-learning {Model}},
	isbn = {978-1-4503-8972-3},
	url = {https://doi.org/10.1145/3473141.3473236},
	doi = {10.1145/3473141.3473236},
	abstract = {Although there is no distinctive header, this is the abstract. This submission template allows authors to submit their papers E-learning has become one of the most used electronic systems in the field of education. Although it is beneficial, there are still some lacking capabilities and considerations that can negatively affect the performance of the students. This leads to the innovation that makes e-learning systems adaptive to the users’ personality, knowledge, behavior, interest, or preferences, the system is called personalized e-learning system. This survey paper aims to provide the general parameters in creating a personalized e-learning system based on the 150 research papers collected, and a timespan of 2016 to 2020 as a condition. Through a series of literature reviews of research papers published in the last five years, also related to personalized e-learning systems, this paper presents the common components, tools and algorithms, and learning model that are generally used in developing a personalized e-learning system to help as reference in developing more effective personalized e-learning systems. Moreover, considering the findings of this study, this paper has proposed developing a hybrid e-learning system with a chatbot.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Frontiers} of {Educational} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Acuna, Gabriel Edrick and Alvarez, Luis Antonio and Miraflores, Jeffrey and Samonte, Mary Jane},
	year = {2021},
	note = {event-place: Bangkok, Thailand},
	keywords = {Adaptive, Chatbot, Myer-Briggs Type Indicator Theory, Personalized e-Learning Model},
	pages = {120--125},
}

@inproceedings{huang_computing_2019,
	address = {New York, NY, USA},
	series = {{AIPR} '19},
	title = {Computing the semantic similarity between documents by the copula-based econometric models},
	isbn = {978-1-4503-7229-9},
	url = {https://doi.org/10.1145/3357254.3357277},
	doi = {10.1145/3357254.3357277},
	abstract = {Semantic similarity is important information with which decision-makers can cluster, classify, or compare documents in text mining. Statistical and topological methods are two major ways to determine semantic similarity. However, conventional methods ignore the time factor when calculating the similarity between documents. It should be highlighted that narrative emotions play a critical role in comparing documents. In this paper, copula-based econometric models, including ARMA and GARCH families, are used to calculate the narrative semantic similarity between documents.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Artificial} {Intelligence} and {Pattern} {Recognition}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Jih-Jeng},
	year = {2019},
	note = {event-place: Beijing, China},
	keywords = {text mining, semantic similarity, copula, econometric models},
	pages = {134--139},
}

@article{lin_leveraging_2024,
	title = {Leveraging {Dual} {Gloss} {Encoders} in {Chinese} {Biomedical} {Entity} {Linking}},
	volume = {23},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3638555},
	doi = {10.1145/3638555},
	abstract = {Entity linking is the task of assigning a unique identity to named entities mentioned in a text, a sort of word sense disambiguation that focuses on automatically determining a pre-defined sense for a target entity to be disambiguated. This study proposes the DGE (Dual Gloss Encoders) model for Chinese entity linking in the biomedical domain. We separately model a dual encoder architecture, comprising a context-aware gloss encoder and a lexical gloss encoder, for contextualized embedding representations. DGE are then jointly optimized to assign the nearest gloss with the highest score for target entity disambiguation. The experimental datasets consist of a total of 10,218 sentences that were manually annotated with glosses defined in the BabelNet 5.0 across 40 distinct biomedical entities. Experimental results show that the DGE model achieved an F1-score of 97.81, outperforming other existing methods. A series of model analyses indicate that the proposed approach is effective for Chinese biomedical entity linking.},
	number = {2},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Lin, Tzu-Mi and Hung, Man-Chen and Lee, Lung-Hao},
	month = feb,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {lexical semantics, biomedical informatics, natural language understanding, Word sense disambiguation, language transformers},
}

@inproceedings{sahlmann_ontology-based_2018,
	address = {New York, NY, USA},
	series = {{IOT} '18},
	title = {Ontology-based virtual {IoT} devices for edge computing},
	isbn = {978-1-4503-6564-2},
	url = {https://doi.org/10.1145/3277593.3277597},
	doi = {10.1145/3277593.3277597},
	abstract = {An IoT network may consist of hundreds heterogeneous devices. Some of them may be constrained in terms of memory, power, processing and network capacity. Manual network and service management of IoT devices are challenging. We propose a usage of an ontology for the IoT device descriptions enabling automatic network management as well as service discovery and aggregation. Our IoT architecture approach ensures interoperability using existing standards, i.e. MQTT protocol and Semantic Web technologies. We herein introduce virtual IoT devices and their semantic framework deployed at the edge of network. As a result, virtual devices are enabled to aggregate capabilities of IoT devices, derive new services by inference, delegate requests/responses and generate events. Furthermore, they can collect and pre-process sensor data. These tasks on the edge computing overcome the shortcomings of the cloud usage regarding siloization, network bandwidth, latency and speed. We validate our proposition by implementing a virtual device on a Raspberry Pi.},
	booktitle = {Proceedings of the 8th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Sahlmann, Kristina and Schwotzer, Thomas},
	year = {2018},
	note = {event-place: Santa Barbara, California, USA},
	keywords = {internet of things, semantic interoperability, edge computing, M2M, MQTT, oneM2M ontology},
}

@article{bomba_choreographer-performer_2024,
	title = {The {Choreographer}-{Performer} {Continuum}: {A} {Diffraction} {Tool} to {Illuminate} {Authorship} in {More} {Than} {Human} {Co}-{Performances}},
	volume = {31},
	issn = {1073-0516},
	url = {https://doi.org/10.1145/3689040},
	doi = {10.1145/3689040},
	abstract = {The design of robust and trustworthy Generative AI (GenAI) requires a deep understanding of the agencies emerging from human interactions with them. To contribute to this goal, we retrospectively studied an art project involving a visual artist, a computer scientist, an artistic director, and a generative model (GPT-2). The model was fine-tuned with trip reports describing the experience of eating psychedelic mushrooms. Building on agential realism, we analysed the co-performance between the artist and the model as their agency moved along the choreographer-performer continuum. Results reveal ontological surprises, leading to the proposal of entangled authorship to de-individualise the production of knowledge from a More Than Human perspective. The paper illustrates how art can expose different forms of relationships, challenging the idea of GenAI as just a tool that simplifies or replaces human labour. We conclude by emphasising the transformational potential of GenAI for novel modes of engagement between humans and machines.},
	number = {6},
	journal = {ACM Trans. Comput.-Hum. Interact.},
	author = {Bomba, Federico and Menéndez-Blanco, María and Grigis, Paolo and Cremaschi, Michele and De Angeli, Antonella},
	month = dec,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Large Language Models, Large language model, Language model, Agency, Agential realism, Performance, Creative AI, Agential Realism, AI and Art, Hallucination, Creatives, Psychoacoustic, AI and art, Continuum mechanics, Human form models, Humaninteraction},
	annote = {Cited by: 2},
}

@inproceedings{li_research_2024,
	address = {New York, NY, USA},
	series = {{CISAI} '24},
	title = {Research on {Chinese} {Knowledge} {Base} and {Knowledge} {Q}\&amp;{A} {Technology} for {Power} {Grid} {Dispatching}},
	isbn = {979-8-4007-0725-4},
	url = {https://doi.org/10.1145/3703187.3703192},
	doi = {10.1145/3703187.3703192},
	abstract = {To support online professional knowledge query for power grid dispatchers, this article proposes a method for constructing a knowledge base based on knowledge graph, which implements systematic organization and management of knowledge resources in the field of power-grid dispatching. Moreover, a questions and answers (Q\&amp;A) service is design based on large language model and proposed knowledge base. Based on the constructed knowledge base and Q\&amp;A service, auxiliary learning functions can be provided in the domain of power grid operation. This enables accurate acquisition of professional knowledge through Chinese natural language interaction, enhancing the effectiveness and flexibility of online training for power-grid dispatchers.},
	booktitle = {Proceedings of the 2024 7th {International} {Conference} on {Computer} {Information} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Li, Chengxue and Chen, Xuyang and Ding, Min and Jin, Wei and Gao, Feng},
	year = {2024},
	keywords = {Knowledge graph, Large language model, Question answering, Graph database},
	pages = {18--23},
}

@inproceedings{al-fedaghi_modeling_2018,
	address = {New York, NY, USA},
	series = {{ICGDA} '18},
	title = {Modeling of an enterprise and information system: process specification based on the flow of things},
	isbn = {978-1-4503-6445-4},
	url = {https://doi.org/10.1145/3220228.3220246},
	doi = {10.1145/3220228.3220246},
	abstract = {Current modeling of enterprise and information systems is based on diverse methods such as function-orientation, data-orientation, process-orientation object-orientation, and object/process-orientation. Other emerging modeling methods include the ontology capture method and Business Process Modeling Notation (BPMN). These approaches have been criticized for lack of execution environment into which events that cause change could be incorporated. Such a topic is important in the study of dynamic behavior of systems for both analysis and control. This paper further develops a new approach oriented toward things that flow. Specifically, the paper concentrates on the study of process specification in analysis and design of enterprise/system architectures in order to most effectively facilitate their control. Here we produce a single, integrated diagrammatic representation that uniformly incorporates structural and behavioral aspects into an underlying conceptual description. The viability of the model is demonstrated by applying it to a case study of services provided by an existing organizational unit.},
	booktitle = {Proceedings of the {International} {Conference} on {Geoinformatics} and {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Al-Fedaghi, Sabah and Alduwaisan, Yousef},
	year = {2018},
	note = {event-place: Prague, Czech Republic},
	keywords = {UML, BPMN, conceptual modeling, data flow models, process specification},
	pages = {142--150},
}

@inproceedings{vedula_bolt-k_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {{BOLT}-{K}: {Bootstrapping} {Ontology} {Learning} via {Transfer} of {Knowledge}},
	isbn = {978-1-4503-6674-8},
	url = {https://doi.org/10.1145/3308558.3313511},
	doi = {10.1145/3308558.3313511},
	abstract = {Dynamically extracting and representing continually evolving knowledge entities is an essential scaffold for grounded intelligence and decision making. Creating knowledge schemas for newly emerging, unfamiliar, domain-specific ideas or events poses the following challenges: (i) detecting relevant, often previously unknown concepts associated with the new domain; and (ii) learning ontological, semantically accurate relationships among the new concepts, despite having severely limited annotated data. To this end, we propose a novel LSTM-based framework with attentive pooling, BOLT-K, to learn an ontology for a target subject or domain. We bootstrap our ontology learning approach by adapting and transferring knowledge from an existing, functionally related source domain. We also augment the inadequate labeled data available for the target domain with various strategies to minimize human expertise during model development and training. BOLT-K first employs semantic and graphical features to recognize the entity or concept pairs likely to be related to each other, and filters out spurious concept combinations. It is then jointly trained on knowledge from the target and source domains to learn relationships among the target concepts. The target concepts and their corresponding relationships are subsequently used to construct an ontology. We extensively evaluate our framework on several, real-world bio-medical and commercial product domain ontologies. We obtain significant improvements of 5-25\% F1-score points over state-of-the-art baselines. We also examine the potential of BOLT-K in detecting the presence of novel kinds of relationships that were unseen during training.},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Vedula, Nikhita and Maneriker, Pranav and Parthasarathy, Srinivasan},
	year = {2019},
	note = {event-place: San Francisco, CA, USA},
	pages = {1897--1908},
}

@inproceedings{hoppe_support_2023,
	address = {New York, NY, USA},
	series = {{WebMedia} '23},
	title = {Support for {Single} {Page} {Application} {Frameworks} on {FrameWeb}},
	isbn = {979-8-4007-0908-1},
	url = {https://doi.org/10.1145/3617023.3617059},
	doi = {10.1145/3617023.3617059},
	abstract = {In the field of Web Engineering, many methods have been proposed to guide developers in designing and coding Web applications. The FrameWeb method is a model-driven approach that targets the development of systems that use certain kinds of frameworks in their architecture, proposing the use of models that incorporate concepts from these frameworks during design. Currently, the FrameWeb method does not consider SPA (Single Page Application) frameworks and, in recent years, they have gained a lot of popularity among developers. In this work, we propose to add support for SPA frameworks to FrameWeb. With our research, we have managed to update the FrameWeb meta-model so that its modeling language now supports SPA frameworks and their constructs. FrameWeb tools (graphical editor and code generator) also evolved to support the new elements. Experiments of modeling existing SPAs with this new version of FrameWeb, generating code from the models and comparing with the original, showed that, in average, around 69\% of the HTML tags could be generated from the models. The support for SPA frameworks in FrameWeb allows developers to design and model their applications using constructs that relate to the frameworks used in practice, facilitating developer communication using the models and generating code to improve developer productivity.},
	booktitle = {Proceedings of the 29th {Brazilian} {Symposium} on {Multimedia} and the {Web}},
	publisher = {Association for Computing Machinery},
	author = {Hoppe, Pedro Henrique Brunoro and Souza, Vítor Estêvão Silva},
	year = {2023},
	note = {event-place: Ribeirão Preto, Brazil},
	keywords = {Reuse, Software Engineering, tools, DSL, method, language, FrameWeb, MDD, SPA Frameworks, Web Engineering, WIS Frameworks},
	pages = {260--268},
}

@inproceedings{jokinen_exploring_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {Exploring a {Japanese} {Cooking} {Database}},
	isbn = {979-8-4007-0323-2},
	url = {https://doi.org/10.1145/3610978.3640622},
	doi = {10.1145/3610978.3640622},
	abstract = {The paper describes ongoing work applying Generative AI to a real world application. We use Retrieval Augmented Generation and other GenAI tools that combine large language models with Neo4j knowledge graphs. These tools help a robot to chat in English about Japanese cooking using a knowledge base that is in Japanese.},
	booktitle = {Companion of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Jokinen, Kristiina and Wilcock, Graham},
	year = {2024},
	note = {event-place: Boulder, CO, USA},
	keywords = {knowledge graphs, large language models, generative AI, retrieval augmented generation, semantic search, graph databases, cypher query language, Japanese cooking, social robots},
	pages = {578--582},
}

@inproceedings{steimann_semantics_2022,
	address = {New York, NY, USA},
	series = {{SLE} 2022},
	title = {The {Semantics} of {Plurals}},
	isbn = {978-1-4503-9919-7},
	url = {https://doi.org/10.1145/3567512.3567516},
	doi = {10.1145/3567512.3567516},
	abstract = {Inside many software languages lives an expression language that caters for the computation of single values from single values. These languages' fixation on single-valuedness is often at odds with their application domains, in which many values, or plurals, regularly occur in the places of single. While the classical mathematical means of dealing with plurals is the set, in computing, other representations have evolved, notably strings and the much lesser known bunches. We review bunch theory in the context of expression languages including non-recursive functions, and show how giving bunches set semantics suggests that evaluating bunch functions amounts to computing with relations. We maintain that the ensuing seamless integration of relations in expression languages that otherwise know only functions makes a worthwhile contribution in a field in which the difference between modeling, with its preference for relations, and programming, with its preference for functions, is increasingly considered accidental.},
	booktitle = {Proceedings of the 15th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Steimann, Friedrich and Freitag, Marius},
	year = {2022},
	note = {event-place: Auckland, New Zealand},
	keywords = {modeling, bunches, collections, denotational semantics, relational languages},
	pages = {36--54},
}

@inproceedings{lu_trend_2021,
	address = {New York, NY, USA},
	series = {{CIPAE} 2021},
	title = {Trend estimation model of students' thought and behavior based on big data},
	isbn = {978-1-4503-8996-9},
	url = {https://doi.org/10.1145/3456887.3457465},
	doi = {10.1145/3456887.3457465},
	abstract = {With the rapid development of global informatization, the ways for contemporary college students to acquire knowledge are enriched. College students have active thinking, strong ability to accept new things, easy to be influenced by various cultures, open-minded, and novel values. College students' ideas have gradually matured, but they are highly malleable and likely to be influenced by the external environment. In order to quantitatively analyze the trend of students' thinking and behavior, an estimation model of students' thinking and behavior trend based on big data is proposed. Constructing semantic ontology big data distribution set of students' thought and behavior trends, establishing semantic ontology fusion feature distribution set of students' thought and behavior trend estimation by adopting multi-source parameter distributed reconstruction and phase space fusion analysis methods, analyzing the parameter feature quantity of students' thought and behavior trend distribution fusion by combining ambiguity detection and information feature matching methods, and constructing association rule distribution set of students' thought and behavior trend estimation by adopting global ambiguity reconstruction and feature reconstruction methods. Through fuzzy detection and information fusion, the semantic structure characteristics of students' ideological and behavioral trends are analyzed. By matching ideological and behavioral characteristics and mining statistical information, students' ideological and behavioral trends are estimated and self-adaptive convergence control is realized, and the optimal solution of students' ideological and behavioral trends estimation is obtained. The simulation results show that this method is highly adaptive and stable in estimating the trend of students' thinking and behavior, and improves the accurate probability of estimating the trend of students' thinking and behavior.},
	booktitle = {2021 2nd {International} {Conference} on {Computers}, {Information} {Processing} and {Advanced} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Lu, Bai},
	year = {2021},
	note = {event-place: Ottawa, ON, Canada},
	keywords = {Big data, Information fusion, Students' thought and behavior, Trend estimation},
	pages = {1084--1089},
}

@article{russo_agile_2021,
	title = {The {Agile} {Success} {Model}: {A} {Mixed}-methods {Study} of a {Large}-scale {Agile} {Transformation}},
	volume = {30},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3464938},
	doi = {10.1145/3464938},
	abstract = {Organizations are increasingly adopting Agile frameworks for their internal software development. Cost reduction, rapid deployment, requirements and mental model alignment are typical reasons for an Agile transformation. This article presents an in-depth field study of a large-scale Agile transformation in a mission-critical environment, where stakeholders’ commitment was a critical success factor. The goal of such a transformation was to implement mission-oriented features, reducing costs and time to operate in critical scenarios. The project lasted several years and involved over 40 professionals. We report how a hierarchical and plan-driven organization exploited Agile methods to develop a Command \&amp; Control (C2) system. Accordingly, we first abstract our experience, inducing a success model of general use for other comparable organizations by performing a post-mortem study. The goal of the inductive research process was to identify critical success factors and their relations. Finally, we validated and generalized our model through Partial Least Squares - Structural Equation Modelling, surveying 200 software engineers involved in similar projects. We conclude the article with data-driven recommendations concerning the management of Agile projects.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Russo, Daniel},
	month = jul,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Agile, ethnography, grounded theory, mixed methods research, multivariate analysis, partial least squares, structural equation modeling},
}

@inproceedings{rasti_symboleo2sc_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {{Symboleo2SC}: from legal contract specifications to smart contracts},
	isbn = {978-1-4503-9466-6},
	url = {https://doi.org/10.1145/3550355.3552407},
	doi = {10.1145/3550355.3552407},
	abstract = {Smart contracts (SCs) are software systems that monitor and control the execution of legal contracts to ensure compliance with the contracts' terms and conditions. They often exploit Internet-of-Things technologies to support their monitoring functions, and blockchain technology to ensure the integrity of their data. Ethereum and business blockchain platforms, such as Hyperledger Fabric, are popular choices for SC development. However, there is a gap in the knowledge of SCs between developers and legal experts. Symboleo is a formal specification language for legal contracts that was introduced to address this issue. Symboleo specifications directly encode legal concepts such as parties, obligations, and powers. In this paper, we propose a tool-supported method for translating Symboleo specifications into smart contracts. We have extended the current Symboleo IDE, implemented the ontology and semantics of Symboleo into a reusable library, and developed the Symboleo2SC tool to generate Hyperledger Fabric code exploiting this library. Symboleo2SC was evaluated with three sample contracts. The results shows that legal contract specifications in Symboleo can be fully converted to SCs for monitoring purposes. Moreover, Symboleo2SC helps simplify the SC development process, saves development effort, and helps reduce risks of coding errors.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rasti, Aidin and Amyot, Daniel and Parvizimosaed, Alireza and Roveri, Marco and Logrippo, Luigi and Anda, Amal Ahmed and Mylopoulos, John},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {blockchain, smart contracts, domain-specific languages, code generation, legal ontology},
	pages = {300--310},
}

@article{zhang_deep_2020,
	title = {A {Deep} {Learning} {Framework} for {Gene} {Ontology} {Annotations} {With} {Sequence}- and {Network}-{Based} {Information}},
	volume = {18},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2020.2968882},
	doi = {10.1109/TCBB.2020.2968882},
	abstract = {Knowledge of protein functions plays an important role in biology and medicine. With the rapid development of high-throughput technologies, a huge number of proteins have been discovered. However, there are a great number of proteins without functional annotations. A protein usually has multiple functions and some functions or biological processes require interactions of a plurality of proteins. Additionally, Gene Ontology provides a useful classification for protein functions and contains more than 40,000 terms. We propose a deep learning framework called DeepGOA to predict protein functions with protein sequences and protein-protein interaction (PPI) networks. For protein sequences, we extract two types of information: sequence semantic information and subsequence-based features. We use the word2vec technique to numerically represent protein sequences, and utilize a Bi-directional Long and Short Time Memory (Bi-LSTM) and multi-scale convolutional neural network (multi-scale CNN) to obtain the global and local semantic features of protein sequences, respectively. Additionally, we use the InterPro tool to scan protein sequences for extracting subsequence-based information, such as domains and motifs. Then, the information is plugged into a neural network to generate high-quality features. For the PPI network, the Deepwalk algorithm is applied to generate its embedding information of PPI. Then the two types of features are concatenated together to predict protein functions. To evaluate the performance of DeepGOA, several different evaluation methods and metrics are utilized. The experimental results show that DeepGOA outperforms DeepGO and BLAST.},
	number = {6},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Zhang, Fuhao and Song, Hong and Zeng, Min and Wu, Fang-Xiang and Li, Yaohang and Pan, Yi and Li, Min},
	month = jan,
	year = {2020},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {2208--2217},
}

@inproceedings{dewitte_comparison_2019,
	address = {New York, NY, USA},
	series = {{SAC} '19},
	title = {A comparison of system description models for data protection by design},
	isbn = {978-1-4503-5933-7},
	url = {https://doi.org/10.1145/3297280.3297595},
	doi = {10.1145/3297280.3297595},
	abstract = {Since the General Data Protection Regulation (GDPR) entered into force, every actor involved in the processing of personal data must comply with Data Protection by Design (DPbD). Doing so requires assessing the risks to data subjects' rights and freedoms and implementing appropriate countermeasures. While legal experts traditionally apply Data Protection Impact Assessments (DPIA), software engineers rely on threat modeling for their assessment.Despite significant differences, both approaches nonetheless revolve around (i) a description of the system and (ii) the identification, assessment and mitigation of specific risks. In practice, however, DPIAs and threat modeling are usually performed in complete isolation, following their own, unharmonized lexicon and abstractions. Such as disconnect lowers the quality of the assessment and of the conceptual and architectural trade-offsIn this paper, we present (i) an overview of the legal and architectural modeling requirements and (ii) incentives and recommendations for aligning both modeling paradigms in order to support data protection by design from both a legal and a technical perspective.},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Dewitte, Pierre and Wuyts, Kim and Sion, Laurens and Van Landuyt, Dimitri and Emanuilov, Ivo and Valcke, Peggy and Joosen, Wouter},
	year = {2019},
	note = {event-place: Limassol, Cyprus},
	keywords = {threat modeling, privacy, data protection by design, system model},
	pages = {1512--1515},
}

@article{niraula_linguistic_2022,
	title = {Linguistic {Taboos} and {Euphemisms} in {Nepali}},
	volume = {21},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3524111},
	doi = {10.1145/3524111},
	abstract = {Languages across the world have words, phrases, and behaviors—the taboos—that are avoided in public communication considering them as obscene or disturbing to the social, religious, and ethical values of society. However, people deliberately use these linguistic taboos and other language constructs to make hurtful, derogatory, and obscene comments. It is nearly impossible to construct a universal set of offensive or taboo terms because offensiveness is determined entirely by different factors such as socio-physical setting, speaker-listener relationship, and word choices. In this article, we present a detailed corpus-based study of offensive language in Nepali. We identify and describe more than 18 different categories of linguistic offenses including politics, religion, race, and sex. We discuss 12 common euphemisms, such as synonym, metaphor, and circumlocution. In addition, we introduce a manually constructed dataset of more than 1,000 offensive and taboo terms popular among contemporary speakers. We describe the first experiments that provide baseline results in detecting offensive language in Nepali. This in-depth study of offensive language and resource will provide a foundation for several downstream tasks, such as offensive language detection and language learning.},
	number = {6},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Niraula, Nobal B. and Dulal, Saurab and Koirala, Diwa},
	month = nov,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {dataset, low-resource language, linguistic taboo, Nepali language, Offensive language, offensive language detection},
}

@inproceedings{spiliotopoulos_citizen_2020,
	address = {New York, NY, USA},
	series = {{MEDES} '19},
	title = {Citizen {Engagement} for {Transparent} and {Accountable} {Policy} {Modelling}},
	isbn = {978-1-4503-6238-2},
	url = {https://doi.org/10.1145/3297662.3365813},
	doi = {10.1145/3297662.3365813},
	abstract = {This work presents a platform for linked legislative data to engage citizens in transparent and effective democracies. With a focus on scaling up participatory approaches from local to national level, the approach extends well established and open source tools and technologies, to build mobile monitoring and analysis tools that increase transparency of law-making and implementation to citizens. This is achieved by combining open data and open services with user and citizen generated content, in order to address citizen's needs in the context of open government. Data and feeds from trusted sources are interconnected with new and re-purposed data feeds generated by users via the social web to form a meaningful, searchable, customizable, reusable and open data-focused personalised mobile public service approach. The framework exploits the social aspects of open data, as well as the training of users, citizens and public servants to be able to understand and demand useful public open data, as well as facilitate the opening of more data.},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Spiliotopoulos, Dimitris and Margaris, Dionisis and Vassilakis, Costas},
	year = {2020},
	note = {event-place: Limassol, Cyprus},
	keywords = {Natural Language Processing, Accountability, e-Government, Transparency, Citizen Engagement, Legislation, Mobile Public Services, Policy Modelling},
	pages = {158--165},
}

@inproceedings{feng_cmdbench_2024,
	address = {New York, NY, USA},
	series = {{GUIDE}-{AI} '24},
	title = {{CMDBench}: {A} {Benchmark} for {Coarse}-to-fine {Multimodal} {Data} {Discovery} in {Compound} {AI} {Systems}},
	isbn = {979-8-4007-0694-3},
	url = {https://doi.org/10.1145/3665601.3669846},
	doi = {10.1145/3665601.3669846},
	abstract = {Compound AI systems (CASs) that employ LLMs as agents to accomplish knowledge-intensive tasks via interactions with tools and data retrievers have garnered significant interest within database and AI communities. While these systems have the potential to supplement typical analysis workflows of data analysts in enterprise data platforms, unfortunately, CASs are subject to the same data discovery challenges that analysts have encountered over the years — silos of multimodal data sources, created across teams and departments within an organization, make it difficult to identify appropriate data sources for accomplishing the task at hand. Existing data discovery benchmarks do not model such multimodality and multiplicity of data sources. Moreover, benchmarks of CASs prioritize only evaluating end-to-end task performance. To catalyze research on evaluating the data discovery performance of multimodal data retrievers in CASs within a real-world setting, we propose CMDBench, a benchmark modeling the complexity of enterprise data platforms. We adapt existing datasets and benchmarks in open-domain — from question answering and complex reasoning tasks to natural language querying over structured data — to evaluate coarse- and fine-grained data discovery and task execution performance. Our experiments reveal the impact of data retriever design on downstream task performance — 46\% drop in task accuracy on average — across various modalities, data sources, and task difficulty. The results indicate the need to develop optimization strategies to identify appropriate LLM agents and retrievers for efficient execution of CASs over enterprise data.},
	booktitle = {Proceedings of the {Conference} on {Governance}, {Understanding} and {Integration} of {Data} for {Effective} and {Responsible} {AI}},
	publisher = {Association for Computing Machinery},
	author = {Feng, Yanlin and Rahman, Sajjadur and Feng, Aaron and Chen, Vincent and Kandogan, Eser},
	year = {2024},
	note = {event-place: Santiago, AA, Chile},
	keywords = {LLMs, Benchmark, Compound AI Systems., Data Discovery},
	pages = {16--25},
}

@article{ranjan_mcws-transformers_2022,
	title = {{MCWS}-{Transformers}: {Towards} an {Efficient} {Modeling} of {Protein} {Sequences} via {Multi} {Context}-{Window} {Based} {Scaled} {Self}-{Attention}},
	volume = {20},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2022.3173789},
	doi = {10.1109/TCBB.2022.3173789},
	abstract = {This paper advances the self-attention mechanism in the standard transformer network specific to the modeling of the protein sequences. We introduce a novel \&lt;italic\&gt;context-window based scaled self-attention\&lt;/italic\&gt; mechanism for processing protein sequences that is based on the notion of (i) \&lt;italic\&gt;local context\&lt;/italic\&gt; and (ii) \&lt;italic\&gt;large contextual pattern\&lt;/italic\&gt;. Both notions are essential to building a good representation for protein sequences. The proposed \&lt;italic\&gt;context-window based scaled self-attention\&lt;/italic\&gt; mechanism is further used to build the \&lt;italic\&gt;multi context-window based scaled (MCWS) transformer\&lt;/italic\&gt; network for the protein function prediction task at the protein sub-sequence level. Overall, the proposed \&lt;italic\&gt;MCWS transformer\&lt;/italic\&gt; network produced improved predictive performances, outperforming existing state-of-the-art approaches by substantial margins. With respect to the standard transformer network, the proposed network produced improvements in F1-score of +2.30\% and +2.08\% on the biological process (BP) and molecular function (MF) datasets, respectively. The corresponding improvements over the state-of-the-art ProtVecGen-Plus+ProtVecGen-Ensemble approach are +3.38\% (BP) and +2.86\% (MF). Equally important, robust performances were obtained across protein sequences of different lengths.},
	number = {2},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Ranjan, Ashish and Fahad, Md Shah and Fernández-Baca, David and Tripathi, Sudhakar and Deepak, Akshay},
	month = may,
	year = {2022},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {1188--1199},
}

@inproceedings{chen_formal_2021,
	address = {New York, NY, USA},
	series = {{ICAIIS} 2021},
	title = {Formal {Semantic} {Model} for {Mobile} {Cloud} {Service} {System}},
	isbn = {978-1-4503-9020-0},
	url = {https://doi.org/10.1145/3469213.3470326},
	doi = {10.1145/3469213.3470326},
	abstract = {The paper proposes a semantic retrieval model for mobile learning resources based on ontology. The model includes three modules: word segmentation of retrieval information, semantic expansion, and semantic retrieval. We describe the agent as an object node in the category theory, the interaction and dependency between the agents as a morphism, and the entire cloud service system as a type category graph. The case study shows that the framework cannot only support the modeling and analysis of the service system at the abstract level, but also support the service composition by mapping the abstract model to the realization technology, which can analyze the correctness of the requirement decomposition and service composition well. It can be used to guide the description and construction of the service system.},
	booktitle = {2021 2nd {International} {Conference} on {Artificial} {Intelligence} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Qiuping},
	year = {2021},
	note = {event-place: Chongqing, China},
}

@article{das_imagetext_2023,
	title = {Image–{Text} {Multimodal} {Sentiment} {Analysis} {Framework} of {Assamese} {News} {Articles} {Using} {Late} {Fusion}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3584861},
	doi = {10.1145/3584861},
	abstract = {Before the arrival of the web as a corpus, people detected positive and negative news based on the understanding of the textual content from physical newspaper rather than an automatic identification approach from readily available e-newspapers. Thus, the earlier sentiment analysis approach is based on unimodal data, and less effort is paid to the multimodal data. However, the presence of multimodal information helps us to get a clearer understanding of the sentiment. To the best of our knowledge, less work has been introduced on the image–text multimodal sentiment analysis framework of Assamese, a low-resource Indian language mostly spoken in the northeast part of India. We built an Assamese news articles dataset consisting of news text and associated images and one image caption to conduct an experimental study. Focusing on important words and discriminative regions of the images mostly related to sentiment, two individual unimodal such as textual and visual models are proposed. The visual model is developed using an encoder-decoder–based image caption generation system. An image–text multimodal approach is proposed to explore the internal correlation between textual and visual features for joint sentiment classification. Finally, we propose the multimodal sentiment analysis framework, i.e., Textual Visual Multimodal Fusion, by employing a late fusion scheme to merge the three different modalities for the final sentiment prediction. Experimental results conducted on the Assamese dataset built in-house demonstrate that the contextual integration of multimodal features delivers better performance than unimodal features.},
	number = {6},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Das, Ringki and Singh, Thoudam Doren},
	month = jun,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {caption generation, late fusion, low resource language, machine learning classifier, Multimodal sentiment analysis},
}

@article{kursuncu_modeling_2019,
	title = {Modeling {Islamist} {Extremist} {Communications} on {Social} {Media} using {Contextual} {Dimensions}: {Religion}, {Ideology}, and {Hate}},
	volume = {3},
	url = {https://doi.org/10.1145/3359253},
	doi = {10.1145/3359253},
	abstract = {Terror attacks have been linked in part to online extremist content. Online conversations are cloaked in religious ambiguity, with deceptive intentions, often twisted from mainstream meaning to serve a malevolent ideology. Although tens of thousands of Islamist extremism supporters consume such content, they are a small fraction relative to peaceful Muslims. The efforts to contain the ever-evolving extremism on social media platforms have remained inadequate and mostly ineffective. Divergent extremist and mainstream contexts challenge machine interpretation, with a particular threat to the precision of classification algorithms. Radicalization is a subtle long-running persuasive process that occurs over time. Our context-aware computational approach to the analysis of extremist content on Twitter breaks down this persuasion process into building blocks that acknowledge inherent ambiguity and sparsity that likely challenge both manual and automated classification. Based on prior empirical and qualitative research in social sciences, particularly political science, we model this process using a combination of three contextual dimensions – religion, ideology, and hate – each elucidating a degree of radicalization and highlighting independent features to render them computationally accessible. We utilize domain-specific knowledge resources for each of these contextual dimensions such as Qur'an for religion, the books of extremist ideologues and preachers for political ideology and a social media hate speech corpus for hate. The significant sensitivity of the Islamist extremist ideology and its local and global security implications require reliable algorithms for modelling such communications on Twitter. Our study makes three contributions to reliable analysis: (i) Development of a computational approach rooted in the contextual dimensions of religion, ideology, and hate, which reflects strategies employed by online Islamist extremist groups, (ii) An in-depth analysis of relevant tweet datasets with respect to these dimensions to exclude likely mislabeled users, and (iii) A framework for understanding online radicalization as a process to assist counter-programming. Given the potentially significant social impact, we evaluate the performance of our algorithms to minimize mislabeling, where our context-aware approach outperforms a competitive baseline by 10.2\% in precision, thereby enhancing the potential of such tools for use in human review.},
	number = {CSCW},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Kursuncu, Ugur and Gaur, Manas and Castillo, Carlos and Alambo, Amanuel and Thirunarayan, Krishnaprasad and Shalin, Valerie and Achilov, Dilshod and Arpinar, I. Budak and Sheth, Amit},
	month = nov,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {user modeling, contextual dimensions, islamist extremism, multi-dimensional modeling, radicalization},
}

@inproceedings{tiso_method_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {A method for developing model to text transformations},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167370},
	doi = {10.1145/3167132.3167370},
	abstract = {In the field of business process development, model transformations play a key role, for example for moving from business process models to either code or inputs for simulation systems, as well as to convert models expressed with notation A into equivalent models expressed with notation B. In the literature, many cases of useful transformations of business process models can be found. However, in general each transformation has been developed in an ad-hoc fashion, at a quite low-level, and its quality is often neglected. To ensure the quality of the transformations is important to apply to them all the well-known software engineering principles and practices, from the requirements definition to the testing activities. For this reason, we propose a method, MeDMoT, for developing non-trivial Model to Text Transformations, which prescribes how to: (1) capture and specify the transformation requirements; (2) design the transformation, (3) implement the transformation and (4) test the transformation. The method has been applied in several case studies, including a transformation of UML business processes into inputs for an agent-based simulator.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Tiso, Alessandro and Reggio, Gianna and Leotta, Maurizio and Ricca, Filippo},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {UML, model-driven, model to text transformations},
	pages = {138--141},
}

@article{yoshino_overview_2023,
	title = {Overview of the {Tenth} {Dialog} {System} {Technology} {Challenge}: {DSTC10}},
	volume = {32},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2023.3293030},
	doi = {10.1109/TASLP.2023.3293030},
	abstract = {This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Yoshino, Koichiro and Chen, Yun-Nung and Crook, Paul and Kottur, Satwik and Li, Jinchao and Hedayatnia, Behnam and Moon, Seungwhan and Fei, Zhengcong and Li, Zekang and Zhang, Jinchao and Feng, Yang and Zhou, Jie and Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hakkani-Tur, Dilek and Damavandi, Babak and Geramifard, Alborz and Hori, Chiori and Shah, Ankit and Zhang, Chen and Li, Haizhou and Sedoc, João and D'Haro, Luis F. and Banchs, Rafael and Rudnicky, Alexander},
	month = jul,
	year = {2023},
	note = {Publisher: IEEE Press},
	pages = {765--778},
}

@inproceedings{hoseini_sedar_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {{SEDAR}: {A} {Semantic} {Data} {Reservoir} for {Heterogeneous} {Datasets}},
	isbn = {979-8-4007-0124-5},
	url = {https://doi.org/10.1145/3583780.3614753},
	doi = {10.1145/3583780.3614753},
	abstract = {Data lakes have emerged as a solution for managing vast and diverse datasets for modern data analytics. To prevent them from becoming ungoverned, semantic data management techniques are crucial, which involve connecting metadata with knowledge graphs, following the principles of Linked Data. This semantic layer enables more expressive data management, integration from various sources and enhances data access utilizing the concepts and relations to semantically enrich the data. Some frameworks have been proposed, but requirements like data versioning, linking of datasets, managing machine learning projects, automated semantic modeling and ontology-based data access are not supported in one uniform system. We demonstrate SEDAR, a comprehensive semantic data lake that includes support for data ingestion, storage, processing, and governance with a special focus on semantic data management. The demo will showcase how the system allows for various ingestion scenarios, metadata enrichment, data source linking, profiling, semantic modeling, data integration and processing inside a machine learning life cycle.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Hoseini, Sayed and Ali, Ahmed and Shaker, Haron and Quix, Christoph},
	year = {2023},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {ontology-based data access, semantic modeling, data lake, semantic data lake, semantic data management},
	pages = {5056--5060},
}

@inproceedings{zhou_modeling_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Modeling and {Leveraging} {Analytic} {Focus} {During} {Exploratory} {Visual} {Analysis}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445674},
	doi = {10.1145/3411764.3445674},
	abstract = {Visual analytics systems enable highly interactive exploratory data analysis. Across a range of fields, these technologies have been successfully employed to help users learn from complex data. However, these same exploratory visualization techniques make it easy for users to discover spurious findings. This paper proposes new methods to monitor a user’s analytic focus during visual analysis of structured datasets and use it to surface relevant articles that contextualize the visualized findings. Motivated by interactive analyses of electronic health data, this paper introduces a formal model of analytic focus, a computational approach to dynamically update the focus model at the time of user interaction, and a prototype application that leverages this model to surface relevant medical publications to users during visual analysis of a large corpus of medical records. Evaluation results with 24 users show that the modeling approach has high levels of accuracy and is able to surface highly relevant medical abstracts.},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Zhilan and Wen, Ximing and Wang, Yue and Gotz, David},
	year = {2021},
	note = {event-place: Yokohama, Japan},
	keywords = {Visual Analytics, User Modeling, Analytic Focus, Insight Provenance},
}

@inproceedings{pico-valencia_agent_2019,
	address = {Richland, SC},
	series = {{AAMAS} '19},
	title = {An {Agent} {Model} {Based} on {Open} {Linked} {Data} for {Building} {Internet} of {Agents} {Ecosystems}},
	isbn = {978-1-4503-6309-9},
	abstract = {This paper presents an smart, collaborative and self-adaptive reactive agent model aimed at managing the resources of objects connected to Internet of Things (IoT). This agent model, called Linked Open Agent (LOA), is described using both a semantic agent contract built from descriptors published as linked data, and a workflow for agent control that is completed at runtime by the agent itself to address its behavior. The accuracy for semantic discovering agents partners was evaluated and compared with generic models of discovery such as the Yellow Pages of Java Agent DEvelopment Framework (JADE) and the Java implementation of the Universal Description, Discovery, and Integration (jUDDI). The results demonstrated that our method had a better accuracy for recovering agents than the accuracy of JADE and JUDDI.},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Pico-Valencia, Pablo and Holgado-Terriza, Juan A. and Senso, José},
	year = {2019},
	note = {event-place: Montreal QC, Canada},
	keywords = {internet of things, linked open data, contract, internet of agents},
	pages = {1536--1538},
}

@inproceedings{hatherall_responsible_2023,
	address = {New York, NY, USA},
	series = {{TAS} '23},
	title = {Responsible {Agency} {Through} {Answerability}: {Cultivating} the {Moral} {Ecology} of {Trustworthy} {Autonomous} {Systems}},
	isbn = {979-8-4007-0734-6},
	url = {https://doi.org/10.1145/3597512.3597529},
	doi = {10.1145/3597512.3597529},
	abstract = {The decades-old debate over so-called ‘responsibility gaps’ in intelligent systems has recently been reinvigorated by rapid advances in machine learning techniques that are delivering many of the capabilities of machine autonomy that Matthias [1] originally anticipated. The emerging capabilities of intelligent learning systems highlight and exacerbate existing challenges with meaningful human control of, and accountability for, the actions and effects of such systems. The related challenge of human ‘answerability’ for system actions and harms has come into focus in recent literature on responsibility gaps [2, 3]. We describe a proposed interdisciplinary approach to designing for answerability in autonomous systems, grounded in an instrumentalist framework of ‘responsible agency cultivation’ drawn from moral philosophy and cognitive sciences as well as empirical results from structured interviews and focus groups in the application domains of health, finance and government. We outline a prototype dialogue agent informed by these emerging results and designed to help bridge the structural gaps in organisations that typically impede the human agents responsible for an autonomous sociotechnical system from answering to vulnerable patients of responsibility.},
	booktitle = {Proceedings of the {First} {International} {Symposium} on {Trustworthy} {Autonomous} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hatherall, Louise and Keküllüoğlu, Dilara and Kokciyan, Nadin and Rovatsos, Michael and Sethi, Nayha and Vierkant, Tillmann and Vallor, Shannon},
	year = {2023},
	note = {event-place: Edinburgh, United Kingdom},
	keywords = {Agency, AI ethics, Answerability, Dialogue agents, Responsibility gaps, Sociotechnical Systems Design},
}

@inproceedings{amrani_towards_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {Towards a formal specification of multi-paradigm modelling},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00067},
	doi = {10.1109/MODELS-C.2019.00067},
	abstract = {The notion of a programming paradigm is used to classify programming languages and their accompanying workflows based on their salient features. Similarly, the notion of a modelling paradigm can be used to characterise the plethora of modelling approaches used to engineer complex Cyber-Physical Systems (CPS). Modelling paradigms encompass formalisms, abstractions, workflows and supporting tool(chain)s. A precise definition of this modelling paradigm notion is lacking however. Such a definition will increase insight, will allow for formal reasoning about the consistency of modelling frameworks and may serve as the basis for the construction of new modelling, simulation, verification, synthesis, ... environments to support design of CPS. We present a formal framework aimed at capturing the notion of modelling paradigm, as a first step towards a comprehensive formalisation of multi-paradigm modelling. Our formalisation is illustrated by CookieCAD, a simple Computer-Aided Design paradigm used in the development of cookie stencils.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Amrani, Moussa and Blouin, Dominique and Heinrich, Robert and Rensink, Arend and Vangheluwe, Hans and Wortmann, Andreas},
	year = {2021},
	pages = {419--424},
}

@inproceedings{paulus_plasma_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {The {PLASMA} {Framework}: {Laying} the {Path} to {Domain}-{Specific} {Semantics} in {Dataspaces}},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587662},
	doi = {10.1145/3543873.3587662},
	abstract = {Modern data management is evolving from centralized integration-based solutions to a non-integration-based process of finding, accessing and processing data, as observed within dataspaces. Common reference dataspace architectures assume that sources publish their own domain-specific schema. These schemas, also known as semantic models, can only be partially created automatically and require oversight and refinement by human modellers. Non-expert users, such as mechanical engineers or municipal workers, often have difficulty building models because they are faced with multiple ontologies, classes, and relations, and existing tools are not designed for non-expert users. The PLASMA framework consists of a platform and auxiliary services that focus on providing non-expert users with an accessible way to create and edit semantic models, combining automation approaches and support systems such as a recommendation engine. It also provides data conversion from raw data to RDF. In this paper we highlight the main features, like the modeling interface and the data conversion engine. We discuss how PLASMA as a tool is suitable for building semantic models by non-expert users in the context of dataspaces and show some applications where PLASMA has already been used in data management projects.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Paulus, Alexander and Pomp, André and Meisen, Tobias},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {semantic model, resource description framework, graphical user interface, dataspace, semantic mapping},
	pages = {1474--1479},
}

@article{zhao_biomedical_2021,
	title = {Biomedical {Data} and {Deep} {Learning} {Computational} {Models} for {Predicting} {Compound}-{Protein} {Relations}},
	volume = {19},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2021.3069040},
	doi = {10.1109/TCBB.2021.3069040},
	abstract = {The identification of compound-protein relations (CPRs), which includes compound-protein interactions (CPIs) and compound-protein affinities (CPAs), is critical to drug development. A common method for compound-protein relation identification is the use of \&lt;italic\&gt;in vitro\&lt;/italic\&gt; screening experiments. However, the number of compounds and proteins is massive, and \&lt;italic\&gt;in vitro\&lt;/italic\&gt; screening experiments are labor-intensive, expensive, and time-consuming with high failure rates. Researchers have developed a computational field called virtual screening (VS) to aid experimental drug development. These methods utilize experimentally validated biological interaction information to generate datasets and use the physicochemical and structural properties of compounds and target proteins as input information to train computational prediction models. At present, deep learning has been widely used in computer vision and natural language processing and has experienced epoch-making progress. At the same time, deep learning has also been used in the field of biomedicine widely, and the prediction of CPRs based on deep learning has developed rapidly and has achieved good results. The purpose of this study is to investigate and discuss the latest applications of deep learning techniques in CPR prediction. First, we describe the datasets and feature engineering (i.e., compound and protein representations and descriptors) commonly used in CPR prediction methods. Then, we review and classify recent deep learning approaches in CPR prediction. Next, a comprehensive comparison is performed to demonstrate the prediction performance of representative methods on classical datasets. Finally, we discuss the current state of the field, including the existing challenges and our proposed future directions. We believe that this investigation will provide sufficient references and insight for researchers to understand and develop new deep learning methods to enhance CPR predictions.},
	number = {4},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Zhao, Qichang and Yang, Mengyun and Cheng, Zhongjian and Li, Yaohang and Wang, Jianxin},
	month = mar,
	year = {2021},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {2092--2110},
}

@inproceedings{zhou_grease_2020,
	address = {New York, NY, USA},
	series = {{WSDM} '20},
	title = {{GREASE}: {A} {Generative} {Model} for {Relevance} {Search} over {Knowledge} {Graphs}},
	isbn = {978-1-4503-6822-3},
	url = {https://doi.org/10.1145/3336191.3371772},
	doi = {10.1145/3336191.3371772},
	abstract = {Relevance search is to find top-ranked entities in a knowledge graph (KG) that are relevant to a query entity. Relevance is ambiguous, particularly over a schema-rich KG like DBpedia which supports a wide range of different semantics of relevance based on numerous types of relations and attributes. As users may lack the expertise to formalize the desired semantics, supervised methods have emerged to learn the hidden user-defined relevance from user-provided examples. Along this line, in this paper we propose a novel generative model over KGs for relevance search, named GREASE. The model applies to meta-path based relevance where a meta-path characterizes a particular type of semantics of relating the query entity to answer entities. It is also extended to support properties that constrain answer entities. Extensive experiments on two large-scale KGs demonstrate that GREASE has advanced the state of the art in effectiveness, expressiveness, and efficiency.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Tianshuo and Li, Ziyang and Cheng, Gong and Wang, Jun and Wei, Yu'Ang},
	year = {2020},
	note = {event-place: Houston, TX, USA},
	keywords = {knowledge graph, generative model, meta-path, relevance search},
	pages = {780--788},
}

@inproceedings{yu_research_2023,
	address = {New York, NY, USA},
	series = {{ICCAI} '23},
	title = {Research on enterprise text classification methods of {BiLSTM} and {CNN} based on {BERT}},
	isbn = {978-1-4503-9902-9},
	url = {https://doi.org/10.1145/3594315.3594362},
	doi = {10.1145/3594315.3594362},
	abstract = {The traditional enterprise text data classification method ignores the context of the text. Each word is independent from each other and cannot represent semantic information. The text description and classification effect is poor, and the feature engineering needs human intervention, so the generalization ability is not strong. In view of the low efficiency and accuracy of enterprise text data classification, this paper proposes a bidirectional encoder representation based on Transformer (BERT) The enterprise text classification model BBLC-ATT based on convolutional neural networks (CNN) and bi-directional long short-term memory (BiLSTM) neural networks and attention mechanism (Attention). The model uses BERT training word vector and combines the features of CNN and BiLSTM to capture local potential features and context information. Secondly, the feature vectors extracted from the hybrid network layer are input into the self-attention layer to extract the syntactic and semantic features between words in the enterprise text sentences. Finally, this paper compares BBLC-ATT model with traditional deep learning model in terms of accuracy, accuracy, recall and F1 value. The experimental results show that the BBLC-ATT model is superior to other models in all evaluation indicators, and the accuracy rate is increased by 3.28\% - 15.86\%.},
	booktitle = {Proceedings of the 2023 9th {International} {Conference} on {Computing} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Yang and Liu, Xiangzhi},
	year = {2023},
	note = {event-place: Tianjin, China},
	keywords = {Natural language processing, BERT model, BiLSTM model, Attention model, CNN model, Text content classification},
	pages = {491--495},
}

@article{acharya_multi-factored_2020,
	title = {Multi-{Factored} {Gene}-{Gene} {Proximity} {Measures} {Exploiting} {Biological} {Knowledge} {Extracted} from {Gene} {Ontology}: {Application} in {Gene} {Clustering}},
	volume = {17},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2018.2849362},
	doi = {10.1109/TCBB.2018.2849362},
	abstract = {To describe the cellular functions of proteins and genes, a potential dynamic vocabulary is Gene Ontology (GO), which comprises of three sub-ontologies namely, Biological-process, Cellular-component, and Molecular-function. It has several applications in the field of bioinformatics like annotating/measuring gene-gene or protein-protein semantic similarity, identifying genes/proteins by their GO annotations for disease gene and target discovery, etc. To determine semantic similarity between genes, several semantic measures have been proposed in literature, which involve information content of \&lt;italic\&gt;GO-terms\&lt;/italic\&gt;, GO tree structure, or the combination of both. But, most of the existing semantic similarity measures do not consider different topological and information theoretic aspects of \&lt;italic\&gt;GO-terms\&lt;/italic\&gt; collectively. Inspired by this fact, in this article, we have first proposed three novel semantic similarity/distance measures for genes covering different aspects of GO-tree. These are further implanted in the frameworks of well-known multi-objective and single-objective based clustering algorithms to determine functionally similar genes. For comparative analysis, 10 popular existing GO based semantic similarity/distance measures and tools are also considered. Experimental results on \&lt;italic\&gt;Mouse genome\&lt;/italic\&gt;, \&lt;italic\&gt;Yeast\&lt;/italic\&gt;, and \&lt;italic\&gt;Human genome\&lt;/italic\&gt; datasets evidently demonstrate the supremacy of multi-objective clustering algorithms in association with proposed multi-factored similarity/distance measures. Clustering outcomes are further validated by conducting some biological/statistical significance tests. Supplementary information is available at \&lt;uri\&gt;https://www.iitp.ac.in/sriparna/journals.html\&lt;/uri\&gt;.},
	number = {1},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Acharya, Sudipta and Saha, Sriparna and Pradhan, Prasanna},
	month = feb,
	year = {2020},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {207--219},
}

@inproceedings{azarm_ontology_2018,
	address = {New York, NY, USA},
	series = {{SEHS} '18},
	title = {An ontology for a patient-centric healthcare interoperability framework},
	isbn = {978-1-4503-5734-0},
	url = {https://doi.org/10.1145/3194696.3194706},
	doi = {10.1145/3194696.3194706},
	abstract = {Healthcare clients are increasingly interested to be involved and informed of their healthcare delivery and status [1] [2]. They need to be able to access, view, and analyze their health data easily and securely. Clients need one single gateway to their medical records. Some healthcare providers are creating portals for their clients to flow some data for them to view [3]. In addition, clients can request a portion of their health data in paper format from their healthcare providers by filling in forms and manually submitting their requests. But, this is not sufficient for the average healthcare client. There is a need for a platform independent tool that can automatically gather and combine a client's health information from the various providers in their circle of care and provide the information securely and electronically without inconveniencing the client with multiple requests and sharing agreements [4]. Healthcare providers can also benefit from such a tool in the sense of gaining insights from their colleagues' efforts automatically and without starting a separate quest for each piece of information. In this paper we propose framework and toolset that can provide a secure single point of access to a client's full picture of their personal health information. In particular, we delve into one of the key components of our framework which is our proposed ontology.},
	booktitle = {Proceedings of the {International} {Workshop} on {Software} {Engineering} in {Healthcare} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Azarm, Mana and Peyton, Liam},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {healthcare API, healthcare applications, healthcare integration, healthcare ontology, patient centric},
	pages = {34--41},
}

@inproceedings{fujino_extracting_2020,
	address = {New York, NY, USA},
	series = {{ICBDR} '19},
	title = {Extracting 4-{Attributes} {Vessel} {Courses} from {AIS} {Data} with {PQK}-{Means} and {Topic} {Model}},
	isbn = {978-1-4503-7201-5},
	url = {https://doi.org/10.1145/3372454.3372465},
	doi = {10.1145/3372454.3372465},
	abstract = {AIS (Automatic Identification System) data received from moving vessels over an area of interest can be of very much interest for deriving maritime trajectory patterns. In this paper, a novel approach to extract course patterns from AIS data of vessels is presented. From machine learning and natural language processing principles, a topic model might be used for extracting implicit patterns underlying massive and unstructured collection of incoming data. To apply topic model to AIS data, PQk-means vector quantization to convert AIS data record to code documents is introduced. Then, a topic model is applied to extract course patterns from AIS data. In fact, courses, not only encompasses trajectory locations, but also headings and speeds, are recognized by the proposed algorithm. The performance of PQk-means is evaluated using the relative root mean square error and elapsed time. The potential of the approach is illustrated by a series of experimental results derived from practical AIS data set in a region of North West France.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Big} {Data} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Fujino, Iwao and Claramunt, Christophe and Boudraa, Abdel-Ouahab},
	year = {2020},
	note = {event-place: Cergy-Pontoise, France},
	keywords = {Automatic Identification System (AIS), Course Pattern Extraction, Maritime Big Data, PQk-means, Topic Model, Vector Quantization},
	pages = {129--135},
}

@inproceedings{li_reciptor_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {{RECIPTOR}: {An} {Effective} {Pretrained} {Model} for {Recipe} {Representation} {Learning}},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3403223},
	doi = {10.1145/3394486.3403223},
	abstract = {Recipe representation plays an important role in food computing for perception, recognition, recommendation and other applications. Learning pretrained recipe embeddings is a challenging task, as there is a lack of high quality annotated food datasets. In this paper, we provide a joint approach for learning effective pretrained recipe embeddings using both the ingredients and cooking instructions. We present RECIPTOR, a novel set transformer-based joint model to learn recipe representations, that preserves permutation-invariance for the ingredient set and uses a novel knowledge graph (KG) derived triplet sampling approach to optimize the learned embeddings so that related recipes are closer in the latent semantic space. The embeddings are further jointly optimized by combining similarity among cooking instructions with a KG based triplet loss. We experimentally show that RECIPTOR's recipe embeddings outperform state-of-the-art baselines on two newly designed downstream classification tasks by a wide margin.},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \&amp; {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Li, Diya and Zaki, Mohammed J.},
	year = {2020},
	note = {event-place: Virtual Event, CA, USA},
	keywords = {representation learning, food computing, food knowledge graph, recipe embedding, set transformer},
	pages = {1719--1727},
}

@inproceedings{hassane_model_2020,
	address = {New York, NY, USA},
	series = {{SAM} '20},
	title = {A {Model} {Traceability} {Framework} for {Network} {Service} {Management}},
	isbn = {978-1-4503-8140-6},
	url = {https://doi.org/10.1145/3419804.3420267},
	doi = {10.1145/3419804.3420267},
	abstract = {Automating enactment along with traceability management of processes using model-driven engineering methods could be of significant benefit to the Network Functions Virtualization (NFV) paradigm in view of its move towards zero-touch automation of the orchestration and management of network services (NS). Earlier, we proposed an integrated process modelling and enactment environment with traceability support, MAPLE-T, for NS management. In this paper, we extend MAPLE-T with the notion of intents. We propose the usage of intents at both the process model (PM) and model-transformation levels as part of our traceability information. We define intents as information representing the objective of the PM actions/activities and their implementations. We extend MAPLE-T with traceability visualization support to visualize trace links relating models at different levels through the captured intents. The intent-enriched traceability information and the enhanced visualization enable semantically richer traceability analysis. We apply our traceability generation and analysis approach to the NS design process in order to show the benefits of intents not only for the process, but also for the whole NS lifecycle management operations.},
	booktitle = {Proceedings of the 12th {System} {Analysis} and {Modelling} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Hassane, Omar and Mustafiz, Sadaf and Khendek, Ferhat and Toeroe, Maria},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {megamodelling, traceability, model transformation, process modelling},
	pages = {64--73},
}

@inproceedings{damm_exploiting_2018,
	address = {New York, NY, USA},
	series = {{SEFAIS} '18},
	title = {Exploiting learning and scenario-based specification languages for the verification and validation of highly automated driving},
	isbn = {978-1-4503-5739-5},
	url = {https://doi.org/10.1145/3194085.3194086},
	doi = {10.1145/3194085.3194086},
	abstract = {We propose a series of methods based on learning key structural properties from traffic data-basis and on statistical model checking, ultimately leading to the construction of a scenario catalogue capturing requirements for controlling criticality for highly autonomous vehicles. We sketch underlying mathematical foundations which allow to derive formal confidence levels that vehicles tested by such a scenario catalogue will maintain the required control of criticality in real traffic matching the probability distributions of key parameters of data recorded in the reference data base employed for this process.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Software} {Engineering} for {AI} in {Autonomous} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Damm, Werner and Galbas, Roland},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {learning, verification and validation, formal specification, highly automated driving, requirement analysis, statistical model-checking},
	pages = {39--46},
}

@article{liebeskind_machine_2024,
	title = {Machine {Translation} for {Historical} {Research}: {A} {Case} {Study} of {Aramaic}-{Ancient} {Hebrew} {Translations}},
	volume = {17},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3627168},
	doi = {10.1145/3627168},
	abstract = {In this article, by the ability to translate Aramaic to another spoken languages, we investigated machine translation in a cultural heritage domain for two primary purposes: evaluating the quality of ancient translations and preserving Aramaic (an endangered language). First, we detailed the construction of a publicly available Biblical parallel Aramaic-Hebrew corpus based on two ancient (early 2nd to late 4th century) Hebrew-Aramaic translations: Targum Onkelus and Targum Jonathan. Then using the statistical machine translation approach, which in our use case significantly outperforms neural machine translation, we validated the excepted high quality of the translations. The trained model failed to translate Aramaic texts of other dialects. However, when we trained the same statistical machine translation model on another Aramaic-Hebrew corpus of a different dialect (Zohar, 13th century), a very high translation score was achieved. We examined an additional important cultural heritage source of Aramaic texts, the Babylonian Talmud (early 3rd to late 5th century). Since we do not have a parallel Aramaic-Hebrew corpus of the Talmud, we used the model trained on the Bible corpus for translation. We performed an analysis of the results and suggest some potential promising future research.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Liebeskind, Chaya and Liebeskind, Shmuel and Bouhnik, Dan},
	month = feb,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {neural machine translation, low-resource languages, Aramaic-Hebrew, Bible translation, statistical machine translation},
}

@inproceedings{wagner_process_2019,
	address = {San Diego, CA, USA},
	series = {{SummerSim} '19},
	title = {Process design modeling with extended event graphs},
	abstract = {Schruben's Event Graphs (EGs), defining the event types of a simulation model and event scheduling arrows between them, representing causal regularities, provide an elegant visual modeling language and formalism for event-based simulation, which can be viewed as the most fundamental Discrete Event Simulation (DES) approach. We show how to extend and visually improve the language of EGs by adding elements of the Business Process Modeling Notation (BPMN): (1) mini diamonds for designating conditional control flow arrows, (2) Gateways for conditional and parallel branching, (3) typed Data Objects for accommodating object-oriented (OO) state structure modeling, and (4) Activities. The resulting extension of EGs, called Discrete Event Process Modeling Notation (DPMN), is more expressive and visually more clear than traditional EGs, and its visual syntax is harmonized with BPMN process diagrams, thus building a bridge between the DES and the Business Process Management research communities.},
	booktitle = {Proceedings of the 2019 {Summer} {Simulation} {Conference}},
	publisher = {Society for Computer Simulation International},
	author = {Wagner, Gerd},
	year = {2019},
	note = {event-place: Berlin, Germany},
	keywords = {BPMN, discrete event simulation, DPMN, event graphs, object event simulation},
}

@article{denisenko_psycholinguistics-inspired_2024,
	title = {A {Psycholinguistics}-inspired {Method} to {Counter} {IP} {Theft} {Using} {Fake} {Documents}},
	volume = {15},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3651313},
	doi = {10.1145/3651313},
	abstract = {Intellectual property (IP) theft is a growing problem. We build on prior work to deter IP theft by generating n fake versions of a technical document so a thief has to expend time and effort in identifying the correct document. Our new SbFAKE framework proposes, for the first time, a novel combination of language processing, optimization, and the psycholinguistic concept of surprisal to generate a set of such fakes. We start by combining psycholinguistic-based surprisal scores and optimization to generate two bilevel surprisal optimization problems (an Explicit one and a simpler Implicit one) whose solutions correspond directly to the desired set of fakes. As bilevel problems are usually hard to solve, we then show that these two bilevel surprisal optimization problems can each be reduced to equivalent surprisal-based linear programs. We performed detailed parameter tuning experiments and identified the best parameters for each of these algorithms. We then tested these two variants of SbFAKE (with their best parameter settings) against the best performing prior work in the field. Our experiments show that SbFAKE is able to more effectively generate convincing fakes than past work. In addition, we show that replacing words in an original document with words having similar surprisal scores generates greater levels of deception.},
	number = {2},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Denisenko, Natalia and Zhang, Youzhi and Pulice, Chiara and Bhattasali, Shohini and Jajodia, Sushil and Resnik, Philip and Subrahmanian, V.S.},
	month = jun,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {AI for security, fake document generation},
}

@inproceedings{todorov_beyondfacts_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{BeyondFacts} 2025: 5th {International} {Workshop} on {Computational} {Methods} for {Online} {Discourse} {Analysis}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3717540},
	doi = {10.1145/3701716.3717540},
	abstract = {This workshop explores the intersection of computational and interdisciplinary approaches to analyzing online discourse, including claims, arguments, and opinions on controversial topics. With the rise of mis- and disinformation, bias, and echo chambers, NLP-based methods such as argument mining, stance detection, and fact verification have become essential. However, these tasks require robust conceptual foundations across fields like communication studies, computational linguistics, and computer science. BeyondFacts fosters collaboration among diverse research communities-including social sciences, political science, computational journalism, and computer science-to enhance machine-interpretation and analysis of societal debates using techniques from Web mining, AI, and NLP.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Todorov, Konstantin and Fafalios, Pavlos and Dietze, Stefan and Dimitrov, Dimitar},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {knowledge graphs, llms, computational journalism, computational fact-checking, intent detection, online discourse analysis, mis- and dis-information spread and detection, social media mining, stance viewpoint discovery, web mining},
	pages = {2612--2615},
}

@article{wei_more_2024,
	title = {More {Than} {Syntaxes}: {Investigating} {Semantics} to {Zero}-shot {Cross}-lingual {Relation} {Extraction} and {Event} {Argument} {Role} {Labelling}},
	volume = {23},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3582261},
	doi = {10.1145/3582261},
	abstract = {Syntactic dependency structures are commonly utilized as language-agnostic features to solve the word order difference issues in zero-shot cross-lingual relation and event extraction tasks. However, while sentences in multiple forms can be employed to express the same meaning, the syntactic structure may vary considerably in specific scenarios. To fix this problem, we find semantics are rarely considered, which could provide a more consistent semantic analysis of sentences and be served as another bridge between different languages. Therefore, in this article, we introduce Syntax and Semantic Driven Network (SSDN) to equip syntax and semantic knowledge across languages simultaneously. Specifically, predicate–argument structures from semantic role labelling are explicitly incorporated into word representations. Then, a semantic-aware relational graph convolutional network and a transformer-based encoder are utilized to model both semantic dependency and syntactic dependency structures, respectively. Finally, a fusion module is introduced to integrate output representations adaptively. We conduct experiments on the widely used Automatic Content Extraction 2005 English, Chinese, and Arabic datasets. The evaluation results demonstrate that the proposed method achieves the state-of-the-art performance. Further study also indicates SSDN could produce robust representations that facilitate the transfer operations across languages.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Wei, Kaiwen and Jin, Li and Zhang, Zequn and Guo, Zhi and Li, Xiaoyu and Liu, Qing and Feng, Weimiao},
	month = may,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {semantic parsing, Cross-lingual relation and event extraction, relational graph convolutional network, zero-resource transfer},
}

@inproceedings{lu_hiprompt_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {{HiPrompt}: {Few}-{Shot} {Biomedical} {Knowledge} {Fusion} via {Hierarchy}-{Oriented} {Prompting}},
	isbn = {978-1-4503-9408-6},
	url = {https://doi.org/10.1145/3539618.3591997},
	doi = {10.1145/3539618.3591997},
	abstract = {Medical decision-making processes can be enhanced by comprehensive biomedical knowledge bases, which require fusing knowledge graphs constructed from different sources via a uniform index system. The index system often organizes biomedical terms in a hierarchy to provide the aligned entities with fine-grained granularity. To address the challenge of scarce supervision in the biomedical knowledge fusion (BKF) task, researchers have proposed various unsupervised methods. However, these methods heavily rely on ad-hoc lexical and structural matching algorithms, which fail to capture the rich semantics conveyed by biomedical entities and terms. Recently, neural embedding models have proved effective in semantic-rich tasks, but they rely on sufficient labeled data to be adequately trained. To bridge the gap between the scarce-labeled BKF and neural embedding models, we propose HiPrompt, a supervision-efficient knowledge fusion framework that elicits the few-shot reasoning ability of large language models through hierarchy-oriented prompts. Empirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the effectiveness of HiPrompt.},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Lu, Jiaying and Shen, Jiaming and Xiong, Bo and Ma, Wenjing and Staab, Steffen and Yang, Carl},
	year = {2023},
	note = {event-place: Taipei, Taiwan},
	keywords = {few-shot prompting, large language models for resource-constrained field, re-rank, retrieve \&amp, biomedical knowledge fusion},
	pages = {2052--2056},
}

@inproceedings{liu_hisdom_2019,
	address = {New York, NY, USA},
	series = {{BDCAT} '19},
	title = {{HISDOM}: {A} {Hybrid} {Ontology} {Mapping} {System} based on {Convolutional} {Neural} {Network} and {Dynamic} {Weight}},
	isbn = {978-1-4503-7016-5},
	url = {https://doi.org/10.1145/3365109.3368779},
	doi = {10.1145/3365109.3368779},
	abstract = {In the information explosion era, mapping multiple ontologies in different knowledge bases could provide a common layer from which several ontologies could be accessed and exchanged in semantically sound manners. However, most ontology mapping approaches rely heavily on human annotation and are restricted to limited domains. Moreover, most prior work is limited to combining a few mapping factors with moderate mapping accuracy, and the ontology mapping weigh calculation process is not adaptive and dynamic. Based on the current generalization of ontology mapping methods, we propose a novel ontology mapping system called HISDOM to improve the generalization performance of ontology mapping. HISDOM uses comprehensive factors like concept names, attributes, instances, and structural similarities to determine the similarity of ontology. A key novelty of HISDOM is leveraging CNN to calculate the comment similarity to assist the mapping process of concepts in the ontology. Then, HISDOM dynamically derives the weight of different factors in the overall ontology similarity proportional to the amount of information of each factor in the ontology. Finally, based on the overall similarity, HISDOM determines whether the two concepts in the ontology can be mapped. We study the performance of HISDOM through extensive experiments. The results show that HISDOM outperforms several baselines in ontology mapping tasks, the dynamic and adaptive weighting mechanism is effective, and all the mapping factors of HISDOM are positive towards improving the ontology mapping accuracy.},
	booktitle = {Proceedings of the 6th {IEEE}/{ACM} {International} {Conference} on {Big} {Data} {Computing}, {Applications} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Jie and Tang, Yan and Xu, Xinyi},
	year = {2019},
	note = {event-place: Auckland, New Zealand},
	keywords = {ontology mapping, convolutional neural network, dynamic weight, hybrid similarity},
	pages = {67--70},
}

@inproceedings{huang_semantic-aware_2024,
	address = {New York, NY, USA},
	series = {{HPDC} '24},
	title = {Semantic-{Aware} {Log} {Understanding} and {Analysis}},
	isbn = {979-8-4007-0413-0},
	url = {https://doi.org/10.1145/3625549.3658830},
	doi = {10.1145/3625549.3658830},
	abstract = {The exponential growth in system complexity and the corresponding surge in log data volume necessitate advanced log analysis techniques for efficient system management and anomaly detection. Traditional log understanding and analysis methods often fail to capture the rich semantic context inherent in log messages, leading to suboptimal monitoring and diagnostic capabilities. This paper aims to bridge the semantic gap by integrating cutting-edge semantic technologies into the log analysis pipeline. We leverage natural language processing, information retrieval, and large language models to enrich log data with semantic information, facilitating a deeper understanding of log messages. Our methodology enhances anomaly detection accuracy by utilizing hierarchical contextual information and pre-training technology, and refining log-based QA processes by log retrieval and log reader. Preliminary results demonstrate a significant improvement in identifying and diagnosing system anomalies, as well as in the automated answering log questions. This research not only presents a breakthrough in log data analysis but also sets the stage for future advancements in intelligent system monitoring and proactive fault resolution. Through this semantic-aware approach, we envision a new paradigm in log analysis that transcends traditional machine learning methods, offering a more robust and intuitive understanding of system behaviors and states.},
	booktitle = {Proceedings of the 33rd {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Shaohan and Luan, Zhongzhi},
	year = {2024},
	note = {event-place: Pisa, Italy},
	keywords = {natural language processing, anomaly detection, log parsing, log understanding, semantic-aware analysis},
	pages = {413--416},
}

@inproceedings{wagner_business_2021,
	address = {Orlando, Florida},
	series = {{WSC} '20},
	title = {Business process modeling and simulation with {DPMN}: resource-constrained activities},
	isbn = {978-1-7281-9499-8},
	abstract = {This tutorial article, which is extracted from (Wagner 2019), shows how to use UML Class Diagrams and Discrete Event Process Modeling Notation (DPMN) Process Diagrams for making simulation models of business processes with resource-constrained activities based on the DES paradigm of Object Event Modeling and Simulation. In this approach, the state structure of a business system is captured by a UML Class Diagram, which defines the types of objects, events and activities underlying a DPMN Process Diagram, which captures the causal regularities of the system in the form of a set of event rules. DPMN Process Diagrams extend the Event Graphs proposed by Schruben (1983) by adding elements from the Business Process Modeling Notation (BPMN), viz. data objects and activities, and, as its main innovation over BPMN, resource-dependent activity start arrows.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Wagner, Gerd},
	year = {2021},
	pages = {45--59},
}

@inproceedings{raich_spatial_2021,
	address = {New York, NY, USA},
	series = {{ICVISP} 2020},
	title = {Spatial {Extension} {Model} for {Multimodal} {Traffic} {Management}},
	isbn = {978-1-4503-8953-2},
	url = {https://doi.org/10.1145/3448823.3448854},
	doi = {10.1145/3448823.3448854},
	abstract = {The management of traffic information is an essential component of Intelligent Transport Systems (ITS). This traffic is no longer confined to roads but is increasingly emerging into the airspace. Recent developments in the field of unmanned aerial systems (UAS) enable new ways of transportation. Furthermore, the sensor systems of these traffic participants, such as cars, drones, and also intelligent infrastructure, are becoming increasingly prominent and powerful. Thus, future ITS will be confronted with the challenge of ever-increasing amounts of data from a wide variety of different traffic participants. Based on this, a novel concept is presented, how data from multi-modal traffic users can be accumulated. To accomplish this, a parametrized geographic data-centric model is presented that can map traffic routes of arbitrary shape in 2- and 3D environments. By this, traffic management of multimodal vehicles (cars, UAS, etc.) can be represented by one unique multimodal model. Furthermore, the parametrized data model allows situational data processing (e.g. congestion, weather condition, etc.) on a local and global basis. The geographic model extends Geo JSON as its foundation in order to rely on well-established standards.},
	booktitle = {Proceedings of the 2020 4th {International} {Conference} on {Vision}, {Image} and {Signal} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Raich, Krispin and Kathrein, Robert and Erharter, Michael and Döller, Mario},
	year = {2021},
	note = {event-place: Bangkok, Thailand},
	keywords = {3D corridor, air traffic management, drone corridor, intelligent transportation systems, UAS paths},
}

@inproceedings{li_automated_2021,
	address = {Orlando, Florida},
	series = {{WSC} '20},
	title = {Automated abstraction of operation processes from unstructured text for simulation modeling},
	isbn = {978-1-7281-9499-8},
	abstract = {Abstraction of operation processes is a fundamental step for simulation modeling. To reliably abstract an operation process, modelers rely on text information to study and understand details of operations. Aiming at reducing modelers' interpretation load and ensuring the reliability of the abstracted information, this research proposes a systematic methodology to automate the abstraction of operation processes. The methodology applies rule-based information extraction to automatically extract operation process-related information from unstructured text and creates graphical representations of operation processes using the extracted information. To demonstrate the applicability and feasibility of the proposed methodology, a text description of an earthmoving operation is used to create its corresponding graphical representation. Overall, this research enhances the state-of-the-art simulation modeling through achieving automated abstraction of operation processes, which largely reduces modelers' interpretation load and ensures the reliability of the abstracted operation processes.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Li, Yitong and Ji, Wenying and AbouRizk, Simaan M.},
	year = {2021},
	pages = {2517--2525},
}

@inproceedings{bertalan_using_2019,
	address = {New York, NY, USA},
	series = {{WebMedia} '19},
	title = {Using topic modeling to find main discussion topics in brazilian political websites},
	isbn = {978-1-4503-6763-9},
	url = {https://doi.org/10.1145/3323503.3360644},
	doi = {10.1145/3323503.3360644},
	abstract = {Knowing the main discussion topics debated by the general public is a valuable asset to politicians and professionals involved with politics. Lately, alternative media websites became popular venues in which political ideas are debated without the influence of mainstream media. In this article, we propose the construction of a topic modeling framework, using LSI, LDA, and HDP, to identify main discussion issues in political websites. Experiments show that these models presented results similar to state of the art, offering a viable solution to track political discourse in left-wing and right-wing websites.},
	booktitle = {Proceedings of the 25th {Brazillian} {Symposium} on {Multimedia} and the {Web}},
	publisher = {Association for Computing Machinery},
	author = {Bertalan, Vithor Gomes and Ruiz, Evandro Eduardo Seron},
	year = {2019},
	note = {event-place: Rio de Janeiro, Brazil},
	keywords = {topic modeling, hierarchial dirichlet process, latent dirichlet allocation, latent semantic indexing, political texts, topic coherence},
	pages = {245--248},
}

@inproceedings{wilsdorf_conceptual_2021,
	address = {Orlando, Florida},
	series = {{WSC} '20},
	title = {Conceptual models in simulation studies: making it explicit},
	isbn = {978-1-7281-9499-8},
	abstract = {Conceptual models play an important role in conducting simulation studies. A formal or at least explicit specification of conceptual models is key for effectively exploiting them during simulation studies and thereafter, for interpreting and reusing the simulation results. However, the perception of conceptual models varies strongly and with it possible means for specification. A broad definition of the conceptual model, i.e., as a loose collection of early-stage products of the simulation study, holds the potential to unify existing definitions, but also poses specific challenges for specification. To approach these challenges, without claiming to be exhaustive, we identify a set of products, which includes research question, data, and requirements, and define relations and properties of these products. Based on a cell biological case study and a prototypical implementation, we show how the formal structuring of the conceptual model assists in building a simulation model.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Wilsdorf, Pia and Haack, Fiete and Uhrmacher, Adelinde M.},
	year = {2021},
	pages = {2353--2364},
}

@article{chen_expressing_2025,
	title = {Expressing the {Needs} in {Smart} {Home}: {What} {Is} the {End} {Users}’ {Favorite} {Way}},
	volume = {32},
	issn = {1073-0516},
	url = {https://doi.org/10.1145/3715114},
	doi = {10.1145/3715114},
	abstract = {The Internet of Things (IoT) has witnessed remarkable advancements, enabling smart homes with user-centric features. To effectively articulate their personalized needs, it becomes crucial to equip end users with programming capabilities. Currently, the executable Trigger-Action Programming (TAP) rules have become the mainstream paradigm for IoT end-user programming. To simplify the creation of TAP rules, many studies have proposed various levels of requirements abstraction, yet the connections between them remain unclear. In this article, we employ a mixed-methods study to identify the preferred way of expressing end users’ requirements in practical scenarios. Subsequently, from the perspective of requirements engineering, we categorize the needs of smart home into three hierarchical levels of abstraction. Accordingly, we propose an innovative multi-level requirements description language called SH-RDL. We also address potential challenges and conduct an evaluation to validate SH-RDL’s usability, understandability and error-prevention. This will aid in the broader adoption of IoT end-user programming.},
	number = {2},
	journal = {ACM Trans. Comput.-Hum. Interact.},
	author = {Chen, Xiaohong and Chen, Shi and Jin, Zhi and Bian, Han and Chen, Zihan and Li, Haotian},
	month = apr,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Requirements Engineering, IoT End-User Programming, Requirements Description Language, Smart Homes, User Intentions},
}

@inproceedings{baader_patient_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {Patient {Selection} for {Clinical} {Trials} {Using} {Temporalized} {Ontology}-{Mediated} {Query} {Answering}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3191538},
	doi = {10.1145/3184558.3191538},
	abstract = {Finding suitable candidates for clinical trials is a labor-intensive task that requires expert medical knowledge. Our goal is to design (semi-)automated techniques that can support clinical researchers in this task. We investigate the issues involved in designing formal query languages for selecting patients that are eligible for a given clinical trial, leveraging existing ontology-based query answering techniques. In particular, we propose to use a temporal extension of existing approaches for accessing data through ontologies written in Description Logics. We sketch how such a query answering system could work and show that eligibility criteria and patient data can be adequately modeled in our formalism.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Baader, Franz and Borgwardt, Stefan and Forkel, Walter},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {clinical trials, patient cohort recruitment, temporal description logic},
	pages = {1069--1074},
}

@inproceedings{barth_systematization_2020,
	address = {New York, NY, USA},
	series = {{ICISS} '20},
	title = {Systematization of {Digital} {Twins}: {Ontology} and {Conceptual} {Framework}},
	isbn = {978-1-4503-7725-6},
	url = {https://doi.org/10.1145/3388176.3388209},
	doi = {10.1145/3388176.3388209},
	abstract = {The development and progress in information and communication technologies will transform traditional products into smart products and allow to offer novel smart services [1]. Herein, the digital twin (DT) concept is regarded as a key technology to create value with smart services [2]. Although the research and applications of DTs emerge continuously many concerns are to be scrutinized [3]. The lack of a shared conceptual framework for DTs with an unambiguous terminology [4] complicates cross-functional discussions. Therefore, a systematization of the main dimensions of DTs is proposed in the form of an ontology and a conceptual framework thereof derived. The research questions addressed in this paper are a) «Which dimensions are used to classify and structure DTs in academic literature?», b) «What are the fundamental differences or specifications within these dimensions?» and c) «How do these different specifications relate to each other?» The focus of the research is on the objective to find classification systematics that are a) representing the entire spectrum of DTs, b) universally valid in all DT related domains and c) applicable in research and practice. A systematic literature review on the relevant aspects of DTs was conducted and the findings iteratively advanced within workshop sessions with academic experts. DTs are considered as integrators of physical and digital worlds as well as internal and external value creation. Further, the creation of DTs requires per definition the use of digital data. Hence, the proposed ontology and conceptual framework for DTs include the following main dimensions to consider for every DT: Data resources, external value creation and internal value creation. The main subdimensions of the data resources are the data sources to obtain the data, the data categories and the data formats. The main subdimension of the external value creation are the attributes of the services as the basis of the value propositions, the level of smartness of the connected products and the actors on the different levels of the ecosystem. The main subdimensions of the internal value creation are the lifecycle phases of products, the product management levels and the different generations of both. The proposed ontology and conceptual framework support researchers and practitioners in positioning and structuring their intended DT activities and communicating them to internal and external stakeholders. The holistic view on the data resource dimension further allows to easily deduct the needed data for certain applications or deduct possible applications from already available data.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Information} {Science} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Barth, Linard and Ehrat, Matthias and Fuchs, Rainer and Haarmann, Jens},
	year = {2020},
	note = {event-place: Cambridge, United Kingdom},
	keywords = {ontology, Digital Twin, conceptual framework, systematization},
	pages = {13--23},
}

@inproceedings{martin_enhancing_2018,
	address = {New York, NY, USA},
	series = {{UMAP} '18},
	title = {Enhancing {Session}-{Based} {Recommendations} through {Sequential} {Modeling}},
	isbn = {978-1-4503-5589-6},
	url = {https://doi.org/10.1145/3209219.3209259},
	doi = {10.1145/3209219.3209259},
	abstract = {Recommender systems typically determine the items they should recommend by learning models of user-preferences. Most often, those preferences are modeled as static and independent of context. In real life however, users consider items in sequence: TV series are watched episode by episode and accessories are chosen after the main appliance. Unfortunately, since sequences are more complex to model, they are often not taken into account. We developed an efficient sequence-modeling approach based on Bayesian Variable-order Markov Models and combined it with an existing content-based system, the Ontology Filtering. We tested this approach through live evaluations on two e-commerce sites. It dramatically increased performance, more than doubling the CTR and strongly increasing recommendation-mediated sales. These tests also confirm that the technique works efficiently and reliably in a production setting.},
	booktitle = {Proceedings of the 26th {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Martin, Stéphane and Faltings, Boi and Schickel, Vincent},
	year = {2018},
	note = {event-place: Singapore, Singapore},
	keywords = {e-commerce, recommender systems, context-tree, sequence-modeling, variable-order Markov model},
	pages = {359--360},
}

@article{mousavi_developing_2021,
	title = {Developing the {Persian} {Wordnet} of {Verbs} {Using} {Supervised} {Learning}},
	volume = {20},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3450969},
	doi = {10.1145/3450969},
	abstract = {Nowadays, wordnets are extensively used as a major resource in natural language processing and information retrieval tasks. Therefore, the accuracy of wordnets has a direct influence on the performance of the involved applications. This paper presents a fully-automated method for extending a previously developed Persian wordnet to cover more comprehensive and accurate verbal entries. At first, by using a bilingual dictionary, some Persian verbs are linked to Princeton WordNet synsets. A feature set related to the semantic behavior of compound verbs as the majority of Persian verbs is proposed. This feature set is employed in a supervised classification system to select the proper links for inclusion in the wordnet. We also benefit from a pre-existing Persian wordnet, FarsNet, and a similarity-based method to produce a training set. This is the largest automatically developed Persian wordnet with more than 27,000 words, 28,000 PWN synsets and 67,000 word-sense pairs that substantially outperforms the previous Persian wordnet with about 16,000 words, 22,000 PWN synsets and 38,000 word-sense pairs.},
	number = {4},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Mousavi, Zahra and Faili, Heshaam},
	month = may,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology, Persian language, wordnet, verbs},
}

@inproceedings{huang_analyzing_2024,
	address = {New York, NY, USA},
	series = {{IMC} '24},
	title = {Analyzing {Corporate} {Privacy} {Policies} using {AI} {Chatbots}},
	isbn = {979-8-4007-0592-2},
	url = {https://doi.org/10.1145/3646547.3689015},
	doi = {10.1145/3646547.3689015},
	abstract = {In this paper, we present and evaluate an automated pipeline for the large-scale analysis of corporate privacy policies. Organizations usually develop their privacy policies in isolation to best balance their business needs, user rights, as well as regulatory requirements. A wide-ranging and structured analysis of corporate privacy policies is essential to facilitate a deeper understanding of how organizations have balanced competing requirements. Our approach consists of a web crawler that can navigate to and scrape content from web pages that contain privacy policies, and a set of AI chatbot task prompts to process and extract structured/labeled annotations from the raw data. The analysis includes the types of collected user data, the purposes for which data is collected and processed, data retention and protection practices, and user rights and choices. Our validation shows that our annotations are highly accurate and consistent. We use this architecture to gather data on the privacy policies of companies in the Russell 3000 index, resulting in hundreds of thousands of annotations across all categories. Analysis of the resulting data allows us to obtain unique insights into the state of the privacy policy ecosystem as a whole.},
	booktitle = {Proceedings of the 2024 {ACM} on {Internet} {Measurement} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Ziyuan and Tang, Jiaming and Karir, Manish and Liu, Mingyan and Sarabi, Armin},
	year = {2024},
	note = {event-place: Madrid, Spain},
	keywords = {large language models, web crawling, ai chatbots, privacy policies, text annotation},
	pages = {505--515},
}

@inproceedings{foster_formal_2020,
	address = {New York, NY, USA},
	series = {{FormaliSE} '20},
	title = {Formal {Model}-{Based} {Assurance} {Cases} in {Isabelle}/{SACM}: {An} {Autonomous} {Underwater} {Vehicle} {Case} {Study}},
	isbn = {978-1-4503-7071-4},
	url = {https://doi.org/10.1145/3372020.3391559},
	doi = {10.1145/3372020.3391559},
	abstract = {Isabelle/SACM is a tool for automated construction of model-based assurance cases with integrated formal methods, based on the Isabelle proof assistant. Assurance cases show how a system is safe to operate, through a human comprehensible argument demonstrating that the requirements are satisfied, using evidence of various provenances. They are usually required for certification of critical systems, often with evidence that originates from formal methods. Automating assurance cases increases rigour, and helps with maintenance and evolution. In this paper we apply Isabelle/SACM to a fragment of the assurance case for an autonomous underwater vehicle demonstrator. We encode the metric unit system (SI) in Isabelle, to allow modelling requirements and state spaces using physical units. We develop a behavioural model in the graphical RoboChart state machine language, embed the artifacts into Isabelle/SACM, and use it to demonstrate satisfaction of the requirements.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Formal} {Methods} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Foster, Simon and Nemouchi, Yakoub and O'Halloran, Colin and Stephenson, Karen and Tudor, Nick},
	year = {2020},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {Isabelle/HOL, Verification, Autonomous Systems, Assurance Cases},
	pages = {11--21},
}

@inproceedings{zhang_knowledge_2023,
	address = {New York, NY, USA},
	series = {{IoTDI} '23},
	title = {A {Knowledge} {Graph} {Question} {Answering} {Approach} to {IoT} {Forensics}},
	isbn = {979-8-4007-0037-8},
	url = {https://doi.org/10.1145/3576842.3589161},
	doi = {10.1145/3576842.3589161},
	abstract = {Internet of Things (IoT) forensics has been a particularly challenging task for forensic practitioners due to the heterogeneity of IoT environments as well as the complexity and volume of IoT data. With the advent of artificial intelligence, question-answering (QA) systems have emerged as a potential solution for users to access sophisticated forensic knowledge and data. In this light, we present a novel IoT forensics framework that employs knowledge graph question answering (KGQA). Our framework enables investigators to access forensic artifacts and cybersecurity knowledge using natural language questions facilitated by a deep-learning-powered KGQA model. The proposed framework demonstrates high efficacy in answering natural language questions over the experimental IoT forensic knowledge graph.},
	booktitle = {Proceedings of the 8th {ACM}/{IEEE} {Conference} on {Internet} of {Things} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Ruipeng and Xie, Mengjun},
	year = {2023},
	note = {event-place: San Antonio, TX, USA},
	keywords = {Knowledge Graph, Internet of Things, Question Answering, Digital Forensics, Ontology Design},
	pages = {446--447},
}

@inproceedings{cai_apt_2021,
	address = {New York, NY, USA},
	series = {{CIAT} 2020},
	title = {An {APT} {Group} {Knowledge} {Model} based on {MDATA}},
	isbn = {978-1-4503-8782-8},
	url = {https://doi.org/10.1145/3444370.3444600},
	doi = {10.1145/3444370.3444600},
	abstract = {Situational awareness is significant for cyber security, which can help researchers and security analysts obtain the network security situation comprehensively and accurately. Advanced Persistent Threat (APT) attack could cause severe consequences to cyberspace and detecting such attacks have become a very important part of cyber security situational awareness. Some APT attacks may belong to a same group, many countries and organizations have established databases for APT groups, such as adopting knowledge graph (KG) to represent the knowledge. However, cyberspace security knowledge varies by temporal and spatial characteristics, such as the attack technologies are updated very frequently, traditional KG cannot represent such knowledge timely. To address this problem, the MDATA (Multi-dimensional Data Association and inTelligent Analysis) model is proposed in [1], which is a supplement and improvement to traditional KG. In this paper, we introduce an APT group knowledge model based on MDATA, which adds spatial-temporal characteristics of the APT groups. We also analyze how this knowledge model could help address the challenges of APT attack awareness.},
	booktitle = {Proceedings of the 2020 {International} {Conference} on {Cyberspace} {Innovation} of {Advanced} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Cai, Yinyin and Gu, Zhaoquan and Wang, Le and Li, Shudong and Han, Weihong},
	year = {2021},
	note = {event-place: Guangzhou, China},
	keywords = {knowledge model, APT attack, cyber situational awareness, MDATA, spatial-temporal characteristics},
	pages = {374--378},
}

@article{buneman_can_2025,
	title = {Can {We} {Measure} the {Impact} of a {Database}?},
	volume = {68},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3704723},
	doi = {10.1145/3704723},
	abstract = {If we want to measure the impact of a database, can we use its organization to treat it the same way we treat any other publishing agent, such as a journal or an author?},
	number = {5},
	journal = {Commun. ACM},
	author = {Buneman, Peter and Dosso, Dennis and Lissandrini, Matteo and Silvello, Gianmaria and Sun, He},
	month = apr,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Data Citation, h-index, Scientific and Curated Databases},
	pages = {69--76},
}

@inproceedings{guchev_visual_2019,
	address = {New York, NY, USA},
	series = {{UMAP} '19},
	title = {Visual {Annotations} for {Hybrid} {Graph}-based {User} {Model}},
	isbn = {978-1-4503-6021-0},
	url = {https://doi.org/10.1145/3320435.3320472},
	doi = {10.1145/3320435.3320472},
	abstract = {Structured user model data not only allow system personalization, but also may be of interest as a source for analysis: in particular, for the study of general trends and for the detection of anomalies in preferences and mutually-referenced features among different user models. Such sources are multidimensional and interrelated, and recently started to be represented as graph-based datasets. Among the most effective ways of studying such data is visual exploration based on data-driven graph drawing approaches: in particular, node-link and node-link-group diagrams. The paper provides an overview of advanced approaches to the graphical representation of multidimensional data derived from user modeling and presents a proposal for developing flexible and scalable user interfaces for the hypergraph-based visual exploration of relations within a user model (UM). Then, we propose these principles in the visualization of an existing adaptive system.},
	booktitle = {Proceedings of the 27th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Guchev, Vladimir and Cena, Federica and Vernero, Fabiana and Gena, Cristina},
	year = {2019},
	note = {event-place: Larnaca, Cyprus},
	keywords = {graph and hypergraph drawing, user model visualization},
	pages = {31--35},
}

@inproceedings{gui_context-aware_2025,
	address = {New York, NY, USA},
	series = {{IoT} '24},
	title = {A {Context}-aware {Conversational} {Interface} for {Controlling} {Internet}-of-{Things} {Devices}},
	isbn = {979-8-4007-1285-2},
	url = {https://doi.org/10.1145/3703790.3703811},
	doi = {10.1145/3703790.3703811},
	abstract = {This paper presents a demonstration of a context-aware conversational interface for interacting with multiple IoT devices using multi-turn natural language commands. Our system facilitates device discovery and identification through a Knowledge Graph (KG) and addresses the interoperability issue by using the Web of Things (WoT) specifications as an intermediate abstraction layer. The proposed system comprises four major components: a context-aware multi-turn dialogue interface, a device discovery and identification module using a domain-specific KG, a text-to-code module to parse natural language commands into an executable code format, and a customized code execution engine. We demonstrate our system using two Philips Hue smart lamps and one Elgato Stream Deck controller. The Philips Hue smart lamp device can be controlled standalone via natural language, allowing users to adjust the power state, the color, and the brightness. Also, the lamp can be integrated with the Elgato Stream Deck Controller to enable user-defined automation rules following the Trigger Action programming (TAP) paradigm.},
	booktitle = {Proceedings of the 14th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Gui, Zhou and Freund, Michael and Harth, Andreas},
	year = {2025},
	keywords = {Knowledge Graph, Internet of Things, Natural Language Understanding, Conversational Interface, Trigger Action Programming},
	pages = {160--163},
}

@article{hakala_neural_2020,
	title = {Neural {Network} and {Random} {Forest} {Models} in {Protein} {Function} {Prediction}},
	volume = {19},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2020.3044230},
	doi = {10.1109/TCBB.2020.3044230},
	abstract = {Over the past decade, the demand for automated protein function prediction has increased due to the volume of newly sequenced proteins. In this paper, we address the function prediction task by developing an ensemble system automatically assigning Gene Ontology (GO) terms to the given input protein sequence. We develop an ensemble system which combines the GO predictions made by random forest (RF) and neural network (NN) classifiers. Both RF and NN models rely on features derived from BLAST sequence alignments, taxonomy and protein signature analysis tools. In addition, we report on experiments with a NN model that directly analyzes the amino acid sequence as its sole input, using a convolutional layer. The Swiss-Prot database is used as the training and evaluation data. In the CAFA3 evaluation, which relies on experimental verification of the functional predictions, our submitted ensemble model demonstrates competitive performance ranking among top-10 best-performing systems out of over 100 submitted systems. In this paper, we evaluate and further improve the CAFA3-submitted system. Our machine learning models together with the data pre-processing and feature generation tools are publicly available as an open source software at \&lt;uri\&gt;https://github.com/TurkuNLP/CAFA3\&lt;/uri\&gt;.},
	number = {3},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Hakala, Kai and Kaewphan, Suwisa and Björne, Jari and Mehryary, Farrokh and Moen, Hans and Tolvanen, Martti and Salakoski, Tapio and Ginter, Filip},
	month = dec,
	year = {2020},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {1772--1781},
}

@article{jain_automatic_2024,
	title = {Automatic {Construction} of {Interval}-{Valued} {Fuzzy} {Hindi} {WordNet} using {Lexico}-{Syntactic} {Patterns} and {Word} {Embeddings}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3643132},
	doi = {10.1145/3643132},
	abstract = {A computational lexicon is the backbone of any language processing system. It helps computers to understand the language complexity as a human does by inculcating words and their semantic associations. Manually constructed famous Hindi WordNet (HWN) consists of various classical semantic relations (crisp relations). To handle uncertainty and represent Hindi WordNet more semantically, Type- 1 fuzzy graphs are applied to relations of Hindi WordNet. But uncertainty in the crisp membership degree is not considered in Type 1 fuzzy set (T1FS). Also collecting billions (5,55,69,51,753 relations in HWN) of membership values from experts (humans) is not feasible. This paper applied the concept of Interval-Valued Fuzzy graphs and proposed Interval- Valued Fuzzy Hindi WordNet (IVFHWN). IVFHWN automatically identifies Interval- Valued Fuzzy relations between words and their degree of membership using word embeddings and lexico-syntactic patterns. The experimental results for the word sense disambiguation problem show better outcomes when IVFHWN is being used in place of Type 1 Fuzzy Hindi WordNet and classical Hindi WordNet.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Jain, Minni and Jindal, Rajni and Jain, Amita},
	month = feb,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural Language Processing, Computational Lexicon, Hindi WordNet, Interval- Valued Fuzzy Graphs, Low-Resource Language},
	annote = {Just Accepted},
}

@article{magnaudet_djnnsmala_2018,
	title = {Djnn/{Smala}: {A} {Conceptual} {Framework} and a {Language} for {Interaction}-{Oriented} {Programming}},
	volume = {2},
	url = {https://doi.org/10.1145/3229094},
	doi = {10.1145/3229094},
	abstract = {The persistent difficulty to develop and maintain interactive software has unveiled the inadequacy of traditional imperative programming languages. In the recent years, several solutions have been proposed to enrich the existing languages with constructs dedicated to interaction. In this paper, we propose a different approach that takes interaction as the primary concern to build a new programming language. We present Djnn, a conceptual framework based on the concepts of process and process activation, then we introduce Smala a programming language derived from this framework. We propose a solution for the unification of the concepts of event and data-flow, and for the derivation of complex control structures from a small set of basic ones. We detail the syntax and the semantics of Smala. Finally, we illustrate through a real-size application how it enables building all parts of an interactive software. Djnn and Smala may offer designers and programmers usable means to think of interactions and translate them into running code.},
	number = {EICS},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Magnaudet, Mathieu and Chatty, Stéphane and Conversy, Stéphane and Leriche, Sébastien and Picard, Celia and Prun, Daniel},
	month = jun,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {djnn, gui programming, interactive software, reactive programming, smala},
}

@inproceedings{yang_construction_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Construction of {Domain}-{Specific} {Knowledge} {Graph} for {Advanced} {Persistent} {Threat} {Behaviour} {Analysis} and {Detection}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715454},
	doi = {10.1145/3701716.3715454},
	abstract = {Advanced Persistent Threat (APT) represents a sophisticated and targeted attack campaign, often orchestrated by well-resourced organizations. Understanding APT is crucial for adapting to their evolving tactics and effectively mitigating their infiltration methods. A key approach to accurately analysing and detecting APTs involves studying the behaviour, identifying their attack stage and uncovering the employed components. Existing works for APT detection heavily rely on network traffic analysis, limiting their practical applicability in real-world scenarios. This paper introduces a novel method to analyse and detect APT behaviours by constructing a domain-specific knowledge graph (APT-KG), utilising the MITRE ATT\&amp;CK framework as its foundation. A hierarchical clustering-based model is proposed to uncover the correlations among various network attack techniques. It first vectorizes the attack techniques according to ATT\&amp;CK standards, filtering out low-frequency techniques to optimise dimensionality reduction. Parent-child relationships within the ATT\&amp;CK classification further facilitate this process. We observe that this strategy can reveal fully connected relationships among high-frequency techniques, while connections are preserved within clusters between high and low-frequency techniques. This structured representation enables the inference of potential attack patterns, achieving a prediction accuracy of 88.11\%. We then integrate the identified associations with APT-KG. Experimental validation demonstrates that APT-KG significantly enhances understanding of attack interrelations and improves the efficiency of APT detection and response mechanisms.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Yang, Yitian and Chen, Huaming},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {knowledge graph, data mining, network security, ontology construction, advanced persistent threat (apt)},
	pages = {1480--1484},
}

@inproceedings{camelo_mech_2022,
	address = {New York, NY, USA},
	series = {{ICFET} '22},
	title = {Mech {Desk}: {An} ontology based system to help drivers diagnosis vehicle problems},
	isbn = {978-1-4503-9640-0},
	url = {https://doi.org/10.1145/3545862.3545890},
	doi = {10.1145/3545862.3545890},
	abstract = {Semantic Web is a vision about an extension of the existing World Wide Web, which provides tools and technologies to support the transparent exchange of information and knowledge among organizations. As one of the building blocks of Semantic Technology, ontologies are part of the W3C standards stack for the Semantic Web. Nowadays, multiple areas can be aborded by ontologies and the semantic web world, as the subject of this project, mechanics. Mechanics have been accentuated in a visible way, where the reality of living without means of transportation is not feasible in people's lives. The development of new methods to increase the knowledge of drivers and everyday people about automated vehicles is essential. Regarding cars, revisions, maintenance, inspections, change of parts, among others, are necessary and "mandatory" subjects and due to this, it is possible to prevent future damage by prolonging the life of the car. In certain cases, this doesn't happen, either due to wear of parts or unforeseen events, and despite being a busy market, drivers are not always informed about the best cares to take or the problems that may arise. As such, the theme of this project is to make a relationship between mechanical details, issues, and solutions, throughout an ontology, to help an everyday driver to a better perception of what he encounters at hand. For that purpose, the defined ontology was exposed via a mobile application, with it providing to the user, several details that he can or not relate, and trough them, provide a connection with a certain problem and solution. The semantic web ontology was developed in Protégé, exposed into Apache Jena Fuseki server, and was running in an Azure Virtual Machine, allowing it to be available into the OutSystems application.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Frontiers} of {Educational} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Camelo, Diogo and Ascensão, João and Alves, Rui and Matos, Paulo},
	year = {2022},
	note = {event-place: Yokohama, Japan},
	keywords = {‘Mobile Development', ‘Ontology’, 'Semantic Web’, 'Vehicles Maintenance'},
	pages = {169--175},
}

@inproceedings{ballarin_measures_2018,
	address = {New York, NY, USA},
	series = {{MODELS} '18},
	title = {Measures to report the {Location} {Problem} of {Model} {Fragment} {Location}},
	isbn = {978-1-4503-4949-9},
	url = {https://doi.org/10.1145/3239372.3239397},
	doi = {10.1145/3239372.3239397},
	abstract = {Model Fragment Location (MFL) aims at identifying model elements that are relevant to a requirement, feature, or bug. Many MFL approaches have been introduced in the last few years to address the identification of the model elements that correspond to a specific functionality. However, there is a lack of detail when the measurements about the search space (models) and the measurements about the solution to be found (model fragment) are reported. Generally, the only reported measure is the model size. In this paper, we propose using five measurements (size, volume, density, multiplicity, and dispersion) to report the location problems. These measurements are the result of analyzing 1,308 MFLs in a family of industrial models over the last four years. Using two MFL approaches, we emphasize the importance of these measurements in order to compare results. Our work not only proposes improving the reporting of the location problem, but it also provides real measurements of location problems that are useful to other researchers in the design of synthetic location problems.},
	booktitle = {Proceedings of the 21th {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ballarín, Manuel and Marcén, Ana C. and Pelechano, Vicente and Cetina, Carlos},
	year = {2018},
	note = {event-place: Copenhagen, Denmark},
	keywords = {Bug Location, Feature Location, Model Fragment Location, Traceability Link Recovery},
	pages = {189--199},
}

@inproceedings{ciccozzi_towards_2018,
	address = {New York, NY, USA},
	series = {{MODELS} '18},
	title = {Towards a body of knowledge for model-based software engineering},
	isbn = {978-1-4503-5965-8},
	url = {https://doi.org/10.1145/3270112.3270121},
	doi = {10.1145/3270112.3270121},
	abstract = {Model-based Software Engineering (MBSE) is now accepted as a Software Engineering (SE) discipline and is being taught as part of more general SE curricula. However, an agreed core of concepts, mechanisms and practices — which constitutes the Body of Knowledge of a discipline — has not been captured anywhere, and is only partially covered by the SE Body of Knowledge (SWEBOK). With the goals of characterizing the contents of the MBSE discipline, promoting a consistent view of it worldwide, clarifying its scope with regard to other SE disciplines, and defining a foundation for a curriculum development on MBSE, this paper provides a proposal for an extension of the contents of SWEBOK with the set of fundamental concepts, terms and mechanisms that should constitute the MBSE Body of Knowledge.},
	booktitle = {Proceedings of the 21st {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Ciccozzi, Federico and Famelis, Michalis and Kappel, Gerti and Lambers, Leen and Mosser, Sebastien and Paige, Richard F. and Pierantonio, Alfonso and Rensink, Arend and Salay, Rick and Taentzer, Gabi and Vallecillo, Antonio and Wimmer, Manuel},
	year = {2018},
	note = {event-place: Copenhagen, Denmark},
	keywords = {model-based software engineering, body of knowledge},
	pages = {82--89},
}

@article{haffar_synergistic_2023,
	title = {A {Synergistic} {Bidirectional} {LSTM} and {N}-gram {Multi}-channel {CNN} {Approach} {Based} on {BERT} and {FastText} for {Arabic} {Event} {Identification}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3626568},
	doi = {10.1145/3626568},
	abstract = {Event extraction from texts continues to pose a challenge for many NLP systems. This article presents a novel neural network architecture that can extract and classify events from Arabic sentences. The model combines word representations and Part-Of-Speech (POS) tags and uses a bidirectional LSTM layer and a dual combined convolutional neural network. The first layer of the network focuses on sentence representations, while the second layer focuses on POS representations. The model takes advantage of both N-gram character features from FastText and contextual representations from bidirectional encoder representations from transformers. This combination proves to be successful, as evidenced by the good results obtained from evaluating the model on the Arabic TimeML corpus. Our results show that combining both contextual and N-gram representations outperforms the traditional skip-gram model.},
	number = {11},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Haffar, Nafaa and Zrigui, Mounir},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deep learning, Arabic language, BERT representation, events identification, FastText representation, TimeML standard},
}

@inproceedings{schlie_incremental_2020,
	address = {New York, NY, USA},
	series = {{SPLC} '20},
	title = {Incremental feature model synthesis for clone-and-own software systems in {MATLAB}/{Simulink}},
	isbn = {978-1-4503-7569-6},
	url = {https://doi.org/10.1145/3382025.3414973},
	doi = {10.1145/3382025.3414973},
	abstract = {Families of related MATLAB/Simulink systems commonly emerge ad hoc using clone-and-own practices. Extractively migrating systems towards a software product line (SPL) can be a remedy. A feature model (FM) represents all potential configurations of an SPL, ideally, in non-technical domain terms. However, yielding a sensible FM from automated synthesis remains a major challenge due to domain knowledge being a prerequisite for features to be adequate abstractions. In incremental reverse engineering, subsequent generation of FMs may further overwrite changes and design decisions made during previous manual FM refinement.In this paper, we propose an approach to largely automate the synthesis of a suitable FM from a set of cloned MATLAB/Simulink models as part of reverse engineering an SPL. We fully automate the extraction of an initial, i.e., a technical, FM that closely aligns with realization artifacts and their variability, and further provide operations to manually refine it to incorporate domain knowledge. Most importantly, we provide concepts to capture such operations and to replay them on a structurally different technical FM stemming from a subsequent reverse engineering increment that included further systems of the portfolio. We further provide an implementation and demonstrate the feasibility of our approach using two MATLAB/Simulink data sets from the automotive domain.},
	booktitle = {Proceedings of the 24th {ACM} {Conference} on {Systems} and {Software} {Product} {Line}: {Volume} {A} - {Volume} {A}},
	publisher = {Association for Computing Machinery},
	author = {Schlie, Alexander and Knüppel, Alexander and Seidl, Christoph and Schaefer, Ina},
	year = {2020},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {mapping, synthesis, 150\% model, clone-and-own, feature model, incremental, individual, MATLAB/Simulink, refinement, variability},
}

@inproceedings{kenner_model-based_2020,
	address = {New York, NY, USA},
	series = {{SPLC} '20},
	title = {Model-{Based} {Evaluation} of {Vulnerabilities} in {Software} {Systems}},
	isbn = {978-1-4503-7570-2},
	url = {https://doi.org/10.1145/3382026.3431246},
	doi = {10.1145/3382026.3431246},
	abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
	booktitle = {Proceedings of the 24th {ACM} {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} {B}},
	publisher = {Association for Computing Machinery},
	author = {Kenner, Andy},
	year = {2020},
	note = {event-place: Montreal, QC, Canada},
	keywords = {Vulnerability, Feature Model, Exploit, Variability Model, Vulnerability Analysis and Management},
	pages = {112--119},
}

@inproceedings{husunbeyi_integrating_2025,
	address = {New York, NY, USA},
	series = {{MAD}' 25},
	title = {Integrating {Semantic} {Representations} in a {Cross}-{Modal} {Approach} to {Fact}-{Checking}},
	isbn = {979-8-4007-1891-5},
	url = {https://doi.org/10.1145/3733567.3735567},
	doi = {10.1145/3733567.3735567},
	abstract = {We propose a cross-modal approach with deep fusion of a language model and graph structures based on Abstract Meaning Representations (AMRs) enriched with Wikidata to address the fact-checking problem. We collect and make available a large dataset of fact-checked claim sentences, and systematically compare a transformer-based model with Graph Neural Networks (GNNs) based on AMR graphs and extended with external information. Furthermore, we evaluate the integration of language models and GNNs for the fact verification task. While GNN models on AMR-based graphs alone yield lower scores than transformer based language models on their own, the combined cross-modal approach—leveraging a multilayer and deep interaction between textual and structural information—demonstrates the best performance. Finally, we evaluate the generalization capability of this cross-modal approach integrating AMR-based graph structures on out-of-domain English and German claims.},
	booktitle = {Proceedings of the 4th {ACM} {International} {Workshop} on {Multimedia} {AI} against {Disinformation}},
	publisher = {Association for Computing Machinery},
	author = {Hüsünbeyi, Z. Melce and Seddah, Djamé and Scheffler, Tatjana},
	year = {2025},
	keywords = {fact-checking, external knowledge, AMR graphs, cross-modal, deep and interactive fusion of LMs and GNNs},
	pages = {17--27},
}

@inproceedings{villaca_microservice_2020,
	address = {New York, NY, USA},
	series = {{SBSI} '20},
	title = {Microservice {Architecture} for {Multistore} {Database} {Using} {Canonical} {Data} {Model}},
	isbn = {978-1-4503-8873-3},
	url = {https://doi.org/10.1145/3411564.3411629},
	doi = {10.1145/3411564.3411629},
	abstract = {In a microservice architecture, solutions are created by teams focused on specific domains and needs. They independently develop and deploy distributed services in the network. One characteristic of microservices is decentralized data management. Each microservice may use different data management technology which best fit its needs. Hence, it is an issue to integrate data of heterogenous microservices to come up with consolidate data views. Flexible and efficient solutions in this scenario are needed. This work is based on the use of a canonical data model as the mechanism for data integration in microservices. The canonical data model is the reference for query specifications and data integration. This work proposes and implements a microservice architecture based on this strategy and it is composed by nodes that intercommunicate through several mechanisms (e.g., SPARQL, GraphQL and JDBC queries, calls to REST services and proprietary APIs). The solution was analyzed in a proof of concept in a fictitious scenario but using real services available at DBPedia and Twitter. The evaluation goal was to qualitatively analyze the use of the architecture in the design, development and execution of microservices in order to identify the characteristics one should consider when using the canonical data model strategy. The evaluation employed the criteria of ISO/IEC 25010 model that most relate to the SOA challenges, which were: usability; performance; compatibility; and, maintainability. The identified advantages and disadvantages of using the architecture (i.e., the strategy) can be used by architects and developers to make their development decisions.},
	booktitle = {Proceedings of the {XVI} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Villaça, Luís Henrique Neves and Azevedo, Leonardo Guerreiro and Siqueira, Sean Wolfgand Matsui},
	year = {2020},
	note = {event-place: São Bernardo do Campo, Brazil},
	keywords = {service-oriented architecture, canonical data model, microservice architecture, multistore, polyglot persistence},
}

@inproceedings{wang_family_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {A {Family} of {Fuzzy} {Orthogonal} {Projection} {Models} for {Monolingual} and {Cross}-lingual {Hypernymy} {Prediction}},
	isbn = {978-1-4503-6674-8},
	url = {https://doi.org/10.1145/3308558.3313439},
	doi = {10.1145/3308558.3313439},
	abstract = {Hypernymy is a semantic relation, expressing the “is-a” relation between a concept and its instances. Such relations are building blocks for large-scale taxonomies, ontologies and knowledge graphs. Recently, much progress has been made for hypernymy prediction in English using textual patterns and/or distributional representations. However, applying such techniques to other languages is challenging due to the high language dependency of these methods and the lack of large training datasets of lower-resourced languages. In this work, we present a family of fuzzy orthogonal projection models for both monolingual and cross-lingual hypernymy prediction. For the monolingual task, we propose a Multi-Wahba Projection (MWP) model to distinguish hypernymy vs. non-hypernymy relations based on word embeddings. This model establishes distributional fuzzy mappings from embeddings of a term to those of its hypernyms and non-hypernyms, which consider the complicated linguistic regularities of these relations. For cross-lingual hypernymy prediction, a Transfer MWP (TMWP) model is proposed to transfer the semantic knowledge from the source language to target languages based on neural word translation. Additionally, an Iterative Transfer MWP (ITMWP) model is built upon TMWP, which augments the training sets of target languages when target languages are lower-resourced with limited training data. Experiments show i) MWP outperforms previous methods over two hypernymy prediction tasks for English; and ii) TMWP and ITMWP are effective to predict hypernymy over seven non-English languages.},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Chengyu and Fan, Yan and He, Xiaofeng and Zhou, Aoying},
	year = {2019},
	note = {event-place: San Francisco, CA, USA},
	keywords = {cross-lingual transfer learning, hypernymy prediction, Multi-Wahba Projection},
	pages = {1965--1976},
}

@article{liu_attention-based_2020,
	title = {An {Attention}-based {Deep} {Relevance} {Model} for {Few}-shot {Document} {Filtering}},
	volume = {39},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3419972},
	doi = {10.1145/3419972},
	abstract = {With the large quantity of textual information produced on the Internet, a critical necessity is to filter out the irrelevant information and organize the rest into categories of interest (e.g., an emerging event). However, supervised-learning document filtering methods heavily rely on a large number of labeled documents for model training. Manually identifying plenty of positive examples for each category is expensive and time-consuming. Also, it is unrealistic to cover all the categories from an evolving text source that covers diverse kinds of events, user opinions, and daily life activities. In this article, we propose a novel attention-based deep relevance model for few-shot document filtering (named ADRM), inspired by the relevance feedback methodology proposed for ad hoc retrieval. ADRM calculates the relevance score between a document and a category by taking a set of seed words and a few seed documents relevant to the category. It constructs the category-specific conceptual representation of the document based on the corresponding seed words and seed documents. Specifically, to filter irrelevant yet noisy information in the seed documents, ADRM employs two types of attention mechanisms (namely whole-match attention and max-match attention) and generates category-specific representations for them. Then ADRM is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process, a self-attention layer, and a relevance aggregation layer. Extensive experiments on three real-world datasets show that ADRM consistently outperforms the existing technical alternatives, including the conventional classification and retrieval baselines, and the state-of-the-art deep relevance ranking models for few-shot document filtering. We also perform an ablation study to demonstrate that each component in ADRM is effective for enhancing filtering performance. Further analysis shows that ADRM is robust under varying parameter settings.},
	number = {1},
	journal = {ACM Trans. Inf. Syst.},
	author = {Liu, Bulou and Li, Chenliang and Zhou, Wei and Ji, Feng and Duan, Yu and Chen, Haiqing},
	month = oct,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deep learning, Few-shot learning, document filtering},
}

@article{vedula_modeling_2021,
	title = {Modeling knowledge and functional intent for context-aware pragmatic analysis},
	volume = {2021},
	issn = {1931-1745},
	url = {https://doi.org/10.1145/3447879.3447882},
	doi = {10.1145/3447879.3447882},
	abstract = {Nikhita Vedula is an Applied Scientist at Amazon Alexa Science. She obtained her PhD in Computer Science and Engineering from the Ohio State University in August 2020, advised by Professor Srinivasan Parthasarathy. She received her bachelor's degree from the National Institute of Technology, Nagpur, India in 2015. Her research interests are at the intersection of data mining, natural language processing and social computing. Over the course of her PhD, her research involved designing efficient and novel machine learning and computational linguistic techniques that extract, interpret and transform the vast, unstructured digital content into structured knowledge representations in diverse contexts. She has worked with researchers from interdisciplinary fields such as emergency response, marketing, sociology and psychology. She performed research internships at Nokia Bell Laboratories, Adobe Research and Amazon Alexa AI. Her work has been published at several top data mining conferences such as the Web Conference, SIGIR, WSDM and ICDM. Her work on detecting user intentions from their natural language interactions won the Best paper award at the Web Conference 2020. She was a recipient of a Graduate Research Award (2020), a Presidential Fellowship (2019) and a University Graduate Fellowship (2015) at the Ohio State University. She was also selected as a Rising Star in EECS (2019).},
	number = {Winter},
	journal = {SIGWEB Newsl.},
	author = {Vedula, Nikhita},
	month = feb,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
}

@inproceedings{partridge_coordinate_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {Coordinate systems: level ascending ontological options},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00017},
	doi = {10.1109/MODELS-C.2019.00017},
	abstract = {A major challenge faced in the deployment of collaborating unmanned vehicles is enabling the semantic interoperability of sensor data. One aspect of this, where there is significant opportunity for improvement, is characterizing the coordinate systems for sensed position data. We are involved in a proof of concept project that addresses this challenge through a foundational conceptual model using a constructional approach based upon the BORO Foundational Ontology. The model reveals the characteristics as sets of options for configuring the coordinate systems. This paper examines how these options involve, ontologically, ascending levels. It identifies two types of levels, the well-known type levels and the less well-known tuple/relation levels.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Partridge, Chris and Mitchell, Andrew and Loneragan, Michael and Atkinson, Hayden and de Cesare, Sergio and Khan, Mesbah},
	year = {2021},
	keywords = {BORO foundational ontology, constructional ontology, geometric coordinate system ontology, multi-level options, multi-platform-domain sensor system, power-tuple-builder, power-type-builder},
	pages = {78--87},
}

@inproceedings{cui_design_2021,
	address = {New York, NY, USA},
	series = {{ICISCAE} 2021},
	title = {Design of {Intelligent} {Recognition} {English} {Translation} {Model} {Based} on {Feature} {Extraction} {Algorithm}},
	isbn = {978-1-4503-9025-5},
	url = {https://doi.org/10.1145/3482632.3482749},
	doi = {10.1145/3482632.3482749},
	abstract = {In recent years, with the deepening of globalization, international cooperation is becoming more and more extensive, and the importance of English is increasing. Aiming at the problems that the semantic context of English is not obvious in the process of English translation in traditional English translation system, the optimal translation solution is not reached in the process of selecting the optimal feature semantics, and the translation accuracy is low, an intelligent recognition English translation model based on feature extraction algorithm is designed. Search module is used to complete the search of basic meaning and subject content of vocabulary to be proofread, grasp the user's behavior data through behavior log and optimize the system; In the method based on the maximum entropy principle, the whole task of clause recognition is divided into three parts: sentence head recognition, sentence tail recognition and complete clause recognition. Experimental results show that the proposed algorithm has higher recognition rate.},
	booktitle = {2021 4th {International} {Conference} on {Information} {Systems} and {Computer} {Aided} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Cui, Gaili},
	year = {2021},
	note = {event-place: Dalian, China},
	pages = {553--557},
}

@inproceedings{sheng_integrating_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {Integrating {Pattern}- and {Fact}-based {Fake} {News} {Detection} via {Model} {Preference} {Learning}},
	isbn = {978-1-4503-8446-9},
	url = {https://doi.org/10.1145/3459637.3482440},
	doi = {10.1145/3459637.3482440},
	abstract = {To defend against fake news, researchers have developed various methods based on texts. These methods can be grouped as 1) pattern-based methods, which focus on shared patterns among fake news posts rather than the claim itself; and 2) fact-based methods, which retrieve from external sources to verify the claim's veracity without considering patterns. The two groups of methods, which have different preferences of textual clues, actually play complementary roles in detecting fake news. However, few works consider their integration. In this paper, we study the problem of integrating pattern- and fact-based models into one framework via modeling their preference differences, i.e., making the pattern- and fact-based models focus on respective preferred parts in a post and mitigate interference from non-preferred parts as possible. To this end, we build a Preference-aware Fake News Detection Framework (Pref-FEND), which learns the respective preferences of pattern- and fact-based models for joint detection. We first design a heterogeneous dynamic graph convolutional network to generate the respective preference maps, and then use these maps to guide the joint learning of pattern- and fact-based models for final prediction. Experiments on two real-world datasets show that Pref-FEND effectively captures model preferences and improves the performance of models based on patterns, facts, or both.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Sheng, Qiang and Zhang, Xueyao and Cao, Juan and Zhong, Lei},
	year = {2021},
	note = {event-place: Virtual Event, Queensland, Australia},
	keywords = {graph neural networks, pattern mining, fact-checking, fake news detection, preference learning},
	pages = {1640--1650},
}

@article{trinh_vietmedkg_2025,
	title = {{VietMedKG}: {Knowledge} {Graph} and {Benchmark} for {Traditional} {Vietnamese} {Medicine}},
	volume = {24},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3744740},
	doi = {10.1145/3744740},
	abstract = {Traditional Vietnamese Medicine (TVM) and Traditional Chinese Medicine (TCM) have shared significant similarities due to their geographical location, cultural exchanges, and hot and humid climatic conditions. However, unlike TCM, which has substantial works published to construct a knowledge graph, there is a notable absence of a comprehensive knowledge graph for TVM. This article presents the first endeavor to build a knowledge graph for TVM based on extensive existing resources from TCM. We name our knowledge graph as VietMedKG. We propose a translation and filtration process to adapt TCM knowledge graphs to TVM, identifying the overlapping and unique elements of TVM. In addition, the constructed knowledge graph is then exploited further for developing a curated benchmark for the knowledge graph-based question-answering problem with the potential to support doctors and patients in assisting doctors and patients in identifying various diseases. Our work will not only bridge the gap between TCM and TVM but also set the foundation for future research into TVM community. Our source code is publicly available at .},
	number = {7},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Trinh, Tam and Dao, Anh and Hy, Thi Hong Nhung and Hy, Truong Son},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge graph, retrieval augmented generation, graph-based question answering, traditional vietnamese mecidine},
}

@inproceedings{agapito_13th_2024,
	address = {New York, NY, USA},
	series = {{BCB} '24},
	title = {13th {Workshop} on {Parallel} and {AI}-based {Bioinformatics} and {Biomedicine} ({ParBio}): {Editorial}},
	isbn = {979-8-4007-1302-6},
	url = {https://doi.org/10.1145/3698587.3701527},
	doi = {10.1145/3698587.3701527},
	abstract = {The goal of ParBio is to bring together scientists in high-performance computing, computational biology, and medicine to discuss parallel implementation of bioinformatics and biomedical applications and the challenges and opportunities of moving these applications to the cloud or edge. The workshop will also address Artificial Intelligence (AI), Large Language Models (LLMs), machine learning, and big data analytics in healthcare and bioinformatics, focusing on the integrated analysis of molecular and clinical data. This is motivated by the increasing production of experimental and clinical data and the shift towards data storage, integration, and analysis.},
	booktitle = {Proceedings of the 15th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Agapito, Giuseppe and Cannataro, Mario and Lloyd, Wes J. and Zucco, Chiara},
	year = {2024},
	note = {event-place: Shenzhen, China},
	keywords = {Bioinformatics, Machine Learning, Parallel algorithms},
}

@inproceedings{aryan_explainable_2021,
	address = {New York, NY, USA},
	series = {{MSCPES} '21},
	title = {Explainable cyber-physical energy systems based on knowledge graph},
	isbn = {978-1-4503-8608-1},
	url = {https://doi.org/10.1145/3470481.3472704},
	doi = {10.1145/3470481.3472704},
	abstract = {Explainability can help cyber-physical systems alleviating risk in automating decisions that are affecting our life. Building an explainable cyber-physical system requires deriving explanations from system events and causality between the system elements. Cyber-physical energy systems such as smart grids involve cyber and physical aspects of energy systems and other elements, namely social and economic. Moreover, a smart-grid scale can range from a small village to a large region across countries. Therefore, integrating these varieties of data and knowledge is a fundamental challenge to build an explainable cyber-physical energy system. This paper aims to use knowledge graph based framework to solve this challenge. The framework consists of an ontology to model and link data from various sources and graph-based algorithm to derive explanations from the events. A simulated demand response scenario covering the above aspects further demonstrates the applicability of this framework.},
	booktitle = {Proceedings of the 9th {Workshop} on {Modeling} and {Simulation} of {Cyber}-{Physical} {Energy} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Aryan, Peb Ruswono and Ekaputra, Fajar Juang and Sabou, Marta and Hauer, Daniel and Mosshammer, Ralf and Einfalt, Alfred and Miksa, Tomasz and Rauber, Andreas},
	year = {2021},
	note = {event-place: Virtual Event},
	keywords = {knowledge graphs, ontologies, explainability, smart grid simulation, smart grids},
}

@inproceedings{shengxin_establishment_2021,
	address = {New York, NY, USA},
	series = {{ISAIMS} '21},
	title = {The establishment and evaluation of the automatic crisis balance analysis model for social network users based on artificial intelligence technology},
	isbn = {978-1-4503-9558-8},
	url = {https://doi.org/10.1145/3500931.3500960},
	doi = {10.1145/3500931.3500960},
	abstract = {Online social media provides people with a platform to express their emotions anonymously. Social media has been identified as an important data source for suicide prevention related to emotional problems in China. Almost Three million messages were published by 450,000 users in a particular Chinese social media data base. This study aims to develop a Crisis Balance Analysis Model based on concepts of "balancing factors" as described by Aguilera. Through interactions with psychological experts, deep learning architecture that was built and refined. Three annotation levels free annotations (zero cost), easy annotations (by psychology students), and hard annotations (by psychology experts) were used. Our Model was evaluated accordingly and showed that its performance at each level was promising. Finally, suicide risks, cognitive distortions and interpersonal problems could be identified for messages from social media users using this model, which providing basis for proactive crisis intervention.},
	booktitle = {Proceedings of the 2nd {International} {Symposium} on {Artificial} {Intelligence} for {Medicine} {Sciences}},
	publisher = {Association for Computing Machinery},
	author = {Shengxin, Hu and Qing, Wang and Lu, Chen and Xingxin, Zhang and Leiqing, Huang and Tianyu, He and Songhe, Li and Xiangmin, Dong and Bingxiang, Yang},
	year = {2021},
	note = {event-place: Beijing, China},
	keywords = {Depression, Suicide prevention, Crisis, Artificial intelligent},
	pages = {157--161},
}

@inproceedings{holubova_unlocking_2019,
	address = {New York, NY, USA},
	series = {{SBD} '19},
	title = {Unlocking the potential of {nextGen} multi-model databases for semantic big data projects},
	isbn = {978-1-4503-6766-0},
	url = {https://doi.org/10.1145/3323878.3325807},
	doi = {10.1145/3323878.3325807},
	abstract = {A new vision in semantic big data processing is to create enterprise data hubs, with a 360° view on all data that matters to a corporation. As we discuss in this paper, a new generation of multi-model database systems seems a promising architectural choice for building such scalable, non-native triple stores. In this paper, we first characterize this new generation of multi-model databases. Then, discussing an example scenario, we show how they allow for agile and flexible schema management, spanning a large design space for creative and incremental data modelling. We identify the challenge of generating sound triple-views from data stored in several, interlinked models, for SPARQL querying. We regard this as one of several appealing research challenges where the semantic big data and the database architecture community may join forces.},
	booktitle = {Proceedings of the {International} {Workshop} on {Semantic} {Big} {Data}},
	publisher = {Association for Computing Machinery},
	author = {Holubová, Irena and Scherzinger, Stefanie},
	year = {2019},
	note = {event-place: Amsterdam, Netherlands},
	keywords = {semantic data management, multi-model DBMS, schema evolution},
}

@article{bensalem_arabic_2023,
	title = {An {Arabic} {Probabilistic} {Parser} {Based} on a {Property} {Grammar}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3612921},
	doi = {10.1145/3612921},
	abstract = {The specificities of Arabic parsing, such as agglutination, vocalization, and the relatively order-free words in Arabic sentences, remain major issues to consider. To promote its robustness, such parseing should define different types of constraints. Property Grammar (PG) formalism verifies the satisfiability of the constraints directly on the units of the structure, thanks to its properties (or relations). In this context, we propose to build a probabilistic parser with syntactic properties, using a PG, and we measure the production rules in terms of different implicit information and in particular the syntactic properties. We experimented with our parser on the treebank ATB, using the parsing algorithm CYK, and we obtained encouraging results. Our method is also automatic for implementation of most property types. Its generalization for other languages or corpus domains (using treebanks) could be a good perspective. Its combination with pre-trained models of BERT may also make our parser faster.},
	number = {10},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Bensalem, Raja and Haddar, Kais and Blache, Philippe},
	month = oct,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Arabic language, lexicalized grammar, Probabilistic parser, property grammar formalism},
}

@inproceedings{ishii_can_2020,
	address = {New York, NY, USA},
	series = {{IVA} '20},
	title = {Can {Prediction} of {Turn}-management {Willingness} {Improve} {Turn}-changing {Modeling}?},
	isbn = {978-1-4503-7586-3},
	url = {https://doi.org/10.1145/3383652.3423907},
	doi = {10.1145/3383652.3423907},
	abstract = {For smooth conversation, participants must carefully monitor the turn-management (a.k.a. speaking and listening) willingness of other conversational partners and adjust turn-changing behaviors accordingly. Many studies have focused on predicting the actual moments of speaker changes (a.k.a. turn-changing), but to the best of our knowledge, none of them explicitly modeled the turn-management willingness from both speakers and listeners in dyad interactions. We address the problem of building models for predicting this willingness of both. Our models are based on trimodal inputs, including acoustic, linguistic, and visual cues from conversations. We also study the impact of modeling willingness to help improve the task of turn-changing prediction. We introduce a dyadic conversation corpus with annotated scores of speaker/listener turn-management willingness. Our results show that using all of three modalities of speaker and listener is important for predicting turn-management willingness. Furthermore, explicitly adding willingness as a prediction task improves the performance of turn-changing prediction. Also, turn-management willingness prediction becomes more accurate with this multi-task learning approach.},
	booktitle = {Proceedings of the 20th {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author = {Ishii, Ryo and Ren, Xutong and Muszynski, Michal and Morency, Louis-Philippe},
	year = {2020},
	note = {event-place: Virtual Event, Scotland, UK},
	keywords = {multimodal signal processing, multitask learning, turn-changing prediction, turn-management willingness},
}

@inproceedings{liang_speech-driven_2020,
	address = {New York, NY, USA},
	series = {{ISAIMS} '20},
	title = {A {Speech}-{Driven} 3-{D} {Lip} {Synthesis} with {Realistic} {Dynamics} in {Mandarin} {Chinese}},
	isbn = {978-1-4503-8860-3},
	url = {https://doi.org/10.1145/3429889.3429904},
	doi = {10.1145/3429889.3429904},
	abstract = {In this paper, a new speech-driven lip synchronization method is developed, predicting the 3-D geometric shape of the lip without using speech recognition model in the visualization procedure, and can be trained and evaluated with realistic dynamics. Videos of Mandarin Chinese words are used. Speech signals are calculated into MFCC as audio features. 68-points facial landmarks are annotated from the corresponding videos through the prediction algorithm from the Dlib Library. Eos, a 3-D Morphable Face Model, is applied, using the facial landmarks, to predict the 3-D shape, where we can acquire 3-D landmarks. A machine-learning sequence-tagging model, averaged Structured Perceptron using Viterbi algorithm, is applied for modelling the direct prediction of labial parameters from the acoustic MFCC parameters. The 3-D labial area shape from the 'eos' prediction of a frame is morphed according to the predicted 3-D labial landmarks, forming the 3-D lip sequence, which can be plotted synchronically with the acoustic signal. In this 3-D lip synthesis, acoustic features and realistic lip shapes are directly mapped, where lip units and speech recognition are not applied, preserving more realistic articulatory or personality details; and the predicted geometric shapes are comparable with realistic dynamics, with the comparison indicating that this synthesis is of good effect.},
	booktitle = {Proceedings of the 1st {International} {Symposium} on {Artificial} {Intelligence} in {Medical} {Sciences}},
	publisher = {Association for Computing Machinery},
	author = {Liang, Changwei and Pan, Xiaosheng and Kong, Jiangping},
	year = {2020},
	note = {event-place: Beijing, China},
	keywords = {Mandarin Chinese, realistic dynamics, speech-driven, 3-D lip synthesis},
	pages = {79--84},
}

@inproceedings{kabanda_bayesian_2021,
	address = {New York, NY, USA},
	series = {{AADNIC}-{ABMECR} '20},
	title = {A bayesian network model for machine learning and cyber security},
	isbn = {978-1-4503-8767-5},
	url = {https://doi.org/10.1145/3440094.3440389},
	doi = {10.1145/3440094.3440389},
	abstract = {The phenomenal growth in the use of internet-based technologies has resulted in complexities in cyber security subjecting organizations to cyber-attacks. This research is purposed to develop a cyber-security system that uses the Bayesian Network structure and Machine Learning. The research determined the cyber-security framework appropriate for a developing nation; evaluated network detection and prevention systems that use Artificial Intelligence paradigms such as finite automata, neural networks, genetic algorithms, fuzzy logic, support vector machines, or diverse data-mining-based approaches; analyzed Bayesian Networks that can be represented as graphical models and are directional to represent cause-effect relationships; and developed a Bayesian Network model that can handle complexity in cybersecurity. The Pragmatism paradigm used in this research, as a philosophy is intricately related to the mixed-method approach, which is largely quantitative with the research design being a survey and an experiment, but supported by qualitative approaches where Focus Group discussions were held. The Artificial Intelligence paradigms evaluated include machine learning methods, autonomous robotic vehicles, artificial neural networks, and fuzzy logic. Alternative improved solutions discussed include the use of machine learning algorithms specifically Artificial Neural Networks (ANN), Decision Tree C4.5, Random Forests, and Support Vector Machines (SVM).},
	booktitle = {Proceedings of the 2nd {Africa}-{Asia} {Dialogue} {Network} ({AADN}) {International} {Conference} on {Advances} in {Business} {Management} and {Electronic} {Commerce} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Kabanda, Gabriel},
	year = {2021},
	note = {event-place: Ganzhou, China},
	keywords = {cybersecurity, artificial intelligence (AI), machine learning (ML), artificial neural networks (ANN) and decision tree, Bayesian network model},
}

@inproceedings{laamech_idsm-o_2022,
	address = {New York, NY, USA},
	series = {{MEDES} '22},
	title = {{IdSM}-{O}: {An} {IoT} {Data} {Sharing} {Management} {Ontology} for {Data} {Governance}},
	isbn = {978-1-4503-9219-8},
	url = {https://doi.org/10.1145/3508397.3564825},
	doi = {10.1145/3508397.3564825},
	abstract = {The main purpose of IoT is to deliver reliable, high quality services and innovative solutions by transforming the captured data into meaningful information, and thus improving user's daily life. In this regard, it is in the interest of the community to encourage entities within IoT environments to share their data, and therefore serve public interest and contribute to the innovation and technological progress. Meanwhile, the distributed nature of IoT networks and the diversity of its actors lead to the recognition of security and data sharing management as one of the major challenges of the IoT domain. For instance, due to insufficient governance of the shared data within IoT environments, data provider retains little to no control over his assets once he has agreed to share them. Furthermore, data consumers are not able to trace the source of the available resource nor its history processing to assess its quality. All this creates a digital environment that is certainly functional but lacks mutual trust between its actors, which can prevent the domain's full potential to be exploited, and therefore disrupt the implemented services. In our work, we propose an approach to improve data sharing management using three main elements: semantic modeling, usage control policies, and data provenance.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Laamech, Nouha and Munier, Manuel and Pham, Congduc},
	year = {2022},
	note = {event-place: Venice, Italy},
	keywords = {data governance, semantic modeling, data sharing management, usage control},
	pages = {88--95},
}

@inproceedings{kostovska_option_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {{OPTION}: optimization algorithm benchmarking ontology},
	isbn = {978-1-4503-8351-6},
	url = {https://doi.org/10.1145/3449726.3459579},
	doi = {10.1145/3449726.3459579},
	abstract = {Many platforms for benchmarking optimization algorithms offer users the possibility of sharing their experimental data with the purpose of promoting reproducible and reusable research. However, different platforms use different data models and formats, which drastically inhibits identification of relevant data sets, their interpretation, and their interoperability. Consequently, a semantically rich, ontology-based, machine-readable data model is highly desired.We report in this paper on the development of such an ontology, which we name OPTION (OPTImization algorithm benchmarking ONtology). Our ontology provides the vocabulary needed for semantic annotation of the core entities involved in the benchmarking process, such as algorithms, problems, and evaluation measures. It also provides means for automated data integration, improved interoperability, powerful querying capabilities and reasoning, thereby enriching the value of the benchmark data. We demonstrate the utility of OPTION by annotating and querying a corpus of benchmark performance data from the BBOB workshop data - a use case which can be easily extended to cover other benchmarking data collections.},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Kostovska, Ana and Vermetten, Diederick and Doerr, Carola and Džeroski, Sašo and Panov, Panče and Eftimov, Tome},
	year = {2021},
	note = {event-place: Lille, France},
	pages = {239--240},
}

@article{pennanen_product_2025,
	title = {From {Product} to {Producer}: {The} {Impact} of {Perceptual} {Evidence} and {Robot} {Embodiment} on the {Human} {Assessment} of {AI} {Creativity}},
	volume = {14},
	url = {https://doi.org/10.1145/3711939},
	doi = {10.1145/3711939},
	abstract = {While creative artificial intelligence (AI) is becoming integral to our lives, we know little about what makes us call AI “creative”. Informed by prior theoretical and empirical work, we investigate how perceiving evidence of a creative act beyond the final product affects our assessment of robot creativity. We study embodiment morphology as a potential moderator of this relationship, informing a 3 × 2 factorial design. In two lab experiments on visual art, participants (N = 30 + 60) assessed drawings produced by two physical robots with different morphologies, under exposure to product, process and producer as three levels of perceptual evidence. The data supports that the human assessment of robot creativity is significantly higher the more is revealed beyond the product about the creation process, and eventually the producer. We find no significant effects of embodiment morphology, contrasting existing hypotheses and offering a more detailed understanding for future work. The latter is also informed by additional exploratory analyses revealing factors potentially influencing creativity assessments, including perceived robot likeability and participants’ experience with robotics and AI. Our insights empirically ground existing design patterns, foster fairness and validity in system comparisons, and contribute to a deeper understanding of our relationship with creative AI and thus its adoption in society.},
	number = {3},
	journal = {J. Hum.-Robot Interact.},
	author = {Pennanen, Niki and Linkola, Simo and Kantosalo, Anna and Hiillos, Nicolas and Männistö, Tomi and Guckelsberger, Christian},
	month = apr,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Embodiment, Computational Creativity, Creative AI, Human Perception, Human-Robot Interaction},
}

@inproceedings{ilani_invited_2023,
	address = {New York, NY, USA},
	series = {{ApPLIED} 2023},
	title = {Invited {Paper}: {Common} {Public} {Knowledge} for {Enhancing} {Machine} {Learning} {Data} {Sets}},
	isbn = {979-8-4007-0128-3},
	url = {https://doi.org/10.1145/3584684.3597263},
	doi = {10.1145/3584684.3597263},
	abstract = {In this study, we show the advantages of incorporating multi-source knowledge from publicly available sources, such as ChatGPT and Wikipedia, into existing datasets to enhance the performance of machine learning models for routine tasks, such as classification. specifically, we propose the utilization of supplementary data from external sources and demonstrate the utility of widely accessible knowledge in the context of the Forest Cover Type Prediction task launched by the Roosevelt National Forest of Northern Colorado. Additionally, we exhibit an improvement in classification accuracy for the Isolated Letter Speech Recognition dataset when incorporating information on regional accents in the prediction of spoken English letter names.},
	booktitle = {Proceedings of the 5th {Workshop} on {Advanced} {Tools}, {Programming} {Languages}, and {PLatforms} for {Implementing} and {Evaluating} {Algorithms} for {Distributed} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ilani, Arnon and Dolev, Shlomi},
	year = {2023},
	note = {event-place: Orlando, FL, USA},
	keywords = {ChatGPT, machine learning, ontology, feature engineering, forest management, isolated letter, random forests, speech recognition, tree cover type, world knowledge},
}

@inproceedings{barroso_design_2019,
	address = {New York, NY, USA},
	series = {{SBSI} '19},
	title = {Design {Science} {Research} to design a conceptual model about prosopographic information related to politicians},
	isbn = {978-1-4503-7237-4},
	url = {https://doi.org/10.1145/3330204.3330233},
	doi = {10.1145/3330204.3330233},
	abstract = {The growing demand for information about politicians and the speed with which news is propagated by media and social networks reveals in contemporary times a more participatory view of the citizen in politics and social control in government actions. Speculation about information about politicians and their respective parties have become more constant, such as in electoral periods, investigations on processes, journalistic interest, etc. This can be observed during the electoral period of 2018 in which the search for information about the candidates and their respective parties was notorious. Given that information transparency is paramount for a democratic regime, the organization and consolidation of data from official sources is a reliable tool for consultation and dissemination of knowledge. According to this period, this work sought to understand some cognitive models that explain electoral behavior, which led to the investigation of factors that influence the decision to vote. The observed cognitive models indicate some attitudes, opinions, satisfactions, events and government assessments that interfere to some degree in the choice of vote, approval or disapproval for some purpose. Among the factors of influence observed, there are prosopographic information, which deals with the biography of politicians, and which are available in the various transparency portals. However, the data contained in these platforms are not organized in such a way as to make the information more intelligible and more reliable to the citizen. In this sense, this research is interested in the prosopographic information content, which would be better evaluated if the data of the politicians were organized according to a conceptual model of knowledge. The methodological approach adopted in this research is the Design Science Reserch (DSR), which directs the construction of an artifact in a given context, whose theoretical conjectures are based on the search and production of knowledge. As a result, research contributes to the knowledge base, as it discusses the evolution of cognitive models and through the design of the artifact, delivers reliable and relevant information about the life of a politician.},
	booktitle = {Proceedings of the {XV} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Barroso, José S. and Pimentel, Mariano and Nunes, Vanessa and Cappelli, Claudia},
	year = {2019},
	note = {event-place: Aracaju, Brazil},
	keywords = {Conceptual Modeling, Data Transparency, Organized Information, Prosopography Politicial},
}

@inproceedings{zhou_dataset_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Dataset for {Industrial} {Question} {Answering} with {Explanation} and {Scalable} {Ensemble} {Generation}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715310},
	doi = {10.1145/3701716.3715310},
	abstract = {The digital and green transition under Industry 4.0 has accelerated the adoption of AI in industries such as manufacturing, energy, and mining. Question Answering with Explanation (QAE), as a way of human interaction with AI, is crucial for enhancing transparency and trust in high-stakes industrial applications. However, industrial QAE remains underexplored due to the lack of publicly available, high-quality datasets, hindered by the need for expert effort and corporate restrictions. To this end, we introduce PANDAX ( https://doi.org/10.5281/zenodo.14510798 ), the first open-source industrial QAE dataset, and SEG, a scalable method for generating high-quality QAE datasets using LLMs. PANDAX focuses on three key topics of industrial system information: partonomy, functionality, and parameters, across critical domains such as green technology and cooling systems. SEG ensures scalability and quality through ensemble generation, majority voting, expert ranking, etc. The human evaluation validates PANDAX's high quality, positioning it as a valuable resource for advancing QAE techniques, benchmarking language technologies, and supporting research in explainable AI for industrial systems.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Yan and Zhou, Baifan and Li, Huajian and Lyu, Qianhang and Qu, Yuanwei and Waaler, Arild and Yu, Ingrid C.},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {dataset generation, industrial dataset resource, question answering with explanation, system information},
	pages = {825--828},
}

@inproceedings{esteva_modeling_2020,
	address = {New York, NY, USA},
	series = {{JCDL} '20},
	title = {Modeling {Data} {Curation} to {Scientific} {Inquiry}: {A} {Case} {Study} for {Multimodal} {Data} {Integration}},
	isbn = {978-1-4503-7585-6},
	url = {https://doi.org/10.1145/3383583.3398539},
	doi = {10.1145/3383583.3398539},
	abstract = {Scientific data publications may include interactive data applications designed by scientists to explore a scientific problem. Defined as knowledge systems, their development is complex when data are aggregated from multiple sources over time. Multimodal data are created, encoded, and maintained differently, and even when reporting about identical phenomena, fields and their values may be inconsistent across datasets. To assure the validity and accuracy of the application, the data has to abide by curation requirements similar to those ruling digital libraries. We present a novel, inquiry-driven curation approach aimed to optimize multimodal datasets curation and maximize data reuse by domain researchers. We demonstrate the method through the ASTRIAGraph project, in which multiple data sources about near earth space objects are aggregated into a central knowledge system. The process involves multidisciplinary collaboration, resulting in the design of a data model as the backbone for both data curation and scientific inquiry. We demonstrate a) how data provenance information is needed to assess the uncertainty of the results of scientific inquiries involving multiple data sources, and b) that continuous curation of integrated datasets is facilitated when undertaken as integral to the research project. The approach provides flexibility to support expansion of scientific inquiries and data in the knowledge system, and allows for transparent and explainable results.},
	booktitle = {Proceedings of the {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} in 2020},
	publisher = {Association for Computing Machinery},
	author = {Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta, Amit and Jah, Moriba},
	year = {2020},
	note = {event-place: Virtual Event, China},
	keywords = {data integration, data curation, graph database, big data, data model, knowledge system, space traffic management},
	pages = {235--242},
}

@article{rocha_silva_ensuring_2020,
	title = {Ensuring the {Consistency} between {User} {Requirements} and {Task} {Models}: {A} {Behavior}-{Based} {Automated} {Approach}},
	volume = {4},
	url = {https://doi.org/10.1145/3394979},
	doi = {10.1145/3394979},
	abstract = {Evaluating and ensuring the consistency between user requirements and modeling artifacts is a long-time issue for model-based software design. Conflicts in requirements specifications can lead to many design errors and have a decisive impact on the quality of systems under development. This article presents an approach based on Behavior-Driven Development (BDD) to provide automated assessment for task models, which are intended to model the flow of user and system tasks in an interactive system. The approach has been evaluated by exploiting user requirements described by a group of experts in the domain of business trips. Such requirements gave rise to a set of BDD stories that have been used to automatically assess scenarios extracted from task models that were reengineered from an existing web system for booking business trips. The results have shown our approach, by performing a static analysis of the source files, was able to identify different types of inconsistencies between the user requirements and the set of task models analyzed.},
	number = {EICS},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Rocha Silva, Thiago and Winckler, Marco and Trætteberg, Hallvard},
	month = jun,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {user stories, task models, automated requirements assessment, behavior-driven development (BDD)},
}

@inproceedings{singh_exploring_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Exploring {Positionality} in {HCI}: {Perspectives}, {Trends}, and {Challenges}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713280},
	doi = {10.1145/3706598.3713280},
	abstract = {Positionality acknowledges that researchers’ subjectivities, values and experiences influence approaches to and outcomes of research. It underlines and promotes self-awareness and explicit demonstration of reflexivity. To understand how positionality is conceptualised and used in HCI, we conducted two studies: (i) a scoping review of positionality and reflexivity statements in CHI papers from the last 11 years and (ii) a survey of HCI researchers (n=75). Our findings show that positionality statements are often viewed as a box-ticking exercise and their influence on the research is seldom discussed. They are also often restricted to more sensitive areas of research and may impact marginalised identities. We argue that positionality statements may be valuable but not as markers of methodological rigour; their content should be at the discretion of authors and methodologically consistent. Our contributions include a current snapshot of positionality in HCI and reflections on its current role and future directions in HCI.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Singh, Aneesha and Dechant, Martin Johannes and Patel, Dilisha and Soubutts, Ewan and Barbareschi, Giulia and Ayobi, Amid and Newhouse, Nikki},
	year = {2025},
	keywords = {methods, methodology, identity, positionality, reflexivity},
}

@inproceedings{kesper_detecting_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Detecting quality problems in research data: a model-driven approach},
	isbn = {978-1-4503-7019-6},
	url = {https://doi.org/10.1145/3365438.3410987},
	doi = {10.1145/3365438.3410987},
	abstract = {As scientific progress highly depends on the quality of research data, there are strict requirements for data quality coming from the scientific community. A major challenge in data quality assurance is to localise quality problems that are inherent to data. Due to the dynamic digitalisation in specific scientific fields, especially the humanities, different database technologies and data formats may be used in rather short terms to gain experiences. We present a model-driven approach to analyse the quality of research data. It allows abstracting from the underlying database technology. Based on the observation that many quality problems show anti-patterns, a data engineer formulates analysis patterns that are generic concerning the database format and technology. A domain expert chooses a pattern that has been adapted to a specific database technology and concretises it for a domain-specific database format. The resulting concrete patterns are used by data analysts to locate quality problems in their databases. As proof of concept, we implemented tool support that realises this approach for XML databases. We evaluated our approach concerning expressiveness and performance in the domain of cultural heritage based on a qualitative study on quality problems occurring in cultural heritage data.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kesper, Arno and Wenz, Viola and Taentzer, Gabriele},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {data quality, model-driven development, pattern matching},
	pages = {354--364},
}

@inproceedings{renda_melody_2023,
	address = {New York, NY, USA},
	series = {{HT} '23},
	title = {Melody: {A} {Platform} for {Linked} {Open} {Data} {Visualisation} and {Curated} {Storytelling}},
	isbn = {979-8-4007-0232-7},
	url = {https://doi.org/10.1145/3603163.3609035},
	doi = {10.1145/3603163.3609035},
	abstract = {Data visualisation and storytelling techniques help experts highlight relations between data and share complex information with a broad audience. However, existing solutions targeted to Linked Open Data visualisation have several restrictions and lack the narrative element. In this article we present MELODY, a web interface for authoring data stories based on Linked Open Data. MELODY has been designed using a novel methodology that harmonises existing Ontology Design and User Experience methodologies (eXtreme Design and Design Thinking), and provides reusable User Interface components to create and publish web-ready article-alike documents based on data retrievable from any SPARQL endpoint. We evaluate the software by comparing it with existing solutions, and we show its potential impact in projects where data dissemination is crucial.},
	booktitle = {Proceedings of the 34th {ACM} {Conference} on {Hypertext} and {Social} {Media}},
	publisher = {Association for Computing Machinery},
	author = {Renda, Giulia and Daquino, Marilena and Presutti, Valentina},
	year = {2023},
	note = {event-place: Rome, Italy},
	keywords = {Linked Open Data, design thinking, data visualization, ontology design, storytelling},
}

@inproceedings{widianto_empowerment_2022,
	address = {New York, NY, USA},
	series = {{ICLIQE} '21},
	title = {The {Empowerment} of {Critical} {Thinking} {Skills} through {Problem}-{Based} {Learning} {Model} {Viewed} {From} {Epigenetic}},
	isbn = {978-1-4503-8692-0},
	url = {https://doi.org/10.1145/3516875.3516922},
	doi = {10.1145/3516875.3516922},
	abstract = {Critical thinking skills can face 21st-century challenges, which increasingly require technology and science in a global society in this world. Thus, education must be oriented towards mathematics and natural sciences accompanied by social and human sciences. Therefore, this study aims to describe the empowerment of critical thinking skills through a problem-based learning model viewed from the epigenetic aspect. This research employed library research. In this study, data collection was obtained from news, articles in journals, and relevant books. The analysis was carried out in four stages: 1) data collection, 2) data reduction, 3) data display, and 4) conclusion. This study's results revealed that learning needs to pay attention to nature and nurture because it dramatically affects the mastery of thinking skills, and bridging the two things in learning will be a challenge for science teachers in the future. The problem-based learning model can be utilized as a natural science learning model that can empower critical thinking skills from an epigenetic perspective. It can be concluded that the cellular and molecular mechanisms of learning and memory have long been a major focus of neurology and molecular biology, a concern regarding the epigenetic mechanisms behind dynamic changes in the transcription of genes responsible for memory formation and maintenance.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Learning} {Innovation} and {Quality} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Widianto, Idam Ragil Widianto and Ardiansyah, Roy and Saputri, Dwi Yuniasih},
	year = {2022},
	note = {event-place: Surakarta, Indonesia},
}

@inproceedings{sabuncuoglu_justified_2025,
	address = {New York, NY, USA},
	series = {{FAccT} '25},
	title = {Justified {Evidence} {Collection} for {Argument}-based {AI} {Fairness} {Assurance}},
	isbn = {979-8-4007-1482-5},
	url = {https://doi.org/10.1145/3715275.3732003},
	doi = {10.1145/3715275.3732003},
	abstract = {It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system’s lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework’s effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments.},
	booktitle = {Proceedings of the 2025 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Sabuncuoglu, Alpay and Burr, Christopher and Maple, Carsten},
	year = {2025},
	keywords = {continuous fairness monitoring, large language models in finance, system transparency artefacts, trustworthy and ethical assurance},
	pages = {18--28},
}

@inproceedings{akbulut_all_2025,
	address = {San Jose, California, USA},
	series = {{AIES} '24},
	title = {All {Too} {Human}? {Mapping} and {Mitigating} the {Risks} from {Anthropomorphic} {AI}},
	abstract = {The development of highly-capable conversational agents, underwritten by large language models, has the potential to shape user interaction with this technology in profound ways, particularly when the technology is anthropomorphic, or appears human-like. Although the effects of anthropomorphic AI are often benign, anthropomorphic design features also create new kinds of risk. For example, users may form emotional connections to human-like AI, creating the risk of infringing on user privacy and autonomy through over-reliance. To better understand the possible pitfalls of anthropomorphic AI systems, we make two contributions: first, we explore anthropomorphic features that have been embedded in interactive systems in the past, and leverage this precedent to highlight the current implications of anthropomorphic design. Second, we propose research directions for informing the ethical design of anthropomorphic AI. In advancing the responsible development of AI, we promote approaches to the ethical foresight, evaluation, and mitigation of harms arising from user interactions with anthropomorphic AI.},
	booktitle = {Proceedings of the 2024 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {AAAI Press},
	author = {Akbulut, Canfer and Weidinger, Laura and Manzini, Arianna and Gabriel, Iason and Rieser, Verena},
	year = {2025},
	pages = {13--26},
}

@article{lorvao_antunes_strategic_2024,
	title = {Strategic {Analysis} in the {Public} {Sector} {Using} {Semantic} {Web} {Technologies}},
	volume = {5},
	url = {https://doi.org/10.1145/3656587},
	doi = {10.1145/3656587},
	abstract = {This article addresses the complex challenges that public organizations face in designing, implementing, and evaluating their strategies, where public interest and regulatory compliance often intertwine with strategic objectives. This research investigates the application of ontologies in the field of public sector strategy management to enhance the capacity of organizations to make informed data-driven decisions, efficiently allocate resources, and effectively navigate the intricate landscape of the public sector. The LNEC - National Laboratory for Civil Engineering’s strategy is used as an exploratory case study. Semantic web technologies are used to perform strategy analysis, including validating the strategy formulation and supporting the strategy execution by assessing performance indicators, verifying the design of cause-and-effect relationships between strategic objectives, and monitoring and empirically validating these relationships. The increased interoperability of these technologies enables information sharing across systems and organizations. Following the strategy analysis, recommendations are provided, leading to a more robust and data-driven strategic management approach, enabling accurate, traceable, and continuous monitoring of an organization’s strategy. Theoretical and practical implications are discussed, along with limitations and future work. This research offers a blueprint for public sector organizations seeking to optimize their strategies, foster transparency, and deliver more effective services to the public they serve.},
	number = {3},
	journal = {Digit. Gov.: Res. Pract.},
	author = {Lorvão Antunes, António and Barateiro, José and Cardoso, Elsa},
	month = sep,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Ontology, Semantic Web, Balanced Scorecard, Public Sector, Strategy Analysis},
}

@article{moscato_few-shot_2023,
	title = {Few-shot {Named} {Entity} {Recognition}: {Definition}, {Taxonomy} and {Research} {Directions}},
	volume = {14},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3609483},
	doi = {10.1145/3609483},
	abstract = {Recent years have seen an exponential growth (+98\% in 2022 w.r.t. the previous year) of the number of research articles in the few-shot learning field, which aims at training machine learning models with extremely limited available data. The research interest toward few-shot learning systems for Named Entity Recognition (NER) is thus at the same time increasing. NER consists in identifying mentions of pre-defined entities from unstructured text, and serves as a fundamental step in many downstream tasks, such as the construction of Knowledge Graphs, or Question Answering. The need for a NER system able to be trained with few-annotated examples comes in all its urgency in domains where the annotation process requires time, knowledge and expertise (e.g., healthcare, finance, legal), and in low-resource languages. In this survey, starting from a clear definition and description of the few-shot NER (FS-NER) problem, we take stock of the current state-of-the-art and propose a taxonomy which divides algorithms in two macro-categories according to the underlying mechanisms: model-centric and data-centric. For each category, we line-up works as a story to show how the field is moving toward new research directions. Eventually, techniques, limitations, and key aspects are deeply analyzed to facilitate future studies.},
	number = {5},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Moscato, Vincenzo and Postiglione, Marco and Sperlí, Giancarlo},
	month = oct,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Named Entity Recognition, Few-shot learning},
}

@inproceedings{sai_p_semantic_2024,
	address = {New York, NY, USA},
	series = {{CSAI} '23},
	title = {Semantic {Topic} {Extraction} from {Research} {Artifacts}},
	isbn = {979-8-4007-0868-8},
	url = {https://doi.org/10.1145/3638584.3638670},
	doi = {10.1145/3638584.3638670},
	abstract = {The GENESIS project introduces a novel framework for Research Artifact Data Semantics to enhance semantic web technologies within academic institutions. This paper introduces a system developed as part of the GENESIS challenge, focusing on extracting and labelling topics from diverse research artifacts. The approach employs knowledge graphs, topic models, and named entity recognition to infer meaningful topics. Evaluation encompasses measures such as topic coherence and semantic similarity. Future work includes refining entity linking techniques, optimizing system performance, and implementing advanced graph centrality algorithms. This contribution advances semantic understanding in various research contexts.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Sai P, Daiveek and Rajesh, Anouksha},
	year = {2024},
	note = {event-place: Beijing, China},
	keywords = {natural language processing, knowledge graph, linked data, topic modelling, text summarization},
	pages = {539--545},
}

@inproceedings{moore_automated_2024,
	address = {New York, NY, USA},
	series = {L@{S} '24},
	title = {Automated {Generation} and {Tagging} of {Knowledge} {Components} from {Multiple}-{Choice} {Questions}},
	isbn = {979-8-4007-0633-2},
	url = {https://doi.org/10.1145/3657604.3662030},
	doi = {10.1145/3657604.3662030},
	abstract = {Knowledge Components (KCs) linked to assessments enhance the measurement of student learning, enrich analytics, and facilitate adaptivity. However, generating and linking KCs to assessment items requires significant effort and domain-specific knowledge. To streamline this process for higher-education courses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs) in Chemistry and E-Learning. We analyzed discrepancies between the KCs generated by the Large Language Model (LLM) and those made by humans through evaluation from three domain experts in each subject area. This evaluation aimed to determine whether, in instances of non-matching KCs, evaluators showed a preference for the LLM-generated KCs over their human-created counterparts. We also developed an ontology induction algorithm to cluster questions that assess similar KCs based on their content. Our most effective LLM strategy accurately matched KCs for 56\% of Chemistry and 35\% of E-Learning MCQs, with even higher success when considering the top five KC suggestions. Human evaluators favored LLM-generated KCs, choosing them over human-assigned ones approximately two-thirds of the time, a preference that was statistically significant across both domains. Our clustering algorithm successfully grouped questions by their underlying KCs without needing explicit labels or contextual information. This research advances the automation of KC generation and classification for assessment items, alleviating the need for student data or predefined KC labels.},
	booktitle = {Proceedings of the {Eleventh} {ACM} {Conference} on {Learning} @ {Scale}},
	publisher = {Association for Computing Machinery},
	author = {Moore, Steven and Schmucker, Robin and Mitchell, Tom and Stamper, John},
	year = {2024},
	note = {event-place: Atlanta, GA, USA},
	keywords = {Language model, E-learning, Automated generation, Students, concept labeling, knowledge component, knowledge labeling, learning engineering, multiple-choice question, Clustering algorithms, Domain Knowledge, Labelings, Education computing, Multiple-choice questions, Concept labeling, E - learning, Knowledge components, Knowledge labeling, Learning engineering},
	pages = {122--133},
	annote = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{yang_what_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {What a {Surprise}! {Computing} {Rewritten} {Modules} {Can} {Be} as {Efficient} as {Computing} {Subset} {Modules}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679528},
	doi = {10.1145/3627673.3679528},
	abstract = {Uniform Interpolation (UI) is an advanced non-standard reasoning service that seeks to refine ontologies by creating rewritten modules. These modules, known as uniform interpolants, retain only "relevant names" while preserving their meanings in the absence of other names. UI holds significant potential across various domains where tailored ontology modules are required. However, realizing its full potential demands highly optimized techniques for generating such modules. Previous studies have identified notable challenges in generating uniform interpolants for EL-ontologies, where their computation is substantially more complex and computationally demanding than standard subset modules.Despite these obstacles, this paper introduces an advanced "forgetting" method tailored for computing uniform interpolants of ELIO-ontologies with ABoxes. We show that with effective normalization and inference strategies, these uniform interpolants can be computed efficiently, matching the speed of standard module computation. A comprehensive evaluation using a prototype implementation of this method achieved a 100\% success rate on two major benchmark datasets, Oxford-ISG and BioPortal, with results delivered within seconds. The efficiency of our approach is attributed to our novel linear strategy for introducing definers, in sharp contrast to existing strategies that lead to an exponential increase in definers and computational inefficiency. Our method is unique in its ability to create signature-restricted modules for large-scale ontologies, making it a vital addition to the community's toolkit.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Zhihao and Zhao, Yizheng},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {ontologies, forgetting, uniform interpolation, description logics},
	pages = {2940--2949},
}

@inproceedings{neubauer_model-based_2020,
	address = {San Diego, CA, USA},
	series = {{SummerSim} '20},
	title = {Model-based development and simulative verification of logical vehicle functions using executable {UN}/{ECE} regulations},
	isbn = {978-1-7138-1429-0},
	abstract = {On the way towards autonomous driving, a steadily increasing number of Advanced Driver Assistance Systems leads to a tremendous test effort to approve their safe operation. However, route-based real-world tests cannot cover this effort sufficiently which is why virtual testing has become part of the vehicle development and approval process as well. Since regulations such as those of the United Nations Economic Commission for Europe, are mandatory for vehicle approval there is a need to integrate their prescribed test scenarios into virtual test environments. In this paper, we present a novel approach to transform these textually available scenarios into executable state machines. It is complemented by a holistic simulation-based verification where model-based vehicle functions are stimulated by sensor data of the virtual vehicle under test to achieve meaningful and more realistic results. We prototyped a tool-chain to execute approval-related test scenarios on the example of an Advanced Emergency Braking System.},
	booktitle = {Proceedings of the 2020 {Summer} {Simulation} {Conference}},
	publisher = {Society for Computer Simulation International},
	author = {Neubauer, Kevin and Bucher, Harald and Haas, Benedikt and Becker, Jürgen},
	year = {2020},
	note = {event-place: Virtual Event, Spain},
	keywords = {simulation, model-based, verification, E/E architecture, homologation},
}

@inproceedings{li_deepfca_2020,
	address = {New York, NY, USA},
	series = {{ICMHI} '20},
	title = {{DeepFCA}: {Matching} {Biomedical} {Ontologies} {Using} {Formal} {Concept} {Analysis} {Embedding} {Techniques}},
	isbn = {978-1-4503-7776-8},
	url = {https://doi.org/10.1145/3418094.3418121},
	doi = {10.1145/3418094.3418121},
	abstract = {Biomedical ontologies contain target domain knowledge. In many cases, multiple ontologies are created independently for different purposes in the same biomedical domain. To fuse and extend existing knowledge, we need to find the corresponding entities (i.e. classes and properties) from different ontologies. Formal Concept Analysis (FCA) is a mature mathematical tool for biomedical ontology matching tasks and has achieved competitive performance. The FCA-based method mainly matches the ontologies through lexical tokens and structural information. This method ignores the inherent semantics of entities. On the other hand, representation learning techniques are widely used in different NLP tasks to capture the semantic similarity of words. In this paper, we propose a novel biomedical ontology matching method which we dub DeepFCA. We use pre-trained word vectors to initialize the vector representations onto which semantic information is inscribed. FCA embedding techniques are used to refine these vectors. DeepFCA combines FCA and word2vec methods to enhance the performance of biomedical ontology matching. To the best of our knowledge, this is the first attempt to apply FCA embedding techniques to biomedical ontology matching. Experiments on real-world biomedical ontologies show that DeepFCA improves the recall and F1-measure compared with the traditional FCA-based algorithm. It also achieves competitive performance compared with several state-of-the-art systems.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Medical} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Li, Guoxuan},
	year = {2020},
	note = {event-place: Kamakura City, Japan},
	keywords = {Artificial intelligence, Biomedical ontology matching, Word embedding, Formal concept analysis},
	pages = {259--265},
}

@article{shoaib_context-aware_2023,
	title = {Context-aware {Urdu} {Information} {Retrieval} {System}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3502854},
	doi = {10.1145/3502854},
	abstract = {World Wide Web (WWW) is playing a vital role for sharing dynamic knowledge in every field of life. The information on web comprises a huge amount of data in different forms such as structured, semi structured, or few is totally in unstructured format. Due to huge size of information, searching from larger textual data about the specific topic or getting precise information is a challenging task. All this leads to the problem of word sense ambiguity (WSA). Urdu language-based information retrieval system using different techniques related to Web Semantic Search Engine architecture is proposed to efficiently retrieve the relevant information and solve the problem of WSA. The proposed system has average precision ratio 96\% as compared to average precision ratio of 74\% and 75\% average precision Google for single word query. For the long text queries, our system outperforms the existing famous search engines with 92\% accuracy such as Bing and Google having 16.50\% and 16\% accuracy, respectively. Similarly, the proposed system for single word query, the recall ratio is 32.25\% as compared to 25\% and 25\% of Bing and Google. The results of recall ratio for long text query are improved as well, showing 6.38\% as compared to 6.20\% and 4.8\% of Bing and Google, respectively. The results showed that the proposed system gives better and efficient results as compared to the existing systems for Urdu language.},
	number = {3},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Shoaib, Umar and Fiaz, Laiba and Chakraborty, Chinmay and Rauf, Hafiz Tayyab},
	month = apr,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, information retrieval, semantic web, corpus, Urdu language, context-based, keywords, quad extraction, searching and indexing, triplets, Uniform Resource Identifier, Web Semantic Search Engine, WSA},
}

@inproceedings{xu_method_2024,
	address = {New York, NY, USA},
	series = {{PEAI} '24},
	title = {A {Method} for {Constructing} a {Knowledge} {Graph} of {Electric} {Power} {Digital} {Marketing} {Based} on {Artificial} {Intelligence} {Deep} {Learning}},
	isbn = {979-8-4007-1663-8},
	url = {https://doi.org/10.1145/3674225.3674330},
	doi = {10.1145/3674225.3674330},
	abstract = {With the rapid development of digitalization and informatization in the power industry, power companies have accumulated a large amount of data in various business fields. This article focuses on the intelligent application requirements in the field of electric power marketing, and designs and constructs a knowledge graph of electric power marketing business that includes domain background knowledge. Firstly, utilize the relationships between the basic business data tables organized by domain experts to construct a conceptual ontology. Next, through operations such as data cleaning, data filtering, and feature selection, traverse the data tables of the business database, use knowledge graph tools to obtain a knowledge graph, and finally use the power marketing knowledge graph to build an intelligent question answering application that supports natural language question answering services in the field of power marketing, better serving power users. Experimental results have shown that the power marketing knowledge graph constructed in this article, along with intelligent question answering applications, can accurately answer user questions and significantly improve user satisfaction.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Power} {Electronics} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Jianing and Lou, Fei and Jiang, Ying and Chen, Bo and Zhong, Zhenyuan},
	year = {2024},
	note = {event-place: Xiamen, China},
	keywords = {Knowledge graph, Natural language processing, Electricity Marketing},
	pages = {582--587},
}

@inproceedings{sakor_integrating_2025,
	address = {New York, NY, USA},
	series = {{WSDM} '25},
	title = {Integrating {Knowledge} {Graphs} and {Neuro}-{Symbolic} {AI}: {LDM} {Enables} {FAIR} and {Federated} {Research} {Data} {Management}},
	isbn = {979-8-4007-1329-3},
	url = {https://doi.org/10.1145/3701551.3704125},
	doi = {10.1145/3701551.3704125},
	abstract = {Managing research digital objects (RDOs) in compliance with FAIR principles is crucial for ensuring accessibility, interoperability, and reusability across scientific domains. The Leibniz Data Manager (LDM) is a state-of-the-art framework that integrates Knowledge Graphs (KGs) and Neuro-Symbolic AI, combining the reasoning power of Large Language Models (LLMs) with structured metadata. LDM supports the management and enhancement of RDOs through entity linking, connecting datasets to external KGs like Wikidata and the Open Research Knowledge Graph (ORKG). Additionally, LDM offers federated query processing across KGs, enabling users to explore related papers, datasets, and resources through natural language questions. This demo showcases LDM's capabilities to explore RDOs, compare existing datasets, and extend metadata. By blending Neuro-Symbolic AI with FAIR and federated research data management, LDM offers a powerful tool for accelerating data-driven discovery in science. LDM is publicly accessible at https://service.tib.eu/ldmservice/.},
	booktitle = {Proceedings of the {Eighteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Sakor, Ahmad and Brunet, Mauricio and Iglesias, Enrique and Rivas, Ariam and Rohde, Philipp D. and Kraft, Angelina and Vidal, Maria-Esther},
	year = {2025},
	note = {event-place: Hannover, Germany},
	keywords = {data science, digital repositories, federated search},
	pages = {1044--1047},
}

@article{wu_personalized_2023,
	title = {Personalized {News} {Recommendation}: {Methods} and {Challenges}},
	volume = {41},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3530257},
	doi = {10.1145/3530257},
	abstract = {Personalized news recommendation is important for users to find interesting news information and alleviate information overload. Although it has been extensively studied over decades and has achieved notable success in improving user experience, there are still many problems and challenges that need to be further studied. To help researchers master the advances in personalized news recommendation, in this article, we present a comprehensive overview of personalized news recommendation. Instead of following the conventional taxonomy of news recommendation methods, in this article, we propose a novel perspective to understand personalized news recommendation based on its core problems and the associated techniques and challenges. We first review the techniques for tackling each core problem in a personalized news recommender system and the challenges they face. Next, we introduce the public datasets and evaluation methods for personalized news recommendation. We then discuss the key points on improving the responsibility of personalized news recommender systems. Finally, we raise several research directions that are worth investigating in the future. This article can provide up-to-date and comprehensive views on personalized news recommendation. We hope this article can facilitate research on personalized news recommendation as well as related fields in natural language processing and data mining.},
	number = {1},
	journal = {ACM Trans. Inf. Syst.},
	author = {Wu, Chuhan and Wu, Fangzhao and Huang, Yongfeng and Xie, Xing},
	month = jan,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language processing, personalization, survey, user modeling, News recommendation},
}

@inproceedings{li_research_2022,
	address = {New York, NY, USA},
	series = {{AIAM2021}},
	title = {Research on {Financial} {Risk} {Control} {Model} and {Algorithm} {Based} on {Machine} {Learning} under the {Background} of {Rural} {Revitalization} {Strategy}},
	isbn = {978-1-4503-8504-6},
	url = {https://doi.org/10.1145/3495018.3501225},
	doi = {10.1145/3495018.3501225},
	abstract = {The strategy of rural revitalization is of great significance to the reconstruction of rural economic growth, in which rural industries, represented by the integration and development of rural industries, have sprung up. The purpose of this paper is to use machine learning (ML) technology to build an effective risk control model, so as to help Internet finance enterprises better control the loan risk. Sample data of Internet financial platform borrowers are extracted from multiple dimensions, and then the data are further processed, and the data used to build the model is extracted by feature engineering. Combined with the Gradient Boosting Decision Tree (GBDT) algorithm in ML algorithm, the comprehensive evaluation is carried out by using the basic information of bank customers, flow records, user detection information and user detection scale. The performance of the wind control model is further improved by ML, which provides guidance and reference for the performance improvement of the model.},
	booktitle = {2021 3rd {International} {Conference} on {Artificial} {Intelligence} and {Advanced} {Manufacture}},
	publisher = {Association for Computing Machinery},
	author = {Li, Shaoyi},
	year = {2022},
	note = {event-place: Manchester, United Kingdom},
	pages = {3010--3014},
}

@inproceedings{nevsky_i_2024,
	address = {New York, NY, USA},
	series = {{ASSETS} '24},
	title = {"{I} {Wish} {You} {Could} {Make} the {Camera} {Stand} {Still}": {Envisioning} {Media} {Accessibility} {Interventions} with {People} with {Aphasia}},
	isbn = {979-8-4007-0677-6},
	url = {https://doi.org/10.1145/3663548.3675598},
	doi = {10.1145/3663548.3675598},
	abstract = {Audiovisual media is integral to modern living, yet is not always accessible to all. Modern accessibility interventions, such as subtitles, support many, however, communities with complex communication needs are largely unconsidered. In this work, we envision future accessibility interventions from the ground up with one such community – people with aphasia. Over two workshops and a probe activity, we problematise the space of audiovisual consumption by people with aphasia, and co-envision directions for development in accessible audiovisual media. From low-fi diegetic prototypes to mid-fidelity solutions, we explore new visions of accessibility interventions for complex communication needs – notably enabling high levels of content manipulation and personalisation. Our findings raise open questions and set directions for the research community in developing accessibility interventions for audiovisual media to support users with diverse needs in accessing audiovisual content.},
	booktitle = {Proceedings of the 26th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {Association for Computing Machinery},
	author = {Nevsky, Alexandre and Bircanin, Filip and Cruice, Madeline N and Wilson, Stephanie and Simperl, Elena and Neate, Timothy},
	year = {2024},
	note = {event-place: St. John's, NL, Canada},
	keywords = {prototype, Accessibility, aphasia, audiovisual, complex communication needs, envisioning, media, probes},
}

@inproceedings{junaid_evaluating_2021,
	address = {New York, NY, USA},
	series = {{EASE} '21},
	title = {Evaluating the {Effectiveness} of {Problem} {Frames} for {Contextual} {Modeling} of {Cyber}-{Physical} {Systems}: a {Tool} {Suite} with {Adaptive} {User} {Interfaces}},
	isbn = {978-1-4503-9053-8},
	url = {https://doi.org/10.1145/3463274.3463344},
	doi = {10.1145/3463274.3463344},
	abstract = {Bridging the gap between academic research and industrial application is an important issue to promote Jackson's Problem Frames approach (PF) to the software engineering community. Various attempts have been made to tackle this problem, such as defining formal semantics of PF for software development, and providing a semi-formal approach to model transformations of problem diagrams, with automated tool support. In this paper, we propose to exclusively focus on exploring and evaluating the effectiveness of Jackson's problem diagrams for modeling the context of cyber-physical systems, by developing a suite of support tools enhanced with adaptive user interfaces, and empirically and comprehensively assess its usability. This paper introduces the state of the art, corresponding research questions, research methodologies and current progress of our research.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Junaid, Waqas},
	year = {2021},
	note = {event-place: Trondheim, Norway},
	keywords = {cyber-physical systems, context-based knowledge, adaptive user interfaces, Problem Frames},
	pages = {284--287},
}

@inproceedings{gazzawe_use_2018,
	address = {New York, NY, USA},
	series = {{ICSCA} '18},
	title = {Use of {Ontology} in {Identifying} {Missing} {Artefact} {Links}},
	isbn = {978-1-4503-5414-1},
	url = {https://doi.org/10.1145/3185089.3185092},
	doi = {10.1145/3185089.3185092},
	abstract = {The techniques of requirement traceability have evolved over recent years. However, as much as they have contributed to the software engineering field, significant ambiguity remains in many software engineering processes. This paper reports on an investigation of requirement traceability artefacts, stakeholders, and SDLC development models. Data were collected to gather evidence of artefacts and their properties from previous studies. The aim was to find the missing link between artefacts and their relationship to one another, the stakeholders, and SDLC models. This paper undertakes the first phase of the main research project, which aims to develop a framework for guiding software developers to actively manage traceability. After inquiring into and examining previous research on this topic, the links between artefacts and their functions were identified. The analysis resulted in the development of a new model for requirement traceability, defined in the form of an ontology portraying the contributively relations between software artefacts using common properties with the aid of Protégé Software. This study thus provides an important insight into the future of the requirement artefacts relation, and thereby lays an important foundation towards increasing our understanding of their potential and limitations.},
	booktitle = {Proceedings of the 2018 7th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Gazzawe, Foziah and Lock, Russell and Dawson, Christian},
	year = {2018},
	note = {event-place: Kuantan, Malaysia},
	keywords = {Artefacts Link, Mapping the Requirement Artefacts, Requirement Artefacts, Requirements Traceability},
	pages = {6--9},
}

@inproceedings{shangguan_hierarchical_2019,
	address = {New York, NY, USA},
	series = {{ICMRE}'19},
	title = {A {Hierarchical} {Digital} {Twin} {Model} {Framework} for {Dynamic} {Cyber}-{Physical} {System} {Design}},
	isbn = {978-1-4503-6095-1},
	url = {https://doi.org/10.1145/3314493.3314504},
	doi = {10.1145/3314493.3314504},
	abstract = {Cyber-physical system (CPS) is a new trend in the complex system related research works, where network connectivity enhances computing power and systemic behavior emerges through the competition, interaction, collaboration and integration among individual interweaving, which consists of real-time monitoring, data management, physical feedback control. From this perspective, CPS is a dynamic entity with rich functions. However, designers may encounter a difficult situation, in which subsequent dynamic changes of the system are discussed and appropriate functionalities are added in the early design phase. Since the digital twin is the digital duplicate of the physical entity, it can dynamically evolve following the product life cycle. In this paper, we propose a hierarchical digital twin model framework for CPS design. In the light of digital twin concept, the hierarchical high-level models facilitate storage of information from the entire product life cycle. Finally, an industrial robot application is presented to demonstrate the efficacy of the model framework.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Mechatronics} and {Robotics} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shangguan, Duansen and Chen, Liping and Ding, Jianwan},
	year = {2019},
	note = {event-place: Rome, Italy},
	keywords = {Digital Twin, CPS, Simulation, Complex System, Industrial Robot, Modeling\&amp},
	pages = {123--129},
}

@inproceedings{bekele_ontology_2018,
	address = {USA},
	series = {{CASCON} '18},
	title = {Ontology driven temporal event annotator {mHealth} application framework},
	abstract = {We present an application (app) framework to facilitate the collection of gold standard temporal event annotations. These data will enable training and evaluation of machine learning algorithms for predicting events of clinical significance. Recording of such data using pen and paper can prove to be tedious and error-prone due to the variation in the types of events and the frequency of occurrence. To address this problem, we developed an mHealth application framework that presents an intuitive and configurable user interface for annotating a timeline with events.The presented Temporal Event Annotator (TEA) app framework supports dynamically building a customized application inclusive of events, event categories, and study attributes based on the design input of a specific study. This is accomplished by presenting a terminology schema for the hierarchical definition of event types and an additional user interface (UI) schema to support UI-specific attributes.We describe the framework architecture independent of specific technology implementations. We also describe specific instantiations of the framework that we used to develop and evaluate apps for three different use cases: 1) patient monitoring in the Neonatal Intensive Care Unit (NICU), 2) estimating patient stress levels during immersive rehabilitation therapy, and 3) quantifying the patient experience during emergency neonatal transport. The TEA framework provides a reliable and intuitive solution for temporal event annotation that accounts for the unique experimental requirements of each study.},
	booktitle = {Proceedings of the 28th {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Bekele, Amente and Samuel, Joe and Nizami, Shermeen and Basharat, Amna and Giffen, Randy and Green, James R.},
	year = {2018},
	note = {event-place: Markham, Ontario, Canada},
	keywords = {healthcare, data model, data entry and integration, life and medical sciences, medical event annotations, mobile applications},
	pages = {309--314},
}

@inproceedings{nimpattanavong_am_2023,
	address = {New York, NY, USA},
	series = {{IAIT} '23},
	title = {Am {I} {Fighting} {Well}? {Fighting} {Game} {Commentary} {Generation} {With} {ChatGPT}},
	isbn = {979-8-4007-0849-7},
	url = {https://doi.org/10.1145/3628454.3629551},
	doi = {10.1145/3628454.3629551},
	abstract = {This paper presents a new approach for leveraging ChatGPT in fighting game commentary generation task. Commentary generation often relies on deep learning techniques, which typically demand extensive data to achieve effectiveness. Large language models (LLMs) have become essential due to their remarkable ability to process data efficiently, thanks to their extensive training on vast datasets. Our proposed approach integrates the use of LLMs, specifically the GPT-3.5 model, for generating commentaries through the utilization of various prompts with data from the open-source fighting game, DareFightingICE. Four prompt variants are employed to assess the effectiveness of each prompt components. Objective evaluation using natural language metrics reveals that different prompt components significantly affect the generated commentaries. Additionally, subjective evaluation through a questionnaire reveals that prompts without parameter definitions received the highest preference from human evaluators. These results suggest that LLMs exhibit versatility in generating fighting game commentaries and hold promise for broader applications.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Advances} in {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Nimpattanavong, Chollakorn and Taveekitworachai, Pittawat and Khan, Ibrahim and Nguyen, Thai Van and Thawonmas, Ruck and Choensawat, Worawat and Sookhanaphibarn, Kingkarn},
	year = {2023},
	note = {event-place: Bangkok, Thailand},
	keywords = {ChatGPT, Prompt Engineering, Commentary Generation, DareFightingICE, Fighting Game},
}

@inproceedings{saad_building_2020,
	address = {New York, NY, USA},
	series = {{iiWAS2019}},
	title = {Building a {Semantic} {Model} for {Linking} and {Visualizing} {Patent} {Citations} ({SeMViPaC})},
	isbn = {978-1-4503-7179-7},
	url = {https://doi.org/10.1145/3366030.3366110},
	doi = {10.1145/3366030.3366110},
	abstract = {Patents are a high-quality resource of information that is currently insufficiently leveraged. In times when continuously rising prices for basic knowledge access threaten to throttle academic research everywhere, this is a resource that can not be longer neglected. Thus, in the project we plan to extract and semantically describe citations from patents. Furthermore, we will establish linking and data integration between disparate data sources, i.e. linking patents with resources in the LOD (Linked Open Data) cloud. Based on the developed semantic citation model a visualization tool to explore and gain better insight and understanding of the extracted citation data will be developed and made available to the public.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Saad, Farag and Hackl-Sommer, Rene},
	year = {2020},
	note = {event-place: Munich, Germany},
	keywords = {Semantic, Visualization, Exploration, Patents, Citations},
	pages = {645--648},
}

@article{kalra_ontology-based_2018,
	title = {Ontology-based framework for internal-external quality trade-offs and tenant management in multi-tenant applications},
	volume = {17},
	issn = {1559-6915},
	url = {https://doi.org/10.1145/3183628.3183632},
	doi = {10.1145/3183628.3183632},
	abstract = {Software Quality Attributes (QAs) can be categorized as either internal to the system as experienced by the developers or external to the system perceived by the end users. These QA categories have trade-off among them - an emphasis on internal QA may result in a compromise of an external QA. For example, there is a trade-off between maintainability and performance. Model-driven development approaches manage this trade-off and increase the degree of internal QA maintainability. In this work, we propose an ontology-based communication mechanism among software components to handle the trade-off. The approach increases the degree of internal QAs such as modifiability, maintainability, testability during the design and development phases without compromising the external QAs for the end users during the operation phase. We also evaluate a prototype system to validate the proposed approach using Software Architecture Analysis Method (SAAM). It is also easier to integrate into the software development lifecycle as compared to existing model-driven approaches. The internal quality attributes become more significant in a multi-tenant scenario than conventional software. It requires managing dynamic requirements of tenants continuously. The proposed approach also useful in such scenario to reduce the maintenance overhead without compromising the degree of multi-tenancy.},
	number = {4},
	journal = {SIGAPP Appl. Comput. Rev.},
	author = {Kalra, Sumit and Prabhakar, T. V.},
	month = jan,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {external quality attributes, internal quality attributes, multi-tenant, quality attributes trade-off, software product quality attributes},
	pages = {46--58},
}

@inproceedings{kumar_fuzzy_2020,
	address = {New York, NY, USA},
	series = {{ICIIT} '20},
	title = {Fuzzy {Logic} based {Hybrid} {Model} for {Automatic} {Extractive} {Text} {Summarization}},
	isbn = {978-1-4503-7659-4},
	url = {https://doi.org/10.1145/3385209.3385235},
	doi = {10.1145/3385209.3385235},
	abstract = {In the contemporary age of information, accessing data becomes easy, but finding knowledge is very difficult. The participation \&amp; publishing of information has consequently escalated the suffering of 'Information Glut.' Assisting users' informational searches with reduced reading or surfing time by extracting and evaluating accurate, authentic \&amp; relevant information are the primary concerns in the present milieu. Automatic text summarization condenses an original document into a shorter form to create a smaller, compact version from the abundant information that is available, preserving the content \&amp; meaning such that it meets the needs of the user. Though many summarization techniques have been proposed, there are no 'silver bullets' to achieve the superlative results as of human-generated summaries. Fuzzy Logic has appeared as a robust theoretical framework for studying human reasoning. A new hybrid model based on fuzzy logic has been proposed using two graph-based techniques named TextRank and LexRank and one semantic-based technique named Latent semantic analysis (LSA). The techniques are evaluated on the Opinosis dataset using 'ROUGE-1' (Recall-Oriented Understudy for Gisting Evaluation-1) and 'time to extract the keywords.' The proposed technique has outperformed the existing techniques when compared with the results given by the original studies.},
	booktitle = {Proceedings of the 2020 5th {International} {Conference} on {Intelligent} {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Kumar, Akshi and Sharma, Aditi and Nayyar, Anand},
	year = {2020},
	note = {event-place: Hanoi, Viet Nam},
	keywords = {Fuzzy logic, Automatic text summarization, Extractive Text summarization, Hybrid Model, LexRank, LSA, TextRank},
	pages = {7--15},
}

@inproceedings{merono-penuela_multi-domain_2021,
	address = {New York, NY, USA},
	series = {K-{CAP} '21},
	title = {Multi-domain and {Explainable} {Prediction} of {Changes} in {Web} {Vocabularies}},
	isbn = {978-1-4503-8457-5},
	url = {https://doi.org/10.1145/3460210.3493583},
	doi = {10.1145/3460210.3493583},
	abstract = {Web vocabularies (WV) have become a fundamental tool for structuring Web data: over 10 million sites use structured data formats and ontologies to markup content. Maintaining these vocabularies and keeping up with their changes are manual tasks with very limited automated support, impacting both publishers and users. Existing work shows that machine learning can be used to reliably predict vocabulary changes, but on specific domains (e.g. biomedicine) and with limited explanations on the impact of changes (e.g. their type, frequency, etc.). In this paper, we describe a framework that uses various supervised learning models to learn and predict changes in versioned vocabularies, independent of their domain. Using well-established results in ontology evolution we extract domain-agnostic and human-interpretable features and explain their influence on change predictability. Applying our method on 139 WV from 9 different domains, we find that ontology structural and instance data, the number of versions, and the release frequency highly correlate with predictability of change. These results can pave the way towards integrating predictive models into knowledge engineering practices and methods.},
	booktitle = {Proceedings of the 11th {Knowledge} {Capture} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Meroño-Peñuela, Albert and Pernisch, Romana and Guéret, Christophe and Schlobach, Stefan},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {ontology evolution, change modelling, vocabulary change},
	pages = {193--200},
}

@inproceedings{barria-pineda_explaining_2019,
	address = {New York, NY, USA},
	series = {{UMAP}'19 {Adjunct}},
	title = {Explaining {Need}-based {Educational} {Recommendations} {Using} {Interactive} {Open} {Learner} {Models}},
	isbn = {978-1-4503-6711-0},
	url = {https://doi.org/10.1145/3314183.3323463},
	doi = {10.1145/3314183.3323463},
	abstract = {Students might pursue different goals throughout their learning process. For example, they might be seeking new material to expand their current level of knowledge, repeating content of prior classes to prepare for an exam, or working on addressing their most recent misconceptions. Multiple potential goals require an adaptive e-learning system to recommend learning content appropriate for students' intent and to explain this recommendation in the context of this goal. In our prior work, we explored explainable recommendations for the most typical 'knowledge expansion goal". In this paper, we focus on students' immediate needs to remedy misunderstandings when they solve programming problems. We generate learning content recommendations to target the concepts with which students have struggled more recently. At the same time, we produce explanations for this recommendation goal in order to support students' understanding of why certain learning activities are recommended. The paper provides an overview of the design of this explainable educational recommender system and describes its ongoing evaluation},
	booktitle = {Adjunct {Publication} of the 27th {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Barria-Pineda, Jordan and Akhuseyinoglu, Kamil and Brusilovsky, Peter},
	year = {2019},
	note = {event-place: Larnaca, Cyprus},
	keywords = {educational recommender systems, explanations, open learner models},
	pages = {273--277},
}

@inproceedings{zhao_introduce_2025,
	address = {New York, NY, USA},
	series = {Websci {Companion} '25},
	title = {Introduce an {Auditing} {Layer} to {Web} {Science}},
	isbn = {979-8-4007-1535-8},
	url = {https://doi.org/10.1145/3720554.3736184},
	doi = {10.1145/3720554.3736184},
	abstract = {Scientific discoveries increasingly depend on data and data processing, and Web Science is no exception. As an established practice, data-intensive research typically uses scientific workflows and provenance to facilitate data and method sharing while automatically preserving processing history. Prior research has reported the possibility of ex-post policy-based compliance checking from provenance data. Based on these works, in this paper, we present the conceptual design of a framework of data-harvesting Web Science practices, especially by introducing a common auditing layer. We discuss the framework’s practical, scientific, and ethical advantages, including its applicability in the period of large language model (LLM), autonomous agent, and artificial intelligence (AI) explosion. We hope this framework design can incubate a new norm for research practice to be transparent, ethical, and lightweight.},
	booktitle = {Companion {Publication} of the 17th {ACM} {Web} {Science} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Rui and Wright, Jesse},
	year = {2025},
	keywords = {transparency, usage control, audit, Data governance},
	pages = {49--53},
}

@article{pradhan_phantom_2019,
	title = {"{Phantom} {Friend}" or "{Just} a {Box} with {Information}": {Personification} and {Ontological} {Categorization} of {Smart} {Speaker}-based {Voice} {Assistants} by {Older} {Adults}},
	volume = {3},
	url = {https://doi.org/10.1145/3359316},
	doi = {10.1145/3359316},
	abstract = {As voice-based conversational agents such as Amazon Alexa and Google Assistant move into our homes, researchers have studied the corresponding privacy implications, embeddedness in these complex social environments, and use by specific user groups. Yet it is unknown how users categorize these devices: are they thought of as just another object, like a toaster? As a social companion? Though past work hints to human-like attributes that are ported onto these devices, the anthropomorphization of voice assistants has not been studied in depth. Through a study deploying Amazon Echo Dot Devices in the homes of older adults, we provide a preliminary assessment of how individuals 1) perceive having social interactions with the voice agent, and 2) ontologically categorize the voice assistants. Our discussion contributes to an understanding of how well-developed theories of anthropomorphism apply to voice assistants, such as how the socioemotional context of the user (e.g., loneliness) drives increased anthropomorphism. We conclude with recommendations for designing voice assistants with the ontological category in mind, as well as implications for the design of technologies for social companionship for older adults.},
	number = {CSCW},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Pradhan, Alisha and Findlater, Leah and Lazar, Amanda},
	month = nov,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, older adults, anthropomorphism, personification, smart speakers, voice assistants},
}

@inproceedings{pouri_relevance_2020,
	address = {New York, NY, USA},
	series = {{ICT4S2020}},
	title = {The {Relevance} of {Digital} {Sharing} {Business} {Models} for {Sustainability}},
	isbn = {978-1-4503-7595-5},
	url = {https://doi.org/10.1145/3401335.3401344},
	doi = {10.1145/3401335.3401344},
	abstract = {There is a growing discussion about the "Digital Sharing Economy" (DSE). The pervasiveness of digital platforms and the growing interest in a sharing (rather than ownership) style of consumption have allowed for sharing practices to scale up and become a widespread phenomenon. Digital sharing platforms offer a wide variety of services which appear to be more affordable, efficient, and accessible than their conventional counterparts, making them more attractive in the eyes of consumers. The DSE has manifested itself most remarkably in consumer-to-consumer (C2C) and business-to-consumer (B2C) sharing models. New business models have been created to capture and offer the values driving the emerging sharing trend.The innovative, digitally enabled mode of providing access to resources as a service in the DSE has changed consumption patterns both at micro level, as a change in individual lifestyles, and at macro level, manifested in a transformation of socio-economic structures. These ongoing changes may have both positive and negative implications for society from a sustainability perspective. Recognising that the (potential and actual) impacts of sharing platforms on sustainability have not been studied in a systematic way yet, the present paper aims to develop a systematic insight into this interaction by focusing on the business models emerging around sharing platforms as a central starting point. To achieve this, we use a typology of business models that recognizes the affordances and key attributes of sharing in the DSE. The typology covers both C2C and B2C models of sharing. Based on this typology, we discuss the implications of each type of sharing model for sustainability by asking two central questions: How may the given type of sharing affect resource consumption? And what will be the potential impacts on social practices and structures? We hope that the present study can serve as a guideline for assessing the sustainability impacts of sharing platforms – either already operating in the market or envisaged. By highlighting the aspects most relevant from a sustainability point of view, we expect to contribute to an evolution of the DSE business models towards sustainable development.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {ICT} for {Sustainability}},
	publisher = {Association for Computing Machinery},
	author = {Pouri, Maria J. and Hilty, Lorenz M.},
	year = {2020},
	note = {event-place: Bristol, United Kingdom},
	keywords = {Digital sharing economy, Information and communication technology (ICT), Resource consumption, Sharing business models, Sharing platforms, Socio-economic structures, Sustainability impacts},
	pages = {77--87},
}

@article{mishra_crowdsourcing_2021,
	title = {Crowdsourcing and {Evaluating} {Concept}-driven {Explanations} of {Machine} {Learning} {Models}},
	volume = {5},
	url = {https://doi.org/10.1145/3449213},
	doi = {10.1145/3449213},
	abstract = {An important challenge in building explainable artificially intelligent (AI) systems is designing interpretable explanations. AI models often use low-level data features which may be hard for humans to interpret. Recent research suggests that situating machine decisions in abstract, human understandable concepts can help. However, it is challenging to determine the right level of conceptual mapping. In this research, we explore granularity (of data features) and context (of data instances) as dimensions underpinning conceptual mappings. Based on these measures, we explore strategies for designing explanations in classification models. We introduce an end-to-end concept elicitation pipeline that supports gathering high-level concepts for a given data set. Through crowd-sourced experiments, we examine how providing conceptual information shapes the effectiveness of explanations, finding that a balance between coarse and fine-grained explanations help users better estimate model predictions. We organize our findings into systematic themes that can inform design considerations for future systems.},
	number = {CSCW1},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Mishra, Swati and Rzeszotarski, Jeffrey M.},
	month = apr,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {machine learning, classification, concepts, explanations},
}

@article{guittoum_leveraging_2023,
	title = {Leveraging {Semantic} {Technologies} for {Collaborative} {Inference} of {Threatening} {IoT} {Dependencies}},
	volume = {23},
	issn = {1559-6915},
	url = {https://doi.org/10.1145/3626307.3626310},
	doi = {10.1145/3626307.3626310},
	abstract = {IoT Device Management (DM) refers to the remote administration of customer devices. In practice, DM is ensured by multiple actors such as operators or device manufacturers, each operating independently via their DM solution. These siloed DM solutions are limited in addressing IoT threats related to device dependencies, such as cascading failures, as these threats spread across devices managed by different DM actors, and their mitigation can no longer be performed without collaborative DM efforts. The first step toward collaborative mitigation of these threats is the identification of threatening dependency topology. However, this task is challenging, requiring the inference of dependencies from the data held by different actors. In this work, we propose a collaborative framework that infers the threatening topology of dependencies by accessing and aggregating data from legacy DM solutions. It combines the assets of Semantic Web standards and Digital Twin technology to capture on-demand the topology of dependencies, and it is designed to be used in business applications such as customer care to enhance customer Quality of Experience. We integrate our solution within the in-use Orange's Digital Twin platform Thing in the future and demonstrate its effectiveness by automatically inferring threatening dependencies in the two settings: a simulated smart home scenario managed by ground-truth DM solutions, such as Orange's implementation of the USP Controller and Samsung's SmartThings Platform, and a realistic smart home called DOMUS testbed.},
	number = {3},
	journal = {SIGAPP Appl. Comput. Rev.},
	author = {Guittoum, Amal and Aïssaoui, François and Bolle, Sébastien and Boyer, Fabienne and De Palma, Noel},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, SHACL, semantic web, digital twin, inference, collaboration, dependencies management, entity resolution, IoT device management, thing description},
	pages = {32--48},
}

@inproceedings{queiroz_evaluating_2018,
	address = {New York, NY, USA},
	series = {{IHC} '18},
	title = {Evaluating {Usability} of {IFML} {Models}: {How} {Usability} is {Perceived} and {Propagated}},
	isbn = {978-1-4503-6601-4},
	url = {https://doi.org/10.1145/3274192.3274213},
	doi = {10.1145/3274192.3274213},
	abstract = {System acceptance is strongly related to its usability. It is important that the usability is carefully designed during the system design steps and later propagated to the interface. This care avoids reworking the interface design of an application in the future because of its usability. However, we did not find works that specifically addresses the modeling of interfaces in conjunction with usability. The Interaction Flow Modeling Language (IFML) is a proposal that supports the modeling of the interface. This work investigates the following research question: "usability in IFML models is perceived and propagated to the final interface?" In order to answer this research question, we performed an empirical study to evaluate how usability is perceived by participants through IFML models and if usability is propagated to an interface prototype. The study showed that not all aspects of usability are easily perceived and propagated in the interface through IFML models.},
	booktitle = {Proceedings of the 17th {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Queiroz, Randerson and Marques, Anna Beatriz and Lopes, Adriana and Oliveira, Edson and Conte, Tayana},
	year = {2018},
	note = {event-place: Belém, Brazil},
	keywords = {IFML, Usability, Empirical studies, User Interface, FUFs, Interface Model},
}

@article{lara_refactoring_2018,
	title = {Refactoring {Multi}-{Level} {Models}},
	volume = {27},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3280985},
	doi = {10.1145/3280985},
	abstract = {Multi-level modelling promotes flexibility in modelling by enabling the use of several meta-levels instead of just two, as is the case in mainstream two-level modelling approaches. While this approach leads to simpler models for some scenarios, it introduces an additional degree of freedom as designers can decide the meta-level where an element should reside, having to ascertain the suitability of such decisions.In this respect, model refactorings have been successfully applied in the context of two-level modelling to rearrange the elements of a model while preserving its meaning. Following this idea, we propose a catalogue of 17 novel refactorings specific to multi-level models. Their objective is to help designers in rearranging elements across and within meta-levels and exploring the consequences. In this article, we detail each refactoring in the catalogue, show a classification across different dimensions, and describe the support we provide in our MetaDepth tool. We present two experiments to assess two aspects of our refactorings. The first one validates the predicted semantic side effects of the refactorings on the basis of more than 210.000 refactoring applications. The second one measures the impact of refactorings on three quality attributes of multi-level models.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Lara, Juan De and Guerra, Esther},
	month = nov,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Meta-modelling, MetaDepth, multi-level modelling, model refactoring},
}

@inproceedings{liu_resume_2021,
	address = {New York, NY, USA},
	series = {{ICBDC} '21},
	title = {Resume {Parsing} based on {Multi}-label {Classification} using {Neural} {Network} models},
	isbn = {978-1-4503-8980-8},
	url = {https://doi.org/10.1145/3469968.3469998},
	doi = {10.1145/3469968.3469998},
	abstract = {Application for jobs usually brings much work for both appliers and HR. Appliers want to apply for the jobs which they are most suitable. The number of applications for a particular position can be significant, making the candidates’ selection cumbersome for HR. Nowadays, hiring processes are often conducted through the Virtual mode with emails. This creates chances for analyzing the data in the resume. Therefore, to enhance selection problems’ efficiency, resume parsing algorithms have been developed in recent years to predict resume-based skills or good jobs quickly. The artificial neural network is a hot spot in the field of artificial intelligence since the 1980s. It abstracts the human brain's neural network from the angle of information processing, establishes some simple models, and forms different networks according to different connection modes. In recent years, neural networks-based algorithms perform high efficiency in processing text classification. This paper put forward some of the efficient algorithms used in text classification, Like BPNN, CNN, BiLSTM, and CRNN, for resume parsing. The original resumes are parsed by splitting them into words, and word base is trained to get the most appropriate word, which has a high score in the resume is resulting suitable job for each resume. The CRNN performs best in resume parsing, which the accuracy can reach 96\%. CNN places the lowest accuracy. The BPNN achieves good accuracy but brings inflexible.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Big} {Data} and {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Jiahao and Shen, Yifan and Zhang, Yijie and krishnamoorthy, Sujatha},
	year = {2021},
	note = {event-place: Shenzhen, China},
	keywords = {Bi-LSTM, Neural Network, BPNN, Resume Parsing},
	pages = {177--185},
}

@inproceedings{edgar_sustainont_2020,
	address = {New York, NY, USA},
	series = {{SBD} '20},
	title = {{SustainOnt}: an ontology for defining an index of neighborhood sustainability across domains},
	isbn = {978-1-4503-7974-8},
	url = {https://doi.org/10.1145/3391274.3393640},
	doi = {10.1145/3391274.3393640},
	abstract = {Massive amounts of data, both structured and unstructured, are available to be harvested for competitive business advantage, sound government policies, and new insights in a broad array of applications. This paper specifically focuses on extraction, integration, and querying of open data available about environmental sustainability. The global trend toward urbanization has created a need for residents of urban neighborhoods to better understand the factors impacting the social, environmental, and economic sustainability of an area. To date, there is no concise representation of all aspects of sustainability. This paper aims to fill this gap. A version of sustainability resting on economic, societal, and environmental development as the three main indicators was chosen to inform an ontology called SustainOnt used to organize and analyze relevant data from various sources. The newly-linked data is made available through a dual-platform application aimed at reaching a wide array of audiences. An initial prototype has been designed, using data for a small region, to provide a sustainability index of each city and/or neighborhood area that can be more accessible to people without the means to directly analyze the available data.},
	booktitle = {Proceedings of {The} {International} {Workshop} on {Semantic} {Big} {Data}},
	publisher = {Association for Computing Machinery},
	author = {Edgar, Vatricia and La Place, Cecilia and Schmidt, Julia and Bansal, Ajay and Bansal, Srividya},
	year = {2020},
	note = {event-place: Portland, Oregon},
	keywords = {ontology, sustainability, data integration, linked open data},
}

@inproceedings{rashid_music_2018,
	address = {New York, NY, USA},
	series = {{SAAM} '18},
	title = {A {Music} {Theory} {Ontology}},
	isbn = {978-1-4503-6495-9},
	url = {https://doi.org/10.1145/3243907.3243913},
	doi = {10.1145/3243907.3243913},
	abstract = {Many existing music ontologies have focused on expressing metadata related to performances or recordings, aiding with recommendations of songs or artists, and studying the psychological affects of music. These music ontologies provide a foundation for describing many practical aspects related to music. We believe further primitives are needed in order to represent written music and provide a foundation for performing analysis of music. We are motivated by questions related to analyzing music that might inform composers or musicians. Informational elements may include possible underlying chords from a set of notes, as well as summaries of key signatures or scales used in a given song. In order to leverage Semantic Web technologies to answer such questions, we present our Music Theory Ontology that expands on existing work by including theoretical concepts that were absent from previous music ontologies. We further describe a methodology for using the ontology to infer new knowledge. We demonstrate this capability by inferring the notes in various scales and chords, and evaluate the ontology in terms of competency question answering.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Semantic} {Applications} for {Audio} and {Music}},
	publisher = {Association for Computing Machinery},
	author = {Rashid, Sabbir M. and De Roure, David and McGuinness, Deborah L.},
	year = {2018},
	note = {event-place: Monterey, CA, USA},
	pages = {6--14},
}

@inproceedings{kousha_democratizing_2023,
	address = {New York, NY, USA},
	series = {{SC}-{W} '23},
	title = {Democratizing {HPC} {Access} and {Use} with {Knowledge} {Graphs}},
	isbn = {979-8-4007-0785-8},
	url = {https://doi.org/10.1145/3624062.3624094},
	doi = {10.1145/3624062.3624094},
	abstract = {The field of High-Performance Computing (HPC) is undergoing rapid evolution, with an expanding and diverse user base harnessing its unparalleled computational capabilities. As the range of HPC applications grows, newcomers to the field are faced with the daunting task of optimizing their applications for efficient execution on HPC systems. Traditional documentation, often spanning dozens of pages, is cumbersome for finding answers and ill-suited for integration with emerging conversational AI-powered user interfaces like chatbots. Addressing this challenge, we propose a novel HPC ontology crafted to encapsulate HPC runtime relations in a scalable fashion. Our proposed ontology not only facilitates the transfer and querying of this knowledge but also serves as a foundational pillar for our AI-powered Speech Assistant Interface (SAI)[13]. This ensures reproducibility, reliability, and optimal performance when executing tasks. In this paper, we elucidate the relationships and properties underpinning our ontology and showcase how users can interact with knowledge graphs based on our proposed ontology to derive insights.},
	booktitle = {Proceedings of the {SC} '23 {Workshops} of the {International} {Conference} on {High} {Performance} {Computing}, {Network}, {Storage}, and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Kousha, Pouya and Sathu, Vivekananda and Lieber, Matthew and Subramoni, Hari and Panda, Dhabaleswar K.},
	year = {2023},
	note = {event-place: Denver, CO, USA},
	keywords = {Ontology, Knowledge Graph, HPC, Documentation},
	pages = {243--251},
}

@inproceedings{tudhope_20th_2022,
	address = {New York, NY, USA},
	series = {{JCDL} '22},
	title = {20th {European} {NKOS} workshop: networked knowledge organization systems and services},
	isbn = {978-1-4503-9345-4},
	url = {https://doi.org/10.1145/3529372.3530915},
	doi = {10.1145/3529372.3530915},
	abstract = {The workshop will explore the potential of Knowledge Organization Systems (KOS), such as classification systems, taxonomies, thesauri, ontologies, and lexical databases, in the context of current developments and possibilities. These tools help model the underlying semantic structure of a domain for purposes of information retrieval, knowledge discovery, language engineering, etc. The workshop provides an opportunity to discuss projects, research and development activities, evaluation approaches, lessons learned, and research findings. The main theme of the workshop is Designing for Cultural Hospitality and Indigenous Knowledge in KOS.},
	booktitle = {Proceedings of the 22nd {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {Association for Computing Machinery},
	author = {Tudhope, Douglas and Gnoli, Claudio and Golub, Koraljka and Mayr, Philipp},
	year = {2022},
	note = {event-place: Cologne, Germany},
	keywords = {ontologies, knowledge organization systems, classification systems, taxonomies, terminology services, thesauri, vocabulary mapping},
}

@inproceedings{benabdellah_agent_2021,
	address = {New York, NY, USA},
	series = {{NISS} '21},
	title = {An agent organizational method for modeling the complexity of the design process},
	isbn = {978-1-4503-8871-9},
	url = {https://doi.org/10.1145/3454127.3456595},
	doi = {10.1145/3454127.3456595},
	abstract = {The management of the design process is a challenging mission; and most researchers would argue that design is linked to intentional action and it cannot emerge out of complexity. In fact, the interactions between processes, operators, and activities define an unexpected emergent behavior, which is based on complex assumptions such as non-linearity, dynamic and adaptive firm behavior. Therefore, we need a complex thinking. This article proposes to explore how we may deepen our understanding of design process as a complex adaptive system. In fact, this new understanding creates a quite challenge for researches to develop appropriate tools to support design reasoning and decision-making. In this respect, the aim of this paper is first to define the complexity of design process as a complexity of system, by matching its characteristics with those of complex adaptive systems (CAS). Second, the paper provides an agent organizational modelization of the design process in order to support its complexity by following the ASPECS methodology which is an agent-oriented software process for engineering complex systems as well as the knowledge identification of the design process using the RIOCK meta-model.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Networking}, {Information} {Systems} \&amp; {Security}},
	publisher = {Association for Computing Machinery},
	author = {Benabdellah, Abla Chaouni and Benghabrit, Asmaa and Bouhaddou, Imane and Zekhnini, Kamar},
	year = {2021},
	note = {event-place: KENITRA, AA, Morocco},
}

@article{jia_taxonomy_2023,
	title = {Taxonomy of {Abstractive} {Dialogue} {Summarization}: {Scenarios}, {Approaches}, and {Future} {Directions}},
	volume = {56},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3622933},
	doi = {10.1145/3622933},
	abstract = {Abstractive dialogue summarization generates a concise and fluent summary covering the salient information in a dialogue among two or more interlocutors. It has attracted significant attention in recent years based on the massive emergence of social communication platforms and an urgent requirement for efficient dialogue information understanding and digestion. Different from news or articles in traditional document summarization, dialogues bring unique characteristics and additional challenges, including different language styles and formats, scattered information, flexible discourse structures, and unclear topic boundaries. This survey provides a comprehensive investigation of existing work for abstractive dialogue summarization from scenarios, approaches to evaluations. It categorizes the task into two broad categories according to the type of input dialogues, i.e., open-domain and task-oriented, and presents a taxonomy of existing techniques in three directions, namely, injecting dialogue features, designing auxiliary training tasks, and using additional data. A list of datasets under different scenarios and widely accepted evaluation metrics are summarized for completeness. After that, the trends of scenarios and techniques are summarized, together with deep insights into correlations between extensively exploited features and different scenarios. Based on these analyses, we recommend future directions, including more controlled and complicated scenarios, technical innovations and comparisons, publicly available datasets in special domains, and so on.},
	number = {3},
	journal = {ACM Comput. Surv.},
	author = {Jia, Qi and Liu, Yizhu and Ren, Siyu and Zhu, Kenny Q.},
	month = oct,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {abstractive summarization, dialogue context modeling, Dialogue summarization},
}

@inproceedings{zhang_research_2024,
	address = {New York, NY, USA},
	series = {{CMNM} '24},
	title = {Research on named entity recognition in the field of {CNC} machine tool design based on deep {learningKnowledge} map of mechanical field},
	isbn = {979-8-4007-0976-0},
	url = {https://doi.org/10.1145/3677779.3677821},
	doi = {10.1145/3677779.3677821},
	abstract = {Our goal is to extract entities from the text data of unstructured CNC machine tool design for the construction of knowledge graph. The key entity extraction problem in the construction of CNC machine tool design knowledge graph is studied. In order to realize the recognition of named entities, we have formulated the standard and labeling method of knowledge classification for the field of CNC machine tools, and constructed the corresponding domain data set. In addition, we also propose an entity recognition technology based on RoBertTa-BiLSTM-LCRF for CNC machine tool design text. Firstly, we fine-tune the RoBertTa-BiLSTM-LCRF model using data sets in the field of CNC machine tools, and then use RoBERTa to encode the text to generate a vector representation ; next, we use bidirectional long short-term memory ( BiLSTM ) to extract the features of vectors. Finally, we introduce LCRF as the overall optimization layer of the label, so as to derive the best answer and label the entity.The experimental results show that the F1 value of the model in the data set reaches 71.16 \% ; for most of the key entities, the value of F1 exceeds 65 \% ; this method shows significant advantages in the entity recognition of CNC machine tool design knowledge. It can accurately identify the core entities in the machine tool design knowledge document, and provides a solid data support for the construction of CNC machine tool design knowledge graph.},
	booktitle = {Proceedings of the {International} {Conference} on {Modeling}, {Natural} {Language} {Processing} and {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Shuai and Guan, Yanzhi and Gu, Zhongyu},
	year = {2024},
	note = {event-place: Xi'an, China},
	pages = {257--262},
}

@inproceedings{das_swain_modeling_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Modeling {Organizational} {Culture} with {Workplace} {Experiences} {Shared} on {Glassdoor}},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376793},
	doi = {10.1145/3313831.3376793},
	abstract = {Organizational culture (OC) encompasses the underlying beliefs, values, and practices that are unique to an organization. However, OC is inherently subjective and a coarse construct, and therefore challenging to quantify. Alternatively, self-initiated workplace reviews on online platforms like Glassdoor provide the opportunity to leverage the richness of language to understand OC. In as much, first, we use multiple job descriptors to operationalize OC as a word vector representation. We validate this construct with language used in 650k different Glassdoor reviews. Next, we propose a methodology to apply our construct on Glassdoor reviews to quantify the OC of employees by sector. We validate our measure of OC on a dataset of 341 employees by providing empirical evidence that it helps explain job performance. We discuss the implications of our work in guiding tailored interventions and designing tools for improving employee functioning.},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Das Swain, Vedant and Saha, Koustuv and Reddy, Manikanta D. and Rajvanshy, Hemang and Abowd, Gregory D. and De Choudhury, Munmun},
	year = {2020},
	note = {event-place: Honolulu, HI, USA},
	keywords = {social media, glassdoor, organizational culture, wordvector},
	pages = {1--15},
}

@inproceedings{cederbladh_automation_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Automation {Support} for {System} {Simulation} and {Architecture} {Layout} {Design} in {Cyber}-{Physical} {Systems} {Engineering}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3686250},
	doi = {10.1145/3652620.3686250},
	abstract = {Simulations have long been part of hardware-centric system domains. Similarly, architecture design is a common practice for complex industrial systems, which comprise many components that can be arranged in different layouts according to given requirements. Configuring simulation models and choosing the architecture design can be time-consuming activities. This paper presents a model-driven approach to automate the simulation configuration and architecture layouting engineering activities by leveraging model-driven optimisation techniques. The approach leverages a research solution, MOMoT (Marrying Optimisation and Model Transformations), an academic tool that combines search-based algorithms and model transformations. MOMoT is extended with two software modules, leveraging the Functional Mock-up Interface standard for simulation configuration and an architectural description language to design architecture layouts. Our solution is presented in the context of Volvo Construction Equipment's industrial use case, which is part of the European-funded project AIDOaRt. Our approach contributes to automated decision support to simulation and architecture design through model-driven optimisation while preserving the organisation's engineering practices.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cederbladh, Johan and Eisenberg, Martin and Berardinelli, Luca and Bilic, Damir},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {simulation, models, architecture, layout, optimisation},
	pages = {299--310},
}

@article{ansar_texim_2025,
	title = {{TexIm} {FAST}: {Text}-to-{Image} {Encoding} for {Semantic} {Similarity} {Evaluation} of {Disproportionate} {Sequences}},
	volume = {21},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3735974},
	doi = {10.1145/3735974},
	abstract = {One of the principal objectives of Natural Language Processing (NLP) is to generate meaningful representations from text. Improving the informativeness of the representations has led to a tremendous rise in the dimensionality and the memory footprint. It leads to a cascading effect amplifying the complexity of the downstream model by increasing its parameters. The available techniques cannot be applied to cross-modal applications such as text-to-image. To ameliorate these issues, a novel Text-to-Image Fixed-dimensional encoding technique through a self-supervised Variational Auto-Encoder (VAE) for semantic evaluation applying transformers (TexIm FAST) has been proposed in this article. The pictorial representations allow oblivious inference while retaining the linguistic intricacies and are potent in cross-modal applications. TexIm FAST deals with variable-length sequences and generates uniform-dimensional images with over 75\% reduced memory footprint. It enhances the efficiency of the models for downstream tasks by reducing its parameters. The efficacy of TexIm FAST has been extensively analyzed for the task of Semantic Textual Similarity (STS) on a benchmark dataset and two new datasets put forth containing disproportionate sequences. The results demonstrate its exceptional ability to compare disparate-length sequences such as a text with its summary with 3\% improvement in accuracy compared to the SOTA despite having 68\% less parameters.},
	number = {6},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {Ansar, Wazib and Goswami, Saptarsi and Chakrabarti, Amlan and Chakraborty, Basabi},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Semantic Similarity, Text-to-Image, Oblivious Inference, Text Embedding, Transformers NLP, Variational Auto-Encoder},
}

@inproceedings{oliveira_model-driven_2018,
	address = {New York, NY, USA},
	series = {{WebMedia} '18},
	title = {A {Model}-{Driven} {Approach} to {Evolve} {Recommender} {Systems}},
	isbn = {978-1-4503-5867-5},
	url = {https://doi.org/10.1145/3243082.3267457},
	doi = {10.1145/3243082.3267457},
	abstract = {Recommender systems have become an important issue on Web applications, but its research is usually focused on algorithms and data optimization. However, as the recommendation techniques improve and these systems become more commonly used in software applications, there is the need of easily adapt and evolve them. To address this need, we propose a model-driven approach to evolve recommender systems and present an architecture solution from our research, using an events management system as the domain for an use-case scenario. Future work might demonstrate the architecture feasibility.},
	booktitle = {Proceedings of the 24th {Brazilian} {Symposium} on {Multimedia} and the {Web}},
	publisher = {Association for Computing Machinery},
	author = {Oliveira, Yuri and Silveira, Leonardo and Souza, Cidcley},
	year = {2018},
	note = {event-place: Salvador, BA, Brazil},
	keywords = {software architecture, model-driven engineering, recommender systems},
	pages = {169--172},
}

@article{omar_universal_2023,
	title = {A {Universal} {Question}-{Answering} {Platform} for {Knowledge} {Graphs}},
	volume = {1},
	url = {https://doi.org/10.1145/3588911},
	doi = {10.1145/3588911},
	abstract = {Knowledge from diverse application domains is organized as knowledge graphs (KGs) that are stored in RDF engines accessible in the web via SPARQL endpoints. Expressing a well-formed SPARQL query requires information about the graph structure and the exact URIs of its components, which is impractical for the average user. Question answering (QA) systems assist by translating natural language questions to SPARQL. Existing QA systems are typically based on application-specific human-curated rules, or require prior information, expensive pre-processing and model adaptation for each targeted KG. Therefore, they are hard to generalize to a broad set of applications and KGs. In this paper, we propose KGQAn, a universal QA system that does not need to be tailored to each target KG. Instead of curated rules, KGQAn introduces a novel formalization of question understanding as a text generation problem to convert a question into an intermediate abstract representation via a neural sequence-to-sequence model. We also develop a just-in-time linker that maps at query time the abstract representation to a SPARQL query for a specific KG, using only the publicly accessible APIs and the existing indices of the RDF store, without requiring any pre-processing. Our experiments with several real KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin the state-of-the-art in terms of quality of answers and processing time, especially for arbitrary KGs, unseen during the training.},
	number = {1},
	journal = {Proc. ACM Manag. Data},
	author = {Omar, Reham and Dhall, Ishika and Kalnis, Panos and Mansour, Essam},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {knowledge graphs, RDF, just-in-time entity and relation linking, natural language question answering, seq2seq models},
}

@inproceedings{bohm_towards_2025,
	address = {New York, NY, USA},
	series = {{ECSEE} '25},
	title = {Towards a {Semantic} {Representation} of {Framework} {Recommendations} for {Curricular} {Specifications} in {Higher} {Education}},
	isbn = {979-8-4007-1282-1},
	url = {https://doi.org/10.1145/3723010.3723036},
	doi = {10.1145/3723010.3723036},
	abstract = {Curricular specifications play an important role in the Higher Education sector and the domain of Computer Science and Software Engineering is characterized by a wide range of education programs with a broad range of topic. Therefore, recommendation frameworks play an important role and their usage is beneficial for a unification of education profiles in a systematic way. This research is contributing to this development by exploring how a recommendation for the domain of Business Informatics in German speaking countries can be improved by formalizing the recommendations in a semantic model that relies on sophisticated European ontologies in the domain like the European Learning Model (ELM) and related data models. It employs Generative Artificial Intelligence Systems to create semantic models in an experimental way and evaluates the resulting model quality. The results show that a formalization using GenAI has a high potential, but currently also shows deficits in the correctness of the resulting models, requiring human oversight during the model creation.},
	booktitle = {Proceedings of the 6th {European} {Conference} on {Software} {Engineering} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Böhm, Karsten},
	year = {2025},
	keywords = {Semantic Web, European Learning Model, Higher Education, Business Informatics, Competence Specification, Learning Framework},
	pages = {154--160},
}

@article{boudi_deep_2023,
	title = {A {Deep} {Reinforcement} {Learning} {Framework} with {Formal} {Verification}},
	volume = {35},
	issn = {0934-5043},
	url = {https://doi.org/10.1145/3577204},
	doi = {10.1145/3577204},
	abstract = {Artificial Intelligence (AI) and data are reshaping organizations and businesses. Human Resources (HR) management and talent development make no exception, as they tend to involve more automation and growing quantities of data. Because this brings implications on workforce, career transparency, and equal opportunities, overseeing what fuels AI and analytical models, their quality standards, integrity, and correctness becomes an imperative for those aspiring to such systems. Based on an ontology transformation to B-machines, this article presents an approach to constructing a valid and error-free career agent with Deep Reinforcement Learning (DRL). In short, the agent's policy is built on a framework we called Multi State-Actor (MuStAc) using a decentralized training approach. Its purpose is to predict both relevant and valid career steps to employees, based on their profiles and company pathways (observations). Observations can comprise various data elements such as the current occupation, past experiences, performance, skills, qualifications, and so on. The policy takes in all these observations and outputs the next recommended career step, in an environment set as the combination of an HR ontology and an Event-B model, which generates action spaces with respect to formal properties. The Event-B model and formal properties are derived using OWL to B transformation.},
	number = {1},
	journal = {Form. Asp. Comput.},
	author = {Boudi, Zakaryae and Wakrime, Abderrahim Ait and Toub, Mohamed and Haloua, Mohamed},
	month = mar,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Model Transformation, Event-B, Formal Verification, AI Control, Atelier B, Safe AI, Safe RL},
}

@inproceedings{firsov_useing_2019,
	address = {New York, NY, USA},
	series = {{SIGIR}'19},
	title = {{USEing} {Transfer} {Learning} in {Retrieval} of {Statistical} {Data}},
	isbn = {978-1-4503-6172-9},
	url = {https://doi.org/10.1145/3331184.3331427},
	doi = {10.1145/3331184.3331427},
	abstract = {DSSM-like models showed good results in retrieval of short documents that semantically match the query. However, these models require large collections of click-through data that are not available in some domains. On the other hand, the recent advances in NLP demonstrated the possibility to fine-tune language models and models trained on one set of tasks to achieve a state of the art results on a multitude of other tasks or to get competitive results using much smaller training sets. Following this trend, we combined DSSM-like architecture with USE (Universal Sentence Encoder) and BERT (Bidirectional Encoder Representations from Transformers) models in order to be able to fine-tune them on a small amount of click-through data and use them for information retrieval. This approach allowed us to significantly improve our search engine for statistical data.},
	booktitle = {Proceedings of the 42nd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Firsov, Anton and Bugay, Vladimir and Karpenko, Anton},
	year = {2019},
	note = {event-place: Paris, France},
	keywords = {information retrieval, language model, transfer learning},
	pages = {1391--1392},
}

@article{ozdemir_enhancing_2025,
	title = {Enhancing {Cultural} {Heritage} {Archive} {Analysis} via {Automated} {Entity} {Extraction} and {Graph}-{Based} {Representation} {Learning}},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3746658},
	doi = {10.1145/3746658},
	abstract = {Recent efforts to digitize textual, visual, and physical forms of cultural heritage require advanced tools for preservation and analysis. The availability of extensive online data creates a need for intelligent systems to help users and archivists understand latent relationships in these collections. A major challenge in cultural heritage studies is the labor-intensive process of analyzing these materials. Inconsistent linguistic terms and ambiguous concepts in digital documents make it difficult to uncover relationships without expert supervision. Moreover, while advanced models based on large-scale pretraining demonstrate strong performance in extracting semantic relationships, they depend on extensive pretraining on large external datasets, limiting their applicability for smaller or specialized collections. We propose a system that combines natural language processing for entity extraction with graph representation learning to model relationships among documents, categories, and n-grams, resulting in a fully-connected network representation. Unlike methods requiring large-scale pretraining, our approach operates effectively using only the information available in the dataset itself, making it particularly suited for smaller cultural heritage document collections. The system extracts significant terms from document metadata, produces embeddings for each document, and uses these embeddings to build a recommendation system for entity discovery. We tested the system on a collection of early 20th-century documents from Crete, evaluating its performance against alternative methods in collaboration with experts from the archival research organization SALT. This approach not only facilitates deeper insights into smaller, specialized collections but also reduces dependency on vast external training resources, enhancing its practical utility in cultural heritage studies.},
	journal = {J. Comput. Cult. Herit.},
	author = {Ozdemir, Anil and Odaci, Berke and Tanatar Baruh, Lorans and Varol, Onur and Balcisoy, Selim},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural Language Processing, Machine Learning, Recommendation Systems, Graph Representation Learning},
	annote = {Just Accepted},
}

@article{koto_ffci_2022,
	title = {{FFCI}: {A} {Framework} for {Interpretable} {Automatic} {Evaluation} of {Summarization}},
	volume = {73},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.13167},
	doi = {10.1613/jair.1.13167},
	abstract = {In this paper, we propose FFCI, a framework for fine-grained summarization evaluation that comprises four elements: faithfulness (degree of factual consistency with the source), focus (precision of summary content relative to the reference), coverage (recall of summary content relative to the reference), and inter-sentential coherence (document fluency between adjacent sentences). We construct a novel dataset for focus, coverage, and inter-sentential coherence, and develop automatic methods for evaluating each of the four dimensions of FFCI based on cross-comparison of evaluation metrics and model-based evaluation methods, including question answering (QA) approaches, semantic textual similarity (STS), next-sentence prediction (NSP), and scores derived from 19 pre-trained language models. We then apply the developed metrics in evaluating a broad range of summarization models across two datasets, with some surprising findings.},
	journal = {J. Artif. Int. Res.},
	author = {Koto, Fajri and Baldwin, Timothy and Lau, Jey Han},
	month = may,
	year = {2022},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
	keywords = {machine learning, neural networks, natural language},
}

@inproceedings{lin_modeling_2018,
	address = {New York, NY, USA},
	series = {{IMMS} '18},
	title = {Modeling framework of general simulation model based on model template},
	isbn = {978-1-4503-6486-7},
	url = {https://doi.org/10.1145/3277139.3277154},
	doi = {10.1145/3277139.3277154},
	abstract = {The model is the core of the simulation system, aiming at the problems of low reuse efficiency of the current simulation model and long model development cycle, this paper proposes a generic model generation technology framework. First, the simulation model is split into independent components through military concept analysis, and then the common concept of the same model is extracted to form a model template with a unified description specification. The specific simulation model is instantiated quickly through parameterized configuration. The modeling framework can effectively support the generation and application of the platform models in military simulation systems, and has great scalability and reusability.},
	booktitle = {Proceedings of the 1st {International} {Conference} on {Information} {Management} and {Management} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Menglong and Yao, Yiping},
	year = {2018},
	note = {event-place: Chengdu, China},
	keywords = {component-based, model template, modeling framework, simulation model},
	pages = {223--227},
}

@inproceedings{molina_de_armas_proposal_2024,
	address = {New York, NY, USA},
	series = {{SBSI} '24},
	title = {A {Proposal} of a {Knowledge} {Graph} for {Digital} {Engineering} {Systems} {Integration} for {Operation} and {Maintenance} {Activities} in {Industrial} {Plants}},
	isbn = {979-8-4007-0996-8},
	url = {https://doi.org/10.1145/3658271.3658339},
	doi = {10.1145/3658271.3658339},
	abstract = {Context: Over the last years, we have observed Knowledge Graphs (KGs) being used more and more as a tool for representing knowledge, data integration and querying data. Problem: There are many distinguished yet partially-integrated information management systems used to support the life-cycle of Oil and Gas industrial plants. Our approach considers a 3D plants viewer system, a visual navigation system on platforms, and the integrated intelligent search system. However, these systems lack a semantic integration that can guide the user actions over each functionality for a unique asset. Solution: This paper presents the use of KGs to represent and help monitoring and controlling operational and maintenance activities within an Oil and Gas industrial environment. Our approach highlights the challenges and initial work required to establish a fully-integrated management domain, where the execution of the aforementioned activities can easily be managed. SI Theory: This study draws inspiration from Representation Theory, which posits that an information system faithfully mirrors specific phenomena occurring in the physical world. Method: To develop this work, it was necessary to review the literature related to the development of KGs and ontologies. The generated KG was developed using well-established standards like the Industrial Data Ontology (IDO), and the Capital Facilities Information Handover Specification (CFIHOS), complemented with the use of other ontologies. Summary of Results: A prototype of the conceptual KG was implemented, verifying the viability of our approach for data integration. Contributions and Impact in IS area: The resulted graph contains the main terms in compliance with international semantic standards for representing operational and maintenance activities data associated with facilities involved in Oil and Gas production. Finally, the KG resulting from this effort can be further extended through the incorporation of new tools and subdomains in the industrial plants life-cycle.},
	booktitle = {Proceedings of the 20th {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Molina De Armas, Elvismary and Hamazaki Da Silva, Geiza Maria and Torres Izquierdo, Yenier and Lemos, Melissa and De Lima Britto, Paulo Vinícius and Corseuil, Eduardo Thadeu and Souza Garcia, Robinson Luiz},
	year = {2024},
	note = {event-place: Juiz de Fora, Brazil},
	keywords = {Ontology, Knowledge Graphs, Data Integration, Digital Engineering, Industrial Plants, Operation and Maintenance activities},
}

@inproceedings{ma_modeling_2019,
	address = {New York, NY, USA},
	series = {{CSAE} '19},
	title = {A {Modeling} {Tool} for {Sensor}-based {Mobile} {Applications}},
	isbn = {978-1-4503-6294-8},
	url = {https://doi.org/10.1145/3331453.3361301},
	doi = {10.1145/3331453.3361301},
	abstract = {With the variety of the sensors on mobile devices, sensor-based mobile applications are constantly emerging. Usually, it is necessary to model sensor-based mobile applications, and the modeling requires the domain metamodels and the modeling tools. This paper presents a set of the modeling concepts from the aspects of sensor data perception, sensor data understanding, and sensor data processing, and then builds a metamodel with UML profile mechanism. Moreover, the paper proposes a modeling tool architecture and discusses the design and implementation of key modules in the architecture based on this metamodel. According to the metamodel, architecture, and key modules, the developers can build the tools to model sensor-based mobile applications and generate key code to greatly improve the development productivity and quality of the applications.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Computer} {Science} and {Application} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Zhiyi and Wang, Xiaoxi and Chen, Hongjie and Qiu, Ye},
	year = {2019},
	note = {event-place: Sanya, China},
	keywords = {Sensors, Mobile applications, Metamodel, Modeling tool},
}

@inproceedings{he_cement-_2022,
	address = {New York, NY, USA},
	series = {e-{Energy} '22},
	title = {Cement-α: an ontology-based data access system for building analytics with multiple data sources},
	isbn = {978-1-4503-9397-3},
	url = {https://doi.org/10.1145/3538637.3538838},
	doi = {10.1145/3538637.3538838},
	abstract = {To enhance the portability of data-driven building analytics across buildings, data models have been developed to provide unified representations of building data, such as Brick, which defines how to construct an ontology to capture the semantics of building entities and the relationships among them. Unfortunately, existing data models are all developed for the data of building systems, e.g., the HVAC system of a building. Yet, building analytics can require data that are external to a building system, e.g., weather data for cooling load forecasting. Such external data can be accessed from external sources, e.g., observatory, yet these data have their own data models and storage methods.To enable portable data access from multiple data sources for building analytics, in this paper, we firstly define building periphery data, and then we study the approach to develop a data model to support building analytics which require both building data and building periphery data. We further develop Cement-α, an ontology-based data access system to extract both building and building periphery data. We evaluate the Cement-α qualitatively and quantitatively by using four real-world building analytics, and find that the development effort of building analytics with multiple data sources can be reduced by an average of 57.2\%.},
	booktitle = {Proceedings of the {Thirteenth} {ACM} {International} {Conference} on {Future} {Energy} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {He, Fang and Zhang, Xiaoyang and Wang, Dan},
	year = {2022},
	note = {event-place: Virtual Event},
	keywords = {machine learning, smart building, data access, data analytics},
	pages = {436--437},
}

@inproceedings{alaa_el_din_talha_semantic_2020,
	address = {New York, NY, USA},
	series = {{ICCBD} '20},
	title = {A {Semantic} {Based} {Annotation} {Technique} for the {Internet} of {Things}},
	isbn = {978-1-4503-8786-6},
	url = {https://doi.org/10.1145/3418688.3418696},
	doi = {10.1145/3418688.3418696},
	abstract = {Due to the recent deployments of Internet of Things (IoT) technologies in many real-life applications, enormous amount of diverse and real-time streams of data are being generated. To facilitate dealing with the heterogeneity of IoT data streams, semantic technologies became the main element to guarantee data interoperability, with its nature to unify concepts, extend knowledge, and share a machine-readable representation of data. In this paper, we propose an adaptable approach for IoT data semantic annotation, to achieve an efficient way to enrich data semantically considering its heterogeneity, volume, and frequency. A use case is implemented using Apache Kafka, Spark to deal with data streams in real-time, and a Semantic ontology model extending the SOSA standard ontology is developed to annotate data into enriched Resource Description Framework (RDF) triples.},
	booktitle = {Proceedings of the 2020 3rd {International} {Conference} on {Computing} and {Big} {Data}},
	publisher = {Association for Computing Machinery},
	author = {Alaa El Din Talha, Shorouk},
	year = {2020},
	note = {event-place: Taichung, Taiwan},
	keywords = {Ontology, Internet of Things, Semantic Annotation, stream processing},
	pages = {42--47},
}

@inproceedings{balashankar_enhancing_2021,
	address = {New York, NY, USA},
	series = {{WSDM} '21},
	title = {Enhancing {Neural} {Recommender} {Models} through {Domain}-{Specific} {Concordance}},
	isbn = {978-1-4503-8297-7},
	url = {https://doi.org/10.1145/3437963.3441784},
	doi = {10.1145/3437963.3441784},
	abstract = {Recommender models trained on historical observational data alone can be brittle when domain experts subject them to counterfactual evaluation. In many domains, experts can articulate common, high-level mappings or rules between categories of inputs (user's history) and categories of outputs (preferred recommendations). One challenge is to determine how to train recommender models to adhere to these rules. In this work, we introduce the goal of domain-specific concordance: the expectation that a recommender model follow a set of expert-defined categorical rules. We propose a regularization-based approach that optimizes for robustness on rule-based input perturbations. To test the effectiveness of this method, we apply it in a medication recommender model over diagnosis-medicine categories, and in movie and music recommender models, on rules over categories based on movie tags and song genres. We demonstrate that we can increase the category-based robustness distance by up to 126\% without degrading accuracy, but rather increasing it by up to 12\% compared to baseline models in the popular MIMIC-III, MovieLens-20M and Last.fm Million Song datasets.},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Balashankar, Ananth and Beutel, Alex and Subramanian, Lakshminarayanan},
	year = {2021},
	note = {event-place: Virtual Event, Israel},
	keywords = {information systems, recommender systems},
	pages = {1002--1010},
}

@article{nguyen_use_2020,
	title = {Use {Chou}'s 5-{Steps} {Rule} {With} {Different} {Word} {Embedding} {Types} to {Boost} {Performance} of {Electron} {Transport} {Protein} {Prediction} {Model}},
	volume = {19},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2020.3010975},
	doi = {10.1109/TCBB.2020.3010975},
	abstract = {Living organisms receive necessary energy substances directly from cellular respiration. The completion of electron storage and transportation requires the process of cellular respiration with the aid of electron transport chains. Therefore, the work of deciphering electron transport proteins is inevitably needed. The identification of these proteins with high performance has a prompt dependence on the choice of methods for feature extraction and machine learning algorithm. In this study, protein sequences served as natural language sentences comprising words. The nominated word embedding-based feature sets, hinged on the word embedding modulation and protein motif frequencies, were useful for feature choosing. Five word embedding types and a variety of conjoint features were examined for such feature selection. The support vector machine algorithm consequentially was employed to perform classification. The performance statistics within the 5-fold cross-validation including average accuracy, specificity, sensitivity, as well as MCC rates surpass 0.95. Such metrics in the independent test are 96.82, 97.16, 95.76 percent, and 0.9, respectively. Compared to state-of-the-art predictors, the proposed method can generate more preferable performance above all metrics indicating the effectiveness of the proposed method in determining electron transport proteins. Furthermore, this study reveals insights about the applicability of various word embeddings for understanding surveyed sequences.},
	number = {2},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Nguyen, Trinh-Trung-Duong and Ho, Quang-Thai and Le, Nguyen-Quoc-Khanh and Phan, Van-Dinh and Ou, Yu-Yen},
	month = jul,
	year = {2020},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {1235--1244},
}

@article{ranta_abstract_2020,
	title = {Abstract {Syntax} as {Interlingua}: {Scaling} {Up} the {Grammatical} {Framework} from {Controlled} {Languages} to {Robust} {Pipelines}},
	volume = {46},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00378},
	doi = {10.1162/coli_a_00378},
	abstract = {Abstract syntax is an interlingual representation used in compilers. Grammatical Framework (GF) applies the abstract syntax idea to natural languages. The development of GF started in 1998, first as a tool for controlled language implementations, where it has gained an established position in both academic and commercial projects. GF provides grammar resources for over 40 languages, enabling accurate generation and translation, as well as grammar engineering tools and components for mobile and Web applications. On the research side, the focus in the last ten years has been on scaling up GF to wide-coverage language processing. The concept of abstract syntax offers a unified view on many other approaches: Universal Dependencies, WordNets, FrameNets, Construction Grammars, and Abstract Meaning Representations. This makes it possible for GF to utilize data from the other approaches and to build robust pipelines. In return, GF can contribute to data-driven approaches by methods to transfer resources from one language to others, to augment data by rule-based generation, to check the consistency of hand-annotated corpora, and to pipe analyses into high-precision semantic back ends. This article gives an overview of the use of abstract syntax as interlingua through both established and emerging NLP applications involving GF.},
	number = {2},
	journal = {Comput. Linguist.},
	author = {Ranta, Aarne and Angelov, Krasimir and Gruzitis, Normunds and Kolachina, Prasanth},
	month = jun,
	year = {2020},
	note = {Place: Cambridge, MA, USA
Publisher: MIT Press},
	pages = {425--486},
}

@inproceedings{khan_framework_2020,
	address = {New York, NY, USA},
	series = {{ICSCA} '20},
	title = {A {Framework} for {Automated} {Reengineering} of {BPMN} {Models} by {Excluding} {Inefficient} {Activities}},
	isbn = {978-1-4503-7665-5},
	url = {https://doi.org/10.1145/3384544.3384549},
	doi = {10.1145/3384544.3384549},
	abstract = {Business Process Reengineering (BPR), originally floated in the early 1990s, is gaining importance in industry and academia. BPR helps the organization rethink their work rationally by redesigning their current processes and resource consumption. Due to the high rate of software evolution, there is a need to run legacy systems on a new computing platform. BPMN models are subject to erroneous or unnecessary activities that are taking too many resources. Such process models are leading to additional cost and effort. Re-engineering help in improving the legacy system or in this context a set of legacy processes to perform better than before. This work presents a framework for automatic reengineering of a BPMN by identifying activities that are taking too much time and resources but are insignificant to the business process. An extensive literature review has led to the extraction of three important parameters based on which the business process activities can be evaluated as necessary or unnecessary i.e. time, resources and priority of an activity. The proposed model has been validated using a case study on the Claim Management System. This work shall be beneficial for the research community and developers targeting construction of a BPR tool},
	booktitle = {Proceedings of the 2020 9th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Khan, Rimsha and Azam, Farooque and Maqbool, Bilal and Anwar, Muhammad Waseem},
	year = {2020},
	note = {event-place: Langkawi, Malaysia},
	keywords = {BPMN, Business Process Re-engineering (BPR), Resource Optimization, Software Automation},
	pages = {147--151},
}

@article{kaur_joint_2020,
	title = {Joint {Modelling} of {Cyber} {Activities} and {Physical} {Context} to {Improve} {Prediction} of {Visitor} {Behaviors}},
	volume = {16},
	issn = {1550-4859},
	url = {https://doi.org/10.1145/3393692},
	doi = {10.1145/3393692},
	abstract = {This article investigates the cyber-physical behavior of users in a large indoor shopping mall by leveraging anonymized (opt in) Wi-Fi association and browsing logs recorded by the mall operators. Our analysis shows that many users exhibit a high correlation between their cyber activities and their physical context. To find this correlation,propose a mechanism to semantically label a physical space with rich categorical information from DBPedia concepts and compute a contextual similarity that represents a user’s activities with the mall context. We demonstrate the application of cyber-physical contextual similarity in two situations: user visit intent classification and future location prediction. The experimental results demonstrate that exploitation of contextual similarity significantly improves the accuracy of such applications.},
	number = {3},
	journal = {ACM Trans. Sen. Netw.},
	author = {Kaur, Manpreet and Salim, Flora D. and Ren, Yongli and Chan, Jeffrey and Tomko, Martin and Sanderson, Mark},
	month = aug,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {knowledge graph, recommender systems, semantic enrichment, context-aware computing, check-ins, cyber-physical, indoor trajectory, intent recognition, location prediction, logs analysis, movement analysis, retail behaviour, shopping behaviour, user modelling, user profiling, Wi-Fi},
}

@article{tomlein_augmented_2018,
	title = {Augmented {Reality} {Supported} {Modeling} of {Industrial} {Systems} to {Infer} {Software} {Configuration}},
	volume = {2},
	url = {https://doi.org/10.1145/3229087},
	doi = {10.1145/3229087},
	abstract = {This paper proposes and evaluates an approach for building models of installed industrial Cyber-Physical Systems using augmented reality on smartphones. It proposes a visual language for annotating devices, containers, flows of liquids and networking connections in augmented reality. Compared to related work, it provides a more lightweight and flexible approach for building 3D models of industrial systems. The models are further used to automatically infer software configuration of controllable industrial products. This addresses a common problem of error-prone and time-consuming configuration of industrial systems in the current practice. The proposed approach is evaluated in a study with 16 domain experts. The study participants are involved in creating a model of an industrial system for water treatment. Their comments show that the approach can enable a less error-prone configuration for more complex systems. Opportunities for improvement in usability and reflections on the potential of the approach are discussed.},
	number = {EICS},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Tomlein, Matúš and Grønbæk, Kaj},
	month = jun,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {modeling, iot, augmented reality, configuration},
}

@inproceedings{xiao_research_2020,
	address = {New York, NY, USA},
	series = {{ISAIMS} '20},
	title = {Research on {Traditional} {Chinese} {Medicine} {Data} {Mining} {Model} {Based} on {Traditional} {Chinese} {Medicine} {Basic} {Theories} and {Knowledge} {Graphs}},
	isbn = {978-1-4503-8860-3},
	url = {https://doi.org/10.1145/3429889.3429909},
	doi = {10.1145/3429889.3429909},
	abstract = {In recent years, great progress has been made in the study of knowledge graph in various fields, and it has become a hot topic in Traditional Chinese Medicine (TCM) related fields. This paper utilizes a Chinese Herbal Medicine collection, which includes 537 medicines, retrieved from a hospital affiliated with a TCM university, as data source; referenced the Chinese Pharmacopoeia for building the knowledge graph founded on the basic TCM theory. Via associating the prescription with drug properties, taste and meridian tropism of Chinese medicine and visualizing the complex network of Chinese medicine prescription from a novel perspective, the rules in the prescription can be mined in a deeper level, which has a strong practical reference value for developing new clinical medicine and studying the prescription data mining.},
	booktitle = {Proceedings of the 1st {International} {Symposium} on {Artificial} {Intelligence} in {Medical} {Sciences}},
	publisher = {Association for Computing Machinery},
	author = {Xiao, Rui and Hu, Fengju and Pei, Wei and Bie, Minkun},
	year = {2020},
	note = {event-place: Beijing, China},
	keywords = {Knowledge Graph, Data Mining, Basic Theories of TCM},
	pages = {102--106},
}

@inproceedings{loukachevitch_thesaurus-based_2018,
	address = {New York, NY, USA},
	series = {{WIMS} '18},
	title = {Thesaurus-{Based} {Topic} {Models} and {Their} {Evaluation}},
	isbn = {978-1-4503-5489-9},
	url = {https://doi.org/10.1145/3227609.3227659},
	doi = {10.1145/3227609.3227659},
	abstract = {In this paper we study thesaurus-based topic models and evaluate them from the point of view of topic coherence. Thesaurus-based topic model enhances scores of related terms found in the same text, which means that the model encourages these terms to be in the same topics. We evaluate various variants of such models. At the first step, we carry out manual evaluation of the obtained topics. At the second step, we study the possibility to use the collected manual data for evaluating new variants of thesaurus-based models, propose a method and select the best of its parameters in cross-validation. At the third step, we apply the created evaluation method to estimate the influence of word frequencies on adding thesaurus relations during generating topic models.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Web} {Intelligence}, {Mining} and {Semantics}},
	publisher = {Association for Computing Machinery},
	author = {Loukachevitch, Natalia and Ivanov, Kirill and Dobrov, Boris},
	year = {2018},
	note = {event-place: Novi Sad, Serbia},
	keywords = {thesaurus, topic models, content-based analysis},
}

@inproceedings{kouissi_new_2019,
	address = {New York, NY, USA},
	series = {{SCA} '19},
	title = {New approach for modeling and developing multi-agent systems based on case based reasoning},
	isbn = {978-1-4503-6289-4},
	url = {https://doi.org/10.1145/3368756.3369029},
	doi = {10.1145/3368756.3369029},
	abstract = {In this paper, we present a multi agent architecture based on Incremental Dynamic Case-Based Reasoning (IDCBR). Our approach inherits from Model Driven Architecture (MDA [11]), which aims to design, develop and implement models or meta-models of multi-agent systems that we build from AUML. We have designed a generic and scalable class diagram to develop complex multi-agent systems [3] for Decision Support System based on IDCBR to predict and anticipate a dynamic situation. The source code of the models is generated by an open source tool called AndroMDA [13]. The model and source code will be used to design and develop applications to implement and simulate multi-agent models for Management of Common Renewable Resources [4].},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Smart} {City} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Kouissi, Mohamed and Ghouch, Nihad El and En-naimi, El Mokhtar},
	year = {2019},
	note = {event-place: Casablanca, Morocco},
	keywords = {simulation, decision making, AUML, common renewable resources, incremental dynamic case-based reasoning (IDCBR), Jade platform, model driven architecture (MDA), multi agents systems},
}

@inproceedings{huang_modeling_2018,
	address = {New York, NY, USA},
	series = {{ICCSE}'18},
	title = {Modeling and {Analysis} of {Demand} for {Personalized} {Portal}},
	isbn = {978-1-4503-6587-1},
	url = {https://doi.org/10.1145/3265689.3265708},
	doi = {10.1145/3265689.3265708},
	abstract = {E-commerce1 has experienced great growth during the past two decades, which changes the consumption mode of consumers significantly. All researchers from institutions and enterprises want to identify and satisfy the personalized demand intelligently and conveniently by every possible means. In this paper, we proposed smart demand strategy based on holographic demand and model of transaction subject, which is applicable for the decentralized, disintermediated, intelligent e-commerce platform. User demands are classified from two aspects, which will improve the accuracy of demand obtaining. In addition, from standardized description of demand and full life cycle tracking of demand, the user demand will be identified comprehensively. Meanwhile, models of the user, including physical, preference, knowledge, digital label, and social attributes, are built based on his standard description and fragmented description from his interactive objects, which results in a holographic demander. Then, smart demand strategy, i.e. demand forecast and recommendation are proposed. Based on trigger point and demand attributes, the user demand will be updated in real time, which ensures the accuracy of demand accusation and recommendation. The relationship and help degree, based on the interactions within the cyberspace, are important references in filtering the recommendation.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Crowd} {Science} and {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Yadong and Chai, Yueting and Liu, Yi and Zhang, Anting and Wu, Hao},
	year = {2018},
	note = {event-place: Singapore, Singapore},
	keywords = {E-commerce, Demand Strategy, Holographic demand, Personalized portal, Subject Model},
}

@article{tosh_contrastive_2021,
	title = {Contrastive estimation reveals topic posterior information to linear models},
	volume = {22},
	issn = {1532-4435},
	abstract = {Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers trained on these representations perform well in document classification tasks with very few training examples.},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
	month = jan,
	year = {2021},
	note = {Publisher: JMLR.org},
	keywords = {representation learning, contrastive estimation, latent Dirichlet allocation},
}

@article{demir_turkish_2022,
	title = {Turkish {Data}-to-{Text} {Generation} {Using} {Sequence}-to-{Sequence} {Neural} {Networks}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3543826},
	doi = {10.1145/3543826},
	abstract = {End-to-end data-driven approaches lead to rapid development of language generation and dialogue systems. Despite the need for large amounts of well-organized data, these approaches jointly learn multiple components of the traditional generation pipeline without requiring costly human intervention. End-to-end approaches also enable the use of loosely aligned parallel datasets in system development by relaxing the degree of semantic correspondences between training data representations and text spans. However, their potential in Turkish language generation has not yet been fully exploited. In this work, we apply sequence-to-sequence (Seq2Seq) neural models to Turkish data-to-text generation where the input data given in the form of a meaning representation is verbalized. We explore encoder-decoder architectures with attention mechanism in unidirectional, bidirectional, and stacked recurrent neural network (RNN) models. Our models generate one-sentence biographies and dining venue descriptions using a crowdsourced dataset where all field value pairs that appear in meaning representations are fully captured in reference sentences. To support this work, we also explore the performances of our models on a more challenging dataset, where the content of a meaning representation is too large to fit into a single sentence, and hence content selection and surface realization need to be learned jointly. This dataset is retrieved by coupling introductory sentences of person-related Turkish Wikipedia articles with their contained infobox tables. Our empirical experiments on both datasets demonstrate that Seq2Seq models are capable of generating coherent and fluent biographies and venue descriptions from field value pairs. We argue that the wealth of knowledge residing in our datasets and the insights obtained from this study hold the potential to give rise to the development of new end-to-end generation approaches for Turkish and other morphologically rich languages.},
	number = {2},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Demir, Seniz},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Wikipedia, Data-to-text generation, sequence-to-sequence model, Turkish},
}

@inproceedings{guzel_kalayci_ontop-temporal_2018,
	address = {New York, NY, USA},
	series = {{CIKM} '18},
	title = {Ontop-temporal: {A} {Tool} for {Ontology}-based {Query} {Answering} over {Temporal} {Data}},
	isbn = {978-1-4503-6014-2},
	url = {https://doi.org/10.1145/3269206.3269230},
	doi = {10.1145/3269206.3269230},
	abstract = {We present Ontop-temporal, an extension of the ontology-based data access system Ontop for query answering with temporal data and ontologies. Ontop is a system to answer SPARQL queries over various data stores, using standard R2RML mappings and an OWL2QL domain ontology to produce high-level conceptual views over the raw data. The Ontop-temporal extension is designed to handle timestamped log data, by additionally using (i) mappings supporting validity time specification, and (ii) rules based on metric temporal logic to define temporalised concepts. In this demo we present how Ontop-temporal can be used to facilitate the access to the MIMIC-III critical care unit dataset containing log data on hospital admissions, procedures, and diagnoses. We use the ICD9CM diagnoses ontology and temporal rules formalising the selection of patients for clinical trials taken from the clinicaltrials.gov database. We demonstrate how high-level queries can be answered by Ontop-temporal to identify patients eligible for the trials.},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Güzel Kalayci, Elem and Xiao, Guohui and Ryzhikov, Vladislav and Kalayci, Tahir Emre and Calvanese, Diego},
	year = {2018},
	note = {event-place: Torino, Italy},
	keywords = {ontology-based data access, metric temporal logic, mimic-iii},
	pages = {1927--1930},
}

@inproceedings{mebrek_stream_2020,
	address = {New York, NY, USA},
	series = {{SAC} '20},
	title = {A stream reasoning framework based on a multi-agents model},
	isbn = {978-1-4503-6866-7},
	url = {https://doi.org/10.1145/3341105.3374111},
	doi = {10.1145/3341105.3374111},
	abstract = {Processing on-the-fly high volume of data streams is increasingly needed. To cope with the heterogeneity of this data, RDF model is more and more being adopted leading to plethora of RDF Stream Processing (RSP) systems and languages dealing with issues such as continuous querying, incremental reasoning and complex event processing (CEP). However, most of them has implemented centralized approaches and therefore suffer from some limitations as collaboration, sharing, expressiveness and scalability. Multi-agents systems have widely proven their worth and efficiency in particular their intrinsic decentralized property along with their cooperation and communication mechanism. In this paper we propose a new framework MAS4MEAN (Multi-Agent System for streaM rEAsoNing) based on a multi-agents model to embrace their benefits and tackle the challenges of increasing the scalability and ease of deployment in highly dynamic environments. A preliminary experimental evaluation with a real-world dataset show promising results when compared to an existing work.},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Mebrek, Wafaa and Bouzeghoub, Amel},
	year = {2020},
	note = {event-place: Brno, Czech Republic},
	keywords = {stream processing, multi-agents systems, RDF streams, stream reasoning},
	pages = {509--512},
}

@inproceedings{winkelnkemper_reconstructing_2024,
	address = {New York, NY, USA},
	series = {Koli {Calling} '23},
	title = {Reconstructing the {Digital} – {An} {Architectural} {Perspective} for {Non}-{Engineers} ({Discussion} {Paper})},
	isbn = {979-8-4007-1653-9},
	url = {https://doi.org/10.1145/3631802.3631826},
	doi = {10.1145/3631802.3631826},
	abstract = {Knowing and understanding the world of digital artefacts we are living in is a requirement for everyone today, regardless of their general interest in technology. Computer science education, however, often treats pupils as if they all wanted to become engineers. Educational models of computer science are rather not targeted at understanding the behaviour of the digital world, but at constructing it. Our paper complements such classical approaches with an Ontology of the Digital as an approach which reconstructs digital artefacts and thereby creates a model which helps to understand and explain the technological potentials of digital artefacts without relying on minute details of the engineering discipline of computing.},
	booktitle = {Proceedings of the 23rd {Koli} {Calling} {International} {Conference} on {Computing} {Education} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Winkelnkemper, Felix and Schulte, Carsten},
	year = {2024},
	note = {event-place: Koli, Finland},
	keywords = {ontology, CS for all, digital artefacts, explanation model, technological knowledge},
}

@inproceedings{hu_investigation_2021,
	address = {New York, NY, USA},
	series = {{TEEM}'20},
	title = {The {Investigation} and {Novel} {Trinity} {Modeling} for {Museum} {Robots}},
	isbn = {978-1-4503-8850-4},
	url = {https://doi.org/10.1145/3434780.3436541},
	doi = {10.1145/3434780.3436541},
	abstract = {There have been interactive museum tour-guide robots under investigation since the end of twentieth century. However, those researches are limited to localisations and telepresence with less humanoids deployment or human-touched features. This research used a humanoid robot to develop the first Welsh-based museum robots that can speak bilingual, English and Welsh, addressing the design method, constraints and initial experimental results. This article introduces the definition and development of robots and service robots with three aims: 1) to design and pilot service robots in a public educational environment, National Museum of Wales, Cardiff. This is to develop a semi-autonomous robotic museum programme that can guide and educate visitors, explain exhibits and perform surveys based on a higher level of robot technology platform; 2) to perform voice interaction with the visitors and provides an inquiry and corporate branding services by the robotic programme with initial user experiences inquiry; and 3) to provide educational service robot design recommendation and a novel Trinity conceptual model and design principles to the sector based on the findings from objectives 1 and 2, for preliminary study and research on artificial intelligence in education and social cognition. Case study research method is used to lays a reference for museum robotic research, and it is easy to expand functionally, that is, secondary development; developing an autonomous humanoid robot for museum visit and interactive education.},
	booktitle = {Eighth {International} {Conference} on {Technological} {Ecosystems} for {Enhancing} {Multiculturality}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Shuyang and Chew, Esyin},
	year = {2021},
	note = {event-place: Salamanca, Spain},
	keywords = {educational robot, Museum robot, Nao robot, service robot},
	pages = {21--28},
}

@inproceedings{li_overview_2025,
	address = {New York, NY, USA},
	series = {{AISNS} '24},
	title = {An {Overview} of {Event} {Extraction} {Methods} based on {Semantic} {Disambiguation}},
	isbn = {979-8-4007-1123-7},
	url = {https://doi.org/10.1145/3714334.3714388},
	doi = {10.1145/3714334.3714388},
	abstract = {Event extraction is a fundamental and complex task in information extraction, aiming at automatically identifying and extracting structured event information from unstructured text. However, the same word may have different meanings in different contexts, necessitating semantic disambiguation during the classification process. The polysemy of triggers presents a significant challenge to accurate EE. Existing surveys primarily focus on different domains, fields, or technical paradigms, but none provide a systematic summary of the semantic disambiguation techniques employed. This study classifies these techniques into three categories based on the underlying technical frameworks and offers a comprehensive overview of the most advanced methods in each category. Finally, we summarize the application scenarios, strengths, and limitations of existing approaches, and discuss potential directions for future research.},
	booktitle = {Proceedings of the 2024 2nd {International} {Conference} on {Artificial} {Intelligence}, {Systems} and {Network} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Li, Haili and Wang, Xiaodong and Zhou, Yunyan and Liu, Weijie and Pan, Shilong},
	year = {2025},
	keywords = {natural language processing, information extraction, deep neural networks, event extraction, semantic disambiguation},
	pages = {319--326},
}

@inproceedings{franco_use_2020,
	address = {New York, NY, USA},
	series = {{ICIME} 2019},
	title = {On the {Use} of {Business} {Process} {Models} to {Discover} {System} {Requirements}},
	isbn = {978-1-4503-7234-3},
	url = {https://doi.org/10.1145/3373744.3373745},
	doi = {10.1145/3373744.3373745},
	abstract = {A framework of generic categories of process activities is adopted as a framework of generic categories of system goals in order to guide the reasoning of system analysts and stakeholders for the discovery of system goals from business process models. The categories of process activities are organized in four essential aspects of the process concept: input, evolution, evaluation and decision and output. These categories are characterized by verbs which offer a semantic diversity that clarifies the field of reasoning and guides the discovery of goals. This article proposes an approach for obtaining system goals from business process models; the proposal is illustrated with a diverse and rich set of pertinent goals discovered for a system supporting a "booking a flight" process.},
	booktitle = {Proceedings of the 2019 11th {International} {Conference} on {Information} {Management} and {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Franco, Aldrin Jaramillo and Giraldo, Germán Urrego},
	year = {2020},
	note = {event-place: London, United Kingdom},
	keywords = {business process models reuse, Business processes, system goals elicitation},
	pages = {1--9},
}

@inproceedings{yeh_bkontovr_2018,
	address = {New York, NY, USA},
	series = {{ICSEB} '18},
	title = {{BKOntoVR}: {A} {Virtual} {Reality} {Exhibition} {System} for {Biographic} {Ontology}-{Based} {Semantic} {Structure}},
	isbn = {978-1-4503-6127-9},
	url = {https://doi.org/10.1145/3301761.3301775},
	doi = {10.1145/3301761.3301775},
	abstract = {In this article, we illustrate some of the semantic web-related technologies and design a virtual exhibition system for a set of ontology knowledge structures based on biographical history, which we call BKOntoVR. This is an official framework for processing and presenting biographical history-related messages on the semantic web with virtual-reality technology, including biographical events, time and space relationships, related personal messages, and more. We elaborate on this ontology knowledge architecture and explain how to use our ontological structure called BKOnto as a basis for domain-specific knowledge to support virtual presentation. Information management is becoming an important part of cultural collections related technologies, from the management of personal collections to the establishment of large, decentralized "semantic" databases. These semantic databases can be used to a certain extent using semantic web technology to process and construct a machine-understandable data network. Such a data network can be linked to the referenced knowledge structure to give a concept and a relationship form specification associated with a set of descriptive objects in the definition domain (person, thing, place, etc.) - that is, link to its meaning. Biographical history of the specific characters is through the life and other areas of a systematic description of the introduction of a text that form. In this paper, we overview some of the Semantic Web-related technologies and describe a cognitive knowledge structure for biographical knowledge representation based on OWL markup language, we call it BKOnto. This is a formal framework for dealing with information about biographical history on the semantic web, including biographical events, temporal and spatial relationships, related information of persons, and so on. We describe this ontology knowledge structure and explain how BKOnto can act as a basis for more domain-specific knowledge representation. In this study, we further present such cultural collections in a virtual reality form, by transforming the ontology cognitive architecture data into a virtual reality exhibition space, allowing users to present the semantic structure in the form of multimedia in a three-dimensional space more easily. Such a form of presentation can be used in the virtual exhibition of the museum, making it easier for museums to organize cultural collections of semantic structures and exert their influence through the Internet. The empirical study also uses the Mackay Digital Archives Project (http://dlm.csie.au.edu.tw/) as a source of information to demonstrate the ontology knowledge building process of Mackay's biographical stories, as well as related Digital collection of information.},
	booktitle = {Proceedings of the 2018 2nd {International} {Conference} on {Software} and {E}-{Business}},
	publisher = {Association for Computing Machinery},
	author = {Yeh, Jian-hua and Huang, Xin-mao},
	year = {2018},
	note = {event-place: Zhuhai, China},
	keywords = {ontology, semantic web, virtual reality, Biographical knowledge, temporal event, museum exhibition},
	pages = {69--73},
}

@inproceedings{youssef_gqm-based_2021,
	address = {New York, NY, USA},
	series = {{ICSIE} '20},
	title = {{GQM}-based {Tree} {Model} for {Automatic} {Recommendation} of {Design} {Pattern} {Category}},
	isbn = {978-1-4503-7721-8},
	url = {https://doi.org/10.1145/3436829.3436862},
	doi = {10.1145/3436829.3436862},
	abstract = {Software Design Patterns (DP) are formal approaches that propose generic reusable solutions to different design problems. Building DP automatic recommendation system is one of the most challenging topics in the field of software industry to improve the final software quality. Proposing a DP for a design problem requires good base knowledge about each DP and its functionality. In this paper, we propose an approach that automatically recommends the appropriate design pattern category. The proposed approach is a Goal Question Metric (GQM) based tree model of questions. The software engineer answers these questions based on the user requirements, and finally the approach recommends the category of the suitable DP category based on our designed tree model. The GQM is responsible for weight calculation process at each node based on the questions' answers. The software engineer is responsible for delivering the user requirements to our system, via answering the proposed model. The precision and accuracy obtained by our system is 80\% while the recall is 100\%.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Software} and {Information} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Youssef, Clara K. and Ahmed, Farida M. and Hashem, Hashem M. and Talaat, Veronia E. and Shorim, Nada and Ghanim, Taraggy},
	year = {2021},
	note = {event-place: Cairo, Egypt},
	keywords = {Recommendation System, Design Patterns, Decision Trees, Design Pattern selection, Goal-Question-Metric},
	pages = {126--130},
}

@inproceedings{nesic_principles_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Principles of feature modeling},
	isbn = {978-1-4503-5572-8},
	url = {https://doi.org/10.1145/3338906.3338974},
	doi = {10.1145/3338906.3338974},
	abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Nešić, Damir and Krüger, Jacob and Stănciulescu, undefinedtefan and Berger, Thorsten},
	year = {2019},
	note = {event-place: Tallinn, Estonia},
	keywords = {Feature models, software product lines, modeling principles},
	pages = {62--73},
}

@inproceedings{bocciarelli_model_2018,
	address = {San Diego, CA, USA},
	series = {{Mod4Sim} '18},
	title = {Model transformation services for {MSaaS} platforms},
	isbn = {978-1-5108-6018-6},
	abstract = {The development of complex systems may take advantage by the introduction of Modeling \&amp; Simulation (M\&amp;S) based analysis techniques from the early stages of the system lifecycle. However, M\&amp;S approaches typically require significant know-how and effort, as well as remarkable resources to setup and maintain proper execution platforms. Such issues can be tackled by use of automated approaches based on model transformation, which reduce the simulation model building effort, and by the M\&amp;S as a Service (MSaaS) paradigm, which brings the benefits of service-oriented architectures and cloud computing into the M\&amp;S field, so to reduce the costs of M\&amp;S efforts. In this paper, we show how MSaaS platforms can be effectively extended by introducing model transformation services, with specific application to the M\&amp;S-based analysis of complex systems specified by use of SysML. The paper also describes a catalog of currently available model transformation services, in order to show how the proposed MSaaS platform may ease the introduction of M\&amp;S approaches at any stage of the system development cycle.},
	booktitle = {Proceedings of the {Model}-{Driven} {Approaches} for {Simulation} {Engineering} {Symposium}},
	publisher = {Society for Computer Simulation International},
	author = {Bocciarelli, Paolo and D'Ambrogio, Andrea and Giglio, Andrea and Paglia, Emiliano},
	year = {2018},
	note = {event-place: Baltimore, Maryland},
	keywords = {MDA, model-driven, model transformation, cloud computing, MSaaS},
}

@inproceedings{durak_modeling_2018,
	address = {San Diego, CA, USA},
	series = {{Mod4Sim} '18},
	title = {Modeling and simulation based development of an enhanced ground proximity warning system for multicore targets},
	isbn = {978-1-5108-6018-6},
	abstract = {The advances in Cyber-Physical Systems (CPS) are also effecting the aeronautics. The growth of the cyber layer in aircraft is demanding higher throughput and eventually multi-core systems are becoming topics of interest. The development of parallel real-time systems for multicore processors requires new approaches in model-based design and simulation-based verification. The Enhanced Ground Proximity Warning System (EGPWS) is a terrain awareness system that creates aural and visual warnings for the pilot to prevent Controlled Flight into Terrain (CFIT). This paper presents a multi-core parallelization workflow and a corresponding x-in-the-loop testing pipeline for model-based development of an EGPWS.},
	booktitle = {Proceedings of the {Model}-{Driven} {Approaches} for {Simulation} {Engineering} {Symposium}},
	publisher = {Society for Computer Simulation International},
	author = {Durak, Umut and Müller, David and Möcke, Florian and Koch, Claus B.},
	year = {2018},
	note = {event-place: Baltimore, Maryland},
	keywords = {enhanced ground proximity warning systems, model-based development, multi-core parallelization, x-in-the-loop testing},
}

@inproceedings{balaban_mediation-based_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Mediation-{Based} {MLM} in {USE}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688214},
	doi = {10.1145/3652620.3688214},
	abstract = {Multi Level Modeling (MLM) has been around in the modeling community for over twenty years. It has attracted much attention, both on theoretical and practical grounds. Multiple approaches have emerged, with different support for MLM concepts on the levels of syntax, semantics and pragmatics. MLM tools support a variety of applications, ranging from ontology specification to software modeling.This paper introduces the MedMLM-USE tool, which implements the Mediation-based MLM theory as an extension of the USE tool. The USE tool has been selected due to its open, well-structured architecture. The MedMLM-USE application extends: (1) the well-defined meta-model that is supported by USE, with MedMLM concepts; (2) the USE modeling services to support MLM-models. This paper describes the extended meta-model and services, provides examples, defines an inheritance semantics for instance-of, and discusses engineering difficulties.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Balaban, Mira and Hamann, Lars and Khais, Gil and Saad, Amiel Amram and Maraee, Azzam and Sturm, Arnon},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {multi-level modeling, MLM implementation, MLM semantics},
	pages = {818--827},
}

@article{drumheller_online_2020,
	title = {Online system modeling and documentation using {ROS} snapshot},
	volume = {36},
	issn = {1937-4771},
	abstract = {Robotic systems are complex systems running a number of software components to control the hardware. The Robot Operating System (ROS) is often used in such systems. The integration of these software components, even when the interfaces are heavily documented, can become a daunting task. As part of a typical Software Development Life Cycle, requirements and interfaces change for entire software systems as well as the individual software components that make up those systems.Model-Integrated Computing (MIC) can be used to manage such complexity, and provides a means for validating the system integration throughout its lifecycle. Unfortunately, current MIC tools for ROS systems are limited and lack the ability to automatically gather information needed to make models for ROS entities. This paper presents a new tool, ROS Snapshot, that captures a snapshot of an existing ROS-based system during runtime, and fully documents the system using specified metamodels for each ROS entity. The resulting system model can then be used to document the current state of the system, which is especially important when upgrading versions (e.g. conversion from ROS 1.0 to ROS 2.0). We present a set of current applications for the ROS Snapshot tool, as well as future development plans to integrate the acquired system model into a full MIC system.},
	number = {3},
	journal = {J. Comput. Sci. Coll.},
	author = {Drumheller, William R. and Conner, David C.},
	month = oct,
	year = {2020},
	note = {Place: Evansville, IN, USA
Publisher: Consortium for Computing Sciences in Colleges},
	pages = {128--141},
}

@inproceedings{gao_utilizing_2024,
	address = {New York, NY, USA},
	series = {{ICBIT} '24},
	title = {Utilizing {Deep} {Learning} for {Named} {Entity} {Recognition} in {Ancient} {Chinese} {Stroke} {Medical} {Cases}},
	isbn = {979-8-4007-1006-3},
	url = {https://doi.org/10.1145/3700486.3700511},
	doi = {10.1145/3700486.3700511},
	abstract = {Objective: Based on the deep learning method, to construct the named entity recognition model of Ming and Qing dynasty stroke medical cases, to extract as comprehensive as possible the effective information in the Ming and Qing dynasty stroke medical cases, and to realise the transmission and sharing of knowledge.Method: (1) Collation of textual data from ancient stroke cases and comprehensive information labelling of the texts. (2) Train a Bert-BiLSTM-CRF model of ancient stroke medical cases based on the annotated dataset. (3) The constructed model was evaluated using recall and F1 value.Results: The Bert-BiLSTM-CRF model for stroke medical cases was successfully constructed, and automatic identification of key entities was achieved. The evaluation results show that among the 17 named entities designed in this study, the model has a higher accuracy rate than the others for three entities, namely, Chinese medicine, dose, and pulse, and their F1 values are 85\%, 90\%, and 85\%, respectively.Conclusion: The Bert-BiLSTM-CRF model of ancient stroke cases constructed in this study achieves the automatic identification of entities in medical cases, verifies the effectiveness of NER technology in the processing of ancient Chinese medical books, and provides a reference for research in similar fields.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Biomedicine} and {Intelligent} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Gao, Jingjing and Wang, Chunyan and Wang, Ruixiang},
	year = {2024},
	keywords = {Natural language processing, Named entity recognition, Deep learning, Stroke, Ancient medical cases},
	pages = {152--157},
}

@inproceedings{palumbo_sonet_2019,
	address = {New York, NY, USA},
	series = {{GeoHumanities} '19},
	title = {{SONET}: a semantic ontological network graph for managing points of interest data heterogeneity},
	isbn = {978-1-4503-6960-2},
	url = {https://doi.org/10.1145/3356991.3365474},
	doi = {10.1145/3356991.3365474},
	abstract = {Scalability, standardization, and management are important issues when working with very large Volunteered Geographic Information (VGI). VGI is a rich and valuable source of Points of Interest (POI) information, but its inherent heterogeneity in content, structure, and scale across sources present major challenges for interlinking data sources for analysis. To be useful at scale, the raw information needs to be transformed into a standardized schema that can be easily and reliably used by data analysts. In this work, we tackle the problem of unifying POI categories (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving land use maps and population distribution estimation as well as support data analysts wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform or working with projects that are already configured in the OSM schema and wish to add additional sources of information. Graph theory and its implementation through the SONET graph database, provides a programmatic way to organize, store, and retrieve standardized POI categories at multiple levels of abstraction. Additionally, it addresses category heterogeneity across data sources by standardizing and managing categories in a way that makes cross-domain analysis possible.},
	booktitle = {Proceedings of the 3rd {ACM} {SIGSPATIAL} {International} {Workshop} on {Geospatial} {Humanities}},
	publisher = {Association for Computing Machinery},
	author = {Palumbo, Rachel and Thompson, Laura and Thakur, Gautam},
	year = {2019},
	note = {event-place: Chicago, Illinois},
	keywords = {ontology, graph database, big data, openstreetmap, points of interest},
}

@inproceedings{huang_few-shot_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Few-{Shot} {Fine}-{Grained} {Entity} {Typing} with {Automatic} {Label} {Interpretation} and {Instance} {Generation}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539443},
	doi = {10.1145/3534678.3539443},
	abstract = {We study the problem of few-shot Fine-grained Entity Typing (FET), where only a few annotated entity mentions with contexts are given for each entity type. Recently, prompt-based tuning has demonstrated superior performance to standard fine-tuning in few-shot scenarios by formulating the entity type classification task as a ”fill-in-the-blank” problem. This allows effective utilization of the strong language modeling capability of Pre-trained Language Models (PLMs). Despite the success of current prompt-based tuning approaches, two major challenges remain: (1) the verbalizer in prompts is either manually designed or constructed from external knowledge bases, without considering the target corpus and label hierarchy information, and (2) current approaches mainly utilize the representation power of PLMs, but have not explored their generation power acquired through extensive general-domain pre-training. In this work, we propose a novel framework for few-shot FET consisting of two modules: (1) an entity type label interpretation module automatically learns to relate type labels to the vocabulary by jointly leveraging few-shot instances and the label hierarchy, and (2) a type-based contextualized instance generator produces new instances based on given instances to enlarge the training set for better generalization. On three benchmark datasets, our model outperforms existing methods by significant margins.},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Jiaxin and Meng, Yu and Han, Jiawei},
	year = {2022},
	note = {event-place: Washington DC, USA},
	keywords = {few-shot learning, prompt-based learning, entity typing},
	pages = {605--614},
}

@article{hirzel_stream_2018,
	title = {Stream {Processing} {Languages} in the {Big} {Data} {Era}},
	volume = {47},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/3299887.3299892},
	doi = {10.1145/3299887.3299892},
	abstract = {This paper is a survey of recent stream processing languages, which are programming languages for writing applications that analyze data streams. Data streams, or continuous data flows, have been around for decades. But with the advent of the big-data era, the size of data streams has increased dramatically. Analyzing big data streams yields immense advantages across all sectors of our society. To analyze streams, one needs to write a stream processing application. This paper showcases several languages designed for this purpose, articulates underlying principles, and outlines open challenges.},
	number = {2},
	journal = {SIGMOD Rec.},
	author = {Hirzel, Martin and Baudart, Guillaume and Bonifati, Angela and Della Valle, Emanuele and Sakr, Sherif and Akrivi Vlachou, Akrivi},
	month = dec,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {29--40},
}

@inproceedings{xie_novel_2025,
	address = {New York, NY, USA},
	series = {{SECA} '25},
	title = {A {Novel} {Artificial} {Intelligence} {Voice} {Electronic} {Medical} {Record} {Based} on {Blockchain}},
	isbn = {979-8-4007-1513-6},
	url = {https://doi.org/10.1145/3747912.3747914},
	doi = {10.1145/3747912.3747914},
	abstract = {Nowadays, the global digital transformation of healthcare is advancing rapidly with the help of technologies such as electronic medical records, telemedicine, and mobile medical applications. However, there are still challenges in EMR interoperability, security, and data exchange. To address these existing limitations, This study proposes a voice electronic medical record system driven by artificial intelligence and blockchain, which is designed to improve clinical records and nursing coordination. This system adopts a dedicated deep learning architecture. It transcribe the conversations between doctors and patients into text, and then uses natural language processing to extract the relevant medical information. At the same time, it also provides diagnostic prompts, which can reduce the risk of misdiagnosis. Doctors can view and edit these summaries generated by artificial intelligence. Then safely record them on the decentralized crypto blockchain ledger. With federated learning, the model can be continuously improved in multiple centers without infringing on data privacy. This solution integrates automatic speech recognition, distributed ledger technology, and collaborative deep learning, aiming to enhance the EMR efficiency, security, data integrity, and care continuity of medical institutions. The combination of blockchain technology and artificial intelligence technology holds great potential. It can transform fragmented health data into portable and interoperable records under patient control, thus bringing strategic advantages to the health system that is undergoing a comprehensive digital transformation.},
	booktitle = {Proceedings of the 2025 {International} {Conference} on {Software} {Engineering} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Xie, Weiming and Yao, Zhaomin and Bai, Xiaozhou and Mai, Lang and Zhan, Ying and Wu, Xiaodan and Dai, Yingxin and Pei, Yusong and Zhang, Guoxu and Wang, Zhiguo},
	year = {2025},
	keywords = {Blockchain, Artificial Intelligence, Digital Healthcare, Electronic Medical Records, Speech Recognition},
	pages = {12--19},
}

@inproceedings{ananieva_conceptual_2020,
	address = {New York, NY, USA},
	series = {{SPLC} '20},
	title = {A conceptual model for unifying variability in space and time},
	isbn = {978-1-4503-7569-6},
	url = {https://doi.org/10.1145/3382025.3414955},
	doi = {10.1145/3382025.3414955},
	abstract = {Software engineering faces the challenge of developing and maintaining systems that are highly variable in space (concurrent variations of the system at a single point in time) and time (sequential variations of the system due to its evolution). Recent research aims to address this need by managing variability in space and time simultaneously. However, such research often relies on nonuniform terminologies and a varying understanding of concepts, as it originates from different communities: software product-line engineering and software configuration management. These issues complicate the communication and comprehension of the concepts involved, impeding the development of techniques to unify variability in space and time. To tackle this problem, we performed an iterative, expert-driven analysis of existing tools to derive the first conceptual model that integrates and unifies terminologies and concepts of both dimensions of variability. In this paper, we present the unification process of concepts for variability in space and time, and the resulting conceptual model itself. We show that the conceptual model achieves high coverage and that its concepts are of appropriate granularity with respect to the tools for managing variability in space, time, or both that we considered. The conceptual model provides a well-defined, uniform terminology that empowers researchers and developers to compare their work, clarifies communication, and prevents redundant developments.},
	booktitle = {Proceedings of the 24th {ACM} {Conference} on {Systems} and {Software} {Product} {Line}: {Volume} {A} - {Volume} {A}},
	publisher = {Association for Computing Machinery},
	author = {Ananieva, Sofia and Greiner, Sandra and Kühn, Thomas and Krüger, Jacob and Linsbauer, Lukas and Grüner, Sten and Kehrer, Timo and Klare, Heiko and Koziolek, Anne and Lönn, Henrik and Krieter, Sebastian and Seidl, Christoph and Ramesh, S. and Reussner, Ralf and Westfechtel, Bernhard},
	year = {2020},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {variability, product lines, revision management, version control},
}

@inproceedings{cai_agentir_2024,
	address = {New York, NY, USA},
	series = {{SIGIR} '24},
	title = {{AgentIR}: 1st {Workshop} on {Agent}-based {Information} {Retrieval}},
	isbn = {979-8-4007-0431-4},
	url = {https://doi.org/10.1145/3626772.3657989},
	doi = {10.1145/3626772.3657989},
	abstract = {Information retrieval (IR) systems have become an essential component in modern society to help users find useful information, which consists of a series of processes including query expansion, item recall, item ranking and re-ranking, etc. Based on the ranked information list, users can provide their feedbacks. Such an interaction process between users and IR systems can be naturally formulated as a decision-making problem, which can be either one-step or sequential. In the last ten years, deep reinforcement learning (DRL) has become a promising direction for decision-making, since DRL utilizes the high model capacity of deep learning for complex decision-making tasks. On the one hand, there have been emerging research works focusing on leveraging DRL for IR tasks. However, the fundamental information theory under DRL settings, the challenge of RL methods for Industrial IR tasks, or the simulations of DRL-based IR systems, has not been deeply investigated. On the other hand, the emerging LLM provides new opportunities for optimizing and simulating IR systems. To this end, we propose the first Agent-based IR workshop at SIGIR 2024, as a continuation from one of the most successful IR workshops, DRL4IR. It provides a venue for both academia researchers and industry practitioners to present the recent advances of both DRL-based IR systems and LLM-based IR systems from the agent-based IR's perspective, to foster novel research, interesting findings, and new applications.},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Cai, Qingpeng and Zhao, Xiangyu and Pan, Ling and Xin, Xin and Huang, Jin and Zhang, Weinan and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
	year = {2024},
	note = {event-place: Washington DC, USA},
	keywords = {llm, agent-based information retrieval, drl},
	pages = {3025--3028},
}

@inproceedings{iqtidar_model-driven_2019,
	address = {New York, NY, USA},
	series = {{ICCCM} '19},
	title = {Model-{Driven} approach to {Integrate} {Requirements} for {Safety}-{Critical} {Systems}},
	isbn = {978-1-4503-7195-7},
	url = {https://doi.org/10.1145/3348445.3351305},
	doi = {10.1145/3348445.3351305},
	abstract = {A sophisticated approach is required to elicit requirements for Safety-Critical Systems (SCS). Incomplete, inconsistent or ambiguous requirements can result in many safety-critical catastrophes. While specifying a SCS, it is one of the greatest challenges to extract a complete set of consistent requirements. To overcome this problem, we have proposed a meta-model in this paper for integration of requirements which were specified using several representations to ensure the completeness of requirements. The idea is to use a database, for the integration of the extracted data that will implement the meta-model for the requirements. Problems like inconsistencies and ambiguities can be identified and solved according to the defined meta-model, which will help us in the development and testing phase and will minimize the project chaos. We have validated the proposed methodology with using insulin pump system case study. For the Requirement Engineering process of Safety-Critical Systems, the proposed approach is highly beneficial as final product will result in fewer defects, reduced development cost by avoiding rework, easy maintenance, increased satisfaction of the stakeholders and possibility of faster delivery of the safety critical system.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Computer} and {Communications} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Iqtidar, Khushbakht and Azam, Farooque and Anwar, Muhammad Waseem and Amjad, Anam},
	year = {2019},
	note = {event-place: Bangkok, Thailand},
	keywords = {meta-model, model-driven requirement engineering, Safety-Critical System (SCS)},
	pages = {58--62},
}

@article{katyayan_development_2021,
	title = {Development of {Automatic} {Rule}-based {Semantic} {Tagger} and {Karaka} {Analyzer} for {Hindi}},
	volume = {21},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3479155},
	doi = {10.1145/3479155},
	abstract = {Hindi is the third most-spoken language in the world (615 million speakers) and has the fourth highest native speakers (341 million). It is an inflectionally rich and relatively free word-order language with an immense vocabulary set. Despite being such a celebrated language across the globe, very few Natural Language Processing (NLP) applications and tools have been developed to support it computationally. Moreover, most of the existing ones are not efficient enough due to the lack of semantic information (or contextual knowledge). Hindi grammar is based on Paninian grammar and derives most of its rules from it. Paninian grammar very aggressively highlights the role of karaka theory in free-word order languages. In this article, we present an application that extracts all possible karakas from simple Hindi sentences with an accuracy of 84.2\% and an F1 score of 88.5\%. We consider features such as Parts of Speech tags, post-position markers (vibhaktis), semantic tags for nouns and syntactic structure to grab the context in different-sized word windows within a sentence. With the help of these features, we built a rule-based inference engine to extract karakas from a sentence. The application takes in a text file with clean (without punctuation) simple Hindi sentences and gives back karaka tagged sentences in a separate text file as output.},
	number = {2},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Katyayan, Pragya and Joshi, Nisheeth},
	month = nov,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {feature extraction, Karaka analyzer, language resource, semantic tagging},
}

@inproceedings{halilaj_towards_2021,
	address = {New York, NY, USA},
	series = {{SAC} '21},
	title = {Towards a knowledge graph-based approach for context-aware points-of-interest recommendations},
	isbn = {978-1-4503-8104-8},
	url = {https://doi.org/10.1145/3412841.3442056},
	doi = {10.1145/3412841.3442056},
	abstract = {Context-aware Recommender Systems (CARS) are becoming an integral part of the everyday life by providing users the ability to retrieve relevant information based on their contextual situation. To increase the predictive power considering many parameters, such as mood, hunger level and user preferences, information from heterogeneous sources should be leveraged. However, these data sources are typically isolated and unexplored and the efforts for integrating them are exacerbated by variety of data structures used for their modelling and costly pre-processing operations. We propose a Knowledge Graph-based approach to allow integration of data according to abstract semantic models for Points-of-Interests (POI)s recommendation scenarios. By enriching data with information about attributes, relationships and their meaning, additional knowledge can be derived from what already exists. We demonstrate the applicability of the proposed approach with a concrete example showing benefits of the retrieving the dispersed data with a unified access mechanism.},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Halilaj, Lavdim and Lüttin, Jürgen and Rothermel, Susanne and Arumugam, Santhosh Kumar and Dindorkar, Ishan},
	year = {2021},
	note = {event-place: Virtual Event, Republic of Korea},
	keywords = {knowledge graphs, context-aware POI recommendations, ontology-based data integration},
	pages = {1846--1854},
}

@inproceedings{yin_multi-level_2019,
	address = {New York, NY, USA},
	series = {{MM} '19},
	title = {Multi-{Level} {Fusion} based {Class}-aware {Attention} {Model} for {Weakly} {Labeled} {Audio} {Tagging}},
	isbn = {978-1-4503-6889-6},
	url = {https://doi.org/10.1145/3343031.3351090},
	doi = {10.1145/3343031.3351090},
	abstract = {Recognizing ongoing events based on acoustic clues has been a critical research problem for a variety of AI applications. Compared to visual inputs, acoustic cues tend to be less descriptive and less consistent in time domain. The duration of a sound event can be quite short, which creates great difficulties for, especially weakly labeled, audio tagging. To solve these challenges, we present a novel end-to-end multi-level attention model that first makes segment-level predictions with temporal modeling, followed by advanced aggregations along both time and feature domains. Our model adopts class-aware attention based temporal fusion to highlight/suppress the relevant/irrelevant segments to each class. Moreover, to improve the representation ability of acoustic inputs, a new multi-level feature fusion method is proposed to obtain more accurate segment-level predictions, as well as to perform more effective multi-layer aggregation of clip-level predictions. We additionally introduce a weight sharing strategy to reduce model complexity and overfitting. Comprehensive experiments have been conducted on the AudioSet and the DCASE17 datasets. Experimental results show that our proposed method works remarkably well and obtains the state-of-the-art audio tagging results on both datasets. Furthermore, we show that our proposed multi-level fusion based model can be easily integrated with existing systems where additional performance gain can be obtained.},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Yin, Yifang and Chiou, Meng-Jiun and Liu, Zhenguang and Shrivastava, Harsh and Shah, Rajiv Ratn and Zimmermann, Roger},
	year = {2019},
	note = {event-place: Nice, France},
	keywords = {audio tagging, attention-based model, convolutional recurrent neural network, multi-layer feature fusion},
	pages = {1304--1312},
}

@article{fox_multiples_2020,
	title = {Multiples {Over} {Models}: {Interrogating} the {Past} and {Collectively} {Reimagining} the {Future} of {Menstrual} {Sensemaking}},
	volume = {27},
	issn = {1073-0516},
	url = {https://doi.org/10.1145/3397178},
	doi = {10.1145/3397178},
	abstract = {In this article, we describe our efforts to retrace and reimagine period tracking technology—or, mobile applications designed to support the documentation and quantification of menstrual cycle data. In their current form, these systems often encourage those who menstruate to extract intimate information about the body (e.g., consistency or color of menstrual flow, physical and emotional symptoms), while promising to predict fertility and offer insight into managing one's period. In doing so, these technologies subtly dictate the forms of knowledge and types of relationships menstruators are expected to establish with their bodies (i.e., transactional or instrumentalized). Through historical analysis and a series of participatory experiments, we offer a vision for menstrual sensemaking that expands on these forms of interaction and ways of knowing to emphasize multiplicity and dimensionality rather than models, predictability, or a user's relation to averages or norms.},
	number = {4},
	journal = {ACM Trans. Comput.-Hum. Interact.},
	author = {Fox, Sarah E. and Menking, Amanda and Eschler, Jordan and Backonja, Uba},
	month = sep,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Menstruation, self-tracking, transgender health, women's health},
}

@inproceedings{androutsopoulou_framework_2018,
	address = {New York, NY, USA},
	series = {{ICEGOV} '18},
	title = {A framework for evidence based policy making combining big data, dynamic modelling and machine intelligence},
	isbn = {978-1-4503-5421-9},
	url = {https://doi.org/10.1145/3209415.3209427},
	doi = {10.1145/3209415.3209427},
	abstract = {Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Theory} and {Practice} of {Electronic} {Governance}},
	publisher = {Association for Computing Machinery},
	author = {Androutsopoulou, Aggeliki and Charalabidis, Yannis},
	year = {2018},
	note = {event-place: Galway, Ireland},
	keywords = {Big data, data mining, behavioural patterns, dynamic simulation, evidence based policy making, impact assessment, policy Modelling},
	pages = {575--583},
}

@inproceedings{du_construction_2025,
	address = {New York, NY, USA},
	series = {{CIBDA} '25},
	title = {Construction of visual knowledge graph of crop diseases and pests based on deep learning},
	isbn = {979-8-4007-1316-3},
	url = {https://doi.org/10.1145/3746709.3746742},
	doi = {10.1145/3746709.3746742},
	abstract = {In response to the complex entity relationships, data aggregation difficulties, and knowledge sharing challenges in the field of crop pests and diseases, this study leverages the advantages of knowledge graph structure to propose a deep learning-based construction method. This method is grounded in domain ontology and employs a new annotation model to transform entity and relationship extraction into sequence labeling problems, with simultaneous labeling to enhance efficiency. It directly models ternary relationships to address the challenge of extracting overlapping relationships. Using the BERT-BiLSTM +CRF end-to-end model for experimentation, its F1 score reaches 91.34\%, outperforming various classic models. Finally, the extracted knowledge is stored in a Neo4j graph database to achieve knowledge visualization and inference, providing a high-quality knowledge base for downstream applications.},
	booktitle = {Proceedings of the 2025 6th {International} {Conference} on {Computer} {Information} and {Big} {Data} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Du, Xinmeng and Wang, Yiruo and Ying, Qunbo and Zeng, Qunyao and Zhang, Yuanjie and Zhao, Li},
	year = {2025},
	keywords = {deep learning, knowledge graph, model, Crop, entity relationship joint extraction, pest and disease},
	pages = {178--182},
}

@inproceedings{silva_generating_2020,
	address = {New York, NY, USA},
	series = {{SBCARS} '20},
	title = {Generating {Trustworthiness} {Adaptation} {Plans} {Based} on {Quality} {Models} for {Cloud} {Platforms}},
	isbn = {978-1-4503-8754-5},
	url = {https://doi.org/10.1145/3425269.3425272},
	doi = {10.1145/3425269.3425272},
	abstract = {Cloud computing platforms can offer many benefits related to the provision of service processing and storage for hosting client applications. Trustworthiness can be defined as the trust of a customer in a cloud service and its provider; however, the assurance of this property is not trivial. First, trustworthiness in general is not composed by a single quality attribute, but by the combination of multiple attributes, such as data privacy, performance, reliability, etc. Second, during runtime clients can experience a change of the trustworthiness level required by their application due to the degradation of the cloud service. This article presents a solution that monitors during runtime the set of quality attributes of a specific application and generates adaptation plans in order to certify that an adequate resource amount be provided by the cloud in order to keep its trustworthiness level. Our solution is based on quality models to compute the metric associated to each non-functional requirement and their combination them into different types of trustworthiness levels. The main contribution of the solution is to provide an approach which deals with multiple requirements at the same time (or simultaneously) during runtime in order to adapt the cloud resources to keep the trustworthiness level required by the application. The solution was evaluated by an experiment considering a scenario where the application trustworthiness level was composed by three quality attributes: data privacy, performance and reliability. Initial results have shown that the approach is feasible in terms of the execution of the adaptation plans during runtime to certify the trustworthiness level required by the application.},
	booktitle = {Proceedings of the 14th {Brazilian} {Symposium} on {Software} {Components}, {Architectures}, and {Reuse}},
	publisher = {Association for Computing Machinery},
	author = {Silva, Jorge Luiz Machado da and de França, Breno B. Nicolau and Rubira, Cecília Mary Fischer},
	year = {2020},
	note = {event-place: Natal, Brazil},
	keywords = {Cloud Computing, Trustworthiness, Adaptation Planning, Self-adaptive Systems},
	pages = {141--150},
}

@inproceedings{ranganathan_study_2024,
	address = {New York, NY, USA},
	series = {{NLPIR} '23},
	title = {A {Study} {Of} {Dialog} {Summarization} {Across} {Datasets} {And} {Domains}},
	isbn = {979-8-4007-0922-7},
	url = {https://doi.org/10.1145/3639233.3639245},
	doi = {10.1145/3639233.3639245},
	abstract = {This study of dialog summarization covers multi-domain, multimodal and multilingual datasets, and the potential challenges in the different domains. The scope and progress of this rapidly evolving topic rely on the availability of datasets and emerging domains. Such a study can facilitate the cross-application of datasets to different domains to refine models and also aid scenarios where there is a lack of data in privacy-sensitive settings. Further, our work can enable the cross-fertilization of ideas across domains and in different contexts. Our study encompasses current and emerging domains, a comprehensive compilation of datasets, and avenues for further research.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Ranganathan, Aarthi and Tamminaina, Sai Gowtham and Raina, Gaurav},
	year = {2024},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {Machine learning, NLP, Datasets, Dialog summarization, Domains},
	pages = {196--202},
}

@inproceedings{kilanioti_ai-based_2024,
	address = {New York, NY, USA},
	series = {{SETN} '24},
	title = {{AI}-based knowledge graph construction and distributed storage for collaboration on the {Sustainable} {Development} {Goals}},
	isbn = {979-8-4007-0982-1},
	url = {https://doi.org/10.1145/3688671.3688736},
	doi = {10.1145/3688671.3688736},
	abstract = {The achievement of the Sustainable Development Goals (SDGs) is crucial for future generations. The plethora of SDG data available for analysis facilitates the tasks of practitioners that gather and assess SDG data, including intergovernmental organizations, government agencies and social welfare organizations. In this paper, we propose a framework that aspires to have a substantial impact for SDG practitioners: We propose AI-based construction of SDG knowledge graphs. AI-based methods along with dimensionality reduction undertake the task to semantically cluster new uncategorised SDG data and novel indicators and efficiently place them in the environment of a distributed knowledge graph store.},
	booktitle = {Proceedings of the 13th {Hellenic} {Conference} on {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Kilanioti, Irene and Papadopoulos, George Angelos},
	year = {2024},
	keywords = {Artificial Intelligence, distributed knowledge graphs, Hilbert Space Filling Curves, Sustainable Development Goals ontology},
}

@article{roels_conceptual_2019,
	title = {A {Conceptual} {Framework} and {Content} {Model} for {Next} {Generation} {Presentation} {Solutions}},
	volume = {3},
	url = {https://doi.org/10.1145/3331149},
	doi = {10.1145/3331149},
	abstract = {Mainstream presentation tools such as Microsoft PowerPoint were originally built to mimic physical media like photographic slides and still exhibit the same characteristics. However, the state of the art in presentation tools shows that more recent solutions start to go beyond the classic presentation paradigms. For instance, presentations are becoming increasingly non-linear, content is quickly evolving beyond simple text and images and the way we author our presentations is becoming more collaborative. Nevertheless, existing presentation content models are often based on assumptions that do not apply to the current state of presentations any more, making them incompatible for some use cases and limiting the potential of end-user presentation solutions. In order to support state-of-the-art presentation functionality, we rethink the concept of a presentation and introduce a conceptual framework for presentation content. We then present a new content model for presentation solutions based on the Resource-Selector-Link (RSL) hypermedia metamodel. We further discuss an implementation of our model and show some example use cases. We conclude by outlining how design choices in the model address currently unmet needs with regards to extensibility, content reuse, collaboration, semantics, user access management, non-linearity, and context awareness, resulting in better support for the corresponding end-user functionality in presentation tools.},
	number = {EICS},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Roels, Reinout and Signer, Beat},
	month = jun,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {conceptual framework, content model, mindxpres, powerpoint, presentations, slideware},
}

@inproceedings{fenlon_modeling_2020,
	address = {Champaign, Illinois},
	series = {{JCDL} '19},
	title = {Modeling digital humanities collections as research objects},
	isbn = {978-1-7281-1547-4},
	url = {https://doi.org/10.1109/JCDL.2019.00029},
	doi = {10.1109/JCDL.2019.00029},
	abstract = {Advancing digital libraries to increase the sustainability and usefulness of digital scholarship depends on identifying and developing data models capable of representing increasingly complex scholarly products. This paper considers the potential for an emergent model of scientific communication, the research objects data model, to accommodate the complexities of digital humanities collections. Digital humanities collections aggregate and enrich diverse sources of evidence and context, serving simultaneously as "publications" and dynamic, interactive platforms for research. The research objects model is an alternative to traditional formats of publication, facilitating aggregation and description of all of the inputs and outputs of a research process, ranging from datasets to papers to executable code. This model increasingly underpins research infrastructures in some scientific domains, yet its efficacy for representing humanities scholarship, and for undergirding humanities cyberinfrastructure, remains largely untested. This study offers a qualitative content analysis of digital humanities collections relying on a content/context analytical framework for characterizing collection components and their interrelationships. This study then maps those components and relationships into a research objects model to identify the model's strengths and limitations for representing diverse digital humanities scholarship.},
	booktitle = {Proceedings of the 18th {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Fenlon, Katrina},
	year = {2020},
	keywords = {digital humanities, data models, digital libraries, research objects},
	pages = {138--147},
}

@article{chen_modeling_2018,
	title = {Modeling {Queries} with {Contextual} {Snippets} for {Information} {Retrieval}},
	volume = {9},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3161607},
	doi = {10.1145/3161607},
	abstract = {Query expansion under the pseudo-relevance feedback (PRF) framework has been extensively studied in information retrieval. However, most expansion methods are mainly based on the statistics of single terms, which can generate plenty of irrelevant query terms and decrease retrieval performance. To alleviate this problem, we propose an approach that adapts the PRF-based contextual snippets into a context-aware topic model to enhance query representations. Specifically, instead of selecting a series of independent terms, we make full use of the query contextual information and focus on the snippets with the length of n in the PRF documents. Furthermore, we propose a context-aware topic (CAT) model to mine the topic distributions of the query-relevant snippets, namely, fine contextual snippets. In contrast to the traditional topic models that infer the topics from the whole corpus, we establish a bridge between the snippets and the corresponding PRF documents, which can be used for modeling the topics more precisely and efficiently. Finally, the topic distributions of the fine snippets are used for context-aware and topic-sensitive query representations. To evaluate the performance of our approach, we integrate the obtained queries into a topic-based hybrid retrieval model and conduct extensive experiments on various TREC collections. The experimental results show that our query-modeling approach is more effective in boosting retrieval performance compared with the state-of-the-art methods.},
	number = {4},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Chen, Qin and Hu, Qinmin and Huang, Jimmy Xiangji and He, Liang},
	month = jan,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {topic modeling, Contextual snippet, query representation},
}

@inproceedings{owen_dynamic_2019,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '19},
	title = {A {Dynamic} {Hierarchical} {Approach} to {Modelling} and {Orchestrating} the {Web} of {Things} {Using} the {DOM}, {CSS} and {JavaScript}},
	isbn = {978-1-4503-5971-9},
	url = {https://doi.org/10.1145/3290607.3312990},
	doi = {10.1145/3290607.3312990},
	abstract = {There is a lot of work in progress by the W3C and others surrounding a Web standards compliant Web of Things (WoT) which it is hoped will unify the current Internet of Things infrastructure. Our contribution to this uses the Document Object Model (DOM) to represent complex physical environments, with a CSS-like syntax for storing and controlling the state of 'things' within it. We describe how JavaScript can be used in conjunction with these to create an approach which is familiar to Web developers and may help them to transition more smoothly into WoT development. We share our implementation and explore some of the many potential avenues for future research. These include rich WoT development tools and the possibility of content production for physical environments.},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Owen, Alex and Martinez, Kirk},
	year = {2019},
	note = {event-place: Glasgow, Scotland Uk},
	keywords = {internet of things, IoT, linked data, document object model, web development, web of things, WoT},
	pages = {1--6},
}

@inproceedings{johnson_confidentiality_2018,
	address = {New York, NY, USA},
	series = {Programming '18},
	title = {Confidentiality in the process of (model-driven) software development},
	isbn = {978-1-4503-5513-1},
	url = {https://doi.org/10.1145/3191697.3191714},
	doi = {10.1145/3191697.3191714},
	abstract = {Much is now understood about how to develop software that will have good security properties in use. We claim that a topic which needs more attention, in particular from the Bx community, is security, especially confidentiality, in the software development process itself. What is then at issue is not what particular users of the software may be allowed to know, but rather, what particular developers of the software may be allowed to know. How can software development processes guarantee to respect confidentiality without compromising effective development? The question is of general interest across software engineering, but model-driven development (MDD) seems a particularly promising arena in which to address it, because of MDD's focus on separation of concerns. In MDD, different people work with separate models, where (ideally) each model records all and only the information necessary to those who work with it. When necessary, the models are reconciled by bidirectional transformations, which automate a process which would otherwise have to be undertaken manually by the groups of experts meeting and studying both their models in order to bring them back into consistency. In model-driven development confidentiality issues become particularly clear and tractable, and bidirectional transformations have a key technical role. We hope to encourage the community to take up this challenge, and in this paper we begin our own analysis of a selection of the issues, focusing particularly on developing a threat model and some examples of secure restoration of consistency.},
	booktitle = {Companion {Proceedings} of the 2nd {International} {Conference} on the {Art}, {Science}, and {Engineering} of {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Johnson, Michael and Stevens, Perdita},
	year = {2018},
	note = {event-place: Nice, France},
	keywords = {Security, Model-driven software development, Confidentiality, Cospan},
	pages = {1--8},
}

@inproceedings{kingsun_c-dom_2018,
	address = {New York, NY, USA},
	series = {{ACSW} '18},
	title = {C-{DOM}: a structured co-design framework methodology for ontology design and development},
	isbn = {978-1-4503-5436-3},
	url = {https://doi.org/10.1145/3167918.3167944},
	doi = {10.1145/3167918.3167944},
	abstract = {The development of ontologies is traditionally a process of building vocabularies and understanding complexities of a specific domain to link data and infer knowledge. However, this method does not always include the domain experts throughout the entire development process. This paper presents a Co-design framework as a structured methodology for ontology design and development. The framework, known as the Co-Designing Ontologies Methodology (C-DOM), presents a strategy to draw knowledge from the Subject Matter Expert (SME) without the SME needing to know any-thing about how to create an ontology. The C-DOM framework outlines a series of workshops that align to the layers of the Semantic Technologies architecture and has been developed via an iterative design process and tested in different domains with different levels of complexity and purpose. The first iteration focused on the creation of a light-weight ontology to link generic data in the incident and accident. The next iteration built a heavyweight ontology that included complex inference and reasoning within the urban water management domain. This paper describes the C-DOM in detail for re-use within the Semantic Technologies community and outlines its iterative development process undertake.1},
	booktitle = {Proceedings of the {Australasian} {Computer} {Science} {Week} {Multiconference}},
	publisher = {Association for Computing Machinery},
	author = {Kingsun, Melinda and Myers, Trina and Hardy, Dianna},
	year = {2018},
	note = {event-place: Brisband, Queensland, Australia},
	keywords = {co-design, co-development, ontology development methodology},
}

@inproceedings{paley_data_2021,
	address = {New York, NY, USA},
	series = {{ICAIL} '21},
	title = {From data to information: automating data science to explore the {U}.{S}. court system},
	isbn = {978-1-4503-8526-8},
	url = {https://doi.org/10.1145/3462757.3466100},
	doi = {10.1145/3462757.3466100},
	abstract = {The U.S. court system is the nation's arbiter of justice, tasked with the responsibility of ensuring equal protection under the law. But hurdles to information access obscure the inner workings of the system, preventing stakeholders - from legal scholars to journalists and members of the public - from understanding the state of justice in America at scale. There is an ongoing data access argument here: U.S. court records are public data and should be freely available. But open data arguments represent a half-measure; what we really need is open information. This distinction marks the difference between downloading a zip file containing a quarter-million case dockets and getting the real-time answer to a question like "Are pro se parties more or less likely to receive fee waivers?" To help bridge that gap, we introduce a novel platform and user experience that provides users with the tools necessary to explore data and drive analysis via natural language statements. Our approach leverages an ontology configuration that adds domain-relevant data semantics to database schemas to provide support for user guidance and for search and analysis without user-entered code or SQL. The system is embodied in a "natural-language notebook" user experience, and we apply this approach to the space of case docket data from the U.S. federal court system. Additionally, we provide detail on the collection, ingestion and processing of the dockets themselves, including early experiments in the use of language modeling for docket entry classification with an initial focus on motions.},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Paley, Andrew and Zhao, Andong L. Li and Pack, Harper and Servantez, Sergio and Adler, Rachel F. and Sterbentz, Marko and Pah, Adam and Schwartz, David and Barrie, Cameron and Einarsson, Alexander and Hammond, Kristian},
	year = {2021},
	note = {event-place: São Paulo, Brazil},
	keywords = {natural language processing, visualization, information extraction, data analytics, notebook interface},
	pages = {119--128},
}

@inproceedings{almeida_hierarchical_2019,
	address = {New York, NY, USA},
	series = {{SAC} '19},
	title = {A hierarchical architectural model for network security exploring situational awareness},
	isbn = {978-1-4503-5933-7},
	url = {https://doi.org/10.1145/3297280.3297417},
	doi = {10.1145/3297280.3297417},
	abstract = {Often network security technologies used by organizations for securing their computational systems are deficient in providing holistic view of the environment. Based on this, our paper presents an architectural model based on a Situational Awareness approach for securing computational systems in distributed environments. The architecture is called EXEHDA-ISSA and is inspired by SIEM systems. It is composed of three modular software components called Collector, SmartLogger, and Manager. These components are interconnected following a multi-level hierarchical model and provide features such as event collection, hybrid event processing and a hybrid approach to contextual data storage. For the purpose of evaluating this proposal, four case studies were developed to validate the holistic view of security events as well as the model's characteristics such as flexibility, autonomy, scalability and the support to heterogeneity. Finally, the strengths and limitations of our approach are discussed, then followed by future works.},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Almeida, Ricardo Borges and Covalski, Victor and Machado, Roger and Rosa, Diórgenes Yuri Leal da and Yamin, Adenauer Corrêa and Donato, Lucas Medeiros and Pernas, Ana Marilza},
	year = {2019},
	note = {event-place: Limassol, Cyprus},
	keywords = {network security, architectural model, situational awareness},
	pages = {1365--1372},
}

@inproceedings{rinaldi_novel_2022,
	address = {New York, NY, USA},
	series = {{MEDES} '22},
	title = {A {Novel} {Approach} to {Populate} {Multimedia} {Knowledge} {Graph} via {Deep} {Learning} and {Semantic} {Analysis}},
	isbn = {978-1-4503-9219-8},
	url = {https://doi.org/10.1145/3508397.3564846},
	doi = {10.1145/3508397.3564846},
	abstract = {The growth of data in volume and complexity needs automatic tools to manage and process information. Semantic Web Technologies are a silver bullet in this context due to their capacity to transform human-readable contents into machine-readable ones. Knowledge graphs and the related ontologies represent essential tools for managing very large knowledge bases. The population process of these knowledge structures is composed of expensive and time-consuming tasks, and we propose a novel approach to automate the population step. Our approach is based on novel techniques based on semantic analysis and deep learning using NoSQL technologies. Several results to show the effectiveness of our approach is also reported.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Rinaldi, Antonio M. and Russo, Cristiano and Tommasino, Cristian},
	year = {2022},
	note = {event-place: Venice, Italy},
	keywords = {ontology, semantics, deep learning, knowledge graph, NoSQL},
	pages = {40--47},
}

@inproceedings{er-rahmadi_katie_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {{KATIE}: {A} {System} for {Key} {Attributes} {Identification} in {Product} {Knowledge} {Graph} {Construction}},
	isbn = {978-1-4503-9408-6},
	url = {https://doi.org/10.1145/3539618.3591846},
	doi = {10.1145/3539618.3591846},
	abstract = {We present part of Huawei's efforts in building a Product Knowledge Graph (PKG). We want to identify which product attributes (i.e. properties) are relevant and important in terms of shopping decisions to product categories (i.e. classes). This is particularly challenging when the attributes and their values are mined from online product catalogues, i.e. HTML pages. These web pages contain semi-structured data, which do not follow a concerted format and use diverse vocabulary to designate the same features. We propose a system for key attribute identification (KATIE) based on fine-tuning pre-trained models (e.g., DistilBERT) to predict the applicability and importance of an attribute to a category. We also propose an attribute synonyms identification module that allows us to discover synonymous attributes by considering not only their labels' similarities but also the similarity of their values sets. We have evaluated our approach to Huawei categories taxonomy and a set of internally mined attributes from web pages. KATIE guarantees promising performance results compared to the most recent baselines.},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Er-Rahmadi, Btissam and Oncevay, Arturo and Ji, Yuanyi and Pan, Jeff Z.},
	year = {2023},
	note = {event-place: Taipei, Taiwan},
	keywords = {fine-tuning, pre-trained language model, entity resolution, product knowledge graph, relation discovery},
	pages = {3320--3324},
}

@inproceedings{jung_model_2021,
	address = {Vancouver, Canada},
	series = {{LICS} '19},
	title = {Model comparison games for horn description logics},
	abstract = {Horn description logics are syntactically defined fragments of standard description logics that fall within the Horn fragment of first-order logic and for which ontology-mediated query answering is in PTime for data complexity. They were independently introduced in modal logic to capture the intersection of Horn first-order logic with modal logic. In this paper, we introduce model comparison games for the basic Horn description logic hornALC (corresponding to the basic Horn modal logic) and use them to obtain an Ehrenfeucht-Fraïssé type definability result and a van Benthem style expressive completeness result for hornALC. We also establish a finite model theory version of the latter. The Ehrenfeucht-Fraïssé type definability result is used to show that checking hornALC indistinguishability of models is EXPTIME-complete, which is in sharp contrast to ALC indistinguishability (i.e., bisimulation equivalence) checkable in PTime. In addition, we explore the behavior of Horn fragments of more expressive description and modal logics by defining a Horn guarded fragment of first-order logic and introducing model comparison games for it.},
	booktitle = {Proceedings of the 34th {Annual} {ACM}/{IEEE} {Symposium} on {Logic} in {Computer} {Science}},
	publisher = {IEEE Press},
	author = {Jung, Jean Christoph and Papacchini, Fabio and Wolter, Frank and Zakharyaschev, Michael},
	year = {2021},
}

@inproceedings{keller_supporting_2020,
	address = {San Diego, CA, USA},
	series = {{SummerSim} '20},
	title = {Supporting the reuse of algorithmic simulation models},
	isbn = {978-1-7138-1429-0},
	abstract = {Stateless functions, also referred to as algorithmic models, return an output given inputs that all occur at the same time instant. As relatively simple dynamic models, which define the behavior of variables over a timeline, algorithmic models nevertheless encode knowledge of entities that can be essential for use within models in a particular domain. This paper presents a development methodology for representing algorithmic models within the Discrete Event Systems Specifications (DEVS) formalism and employing the System Entity Structure (SES) to organize these models for reuse in new compositions. A use case example is used for illustration of the development process and the benefits in savings of time and effort are illustrated. Finally, some future possibilities to enhance the support of DEVS environments for this methodology are discussed.},
	booktitle = {Proceedings of the 2020 {Summer} {Simulation} {Conference}},
	publisher = {Society for Computer Simulation International},
	author = {Keller, Nicholas and Zeigler, Bernard and Kim, Doohwan and Anderson, Chase and Ceney, James},
	year = {2020},
	note = {event-place: Virtual Event, Spain},
	keywords = {DEVS, reuse, SES, algorithmic},
}

@inproceedings{burgdorf_docsemmap_2022,
	address = {New York, NY, USA},
	series = {{CIKM} '22},
	title = {{DocSemMap} 2.0: {Semantic} {Labeling} based on {Textual} {Data} {Documentations} {Using} {Seq2Seq} {Context} {Learner}},
	isbn = {978-1-4503-9236-5},
	url = {https://doi.org/10.1145/3511808.3557446},
	doi = {10.1145/3511808.3557446},
	abstract = {Methods for automated semantic labeling of data are an indispensable basis for increasing the usability of data. On the one hand, they contribute to the homogenization of the annotations and thus to the increase in quality; on the other hand, they reduce the modeling effort, provided that the quality of the used methodology is sufficient. In the past, research has focused primarily on data- and label-based methods. Another approach that has received recent attention is the incorporation of textual data documentations to support the automatic mapping of datasets to a knowledge graph. However, upon deeper analysis, our recent approach called DocSemMap gives away potential in a number of places. In this paper, we extend the current state of the art approach by uncovering existing shortcomings and presenting our own improvements. Using a sequence-to-sequence model (Seq2Seq), we exploit the context of datasets. An additional introduced classifier provides the linkage of documentation and labels for prediction. Our extended approach achieves a sustainable improvement in comparison to the reference approach.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Burgdorf, Andreas and Paulus, Alexander and Pomp, André and Meisen, Tobias},
	year = {2022},
	note = {event-place: Atlanta, GA, USA},
	keywords = {natural language processing, semantic mapping, classifier, seq2seq},
	pages = {98--107},
}

@inproceedings{guittoum_inferring_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {Inferring {Threatening} {IoT} {Dependencies} using {Semantic} {Digital} {Twins} {Toward} {Collaborative} {IoT} {Device} {Management}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3578573},
	doi = {10.1145/3555776.3578573},
	abstract = {IoT Device Management (DM) refers to registering, configuring, monitoring, and updating IoT devices. DM is facing new challenges as dependencies between IoT devices generate various threats, such as update breaks and cascading failures. Dependencies-related threats are exacerbated by the fragmentation of the DM market, where multiple actors, e.g., operators and device manufacturers, are uncoordinately ensuring DM on interdependent devices, each using its DM solution. Identifying the topology of threatening dependencies is key in developing dependency-aware DM capabilities for legacy DM solutions to tackle dependencies-related threats efficiently. In this work, we apply Semantic Web and Digital Twin technologies to build a decision-support framework that automatically infers the topology of threatening dependencies in IoT systems. We integrate the proposed framework into the in-use Digital Twin platform Thing in the future and demonstrate its effectiveness by inferring threatening dependencies in smart home scenarios managed by ground-truth DM solutions, such as Orange's implementation of the USP Controller and Samsung's SmartThings Platform.},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Guittoum, Amal and Aïssaoui, Francois and Bolle, Sébastien and Boyer, Fabienne and De Palma, Noel},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {ontology, SHACL, semantic web, digital twin, inference, collaboration, dependencies management, entity resolution, IoT device management, thing description},
	pages = {1732--1741},
}

@inproceedings{speth_architecture-based_2024,
	address = {New York, NY, USA},
	series = {{ICSE}-{Companion} '24},
	title = {Architecture-{Based} {Cross}-{Component} {Issue} {Management} and {Propagation} {Analysis}},
	isbn = {979-8-4007-0502-1},
	url = {https://doi.org/10.1145/3639478.3639814},
	doi = {10.1145/3639478.3639814},
	abstract = {This paper addresses the challenge of issue management in complex, component-based software architectures. In these systems, issues in one component often propagate across the architecture along the call chains. Yet, traditional issue management systems (IMSs) are limited to the boundaries of a single component and lack mechanisms for managing issues concerning their architectural dependencies. We present Gropius, a novel method that enhances issue management by integrating issues in an architecture graph. Gropius allows semantically linking issues across different components, synchronizes changes with underlying IMSs like GitHub, and allows modeling the architecture ontologically by defining the components' semantics at runtime. We explore whether combining issue and architecture management improves the development of component-based architectures regarding issue management. We hypothesize that this method will improve the efficiency and effectiveness of identifying and resolving cross-component issues, maintaining a comprehensive view of the application's state.},
	booktitle = {Proceedings of the 2024 {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Speth, Sandro},
	year = {2024},
	note = {event-place: Lisbon, Portugal},
	keywords = {component-based software architecture, issue management, issue propagation analysis, model-based analysis},
	pages = {145--149},
}

@article{duchon_platform_2025,
	title = {A {Platform} {Ecosystem} {Providing} {New} {Data} {For} {The} {Energy} {Transition}},
	volume = {4},
	url = {https://doi.org/10.1145/3717413.3717435},
	doi = {10.1145/3717413.3717435},
	abstract = {There is a great need for high-quality and comprehensive data in the energy sector. This data is collected and preprocessed at considerable expense and is not only required for research, but also by planning offices and other industries in connection with planning activities, such as the creation of municipal heat planning. The NEED ecosystem will accelerate these processes establishing an efficient, robust, and scalable energy data ecosystem. Heterogeneous energy-related data sources will be brought together and automatically linked consistently across different sectors as well as temporal and spatial levels. In this context, existing data sources will not be replaced but rather integrated into the NEED ecosystem as dedicated sources including a semantic description on how to utilize them. In addition to conventional data sources from the various planning levels, we envision a quality assessment scheme based on the FAIR criteria. In reality, we are often faced with missing data, too. To close this gap we explore data-driven, model-driven, AI-based, and tool-driven generation of synthetic data. These heterogeneous data sources will be interlinked using ontology modules which will be represented in a knowledge graph. Via a semantic API, queries will be generated to identify the required data sources, which will be orchestrated to provide the data needed. This will enable researchers, planners, and others including their tools to interact with the NEED ecosystem, while a tool proxy will be able to translate the resulting data into proprietary formats, required by some tools to operate. The NEED ecosystem is planned to be a robust, easy-to-maintain, and flexible infrastructure to enhance planning energy measures at different spatial levels and with different time horizons. We envision to evaluate our NEED approach for the transparent provision of data by integrating relevant data sources as microservices, definition and analysis of application scenarios in the planning domain, as well as the integration of various tools for different planning purposes. With these elements, we will be able to quantify the efficiency of data procurement and demonstrate the functionality of the approach using practical use cases.},
	number = {4},
	journal = {SIGENERGY Energy Inform. Rev.},
	author = {Duchon, Markus and Matar, Jessy and Shoyari, Mahsa Faraji and Perzylo, Alexander and Kessler, Ingmar and Buchenberg, Patrick and Kuhn, Philipp and Hamacher, Thomas and Schlachter, Thorsten and Süss, Wolfgang and Thinh, Nguyen Xuan and Salari, Haniyeh Ebrahimi and Latko, Jasmin and Xu, Minsheng and Shamovich, Maxim and Schlütter, Dominik and Frisch, Jérôme and Rustagi, Kushagar and Kraft, Markus and Ayasse, Carolin and Steinke, Florian and Metzger, Michael and Kuper, Laura},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {226--237},
}

@inproceedings{spitz_versatile_2020,
	address = {New York, NY, USA},
	series = {{SSDBM} '20},
	title = {A {Versatile} {Hypergraph} {Model} for {Document} {Collections}},
	isbn = {978-1-4503-8814-6},
	url = {https://doi.org/10.1145/3400903.3400919},
	doi = {10.1145/3400903.3400919},
	abstract = {Efficiently and effectively representing large collections of text is of central importance to information retrieval tasks such as summarization and search. Since models for these tasks frequently rely on an implicit graph structure of the documents or their contents, graph-based document representations are naturally appealing. For tasks that consider the joint occurrence of words or entities, however, existing document representations often fall short in capturing cooccurrences of higher order, higher multiplicity, or at varying proximity levels. Furthermore, while numerous applications benefit from structured knowledge sources, external data sources are rarely considered as integral parts of existing document models. To address these shortcomings, we introduce heterogeneous hypergraphs as a versatile model for representing annotated document collections. We integrate external metadata, document content, entity and term annotations, and document segmentation at different granularity levels in a joint model that bridges the gap between structured and unstructured data. We discuss selection and transformation operations on the set of hyperedges, which can be chained to support a wide range of query scenarios. To ensure compatibility with established information retrieval methods, we discuss projection operations that transform hyperedges to traditional dyadic cooccurrence graph representations. Using PostgreSQL and Neo4j, we investigate the suitability of existing database systems for implementing the hypergraph document model, and explore the impact of utilizing implicit and materialized hyperedge representations on storage space requirements and query performance.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Spitz, Andreas and Aumiller, Dennis and Soproni, Bálint and Gertz, Michael},
	year = {2020},
	note = {event-place: Vienna, Austria},
}

@inproceedings{colusso_translational_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {A {Translational} {Science} {Model} for {HCI}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300231},
	doi = {10.1145/3290605.3300231},
	abstract = {Using scientific discoveries to inform design practice is an important, but difficult, objective in HCI. In this paper, we provide an overview of Translational Science in HCI by triangulating literature related to the research-practice gap with interview data from many parties engaged (or not) in translating HCI knowledge. We propose a model for Translational Science in HCI based on the concept of a continuum to describe how knowledge progresses (or stalls) through multiple steps and translations until it can influence design practice. The model offers a conceptual framework that can be used by researchers and practitioners to visualize and describe the progression of HCI knowledge through a sequence of translations. Additionally, the model may facilitate a precise identification of translational barriers, which allows devising more effective strategies to increase the use of scientific findings in design practice.},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Colusso, Lucas and Jones, Ridley and Munson, Sean A. and Hsieh, Gary},
	year = {2019},
	note = {event-place: Glasgow, Scotland Uk},
	keywords = {translational science, research-practice gap, translational research},
	pages = {1--13},
}

@inproceedings{khan_exaquery_2024,
	address = {New York, NY, USA},
	series = {{ICPE} '24 {Companion}},
	title = {{ExaQuery}: {Proving} {Data} {Structure} to {Unstructured} {Telemetry} {Data} in {Large}-{Scale} {HPC}},
	isbn = {979-8-4007-0445-1},
	url = {https://doi.org/10.1145/3629527.3652898},
	doi = {10.1145/3629527.3652898},
	abstract = {High-performance computing (HPC) is the cornerstone of technological advancements in our digital age, but its management is becoming increasingly challenging, particularly as systems approach exascale. Operational data analytics (ODA) and holistic monitoring frameworks aim to alleviate this burden by collecting live telemetry from HPC systems. ODA frameworks rely on NoSQL databases for scalability, with implicit data structures embedded in metric names, necessitating domain knowledge for navigating telemetry data relations. To address the imperative need for explicit representation of relations in telemetry data, we propose a novel ontology for ODA, which we apply to a real HPC installation. The proposed ontology captures relationships between topological components and links hardware components(compute nodes, rack, systems) with job's execution and allocations collected telemetry. This ontology forms the basis for constructing a knowledge graph, enabling graph queries for ODA. Moreover, we propose a comparative analysis of the complexity (expressed in lines of code) and domain knowledge requirement (qualitatively assessed by informed end-users) of complex query implementation with the proposed method and NoSQL methods commonly employed in today's ODAs. We focused on six queries informed by facility managers' daily operations, aiming to benefit not only facility managers but also system administrators and user support. Our comparative analysis demonstrates that the proposed ontology facilitates the implementation of complex queries with significantly fewer lines of code and domain knowledge required as compared to NoSQL methods.},
	booktitle = {Companion of the 15th {ACM}/{SPEC} {International} {Conference} on {Performance} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Khan, Junaid Ahmed and Molan, Martin and Angelinelli, Matteo and Bartolini, Andrea},
	year = {2024},
	note = {event-place: London, United Kingdom},
	keywords = {sparql, high performance computing (hpc), operational data analytics(oda), resource description framework (rdf) ontology},
	pages = {127--134},
}

@inproceedings{purpura_probabilistic_2019,
	address = {New York, NY, USA},
	series = {{ICTIR} '19},
	title = {Probabilistic {Word} {Embeddings} in {Neural} {IR}: {A} {Promising} {Model} {That} {Does} {Not} {Work} as {Expected} ({For} {Now})},
	isbn = {978-1-4503-6881-0},
	url = {https://doi.org/10.1145/3341981.3344217},
	doi = {10.1145/3341981.3344217},
	abstract = {In this paper, we discuss how a promising word vector representation based on Probabilistic Word Embeddings (PWE) can be applied to Neural Information Retrieval (NeuIR). We illustrate PWE pros for text retrieval, and identify the core issues which prevent a full exploitation of their potential. In particular, we focus on the application of elliptical probabilistic embeddings, a type of PWE, to a NeuIR system (i.e., MatchPyramid). The main contributions of this paper are: (i) an analysis of the pros and cons of PWE in NeuIR; (ii) an in-depth comparison of PWE against pre-trained Word2Vec, FastText and WordNet word embeddings; (iii) an extension of the MatchPyramid model to take advantage of broader word relations information from WordNet; (iv) a topic-level evaluation of the MatchPyramid ranking models employing the considered word embeddings. Finally, we discuss some lessons learned and outline some open research problems to employ PWE in NeuIR systems more effectively.},
	booktitle = {Proceedings of the 2019 {ACM} {SIGIR} {International} {Conference} on {Theory} of {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Purpura, Alberto and Maggipinto, Marco and Silvello, Gianmaria and Susto, Gian Antonio},
	year = {2019},
	note = {event-place: Santa Clara, CA, USA},
	keywords = {natural language processing, neural information retrieval, probabilistic word embedding},
	pages = {3--10},
}

@inproceedings{mansfield_capturing_2021,
	address = {New York, NY, USA},
	series = {K-{CAP} '21},
	title = {Capturing {Expert} {Knowledge} for {Building} {Enterprise} {SME} {Knowledge} {Graphs}},
	isbn = {978-1-4503-8457-5},
	url = {https://doi.org/10.1145/3460210.3493569},
	doi = {10.1145/3460210.3493569},
	abstract = {Whilst Knowledge Graphs (KGs) are increasingly used in business scenarios, the construction of enterprise ontologies and the population of KGs from existing relational data remains a significant challenge. In this paper we report our experience in supporting CSols (an SME operating in the analytical laboratory domain) in transitioning their data from legacy databases to a bespoke KG. We modelled the KG using a streamlined approach based on state of the art ontology engineering methodologies, that addresses the challenges faced by SMEs when transitioning to new technologies: lack of resources to devote to the transition, paucity of comprehensive data governance policies, and resistance within the organisation to accepting new practices and knowledge. Our approach uses a combination of UML diagrams and a controlled language glossary to support stakeholders in reaching consensus during the knowledge capture phase, thus reducing the intervention of the ontology engineer only to cases where no agreement can be found. We present a case study illustrating the generation of the KG from a UML specification of part of the analytical domain and from legacy relational data, and we discuss the benefits and challenges of the approach.},
	booktitle = {Proceedings of the 11th {Knowledge} {Capture} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Mansfield, Martin and Tamma, Valentina and Goddard, Phil and Coenen, Frans},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {ontology engineering, enterprise knowledge graphs, r2rml, relational data, uml},
	pages = {129--136},
}

@inproceedings{donadello_explaining_2020,
	address = {New York, NY, USA},
	series = {{SAC} '20},
	title = {Explaining reasoning algorithms with persuasiveness: a case study for a behavioural change system},
	isbn = {978-1-4503-6866-7},
	url = {https://doi.org/10.1145/3341105.3373910},
	doi = {10.1145/3341105.3373910},
	abstract = {Explainable AI aims at building intelligent systems that are able to provide a clear, and human understandable, justification of their decisions. This holds for both rule-based and data-driven methods. In management of chronic diseases, the users of such systems are patients that follow strict dietary rules to manage such diseases. After receiving the input of the intake food, the system performs reasoning to understand whether the users follow an unhealthy behaviour. Successively, the system has to communicate the results in a clear and effective way, that is, the output message has to persuade users to follow the right dietary rules. In this paper, we address the main challenges to build such systems: i) the natural language generation of messages that explain the reasoner inconsistency; ii) the effectiveness of such messages at persuading the users. Results prove that the persuasive explanations are able to reduce the unhealthy users' behaviours.},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Donadello, Ivan and Dragoni, Mauro and Eccher, Claudio},
	year = {2020},
	note = {event-place: Brno, Czech Republic},
	keywords = {ontologies, explainable AI, natural language generation, mHealth, explainable reasoning},
	pages = {646--653},
}

@inproceedings{chaveesuk_understanding_2020,
	address = {New York, NY, USA},
	series = {{MSIE} '20},
	title = {Understanding the {Model} of {User} {Adoption} and {Acceptance} of {Technology} by {Thai} {Farmers}: {A} {Conceptual} {Framework}},
	isbn = {978-1-4503-7706-5},
	url = {https://doi.org/10.1145/3396743.3396781},
	doi = {10.1145/3396743.3396781},
	abstract = {Thailand is constantly making policy changes and implementing Industrial revolution agendas like Thailand 4.0. With the advent of Industry revolution 4.0, Thailand government is making policy initiatives and technological advancement for successful transition to industry 5.0. The initiative is focused to offer farmers an opportunities to incorporate technology in their farming and agricultural processes to produce better crops and high-quality food. The Development of Eastern Economic Corridor is a crucial step in this regard as it focused on the evolution and progression of 10 key industries in Thailand. The plan is designed to be driven through agricultural technology and innovativeness. The future benefits of the ICT based 5.0 industrial revolution in agricultural field directly depends on the user adoption and acceptance of agricultural technology. The study, therefore, investigates the acceptance and adoption of ICT based products and services by farmers by utilizing the basics of TAM. The study proposes the framework of FTAM, and identifies the internal and external factors affecting the behavior intentions and attitude of farmers. It was found out that factors like "occupation relevance", "self-efficacy" and "social influence" affect the "Perceived usefulness (PU)" and "Perceived Ease of Use (PEOU) which in return impact their behavior intention and attitude towards utilizing and accepting ICT based products and services in farming and agriculture.},
	booktitle = {Proceedings of the 2020 2nd {International} {Conference} on {Management} {Science} and {Industrial} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chaveesuk, Singha and Chaiyasoonthorn, Wornchanok and Khalid, Bilal},
	year = {2020},
	note = {event-place: Osaka, Japan},
	keywords = {Internet of Things, Industrial Revolution, Information Communication Technology, innovativeness, Perceived Ease of Use, Perceived Usefulness, Technology Acceptance Model},
	pages = {279--285},
}

@inproceedings{frank_contingent_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Contingent level classes: motivation, conceptualization, modeling guidelines, and implications for model management},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3421413},
	doi = {10.1145/3417990.3421413},
	abstract = {It has been known for some time that the level of a class may vary with the context it is used in. There are a few approaches that enable modelers to deal with corresponding requirements. However, they usually provide workarounds to avoid the problem of one class being on different levels at the same time. In this paper, the need for those classes, which are called contingent level classes, is motivated from a conceptual perspective. A conceptualization of contingent level classes is presented that addresses principal integrity issues and accounts for resulting constraints on class properties and relationships. Based on that conceptualization, the paper provides an analysis of specific challenges related to change operations on models that include contingent level classes. Subsequently, a set of patterns for coping with certain kinds of change operations is presented.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Frank, Ulrich and Töpel, Daniel},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {multi-level modeling, change operations, contingent level class, level jump, possible world},
}

@inproceedings{bompotas_towards_2024,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '24},
	title = {Towards {Exploring} {Personalized} {Hyperlink} {Recommendations} {Through} {Machine} {Learning}},
	isbn = {979-8-4007-0466-6},
	url = {https://doi.org/10.1145/3631700.3664913},
	doi = {10.1145/3631700.3664913},
	abstract = {The Internet offers a wealth of content, making it increasingly difficult for users to navigate website information. The volume of hyperlinks on a website often leaves users struggling with content overload, hindering their ability to find relevant information of high interest. This problem highlights the critical need for tools to improve the user experience by providing personalized hyperlink recommendations on a specific website. This paper introduces HypeRec, a browser extension that attempts to address this problem by leveraging and comparing different machine learning and recommendation algorithms to guide users to content consistent with their interests and preferences. Our approach involves extracting hyperlinks from a webpage and subjecting the corresponding textual content to natural language processing techniques. In this way, it simplifies the users’ navigation within a website and promotes a more intuitive and satisfying web browsing experience.},
	booktitle = {Adjunct {Proceedings} of the 32nd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Bompotas, Agorakis and Triantafyllopoulos, Panagiotis and Raptis, George E. and Katsini, Christina and Makris, Christos},
	year = {2024},
	note = {event-place: Cagliari, Italy},
	keywords = {Natural Language Processing, Machine Learning, Personalization, User Experience, Recommendation Systems, Content Overload, Hyperlink Analysis, Web Navigation, Web Usability},
	pages = {528--533},
}

@inproceedings{malmi_theories_2020,
	address = {New York, NY, USA},
	series = {{ICER} '20},
	title = {Theories and {Models} of {Emotions}, {Attitudes}, and {Self}-{Efficacy} in the {Context} of {Programming} {Education}},
	isbn = {978-1-4503-7092-9},
	url = {https://doi.org/10.1145/3372782.3406279},
	doi = {10.1145/3372782.3406279},
	abstract = {Research into the relationship between learning computing and students' attitudes, beliefs, and emotions often builds on theoretical frameworks from the social sciences in order to understand how these factors influence, for example, students' motivation, study practices, and learning results. In this paper we explore the computing education research literature to identify new theoretical constructs that have emerged from this research. We focus on empirical work in programming education that extends or adapts theories or instruments from the social sciences or that independently develops theories specific to programming. From an initial data set of more than 3800 papers published in the years 2010–2019, we identify 50 papers that present a range of domain-specific theoretical constructs addressing emotions, affect, beliefs, attitudes, and self-efficacy. They include 11 validated instruments and a number of statistical models, but also grounded theories and pedagogical models. We summarize the main results of many of these constructs and provide references for all of them. We also investigate how these constructs have informed further research by analysing over 850 papers that cite these 50 papers. We categorize the ways that theories can inform further research, and give examples of papers in each of these categories. Our findings indicate that among these categories, instruments have been most widely used in further research, thus affirming their value in the field.},
	booktitle = {Proceedings of the 2020 {ACM} {Conference} on {International} {Computing} {Education} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Malmi, Lauri and Sheard, Judy and Kinnunen, Päivi and {Simon} and Sinclair, Jane},
	year = {2020},
	note = {event-place: Virtual Event, New Zealand},
	keywords = {theory, emotion, affect, computing education, instrument, research, theoretical construct, attitude, belief, programming, self-efficacy},
	pages = {36--47},
}

@inproceedings{gagliardi_intuitive_2023,
	address = {New York, NY, USA},
	series = {{HT} '23},
	title = {Intuitive {Semantic} {Graph} {Tool} for {Enhanced} {Archive} {Exploration}},
	isbn = {979-8-4007-0232-7},
	url = {https://doi.org/10.1145/3603163.3609069},
	doi = {10.1145/3603163.3609069},
	abstract = {The paper introduces a new method for visualizing and navigating information in a cultural heritage archive in a simple and intuitive way. The proposed approach employs pre-trained language models to cluster data and create semantic graphs. The creation of multilayer maps enables deep exploration of archives with large datasets, while the ability to handle multilingual datasets makes it suitable for archives with documents in various languages. These features combine to provide a user-friendly tool that can be adapted to different contexts and provides an overview of archive contents, to allow even non expert users to successfully query the archive.},
	booktitle = {Proceedings of the 34th {ACM} {Conference} on {Hypertext} and {Social} {Media}},
	publisher = {Association for Computing Machinery},
	author = {Gagliardi, Isabella and Artese, Maria Teresa},
	year = {2023},
	note = {event-place: Rome, Italy},
	keywords = {clustering, transformers, pre-trained language models, Bert, data visualization, archives, non-expert users},
}

@article{degha_ica-crmas_2024,
	title = {{ICA}-{CRMAS}: {Intelligent} {Context}-{Awareness} {Approach} for {Citation} {Recommendation} based on {Multi}-{Agent} {System}},
	volume = {15},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3680287},
	doi = {10.1145/3680287},
	abstract = {Navigating the ever-expanding sea of scientific literature presents a daunting challenge for researchers seeking relevant and up-to-date information. Traditional citation recommendation systems, while well-intentioned, often fall short due to their limited focus on text-based features and lack of contextual awareness. In this article, we introduce the ICA-CRMAS (Intelligent Context-Aware Approach for Citation Recommendation based on Multi-Agent System), an intelligent system that leverages the power of deep learning, semantic analysis, and multimodal learning to overcome these limitations. ICA-CRMAS goes beyond the surface, delving into the rich tapestry of information within academic papers, including figures, which often hold vital contextual clues. By weaving this contextual data directly into its recommendation models, ICA-CRMAS generates highly personalized and relevant suggestions. This comprehensive approach unlocks enhanced accuracy, diversity, and serendipity, enabling researchers to effectively discover papers aligning with their interests and research objectives. ICA-CRMAS illuminates its reasoning. Instead of opaque suggestions, the system provides clear explanations that justify and illustrate recommended citations. This transparency builds user confidence, allowing researchers to critically engage with and trust the system’s recommendations. Evaluation experiments conducted on real-world academic datasets demonstrate that ICA-CRMAS outperforms existing approaches across various metrics. it surpassing its closest competitor by a margin of 7.53 on accuracy, 6.07\% on MRR and by 5.87 on Recall. User feedback further reinforces its effectiveness, with an Overall System Usability Scale (SUS) score of 76.73, exceeding benchmark scores for comparable systems.},
	number = {3},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Degha, Houssem Eddine and Laallam, Fatima Zohra},
	month = sep,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, Multi-agent systems, context-awareness, search engines, personalized information filtering, recommendation systems, scientific papers, user profiles, world wide web},
}

@inproceedings{theisz_join_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Join potency: a way of combining separate multi-level models},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3421411},
	doi = {10.1145/3417990.3421411},
	abstract = {Multi-level modeling has become a mature modeling paradigm both theoretically and by technical means. It has proved itself when a single domain has to be created without accidental complexity. However, when several interconnected domains are to be handled, multi-level modeling is still not as capable as legacy metamodeling. Our position paper aims to narrow the gap by the introduction of a novel technique that can combine several multi-level models from different domains statically. Besides its theoretical proposal, the solution is also defined in our multi-layer modeling framework (Dynamic Multi-Layer Algebra) and is demonstrated by an illustrative example.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Theisz, Zoltán and Bácsi, Sándor and Mezei, Gergely and Somogyi, Ferenc A. and Palatinszky, Dániel},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {multi-level modeling, clabject, megamodeling, potency notion},
}

@inproceedings{zhong_automatic_2019,
	address = {New York, NY, USA},
	series = {{ICAIL} '19},
	title = {Automatic {Summarization} of {Legal} {Decisions} using {Iterative} {Masking} of {Predictive} {Sentences}},
	isbn = {978-1-4503-6754-7},
	url = {https://doi.org/10.1145/3322640.3326728},
	doi = {10.1145/3322640.3326728},
	abstract = {We report on a pilot experiment in automatic, extractive summarization of legal cases concerning Post-traumatic Stress Disorder from the US Board of Veterans' Appeals. We hypothesize that length-constrained extractive summaries benefit from choosing among sentences that are predictive for the case outcome. We develop a novel train-attribute-mask pipeline using a CNN classifier to iteratively select predictive sentences from the case, which measurably improves prediction accuracy on partially masked decisions. We then select a subset for the summary through type classification, maximum marginal relevance, and a summarization template. We use ROUGE metrics and a qualitative survey to evaluate generated summaries along with expert-extracted and expert-drafted summaries. We show that sentence predictiveness does not reliably cover all decision-relevant aspects of a case, illustrate that lexical overlap metrics are not well suited for evaluating legal summaries, and suggest that future work should focus on case-aspect coverage.},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Zhong, Linwu and Zhong, Ziyi and Zhao, Zinian and Wang, Siyuan and Ashley, Kevin D. and Grabmair, Matthias},
	year = {2019},
	note = {event-place: Montreal, QC, Canada},
	keywords = {text classification, legal case summarization},
	pages = {163--172},
}

@article{guo_attentive_2019,
	title = {Attentive {Long} {Short}-{Term} {Preference} {Modeling} for {Personalized} {Product} {Search}},
	volume = {37},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3295822},
	doi = {10.1145/3295822},
	abstract = {E-commerce users may expect different products even for the same query, due to their diverse personal preferences. It is well known that there are two types of preferences: long-term ones and short-term ones. The former refers to users’ inherent purchasing bias and evolves slowly. By contrast, the latter reflects users’ purchasing inclination in a relatively short period. They both affect users’ current purchasing intentions. However, few research efforts have been dedicated to jointly model them for the personalized product search. To this end, we propose a novel Attentive Long Short-Term Preference model, dubbed as ALSTP, for personalized product search. Our model adopts the neural networks approach to learn and integrate the long- and short-term user preferences with the current query for the personalized product search. In particular, two attention networks are designed to distinguish which factors in the short-term as well as long-term user preferences are more relevant to the current query. This unique design enables our model to capture users’ current search intentions more accurately. Our work is the first to apply attention mechanisms to integrate both long- and short-term user preferences with the given query for the personalized search. Extensive experiments over four Amazon product datasets show that our model significantly outperforms several state-of-the-art product search methods in terms of different evaluation metrics.},
	number = {2},
	journal = {ACM Trans. Inf. Syst.},
	author = {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and Wang, Yinglong and Ma, Jun and Kankanhalli, Mohan},
	month = jan,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {attention mechanism, long short-term preference, Personalized product search},
}

@inproceedings{nikolov_combining_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {Combining {RDF} {Graph} {Data} and {Embedding} {Models} for an {Augmented} {Knowledge} {Graph}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3191527},
	doi = {10.1145/3184558.3191527},
	abstract = {Vector embedding models have recently become popular for encoding both structured and unstructured data. In the context of knowledge graphs such models often serve as additional evidence supporting various tasks related to the knowledge base population: e.g., information extraction or link prediction to expand the original dataset. However, the embedding models themselves are often not used directly alongside structured data: they merely serve as additional evidence for structured knowledge extraction. In the metaphactory knowledge graph management platform, we use federated hybrid SPARQL queries for combining explicit information stated in the graph, implicit information from the associated embedding models, and information extracted using vector embeddings in a transparent way for the end user. In this paper we show how we integrated RDF data with vector space models to construct an augmented knowledge graph to be used in customer applications.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Nikolov, Andriy and Haase, Peter and Herzig, Daniel M. and Trame, Johannes and Kozlov, Artem},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {machine learning, deep learning, knowledge graph, graph embeddings, query federation},
	pages = {977--980},
}

@article{v_heterogeneous_2020,
	title = {A {Heterogeneous} {Information} {Network} {Model} for {Long} {Non}-{Coding} {RNA} {Function} {Prediction}},
	volume = {19},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2020.3000518},
	doi = {10.1109/TCBB.2020.3000518},
	abstract = {Exciting information on the functional roles played by long non-coding RNA (lncRNA) has drawn substantial research attention these days. With the advent of techniques such as RNA-Seq, thousands of lncRNAs are identified in very short time spans. However, due to the poor annotation rate, only a few of them are functionally characterised. The wet lab experiments to elucidate lncRNA functions are challenging, slow progressing and sometimes prohibitively expensive. This work attempts to solve the crucial problem of developing computational methods to predict lncRNA functions. The model presented here, predicts the functions of lncRNAs by making use of a meta-path based measure, AvgSim on a Heterogeneous Information Network (HIN). The network is constructed from existing protein and function association data of lncRNAs, lncRNA co-expression data and protein protein interaction data. Out of the 2,758 lncRNA considered for the experiment, the proposed method predicts possible functions for 2,695 lncRNAs with an accuracy of 73.68 percent and found to perform better than the other state-of-the-art approaches for an independent test set. A case study of two well-known lncRNAs (HOTAIR and H19) is conducted and the associated functions are identified. The results were validated using experimental evidence from the literature. The script and data used for the implementation of the model is freely available at: \&lt;uri\&gt;http://bdbl.nitc.ac.in/LncFunPred/index.html\&lt;/uri\&gt;.},
	number = {1},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {V, Sunil Kumar P and Thahsin, Adheeba and M, Manju and G, Gopakumar},
	month = jun,
	year = {2020},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {255--266},
}

@inproceedings{weber_conceptual_2020,
	address = {Champaign, Illinois},
	series = {{JCDL} '19},
	title = {Conceptual models in digital libraries, archives, and museums},
	isbn = {978-1-7281-1547-4},
	url = {https://doi.org/10.1109/JCDL.2019.00117},
	doi = {10.1109/JCDL.2019.00117},
	abstract = {This workshop addresses the development, use, and evolution of conceptual models in the context of digital libraries, archives, and museums. The workshop will convene domain practitioners and researchers in order to formalize a research agenda for conceptual modelling in digital libraries, and foster a cooperative research community to make progress on these topics over the coming decade. Activities at the workshop will include paper presentations, round-table discussions, and a keynote from an expert in the conceptual and logical foundations of information organization systems. Workshop papers as well as a summary of the proceedings will be published in a free and openly accessible repository.},
	booktitle = {Proceedings of the 18th {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Weber, Nicholas and Fenlon, Katrina and Organisciak, Peter and Thomer, Andrea K.},
	year = {2020},
	keywords = {digital libraries, conceptual models},
	pages = {457--458},
}

@inproceedings{pelzetter_declarative_2020,
	address = {New York, NY, USA},
	series = {{W4A} '20},
	title = {A declarative model for accessibility requirements},
	isbn = {978-1-4503-7056-1},
	url = {https://doi.org/10.1145/3371300.3383339},
	doi = {10.1145/3371300.3383339},
	abstract = {The web has become the primary source of information for many people. Many services are provided are on the web. Despite extensive guidelines for the accessibility of web pages, many web sites are not accessible making these web sites difficult or impossible to use for people with disabilities. Evaluating the accessibility of web pages can either be done manually, which is a very laborious task or using automated tools. Unfortunately, the results from different tools are often inconsistent because of the ambiguity of the current guidelines. In this paper, a declarative approach for describing the requirements for accessible web pages is presented. This declarative model will help developers of accessibility evaluation tools to create tools that produce more consistent results and are easier to maintain.},
	booktitle = {Proceedings of the 17th {International} {Web} for {All} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Pelzetter, Jens},
	year = {2020},
	note = {event-place: Taipei, Taiwan},
	keywords = {accessibility, ACT rules, WCAG},
}

@article{xu_how_2025,
	title = {How {Vital} {Is} the {Jurisprudential} {Relevance}: {Law} {Article}-{Intervened} {Legal} {Case} {Retrieval} and {Matching}},
	volume = {43},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3725729},
	doi = {10.1145/3725729},
	abstract = {Legal case retrieval aims to automatically scour comparable legal cases based on a given query, which is crucial for offering relevant precedents to support the judgment in intelligent legal systems. Due to similar goals, it is often associated with a similar case matching task. To address them, a daunting challenge is assessing the uniquely defined legal-rational similarity within the judicial domain, which distinctly deviates from the semantic similarities in general text retrieval. Past works either tagged domain-specific factors or incorporated reference laws to capture legal-rational information. However, their heavy reliance on expert or unrealistic assumptions restricts their practical applicability in real-world scenarios. In this article, we propose an end-to-end model named LCM-LAI to solve the above challenges. Through meticulous theoretical analysis, LCM-LAI employs a dependent multi-task learning framework to capture legal-rational information within legal cases by a law article prediction sub-task, without any additional assumptions in inference. In addition, LCM-LAI proposes an article-aware attention mechanism to evaluate the legal-rational similarity between across-case sentences based on the law distribution, which is more effective than semantic similarity. We perform a series of exhaustive experiments that include two different tasks that involving four real-world datasets. The results demonstrate that LCM-LAI achieves state-of-the-art performance.},
	number = {4},
	journal = {ACM Trans. Inf. Syst.},
	author = {Xu, Nuo and Wang, Pinghui and Liang, Zi and Zhao, Junzhou and Guan, Xiaohong},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {dependent multi-task learning, legal case matching, legal case retrieval},
}

@article{ferro_dagstuhl_2018,
	title = {The {Dagstuhl} {Perspectives} {Workshop} on {Performance} {Modeling} and {Prediction}},
	volume = {52},
	issn = {0163-5840},
	url = {https://doi.org/10.1145/3274784.3274789},
	doi = {10.1145/3274784.3274789},
	abstract = {This paper reports the findings of the Dagstuhl Perspectives Workshop 17442 on performance modeling and prediction in the domains of Information Retrieval, Natural language Processing and Recommender Systems. We present a framework for further research, which identifies five major problem areas: understanding measures, performance analysis, making underlying assumptions explicit, identifying application features determining performance, and the development of prediction models describing the relationship between assumptions, features and resulting performance.},
	number = {1},
	journal = {SIGIR Forum},
	author = {Ferro, Nicola and Fuhr, Norbert and Grefenstette, Gregory and Konstan, Joseph A. and Castells, Pablo and Daly, Elizabeth M. and Declerck, Thierry and Ekstrand, Michael D. and Geyer, Werner and Gonzalo, Julio and Kuflik, Tsvi and Lindn, Krister and Magnini, Bernardo and Nie, Jian-Yun and Perego, Raffaele and Shapira, Bracha and Soboroff, Ian and Tintarev, Nava and Verspoor, Karin and Willemsen, Martijn C. and Zobel, Justin},
	month = aug,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {91--101},
}

@inproceedings{chang_tclens_2025,
	address = {New York, NY, USA},
	series = {{CSAI} '24},
	title = {{TCLens}: {Towards} {Toxicity} {Tags} {Aggregation} of {Massive} {Labels} {Generated} by {Content} {Moderation} for {AIGC}},
	isbn = {979-8-4007-1818-2},
	url = {https://doi.org/10.1145/3709026.3709114},
	doi = {10.1145/3709026.3709114},
	abstract = {The recent boost of artificial intelligence represented by Large Language Models (LLMs) is surging. Due to the outstanding performance of LLMs, AI-Generated Content (AIGC) has also made important progress in multimodal knowledge creation referring to text, image, audio, and video. However, the security, privacy, and ethical risks associated with AIGC (e.g., fake news, social engineering attacks, and toxic content) have deeply weakened the compliance of AIGC. Although existing content moderation solutions can filter out several types of toxic content, the audit performance of different vendors and techniques are of varying quality. Some AIGC service providers improve the moderation effectiveness by introducing multiple sources of audit vendors. Due to the lack of general content moderation standards and taxonomy, the labels of multi-source moderation vendors vary greatly. To this end, We propose a novel massive label aggregation approach for content moderation named TCLens. First, we collect results of multi-vendor content moderation engines for building massive toxic labels for AIGC. Then, we introduce an ontology for better tagging with the capability of automatic updating and vendor-agnostic. Finally, we implement a prototype of TCLens. Our evaluation demonstrates that it outperforms single-source tagging and existing SOTA solutions.},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Chang, Bingtao and Wen, Weiping and Wu, Xiaojie and Cheng, Siyang and Jiang, Jianchun and Mei, Rui},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Language model, Performance, AIGC, Multi-modal, AI-generated content, information content moderation, labels aggregation, toxicity tags, Economic and social effects, Information content moderation, Information contents, Knowledge creations, Label aggregation, Text images, Toxicity tag},
	pages = {466--473},
	annote = {Cited by: 0},
}

@inproceedings{pang_bert_2025,
	address = {New York, NY, USA},
	series = {{CSAE} '24},
	title = {A {BERT} and {TextCNN} integration-based {Method} for {Public} {Complaints} and {Proposals} {Text} {Classification}},
	isbn = {979-8-4007-1809-0},
	url = {https://doi.org/10.1145/3704814.3704819},
	doi = {10.1145/3704814.3704819},
	abstract = {With the wide use of Blockchain technology and online public complaints and proposals (hereinafter referred to as PCP) platforms, huge amounts of data are accordingly produced, which poses a challenge on the government's effectively choosing and classifying the key information. In view of this, this paper endeavors to conduct research on the PCP text classification for a smart government. Firstly, PCP texts are collected automatically and preprocessed, and then a hierarchical classification structure involving targeted government departments and PCP contents is proposed. Finally, a BERT and TextCNN model is adopted for PCP text classification. The experimental results show that the classification structure and model proposed in this paper can effectively categorize the PCP text, providing a scientific analysis and effective technological method to improve the work efficiency and service quality of PCP.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Computer} {Science} and {Application} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Pang, Haijie and Li, Chengji},
	year = {2025},
	keywords = {BERT, Blockchain, Text classification, Public complaints and proposals, TextCNN},
	pages = {15--18},
}

@inproceedings{jin_geographic_2019,
	address = {New York, NY, USA},
	series = {{ICMLSC} '19},
	title = {Geographic {Entity} {Relationship} {Extraction} {Model} {Based} on {Piecewise} {Convolution} of {Residual} {Network}},
	isbn = {978-1-4503-6612-0},
	url = {https://doi.org/10.1145/3310986.3311025},
	doi = {10.1145/3310986.3311025},
	abstract = {Nowadays, geographic entity relationship extraction systems generally rely on artificial feature extraction. These features either require complex and complete data sets, or cannot describe deep features such as semantics. And data sets that can be used for geographic relationship extraction are scarce. To tackle these problems, this paper uses distant supervision to map existing knowledge bases into rich unstructured data which contributes to a large amount of training data. In training, this paper uses the deep residual network to extract more abstract and deeper features. Then the piecewise max pooling and selective attention mechanisms are used to further improve the accuracy of the model. Finally, the experimental results show that the deeper network and the piecewise max pooling significantly improve the extraction results.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Machine} {Learning} and {Soft} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Jin, Ying and Zhao, Shuai and Wu, Yudong},
	year = {2019},
	note = {event-place: Da Lat, Viet Nam},
	keywords = {relationship extraction, distant supervision, ResNet, attention, Gis, PCNN},
	pages = {160--165},
}

@inproceedings{groza_question_2022,
	address = {New York, NY, USA},
	series = {{SAC} '22},
	title = {Question answering over logic puzzles using theorem proving},
	isbn = {978-1-4503-8713-2},
	url = {https://doi.org/10.1145/3477314.3507177},
	doi = {10.1145/3477314.3507177},
	abstract = {We developed a tool to automatically solve logical puzzles in natural language. The solution is composed by a parser and an inference engine. The parser translates the text into First Order Logic (FOL), while the Mace4 model finder computes the interpretation models of the given FOL theory. The solver uses the Prover9 theorem prover to compute Yes/No answers to natural language questions related to each puzzle. Each answer is backup by a graphical representation of its proof, which is in line with Explainable Artificial Intelligence (XAI). The advantage of using reasoning for Natural Language Understanding (NLU) instead of learned models is that the user can obtain an explanation of the reasoning chain. We illustrate how the system performs on 382 knights and knaves puzzles. These features together with the overall performance rate of 80.89\% makes the proposed solution an improvement upon similar solvers for natural language understanding in the puzzles domain.},
	booktitle = {Proceedings of the 37th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Groza, Adrian and Nitu, Cristian},
	year = {2022},
	note = {event-place: Virtual Event},
	keywords = {natural language understanding, question answering, explainable ai, logical puzzles},
	pages = {871--874},
}

@inproceedings{vogt_toward_2020,
	address = {New York, NY, USA},
	series = {{JCDL} '20},
	title = {Toward {Representing} {Research} {Contributions} in {Scholarly} {Knowledge} {Graphs} {Using} {Knowledge} {Graph} {Cells}},
	isbn = {978-1-4503-7585-6},
	url = {https://doi.org/10.1145/3383583.3398530},
	doi = {10.1145/3383583.3398530},
	abstract = {There is currently a gap between the natural language expression of scholarly publications and their structured semantic content modeling to enable intelligent content search. With the volume of research growing exponentially every year, a search feature operating over semantically structured content is compelling. Toward this end, in this work, we propose a novel semantic data model for modeling the contribution of scientific investigations. Our model, i.e. the Research Contribution Model (RCM), includes a schema of pertinent concepts highlighting six core information units, viz. Objective, Method, Activity, Agent, Material, and Result, on which the contribution hinges. It comprises bottom-up design considerations made from three scientific domains, viz. Medicine, Computer Science, and Agriculture, which we highlight as case studies. For its implementation in a knowledge graph application we introduce the idea of building blocks called Knowledge Graph Cells (KGC), which provide the following characteristics: (1) they limit the expressibility of ontologies to what is relevant in a knowledge graph regarding specific concepts on the theme of research contributions; (2) they are expressible via ABox and TBox expressions; (3) they enforce a certain level of data consistency by ensuring that a uniform modeling scheme is followed through rules and input controls; (4) they organize the knowledge graph into named graphs; (5) they provide information for the front end for displaying the knowledge graph in a human-readable form such as HTML pages; and (6) they can be seamlessly integrated into any existing publishing process thatsupports form-based input abstracting its semantic technicalities including RDF semantification from the user. Thus RCM joins the trend of existing work toward enhanced digitalization of scholarly publication enabled by an RDF semantification as a knowledge graph fostering the evolution of the scholarly publications beyond written text.},
	booktitle = {Proceedings of the {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} in 2020},
	publisher = {Association for Computing Machinery},
	author = {Vogt, Lars and D'Souza, Jennifer and Stocker, Markus and Auer, Sören},
	year = {2020},
	note = {event-place: Virtual Event, China},
	keywords = {ontology, open science, digital libraries, fair data principles, machine actionability, scholarly infrastructure, semantic publishing},
	pages = {107--116},
}

@inproceedings{loseto_semantic-based_2023,
	address = {New York, NY, USA},
	series = {{SAC} '23},
	title = {Semantic-based {Adaptation} of {Quality} of {Experience} in {Web} {Multimedia} {Streams}},
	isbn = {978-1-4503-9517-5},
	url = {https://doi.org/10.1145/3555776.3577686},
	doi = {10.1145/3555776.3577686},
	abstract = {Video streaming accounts for the majority of worldwide Internet traffic, and HTTP-based multimedia has the largest share among technologies and protocols. The wide availability of mobile devices and wireless broadband networks currently leads to wider heterogeneity of fruition contexts and frequent condition changes during a streaming session. MPEG-DASH is the reference standard for Dynamic Adaptive Streaming over HTTP: a provider defines several representations for a segmented multimedia source, with different bit rates, allowing a client to dynamically select the best one based on current conditions, and to download the corresponding sequence of segments for smooth playback. MPEG-DASH does not mandate specific bit rate adaptation schemes; conventional approaches are divided in buffer-based, bandwidth-based and hybrid. Nevertheless, Quality of Experience (QoE) can be influenced by many additional factors. This paper proposes a novel QoE adaptation approach based on dynamic ontology-based annotation of streaming context and mobile matchmaking with DASH representation profiles in a Web Ontology Language (OWL) fragment, exploiting a WebAssembly port of an embedded reasoning engine. The proposed framework enables adaptation based not only on network status, but also on client device capabilities, ambient conditions and multimedia content type. A case study validates the proposal, while early experiments support its sustainability.},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Loseto, Giuseppe and Scioscia, Floriano and Ruta, Michele and Gramegna, Filippo and Bilenchi, Ivano},
	year = {2023},
	note = {event-place: Tallinn, Estonia},
	keywords = {web ontology language (OWL), MPEG-DASH, multimedia streaming, quality of experience, semantic matchmaking},
	pages = {1821--1830},
}

@inproceedings{lin_skills_2021,
	address = {New York, NY, USA},
	series = {{LAK21}},
	title = {Skills {Matter}: {Modeling} the relationship between decision making processes and collaborative problem-solving skills during {Hidden} {Profile} {Tasks}},
	isbn = {978-1-4503-8935-8},
	url = {https://doi.org/10.1145/3448139.3448180},
	doi = {10.1145/3448139.3448180},
	abstract = {Collaborative problem-solving (CPS) is one of the most essential 21st century skills for success across educational and professional settings. The hidden-profile paradigm is one of the most prominent avenues of studying group decision making and underlying issues in information sharing. Previous research on the hidden-profile paradigm has primarily focused on static constructs (e.g., group size, group expertise), or on the information itself (whether certain pieces of information is being shared). In the current study, we propose a lens on individual and group’s collaborative problem-solving skills, to explore the relationships between dynamic discourse processes and decision making in a distributed information environment. Specifically, we sought to examine CPS skills in association with decision change and productive decision-making. Our results suggest that while sharing information has significantly positive association with decision change and effective decision-making, other aspects of social processes appear to be negatively correlated with these outcomes. Cognitive CPS skills, however, exhibit a strong positive relationship with making a (productive) change in students final decisions. We also find that these results are more pronounced at the group level, particularly with cognitive CPS skills. Our study shed lights on a more nuanced picture of how social and cognitive CPS interactions are related to effective information sharing and decision making in collaborative problem-solving interactions.},
	booktitle = {{LAK21}: 11th {International} {Learning} {Analytics} and {Knowledge} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Yiwen and Dowell, Nia and Godfrey, Andrew},
	year = {2021},
	note = {event-place: Irvine, CA, USA},
	keywords = {decision making, collaborative problem solving, group processes, Hidden-profile paradigm},
	pages = {428--437},
}

@inproceedings{diaz_gonzalez_applying_2023,
	address = {New York, NY, USA},
	series = {{ICISDM} '23},
	title = {Applying {BioBERT} to {Extract} {Germline} {Gene}-{Disease} {Associations} for {Building} a {Knowledge} {Graph} from the {Biomedical} {Literature}},
	isbn = {979-8-4007-0063-7},
	url = {https://doi.org/10.1145/3603765.3603771},
	doi = {10.1145/3603765.3603771},
	abstract = {Published biomedical information has and continues to rapidly increase. The recent advancements in Natural Language Processing (NLP), have generated considerable interest in automating the extraction, normalization, and representation of biomedical knowledge about entities such as genes and diseases. Our study analyzes germline abstracts in the construction of knowledge graphs of the immense work that has been done in this area for genes and diseases. This paper presents SimpleGermKG, an automatic knowledge graph construction approach that connects germline genes and diseases. For the extraction of genes and diseases, we employ BioBERT, a pre-trained BERT model on biomedical corpora. We propose an ontology-based and rule-based algorithm to standardize and disambiguate medical terms. For semantic relationships between articles, genes, and diseases, we implemented a part-whole relation approach to connect each entity with its data source and visualize them in a graph-based knowledge representation. Lastly, we discuss the knowledge graph applications, limitations, and challenges to inspire the future research of germline corpora. Our knowledge graph contains 297 genes, 130 diseases, and 46,747 triples. Graph-based visualizations are used to show the results.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Information} {System} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Diaz Gonzalez, Armando D. and Hughes, Kevin S. and Yue, Songhui and Hayes, Sean T.},
	year = {2023},
	note = {event-place: Atlanta, USA},
	keywords = {knowledge graph, BioBERT, entity recognition, germline mutations, semantic relation},
	pages = {37--42},
}

@inproceedings{piotrowski_vision_2019,
	address = {New York, NY, USA},
	series = {{DocEng} '19},
	title = {A {Vision} for {User}-{Defined} {Semantic} {Markup}},
	isbn = {978-1-4503-6887-2},
	url = {https://doi.org/10.1145/3342558.3345414},
	doi = {10.1145/3342558.3345414},
	abstract = {Typesetting systems, such as LATEX, permit users to define custom markup and corresponding formatting to simplify authoring, ensure the consistent presentation of domain-specific recurring elements and, potentially, enable further processing, such as the generation of an index of such elements. In XML-based and similar systems, the separation of content and form is also reflected in the processing pipeline: while document authors can define custom markup, they cannot define its semantics. This could be said to be intentional to ensure structural integrity of documents, but at the same time it limits the expressivity of markup. The latter is particularly true for so-called lightweight markup languages like Mark-down, which only define very limited sets of generic elements. This vision paper sketches an approach for user-defined semantic markup that could permit authors to define the semantics of elements by formally describing the relations between its constituent parts and to other elements, and to define a formatting intent that would ensure that a default presentation is always available.},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Document} {Engineering} 2019},
	publisher = {Association for Computing Machinery},
	author = {Piotrowski, Michael},
	year = {2019},
	note = {event-place: Berlin, Germany},
	keywords = {document authoring, document models and structures, markup semantics, scholarly publishing},
}

@article{yu_counterpoint_2022,
	title = {Counterpoint: {Representing} {Forged} {Concepts} as {Emergent} {Variables} {Using} {Composite}-{Based} {Structural} {Equation} {Modeling}},
	volume = {52},
	issn = {0095-0033},
	url = {https://doi.org/10.1145/3505639.3505647},
	doi = {10.1145/3505639.3505647},
	abstract = {Studying and modeling theoretical concepts is a cornerstone activity in information systems (IS) research. Researchers have been familiar with one type of theoretical concept, namely behavioral concepts, which are assumed to exist in nature and measured by a set of observable variables. In this paper, we present a second type of theoretical concept, namely forged concepts, which are designed and assumed to emerge within their environment. While behavioral concepts are classically operationalized as latent variables, forged concepts are better specified as emergent variables. Additionally, we propose composite-based structural equation modeling (SEM) as a subtype of SEM that is eminently suitable to analyze models containing emergent variables. We shed light on the composite-based SEM steps: model specification, model identification, model estimation, and model assessment. Then, we present an illustrative example from the domain of IS research to demonstrate these four steps and show how modeling with emergent variables proceeds.},
	number = {SI},
	journal = {SIGMIS Database},
	author = {Yu, Xi and Zaza, Sam and Schuberth, Florian and Henseler, Jörg},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {behavioral concept, composite model, composite-based structural equation modeling, emergent variables, forged concept},
	pages = {114--130},
}

@inproceedings{irrera_reproducibility_2024,
	address = {New York, NY, USA},
	series = {{RecSys} '24},
	title = {Reproducibility and {Analysis} of {Scientific} {Dataset} {Recommendation} {Methods}},
	isbn = {979-8-4007-0505-2},
	url = {https://doi.org/10.1145/3640457.3688071},
	doi = {10.1145/3640457.3688071},
	abstract = {Datasets play a central role in scholarly communications. However, scholarly graphs are often incomplete, particularly due to the lack of connections between publications and datasets. Therefore, the importance of dataset recommendation—identifying relevant datasets for a scientific paper, an author, or a textual query—is increasing. Although various methods have been proposed for this task, their reproducibility remains unexplored, making it difficult to compare them with new approaches. We reviewed current recommendation methods for scientific datasets, focusing on the most recent and competitive approaches, including an SVM-based model, a bi-encoder retriever, a method leveraging co-authors and citation network embeddings, and a heterogeneous variational graph autoencoder. These approaches underwent a comprehensive analysis under consistent experimental conditions. Our reproducibility efforts show that three methods can be reproduced, while the graph variational autoencoder is challenging due to unavailable code and test datasets. Hence, we re-implemented this method and performed a component-based analysis to examine its strengths and limitations. Furthermore, our study indicated that three out of four considered methods produce subpar results when applied to real-world data instead of specialized datasets with ad-hoc features.},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Irrera, Ornella and Lissandrini, Matteo and Dell'Aglio, Daniele and Silvello, Gianmaria},
	year = {2024},
	note = {event-place: Bari, Italy},
	keywords = {Reproducibility, Recommender Systems, Dataset Recommendations},
	pages = {570--579},
}

@inproceedings{sosnowski_probabilistic_2020,
	address = {New York, NY, USA},
	series = {{PETRA} '20},
	title = {A probabilistic conversational agent for intelligent tutoring systems},
	isbn = {978-1-4503-7773-7},
	url = {https://doi.org/10.1145/3389189.3397978},
	doi = {10.1145/3389189.3397978},
	abstract = {The paper proposes a novel approach for a Conversational Intelligent Tutoring System, which combines Natural Language Processing techniques with dynamic Bayesian Inference for probabilistic student state estimation. Our aim is to develop and implement the probabilistic model for the discourse between the user and the Conversational Agent. Most of the already proposed tutoring systems interacting with the user via natural language use rather simple pattern matching rules to carry on the conversation. Even when the fuzzy pattern matching is employed, already existing approaches use these fuzzy values only to find a single best-matching rule, thus immediately discretising the result. Our proposed approach differs in this regard, as we would like to use the fuzzy values resulting from pattern matching as inputs for our probabilistic model. To be more precise, we would like to treat these values as observations for the state estimation in the dynamic Bayesian Network. Our approach is, not coincidentally, similar to the methods typical for state and action recognition. This paper focuses on the Natural Language processing aspect, while the probabilistic model is described in a separate work.},
	booktitle = {Proceedings of the 13th {ACM} {International} {Conference} on {PErvasive} {Technologies} {Related} to {Assistive} {Environments}},
	publisher = {Association for Computing Machinery},
	author = {Sosnowski, Tomasz and Yordanova, Kristina},
	year = {2020},
	note = {event-place: Corfu, Greece},
	keywords = {natural language processing, conversational agent, semantic similarity, intelligent tutoring system, natural language understanding, computational state space models, dynamic bayesian inference, markov chain, sentence similarity, text similarity},
}

@inproceedings{skobelev_concept_2023,
	address = {New York, NY, USA},
	series = {{ISMSI} '23},
	title = {Concept and {Development} of a {Multi}-{Agent} {Digital} {Twin} of {Plant} {Focused} on {Broccoli}},
	isbn = {978-1-4503-9992-0},
	url = {https://doi.org/10.1145/3596947.3596952},
	doi = {10.1145/3596947.3596952},
	abstract = {The paper discusses the principles of developing a multi-agent digital twin of plants using broccoli as an example of plants. The developed model of the digital twin of plants must meet the following requirements: real-time environmental data acquisition, user feedback collection, continuous adaptation of the plant development plan for each event, individual instance for field or field part. The digital twin of plant is designed as an intelligent cyber-physical system that has a user-defined knowledge bas and a multi-agent system for planning and modeling of plant growth and development, as well as for forecasting crop parameters. For this purpose, a new method for estimate stage duration and yield is proposed, which defines a "tube" – a corridor to each of the factors corresponding plant development. The key factors have been determined during consultations with practicing agronomists but can be adjusted by users experience. This concept was originally introduced for wheat digital twin, but now is scaled and modified to simulate broccoli growth process.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Intelligent} {Systems}, {Metaheuristics} \&amp; {Swarm} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Skobelev, Petr and Simonova, Elena and Tabachinskiy, Aleksey and Kudryakov, Evgeniy and Strizhakov, Anatoly and Goryanin, Oleg and Ermakov, Vasiliy and Chan, Yung-Kuan and Lee, Tzong-Ru and Sung, Yu},
	year = {2023},
	note = {event-place: Virtual Event, Malaysia},
	keywords = {ontology, digital twin, precision farming, knowledge base, broccoli cultivation, intelligent services, multi-agent model, multi-agent technologies},
	pages = {132--138},
}

@inproceedings{beldi_learn2sum_2022,
	address = {New York, NY, USA},
	series = {{MEDES} '22},
	title = {{Learn2Sum}: {A} {New} {Approach} to {Unsupervised} {Text} {Summarization} {Based} on {Topic} {Modeling}},
	isbn = {978-1-4503-9219-8},
	url = {https://doi.org/10.1145/3508397.3564853},
	doi = {10.1145/3508397.3564853},
	abstract = {Due to the enormous volume of data on the web, it is hard for the user to retrieve effective and useful information within the right time. Thus, it has become a need to generate a brief summary from a large amount of textual data according to the user profile. In this context, text summarization is used to identify important information within text documents. It aims to generate shorter versions of the source text, by including only the relevant and salient information. In recent years, the research on summarization techniques based on topic modeling techniques has become a hot topic among researchers thanks to their ability to classify, understand a large text corpora and extract important topics on the text. However, existing studies do not provide the support of personalization when generating summaries because they need to know not only which documents are most helpful to the users, but also which topics and keywords are more or less related to the user' interests. Thus, existing studies lack of the support of adaptive user modeling for user applications in the emerging areas of automatic summarization, topic modeling and visualization. In this context, we propose a new approach of automated text summarization based on topic modeling techniques and taking into account the user's profile which helps to semantically extract relevant topics of textual documents, summarizing information according to the user' topics interests and finally visualize them through a hyper-graph Experiments have been conducted to measure the effectiveness of our solution compared to existing summarizing approaches based on text content. The results show the superiority of our approach.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Management} of {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Beldi, Amal and Sassi, Salma and Jemai, Abedrazzek},
	year = {2022},
	note = {event-place: Venice, Italy},
	keywords = {classification, topic modeling, summarization, graph, text transformation, topics, user profile},
	pages = {136--143},
}

@article{becattini_viscounth_2023,
	title = {{VISCOUNTH}: {A} {Large}-scale {Multilingual} {Visual} {Question} {Answering} {Dataset} for {Cultural} {Heritage}},
	volume = {19},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3590773},
	doi = {10.1145/3590773},
	abstract = {Visual question answering has recently been settled as a fundamental multi-modal reasoning task of artificial intelligence that allows users to get information about visual content by asking questions in natural language. In the cultural heritage domain, this task can contribute to assisting visitors in museums and cultural sites, thus increasing engagement. However, the development of visual question answering models for cultural heritage is prevented by the lack of suitable large-scale datasets. To meet this demand, we built a large-scale heterogeneous and multilingual (Italian and English) dataset for cultural heritage that comprises approximately 500K Italian cultural assets and 6.5M question-answer pairs. We propose a novel formulation of the task that requires reasoning over both the visual content and an associated natural language description, and present baselines for this task. Results show that the current state of the art is reasonably effective but still far from satisfactory; therefore, further research in this area is recommended. Nonetheless, we also present a holistic baseline to address visual and contextual questions and foster future research on the topic.},
	number = {6},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {Becattini, Federico and Bongini, Pietro and Bulla, Luana and Bimbo, Alberto Del and Marinucci, Ludovica and Mongiovì, Misael and Presutti, Valentina},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Visual question answering, cultural heritage},
}

@inproceedings{zavarella_charting_2024,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '24},
	title = {Charting the {Landscape} of {Digital} {Health}: {Towards} {A} {Knowledge} {Graph} {Approach} to {News} {Media} {Analysis}},
	isbn = {979-8-4007-0466-6},
	url = {https://doi.org/10.1145/3631700.3665237},
	doi = {10.1145/3631700.3665237},
	abstract = {In this paper, we present our currently on-going work on a method for analyzing digital health transformation in our society by constructing a Knowledge Graph from a large corpus of 7.8 million English news articles, dating from 1987 through 2023. We firstly sampled around 95k articles relevant to the Digital Health topic by training and deploying a Deep Learning binary classifier via fine-tuning BERT. Successively, by deploying NLP techniques, we extracted triples from the identified articles to form a Digital Health News Knowledge Graph, which consists of 431k distinct triples connecting 186k entities through 1866 relations. The constructed Knowledge Graph provides insights into the evolution of Digital Health in news media and serves as a resource for further research in the field. The analysis that we have carried out reveals significant trends in Digital Health as reflected in the news, with notable peaks coinciding with key events like the COVID-19 pandemic.},
	booktitle = {Adjunct {Proceedings} of the 32nd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Zavarella, Vanni and Reforgiato, Diego and Consoli, Sergio and Fenu, Gianni},
	year = {2024},
	note = {event-place: Cagliari, Italy},
	pages = {419--423},
}

@article{kienzle_toward_2020,
	title = {Toward model-driven sustainability evaluation},
	volume = {63},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3371906},
	doi = {10.1145/3371906},
	abstract = {Exploring the vision of a model-based framework that may enable broader engagement with and informed decision making about sustainability issues.},
	number = {3},
	journal = {Commun. ACM},
	author = {Kienzle, Jörg and Mussbacher, Gunter and Combemale, Benoit and Bastin, Lucy and Bencomo, Nelly and Bruel, Jean-Michel and Becker, Christoph and Betz, Stefanie and Chitchyan, Ruzanna and Cheng, Betty H. C. and Klingert, Sonja and Paige, Richard F. and Penzenstadler, Birgit and Seyff, Norbert and Syriani, Eugene and Venters, Colin C.},
	month = feb,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {80--91},
}

@inproceedings{alghamdi_focus_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {Focus {Set} {Semantic} {Differences}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627563},
	doi = {10.1145/3587259.3627563},
	abstract = {Ontologies are being utilized widely as sources for formally organized information in a range of fields. The SNOMED CT ontology is a key resource in national and international health sectors for automatically linking information captured by diverse clinical information systems and research data ensuring consistent patient data capture and effective data analytics and decision support. Offering a comprehensive multilingual vocabulary for encoding clinical knowledge of multiple domains, the ontology is large and new releases are created regularly to reflect domain changes and user requirements. The main contribution of the paper is a novel automated approach for tracking semantic differences of subdomains in different versions of SNOMED CT targeted at terminologists, debuggers, ontology evaluators and developers of software using SNOMED CT. Whereas the semantic difference sets produced with existing methods are rather large and difficult to analyze, our method produces concise semantic difference sets for user-specified input focus concepts. Our method is based on subontology generation and semantic difference computation using uniform interpolation, which aids in finding inferred differences that other semantic difference tools do not reveal. The obtained semantic difference sets are related to the meaning of focus concept definitions for specific ontology subdomains, where some of these differences would not have been generated without this focused method for computing semantic differences between ontologies. A case study using SNOMED CT has shown the proposed approach is useful for domain experts.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Alghamdi, Ghadah Abdulrahman S and Schmidt, Renate A. and Gao, Yongsheng},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {SNOMED CT, Ontology Engineering, Ontology Extraction, Forgetting/Uniform Interpolation, Modularisation, Semantic Differences, Subontologies, Subontology Extraction},
	pages = {250--258},
}

@inproceedings{grgurina_assessment_2018,
	address = {New York, NY, USA},
	series = {{WiPSCE} '18},
	title = {Assessment of modeling and simulation in secondary computing science education},
	isbn = {978-1-4503-6588-8},
	url = {https://doi.org/10.1145/3265757.3265764},
	doi = {10.1145/3265757.3265764},
	abstract = {The introduction of the new computing science curriculum in the Netherlands in 2019 raises the need for new evidence-based teaching materials that include practical assignments and guidelines for their assessment. As a part of our research project on teaching Computational Science (modeling and simulation), we participate in these efforts and developed a curriculum intervention including a practical assignment and an accompanying assessment instrument consisting of grading rubrics based on the SOLO taxonomy. In this research paper we focus on the assessment instrument. We describe its development and report on a pilot study carried out in the secondary computing science course implementing the curriculum intervention. The instrument proved to be reliable and effective in tracing high and low levels of the students' achievements in modeling and simulation projects and exposed the expected differences in performance levels of various groups of students, which renders it useful for both formative and summative assessment. Furthermore, our application of the instrument has provided new insights into the needs of specific groups of students to receive instruction prior to and during the work on the assignments.},
	booktitle = {Proceedings of the 13th {Workshop} in {Primary} and {Secondary} {Computing} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Grgurina, Natasa and Barendsen, Erik and Suhre, Cor and Zwaneveld, Bert and van Veen, Klaas},
	year = {2018},
	note = {event-place: Potsdam, Germany},
	keywords = {assessment, modeling and simulation, secondary computing education, SOLO taxonomy},
}

@inproceedings{crespo_imp-logics_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {{IMP}-{Logics}: a metamodel for analysis and transformations of {Datalog} programs},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3687790},
	doi = {10.1145/3652620.3687790},
	abstract = {Datalog is a logic-based query-language used for knowledge representation and reasoning. Through the years, the literature has defined highly valuable algorithms, desirable properties, and useful transformations for this language and its extensions (e.g. Datalog±).However, few to none tools facilitate the implementation of such results, making the existence of mature Datalog-based tools scarce.This demonstration presents IMP-Logics, a Java library that offers a metamodel for Datalog and its extensions Datalog± that will allow researchers to easily implement the algorithms and demonstrations of the properties and transformations the community is working on.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Crespo, José Francisco and Juanola, Martí and Oriol, Xavier and Recalde, Martí and Teniente, Ernest},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {metamodel, datalog, datalog±, dependencies},
	pages = {51--55},
}

@inproceedings{demaidi_tone_2019,
	address = {New York, NY, USA},
	series = {{ArabWIC} 2019},
	title = {{TONE}: {A} {Method} for {Terminological} {Ontology} {Evaluation}},
	isbn = {978-1-4503-6089-0},
	url = {https://doi.org/10.1145/3333165.3333179},
	doi = {10.1145/3333165.3333179},
	abstract = {Selecting the most appropriate candidate domain ontology is necessary to ensure that the ontology covers the domain of interest at a reasonable level of detail. Existing approaches have the following drawbacks:(1) Focused on the ontology coverage of concepts in the domain of interest, and ignored the semantic richness associated with each concept.(2) The ontology coverage metrics tend to select either large ontologies with broad scope or ontologies with a small number of concepts which may not capture a particular domain of interest.(3) The approaches are not robust in the coverage and semantic richness metric results when different term extraction and recognition algorithms are used.The limitations mentioned above will result in selecting ontologies which are not related to the domain of interest. Therefore, this paper presents a novel Terminological ONtology Evaluator (TONE). TONE uses a textual corpus to evaluate the ontology coverage and semantic richness. TONE was compared with existing ontology evaluation approaches and it proved that it was able to select the domain ontology which was intentionally developed to cover a specific domain of interest. In addition, TONE proved to be more robust in the coverage and semantic richness metric results compared to existing approaches.},
	booktitle = {Proceedings of the {ArabWIC} 6th {Annual} {International} {Conference} {Research} {Track}},
	publisher = {Association for Computing Machinery},
	author = {Demaidi, Mona Nabil and Gaber, Mohamed Medhat},
	year = {2019},
	note = {event-place: Rabat, Morocco},
	keywords = {Ontologies, ontology evaluation, percentage agreement, semantic richness},
}

@article{du_financial_2024,
	title = {Financial {Sentiment} {Analysis}: {Techniques} and {Applications}},
	volume = {56},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3649451},
	doi = {10.1145/3649451},
	abstract = {Financial Sentiment Analysis (FSA) is an important domain application of sentiment analysis that has gained increasing attention in the past decade. FSA research falls into two main streams. The first stream focuses on defining tasks and developing techniques for FSA, and its main objective is to improve the performances of various FSA tasks by advancing methods and using/curating human-annotated datasets. The second stream of research focuses on using financial sentiment, implicitly or explicitly, for downstream applications on financial markets, which has received more research efforts. The main objective is to discover appropriate market applications for existing techniques. More specifically, the application of FSA mainly includes hypothesis testing and predictive modeling in financial markets. This survey conducts a comprehensive review of FSA research in both the technique and application areas and proposes several frameworks to help understand the two areas’ interactive relationship. This article defines a clearer scope for FSA studies and conceptualizes the FSA-investor sentiment-market sentiment relationship. Major findings, challenges, and future research directions for both FSA techniques and applications have also been summarized and discussed.},
	number = {9},
	journal = {ACM Comput. Surv.},
	author = {Du, Kelvin and Xing, Frank and Mao, Rui and Cambria, Erik},
	month = apr,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {machine learning, natural language processing, deep learning, Financial sentiment analysis, information system, financial forecasting},
}

@inproceedings{benabdallah_technical_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Technical {Mentality}: {Principles} for {HCI} {Research} and {Practice}},
	isbn = {979-8-4007-0330-0},
	url = {https://doi.org/10.1145/3613904.3642720},
	doi = {10.1145/3613904.3642720},
	abstract = {This paper presents a reflection on the role of ontological inquiry in HCI research and practice. Specifically, we introduce philosopher Gilbert Simondon’s proposal of technical mentality, an onto-epistemology based on direct knowledge of technical objects and systems. This paper makes the following contributions: an analysis of Simondon’s ontological critique and its connection to technical mentality; a reflection on the ethical and practical implications of Simondon’s proposal for systems research; an example of technical mentality in practice; and a discussion of how technical mentality might be extended into a design program for HCI through four principles: extension, integration, legibility, and expression.},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Benabdallah, Gabrielle and Peek, Nadya},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Ontology, Design, Philosophy, Gilbert Simondon, Technical Mentality},
}

@inproceedings{khabiri_industry_2019,
	address = {New York, NY, USA},
	series = {{CIKM} '19},
	title = {Industry {Specific} {Word} {Embedding} and its {Application} in {Log} {Classification}},
	isbn = {978-1-4503-6976-3},
	url = {https://doi.org/10.1145/3357384.3357827},
	doi = {10.1145/3357384.3357827},
	abstract = {Word, sentence and document embeddings have become the cornerstone of most natural language processing-based solutions. The training of an effective embedding depends on a large corpus of relevant documents. However, such corpus is not always available, especially for specialized heavy industries such as oil, mining, or steel. To address the problem, this paper proposes a semi-supervised learning framework to create document corpus and embedding starting from an industry taxonomy, along with a very limited set of relevant positive and negative documents. Our solution organizes candidate documents into a graph and adopts different explore and exploit strategies to iteratively create the corpus and its embedding. At each iteration, two metrics, called Coverage and Context Similarity, are used as proxy to measure the quality of the results. Our experiments demonstrate how an embedding created by our solution is more effective than the one created by processing thousands of industry-specific document pages. We also explore using our embedding in downstream tasks, such as building an industry specific classification model given labeled training data, as well as classifying unlabeled documents according to industry taxonomy terms.},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Khabiri, Elham and Gifford, Wesley M. and Vinzamuri, Bhanukiran and Patel, Dhaval and Mazzoleni, Pietro},
	year = {2019},
	note = {event-place: Beijing, China},
	keywords = {natural language processing, word embeddings, text classification},
	pages = {2713--2721},
}

@inproceedings{sarli_interoperability_2018,
	address = {Gothenburg, Sweden},
	series = {{WSC} '18},
	title = {An interoperability model for collaborative development of distributed supply chain simulations},
	isbn = {978153866570},
	abstract = {Development of a collaborative distributed supply chain simulation implies interoperation of heterogeneous systems. Interoperability among several independent systems requires mutual understanding and meaning of shared data represented in a common structure. These two requirements are always a real challenge. In a High Level Architecture (HLA) based supply chain simulation, the federation object model (FOM) performs as a contract where mutual understanding and shared information are described. However, this contract is usually established manually and then the consistency and completeness cannot be guaranteed. Developing FOM and modifying existing systems to comply with the FOM implies a significant amount of time and effort which reduce the benefits of system reuse. This paper presents a heavy-weighted ontology-based method to construct interoperation models of HLA based supply chain simulation in a human-friendly, efficient, consistent and complete way. Besides, this method provides support to collaboration among several organizations of a supply chain.},
	booktitle = {Proceedings of the 2018 {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Sarli, Juan L.},
	year = {2018},
	pages = {4222--4223},
}

@inproceedings{islam_specification-driven_2019,
	address = {New York, NY, USA},
	series = {{MTD}'19},
	title = {Specification-driven {Moving} {Target} {Defense} {Synthesis}},
	isbn = {978-1-4503-6828-5},
	url = {https://doi.org/10.1145/3338468.3356830},
	doi = {10.1145/3338468.3356830},
	abstract = {Cyber agility enables cyber systems to defend proactively against sophisticated attacks by dynamically changing the system configuration parameters (called mutable parameters) in order to deceive adversaries from reaching their goals, disrupt the attack plans by forcing them to change their adversarial behaviors, and/or deterring them through prohibitively increasing the cost for attacks. However, developing cyber agility such as moving target defense techniques that are provable safe is a highly complex task that requires significant time and expertise. Our goal is to address this challenge by providing a framework for automating the creation of configuration-based moving target techniques rapidly and safely.In this paper, we present a cyber agility synthesis framework, called MTDSynth, that contains a formal ontology, MTD policy language, and MTD controller synthesis engine for implementing configuration-based moving target defense techniques. The policy language contains the agility specifications required to model the MTD technique, such as sensors, mutation trigger, mutation parameters, mutation actions, and mutation constraints. Based on the mutation constraints, the MTD controller synthesis engine provides an MTD policy refinement implementation for SDN configuration with provable properties using constraint satisfaction solvers. We show several examples of MTD controller synthesis, including temporal and spatial IP mutation, path mutation, detector mutation.We developed our ActivSDN over OpenDaylight SDN controller as an open programming environment to enable rapid and safe development of MTD sense-making and decision-making actions. Our implementation and evaluation experiments show not only the feasibility of MTD policy refinement but also the insignificant computational overhead of this refinement process.},
	booktitle = {Proceedings of the 6th {ACM} {Workshop} on {Moving} {Target} {Defense}},
	publisher = {Association for Computing Machinery},
	author = {Islam, Md Mazharul and Duan, Qi and Al-Shaer, Ehab},
	year = {2019},
	note = {event-place: London, United Kingdom},
	keywords = {automation, formal language, mtd, sdn},
	pages = {13--24},
}

@article{bartalesi_representing_2025,
	title = {Representing {Geospatial} {Knowledge} in {Narratives}},
	volume = {18},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3703918},
	doi = {10.1145/3703918},
	abstract = {This article explores the representation of geospatial knowledge within narratives through a Semantic Web approach. We introduce the NOnt+Space (NOnt+S) model, an extension of the CIDOC CRM-based Narrative Ontology, which allows the representation of narratives and their geospatial aspects. By leveraging standards such as CRMgeo and GeoSPARQL, NOnt+S ensures systematic and interoperable geospatial representation in narratives, enabling geospatial queries on knowledge graphs. We present an assessment of NOnt+S utilizing data from the H2020 MOVING European project (2021–2024), which collected knowledge about European mountain value chains intended as Cultural Heritage. We have represented this knowledge as geospatial narratives using NOnt+S. GeoSPARQL queries and semantic reasoning applied to the created KG reveal the ontology ability to infer new geospatial knowledge. Our work contributes to the ongoing efforts in the Semantic Web community to integrate and represent geospatial information within narratives, promoting collaboration and interoperability across various scientific domains.},
	number = {1},
	journal = {J. Comput. Cult. Herit.},
	author = {Bartalesi, Valentina and Pratelli, Nicolò},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Semantic Web, Knowledge Graph, GeoSPARQL, Digital Humanities, CRMgeo, Geospatial Narratives},
}

@inproceedings{cheng_semantic_2019,
	address = {New York, NY, USA},
	series = {{AIIPCC} '19},
	title = {The semantic tagging model of chinese question sentence chunk based on description logics},
	isbn = {978-1-4503-7633-4},
	url = {https://doi.org/10.1145/3371425.3371442},
	doi = {10.1145/3371425.3371442},
	abstract = {QA (Question Answering) is one of the hot spots in artificial intelligence field. At present, English QA research has made great progress in large-scale text. There are still many difficulties in Chinese QA and it is impossible to understand the true meaning of questions. To solve the problem of Chinese question sentence can't provide deep semantic information for syntactic analysis, the paper proposes a semantic tagging model of Chinese question sentence chunk based on description logics. With the knowledge of HNC's concept symbol to get semantic information about the word, and by starting with the connotation of the concept categories, it preliminarily analyzes the logical structure of specific question sentences. Finally, with description logic reasoning mechanism, we get semantic view of question sentence and proving it's verify in practical.},
	booktitle = {Proceedings of the {International} {Conference} on {Artificial} {Intelligence}, {Information} {Processing} and {Cloud} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Xianyi and Ji, Guohua and Zhang, Xiaohua and Chen, Fengmei},
	year = {2019},
	note = {event-place: Sanya, China},
	keywords = {QA, description logics, semantic tagging, chunk, HNC},
}

@article{israelsen_good_2025,
	title = {“{A} {Good} {Bot} {Always} {Knows} {Its} {Limitations}”: {Assessing} {Autonomous} {System} {Decision}-{Making} {Competencies} through {Factorized} {Machine} {Self}-{Confidence}},
	volume = {14},
	url = {https://doi.org/10.1145/3732794},
	doi = {10.1145/3732794},
	abstract = {How can intelligent machines assess their competency to complete a task? This question has come into focus for autonomous systems that algorithmically make decisions under uncertainty. We argue that machine self-confidence—a form of meta-reasoning based on self-assessments of system knowledge about the state of the world, itself, and ability to reason about and execute tasks—leads to many computable and useful competency indicators for such agents. This article presents our body of work, so far, on this concept in the form of the Factorized Machine Self-Confidence (FaMSeC) framework, which holistically considers several major factors driving competency in algorithmic decision-making: outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self-confidence indicators are derived via “problem-solving statistics” embedded in Markov Decision Process solvers and related approaches. These statistics come from evaluating probabilistic exceedance margins in relation to certain outcomes and associated competency standards specified by an evaluator. Once designed, and evaluated, the statistics can be easily incorporated into autonomous agents and serve as indicators of competency. We include detailed descriptions and examples for Markov Decision Process agents and show how outcome assessment and solver quality factors can be found for a range of tasking contexts through novel use of meta-utility functions, behavior simulations, and surrogate prediction models. Numerical evaluations are performed to demonstrate that FaMSeC indicators perform as desired (references to human subject studies beyond the scope of this article are provided).},
	number = {4},
	journal = {J. Hum.-Robot Interact.},
	author = {Israelsen, Brett and Ahmed, Nisar R. and Aitken, Matthew and Frew, Eric W. and Lawrence, Dale A. and Argrow, Brian M.},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {autonomous robots, human-autonomy interaction, Markov decision processes, probabilistic models, proficiency assessment},
}

@inproceedings{gounakis_evaluating_2023,
	address = {New York, NY, USA},
	series = {{HT} '23},
	title = {Evaluating a {Radius}-based {Pipeline} for {Question} {Answering} over {Cultural} ({CIDOC}-{CRM} based) {Knowledge} {Graphs}},
	isbn = {979-8-4007-0232-7},
	url = {https://doi.org/10.1145/3603163.3609067},
	doi = {10.1145/3603163.3609067},
	abstract = {CIDOC-CRM is an event-based international standard for cultural documentation that has been widely used for offering semantic interoperability in the Cultural Heritage (CH) domain. Although there are several Knowledge Graphs (KGs) expressed by using CIDOC-CRM, the task of Question Answering (QA) has not been studied over such graphs. For this reason, in this paper we propose and evaluate a Radius-based QA pipeline over CIDOC-CRM KGs for single-entity factoid questions. In particular, we propose a generic QA pipeline that comprises several models and methods, including a keyword search model for recognizing the entity of the question (and linking it to the KG), methods that are based on path expansion for constructing subgraphs of different radius (i.e., path lengths) starting from the recognized entity, i.e., for being used as a context, and pre-trained neural models (based on BERT) for answering the question using the mentioned context. Moreover, since there are no available benchmarks over CIDOC-CRM KGs, we construct (by using a real KG) an evaluation benchmark having 10,000 questions, i.e., 5,000 single-entity factoid, 2,500 comparative and 2,500 confirmation questions. For evaluating the QA pipeline, we use the 5,000 single-entity factoid questions. Concerning the results, the QA pipeline achieves satisfactory results both in the entity recognition step (78\% accuracy) and in the QA process (51\% F1 score).},
	booktitle = {Proceedings of the 34th {ACM} {Conference} on {Hypertext} and {Social} {Media}},
	publisher = {Association for Computing Machinery},
	author = {Gounakis, Nikos and Mountantonakis, Michalis and Tzitzikas, Yannis},
	year = {2023},
	note = {event-place: Rome, Italy},
	keywords = {Natural Language Processing, Knowledge Graph, Linked Data, Entity Recognition, Cultural Heritage, Answer Extraction, Event-Based Ontology, Path Expansion, Resource Description Framework},
}

@inproceedings{ding_textual_2020,
	address = {New York, NY, USA},
	series = {{ICIT} '19},
	title = {Textual {Information} {Extraction} {Model} of {Financial} {Reports}},
	isbn = {978-1-4503-7663-1},
	url = {https://doi.org/10.1145/3377170.3377231},
	doi = {10.1145/3377170.3377231},
	abstract = {This paper proposes a model to extract textual information from financial reports automatically. It takes event extraction as the core, maps narrative information of financial reports into the concepts of financial accounting field, and forms the integration of heterogeneous data of distributed financial information. This study shows that the model can identify text events in large-scale financial reporting corpus, automatically extract events and theirs attribute information, and convert them into structured data. Moreover, this paper presents and evaluates the effect of the model in extracting information from annual reports of listed companies. The experimental results turn out that the model provides semantic mapping between text events and domain knowledge concepts, which is reasonable and reliable to be applied in the field of financial statement analysis.},
	booktitle = {Proceedings of the 2019 7th {International} {Conference} on {Information} {Technology}: {IoT} and {Smart} {City}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Pan and Zhuoqian, Liang and Yuan, Deng},
	year = {2020},
	note = {event-place: Shanghai, China},
	keywords = {Event extraction, Corpus, Financial report, Textual information extraction},
	pages = {404--408},
}

@article{kumari_hindi_2023,
	title = {Hindi {Text} {Summarization} {Using} {Sequence} to {Sequence} {Neural} {Network}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3624013},
	doi = {10.1145/3624013},
	abstract = {Text summarizing reduces a large block of text data to a precise, short, and intelligible text that conveys the whole meaning of the actual text in a few words while maintaining the original context. Due to a lack of relevant summaries, it is hard to understand the main idea of the document. Text summarization using the abstractive technique is well-studied in English, although it is still in its infancy in Indian regional languages. In this study, we investigate the effectiveness of using a sequence-to-sequence (Seq2Seq) neural network based on attention and its optimization for text summarization for the Hindi language (HiATS), explicitly comparing the Adam and RMSprop optimizers. Our method allows the model to take the Hindi language dataset and, as output, provides a concise summary that accurately reflects the gist of the original text. The performance of the models will be evaluated using Rouge-1 and Rouge-2 metrics.},
	number = {10},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Kumari, Namrata and Singh, Pardeep},
	month = oct,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {neural network, word embedding, Abstractive text summarization, optimizers},
}

@inproceedings{hu_autonomous_2025,
	address = {New York, NY, USA},
	series = {{AAR} '25},
	title = {Autonomous {Realities}: {A} {Journey} into {Protocolizing} {Digital} {Object} {Permanence} in a {Future} of {Many} {Mixed} {Realities}},
	isbn = {979-8-4007-2003-1},
	url = {https://doi.org/10.1145/3744169.3744197},
	doi = {10.1145/3744169.3744197},
	abstract = {Major technology companies envision a future where mixed reality (MR) devices become as ubiquitous as smartphones are today. Yet most collaborative MR research assumes users share a single augmented layer—an assumption that may not hold true. Rather, MR technology is inherently permissionless: users control what they see, making each person’s augmented layers private and unique. We are moving toward a future of multiple overlapping and co-existing mixed realities. This paper employs protocol fiction as a speculative design method to explore this near-future scenario. We follow the journey of a fictional digital pet rock as it travels through successive protocol eras of mixed reality, adapting to the changing infrastructures and protocols it encounters. Through a comic-style narrative, the story unfolds across four protocol-defined chapters: Centralized Realities, Distributed Realities, Persistent Realities, and Autonomous Realities. Each chapter examines moments when digital pet rock owners—wearing MR headsets—engage in social encounters, revealing how protocols shape the ontological nature of digital object permanence and highlight the socio-technical challenges of constructing consensus reality.},
	booktitle = {Proceedings of the {Sixth} {Decennial} {Aarhus} {Conference}: {Computing} {X} {Crisis}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Botao Amber},
	year = {2025},
	keywords = {Design Fiction, Merging Mixed Reality, Metaverse Interoperability, Object Permanence, Ontology of Digital Object, Protocol Design, Protocol Fiction, Social Mixed Reality},
	pages = {290--302},
}

@inproceedings{prabowo_brick-by-brick_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Brick-by-{Brick}: {Cyber}-{Physical} {Building} {Data} {Classification} {Challenge}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3718483},
	doi = {10.1145/3701716.3718483},
	abstract = {optimization essential in combating climate change. Cyber-Physical Buildings, enabled by the integration of Internet-of-Things (IoT) devices and advanced data analytics tools like AI, offer a smart and effective approach to energy management. A key challenge, however, lies in automating the semantic labeling of IoT devices to ensure machine-interpretable data. The ”Brick-by-Brick: Cyber-Physical Building Data Classification Challenge” aims to tackle this challenge by classifying time-series data from IoT devices within buildings. Participants will engage with a dataset consisting of over 10,000 time-series streams collected over three years across three buildings, representing 91 unique semantic classes. Both the dataset and baselines are established in a published paper. With a total prize pool of 20,000 AUD, the competition is ready to launch in December 2024 and run through February 2025, hosted by AIcrowd. This challenge invites researchers, practitioners, and technologists to drive AI-enabled solutions for advancing the next generation of environmentally sustainable cyber-physical buildings. Additional details on the dataset, benchmark, and code can be found in the official repository ( https://github.com/cruiseresearchgroup/DIEF\_BTS). The challenge was published on AIcrowd www.aicrowd.com/challenges/brick-by-brick-2024.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Prabowo, Arian and Lin, Xiachong and Razzak, Imran and Xue, Hao and Amos, Matthew and White, Stephen D. and Salim, Flora D.},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {machine learning, ontology, classification, building, timeseries},
	pages = {3021--3025},
}

@inproceedings{khallouki_ontology-based_2018,
	address = {New York, NY, USA},
	series = {{LOPAL} '18},
	title = {An {Ontology}-based {Context} awareness for {Smart} {Tourism} {Recommendation} {System}},
	isbn = {978-1-4503-5304-5},
	url = {https://doi.org/10.1145/3230905.3230935},
	doi = {10.1145/3230905.3230935},
	abstract = {Smart tourism concept appeared with the development of Smart Cities. Bringing Smartness into Tourism needs a dynamic and interconnected system on which information relating to tourism activities could be exchanged in real time. In this paper, we introduce a new approach for designing mobile tourism recommendation system using context awareness. The proposed approach combines Internet of Things (IoT) technologies with semantic web services to predict the tourist real-time context and provide the suitable services.},
	booktitle = {Proceedings of the {International} {Conference} on {Learning} and {Optimization} {Algorithms}: {Theory} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Khallouki, Hajar and Abatal, Ahmed and Bahaj, Mohamed},
	year = {2018},
	note = {event-place: Rabat, Morocco},
	keywords = {semantic web, IoT, context awareness, real-time context, Smart tourism, tourism recommendation system},
}

@inproceedings{yeom_embedding_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Embedding {Two}-{View} {Knowledge} {Graphs} with {Class} {Inheritance} and {Structural} {Similarity}},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671941},
	doi = {10.1145/3637528.3671941},
	abstract = {Numerous large-scale knowledge graphs (KGs) fundamentally represent two-view KGs: an ontology-view KG with abstract classes in ontology and an instance-view KG with specific collections of entities instantiated from ontology classes. Two-view KG embedding aims to jointly learn continuous vector representations of entities and relations in the aforementioned two-view KGs. In essence, an ontology schema exhibits a tree-like structure guided by class hierarchies, which leads classes to form inheritance hierarchies. However, existing two-view KG embedding models neglect those hierarchies, which provides the necessity to reflect class inheritance. On the other hand, KG is constructed based on a pre-defined ontology schema that includes heterogeneous relations between classes. Furthermore, these relations are defined within the scope of those among classes since instances inherit all the properties of their corresponding classes, which reveals structural similarity between two multi-relational networks. Despite the consideration to bridge the gap among two-view KG representations, existing methods ignore the existence of structural similarity between two-view KGs. To address these issues, we propose a novel two-view KG embedding model, CISS, considering Class Inheritance and Structural Similarity between two-view KGs. To deal with class inheritance, we utilize class sets, each of which is composed of sibling classes, to learn fine-grained class representations. In addition, we configure virtual instance-view KG from clustered instances and compare subgraph representations of two-view KGs to enhance structural similarity between them. Experimental results show our superior performance compared to existing models.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Yeom, Kyuhwan and Yang, Hyeongjun and Park, Gayeon and Jeon, Myeongheon and Ko, Yunjeong and Oh, Byungkook and Lee, Kyong-Ho},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {ontology, knowledge graph, class inheritance, structural similarity},
	pages = {3931--3941},
}

@inproceedings{plakidas_model-based_2018,
	address = {New York, NY, USA},
	series = {{ECSA} '18},
	title = {Model-based support for decision-making in architecture evolution of complex software systems},
	isbn = {978-1-4503-6483-6},
	url = {https://doi.org/10.1145/3241403.3241426},
	doi = {10.1145/3241403.3241426},
	abstract = {Design decision support for software architects in complex industrial software systems, such as software ecosystems and systems-of-systems, which feature extensive reuse of third-party solutions and a variety of deployment options, is still an open challenge. We describe three industrial use cases involving considerable re-architecting, where on-premises solutions were migrated to a cloud-based IoT platforms. Based on these use cases, we analyse the challenges and derive requirements for an architecture knowledge model supporting this process. The presented methodology builds upon existing approaches and proposes a model for the description of extant software applications and the management of domain knowledge. We demonstrate its use to support the evolution and/or composition of software applications in a migration scenario in a systematic and traceable manner.},
	booktitle = {Proceedings of the 12th {European} {Conference} on {Software} {Architecture}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
	year = {2018},
	note = {event-place: Madrid, Spain},
	keywords = {model-based decision support, software architecture evolution, software migration, software variability management, systems-of-systems composition},
}

@inproceedings{fritz_context-sensitive_2021,
	address = {New York, NY, USA},
	series = {{NLPIR} '20},
	title = {Context-sensitive {Assistance} in {Requirements}-based {Knowledge} {Management}},
	isbn = {978-1-4503-7760-7},
	url = {https://doi.org/10.1145/3443279.3443306},
	doi = {10.1145/3443279.3443306},
	abstract = {In this paper, a concept of a digital assistance system is presented which, based on computer linguistic methods, supports the user in the tasks of requirement-based knowledge management. The concept is divided into six modules that offer context-sensitive support in the identification, documentation, linking, modification and reuse of requirements and the associated knowledge. Since this concept was developed as part of the BMBF-funded SME Innovative Project DAM4KMU, which is primarily aimed at German SMEs, the concept developed was specially designed for processing German-language texts.The digital assistance system pursues the goal, on the one hand, of increasing the quality of the documentation by supporting the user in the creation of complete formulations. On the other hand, with the help of the most modern language models, possible relationships between the information should be identified and linked to each other in a partially automated manner. In addition, the integration of web crawling technologies should make the knowledge available on the Internet available in a context-sensitive manner, in order to lift possible innovations on the one hand and not to forget possible non-considered boundary conditions on the other.The automatic linking of all information is intended to ensure a continuous exchange of knowledge, which should reduce misunderstandings and non-communicated changes to requirements or goals to a minimum.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Fritz, Simon and Jaenicke, Matthias and Ovtcharova, Jivka and Wicaksono, Hendro},
	year = {2021},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {Knowledge management, Natural Language Processing, Requirements Engineering, Digital Assistance, Web Crawling},
	pages = {47--54},
}

@inproceedings{rizvi_efficient_2018,
	address = {New York, NY, USA},
	series = {{CODASPY} '18},
	title = {Efficient {Authorization} of {Graph} {Database} {Queries} in an {Attribute}-{Supporting} {ReBAC} {Model}},
	isbn = {978-1-4503-5632-9},
	url = {https://doi.org/10.1145/3176258.3176331},
	doi = {10.1145/3176258.3176331},
	abstract = {Neo4j is a popular graph database that offers two versions; a paid enterprise edition and a free community edition. The enterprise edition offers customizable Role-Based Access Control (RBAC) features through custom developed procedures, while the community edition does not offer any access control support. Being a graph database, Neo4j is a natural application for Relationship-Based Access Control (ReBAC), an access control paradigm where authorization decisions are based on relationships between subjects and resources in the system. In this paper we present AReBAC, an attribute-supporting ReBAC model for Neo4j (applicable to both editions) that provides finer grained access control. AReBAC employs Nano-Cypher, a declarative policy language based on Neo4j»s Cypher query language, the result of which allows us to weave database queries with access control policies and evaluate both simultaneously. Evaluating the combined query and policy produces a result that i) matches the search criteria, and ii) the requesting subject has access to. Our experiments show that our evaluation algorithm performs faster than Neo4j»s query evaluation engine when evaluating queries that are expressible using Nano-Cypher.},
	booktitle = {Proceedings of the {Eighth} {ACM} {Conference} on {Data} and {Application} {Security} and {Privacy}},
	publisher = {Association for Computing Machinery},
	author = {Rizvi, Syed Zain R. and Fong, Philip W. L.},
	year = {2018},
	note = {event-place: Tempe, AZ, USA},
	keywords = {graph database, attributes, neo4j, relatioship-based access control},
	pages = {204--211},
}

@inproceedings{benaben_integrating_2020,
	address = {National Harbor, Maryland},
	series = {{WSC} '19},
	title = {Integrating model-driven engineering as the next challenge for artificial intelligence: application to risk and crisis management},
	isbn = {978-1-7281-3283-9},
	abstract = {Artificial Intelligence (AI) is currently on top of the hype regarding simultaneously research publications and industrial development. However, the current status of AI makes it quite far and different from the current understanding of Human intelligence. One suggestion that is made in this article is that Model-Driven approaches could be considered as an interesting avenue to complement classical visions of AI and to provide some missing features. Specifically, the use of Model-Driven Engineering tools (such as metamodel and model transformation) could benefit to the domain of AI by introducing a way to extend the apprehension of unknown situations. To support that proposal, an illustrative example is provided regarding the domain of risk and crisis management.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Benaben, Frederick and Lauras, Matthieu and Fertier, Audrey and Salatgé, Nicolas},
	year = {2020},
	pages = {1549--1563},
}

@article{kapugama_geeganage_text2el_2024,
	title = {{Text2EL}+: {Expert} {Guided} {Event} {Log} {Enrichment} {Using} {Unstructured} {Text}},
	volume = {16},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3640018},
	doi = {10.1145/3640018},
	abstract = {Through the application of process mining, business processes can be improved on the basis of process execution data captured in event logs. Naturally, the quality of this data determines the quality of the improvement recommendations. Improving data quality is non-trivial, and there is great potential to exploit unstructured text, e.g., from notes, reviews, and comments, for this purpose and to enrich event logs. To this end, this article introduces Text2EL+\&nbsp;, a three-phase approach to enrich event logs using unstructured text. In its first phase, events and (case and event) attributes are derived from unstructured text linked to organisational processes. In its second phase, these events and attributes undergo a semantic and contextual validation before their incorporation in the event log. In its third and final phase, recognising the importance of human domain expertise, expert guidance is used to further improve data quality by removing redundant and irrelevant events. Expert input is used to train a Named Entity Recognition (NER) model with customised tags to detect event log elements. The approach applies natural language processing techniques, sentence embeddings, training pipelines and models, as well as contextual and expression validation. Various unstructured clinical notes associated with a healthcare case study were analysed, and completeness, concordance, and correctness of the derived event log elements were evaluated through experiments. The results show that the proposed method is feasible and applicable.},
	number = {1},
	journal = {J. Data and Information Quality},
	author = {Kapugama Geeganage, Dakshi Tharanga and Wynn, Moe Thandar and ter Hofstede, Arthur H. M.},
	month = mar,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language processing, unstructured text, Event data quality, event log, process mining, semantic validation},
}

@inproceedings{saboia_openbridge_2025,
	address = {New York, NY, USA},
	series = {{PEARC} '25},
	title = {{OpenBridge}: {Bridging} {Domain} {Scientists} and {APIs} through {AI}-{Powered} {Interfaces}},
	isbn = {979-8-4007-1398-9},
	url = {https://doi.org/10.1145/3708035.3736045},
	doi = {10.1145/3708035.3736045},
	abstract = {We propose OpenBridge, an MCP-compliant server that enables AI assistants to access scientific APIs through natural language. It converts OpenAPI Specification (OAS) endpoints into Model Context Protocol (MCP) tools and enriches responses with JavaScript Object Notation for Linked Data (JSON-LD) for improved semantic clarity. As we demonstrate using data from Paper Analytical Devices (PADs), OpenBridge allows researchers to retrieve and explore structured data without writing code. Thus, it offers a new approach to bridging domain experts and data systems through AI-assisted, semantically aware interfaces. The implementation is available at https://github.com/PaperAnalyticalDeviceND/OpenBridge.},
	booktitle = {Practice and {Experience} in {Advanced} {Research} {Computing} 2025: {The} {Power} of {Collaboration}},
	publisher = {Association for Computing Machinery},
	author = {Saboia, Priscila and Sweet, James and Sweet, Christopher},
	year = {2025},
	keywords = {OpenAPI, AI Agents, Model Context Protocol, Research Data Infrastructure},
}

@inproceedings{daquin_combining_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {Combining representation formalisms for reasoning upon mathematical knowledge},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3627549},
	doi = {10.1145/3587259.3627549},
	abstract = {Knowledge in mathematics (definitions, theorems, proofs, etc.) is usually expressed in a way that combines natural language and mathematical expressions (e.g. equations). Using an ontology formalism such as OWL\&nbsp;DL is well-suited for formalizing the natural language part, but complex mathematical expressions can be better handled by symbolic computation systems. We examine this representation issue and propose an original extension of OWL\&nbsp;DL by call formulas, i.e., formulas from which assertions can be drawn thanks to calls to external functions. Using this formalism makes it possible to classify a mathematical problem defined by its relations to instances and classes and by some mathematical expressions: if a theorem for solving this problem is represented in the knowledge base, it can be retrieved, and thus, the problem can be solved by applying this theorem. We describe an inference algorithm and discuss its properties as well as its limitations. Indeed, the proposed extension, algorithm, and implementation represent a first step towards a combined formalism for representing mathematical knowledge, with some open issues regarding the representation of more complex problems: the resolution of multiscale, multiphysics cases in physics are foreseen.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {D'Aquin, Mathieu and Bunoiu, Renata and Cirstea, Horatiu and Lenczner, Michel and Lieber, Jean and Zamkotsian, Frédéric},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {knowledge representation, knowledge modeling, mathematical knowledge},
	pages = {180--187},
}

@inproceedings{kaindl_inductive_2018,
	address = {New York, NY, USA},
	series = {{SPLC} '18},
	title = {An inductive learning perspective on automated generation of feature models from given product specifications},
	isbn = {978-1-4503-6464-5},
	url = {https://doi.org/10.1145/3233027.3233031},
	doi = {10.1145/3233027.3233031},
	abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
	booktitle = {Proceedings of the 22nd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} 1},
	publisher = {Association for Computing Machinery},
	author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {machine learning, generating feature models, inductive generalization from examples},
	pages = {25--30},
}

@inproceedings{carbonnel_towards_2018,
	address = {New York, NY, USA},
	series = {{VAMOS} '18},
	title = {Towards the {Extraction} of {Variability} {Information} to {Assist} {Variability} {Modelling} of {Complex} {Product} {Lines}},
	isbn = {978-1-4503-5398-4},
	url = {https://doi.org/10.1145/3168365.3168378},
	doi = {10.1145/3168365.3168378},
	abstract = {Software product line engineering gathers a set of methods that rely on systematic reuse and mass customisation to reduce the development time and cost of a set of similar software systems. Boolean feature models are the de facto standard used to represent product line variability in terms of features, a feature being a distinguishable characteristic of one or several softwares. The extractive adoption of a product line from a set of individually developed softwares requires to extract variability information from a collection of software descriptions to model their variability. With the appearance of more and more complex software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of boolean feature models, as multi-valued attributes or UML-like cardinalities have since been proposed to support variability modelling in complex product lines. In this paper, we propose research directions to address the issue of extracting more complex variability information, as a part of extended feature models synthesis from software descriptions. We consider the capabilities of Formal Concept Analysis, a mathematical framework for knowledge discovery, along with two of its extensions called Pattern Structures and Relational Concept Analysis, to answer this problematic. These frameworks bring theoretical foundations to complex variability extraction algorithms.},
	booktitle = {Proceedings of the 12th {International} {Workshop} on {Variability} {Modelling} of {Software}-{Intensive} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Clémentine},
	year = {2018},
	note = {event-place: Madrid, Spain},
	keywords = {Software Product Line, Reverse Engineering, Variability Extraction},
	pages = {113--120},
}

@inproceedings{gueddes_e-saad_2019,
	address = {New York, NY, USA},
	series = {{ICIST} '19},
	title = {e-{SAAD} system: {Ontologies} based approach for home {Care} {Services} platform},
	isbn = {978-1-4503-6292-4},
	url = {https://doi.org/10.1145/3361570.3361597},
	doi = {10.1145/3361570.3361597},
	abstract = {In the first generation of interventions and home support, the needs were triggered generally by phone call and they are described orally. In most cases, there is no registered information about the patient's conditions or medical history. The interveners are either from the private or state health sector. Despite the fact that there are people who can intervene quicker than others, yet they have not been recruited, or work as professional liberal. With the development of technology, systems based on data mining or artificial intelligence have been developed to focus on the intervention's time for example. Although intervention and home-based care are the subject of many studies [1], the resolution of the overall decision-making problem is not sufficiently developed. On the one hand, the state of health presupposes the definition of a patient. There is a set of parameters characterizing the habits of daily life of the person analyzed in parallel with the evolution of physiological and environment. On the other hand, it is necessary to take into consideration the medical Core, his location, his profile not only his professional status but also his abilities and skills that are not explicitly described in his curriculum. Different studies and systems exist in the literature [2]. Each of his studies tackles only a part of the parameters. Indeed, these studies consider either the monitoring of daily activities, the monitoring of physiological data or other environmental parts. Either they consider the specificities of the medical Core's profile, or these systems use a probabilistic data mining that involves many interactions with the experts to interpret the data, either wise an expert system based on the inference rules defined by the medical experts. In addition, most systems do not use controlled vocabulary that provides semantics needed. This complicates information sharing and collaborative work. The objective of the e-SAAD project is to propose a methodological process to facilitate the analysis and procedure of intervention systems and home support. The process should identify the generic and specific aspects of each part. The patient's data set, profile, history, its environment and location should be taken into consideration. As well as the service providers, their profiles, their skills and essentially their availability and locations. These models must be open to be adapted to new data sources.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Information} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Gueddes, Abdelweheb and Mahjoub, Mohamed Ali},
	year = {2019},
	note = {event-place: Cairo, Egypt},
	keywords = {Semantic Web, ontology, home-based care, e-SAAD, Telemedicine},
}

@inproceedings{burgueno_mde_2021,
	address = {Munich, Germany},
	series = {{MODELS} '19 {Companion}},
	title = {{MDE} intelligence 2019: 1st workshop on artificial intelligence and model-driven engineering},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00028},
	doi = {10.1109/MODELS-C.2019.00028},
	abstract = {Model-driven engineering (MDE) and Artificial Intelligence (AI) are two separate fields in computer science, which can clearly benefit from cross-fertilization and collaboration. There are at least two ways in which such integrations—which we call MDE Intelligence—can manifest: (1) MDE can benefit from integrating AI concepts and ideas to increasing the power and flexibility of model-driven techniques by means of the application of AI algorithms. (2) Conversely, AI can benefit from integrating concepts and ideas from MDE—for example, using domain-specific languages and model transformations allows domain experts to directly express and manipulate their problems while providing an auditable computation pipeline.To discuss and further stimulate such integrations, the 1st edition of the Workshop on Artificial Intelligence and Model-driven Engineering (MDE Intelligence) was held on September 16, 2019 in Munich, Germany, as part of the satellite events of the IEEE/ACM 22th International Conference on Model-Driven Engineering Languages and Systems (MODELS 2019).},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion}},
	publisher = {IEEE Press},
	author = {Burgueño, Loli and Burdusel, Alexandru and Gérard, Sébastien and Wimmer, Manuel},
	year = {2021},
	keywords = {artificial intelligence, MDE, MDE intelligence},
	pages = {168--169},
}

@inproceedings{boruah_reasoning_2020,
	address = {New York, NY, USA},
	series = {{AIR} '19},
	title = {Reasoning on {Objects}' {Geometric} {Shapes} for {Prosthetic} {Hand} {Grasping}},
	isbn = {978-1-4503-6650-2},
	url = {https://doi.org/10.1145/3352593.3352604},
	doi = {10.1145/3352593.3352604},
	abstract = {The problem of knowing what to grasp and deciding how to grasp is an open issue for development of intelligent prosthetic hands. To emulate the potentialities of a human hand, knowledge of the grasping domain has to be accumulated and modelled in a machine interpretable format. In this paper, we have tried to comprehend and model a specific part of the knowledge (information) of a prosthetic hand-grasping domain into a reusable Web-Ontology-Language (OWL) format. This ontology build after basic analysis of hand object coordination, can be used for preserving, improving and sharing the captured knowledge. We begin with our description of the required knowledge of a geometrical concept formed during human grasping, to a point where it can be used to plan grasping based on the objects identified. Using tactile and kinesthetic information along with relevant domain concepts, we emphasized on the rationality of designing an ontology for reusability and sustainability of knowledge. We tried to lay down a visual model of the ontology, also called the Ontograph, which illuminates the existence and relationships among the various objects of the grasping domain. We have also checked the decisive capability of the ontology by reasoning it with Description Logic (DL) queries of data property values for individuals of geometric classes. The output of the queries provided us with individuals of the specific geometric pattern, which can be used to decide the type of grasp that could be implemented on objects.},
	booktitle = {Proceedings of the 2019 4th {International} {Conference} on {Advances} in {Robotics}},
	publisher = {Association for Computing Machinery},
	author = {Boruah, Abhijit and Kakoty, Nayan M. and Ali, Tazid},
	year = {2020},
	note = {event-place: Chennai, India},
	keywords = {Ontology, Reasoning, OWL, Knowledge, Tactile, Kinesthetic, Prosthetics},
}

@inproceedings{liao_proactive_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {Proactive {Conversational} {Agents} in the {Post}-{ChatGPT} {World}},
	isbn = {978-1-4503-9408-6},
	url = {https://doi.org/10.1145/3539618.3594250},
	doi = {10.1145/3539618.3594250},
	abstract = {ChatGPT and similar large language model (LLM) based conversational agents have brought shock waves to the research world. Although astonished by their human-like performance, we find they share a significant weakness with many other existing conversational agents in that they all take a passive approach in responding to user queries. This limits their capacity to understand the users and the task better and to offer recommendations based on a broader context than a given conversation. Proactiveness is still missing in these agents, including their ability to initiate a conversation, shift topics, or offer recommendations that take into account a more extensive context. To address this limitation, this tutorial reviews methods for equipping conversational agents with proactive interaction abilities.The full-day tutorial is divided into four parts, including multiple interactive exercises. We will begin the tutorial with an interactive exercise and cover the design of existing conversational systems architecture and challenges. The content includes coverage of LLM-based recent advancements such as ChatGPT and Bard, along with reinforcement learning with human feedback (RLHF) technique. Then we will introduce the concept of proactive conversation agents and preset recent advancements in proactiveness of conversational agents, including actively driving conversations by asking questions, topic shifting, and methods that support strategic planning of conversation. Next, we will discuss important issues in conversational responses' quality control, including safety, appropriateness, language detoxication, hallucination, and alignment. Lastly, we will launch another interactive exercise and discussion with the audience to arrive at concluding remarks, prospecting open challenges and new directions. By exploring new techniques for enhancing conversational agents' proactive behavior to improve user engagement, this tutorial aims to help researchers and practitioners develop more effective conversational agents that can better understand and respond to user needs proactively and safely.},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Lizi and Yang, Grace Hui and Shah, Chirag},
	year = {2023},
	note = {event-place: Taipei, Taiwan},
	keywords = {conversational search, conversational ai, proactive conversation},
	pages = {3452--3455},
}

@inproceedings{suciu_entity_2024,
	address = {New York, NY, USA},
	series = {{ARES} '24},
	title = {Entity {Recognition} on {Border} {Security}},
	isbn = {979-8-4007-1718-5},
	url = {https://doi.org/10.1145/3664476.3669922},
	doi = {10.1145/3664476.3669922},
	abstract = {Entity recognition, also known as named entity recognition (NER), is a fundamental task in natural language processing (NLP) that involves identifying and categorizing entities within text. These entities, such as names of people, organizations, locations, dates, and numerical values, provide structured information from unstructured text data. NER models, ranging from rule-based to machine learning-based approaches, decode linguistic patterns and contextual information to extract entities effectively. This article explores the roles of entities, tokens, and NER models in NLP, detailing their significance in various applications like information retrieval and border security. It delves into the practices of implementing NER in legal document analysis, travel history analysis, and document verification, showcasing its transformative impact in streamlining processes and enhancing security measures. Despite challenges such as ambiguity and data scarcity, ongoing research and emerging trends in multilingual NER and ethical considerations promise to drive innovation in the field. By addressing these challenges and embracing new developments, entity recognition is poised to continue advancing NLP capabilities and powering diverse real-world applications.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Suciu, George and Sachian, Mari-Anais and Bratulescu, Razvan and Koci, Kejsi and Parangoni, Grigor},
	year = {2024},
	note = {event-place: Vienna, Austria},
	keywords = {Machine Learning, SVM, NER, Recognition, Border Security, Entity, Frameworks, RNNs, Travel},
}

@inproceedings{wu_construction_2023,
	address = {New York, NY, USA},
	series = {{IMMS} '23},
	title = {Construction {Safety} {Knowledge} {Graph} {Integrating} {Text} and {Image} {Information}},
	isbn = {979-8-4007-0768-1},
	url = {https://doi.org/10.1145/3625469.3625470},
	doi = {10.1145/3625469.3625470},
	abstract = {To improve the extraction efficiency and visualization of construction safety knowledge, this paper combines knowledge graph technology with construction safety domain, and proposes a basic framework of construction safety knowledge ontology based on the evolution logic of safety events according to the characteristics of knowledge. Considering two types of safety knowledge carriers and data sources, text and image, a knowledge graph is designed to contain text semantic features and image features, and the knowledge services based on different dimensional knowledge queries are validated in the experiments. The results show that the BERT-BiLSTM-CRF algorithm can be used to extract entities in text, and YOLOv5-FastPose can extract excavator poses from images. This paper verifies the applicability of knowledge graphs for safety knowledge mining, visualization and services.},
	booktitle = {Proceedings of the 2023 6th {International} {Conference} on {Information} {Management} and {Management} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Wenjing and Yuan, Qi and Chen, Qiulan and Cao, Yunzhong},
	year = {2023},
	note = {event-place: Chengdu, China},
	keywords = {Knowledge graph, Construction safety management, Entity recognition, Pose estimation},
	pages = {26--32},
}

@inproceedings{jabbar_real-time_2018,
	address = {New York, NY, USA},
	series = {{ICFNDS} '18},
	title = {Real-time {RDF} adaptation model for smart human-care querying in {IoT} based mobile applications},
	isbn = {978-1-4503-6428-7},
	url = {https://doi.org/10.1145/3231053.3231128},
	doi = {10.1145/3231053.3231128},
	abstract = {Majorly, nowadays, the collected raw data through mobiles is huge based on sensors embedded in devices and IoT based applications. These applications use Internet of Things (IoT) and Big Data analytics services and daily activities as routines for recording and analyzing real-time data for human-care. Nowadays, mobile is having services built on sensors to reduce human involvement in data collection. Many issues concerning security and privacy can be resolved if we use data analytics in services to represent data as Resource Description Framework (RDF). The automated transformation mechanism in relational database taken from mobile sensors and applications into semantically annotated RDF stores. This study is comprised of a methodology for refining compatibility between different data models by introducing real-time RDF context model for adopting data to smart querying in mobile applications. Smart querying capabilities come from transformation between sensors with activity services data and RDF data store for mobile applications. Whereas, case study built-up out of applications data is used to show data adaptation process for smart querying for human-care in mobile devices. Multiple queries are used to extract mobile video information smartly and efficiently. According to results shows if standard deviation gets greater than mean that tend of values is spreading over a wider range of values.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Future} {Networks} and {Distributed} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {jabbar, Sohail and Malik, Kaleem Razzaq and Ahmad, Mudassar},
	year = {2018},
	note = {event-place: Amman, Jordan},
	keywords = {IoT, linked data, big data, data modeling, human-care services, mobile application, real-time data transformation, real-time smart querying},
}

@inproceedings{zeynalova_student_2022,
	address = {New York, NY, USA},
	series = {{ICIEI} '22},
	title = {Student {Creativity} and {Talent} {Development} in {Higher} {Education} {Institutions} of {Azerbaijan}},
	isbn = {978-1-4503-9619-6},
	url = {https://doi.org/10.1145/3535735.3535755},
	doi = {10.1145/3535735.3535755},
	abstract = {The aim of the study is to find out how to develop student creativity in Azerbaijan higher education institutions. This research is consists of two parts: descriptive part of the research, where the author tried to analyze background of the study and research have been done in the filed so far. The second part based on a survey, which is conducted for students and teachers of various HEIs of the country. 219 respondents answered the research questions from various universities of Azerbaijan. The results of research indicated that creativity can be strengthen in higher education with help and support of teachers and by knowledge, creativity can be further enhanced in the teaching process depending on the teacher's pedagogical skills and approach, various teaching methods can develop creativity of students in higher education including modern innovative technologies, creativity justifies itself at every moment in teaching and learning process in higher education, motivation is a key issue for increasing creativity.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Information} and {Education} {Innovations}},
	publisher = {Association for Computing Machinery},
	author = {Zeynalova, Nigar},
	year = {2022},
	note = {event-place: Belgrade, Serbia},
	keywords = {development, knowledge, higher education, creativity, innovation, talent},
	pages = {163--175},
}

@inproceedings{leclercq_tensor_2018,
	address = {New York, NY, USA},
	series = {{IDEAS} '18},
	title = {A {Tensor} {Based} {Data} {Model} for {Polystore}: {An} {Application} to {Social} {Networks} {Data}},
	isbn = {978-1-4503-6527-7},
	url = {https://doi.org/10.1145/3216122.3216152},
	doi = {10.1145/3216122.3216152},
	abstract = {In this article, we show how the mathematical object tensor can be used to build a multi-paradigm model for the storage of social data in data warehouses. From an architectural point of view, our approach allows to link different storage systems (polystore) and limits the impact of ETL tools performing model transformations required to feed different analysis algorithms. Therefore, systems can take advantage of multiple data models both in terms of query execution performance and the semantic expressiveness of data representation. The proposed model allows to reach the logical independence between data and programs implementing analysis algorithms. With a concrete case study on message virality on Twitter during the French presidential election of 2017, we highlight some of the contributions of our model.},
	booktitle = {Proceedings of the 22nd {International} {Database} {Engineering} \&amp; {Applications} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Leclercq, Éric and Savonnet, Marinette},
	year = {2018},
	note = {event-place: Villa San Giovanni, Italy},
	keywords = {Associative Array, Multi-paradigm Storage, Multi-relational Networks, OLAP, Polystore, Tensor},
	pages = {110--118},
}

@inproceedings{surkova_word_2020,
	address = {New York, NY, USA},
	series = {{CSIS}'2019},
	title = {Word embedding and cognitive linguistic models in text classification tasks},
	isbn = {978-1-4503-7670-9},
	url = {https://doi.org/10.1145/3373722.3373778},
	doi = {10.1145/3373722.3373778},
	abstract = {The paper considers two linguistic models, analyzed the possibility of their use for the text data classification as well as their associations in the integrated texts presentation. A cognitive approach for the text classification issues is presented. An algorithm to identify the words basic level using WordNet is considered. A model for text classification based on the pre-trained word embeddings is presented. The model consists of three layers: embedding layer Long-Short Term Memory (LSTM) layer, and softmax layer. The model was trained and evaluated on the 20 Newsgroups dataset. The classification quality was assessed by F- measure, precision and recall. The obtained results analysis is carried out. Both described models show good results, low scores for some texts are explained. The advantages and limitations of the linguistic models are shown. In future works the authors are going to combine proposed models and modify them. Thus, for model based on word embedding there are pretty vast opportunities for extension: from experimenting with different word embeddings and various distance metrics to more complicated architecture of layers and even promising state of the art artificial neural network models, activation functions and their modifications. In addition, there is research area of proper ensemble strategy selection.},
	booktitle = {Proceedings of the {XI} {International} {Scientific} {Conference} {Communicative} {Strategies} of the {Information} {Society}},
	publisher = {Association for Computing Machinery},
	author = {Surkova, Anna and Skorynin, Sergey and Chernobaev, Igor},
	year = {2020},
	note = {event-place: St. Petersburg, Russian Federation},
	keywords = {classification, WordNet, data mining, cognitive semantics, thesaurus, sequential data, words vector representations},
}

@article{petermann_tackling_2023,
	title = {Tackling the {Cocktail} {Fork} {Problem} for {Separation} and {Transcription} of {Real}-{World} {Soundtracks}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2023.3290428},
	doi = {10.1109/TASLP.2023.3290428},
	abstract = {Emulating the human ability to solve the cocktail party problem, i.e., focus on a source of interest in a complex acoustic scene, is a long standing goal of audio source separation research. In this paper, we focus on the cocktail fork problem, which takes a three-pronged approach to source separation by separating an audio mixture such as a movie soundtrack or podcast into the three broad categories of speech, music, and sound effects (SFX - understood to include ambient noise and natural sound events). We evaluate several deep learning-based source separation models on this task using simple objective measures such as signal-to-distortion ratio (SDR) as well as objective metrics that better correlate with human perception. Furthermore, we thoroughly evaluate how source separation can influence the downstream transcription asks of speech recognition for speech and audio tagging for music and SFX. We also investigate the task of activity detection on the three sources as a way to further improve source separation and transcription. While we observe that source separation improves transcription performance in comparison to the original soundtrack, performance is still sub-optimal due to artifacts introduced by the separation process. Therefore, we thoroughly investigate how remixing of the three separated source stems at various relative levels can reduce artifacts and consequently improve transcription performance. We find that remixing music and SFX interferences at a target SNR of 17.5 dB reduces speech recognition word error rate, and similar impact from remixing is observed for tagging music and SFX content.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Petermann, Darius and Wichern, Gordon and Subramanian, Aswin Shanmugam and Wang, Zhong-Qiu and Roux, Jonathan Le},
	month = jul,
	year = {2023},
	note = {Publisher: IEEE Press},
	pages = {2592--2605},
}

@inproceedings{kumar_document_2024,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '24},
	title = {Document structure aware {Relation} {Extraction} for {Semantic} {Automation}},
	isbn = {979-8-4007-1634-8},
	url = {https://doi.org/10.1145/3632410.3632466},
	doi = {10.1145/3632410.3632466},
	abstract = {Relational Graph Convolutional Network models are a class of Graph Neural Network models used for link prediction in heterogeneous graphs. They’re being used in a variety of industrial applications including semantic automation tasks in a Lakehouse. In this work, we propose a novel way to incorporate document specific features into a RGCN model that helps improve relation extraction accuracy by about 15 points. Further, we extend this document awareness to semantic tasks on tabular data and discuss our results.},
	booktitle = {Proceedings of the 7th {Joint} {International} {Conference} on {Data} {Science} \&amp; {Management} of {Data} (11th {ACM} {IKDD} {CODS} and 29th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Kumar, Ayush and Shalghar, Abhay M and Chauhan, Harsh and Ganesan, Balaji and Chaudhuri, Ritwik and Kannan, Aswin},
	year = {2024},
	note = {event-place: Bangalore, India},
	keywords = {information extraction, RGCN, semantic automation},
	pages = {232--236},
}

@inproceedings{stach_demand-driven_2022,
	address = {New York, NY, USA},
	series = {{iiWAS2021}},
	title = {Demand-{Driven} {Data} {Provisioning} in {Data} {Lakes}: {BARENTS}\&nbsp;—\&nbsp;{A} {Tailorable} {Data} {Preparation} {Zone}},
	isbn = {978-1-4503-9556-4},
	url = {https://doi.org/10.1145/3487664.3487784},
	doi = {10.1145/3487664.3487784},
	abstract = {Data has never been as significant as it is today. It can be acquired virtually at will on any subject. Yet, this poses new challenges towards data management, especially in terms of storage (data is not consumed during processing, i.\&nbsp;e., the data volume keeps growing), flexibility (new applications emerge), and operability (analysts are no IT experts). The goal has to be a demand-driven data provisioning, i.\&nbsp;e., the right data must be available in the right form at the right time. Therefore, we introduce a tailorable data preparation zone for Data Lakes called BARENTS. It enables users to model in an ontology how to derive information from data and assign the information to use cases. The data is automatically processed based on this model and the refined data is made available to the appropriate use cases. Here, we focus on a resource-efficient data management strategy. BARENTS can be embedded seamlessly into established Big Data infrastructures, e.\&nbsp;g., Data Lakes.},
	booktitle = {The 23rd {International} {Conference} on {Information} {Integration} and {Web} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Stach, Christoph and Bräcker, Julia and Eichler, Rebecca and Giebler, Corinna and Mitschang, Bernhard},
	year = {2022},
	note = {event-place: Linz, Austria},
	keywords = {ontology, knowledge modeling, data management, food analysis, Data Lakes, data pre-processing, data transformation, zone model},
	pages = {187--198},
}

@inproceedings{yang_model_2018,
	address = {New York, NY, USA},
	series = {{AI}-{Science}'18},
	title = {A {Model} {Driven} {Intelligent} {Orchestration} {Approach} to {Service} {Automation} in {Large} {Distributed} {Infrastructures}},
	isbn = {978-1-4503-5862-0},
	url = {https://doi.org/10.1145/3217197.3217207},
	doi = {10.1145/3217197.3217207},
	abstract = {Today's scientific computing applications and workflows operate on heterogeneous and vastly distributed infrastructures. Traditional human-in-the-loop service engineering approach met its insurmountable challenge in dealing with these very complex and diverse networked systems, including conventional and software defined networks, compute, storage, clouds and instruments. Orchestration is the key to integrate and coordinate the networked multi-services and automate end-to-end workflows. In this work, we present a model driven intelligent orchestration approach to this end-to-end automation, which is built upon a semantic modeling solution that supports the full stack of service integration, orchestration, abstraction, and intent and policy representation. We also present the design of a real-world orchestrator called StackV that is able to accommodate highly complex application scenarios such as Software Defined ScienceDMZ (SD-SDMZ) and Hybrid Cloud Inter-Networking (HCIN) by implementing this approach.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Autonomous} {Infrastructure} for {Science}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Xi and Lehman, Tom and Kettimuthu, Raj and Winkler, Linda and Jung, Eun-Sung},
	year = {2018},
	note = {event-place: Tempe, AZ, USA},
	keywords = {Modeling, Distributed Infrastructure, Intelligent Orchestration, Service Automation},
}

@inproceedings{oliveira_towards_2018,
	address = {New York, NY, USA},
	series = {dg.o '18},
	title = {Towards a meta-model for data ecosystems},
	isbn = {978-1-4503-6526-0},
	url = {https://doi.org/10.1145/3209281.3209333},
	doi = {10.1145/3209281.3209333},
	abstract = {Data Ecosystems are socio-technical networks that enable collaboration between autonomous actors such as enterprises, institutions, and individuals. While Data Ecosystems are thus gaining importance, research into Data Ecosystems is still in its infancy stages. The terminology and definitions for Data Ecosystem vary greatly. This diversity imposes a pressing problem for the development of a clear understanding of the new opportunities and emergent challenges in exploiting Data Ecosystems. Accurate definitions are required to get a mutual understanding of what Data Ecosystems involve. Moreover, to the best of our knowledge, a model for describing a Data Ecosystem and its essential concepts has not been proposed yet. In this work, we aim to fill these gaps by reviewing the Data Ecosystem literature, and based on the field literature, we propose a meta-model for describing Data Ecosystems. In particular, the proposed meta-model describes the Data Ecosystem fundamental concepts and their inter-relationships for enabling analysis and description of ecosystems. Especially, the meta-model declares explicitly how all these concepts are related to each other in such holistic view, hence facilitating knowledge creation and management in the ecosystem.},
	booktitle = {Proceedings of the 19th {Annual} {International} {Conference} on {Digital} {Government} {Research}: {Governance} in the {Data} {Age}},
	publisher = {Association for Computing Machinery},
	author = {Oliveira, Marcelo Iury S. and Oliveira, Lairson Emanuel R. A. and Batista, Marlos G. Ribeiro and Lóscio, Bernadette Farias},
	year = {2018},
	note = {event-place: Delft, The Netherlands},
	keywords = {standardization, meta-model, data ecosystem, government data},
}

@inproceedings{guan_research_2024,
	address = {New York, NY, USA},
	series = {{CMNM} '24},
	title = {Research on {Human}-{Computer} {Interaction} {Design} {Standards} in {Artificial} {Intelligence} {Products}},
	isbn = {979-8-4007-0976-0},
	url = {https://doi.org/10.1145/3677779.3677815},
	doi = {10.1145/3677779.3677815},
	abstract = {This article first analyzes the concept and development process of artificial intelligence products, then delves into the application and development of human-computer interaction technology in artificial intelligence products, and then analyzes the standardization research of human-computer interaction design in artificial intelligence products; Finally, the two-dimensional architecture of human-computer interaction design for artificial intelligence products was elaborated, with detailed discussions and planning in terms of technical and value dimensions. The rapid development of artificial intelligence has driven the emergence of a large number of artificial intelligence products, resulting in a fundamental change in the human-computer interaction mode of products and higher requirements for human-computer interaction design. Therefore, how to construct standardized human-computer interaction design patterns in artificial intelligence products in the new era is a key research topic in the development of artificial intelligence products.},
	booktitle = {Proceedings of the {International} {Conference} on {Modeling}, {Natural} {Language} {Processing} and {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Guan, Jing},
	year = {2024},
	note = {event-place: Xi'an, China},
	pages = {220--225},
}

@article{yan_investigating_2024,
	title = {Investigating {Documented} {Privacy} {Changes} in {Android} {OS}},
	volume = {1},
	url = {https://doi.org/10.1145/3660826},
	doi = {10.1145/3660826},
	abstract = {Android has empowered third-party apps to access data and services on mobile devices since its genesis.This involves a wide spectrum of user privacy-sensitive data, such as the device ID and location. In recent years, Android has taken proactive measures to adapt its access control policies for such data, in response to the increasingly strict privacy protection regulations around the world. When each new Android version is released, its privacy changes induced by the version evolution are transparently disclosed, and we refer to them as documented privacy changes (DPCs). Implementing DPCs in Android OS is a non-trivial task, due to not only the dispersed nature of those access control points within the OS, but also the challenges posed by backward compatibility. As a result, whether the actual access control enforcement in the OS implementations aligns with the disclosed DPCs becomes a critical concern. In this work, we conduct the first systematic study on the consistency between the operational behaviors of the OS at runtime and the officially disclosed DPCs. We propose DopCheck, an automatic DPC-driven testing framework equipped with a large language model (LLM) pipeline. It features a serial of analysis to extract the ontology from the privacy change documents written in natural language, and then harnesses the few-shot capability of LLMs to construct test cases for the detection of DPC-compliance issues in OS implementations. We apply DopCheck with the latest versions (10 to 13) of Android Open Source Project (AOSP). Our evaluation involving 79 privacy-sensitive APIs demonstrates that DopCheck can effectively recognize DPCs from Android documentation and generate rigorous test cases. Our study reveals that the status quo of the DPC-compliance issues is concerning, evidenced by 19 bugs identified by DopCheck. Notably, 12 of them are discovered in Android 13 and 6 in Android 10 for the first time, posing more than 35\% Android users to the risk of privacy leakage. Our findings should raise an alert to Android users and app developers on the DPC compliance issues when using or developing an app, and would also underscore the necessity for Google to comprehensively validate the actual implementation against its privacy documentation prior to the OS release.},
	number = {FSE},
	journal = {Proc. ACM Softw. Eng.},
	author = {Yan, Chuan and Meng, Mark Huasong and Xie, Fuman and Bai, Guangdong},
	month = jul,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {testing, privacy, Android, documentation},
}

@inproceedings{barn_towards_2018,
	address = {New York, NY, USA},
	series = {{ICSE}-{SEIS} '18},
	title = {Towards a unified conceptual model for surveillance theories: "we shall meet in the place where there is no darkness" - 1984, george orwell},
	isbn = {978-1-4503-5661-9},
	url = {https://doi.org/10.1145/3183428.3183429},
	doi = {10.1145/3183428.3183429},
	abstract = {The erosion of values such as privacy can be a critical factor in preventing the acceptance of new innovative technology especially in challenging environments such as the criminal justice system. Erosion of privacy happens through either deliberate or inadvertent surveillance. Since Bentham's original liberal project in the 1900s, a literature and a whole study area around theories of surveillance has developed. Increasingly this general body of work has focussed on the role of information technology as a vehicle for surveillance activity. Despite an abundance of knowledge, a unified view of key surveillance concepts that is useful to designers of information systems in preventing or reducing unintended surveillance remains elusive. This paper contributes a conceptual model that synthesises the gamut of surveillance theories as a first step to a theory building effort for use by Information Systems professionals. The model is evaluated using a design science research paradigm using data from both examples of surveillance and a recently completed research project that developed technology for the UK youth justice system.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Society}},
	publisher = {Association for Computing Machinery},
	author = {Barn, Balbir S. and Barn, Ravinder},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {conceptual model, surveillance, privacy, reference model},
	pages = {71--80},
}

@article{bartalesi_using_2023,
	title = {Using {Semantic} {Web} to {Create} and {Explore} an {Index} of {Toponyms} {Cited} in {Medieval} {Geographical} {Works}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3582263},
	doi = {10.1145/3582263},
	abstract = {Western thought in European history was mainly affected by the image of the world created during the Middle Ages and Renaissance. The most popular reason to travel during the Middle Ages was taking a pilgrimage. Jerusalem, Rome, and Santiago de Compostela were the most popular destinations. It is not surprising that a lot of works written by travellers as guides for pilgrims exist. By the beginning of the Renaissance, a more precise image of the world was defined, thanks to the discovery of ancient geographical models, especially the work of Ptolemy. The Italian National Research Project (PRIN) IMAGO — Index Medii Aevi Geographiae Operum — (2020-2023) aims to provide a systematic overview of the medieval and renaissance Latin geographical literature using the Semantic Web technologies and the LOD paradigm. Indeed, until now, this literature has not been studied using digital methods. In particular, this article presents how we formally represented the knowledge about the toponyms, or place names, in the IMAGO ontology. To maximise the interoperability, we developed the IMAGO ontology as an extension of two reference vocabularies: the CIDOC CRM and its extension FRBRoo, including its in-progress reformulation, LRMoo. Furthermore, we used Wikidata as reference knowledge base. As case study, we chose to represent the knowledge related to the toponyms cited by the Italian poet Dante Alighieri in his Latin works. We carried out a first experiment for visualising the knowledge about these toponyms on a map and in the form of tables and CSV files.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Bartalesi, Valentina and Pratelli, Nicolo’ and Lenzi, Emanuele and Pontari, Paolo},
	month = apr,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Semantic Web, ontology, Wikidata, Linked Open Data, CIDOC CRM, Dante Alighieri, toponyms},
}

@inproceedings{thalheimer_microservice-driven_2020,
	address = {New York, NY, USA},
	series = {{SBSI} '20},
	title = {A {Microservice}-driven {Collaborative} {Agent} in {Virtual} {Learning} {Environments}: {A} {Role} {Model} for a {Tracing} {Agent}},
	isbn = {978-1-4503-8873-3},
	url = {https://doi.org/10.1145/3411564.3411630},
	doi = {10.1145/3411564.3411630},
	abstract = {Currently, distance learning comprises almost half of students enrolled in undergraduate courses in Brazil. However, the dropout rate of this modality is over 50\%, and only 22\% of students complete the courses [27]. Despite technological advances and good acceptance of this modality, research indicates that the lack of involvement in a virtual community can lead to feelings of loneliness, low self-esteem, isolation and desmotivation. There is evidence that these feelings are among the main factors responsible for the low performance and high evasion rate. Virtual Learning Environments (VLE) handles a large volume of student interaction data. In this context, it is important to create mechanisms to maintain and manage a data structure to facilitate the processes of transforming data into information and knowledge. This paper aims to present a tracing agent responsible for maintaining and managing the data structure in VLE. The agent acts in the context of a microservice-oriented multi-agent system, interacting and collaborating with other agents in order to improve interaction and decision-making processes. This work becomes original and at the same time innovative, presenting an unprecedented combination of technologies and techniques in the context of VLEs.},
	booktitle = {Proceedings of the {XVI} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Thalheimer, Jéferson Miguel and Filho, Aluizio Haendchen and Briks, Fabio Julio Pereira and Ribeiro, Rafael Castaneda and Concatto, Fernando and Viecelli, Angélica Karize},
	year = {2020},
	note = {event-place: São Bernardo do Campo, Brazil},
	keywords = {Multiagent System, Tracing Agent, Virtual Learning Environment},
}

@inproceedings{baeva_using_2019,
	address = {New York, NY, USA},
	series = {{CompSysTech} '19},
	title = {Using {Lindenmayer} {Systems} {For} {Generative} {Modeling} {Of} {Graphic} {Concepts}, {Set} {In} {Elements} {Of} {Bulgarian} {Folklore} {Embroidery}},
	isbn = {978-1-4503-7149-0},
	url = {https://doi.org/10.1145/3345252.3345295},
	doi = {10.1145/3345252.3345295},
	abstract = {L-systems as a type of fractal model are one of the most widely used examples of the integration of mathematics and information technology. They can be seen in many elements created with the means of computer graphics, and have become a new tool that is relevant for modeling in biology, geology, and other natural sciences. Along with their applications in advanced technology science, L-systems also refer to archaic models that are surprisingly common in traditional designs of different ethnicities, and some of their basic concepts are also fundamental to systems of knowledge about the Bulgarian embroidery.This article reviews and analyzes the generative characteristics of some graphic motifs specific to Bulgarian folklore. The applicability of the study consists in finding tools for a general description of these motifs, which can easily refer us to other elements of human creativity - such as speech or music, and provoke the discovery of relationships and relationships between them.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Computer} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Baeva, Desislava},
	year = {2019},
	note = {event-place: Ruse, Bulgaria},
	keywords = {embroidery simulation, generative art, L-systems},
	pages = {234--239},
}

@inproceedings{aili_relational_2025,
	address = {New York, NY, USA},
	series = {{CSAI} '24},
	title = {Relational {Representation} {Augmented} {Graph} {Attention} {Network} for {Knowledge} {Graph} {Completion}},
	isbn = {979-8-4007-1818-2},
	url = {https://doi.org/10.1145/3709026.3709105},
	doi = {10.1145/3709026.3709105},
	abstract = {Knowledge Graph Completion (KGC) is a popular topic in knowledge graph construction and related applications, aiming to complete the structure of knowledge graphs by predicting missing entities or relations and mining unknown facts in the knowledge graph. In the KGC task, Graph Neural Network (GNN)-based methods have achieved remarkable results due to their advantage of effectively capturing complex relations among entities and generating more accurate and rich entity representations by aggregating information from neighbouring nodes. These methods mainly focus on the representation of entities, and the representation of relations is obtained using simple dimensional transformations or initial embeddings. This treatment ignores the diversity and complex semantics of relations and restricts the efficiency of the model in utilizing relational information in the reasoning process. In this work, we propose the Relational Representation Augmented Graph Attention Network (RRA-GAT), which effectively identifies and weights neighbouring relations that actually contribute to the target relation by filtering out irrelevant information through an attention function based on the information and spatial domain. Furthermore, we capture complex patterns and features in the relational embedding by means of a feed-forward network consisting of a series of linear transformations and nonlinear activation functions. Experiments demonstrate the very advanced performance of RRA-GAT on the link prediction task on standard datasets FB15k-237 and WN18RR (e.g., improved the MRR metric on the WN18RR dataset by 7.8\% relative improvement).},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Aili, Elyar and Yilahun, Hankiz and Imam, Seyyare and Hamdulla, Askar},
	year = {2025},
	keywords = {Knowledge graph completion, Graph neural networks, Knowledge graph embedding},
	pages = {449--455},
}

@inproceedings{martini_computational_2019,
	address = {New York, NY, USA},
	series = {{WebMedia} '19},
	title = {A computational model for ubiquitous intelligent services in indoor agriculture},
	isbn = {978-1-4503-6763-9},
	url = {https://doi.org/10.1145/3323503.3360641},
	doi = {10.1145/3323503.3360641},
	abstract = {The application of ubiquitous computing has increased in recent years, especially due to the development of technologies such as mobile computing, accurate sensors and specific protocols for an IoT. One of the trends in this research area is the use of context awareness. In agriculture, the context can be related to the environment, for example, the conditions found inside a greenhouse. Recently a series of studies proposed the use of sensors to monitor the production or the use of cameras to obtain crop information, providing data, reminders and alerts to farmers. This paper proposes a computational model for Indoor Agriculture called IndoorPlant that uses the contexts history analysis to provide intelligent services such as predict the productivity, indicate the problems that the crop may suffer, give suggestions for improvements in the parameters in the greenhouse, among others. IndoorPlant was tested on cucumber prediction using simulated data that was approved by three farmers with more than 10 years of experience each. The results obtained in the prediction of cucumber, with a coefficient of determination (R2) of 0.9912 for root mean square error (RMSE) of 8,06 units of cucumber.},
	booktitle = {Proceedings of the 25th {Brazillian} {Symposium} on {Multimedia} and the {Web}},
	publisher = {Association for Computing Machinery},
	author = {Martini, Bruno G. and Helfer, Gilson A. and Barbosa, Jorge L. V. and Silva, Marcio R. da and de Figueiredo, Rodrigo M. and Modolo, Regina C. E. and Yamin, Adenauer C.},
	year = {2019},
	note = {event-place: Rio de Janeiro, Brazil},
	keywords = {context awareness, computing in agriculture, indoor agriculture, prediction in agriculture},
	pages = {497--500},
}

@inproceedings{sun_conmask-gnn_2025,
	address = {New York, NY, USA},
	series = {{ICTCE} '24},
	title = {{ConMask}-{GNN}: {Leveraging} {Graph} {Neural} {Networks} for {Enhanced} {Knowledge} {Graph} {Completion} in {Static} {Contexts}},
	isbn = {979-8-4007-0963-0},
	url = {https://doi.org/10.1145/3705391.3705402},
	doi = {10.1145/3705391.3705402},
	abstract = {With the rapid development of knowledge graphs (KG), enhancing the performance of knowledge graph completion has become a critical research challenge. Traditional methods for KG completion rely heavily on entity and relation embeddings, often neglecting the synergistic interaction between graph structures and textual information. To address this issue, this paper proposes a novel approach that combines Graph Neural Networks (GNN) with the ConMask model to improve the effectiveness of static knowledge graph completion tasks. Specifically, the GNN is employed to generate entity embeddings based on graph structures, while ConMask extracts key information from text using a relation-dependent content masking mechanism. The model further integrates the embeddings through a multi-head attention mechanism at the embedding layer, fusing the GNN-based entity embeddings with text embeddings. Additionally, the loss function incorporates contrastive learning, which enhances the representational capacity of the embeddings. Experiments are conducted on standard datasets FB15k-237 and WN18RR, evaluated with metrics such as Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10. The results demonstrate that the proposed model significantly outperforms existing baseline methods in prediction accuracy. Finally, ablation studies validate the critical role of combining GNN and ConMask in improving model performance.},
	booktitle = {Proceedings of the 2024 6th {International} {Conference} on {Telecommunications} and {Communication} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Yuyuan and Li, Tongyan and Chen, Xingyu and Tan, Hao},
	year = {2025},
	keywords = {Knowledge graph completion, ConMask, embedding fusion, Graph Neural Networks (GNN), multi-head attention mechanism},
	pages = {65--69},
}

@inproceedings{da_silva_improving_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {Improving self-adaptive systems conceptual modeling},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167271},
	doi = {10.1145/3167132.3167271},
	abstract = {Self-adaptive Systems (SaSs) operate under uncertainty conditions and have intrinsic properties that have posed some challenges for requirements analysis. Conceptual modeling is useful to requirements analysis because it aids to understand the situation in which a problem occurs. SaSs conceptual modeling is a non-trivial activity because it is necessary to deal with requirements uncertainty, contextual changes, and behavior adaptation. Since conceptual models are built by humans, their quality heavily depends on the humans expertise, which is not a good software engineering practice. Regarding SaSs, the exposure to quality risks increases because of intrinsic characteristic in this class of system. In this paper, we present a SaSs conceptual modeling approach composed of a metamodel and a modeling process. The process defines how to instantiate the metamodel from requirements specifications to create SaSs conceptual models. We performed a controlled experiment with subjects to evaluate our modeling approach effectiveness. As the outcome, we found that our approach had a better performance than an ad hoc approach. The contribution of this paper is a well-defined approach for guiding SaSs conceptual modeling, supported by evidence of its effectiveness by means of an empirical experiment.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {da Silva, João Pablo S. and Ecar, Miguel and Pimenta, Marcelo S. and Kepler, Fabio Natanael and Guedes, Gilleanes T. A. and Betemps, Carlos Michel},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {conceptual modeling, self-adaptive system, requirements analysis, empirical experiment},
	pages = {1292--1299},
}

@inproceedings{kapugama_geeganage_concept_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {Concept {Embedded} {Topic} {Modeling} {Technique}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3186571},
	doi = {10.1145/3184558.3186571},
	abstract = {Text contents are overloaded with the digitization of the data and new contents are transmitted through many sources by generating a large volume of information, which spreads all over the world through different communication media. Therefore, text data is available everywhere and reading, understanding and analysing the text data has become a main activity in daily routine. With the increment of the volume and the variety of information, organizing and searching, the required information has become vital. Topic modelling is the state of the art for information organization, understanding and extracting the content. Most of the prevailing topic models use the probabilistic approaches and consider the frequency and the co-occurrence to discover the topics from collections of documents. The proposed research aims to address the existing problems of topic modeling by introducing a concept embedded topic model which generates the most relevant and meaningful topics by understanding the content. The research includes approaches to understand the semantic elements from the content, domain identification of concepts and provide most suitable topics without getting the number of topics from the user beforehand. Capturing the semantics of document collections and generating the most related set of topics according to the actual meaning will be the significance of this research.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Kapugama Geeganage, Dakshi Tharanga},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {semantics, topic modeling, concepts},
	pages = {831--835},
}

@inproceedings{seok_implementing_2020,
	address = {New York, NY, USA},
	series = {{AICCC} '19},
	title = {Implementing {A} {Semantic}-based {loT} {Mashup} {Service}},
	isbn = {978-1-4503-7263-3},
	url = {https://doi.org/10.1145/3375959.3375975},
	doi = {10.1145/3375959.3375975},
	abstract = {The semantic information provided through the semantic-based IoT system will produce new high value-added products that are completely different from what we have known and experienced. From this point of view, a key issue of current IoT technology and applications is the development of an intelligent IoT platform architecture. Our proposed system collects the IoT data of the sensors from the cloud computer, converts them into RDF, and annotates them with semantics. The converted semantic data are shared and utilized through the ontology repository. We use KT's IoTMakers as a cloud computing environment, and the ontology repository uses Jena's Fuseki server to express SPARQL query results on the Web using Daum Map API and HighCharts API. This gives people the opportunity to access the semantic IoT mash-up service easily and has various application possibilities.},
	booktitle = {Proceedings of the 2019 2nd {Artificial} {Intelligence} and {Cloud} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Seok, Hyunseung and Nam, Sunghyun and Lee, Yongju},
	year = {2020},
	note = {event-place: Kobe, Japan},
	keywords = {IoT, Ontology Modeling, Responsive Web Design, Semantic-based Mashup Service},
	pages = {162--167},
}

@inproceedings{zhu_deep_2024,
	address = {New York, NY, USA},
	series = {{BDE} '24},
	title = {Deep {Learning}-{Based} {Knowledge} {Graph} {Construction} for {Three}-{Dimensional} {Design} {Specification} of {Power} {Plant} {Engineering}},
	isbn = {979-8-4007-1785-7},
	url = {https://doi.org/10.1145/3688574.3688591},
	doi = {10.1145/3688574.3688591},
	abstract = {With the rapid development of natural language processing technology, the field of knowledge engineering is gradually advancing towards a novel phase of knowledge expression and orderly data storage. The purpose of this paper is to explore the construction method of knowledge graph of 3D design specification for power plant engineering based on deep learning, and to realize the extraction of key information in the design specification and the automatic construction of knowledge graph by integrating the deep learning models such as BERT, Bi-LSTM, CRF, and so on.},
	booktitle = {Proceedings of the 2024 6th {International} {Conference} on {Big} {Data} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Jiangtao and Wang, Tiankun and Ma, Xinru and Zuo, Chao and Zhao, Junjie and Luo, Quan},
	year = {2024},
	note = {event-place: Xining, China},
	keywords = {Knowledge graph, Building information modeling, Deep learning, Power plant engineering, Three-dimensional design specification},
	pages = {118--125},
}

@inproceedings{nguyen_dozen_2021,
	address = {New York, NY, USA},
	series = {{SIGIR} '21},
	title = {{DOZEN}: {Cross}-{Domain} {Zero} {Shot} {Named} {Entity} {Recognition} with {Knowledge} {Graph}},
	isbn = {978-1-4503-8037-9},
	url = {https://doi.org/10.1145/3404835.3463113},
	doi = {10.1145/3404835.3463113},
	abstract = {With the new developments of natural language processing, increasing attention has been given to the task of Named Entity Recognition (NER). However, the vast majority of work focus on a small number of large-scale annotated datasets with a limited number of entities such as person, location and organization. While other datasets have been introduced with domain-specific entities, the smaller size of these largely limits the applicability of state-of-the-art deep models. Even if there are promising new approaches for performing zero-shot learning (ZSL), they are not designed for a cross-domain settings. We propose Cross Domain Zero Shot Named Entity Recognition with Knowledge Graph (DOZEN), which learns the relations between entities across different domains from an existing ontology of external knowledge and a set of analogies linking entities and domains. Experiments performed on both large scale and domain-specific datasets indicate that DOZEN is the most suitable option to extracts unseen entities in a target dataset from a different domain.},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Hoang-Van and Gelli, Francesco and Poria, Soujanya},
	year = {2021},
	note = {event-place: Virtual Event, Canada},
	keywords = {natural language processing, knowledge graph, named entity recognition, zero-shot learning, cross-domain machine learning},
	pages = {1642--1646},
}

@inproceedings{wei_deep_2022,
	address = {New York, NY, USA},
	series = {{BCB} '22},
	title = {Deep sequence representation learning for predicting human proteins with liquid-liquid phase separation propensity and synaptic functions},
	isbn = {978-1-4503-9386-7},
	url = {https://doi.org/10.1145/3535508.3545550},
	doi = {10.1145/3535508.3545550},
	abstract = {With advancements in next-generation sequencing techniques, the whole protein sequence repertoire has increased to a great extent. In the meantime, deep learning techniques have promoted the development of computational methods to interpret large-scale proteomic data and facilitate functional studies of proteins. Inferring properties from protein amino acid sequences has been a long-standing problem in Bioinformatics. Extensive studies have successfully applied natural language processing (NLP) techniques for the representation learning of protein sequences. In this paper, we applied the deep sequence model - UDSMProt, to fine-tune and evaluate two protein prediction tasks: (1) predict proteins with liquid-liquid phase separation propensity and (2) predict synaptic proteins. Our results have shown that, without prior domain knowledge and only based on protein sequences, the fine-tuned language models achieved high classification accuracies and outperformed baseline models using compositional k-mer features in both tasks. Hence, it is promising to apply the protein language model to some learning tasks and the fine-tuned models can be used to predict protein candidates for biological studies.},
	booktitle = {Proceedings of the 13th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Wei, Anqi and Wang, Liangjiang},
	year = {2022},
	note = {event-place: Northbrook, Illinois},
	keywords = {protein language model, liquid-liquid phase separation, synaptic proteins},
}

@inproceedings{hamdy_topic_2018,
	address = {New York, NY, USA},
	series = {{ICGDA} '18},
	title = {Topic modelling for automatic selection of software design patterns},
	isbn = {978-1-4503-6445-4},
	url = {https://doi.org/10.1145/3220228.3220263},
	doi = {10.1145/3220228.3220263},
	abstract = {Design pattern is a high-quality and reusable solution to a recurring software design problem. It is considered an important concept in the software engineering field due to its ability to enhance some of the quality attributes of the software systems including maintainability and extensibility. However, novice developers need to be provided by a tool to assist them in selecting the fit design pattern to solve a design problem. The paper proposes a novel approach for the automatic selection of the fit design pattern. This approach is based on using Latent Dirichlet Allocation (LDA) topic model. The topic is a set of words that often appear together. LDA is able to relate words with similar meaning and to differentiate between uses of words with multiple meanings. In this paper LDA is used to analyze the textual descriptions of design patterns and extract the topics then discover the similarity between the target problem scenario and the collection of patterns using Improved Sqrt-Cosine similarity measure (ISCS). The proposed approach was evaluated using Gang of four design patterns. The experimental results showed that the proposed approach outperforms approach based on the traditional vector space model of Unigrams.},
	booktitle = {Proceedings of the {International} {Conference} on {Geoinformatics} and {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Hamdy, Abeer and Elsayed, Mohamed},
	year = {2018},
	note = {event-place: Prague, Czech Republic},
	keywords = {text mining, information retrieval, topic modelling, design pattern selection, DP recommendation, gang of four, LDA and vector space model},
	pages = {41--46},
}

@inproceedings{merkle_cloud-based_2020,
	address = {New York, NY, USA},
	series = {{ISCSIC} 2019},
	title = {Cloud-{Based} {Battery} {Digital} {Twin} {Middleware} {Using} {Model}-{Based} {Development}},
	isbn = {978-1-4503-7661-7},
	url = {https://doi.org/10.1145/3386164.3387296},
	doi = {10.1145/3386164.3387296},
	abstract = {Following the trends of electrification, the energy storage of vehicles is gaining importance as the most expensive part of an electric car. Since lithium-ion batteries are perishable goods and underlie e. g. aging effects, environmental and operating conditions during manufacturing and car usage need close supervision. With regard to the paradigm of digital twins, data from various life cycle phases needs to be collected and processed to improve the general quality of the system. To achieve this complex task, a suitable framework is needed in order to operate the fleet of digital twins during manufacturing processes, the automotive usage and a potential second life. Based on a literature review, we formulate requirements for a digital twin framework in the field of battery systems. We propose a framework to develop and operate a fleet of digital twins during all life cycle phases. Results feature a case study in which we implement the stated framework in a cloud-computing environment using early stages of battery system production as test a bed. With the help of a self-discharge model of li-ion cells, the system can estimate the SOC of battery modules and provide this information to the arrival testing procedures.},
	booktitle = {Proceedings of the 2019 3rd {International} {Symposium} on {Computer} {Science} and {Intelligent} {Control}},
	publisher = {Association for Computing Machinery},
	author = {Merkle, Lukas},
	year = {2020},
	note = {event-place: Amsterdam, Netherlands},
	keywords = {Digital Twin, IoT, Battery System, Control Middleware, Self-Discharge},
}

@article{ding_mabert_2023,
	title = {{MABERT}: {Mask}-{Attention}-{Based} {BERT} for {Chinese} {Event} {Extraction}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3597455},
	doi = {10.1145/3597455},
	abstract = {Event extraction is an essential but challenging task in information extraction. This task has considerably benefited from pre-trained language models, such as BERT. However, when it comes to the trigger-word mismatch problem in languages without natural delimiters, existing methods ignore the complement of lexical information to BERT. In addition, the inherent multi-role noise problem could limit the performance of methods when one sentence contains multiple events. In this article, we propose a Mask-Attention-based BERT (MABERT) framework for Chinese event extraction to address the above problems. Firstly, in order to avoid trigger-word mismatch and integrate lexical features into BERT layers directly, a mask-attention-based transformer augmented with two mask matrices is devised to replace the original one in BERT. By the mask-attention-based transformer, the character sequence interacts with external lexical semantics sufficiently and keeps its structure information at the same time. Moreover, against the multi-role noise problem, we make use of event type information from representation and classification, two aspects to enrich entity features, where type markers and event-schema-based mask matrix are proposed. Experimental results on the widely used ACE2005 dataset show the effectiveness of our proposed MABERT on Chinese event extraction task compared with other state-of-the-art methods.},
	number = {7},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Ding, Ling and Chen, Xiaojun and Wei, Jian and Xiang, Yang},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Language model, Semantics, Event extraction, Event ontology, event ontology, event type markers, mask-attention-based transformer, Classification (of information), Event Types, Events extractions, Additional key word and phrasesevent extraction, Event type marker, Key words, Mask-attention-based transformer, matrix, Noise pollution, Noise problems},
	annote = {Cited by: 8},
}

@inproceedings{nguyen_mofuzz_2021,
	address = {New York, NY, USA},
	series = {{ASE} '20},
	title = {{MoFuzz}: a fuzzer suite for testing model-driven software engineering tools},
	isbn = {978-1-4503-6768-4},
	url = {https://doi.org/10.1145/3324884.3416668},
	doi = {10.1145/3324884.3416668},
	abstract = {Fuzzing or fuzz testing is an established technique that aims to discover unexpected program behavior (e.g., bugs, security vulnerabilities, or crashes) by feeding automatically generated data into a program under test. However, the application of fuzzing to test Model-Driven Software Engineering (MDSE) tools is still limited because of the difficulty of existing fuzzers to provide structured, well-typed inputs, namely models that conform to typing and consistency constraints induced by a given meta-model and underlying modeling framework. By drawing from recent advances on both fuzz testing and automated model generation, we present three different approaches for fuzzing MDSE tools: A graph grammar-based fuzzer and two variants of a coverage-guided mutation-based fuzzer working with different sets of model mutation operators. Our evaluation on a set of real-world MDSE tools shows that our approaches can outperform both standard fuzzers and model generators w.r.t. their fuzzing capabilities. Moreover, we found that each of our approaches comes with its own strengths and weaknesses in terms of fault finding capabilities and the ability to cover different aspects of the system under test. Thus the approaches complement each other, forming a fuzzer suite for testing MDSE tools.},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Hoang Lam and Nassar, Nebras and Kehrer, Timo and Grunske, Lars},
	year = {2021},
	note = {event-place: Virtual Event, Australia},
	keywords = {model-driven software engineering, automated model generation, eclipse modeling framework, fuzzing, modeling tools},
	pages = {1103--1115},
}

@inproceedings{miller_astrolabe_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Astrolabe: {Visual} {Graph} {Database} {Queries} with {Tabular} {Output}},
	isbn = {979-8-4007-0124-5},
	url = {https://doi.org/10.1145/3583780.3615992},
	doi = {10.1145/3583780.3615992},
	abstract = {Graph databases are an established solution for large, highly connected datasets. One challenge associated with deploying graph databases in industrial settings is usability. Typically, developers interact with graph databases through queries in languages such as Cypher or GraphQL. Many end-users, analysts, and administrators are not familiar with these specialized languages. Additionally, these queries return hierarchical data in formats such as JSON (JavaScript Object Notation) or XML (Extensible Markup Language). Additional scripts and interfaces are needed to convert hierarchical data into more easily digested tables. To overcome these challenges, each graph database use-case typically involves significant custom software to explore, view, and export data.We introduce Astrolabe, a generalized interface that addresses the challenges of querying graph databases. In Astrolabe, queries are constructed visually, so users do not need to learn new graph query languages. Results are returned as tables, which can be easily digested by end users or down-stream applications. Astrolabe was designed to function with arbitrary graph databases, so schema definition is not required. Astrolabe revolutionizes graph exploration and querying by allowing graph databases to be viewed as tables, without the need for custom software adapters.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Miller, Michael},
	year = {2023},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {knowledge representation, knowledge graph, data visualization, graph database, graphical user interface, interactive information retrieval, data exploration, no-code, query generation},
	pages = {5248},
}

@article{ferry_cloudmf_2018,
	title = {{CloudMF}: {Model}-{Driven} {Management} of {Multi}-{Cloud} {Applications}},
	volume = {18},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3125621},
	doi = {10.1145/3125621},
	abstract = {While the number of cloud solutions is continuously increasing, the development and operation of large-scale and distributed cloud applications are still challenging. A major challenge is the lack of interoperability between the existing cloud solutions, which increases the complexity of maintaining and evolving complex applications potentially deployed across multiple cloud infrastructures and platforms. In this article, we show how the Cloud Modelling Framework leverages model-driven engineering and supports the DevOps ideas to tame this complexity by providing: (i) a domain-specific language for specifying the provisioning and deployment of multi-cloud applications, and (ii) a models@run-time environment for their continuous provisioning, deployment, and adaptation.},
	number = {2},
	journal = {ACM Trans. Internet Technol.},
	author = {Ferry, Nicolas and Chauvel, Franck and Song, Hui and Rossini, Alessandro and Lushpenko, Maksym and Solberg, Arnor},
	month = jan,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cloud computing, model-driven engineering, DevOps, models@run-time, multi-cloud},
}

@article{duong_fact-checking_2023,
	title = {Fact-checking {Vietnamese} {Information} {Using} {Knowledge} {Graph}, {Datalog}, and {KG}-{BERT}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3624557},
	doi = {10.1145/3624557},
	abstract = {In the era of digital information, ensuring the accuracy and reliability of information is crucial, making fact-checking a vital process. Currently, English fact-checking has thrived due to various language processing tools and ample datasets. However, the same cannot be said for Vietnamese fact-checking, which faces significant challenges due to the lack of such resources. To address these challenges, we propose a model for checking Vietnamese facts by synthesizing three popular technologies: Knowledge Graph (KG), Datalog, and KG-BERT. The KG serves as the foundation for the fact-checking process, containing a dataset of Vietnamese information. Datalog, a logical programming language, is used with inference rules to complete the knowledge within the Vietnamese KG. KG-BERT, a Deep Learning (DL) model, is then trained on this KG to rapidly and accurately classify information that needs fact-checking. Furthermore, to put Vietnamese complex sentences into the fact-checking model, we present a solution for extracting triples from these sentences. This approach also contributes significantly to the ease of constructing foundational datasets for the Vietnamese KG. To evaluate the model's performance, we create a Vietnamese dataset comprising 130,190 samples to populate the KG. Using Datalog, we enrich this graph with additional knowledge. The KG is then utilized to train the KG-BERT model, achieving an impressive accuracy of 95\%. Our proposed solution shows great promise for fact-checking Vietnamese information and has the potential to contribute to the development of fact-checking tools and techniques for other languages. Overall, this research makes a significant contribution to the field of data science by providing an accurate solution for fact-checking information in Vietnamese language contexts.},
	number = {10},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Duong, Huong T. and Ho, Van H. and Do, Phuc},
	month = oct,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {knowledge graph, datalog, Fact checking, inference rule, KG-BERT},
}

@inproceedings{wu_understanding_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Understanding and {Empowering} {Intelligence} {Analysts}: {User}-{Centered} {Design} for {Deepfake} {Detection} {Tools}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713711},
	doi = {10.1145/3706598.3713711},
	abstract = {Intelligence analysts must quickly and accurately examine and report on information in multiple modalities, including video, audio, and images. With the rise of Generative AI and deepfakes, analysts face unprecedented challenges, and require effective, reliable, and explainable media detection and analysis tools. This work explores analysts’ requirements for deepfake detection tools and explainability features. From a study of 30 practitioners from the United States Intelligence Community, we identified the need for a comprehensive and explainable solution that incorporates a wide variety of methods and supports the production of intelligence reports. In response, we propose a design for an analyst-centered tool, and introduce a digital media forensics ontology to support analysts’ interactions with the tool and understanding of its results. We conducted a study grounded in work-related tasks as an initial evaluation of this approach, and report on its potential to assist analysts and areas for improvement in future work.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Y. Kelly and Sohrawardi, Saniat Javid and Gerstner, Candice R. and Wright, Matthew},
	year = {2025},
	keywords = {Ontology, Explainability, Deepfake, Intelligence Community, Qualitative Studies},
}

@inproceedings{sun_timelinekgqa_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{TimelineKGQA}: {A} {Comprehensive} {Question}-{Answer} {Pair} {Generator} for {Temporal} {Knowledge} {Graphs}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715308},
	doi = {10.1145/3701716.3715308},
	abstract = {Question answering over temporal knowledge graphs (TKGs) is crucial for understanding evolving facts and relationships, yet its development is hindered by limited datasets and difficulties in generating custom QA pairs. We propose a novel categorization framework based on timeline-context relationships, along with TimelineKGQA, a universal temporal QA generator applicable to any TKGs. The code is available at: https://github.com/PascalSun/TimelineKGQA as an open source Python package.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Sun, Qiang and Li, Sirui and Huynh, Du and Reynolds, Mark and Liu, Wei},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {knowledge graph, question answering, temporal knowledge graph},
	pages = {797--800},
}

@inproceedings{wang_geospatial_2023,
	address = {New York, NY, USA},
	series = {{SIGSPATIAL} '23},
	title = {Geospatial {Knowledge} {Hypercube}},
	isbn = {979-8-4007-0168-9},
	url = {https://doi.org/10.1145/3589132.3625629},
	doi = {10.1145/3589132.3625629},
	abstract = {Today a tremendous amount of geospatial knowledge is hidden in massive volumes of text data. To facilitate flexible and powerful geospatial analysis and applications, we introduce a new architecture: geospatial knowledge hypercube, a multi-scale, multidimensional knowledge structure that integrates information from geospatial dimensions, thematic themes and diverse application semantics, extracted and computed from spatial-related text data. To construct such a knowledge hypercube, weakly supervised language models are leveraged for automatic, dynamic and incremental extraction of heterogeneous geospatial data, thematic themes, latent connections and relationships, and application semantics, through combining a variety of information from unstructured text, structured tables, and maps. The hypercube lays a foundation for many knowledge discovery and in-depth spatial analysis, and other advanced applications. We have deployed a prototype web application of proposed geospatial knowledge hypercube for public access at: https://hcwebapp.cigi.illinois.edu/.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Zhaonan and Jin, Bowen and Hu, Wei and Jiang, Minhao and Kang, Seungyeon and Li, Zhiyuan and Zhou, Sizhe and Han, Jiawei and Wang, Shaowen},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {geographic information retrieval, knowledge hypercube, weakly-supervised text classification},
}

@article{guzman_framework_2025,
	title = {A {Framework} for {Efficient} {Semantic} {Elicitation} of {EU}-{Wide} {Evidence} for {Public} {Services}},
	url = {https://doi.org/10.1145/3757064},
	doi = {10.1145/3757064},
	abstract = {Digital implementation of the Once-Only Principle (OOP) reduces the administrative burden of accessing public services and enhances public administration performance. It also facilitates the mobility of citizens and businesses within the European Union through legal and effective EU-wide evidence. However, despite the existence of several semantic standards, a lack of an ontology for EU-wide evidence persists, primarily due to the low level of EU harmonisation of information that may prove compliance with procedural requirements. Building EU-wide evidence involves several cross-border communities of practice, composed of experts with different technical, legal, organisational, semantic, idiomatic and cultural backgrounds, spanning various public administration levels and sectors. This diversity introduces a range of cross-border, socio-technical challenges that have not been directly addressed in existing literature or by European projects and initiatives. Based on the lessons learned from the Digital Europe for All (DE4A) large-scale pilot project, the DE4A EU-wide OOP Semantic Elicitation Framework (DEOSEF) is proposed to guide business and semantic experts in the elicitation of EU-wide evidence for target public services, through a collaborative, context-aware, and iterative agile process involving diverse communities of practice. This paper presents DEOSEF as a foundational step for future initiatives involving a combined semantic elicitation-creation of EU-wide evidence.},
	journal = {Digit. Gov.: Res. Pract.},
	author = {Guzmán, Ana Rosa and Karunaratne, Thashmee},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cross-border interoperability, Digital public services, Once-Only principle},
	annote = {Just Accepted},
}

@inproceedings{xue_word_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Word {Embedding} based {Heterogeneous} {Entity} {Matching} on {Web} of {Things}},
	isbn = {978-1-4503-9130-6},
	url = {https://doi.org/10.1145/3487553.3524704},
	doi = {10.1145/3487553.3524704},
	abstract = {Web of Things (WoT) is capable of promoting the knowledge discovery and address interoperability problems of diverse Internet of Things (IoT) applications. However, due to the dynamic and diverse features of data entities on WoT, the heterogeneous entity matching has become arguably the greatest “new frontier” for WoT advancements. Currently, the data entities and the corresponding knowledge on WoT are generally modelled with the ontology, and therefore, matching heterogeneous data entities on WoT can be converted to the problem of matching ontologies. Ontology matching is a complex cognitive process, it is usually initially done manually by domain experts. To effectively distinguish the heterogeneous entities and determine high-quality ontology alignment, this work proposes a word embedding based matching technique. Our approach models the word’s semantic in the vector space, and use two vectors’ cosine angle to measure the corresponding words’ similarity. In addition, the word embedding approach does not depend on a specific knowledge base and retain the rich semantic information of words, which makes our proposal more robust. The experiment uses Ontology Alignment Evaluation Initiative (OAEI)’s benchmark for testing, and the experimental results show that our approach outperforms other advanced matching methods.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Xue, Xingsi and Guo, Jianhua},
	year = {2022},
	note = {event-place: Virtual Event, Lyon, France},
	keywords = {Ontology Matching, Web of Things, Word Embedding},
	pages = {941--947},
}

@inproceedings{saba_contribution_2018,
	address = {New York, NY, USA},
	series = {{ICSENT} 2018},
	title = {Contribution to the development of an energy management solution in a green smart home ({EMSGSH})},
	isbn = {978-1-4503-6101-9},
	url = {https://doi.org/10.1145/3330089.3330101},
	doi = {10.1145/3330089.3330101},
	abstract = {This document offers a smart solution for managing green energy for a home (EMSGSH). It offers services such as optimizing the energy consumed while assuming green energy available at home. However, the random nature of this type of energy in the power generation, we require using a hybrid energy system (HES) accompanied by energy storage in the form of a battery. The home is considered a distributed system, it is composed of a set of elements that are geographically distributed and in permanent interaction. Following all its features, we chose two approaches for this solution, the multi-agent systems (MAS) for the control of the elements and the domain ontology to ensure a good formal representation of the data associated with the system. We propose a master-slave architecture. A slave agent is proposed for each device to control its local consumption and a single master agent is proposed to control all agents. The objective is to optimize the use of green energy and minimize consumption costs by exploiting the offers of electric power suppliers. The last part of this work was reserved to present the agents (tasks and responsibilities, interactions ...) and to model the agent's society using a unified modeling language (UML), also the ontology elements are presented and edited in the Protégé software.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Software} {Engineering} and {New} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Saba, Djamel and Maouedj, Rachid and Berbaoui, Brahim},
	year = {2018},
	note = {event-place: Hammamet, Tunisia},
	keywords = {Ontology, Decision-making, Unified modeling language, Smart home, Energy efficient, Distributed systems, Green energy, Hybrid energy systems, Multi agent systems, Protégé software},
}

@inproceedings{yang_integrating_2024,
	address = {New York, NY, USA},
	series = {{IECT} '24},
	title = {Integrating {AI} with {Pedagogies}: {Drama}, {Multimodal} and the {Production}-oriented {Approach}- a {Study} {Based} on the 6th {SFLEP} {Intercultural} {Competence} {Contest}},
	isbn = {979-8-4007-0992-0},
	url = {https://doi.org/10.1145/3687311.3687339},
	doi = {10.1145/3687311.3687339},
	abstract = {Cultural studies have garnered significant attention in China for many decades, and the cultivation of intercultural competence within the academic sphere has been meticulously developed, particularly within the university context. Although intercultural competence is inherently multidisciplinary, its cultivation is predominantly integrated into the pedagogy of foreign language instruction. The discourse surrounding intercultural communication is mainly led by educators and scholars in the field of foreign language studies. The SFLEP Intercultural Competence Contest comprises three pivotal tasks: the development of intercultural case studies, scenario analysis, and the narration of Chinese stories. The rapid development of AI has opened new avenues in the field of education, particularly in language learning. The integration of AI with traditional pedagogies like drama, multimodal learning, and the production-oriented approach has been observed to enrich the learning experience and improve intercultural competence. A thorough examination of the 6th iteration of the contest provides the foundation for this exploration. The study not only highlights the potential of AI integrated approaches but also underscores their significant relevance in enhancing scaffolding techniques in foreign language education.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Intelligent} {Education} and {Computer} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Xin and Zhao, Fengjuan},
	year = {2024},
	note = {event-place: Guilin, China},
	pages = {151--157},
}

@article{jian_adaptive_2025,
	title = {Adaptive {Modality} {Interaction} {Transformer} for {Multimodal} {Knowledge} {Graph} {Completion}},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3760786},
	doi = {10.1145/3760786},
	abstract = {Knowledge graphs (KGs) are frequently confronted with the challenge of incompleteness, a problem that extends to multimodal knowledge graphs (MKGs). The primary goal of multimodal knowledge graph completion (MKGC) is to predict missing entities within MKGs. However, current MKGC methods face difficulties in adequately addressing modal preferences and imbalances in modal information. To overcome these issues, we introduce AdaMKGC, an innovative hybrid model incorporating an adaptive modality interaction transformer. This model employs a dynamic attention interaction strategy and a self-enhancing sampling approach. AdaMKGC achieves a more precise utilization of multimodal information by integrating modal preference information into modal interactions. Additionally, it effectively mitigates the issue of modal imbalance through targeted sampling and adjustment for entities with deficient information. Experimental evaluations demonstrate AdaMKGC's superior performance in overcoming these prevalent challenges. Compared to existing state-of-the-art MKGC models, AdaMKGC shows a notable enhancement of 28\% in MR on the WN18-IMG dataset and an improvement of 2.7\% in Hits@1 on the FB15k-237-IMG dataset. Our code is available at .},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Jian, Yue and Zhang, Miao and Qin, Ziyue and Xie, Chuyuan and Xiao, Kui and Zhang, Yan and Li, Zhifei},
	month = aug,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Link Prediction, Knowledge Graph Completion, Multimodal Knowledge Graphs},
	annote = {Just Accepted},
}

@inproceedings{wu_attribute_2024,
	address = {New York, NY, USA},
	series = {{ICCIP} '23},
	title = {Attribute {Value} {Extraction} in {Weapon} {Domain} {Based} on {Bi}-{LSTM} and {Attention}},
	isbn = {979-8-4007-0890-9},
	url = {https://doi.org/10.1145/3638884.3638979},
	doi = {10.1145/3638884.3638979},
	abstract = {Aiming at the problem that the traditional extraction method caused by the diversification of weapon attributes has a large amount of work to construct the label of weapon attributes, in this paper, we propose a weapon attribute value extraction method based on bidirectional long-term and short-term memory network (Bi-LSTM) and attention mechanism. The method first uses the Bi-LSTM model to extract the features of the input text and attribute names. Then, the attention mechanism focuses on the relations between words and attributes in the sentence. Afterward, the global BIO tag marks the position of the attribute values in the sentence. In this way, the method can reduce the workload during the corpus preparation period to improve the generalization ability of the model so that it can extract different weapon attribute data. Compared with Bi-LSTM, Bi-LSTM\_CRF, and OpenTag from the experimental results, the F1 values of the proposed model on the weapon domain attribute dataset are increased by about 6.9\%, 5.7\%, and 2.5\%, respectively.},
	booktitle = {Proceedings of the 2023 9th {International} {Conference} on {Communication} and {Information} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Yu and Miao, Lin and Li, Han},
	year = {2024},
	note = {event-place: Lingshui, China},
	keywords = {Natural Language Processing, Knowledge Base, Information Extraction, Attribute Value Extraction},
	pages = {603--610},
}

@article{hamed_query_2023,
	title = {Query {Interface} for {Smart} {City} {Internet} of {Things} {Data} {Marketplaces}: {A} {Case} {Study}},
	volume = {4},
	url = {https://doi.org/10.1145/3609336},
	doi = {10.1145/3609336},
	abstract = {Cities are increasingly becoming augmented with sensors through public, private, and academic sector initiatives. Most of the time, these sensors are deployed with a primary purpose (objective) in mind (e.g., deploy sensors to understand noise pollution) by a sensor owner (i.e., the organization that invests in sensing hardware, e.g., a city council). Over the past few years, communities undertaking smart city development projects have understood the importance of making the sensor data available to a wider community—beyond their primary usage. Different business models have been proposed to achieve this, including creating data marketplaces. The vision is to encourage new startups and small and medium-scale businesses to create novel products and services using sensor data to generate additional economic value. Currently, data are sold as pre-defined independent datasets (e.g., noise level and parking status data may be sold separately). This approach creates several challenges, such as (i) difficulties in pricing, which leads to higher prices (per dataset); (ii) higher network communication and bandwidth requirements; and (iii) information overload for data consumers (i.e., those who purchase data). We investigate the benefit of semantic representation and its reasoning capabilities toward creating a business model that offers data on demand within smart city Internet of Things data marketplaces. The objective is to help data consumers (i.e., small and medium enterprises) acquire the most relevant data they need. We demonstrate the utility of our approach by integrating it into a real-world IoT data marketplace (developed by the synchronicity-iot.eu project). We discuss design decisions and their consequences (i.e., tradeoffs) on the choice and selection of datasets. Subsequently, we present a series of data modeling principles and recommendations for implementing IoT data marketplaces.},
	number = {3},
	journal = {ACM Trans. Internet Things},
	author = {Hamed, Naeima and Gaglione, Andrea and Gluhak, Alex and Rana, Omer and Perera, Charith},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Internet of Things, semantic interoperability, knowledge management, linked data, data discovery, multi-dimensional querying},
}

@inproceedings{cornejo_lupa_categorization_2020,
	address = {New York, NY, USA},
	series = {{SAC} '20},
	title = {A categorization of simultaneous localization and mapping knowledge for mobile robots},
	isbn = {978-1-4503-6866-7},
	url = {https://doi.org/10.1145/3341105.3373974},
	doi = {10.1145/3341105.3373974},
	abstract = {Autonomous robots are playing important roles in academic, technological, and scientific activities. Thus, their behavior is getting more complex. The main tasks of autonomous robots include mapping an environment and localize themselves. These tasks comprise the Simultaneous Localization and Mapping (SLAM) problem. Representation of the SLAM knowledge (e.g., robot characteristics, environment information, mapping and location information), with a standard and well-defined model, provides the base to develop efficient and interoperable solutions. However, as far as we know, there is not a common classification of such knowledge. Many existing works based on Semantic Web, have formulated ontologies to model information related to only some SLAM aspects, without a standard arrangement. In this paper, we propose a categorization of the knowledge managed in SLAM, based on existing ontologies and SLAM principles. We also classify recent and popular ontologies according to our proposed categories and highlight the lessons to learn from existing solutions.},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Cornejo Lupa, Maria A. and Ticona-Herrera, Regina P. and Cardinale, Yudith and Barrios-Aranibar, Dennis},
	year = {2020},
	note = {event-place: Brno, Czech Republic},
	keywords = {ontologies, semantic web, SLAM, mobile robots, semantic robots},
	pages = {956--963},
}

@article{bellomarini_vadalog_2024,
	title = {The {Vadalog} {Parallel} {System}: {Distributed} {Reasoning} with {Datalog}+/-},
	volume = {17},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3704965.3704970},
	doi = {10.14778/3704965.3704970},
	abstract = {Over the past years, there has been a growing demand for ontological reasoning systems based on languages of the Datalog+/- family, such as Vadalog, for their ability to effectively model a wide range of real-world problems with powerful features such as existential quantification. As the scale and complexity of data analysis tasks continue to grow, the ability to distribute the computational workload across multiple non-communicating processors has become vital for these systems to achieve scalable performance.The joint presence of existential quantification and recursion poses new challenges, currently unsolved by existing distributed systems, which only concentrate on Datalog and are therefore unsuitable for ontological reasoning. When working across multiple processors, generating all the facts to answer a specific reasoning query, avoiding duplication, and guaranteeing termination are non-trivial tasks as infinitely many new symbols and facts can be generated by existential quantification and recursion.In this paper, we address such challenges and introduce the first distributed framework in the Datalog+/- space. We propose the condition of homomorphic decomposability, which identifies sets of Datalog+/- rules with good distribution properties. We put homomorphic decomposability into action with a distributed reasoning algorithm for Warded Datalog+/-, the core of Vadalog. We implement Vadalog Parallel, a distributed reasoner for Vadalog and provide experimental evaluation against state-of-the-art systems.},
	number = {13},
	journal = {Proc. VLDB Endow.},
	author = {Bellomarini, Luigi and Benedetto, Davide and Brandetti, Matteo and Sallinger, Emanuel and Vlad, Adriano},
	month = sep,
	year = {2024},
	note = {Publisher: VLDB Endowment},
	pages = {4614--4626},
}

@inproceedings{capodieci_model-driven_2021,
	address = {New York, NY, USA},
	series = {{ICIST} '20},
	title = {Model-{Driven} approach to {Cyber} {Risk} {Analysis} in {Industry} 4.0},
	isbn = {978-1-4503-7655-6},
	url = {https://doi.org/10.1145/3447568.3448541},
	doi = {10.1145/3447568.3448541},
	abstract = {In the contest of industrial process and automation, and in particular in the so-called Industry 4.0, the now intensive application of control systems in interconnected networks has led to an increase in unexpected threats to information security for supervisory control and data acquisition (SCADA) and control systems distributed (DCS).Risk assessment is essential and the its common methods such as HHM, IIM, and RFRM have been successfully applied to SCADA systems.Another equally important need is the use of metrics and methodologies to analyze the risk (PRA- probability risk analysis), which includes methods such as FTA, ETA and FEMA and HAZOP. The goal of these methods is, in general, to determine the impact of a problem on the process plant and the risk reduction associated with a particular countermeasure.In this paper we present a methodology named CRiSP (Cyber Risk Analysis in Industrial Process System Environment). CRiSP defines an approach to analyze the risk related to the manipulation of a single element of the plant and to analyze the consequence to entire plant and in the same time to a restricted portion.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Information} {Systems} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Capodieci, Antonio and Mainetti, Luca and Dipietrangelo, Flavio},
	year = {2021},
	note = {event-place: Lecce, Italy},
	keywords = {Industry 4.0, Cybersecurity, Risk management, Risk Analysis},
}

@inproceedings{naik_multi-turn_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {Multi-turn mediated solutions for {Conversational} {Artificial} {Intelligent} systems leveraging graph-based techniques},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587540},
	doi = {10.1145/3543873.3587540},
	abstract = {The current era is dominated by intelligent Question Answering (QA) systems that can instantly answer almost all their questions, saving users search time and increasing the throughput and precision in the applied domain. A vast amount of work is being carried out in QA systems to deliver better content satisfying users’ information needs [2]. Since QA systems are ascending the cycle of emerging technologies, there are potential research gaps that can be explored. QA systems form a significant part of Conversational Artificial Intelligent systems giving rise to a new research pathway, i.e., Conversational Question Answering (CQA) systems [32]. We propose to design and develop a CQA system leveraging Hypergraph-based techniques. The approach focuses on the multi-turn conversation and multi-context to gauge users’ exact information needs and deliver better answers. We further aim to address "supporting evidence-based retrieval" for fact-based responsible answer generation. Since the QA system requires a large amount of data and processing, we also intend to investigate hardware performance for effective system utilization.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Naik, Riya},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {Question Answering, Contextual Embeddings, Conversational Artificial Intelligence, Evidence-based retrieval, Graph-based models},
	pages = {586--590},
}

@inproceedings{bennaceur_modelling_2019,
	address = {Montreal, Quebec, Canada},
	series = {{SEAMS} '19},
	title = {Modelling and analysing resilient cyber-physical systems},
	url = {https://doi.org/10.1109/SEAMS.2019.00018},
	doi = {10.1109/SEAMS.2019.00018},
	abstract = {From smart buildings to medical devices to smart nations, software systems increasingly integrate computation, networking, and interaction with the physical environment. These systems are known as Cyber-Physical Systems (CPS). While these systems open new opportunities to deliver improved quality of life for people and reinvigorate computing, their engineering is a difficult problem given the level of heterogeneity and dynamism they exhibit. While progress has been made, we argue that complexity is now at a level such that existing approaches need a major re-think to define principles and associated techniques for CPS. In this paper, we identify research challenges when modelling, analysing and engineering CPS. We focus on three key topics: theoretical foundations of CPS, self-adaptation methods for CPS, and exemplars of CPS serving as a research vehicle shared by a larger community. For each topic, we present an overview and suggest future research directions, thereby focusing on selected challenges. This paper is one of the results of the Shonan Seminar 118 on Modelling and Analysing Resilient Cyber-Physical Systems, which took place in December 2018.},
	booktitle = {Proceedings of the 14th {International} {Symposium} on {Software} {Engineering} for {Adaptive} and {Self}-{Managing} {Systems}},
	publisher = {IEEE Press},
	author = {Bennaceur, Amel and Ghezzi, Carlo and Tei, Kenji and Kehrer, Timo and Weyns, Danny and Calinescu, Radu and Dustdar, Schahram and Hu, Zhenjiang and Honiden, Shinichi and Ishikawa, Fuyuki and Jin, Zhi and Kramer, Jeffrey and Litoiu, Marin and Loreti, Michele and Moreno, Gabriel A. and Müller, Hausi A. and Nenzi, Laura and Nuseibeh, Bashar and Pasquale, Liliana and Reisig, Wolfgang and Schmidt, Heinz and Tsigkanos, Christos and Zhao, Haiyan},
	year = {2019},
	pages = {70--76},
}

@inproceedings{karayil_focus-aspect-value_2019,
	address = {New York, NY, USA},
	series = {{ICMR} '19},
	title = {The {Focus}-{Aspect}-{Value} {Model} for {Explainable} {Prediction} of {Subjective} {Visual} {Interpretation}},
	isbn = {978-1-4503-6765-3},
	url = {https://doi.org/10.1145/3323873.3325026},
	doi = {10.1145/3323873.3325026},
	abstract = {Subjective visual interpretation is a challenging yet important topic in computer vision. Many approaches reduce this problem to the prediction of adjective- or attribute-labels from images. However,most of these do not take attribute semantics into account, or only process the image in a holistic manner. Furthermore, there is alack of relevant datasets with fine-grained subjective labels. In this paper, we propose the Focus-Aspect-Value (FAV) model to structure the process of capturing subjectivity in image processing,and introduce a novel dataset following this way of modeling. We run experiments on this dataset to compare several deep learning methods and find that incorporating context information based on tensor multiplication outperforms the default way of information fusion (concatenation).},
	booktitle = {Proceedings of the 2019 on {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Karayil, Tushar and Blandfort, Philipp and Hees, Jörn and Dengel, Andreas},
	year = {2019},
	note = {event-place: Ottawa ON, Canada},
	keywords = {information fusion, neural network, logistic regression, fav, images, subjectivity, zero-shot},
	pages = {16--24},
}

@inproceedings{mckenna_modelling_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {Modelling the {Provenance} of {Linked} {Data} {Interlinks} for the {Library} {Domain}},
	isbn = {978-1-4503-6675-5},
	url = {https://doi.org/10.1145/3308560.3316518},
	doi = {10.1145/3308560.3316518},
	abstract = {As the Web of Data grows, so does the need to establish the quality and trustworthiness of its contents. Increasing numbers of libraries are publishing their metadata as Linked Data (LD). As these institutions are considered authoritative sources of information, it is likely that library LD will be treated with increased credibility over data published by other sources. However, in order to establish this trust, the provenance of library LD must be provided.In 2018 we conducted a survey which explored the position of Information Professionals (IPs), such as librarians, archivists and cataloguers, with regards to LD. Results indicated that IPs find the process of LD interlinking to be a particularly challenging. In order to publish authoritative interlinks, provenance data for the description and justification of the links is required. As such, the goal of this research is to provide a provenance model for the LD interlinking process that meets the requirements of library metadata standards. Many current LD technologies are not accessible to non-technical experts or attuned to the needs of the library domain. By designing a model specifically for libraries, with input from IPs, we aim to facilitate this domain in the process of creating interlink provenance data.},
	booktitle = {Companion {Proceedings} of {The} 2019 {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {McKenna, Lucy and Debruyne, Christophe and O'Sullivan, Declan},
	year = {2019},
	note = {event-place: San Francisco, USA},
	keywords = {semantic web, linked data, provenance, interlinking, library},
	pages = {954--958},
}

@inproceedings{diaz-agudo_towards_2021,
	address = {New York, NY, USA},
	series = {{UMAP} '21},
	title = {Towards {Personalized} {Social} {Recommendations} for {Cultural} {Heritage} {Activities}: {Methods} and technology to enable cohesive and inclusive recommendations},
	isbn = {978-1-4503-8367-7},
	url = {https://doi.org/10.1145/3450614.3463389},
	doi = {10.1145/3450614.3463389},
	abstract = {The aim of the SPICE project is to build social cohesion, both between and within citizen communities, by developing tools and methods to support citizen curation. We define citizen curation as a process in which cultural objects are used as a resource by citizens to develop their own personal interpretations. Within communities, citizens can use their interpretations to build a representation of themselves and their shared perspective on culture. Interpretations can also be used to support social cohesion across groups. In this short position paper we outline the methodologies and technologies needed to be built in order to build a recommender system of cultural objects that will implement these goals of social cohesion and inclusion.},
	booktitle = {Adjunct {Proceedings} of the 29th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Diaz-Agudo, Belen and Bosca, Alessio and Bolioli, Andrea and Jimenez Jimenez Diaz, Guilermo and Kuflik, Tsvi and J. Wecker, Alan},
	year = {2021},
	note = {event-place: Utrecht, Netherlands},
	pages = {199--202},
}

@article{gilray_datalog_2024,
	title = {Datalog with {First}-{Class} {Facts}},
	volume = {18},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3712221.3712232},
	doi = {10.14778/3712221.3712232},
	abstract = {Datalog is a popular logic programming language for deductive reasoning tasks in a wide array of applications, including business analytics, program analysis, and ontological reasoning. However, Datalog's restriction to flat facts over atomic constants leads to challenges in working with tree-structured data, such as derivation trees or abstract syntax trees. To ameliorate Datalog's restrictions, popular extensions of Datalog support features such as existential quantification in rule heads (Datalog*, Datalog∃) or algebraic data types (Soufflé). Unfortunately, these are imperfect solutions for reasoning over structured and recursive data types, with general existentials leading to complex implementations requiring unification, and ADTs unable to trigger rule evaluation and failing to support efficient indexing.We present DL∃!, a Datalog with first-class facts, wherein every fact is identified with a Skolem term unique to the fact. We show that this restriction offers an attractive price point for Datalogbased reasoning over tree-shaped data, demonstrating its application to databases, artificial intelligence, and programming languages. We implemented DL∃! as a system Slog, which leverages the uniqueness restriction of DL∃! to enable a communication-avoiding, massively-parallel implementation built on MPI. We show that Slog outperforms leading systems (Nemo, Vlog, RDFox, and Soufflé) on a variety of benchmarks, with the potential to scale to thousands of threads.},
	number = {3},
	journal = {Proc. VLDB Endow.},
	author = {Gilray, Thomas and Sahebolamri, Arash and Sun, Yihao and Kunapaneni, Sowmith and Kumar, Sidharth and Micinski, Kristopher},
	month = nov,
	year = {2024},
	note = {Publisher: VLDB Endowment},
	pages = {651--665},
}

@inproceedings{cooper_demonstration_2025,
	address = {Melbourne, Australia},
	series = {{HRI} '25},
	title = {Demonstration of an {Open}-source {ROS} 2 {Framework} and {Simulator} for {Situated} {Interactive} {Social} {Robot}},
	abstract = {We introduce an open-source ROS 2 architecture for situated social robots, along with a simulator that allows mixed-reality development and interactions. The architecture is a hybrid symbolic/subsymbolic system that integrates explicit ontology semantics for perception, reasoning, and execution, with LLMs. It features multimodal social perception by leveraging the open source ROS4HRI framework; LLMs (both edge- and cloud-based) to facilitate natural language interaction between the user and system; KnowledgeCore, an open-source knowledge base, to reason about facts in the world; and an intent-based controller to supervise the execution of parallel/sequential tasks and skills. We demonstrate our system architecture with a social robot running the mixed-reality system.},
	booktitle = {Proceedings of the 2025 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {IEEE Press},
	author = {Cooper, Sara and Ros, Raquel and Lemaignan, Séverin and Gebellí, Ferran and Ferrini, Lorenzo and Juri?ić, Luka},
	year = {2025},
	keywords = {mixed-reality simulator, ros 2 framework, situated social robots},
	pages = {1770--1772},
}

@inproceedings{lin_emous_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {{EmoUS}: {Simulating} {User} {Emotions} in {Task}-{Oriented} {Dialogues}},
	isbn = {978-1-4503-9408-6},
	url = {https://doi.org/10.1145/3539618.3592092},
	doi = {10.1145/3539618.3592092},
	abstract = {Existing user simulators (USs) for task-oriented dialogue systems only model user behaviour on semantic and natural language levels without considering the user persona and emotions. Optimising dialogue systems with generic user policies, which cannot model diverse user behaviour driven by different emotional states, may result in a high drop-off rate when deployed in the real world. Thus, we present EmoUS, a user simulator that learns to simulate user emotions alongside user behaviour. EmoUS generates user emotions, semantic actions, and natural language responses based on the user goal, the dialogue history, and the user persona. By analysing what kind of system behaviour elicits what kind of user emotions, we show that EmoUS can be used as a probe to evaluate a variety of dialogue systems and in particular their effect on the user's emotional state. Developing such methods is important in the age of large language model chat-bots and rising ethical concerns.},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Hsien-Chin and Feng, Shutong and Geishauser, Christian and Lubis, Nurul and van Niekerk, Carel and Heck, Michael and Ruppik, Benjamin and Vukovic, Renato and Gasić, Milica},
	year = {2023},
	note = {event-place: Taipei, Taiwan},
	keywords = {dialogue system, emotion simulation, user simulation},
	pages = {2526--2531},
}

@inproceedings{maurino_modelling_2019,
	address = {New York, NY, USA},
	series = {{DSMM}'19},
	title = {Modelling and {Linking} {Company} {Data} in the {euBusinessGraph} {Platform}},
	isbn = {978-1-4503-6823-0},
	url = {https://doi.org/10.1145/3336499.3338012},
	doi = {10.1145/3336499.3338012},
	abstract = {In the business environment, knowledge of company data is essential for a variety of tasks. The European funded project euBusinessGraph enables the establishment of a company data platform where data providers and consumers can publish and access company data. The core of the platform is the semantic data model that is the conceptual representation of company data in a common way so that it is easier to share and interlink company data. In this paper we show how the unified model and Grafterizer, a tool for manipulating and transforming raw data into Linked Data, support the linking challenge proposed in FEIII 2019. Results show that geographical enrichment of RDF data supports the interlinking process between company entities in different datasets.},
	booktitle = {Proceedings of the 5th {Workshop} on {Data} {Science} for {Macro}-{Modeling} with {Financial} and {Economic} {Datasets}},
	publisher = {Association for Computing Machinery},
	author = {Maurino, Andrea and Rula, Anisa and von, Bjørn Marius and Gomez, Mauricio Soto and Elvesæter, Brian and Roman, Dumitru},
	year = {2019},
	note = {event-place: Amsterdam, Netherlands},
	keywords = {RDF, Entity Matching, Company data, Record Linkage},
}

@inproceedings{ahn_bert-based_2023,
	address = {New York, NY, USA},
	series = {{RACS} '23},
	title = {{BERT}-based classification of fungi protein sequences with multiple {GO} labels},
	isbn = {979-8-4007-0228-0},
	url = {https://doi.org/10.1145/3599957.3606249},
	doi = {10.1145/3599957.3606249},
	abstract = {Due to the increase of reported fungi-related diseases, it has come to many health organizations concern that there may be possible highly contagious fungi that may cause yet another pandemic. Though the likelihood of such is low, research is needed to grasp the understanding of unknown fungi. Identifying and figuring out the traits of unknown fungi through in vitro and in vivo experiments take time and resources. In silico methods yield faster results with a slight drop in accuracy. Modern in silico approaches utilizing deep learning, allow for faster and more accurate classifications. In this study, we perform the classification of one or more gene ontologies of fungi protein sequences. We collected open-source protein sequences from UniProt and applied an algorithm to label the sequences with their gene ontologies. We use ProtBERT with additional layers to give classification results to all the different gene ontologies. Experimental results reveal that when classifying with the top 5 most frequent gene ontologies, the model was able to yield 0.7915 for F1-score, 0.7073 for MCC, and 0.8865 for AuROC. With the top 10 most frequent gene ontologies it yielded 0.6490 for F1-score, 0.6836 for MCC, and 0.7653 for AuROC.},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Research} in {Adaptive} and {Convergent} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ahn, Sung-Yoon and Lee, Sang-Woong},
	year = {2023},
	note = {event-place: Gdansk, Poland},
	keywords = {BERT, Gene Ontology, Fungi},
}

@inproceedings{prieur_shadowfax_2024,
	address = {New York, NY, USA},
	series = {{SIGIR} '24},
	title = {Shadowfax: {Harnessing} {Textual} {Knowledge} {Base} {Population}},
	isbn = {979-8-4007-0431-4},
	url = {https://doi.org/10.1145/3626772.3657666},
	doi = {10.1145/3626772.3657666},
	abstract = {Knowledge base population (KBP) from texts involves the extraction and organization of information from unstructured textual data to enhance or create a structured knowledge base. This process is crucial for various applications, such as natural language understanding, question-answering systems, and knowledge-driven decision-making. However the difficulty lies in the complexity of natural language, which is nuanced, ambiguous, and context-dependent. Extracting accurate and reliable information requires overcoming challenges such as entity disambiguation and relation extraction which are time-consuming tasks for users.Shadowfax is an interactive platform designed to support users by streamlining the process of knowledge base population (KPB) from text documents. Unlike other existing tools, it relies on a unified machine learning model to extract relevant information from unstructured text, enabling operational agents to gain a quick overview. The proposed system supports a variety of natural language processing (NLP) tasks using a single architecture, while presenting information in the most comprehensive way possible to the end user.},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Prieur, Maxime and Du Mouza, Cédric and Gadek, Guillaume and Grilheres, Bruno},
	year = {2024},
	note = {event-place: Washington DC, USA},
	keywords = {data mining, information extraction, deep-learning, end-to-end, knowledge base population, user in the loop},
	pages = {2796--2800},
}

@inproceedings{garcia_social_2020,
	address = {New York, NY, USA},
	series = {{iiWAS2019}},
	title = {Social {Media} {Copyright} {Management} using {Semantic} {Web} and {Blockchain}},
	isbn = {978-1-4503-7179-7},
	url = {https://doi.org/10.1145/3366030.3366128},
	doi = {10.1145/3366030.3366128},
	abstract = {Solutions based on distributed ledgers require sophisticated tools for data modelling and integration that can be overcome using semantic and Linked Data technologies. One example is copyright management, where we attempt to adapt the Copyright Ontology so it can be used to build applications that benefit from both worlds, rich information modelling and reasoning together with immutable and accountable information storage that provides trust and confidence on the modelled rights statements. This approach has been applied in the context of an application for the management of social media re-use for journalistic purposes.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {García, Roberto and Gil, Rosa},
	year = {2020},
	note = {event-place: Munich, Germany},
	keywords = {Ontology, Semantic Web, Blockchain, Linked Data, Ethereum, Copyright, Distributed Ledger, Rights Expression Language},
	pages = {339--343},
}

@inproceedings{wu_correlation_2020,
	address = {New York, NY, USA},
	series = {{ISAIMS} '20},
	title = {Correlation between {Pathological} {Voice} {Onset} and {Voice} {Quality} {Based} on {Vocal} {Attack} {Time}({VAT}) and {Multidimensional} {Voice} {Parameters}},
	isbn = {978-1-4503-8860-3},
	url = {https://doi.org/10.1145/3429889.3430079},
	doi = {10.1145/3429889.3430079},
	abstract = {Correlation between pathological Voice Onset and Voice Quality of Chinese patients based on Vocal Attack Time(VAT) and Multidimensional Voice Parameters is discussed in this paper. The test subjects were divided into three groups, one is normal voice group and the other two are pathologic voice groups, namely, vocal cord polyps and non vocal cord polyps. We recorded the EGG signal for the above subjects and extracted the Jitter, Shimmer, HNR and VAT parameters by the relevant software. The VAT and other voice parameters at /a:/, /i:/ and /u:/ vowels were then compared and analyzed in different groups. The results showed that there was no significant difference in the VAT between the three groups at /a:/, /i:/, and /u:/ vowels. In addition, the analysis of VAT changes in a patient with vocal cord polyps before and after surgery revealed that neither the overall difference in stops nor the difference in manner of aspiration or not was significant, indicating VAT is not a specific indicator of vocal cord polyps.},
	booktitle = {Proceedings of the 1st {International} {Symposium} on {Artificial} {Intelligence} in {Medical} {Sciences}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Nankai and Cao, Qingsong and Li, Huanzhe and Hou, Xingquan and Lo, Infat and Kong, Jiangping},
	year = {2020},
	note = {event-place: Beijing, China},
	keywords = {Electroglottography, Pathological Voice, Vocal Attack Time, Voice Onset, Voice Quality},
	pages = {107--111},
}

@inproceedings{wakil_new_2018,
	address = {New York, NY, USA},
	series = {{ICSIM} '18},
	title = {A {New} {Adaptive} {Model} for {Web} {Engineering} {Methods} to {Develop} {Modern} {Web} {Applications}},
	isbn = {978-1-4503-5438-7},
	url = {https://doi.org/10.1145/3178461.3178468},
	doi = {10.1145/3178461.3178468},
	abstract = {With the evolution of modern web applications, several web engineering methods proposed to develop web applications. The modern web applications are; Rich Internet Application (RIA), Semantic Web Application (SWA), Ubiquitous Web Applications (UWA), and Intelligent Web Applications (IWA), with each of them having new features. The problem is that current web engineering methods cannot support new features of modern web applications. However, some of them extended for new concern of web applications but have limited, meaning these methods have a lack of adaptability to support features from modern web applications. In an attempt to solve this gap, we have defined a new adaptive model for the web engineering methods that can support the new features of modern web applications. This model very efficient in the process development and will be to increase the usability of the methods.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Software} {Engineering} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Wakil, Karzan and Jawawi, Dayang N. A.},
	year = {2018},
	note = {event-place: Casablanca, Morocco},
	keywords = {Web Engineering, Adaptive Model, Web Applications},
	pages = {32--39},
}

@article{ungureanu_forsyde-atom_2021,
	title = {{ForSyDe}-{Atom}: {Taming} {Complexity} in {Cyber} {Physical} {System} {Design} with {Layers}},
	volume = {20},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3424667},
	doi = {10.1145/3424667},
	abstract = {We present ForSyDe-Atom, a formal framework intended as an entry point for disciplined design of complex cyber-physical systems. This framework provides a set of rules for combining several domain-specific languages as structured, enclosing layers to orthogonalize the many aspects of system behavior, yet study their interaction in tandem. We define four layers: one for capturing timed interactions in heterogeneous systems, one for structured parallelism, one for modeling uncertainty, and one for describing component properties. This framework enables a systematic exploitation of design properties in a design flow by facilitating the stepwise projection of certain layers of interest, the isolated analysis and refinement on projections, and the seamless reconstruction of a system model by virtue of orthogonalization. We demonstrate the capabilities of this approach by providing a compact yet expressive model of an active electronically scanned array antenna and signal processing chain, simulate it, validate its conformity with the design specifications, refine it, synthesize a sub-system to VHDL and sequential code, and co-simulate the generated artifacts.},
	number = {2},
	journal = {ACM Trans. Embed. Comput. Syst.},
	author = {Ungureanu, George and Medeiros, José Edil Guimarães De and sundström, Timmy and Söderquist, Ingemar and Åhlander, Anders and Sander, Ingo},
	month = jan,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {modeling, simulation, Cyber-physical systems, validation, design methodology, synthesis, models of computation, system design language},
}

@inproceedings{colas_can_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Can {Knowledge} {Graphs} {Simplify} {Text}?},
	isbn = {979-8-4007-0124-5},
	url = {https://doi.org/10.1145/3583780.3615514},
	doi = {10.1145/3583780.3615514},
	abstract = {Knowledge Graph (KG)-to-Text Generation has seen recent improvements in generating fluent and informative sentences which describe a given KG. As KGs are widespread across multiple domains and contain important entity-relation information, and as text simplification aims to reduce the complexity of a text while preserving the meaning of the original text, we propose KGSimple, a novel approach to unsupervised text simplification which infuses KG-established techniques in order to construct a simplified KG path and generate a concise text which preserves the original input's meaning. Through an iterative and sampling KG-first approach, our model is capable of simplifying text when starting from a KG by learning to keep important information while harnessing KG-to-text generation to output fluent and descriptive sentences. We evaluate various settings of the KGSimple model on currently-available KG-to-text datasets, demonstrating its effectiveness compared to unsupervised text simplification models which start with a given complex text. Our code is available on GitHub.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Colas, Anthony and Ma, Haodi and He, Xuanli and Bai, Yang and Wang, Daisy Zhe},
	year = {2023},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {knowledge graph, natural language generation, data-to-text, simulated annealing, KG-to-text, text simplification},
	pages = {379--389},
}

@inproceedings{mildner_listening_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Listening to the {Voices}: {Describing} {Ethical} {Caveats} of {Conversational} {User} {Interfaces} {According} to {Experts} and {Frequent} {Users}},
	isbn = {979-8-4007-0330-0},
	url = {https://doi.org/10.1145/3613904.3642542},
	doi = {10.1145/3613904.3642542},
	abstract = {Advances in natural language processing and understanding have led to a rapid growth in the popularity of conversational user interfaces (CUIs). While CUIs introduce novel benefits, they also yield risks that may exploit people’s trust. Although research looking at unethical design deployed through graphical user interfaces (GUIs) established a thorough understanding of so-called dark patterns, there is a need to continue this discourse within the CUI community to understand potentially problematic interactions. Addressing this gap, we interviewed 27 participants from three cohorts: researchers, practitioners, and frequent users of CUIs. Applying thematic analysis, we construct five themes reflecting each cohort’s insights about ethical design challenges and introduce the CUI Expectation Cycle, bridging system capabilities and user expectations while considering each theme’s ethical caveats. This research aims to inform future development of CUIs to consider ethical constraints while adopting a human-centred approach.},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mildner, Thomas and Cooney, Orla and Meck, Anna-Maria and Bartl, Marion and Savino, Gian-Luca and Doyle, Philip R and Garaialde, Diego and Clark, Leigh and Sloan, John and Wenig, Nina and Malaka, Rainer and Niess, Jasmin},
	year = {2024},
	note = {event-place: Honolulu, HI, USA},
	keywords = {dark patterns, deceptive design patterns, conversational agents, conversational user interfaces, chatbots, CUI, ethical design, thematic analysis, voice agents},
}

@article{de_roode_sok_2025,
	title = {{SoK}: {Unifying} {Definitions} of {Privacy} and {Anonymity} in {Cryptocurrencies} \&amp; {DLTs}},
	url = {https://doi.org/10.1145/3736656},
	doi = {10.1145/3736656},
	abstract = {As interest in the practical use of cryptocurrencies continues to grow, so does the focus on the (perceived) privacy and anonymity of users within this domain. Despite this attention, there is a notable absence of standardized definitions for these terms. This paper aims to address this gap by exploring the various interpretations of privacy, anonymity, and related concepts in the context of cryptocurrencies. Drawing from a thorough review of existing literature, we propose practical definitions for both privacy and anonymity. Utilizing these definitions, we introduce an ontology designed to streamline future research, identify knowledge gaps, and facilitate clearer communication in the field.},
	journal = {Distrib. Ledger Technol.},
	author = {de Roode, Gerard and Everts, Maarten},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, privacy, anonymity, cryptocurrency},
	annote = {Just Accepted},
}

@article{cong_openforge_2025,
	title = {{OpenForge}: {Probabilistic} {Metadata} {Integration}},
	volume = {18},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3746405.3746417},
	doi = {10.14778/3746405.3746417},
	abstract = {Modern data stores increasingly rely on metadata to enable diverse activities such as data cataloging and search. However, metadata curation remains a labor-intensive task, and the broader challenge of metadata maintenance—ensuring its consistency and usefulness—has been largely overlooked. In this work, we tackle the problem of resolving relationships among metadata concepts from disparate sources. Inferring these relationships are critical for creating clean and consistent metadata repositories, and a central challenge for metadata integration.We propose OpenForge, a two-stage prior-posterior framework for metadata integration. In the first stage, OpenForge exploits multiple methods including fine-tuned large language models to obtain prior beliefs about concept relationships. In the second stage, OpenForge refines these predictions using the Markov Random Field, a probabilistic graphical model. We formalize metadata integration as an optimization problem, where the objective is to identify the relationship assignments that maximize the joint probability of assignments. The MRF formulation allows OpenForge to capture prior beliefs while encoding critical relationship properties, such as transitivity, in probabilistic inference. Experiments on four datasets show the effectiveness and efficiency of OpenForge. In a use case of matching two metadata vocabularies, OpenForge outperforms GPT-4, the second-best method, by 25 F1 points.},
	number = {9},
	journal = {Proc. VLDB Endow.},
	author = {Cong, Tianji and Nargesian, Fatemeh and Xing, Junjie and Jagadish, H. V.},
	month = sep,
	year = {2025},
	note = {Publisher: VLDB Endowment},
	pages = {2914--2927},
}

@article{meshi_nimate_2025,
	title = {in({A})n({I})mate - {AI}-{Mediated} {Conversations} with {Inanimate} {Objects}},
	volume = {8},
	url = {https://doi.org/10.1145/3736787},
	doi = {10.1145/3736787},
	abstract = {in(A)n(I)mate is an interactive AI-driven system that invites participants to speak with objects. The piece showcases an innovative use of GPT’s multimodal feature, through its ability to recognize objects in an image and generate responses in its style. Participants place an object of their choice in front of a black box and engage in conversation with it by pressing buttons, often unaware that GPT is generating the responses. in(A)n(I)mate provokes a discussion about human relationships with inanimate matter, and considers the role of non-human agents in mediating and animating objects.},
	number = {3},
	journal = {Proc. ACM Comput. Graph. Interact. Tech.},
	author = {Meshi, Avital and Wright, Adam},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {LLM, GPT, Artificial Intelligence, Creative AI, Interactive art, New Media Art},
}

@article{hogan_knowledge_2021,
	title = {Knowledge {Graphs}},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3447772},
	doi = {10.1145/3447772},
	abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
	number = {4},
	journal = {ACM Comput. Surv.},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and D’amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, José Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	month = jul,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge graphs, ontologies, graph neural networks, graph databases, rule mining, embeddings, graph algorithms, graph query languages, shapes},
}

@inproceedings{wisniewski_automatic_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {Automatic {Translation} of {Competency} {Questions} into {SPARQL}-{OWL} {Queries}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3186575},
	doi = {10.1145/3184558.3186575},
	abstract = {The process of ontology authoring is inseparably connected with the quality assurance phase. One can verify the maturity and correctness of a given ontology by evaluating how many competency questions give correct answers. Competency questions are defined as a set of questions expressed in natural language that the finished ontology should be able to answer to correctly. Although this method can easily indicate what is the development status of an ontology, one has to translate competency questions from natural language into an ontology query language. This task is very hard and time consuming. To overcome this problem, my PhD thesis focuses on methods for automatically checking answerability of competency questions for a given ontology and proposing SPARQL-OWL query (OWL-aware SPARQL query) for each question where it is possible to create the query. Because the task of automatic translation from competency questions to SPARQL-OWL queries is a novel one, besides a method, we have proposed a new benchmark to evaluate such translation.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Wisniewski, Dawid},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {ontology, SPARQL-OWL, word embedding, competency question},
	pages = {855--859},
}

@inproceedings{lopez_what_2022,
	address = {New York, NY, USA},
	series = {{SIGCSE} 2022},
	title = {What is a {Computer} {Scientist}? {Unpacking} the {Ontological} {Beliefs} of {Black} and {Hispanic} {Female} {Computing} {Students}},
	isbn = {978-1-4503-9070-5},
	url = {https://doi.org/10.1145/3478431.3499295},
	doi = {10.1145/3478431.3499295},
	abstract = {Underrepresentation of Black and Hispanic women in computer science is a long-standing problem that looks bleak at every level - undergraduate and graduate. This is prompting scholars to explore reasons for these low participation rates. One framework used to understand participation and persistence in STEM fields is identity. Prior work in computer science education suggest that identity is a strong indicator of persistence in these fields. However, it is hard to understand students' perception of identity without also understanding ontological beliefs with regards to a computer scientist. In this study, we explore the nature of a computer scientist. Guided by social identity theory, we designed a study that asked students to describe their definition or ontological belief of what constitutes a computer scientist in contrast to their ability to ascribe a computer science identity to self. Leveraging qualitative methods, we interviewedn = 24 women in computer science (Black and Hispanic, undergraduate and graduate students), in order to explore the role their ontological beliefs had on their computer science identity salience. The research questions guiding this work are: (1) How do Black and Hispanic women describe or define computer scientists? (2) What impact does this definition have on Black and Hispanic women's ability to claim a computing identity? Results suggest that the wide variation in definitions has a negative impact on computer science identity salience. The findings from this work suggest that computing should consider the impacts of the current messaging of what constitutes a computer scientist.},
	booktitle = {Proceedings of the 53rd {ACM} {Technical} {Symposium} on {Computer} {Science} {Education} - {Volume} 1},
	publisher = {Association for Computing Machinery},
	author = {Lopez, Jake and Ross, Monique and Garcia, Atalie and Uribe-Gosselin, Carolina},
	year = {2022},
	note = {event-place: Providence, RI, USA},
	keywords = {computer science education, computing education, broadening participation, undergraduate curriculum},
	pages = {369--375},
}

@article{quamar_conversational_2020,
	title = {Conversational {BI}: an ontology-driven conversation system for business intelligence applications},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3415478.3415557},
	doi = {10.14778/3415478.3415557},
	abstract = {Business intelligence (BI) applications play an important role in the enterprise to make critical business decisions. Conversational interfaces enable non-technical enterprise users to explore their data, democratizing access to data significantly. In this paper, we describe an ontology-based framework for creating a conversation system for BI applications termed as Conversational BI. We create an ontology from a business model underlying the BI application, and use this ontology to automatically generate various artifacts of the conversation system. These include the intents, entities, as well as the training samples for each intent. Our approach builds upon our earlier work, and exploits common BI access patterns to generate intents, their training examples and adapt the dialog structure to support typical BI operations. We have implemented our techniques in Health Insights (HI), an IBM Watson Healthcare offering, providing analysis over insurance data on claims. Our user study demonstrates that our system is quite intuitive for gaining business insights from data. We also show that our approach not only captures the analysis available in the fixed application dashboards, but also enables new queries and explorations.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Quamar, Abdul and Özcan, Fatma and Miller, Dorian and Moore, Robert J and Niehus, Rebecca and Kreulen, Jeffrey},
	month = aug,
	year = {2020},
	note = {Publisher: VLDB Endowment},
	pages = {3369--3381},
}

@inproceedings{palumbo_cerbero_2019,
	address = {New York, NY, USA},
	series = {{CF} '19},
	title = {{CERBERO}: {Cross}-layer {modEl}-based {fRamework} for multi-{oBjective} {dEsign} of reconfigurable systems in {unceRtain} {hybRid} {envirOnments}: {Invited} paper: {CERBERO} teams from {UniSS}, {UniCA}, {IBM} {Research}, {TASE}, {INSA}-{Rennes}, {UPM}, {USI}, {Abinsula}, {AmbieSense}, {TNO}, {S}\&amp;{T}, {CRF}},
	isbn = {978-1-4503-6685-4},
	url = {https://doi.org/10.1145/3310273.3323436},
	doi = {10.1145/3310273.3323436},
	abstract = {Cyber-Physical Systems (CPS) are embedded computational collaborating devices, capable of sensing and controlling physical elements and, often, responding to humans. Designing and managing systems able to respond to different, concurrent requirements during operation is not straightforward, and introduce the need of proper support at design-time and run-time. The Cross-layer modEl-based fRamework for multi-oBjective dEsign of Reconfigurable systems in unceRtain hybRid envirOnments (CERBERO) EU project has developed a design environment for adaptive CPS. CERBERO approach leverages on model-based methodologies including different technologies and tools developed to cover design and operation from user interactions down to low level computing layer implementation.},
	booktitle = {Proceedings of the 16th {ACM} {International} {Conference} on {Computing} {Frontiers}},
	publisher = {Association for Computing Machinery},
	author = {Palumbo, Francesca and Fanni, Tiziana and Sau, Carlo and Pulina, Luca and Raffo, Luigi and Masin, Michael and Shindin, Evgeny and de Rojas, Pablo Sanchez and Desnos, Karol and Pelcat, Maxime and Rodríguez, Alfonso and Juárez, Eduardo and Regazzoni, Francesco and Meloni, Giuseppe and Zedda, Katiuscia and Myrhaug, Hans and Kaliciak, Leszek and Andriaanse, Joost and de Olivieria Filho, Julio and Muñoz, Pablo and Toffetti, Antonella},
	year = {2019},
	note = {event-place: Alghero, Italy},
	keywords = {CPS, verification, self-adaptation, HW adaptivity, HW reconfiguration, SW adaptivity},
	pages = {320--325},
}

@inproceedings{perez-martinez_moraban_2025,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '25},
	title = {{MoR}℡aban: a {Neurosymbolic} {Framework} for {Motion} {Representation} and {Analysis} based on {Labanotation} and {Laban} {Movement} {Analysis}},
	isbn = {979-8-4007-1399-6},
	url = {https://doi.org/10.1145/3708319.3734180},
	doi = {10.1145/3708319.3734180},
	abstract = {Human motion cannot be fully modeled by subsymbolic representations. While these extract precise hidden patterns in motion data, they are often task-specific and lack a semantic understatement of motion. Symbolic systems that mirror human cognition and explicit expressive processes are necessary for richer motion synthesis and analysis, enabling physical reasoning and expert knowledge encoding. In this work, we propose a neurosymbolic framework that combines Labanotation and Laban Movement Analysis (LMA), originally developed for dance, to represent and analyze human motion symbolically. We expand the existing LabanEditor to support full-body annotation and integrate it with AMASS, Mediapipe, and Kinect inputs through a SMPL-based format. Our system supports automatic annotation for the local functional and expressive aspects of motion, and enables bidirectional conversion between symbols and motion. While still a work in progress, this framework lays the groundwork for explainable, expressive motion modeling that can support human-robot interaction, motion preservation, and psychomotor learning systems.},
	booktitle = {Adjunct {Proceedings} of the 33rd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Perez-Martinez, Roberto and Casas-Ortiz, Alberto and Santos, Olga C.},
	year = {2025},
	keywords = {knowledge representation, expert systems, Labanotation, LMA, motion modeling, movement modeling},
	pages = {353--359},
}

@inproceedings{wagner_adding_2018,
	address = {Gothenburg, Sweden},
	series = {{WSC} '18},
	title = {Adding agent concepts to object event modeling and simulation},
	isbn = {978153866570},
	abstract = {Object Event Modeling and Simulation (OEM\&amp;S) is a general Discrete Event Simulation paradigm combining object-oriented modeling with the event scheduling paradigm. We show how to extend OEM\&amp;S by adding concepts of agent-based modeling and simulation, resulting in a framework that we call Agent/Object Event Modeling and Simulation (A/OEM\&amp;S). The main point for such an extension is to define agents as special objects, which are subject to general (physical) laws of causality captured in the form of event rules, and which have their own behavior allowing them to interact with their inanimate environment and with each other. Because agent behavior is decoupled from physical causality, an A/OE simulator consists of an environment simulator, which simulates the physical world (the objective states of material objects), and agent simulators, which simulate the internal (subjective) states of agents and their behaviors.},
	booktitle = {Proceedings of the 2018 {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Wagner, Gerd and Nardin, Luis G.},
	year = {2018},
	pages = {893--904},
}

@inproceedings{balaban_mediation-based_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Mediation-based {MLM} in {FOModeLer}},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561599},
	doi = {10.1145/3550356.3561599},
	abstract = {MLM has attracted much attention over the last two decades. MLM activities include philosophical discussions about ontologies, requirements and relevant services, and development of theories, languages, and tools. Approaches differ in their support for MLM concepts on the levels of syntax, semantics and pragmatics.The Mediation-based MLM (MedMLM), is a formal theory that defines a multilevel model as an ordered collection of levels that are inter-related by mediators, and can be enriched by inter-level relationships and interactions. The levels of MedMLM are plain class models, and the mediators define inter-level instantiation relations. MedMLM is unique in supporting a modular architecture of levels and mediators.This paper introduces the MedMLM software modeling tool, that is built on top of the FOModeLer class modeling tool. The tool supports MLM construction, querying and reasoning, meta-reasoning, validation, syntax verification, and plain computation. We also compare the MedMLM tool with older MLM approaches using semantic, syntactic, and pragmatic MLM criteria.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Balaban, Mira and Khitron, Igal and Maraee, Azzam and Kifer, Michael},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {multi-level modeling, MLM semantics, executable logic},
	pages = {444--452},
}

@article{hougaard_aiming_2024,
	title = {Aiming, {Pointing}, {Steering}: {A} {Core} {Task} {Analysis} {Framework} for {Gameplay}},
	volume = {8},
	url = {https://doi.org/10.1145/3677057},
	doi = {10.1145/3677057},
	abstract = {Underneath their compelling audiovisual surface, games require players to carry out mundane interaction work, such as pointing, typing, or steering. However, many of these underlying building blocks are not defined rigorously, hampering synthesis and analysis. We elaborate on the origin of tasks within human-computer interaction (HCI) and define tasks' relationship to game terminology (game mechanics, goals, and actions). Our proposed framework draws on systemic-structural theory of activity to aid systematic analysis and exploration of game design by mapping gameplay to abstract core tasks. The framework contains four task tools, applicable when 1) uncovering design properties, 2) designing experimental manipulation, 3) creating behavioral measurements, and 4) describing gameplay in literature reviews of game genres and design techniques. We evaluated our framework as a lens to design purposeful games in three case studies within a scientific education. We invite researchers and practitioners to employ the framework as a microscope, to describe and design games rigorously.},
	number = {CHI PLAY},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Hougaard, Bastian Ilsø and Knoche, Hendrik},
	month = oct,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, abstraction, game design, feedback, task analysis, action, activity theory, core task, design landscape, gameplay, imperative goals, mechanics, task definition},
}

@inproceedings{thapa_hospital_2022,
	address = {New York, NY, USA},
	series = {{ACSW} '22},
	title = {Hospital {Readmission} {Prediction} {Using} {Clinical} {Admission} {Notes}},
	isbn = {978-1-4503-9606-6},
	url = {https://doi.org/10.1145/3511616.3513115},
	doi = {10.1145/3511616.3513115},
	abstract = {Clinical notes contain contextualised information beyond structured data relating to patients’ past and current health conditions. Despite the richness, their unstructured, long, and high dimensional nature presents challenges to traditional text representation techniques. The advancement of deep contextual representation techniques in natural language processing (NLP) has shown remarkable performance in the biomedical and clinical domains for various information extraction and predictive tasks, including hospital readmission. However, most previous works have proposed discharge summary models where on-site medical intervention is impossible, and readmission could still occur. This paper utilises clinical notes recorded during admissions to study the risk of 30-day hospital readmissions. We employ clinical notes from MIMIC-III and consider competing baselines for clinical text representation, where a set of machine learning and deep learning algorithms are used to classify hospital readmission. The study demonstrates that notes captured during admissions play a crucial role to recognise potential readmission risk supporting healthcare practitioners for practical therapeutic intervention and discharge planning.},
	booktitle = {Proceedings of the 2022 {Australasian} {Computer} {Science} {Week}},
	publisher = {Association for Computing Machinery},
	author = {Thapa, Nischay Bikram and Seifollahi, Sattar and Taheri, Sona},
	year = {2022},
	note = {event-place: Brisbane, Australia},
	keywords = {Natural language processing, Electronic health records, Hospital readmission, Embedding techniques},
	pages = {193--199},
}

@inproceedings{weir_dbpal_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {{DBPal}: {A} {Fully} {Pluggable} {NL2SQL} {Training} {Pipeline}},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3380589},
	doi = {10.1145/3318464.3380589},
	abstract = {Natural language is a promising alternative interface to DBMSs because it enables non-technical users to formulate complex questions in a more concise manner than SQL. Recently, deep learning has gained traction for translating natural language to SQL, since similar ideas have been successful in the related domain of machine translation. However, the core problem with existing deep learning approaches is that they require an enormous amount of training data in order to provide accurate translations. This training data is extremely expensive to curate, since it generally requires humans to manually annotate natural language examples with the corresponding SQL queries (or vice versa). Based on these observations, we propose DBPal, a new approach that augments existing deep learning techniques in order to improve the performance of models for natural language to SQL translation. More specifically, we present a novel training pipeline that automatically generates synthetic training data in order to (1) improve overall translation accuracy, (2) increase robustness to linguistic variation, and (3) specialize the model for the target database. As we show, our DBPal training pipeline is able to improve both the accuracy and linguistic robustness of state-of-the-art natural language to SQL translation models.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Weir, Nathaniel and Utama, Prasetya and Galakatos, Alex and Crotty, Andrew and Ilkhechi, Amir and Ramaswamy, Shekar and Bhushan, Rohin and Geisler, Nadja and Hättasch, Benjamin and Eger, Steffen and Cetintemel, Ugur and Binnig, Carsten},
	year = {2020},
	note = {event-place: Portland, OR, USA},
	keywords = {NL2SQL, NLIDB, natural language interface to database, natural language to SQL},
	pages = {2347--2361},
}

@inproceedings{ma_cs-rad_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {{CS}-{RAD}: {Conditional} {Member} {Status} {Refinement} and {Ability} {Discovery} for {Social} {Network} {Applications}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539046},
	doi = {10.1145/3534678.3539046},
	abstract = {In a social network environment, member status represents a member's social value in the network. A member's abilities represent the potential of a member projecting his/her social values to others, and also represent the level of credibility and authority for a member to hold certain status. Therefore, the concepts of status and ability are deeply related, and should be consistent with each other. In this paper, we establish the consistency models among different member status and their abilities through analyzing member data and integrating domain knowledge. We use these models to help our members refine their inconsistent status, at the same time, identify ability gaps. To reliably refine a member status, we introduce a practical and human-in-the-loop methodology to build status hierarchy. Conditioned on the hierarchical structure, our modeling process exploits the associations between status and abilities. We applied the technique to LinkedIn member titles – one of the major types of the member status, and member skills – the main ability representations at LinkedIn. We showed that our models are intuitive and perform well. The skill gaps identified are actionable and concise. In this paper, we also discuss the aspects of building such systems, and how we could deploy the models in production.},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Yiming},
	year = {2022},
	note = {event-place: Washington DC, USA},
	keywords = {ontology, knowledge representation, taxonomy, social networks, user modeling, statistical modeling},
	pages = {3486--3494},
}

@inproceedings{ruscheinski_generating_2018,
	address = {Gothenburg, Sweden},
	series = {{WSC} '18},
	title = {Generating simulation experiments based on model documentations and templates},
	isbn = {978153866570},
	abstract = {An increasing number of approaches for specifying and executing simulation experiments emphasizes the desire to make this part of modeling and simulation studies explicit, and thus, also easier to replicate. We take this one step further by automatically generating simulation experiment specifications from documentations. Based on a template-based approach and documentations, we show how simulation experiment specifications can be generated and executed for experiments, such as statistical model checking and sensitivity analysis, and we identify crucial challenges.},
	booktitle = {Proceedings of the 2018 {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Ruscheinski, Andreas and Budde, Kai and Warnke, Tom and Wilsdorf, Pia and Hiller, Bjarne C. and Dombrowsky, Marcus and Uhrmacher, Adelinde M.},
	year = {2018},
	pages = {715--726},
}

@inproceedings{srivastava_counseling_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Counseling {Summarization} {Using} {Mental} {Health} {Knowledge} {Guided} {Utterance} {Filtering}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539187},
	doi = {10.1145/3534678.3539187},
	abstract = {The psychotherapy intervention technique is a multifaceted conversation between a therapist and a patient. Unlike general clinical discussions, psychotherapy's core components (viz. symptoms) are hard to distinguish, thus becoming a complex problem to summarize later. A structured counseling conversation may contain discussions about symptoms, history of mental health issues, or the discovery of the patient's behavior. It may also contain discussion filler words irrelevant to a clinical summary. We refer to these elements of structured psychotherapy as counseling components. In this paper, the aim is mental health counseling summarization to build upon domain knowledge and to help clinicians quickly glean meaning. We create a new dataset after annotating 12.9K utterances of counseling components and reference summaries for each dialogue. Further, we propose ConSum, a novel counseling-component guided summarization model. ConSum undergoes three independent modules. First, to assess the presence of depressive symptoms, it filters utterances utilizing the Patient Health Questionnaire (PHQ-9), while the second and third modules aim to classify counseling components. At last, we propose a problem-specific Mental Health Information Capture (MHIC) evaluation metric for counseling summaries. Our comparative study shows that we improve on performance and generate cohesive, semantic, and coherent summaries. We comprehensively analyze the generated summaries to investigate the capturing of psychotherapy elements. Human and clinical evaluations on the summary show that ConSum generates quality summary. Further, mental health experts validate the clinical acceptability of the ConSum. Lastly, we discuss the uniqueness in mental health counseling summarization in the real world and show evidences of its deployment on an online application with the support of mpathic.ai},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Srivastava, Aseem and Suresh, Tharun and Lord, Sarah P. and Akhtar, Md Shad and Chakraborty, Tanmoy},
	year = {2022},
	note = {event-place: Washington DC, USA},
	keywords = {natural language processing, dialogue summarization},
	pages = {3920--3930},
}

@inproceedings{yen_that_2018,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '18},
	title = {That {Makes} {Sense}: {Joint} {Sense} {Retrofitting} from {Contextual} and {Ontological} {Information}},
	isbn = {978-1-4503-5640-4},
	url = {https://doi.org/10.1145/3184558.3186906},
	doi = {10.1145/3184558.3186906},
	abstract = {While recent word embedding models demonstrate their abilities to capture syntactic and semantic information, the demand for sense level embedding is getting higher. In this study, we propose a novel joint sense embedding learning model that retrofits the word representation into sense representation from contextual and ontological information. The experiment shows the effectiveness and robustness of our model that outperforms previous approaches in four public available benchmark datasets.},
	booktitle = {Companion {Proceedings} of the {The} {Web} {Conference} 2018},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Yen, Ting-Yu and Lee, Yang-Yin and Huang, Hen-Hsen and Chen, Hsin-Hsi},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {semantic relatedness, joint sense retrofitting, sense embedding},
	pages = {15--16},
}

@article{buron_obi-wan_2020,
	title = {Obi-{Wan}: ontology-based {RDF} integration of heterogeneous data},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3415478.3415512},
	doi = {10.14778/3415478.3415512},
	abstract = {We consider the problem of integrating heterogeneous data (relational, JSON, key-values, graphs etc.) and querying it efficiently. Traditional data integration systems fall into two classes: data warehousing, where all data source content is materialized in a single repository, and mediation, where data remains in their original stores and all data can be queried through a mediator.We propose to demonstrate Obi-Wan, a novel mediator following the Ontology-Based Data access (OBDA) paradigm. Obi-Wan integrates data sources of many data models under an interface based on RDF graphs and ontologies (classes, properties, and relations between them). The novelty of Obi-Wan is to combine maximum integration power (GLAV mappings, see below) with the highest query answering power supported by an RDF mediator: RDF queries not only over the data but also over the integration ontologies. This makes it more flexible and powerful than comparable systems.},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Buron, Maxime and Goasdoué, François and Manolescu, Ioana and Mugnier, Marie-Laure},
	month = aug,
	year = {2020},
	note = {Publisher: VLDB Endowment},
	pages = {2933--2936},
}

@inproceedings{yang_dual_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23},
	title = {A {Dual} {Prompt} {Learning} {Framework} for {Few}-{Shot} {Dialogue} {State} {Tracking}},
	isbn = {978-1-4503-9416-1},
	url = {https://doi.org/10.1145/3543507.3583238},
	doi = {10.1145/3543507.3583238},
	abstract = {Dialogue State Tracking (DST) module is an essential component of task-oriented dialog systems to understand users’ goals and needs. Collecting dialogue state labels including slots and values can be costly, requiring experts to annotate all (slot, value) information for each turn in dialogues. It is also difficult to define all possible slots and values in advance, especially with the wide application of dialogue systems in more and more new-rising applications. In this paper, we focus on improving DST module to generate dialogue states in circumstances with limited annotations and knowledge about slot ontology. To this end, we design a dual prompt learning framework for few-shot DST. The dual framework aims to explore how to utilize the language understanding and generation capabilities of pre-trained language models for DST efficiently. Specifically, we consider the learning of slot generation and value generation as dual tasks, and two kinds of prompts are designed based on this dual structure to incorporate task-related knowledge of these two tasks respectively. In this way, the DST task can be formulated as a language modeling task efficiently under few-shot settings. To evaluate the proposed framework, we conduct experiments on two task-oriented dialogue datasets. The results demonstrate that the proposed method not only outperforms existing state-of-the-art few-shot methods, but also can generate unseen slots. It indicates that DST-related knowledge can be probed from pre-trained language models and utilized to address low-resource DST efficiently with the help of prompt learning.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Yang, Yuting and Lei, Wenqiang and Huang, Pei and Cao, Juan and Li, Jintao and Chua, Tat-Seng},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {Language model, Modeling languages, Prompt learning, Computational linguistics, few-shot learning, prompt learning, Few-shot learning, Dialog state tracking, Dialogue systems, dialogue state tracking, Learning systems, Speech processing, Learning frameworks, State tracking, Task-oriented, Tracking module, User goals},
	pages = {1468--1477},
	annote = {Cited by: 15; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{ferreira_automated_2022,
	address = {New York, NY, USA},
	series = {{SBSI} '22},
	title = {Automated {Statistics} {Extraction} of {Public} {Security} {Events} {Reported} {Through} {Microtexts} on {Social} {Networks}},
	isbn = {978-1-4503-9698-1},
	url = {https://doi.org/10.1145/3535511.3535513},
	doi = {10.1145/3535511.3535513},
	abstract = {Lately, Rio de Janeiro State has been characterized by the occurrence of successive public security events (shootings, assaults, robberies, etc.), causing great insecurity, affecting the daily lives of the population, and worrying public security agencies in the fight against crime. Although the indicators of public security events recently decreased, there is still a feeling of insecurity, while the population uses social networks to notify illegal acts that occurred in their vicinity. Although this collaboration is limited to the crimes that occurred, many published messages are difficult to interpret. Knowledge Discovery is a process of extracting data in an implicit, previously unknown, and useful way that can be applied for different purposes. In this context, Natural Language Processing is a powerful tool that allows the extraction of information from these unstructured data. This work proposes a methodology for automatic knowledge extraction, in the form of statistics related to public security events posted on social networks, particularly the ones occurred in Rio de Janeiro. The main contribution of this work is the proposal of a methodology for the construction of an Information System that allows the collection of statistics of notified public security events. In addition to this methodology, which can also be used in the construction of other Information Systems, this work contributes with a public security event recognition model that has a performance of 95\%, and an available dataset that can be used to support other researches, such as: the identification of new behavior patterns, the discovery of hidden knowledge, among other fronts.},
	booktitle = {Proceedings of the {XVIII} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ferreira, Flávio and Duarte, Julio and Ugulino, Wallace},
	year = {2022},
	note = {event-place: Curitiba, Brazil},
	keywords = {Natural Language Processing, Machine Learning, Artificial Intelligence, Text Classification, Data Mining, Text Mining, Public Security, Twitter.},
}

@inproceedings{nguyen_classification_2024,
	address = {New York, NY, USA},
	series = {{ICIIT} '24},
	title = {Classification of {Rice} {Plant} {Disease} {Based} on {Descriptive} {Information} with {DistilBERT}'s {Architecture}},
	isbn = {979-8-4007-1671-3},
	url = {https://doi.org/10.1145/3654522.3654568},
	doi = {10.1145/3654522.3654568},
	abstract = {Rice has a significant role in human life, and currently, the issue of rice plant diseases is receiving attention in image-related data processing. However, the possibility of applying text classification to address this issue has yet to be explored. Nonetheless, it deserves attention due to the complexity of image-related data. The study gathered descriptive passages on four prevalent rice diseases in Vietnam, with 365 descriptions. The collected data underwent data preprocessing through stopword removal, then visualization to identify crucial words and phrases for disease identification in rice plants. The study used feature extraction and fine-tuning based on DistilBERT's architecture to build models. The research findings showed an impressive peak accuracy rate of 87\%, highlighting the potential of using text classification algorithms to classify diseases in crops using descriptions.},
	booktitle = {Proceedings of the 2024 9th {International} {Conference} on {Intelligent} {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Anh Quynh and Tran, My Tu and Nguyen, Quang Nhat and Huynh, Huy Khai and Le, Lan Thi Thu and Quach, Luyl-Da},
	year = {2024},
	note = {event-place: Ho Chi Minh City, Vietnam},
	keywords = {Text Classification, DistilBERT, Large Language model, Rice Disease},
	pages = {155--163},
}

@inproceedings{pandita_contextual_2024,
	address = {New York, NY, USA},
	series = {{ICIMMI} '23},
	title = {Contextual transcription and {Summarization} of audio using {AI}},
	isbn = {979-8-4007-0941-8},
	url = {https://doi.org/10.1145/3647444.3647871},
	doi = {10.1145/3647444.3647871},
	abstract = {The field of Natural Language Processing (NLP) has revolutionized the way human language interacts with computer systems. NLP applications span machine translation, information extraction, summarization, and question answering, driven by vast computational resources and big data methodologies. Despite these advancements, NLP tools haven't fully integrated with Internet of Things (IoT) devices, like audio recorders, hindering their accessibility and usability. This paper introduces an innovative solution: a method for audio transcription and contextual summarization using NLP, addressing this gap and enhancing comprehension. Our approach employs cutting-edge NLP techniques, including word embedding methods and knowledge-based graphs, to create a system that efficiently converts audio content into written text and generates coherent summaries. Unlike existing AI tools, our system's summaries are not only accurate but also rich and deep, providing insightful representations of the original content. This depth is achieved through advanced linguistic analysis, surpassing tools like ChatGPT. Furthermore, our system breaks language barriers, enabling multilingual data traversal, enhancing accessibility on a global scale. Our research methodology ensures the system's adherence to industry standards like Request for Comments (RFC) and Constrained Application Protocol (CoAP), guaranteeing interoperability and reliability. By incorporating knowledge-based graphs, our system comprehensively understands audio content, enhancing the accuracy of summarization. This approach addresses the unmet need for seamlessly integrating NLP with IoT devices, making the technology accessible to a broader audience.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Information} {Management} \&amp; {Machine} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Pandita, Karan and Thakur, Purab Kulranjan Singh and Annamalai, Suresh},
	year = {2024},
	note = {event-place: Jaipur, India},
	keywords = {NLP, data summarization, audio transcription, Constrained Application Protocol (CoAP), contextual summarization, IOT, knowledge based graphs, Request for Comments (RFC), word embedding methods},
}

@inproceedings{chondamrongkul_semantic-based_2020,
	address = {New York, NY, USA},
	series = {{FormaliSE} '20},
	title = {Semantic-based {Architecture} {Smell} {Analysis}},
	isbn = {978-1-4503-7071-4},
	url = {https://doi.org/10.1145/3372020.3391564},
	doi = {10.1145/3372020.3391564},
	abstract = {Software smells have negative impacts on the reliability and modifiability of software systems. The smells in architecture design can be cascaded down to the implementation level and cause issues that require much effort to fix. Therefore, early detection of the architecture smells can benefit the overall quality of the software system. This paper presents an integration of methods that formally define the software architecture design towards architecture smell detection. Our approach serves as a framework that allows the architectural structures and behaviours to be formally analysed based on a coherent technique. We evaluated the accuracy and performance of our approach with the models generated from open source projects. The results show that our approach is effective and functions well.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Formal} {Methods} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chondamrongkul, Nacha and Sun, Jing and Warren, Ian and Lee, Scott Uk-Jin},
	year = {2020},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {Ontologies, OWL, Cognition, Ontology Web Language, Software architecture, Model checking, Software systems, Architecture Smells, Model Checking, Smell Detection, Software Architecture, Computer architecture},
	pages = {109--118},
}

@inproceedings{chen_modeling_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {Modeling {Relational} {Drug}-{Target}-{Disease} {Interactions} via {Tensor} {Factorization} with {Multiple} {Web} {Sources}},
	isbn = {978-1-4503-6674-8},
	url = {https://doi.org/10.1145/3308558.3313476},
	doi = {10.1145/3308558.3313476},
	abstract = {Modeling the behaviors of drug-target-disease interactions is crucial in the early stage of drug discovery and holds great promise for precision medicine and personalized treatments. The growing availability of new types of data on the internet brings great opportunity of learning a more comprehensive relationship among drugs, targets, and diseases. However, existing methods often consider drug-target interactions or drug-disease interactions separately, which ignores the dependencies among these three entities. Also, many of them cannot directly incorporate rich heterogeneous information from diverse sources. In this work, we investigate the utility of tensor factorization to model the relationships of drug-target-disease, specifically leveraging different types of online data. Our motivation is two-fold. First, in human metabolic systems, many drugs interact with protein targets in cells to modulate target activities, which in turn alter biological pathways to promote healthy functions and to treat diseases. Instead of binary relationships of \&lt;drug, disease\&gt; or \&lt;drug, target\&gt;, a tighter triple relationships \&lt;drug, target, disease\&gt; should be exploited to better understand drug mechanism of actions (MoAs). Second, medical data could be collected from different sources (i.e., drug's chemical structure, target's sequence, or expression measurements). Therefore, effectively exploiting the complementarity among multiple sources is of great importance. Our method elegantly explores a \&lt;drug, target, disease\&gt; tensor together with complementarity among different data sources, thus improves prediction accuracy. We achieve this goal by formulating the problem into a coupled tensor-matrix factorization problem and directly optimize it on the nonlinear manifold. Experimental results on real-world datasets show that the proposed model outperforms several competitive methods. Our model opens up opportunities to use large Web data to predict drugs' MoAs in pharmacological studies.},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Huiyuan and Li, Jing},
	year = {2019},
	note = {event-place: San Francisco, CA, USA},
	keywords = {Multi-view learning, Drug discovery, Tensor factorization, Disease analysis, Grassmann manifold, Manifold optimization},
	pages = {218--227},
}

@inproceedings{rolim_semanticsefaz_2019,
	address = {New York, NY, USA},
	series = {{WebMedia} '19},
	title = {{SemanticSefaz}: an ontology-based semantic portal for the government spending},
	isbn = {978-1-4503-6763-9},
	url = {https://doi.org/10.1145/3323503.3360638},
	doi = {10.1145/3323503.3360638},
	abstract = {Supervision in the public procurement process is considered essential for society as a means of promoting greater security and control against possible fraud and illegal actions. However, the data available on government procurement alone does not allow for the identification of possible signed contracts or bidding processes won by unfit or suspended companies, making it difficult to analyze and supervise by employees of tax agencies such as SEFAZ. In addition, data are often not available in the same common format and differ in their vocabulary, making it difficult for these professionals to find interesting information. As a means of solving these problems, the present work presents SemanticSefaz, a semantic portal for integration between heterogeneous bases focused on the domain of public procurement through a homogeneous view, allowing for semantic queries and subsequent discovery of information that priori were not possible. As a case study, the databases with data on government procurement (SIASG), unhealthy and suspenseful companies (CEIS) and punished companies (CNEP) were used to construct semantic integration. Subsequently, queries of interest to the tax domain were conducted through SemanticSefaz, demonstrating its efficiency for performing faceted queries and semantic navigation. In the end, SemanticSefaz is characterized as a timely tool for integration, visualization, discovery of knowledge to facilitate the work of tax professionals.},
	booktitle = {Proceedings of the 25th {Brazillian} {Symposium} on {Multimedia} and the {Web}},
	publisher = {Association for Computing Machinery},
	author = {Rolim, Tulio Vidal and Vidal, Vânia Maria Ponte and Avila, Caio Viktor S. and Cruz, Matheus Mayron Lima da and Barrio, Matheus and Queiroz, Daniel},
	year = {2019},
	note = {event-place: Rio de Janeiro, Brazil},
	keywords = {semantic web, linked data, government purchasing},
	pages = {493--496},
}

@inproceedings{wu_collaborative_2019,
	address = {New York, NY, USA},
	series = {{ICSLT} '19},
	title = {Collaborative {Model} of {Emerging} {Technologies} in {Asia} {Pacific}},
	isbn = {978-1-4503-6235-1},
	url = {https://doi.org/10.1145/3312714.3312721},
	doi = {10.1145/3312714.3312721},
	abstract = {Emerging technologies became pervasive in this decade due to its significant growth in terms of high-volume transactions and sophisticated businesses. According to Gartner, public cloud in Asia Pacific will continue its high growth and hit a minimum of \$11.5 billion by end of 2018. However the challenge can be arise from the other side as only those enterprises who understand agility and collaboration can be the winner. Market power for a single firm is very low in competitive market furthermore, to survive in uprising competition; companies need to differentiate themselves by customizing their products and services, and to quickly adapt themselves into a common platform in the region.To envision the roadmap for regional prosperity, a three-step approach was proposed, to align culture, technologies, and economy into a transformation process, and eventually to achieve a common model of regional collaboration.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {E}-{Society}, e-{Learning} and e-{Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Eureeka Haishang and Wu, Raymond},
	year = {2019},
	note = {event-place: Vienna, Austria},
	keywords = {Innovation, Collaboration, Consolidation, Governance, Emerging Technologies},
	pages = {73--77},
}

@inproceedings{gyurek_binder_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Binder: {Hierarchical} {Concept} {Representation} through {Order} {Embedding} of {Binary} {Vectors}},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671793},
	doi = {10.1145/3637528.3671793},
	abstract = {For natural language understanding and generation, embedding concepts using an order-based representation is an essential task. Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts. In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include vectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box embedding creates region-based rich representation of concepts, but along the process it sacrifices simplicity, requiring a custom-made optimization scheme for learning the representation. Hyperbolic embedding improves embedding quality by exploiting the ever-expanding property of Hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not simple in the Hyperbolic space. In this work, we propose Binder, a novel approach for order-based representation. Binder uses binary vectors for embedding, so the embedding vectors are compact with an order of magnitude smaller footprint than other methods. Binder uses a simple and efficient optimization scheme for learning representation vectors with a linear time complexity. Our comprehensive experimental results show that Binder is very accurate, yielding competitive results on the representation task. But Binder stands out from its competitors on the transitive closure link prediction task as it can learn concept embeddings just from the direct edges, whereas all existing order-based approaches rely on the indirect edges. In particular, Binder achieves a whopping 70\% higher F1-score than the second best method (98.6\% vs 29\%) in our largest dataset, WordNet Nouns (743,241 edges), when using only direct edges during training.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Gyurek, Croix and Talukder, Niloy and Hasan, Mohammad Al},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {concept graph, binary vector embedding, hierarchical embedding, order embedding},
	pages = {980--991},
}

@inproceedings{henriques_problem_2023,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '23},
	title = {The problem with gender-blind design and how we might begin to address it: {A} model for intersectional feminist ethical deliberation},
	isbn = {978-1-4503-9422-2},
	url = {https://doi.org/10.1145/3544549.3582750},
	doi = {10.1145/3544549.3582750},
	abstract = {Gender-blind design hinges upon an assumption that designing equally is the same as designing for equality. That, however, is inaccurate, as gender-blindness is merely a synonym for neutrality. Neutrality, because it lacks a concerted effort to subvert, favors hegemonic values and epistemologies, which counters the purported aim of equality. Supposedly objective methods of analysis, such as data gathering and interpreting, are not deprived of this hegemonic bias either. As such, through an acknowledgment of ethics, the designer must recognize that they are, indeed, imbuing their values into their designs, which bears influence on the ways in which the user interacts and interprets those designs, a notion which is especially relevant to a field concerned with user experience. This may be done deliberately or by accident, but it is always inevitable. Ethics is, in this way, inextricable from the design process, and, thus, the present article aims to propose that designing for equality requires the designer to act as an ethical agent — responsibly, consciously, and knowingly — especially if one hopes to avoid a design which embodies and communicates oppressive notions. In particular, within the purview of ethics, and by making use of some case-studies and examples, it argues that designing toward gender equality requires not the more typical gender-blind approach, but rather one which is specifically gender-conscious. Further, this article also offers some suggestions as to how we might begin to act as ethical design agents and implement marginalized epistemologies into the design process.},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Henriques, Ana O. and Rafael, Sónia and Almeida, Victor M and Pinto, José Gomes},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {Conceptual Model, Ethics, Feminist Design, Gender-Blind Design},
}

@inproceedings{steimann_kingdoms_2021,
	address = {New York, NY, USA},
	series = {Onward! 2021},
	title = {The kingdoms of objects and values},
	isbn = {978-1-4503-9110-8},
	url = {https://doi.org/10.1145/3486607.3486771},
	doi = {10.1145/3486607.3486771},
	abstract = {THE purpose of the following paper is to consider whether there is a fundamental division of the [data] with which [programming] is concerned into two classes, [objects] and [values], or whether there is any method of overcoming this dualism. My own opinion is that the dualism is ultimate; on the other hand, many [colleagues] with whom, in the main, I am in close agreement, hold that it is not ultimate. (paraphrased after Bertrand Russell)},
	booktitle = {Proceedings of the 2021 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {Association for Computing Machinery},
	author = {Steimann, Friedrich},
	year = {2021},
	note = {event-place: Chicago, IL, USA},
	keywords = {programming languages, metaphysics, object, ontology of computing, particulars, universals, values},
	pages = {125--135},
}

@article{sun_minimising_2022,
	title = {Minimising {Biasing} {Word} {Errors} for {Contextual} {ASR} {With} the {Tree}-{Constrained} {Pointer} {Generator}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2022.3224286},
	doi = {10.1109/TASLP.2022.3224286},
	abstract = {Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40\% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
	month = nov,
	year = {2022},
	note = {Publisher: IEEE Press},
	keywords = {Language model, Decoding, Context modeling, Contextual knowledge, Training, Computational linguistics, Zero-shot learning, Error analysis, Speech recognition, end-to-end, Generators, Speech processing, Contextual speech recognition, Hidden Markov models, language model discounting, minimum Bayes' risk, Pointer generator, End to end, Errors, Recognition error, Language model discounting, Long tail, Minimum bayes risk, Speech communication, Word error rate reductions},
	pages = {345--354},
	annote = {Cited by: 11; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{shaaban_cloudwot_2018,
	address = {New York, NY, USA},
	series = {{iiWAS2018}},
	title = {{CloudWoT} - {A} {Reference} {Model} for {Knowledge}-based {IoT} {Solutions}},
	isbn = {978-1-4503-6479-9},
	url = {https://doi.org/10.1145/3282373.3282400},
	doi = {10.1145/3282373.3282400},
	abstract = {Internet technology has changed how people work, live, communicate, learn and entertain. The internet adoption is rising rapidly, thus creating a new industrial revolution named "Industry 4.0". Industry 4.0 is the use of automation and data transfer in manufacturing technologies. It fosters several technological concepts, one of these is the Internet of Things (IoT). IoT technology is based on a big network of machines, objects, or people called "things" interacting together to achieve a common goal. These things are continuously generating vast amounts of data. Data understanding, processing, securing and storing are significant challenges in the IoT technology which restricts its development. This paper presents a new reference IoT model for future smart IoT solutions called Cloud Web of Things (CloudWoT). CloudWoT aims to overcome these limitations by combining IoT with edge computing, semantic web, and cloud computing. Additionally, this work is concerned with the security issues which threatens data in IoT application domains.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Information} {Integration} and {Web}-{Based} {Applications} \&amp; {Services}},
	publisher = {Association for Computing Machinery},
	author = {Shaaban, Abdelkader Magdy and Schmittner, Christoph and Gruber, Thomas and Mohamed, A. Baith and Quirchmayr, Gerald and Schikuta, Erich},
	year = {2018},
	note = {event-place: Yogyakarta, Indonesia},
	keywords = {Semantic Web, IoT, Edge Computing, Cloud Computing, CPPS, CloudWoT, IACS},
	pages = {272--281},
}

@inproceedings{powell_measuring_2024,
	address = {Santa Fe, New Mexico, USA},
	series = {{JCDL} '23},
	title = {Measuring the {Growth} of {Ideas} in a {Title} {Corpus}},
	isbn = {979-8-3503-9931-8},
	url = {https://doi.org/10.1109/JCDL57899.2023.00063},
	doi = {10.1109/JCDL57899.2023.00063},
	abstract = {Beyond bibliometrics, there is interest in characterizing the evolution of the number of ideas in scientific papers, or more generally, exploring the progress of science. We use various tokenization and phrase extraction strategies combined with lexical diversity metrics to analyze titles in our corpus. We compared four lexical diversity metrics for each corpora variants, to look for indications that new concepts might be emerging over time.},
	booktitle = {Proceedings of the 2023 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Powell, James and Balakireva, Lyudmila},
	year = {2024},
	note = {ISSN: 2575-8152},
	keywords = {Ontologies, Semantics, natural language processing, word embeddings, Measurement, Tokenization, Ecology, lexical diversity, science of science, Ecosystems, Libraries},
	pages = {291--292},
}

@inproceedings{mansar_finsim-2_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {The {FinSim}-2 2021 {Shared} {Task}: {Learning} {Semantic} {Similarities} for the {Financial} {Domain}},
	isbn = {978-1-4503-8313-4},
	url = {https://doi.org/10.1145/3442442.3451381},
	doi = {10.1145/3442442.3451381},
	abstract = {The FinSim-2 is a second edition of FinSim Shared Task on Learning Semantic Similarities for the Financial Domain, colocated with the FinWeb workshop. FinSim-2 proposed the challenge to automatically learn effective and precise semantic models for the financial domain. The second edition of the FinSim offered an enriched dataset in terms of volume and quality, and interested in systems which make creative use of relevant resources such as ontologies and lexica, as well as systems which make use of contextual word embeddings such as BERT[4]. Going beyond the mere representation of words is a key step to industrial applications that make use of Natural Language Processing (NLP). This is typically addressed using either unsupervised corpus-derived representations like word embeddings, which are typically opaque to human understanding but very useful in NLP applications or manually created resources such as taxonomies and ontologies, which typically have low coverage and contain inconsistencies, but provide a deeper understanding of the target domain. Finsim is inspired from previous endeavours in the Semeval community, which organized several competitions on semantic/lexical relation extraction between concepts/words. This year, 18 system runs were submitted by 7 teams and systems were ranked according to 2 metrics, Accuracy and Mean rank. All the systems beat our baseline 1 model by over 15 points and the best systems beat the baseline 2 by over 1 ∼ 3 points in accuracy.},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Mansar, Youness and Kang, Juyeon and Maarouf, Ismail El},
	year = {2021},
	note = {event-place: Ljubljana, Slovenia},
	keywords = {Natural Language Processing, Word embeddings, Domain specific ontology, Financial documents processing, Hypernym-hyponym relation extraction},
	pages = {288--292},
}

@inproceedings{giallonardo_making_2020,
	address = {New York, NY, USA},
	series = {{GoodTechs} '20},
	title = {Making {Smart} {Buildings} and {Personal} {Systems} {Cooperate} via {Knowledge} {Base} {Overlays}},
	isbn = {978-1-4503-7559-7},
	url = {https://doi.org/10.1145/3411170.3411261},
	doi = {10.1145/3411170.3411261},
	abstract = {Reactive IoT applications often have to deal with the data source Babel arising from their need to operate on context information originated from different data sources. Semantic knowledge bases can be fruitfully deployed to alleviate this problem: they provide a unified access point for context information including both long term (such as the structure of the environment) and transient (such as sensor readings) data thanks to their ability to host elements responding to different schemas within the same container. In previous works we introduced an architecture to create reactive IoT systems based on a semantic knowledge base that also hosts the definition of their behavior and on an accompanying reactive machinery. In this paper, we introduce the use of knowledge base overlays, i.e. containers providing a live, unified view over (parts of) different underlying knowledge bases, as a mechanism to enable interoperation between multiple IoT semantics-based systems. Specifically we explore the benefits of this approach in a case study in which a semantic IoT system governing a smart building interacts with the personal semantic systems of the people entering the building.},
	booktitle = {Proceedings of the 6th {EAI} {International} {Conference} on {Smart} {Objects} and {Technologies} for {Social} {Good}},
	publisher = {Association for Computing Machinery},
	author = {Giallonardo, Ester and Poggi, Francesco and Rossi, Davide and Zimeo, Eugenio},
	year = {2020},
	note = {event-place: Antwerp, Belgium},
	keywords = {Ontologies, Semantic Sensor Networks, Internet of Things (IoT), Context modeling, Context-awareness, Semantic modeling, Models@runtime, Reactive systems, Smart Environment, Smart moving},
	pages = {181--186},
}

@inproceedings{arshad_high_2019,
	address = {New York, NY, USA},
	series = {{UCC}'19},
	title = {High {Performance} {Dynamic} {Graph} {Model} for {Consistent} {Data} {Integration}},
	isbn = {978-1-4503-6894-0},
	url = {https://doi.org/10.1145/3344341.3368806},
	doi = {10.1145/3344341.3368806},
	abstract = {In a distributed environment, data from heterogeneous sources are brought together in a unified and consistent manner for analytics and insights. Inconsistencies arising due to the dynamic nature of sources such as addition/deletion of column or merging of columns can compromise the consistency of the distributed system. This can lead to the linking of inaccurate records and faulty data entries. Resulting in false reports and erroneous analyses. Furthermore, issues such as performance guarantees and scalability fuel the existing challenges. We have proposed an alternate graph-based approach to integrate data using an in-memory environment. The central idea of the approach is the use of graphs to integrate heterogeneous data sources in a distributed environment. The underlying approach provides both high-performance and scalability to address changes in a dynamic system for data integration. This allows the generation of graphs from individual source data and modifications in a consistent manner so that the state of the overall distributed system always remains coherent. It provides a novel way of combining consistent data integration and performance in a distributed sys-tem. Our system performs better than existing graph systems for dynamic graph evolution ensuring consistency and provides the necessary scalability guarantees as the size of the data increases. Results also show the correctness of the approach when integrating disparate data-sets},
	booktitle = {Proceedings of the 12th {IEEE}/{ACM} {International} {Conference} on {Utility} and {Cloud} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Arshad, Bilal and Anjum, Ashiq},
	year = {2019},
	note = {event-place: Auckland, New Zealand},
	keywords = {data integration, graphs, performance, scalability, consistency, dynamic graphs},
	pages = {263--272},
}

@inproceedings{taffa_bridge-generate_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {Bridge-{Generate}: {Scholarly} {Hybrid} {Question} {Answering}},
	isbn = {979-8-4007-1331-6},
	url = {https://doi.org/10.1145/3701716.3715459},
	doi = {10.1145/3701716.3715459},
	abstract = {Answering scholarly hybrid questions requires access to bibliographic facts stored in structured data, such as a Knowledge Graph (KG) and textual information. Existing Scholarly Hybrid Question Answering (SHQA) approaches rely on retrieving KG triples and documents from the Wikipedia text corpus and prompt an LLM (Large Language Model) for answers. However, the retrieval is heavily keyword-based, introducing noise into the context. Furthermore, despite detecting the entities in the question, the models do not attempt any question analysis. Therefore, we propose a new SHQA system that employs a bridge-generate approach. During the bridge phase, our system recursively identifies entity-encapsulating phrases within the question and resolves the entities leveraging the underlying KGs. It then formulates assertion statements based on the resolved entities and their corresponding phrases. In the generation phase, the system auto-generates context guided by the question and the assertions. Finally, it returns an answer prompting an LLM with the generated context, the assertions, and the question. Our approach outperforms previous approaches, addressing the identified gaps.},
	booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Taffa, Tilahun Abedissa and Usbeck, Ricardo},
	year = {2025},
	note = {event-place: Sydney NSW, Australia},
	keywords = {question answering, hybrid question answering, scholarly hybrid question answering, scholarly question answering},
	pages = {1321--1325},
}

@inproceedings{fadhlillah_managing_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {Managing {Variability} of {Cyber}-{Physical} {Production} {Systems}: {Towards} {Consistency} {Management}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688216},
	doi = {10.1145/3652620.3688216},
	abstract = {Engineering Cyber-Physical Production Systems (CPPSs) involves several different disciplines, where team members range from mechanical, electrical, and automation engineers, to control software engineers. When developing variability-intensive software systems such as CPPSs, engineers create heterogeneous engineering artifacts of varying granularity, structure, and level of abstraction in the problem and solution space, e.g., CAD drawings, delta models, and control software artifacts. Managing consistency among these heterogeneous artifacts is essential during the development and maintenance of these systems to reduce development costs and runtime failures. Software product line engineering provides approaches to manage the variability of heterogeneous artifacts. However, these approaches must be adapted and extended to manage consistency in CPPSs and address the additional multidimensional challenges in CPPSs. In this short paper, we outline these challenges, motivate them using a case study, and discuss potential solutions to manage the consistency of engineering artifacts expressing CPPS control software variability. We thereby lay the grounds for a deeper understanding of possible inconsistencies and exploring new methods for managing consistency in control software variability in CPPSs.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fadhlillah, Hafiyyan Sayyid and Greiner, Sandra and Feichtinger, Kevin and Rabiser, Rick and Zoitl, Alois},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {cyber-physical production systems engineering, heterogeneous multi-modeling, software modeling consistency},
	pages = {945--949},
}

@inproceedings{gong_multi-label_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Multi-{Label} {Zero}-{Shot} {Product} {Attribute}-{Value} {Extraction}},
	isbn = {979-8-4007-0171-9},
	url = {https://doi.org/10.1145/3589334.3645649},
	doi = {10.1145/3589334.3645649},
	abstract = {E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation. However, attribute value information is typically not available for new products. To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model. Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for graph nodes. Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes. This enables HyperPAVE to identify new attribute values without the need for labeled training data. We conduct extensive experiments with ablation studies on different categories of the MAVE dataset. The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based large language models for attribute value extraction in the zero-shot setting.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Gong, Jiaying and Eldardiry, Hoda},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {zero-shot learning, attribute value extraction, heterogeneous hypergraph, inductive link prediction},
	pages = {2259--2270},
}

@inproceedings{autili_model-driven_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {Model-driven adaptation of service choreographies},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167287},
	doi = {10.1145/3167132.3167287},
	abstract = {Service choreographies represent a powerful and flexible approach to compose software services in a fully distributed way. A key enabler for the actual realization of choreographies is the ability to automatically compose services, and perform exogenous coordination and adaptation of their interaction. This is a nontrivial and error prone task. Automatic support for realizing choreographies is needed. In this paper we focus on adapter generation and describe our novel approach to the synthesis of service Adapters. When needed, adapters permit to correctly bind concrete services to (abstract) choreography roles by solving possible protocol mismatches. Enterprise Integration Patterns are used as adaptation primitives and composed to realize complex adaptation policies.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Autili, Marco and Di Salle, Amleto and Gallo, Francesco and Pompilio, Claudio and Tivoli, Massimo},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {adaptation, model-driven, enterprise integration pattern, service choreography, service-oriented computing},
	pages = {1441--1450},
}

@inproceedings{van_drie_dutch_2023,
	address = {New York, NY, USA},
	series = {{ICAIL} '23},
	title = {The {Dutch} {Law} as a {Semantic} {Role} {Labeling} {Dataset}},
	isbn = {979-8-4007-0197-9},
	url = {https://doi.org/10.1145/3594536.3595124},
	doi = {10.1145/3594536.3595124},
	abstract = {Legal documents, and specifically law texts, are not easy to understand by humans. The specific terminology and sentence constructions are particular, which also makes it a difficult machine understanding task. In this paper, we present a publicly available benchmark dataset containing Dutch law texts which can be used to train AI models that assist humans equipped with the task of interpreting legal texts. However, the dataset can be used in a broader context, such as semantic role labeling of Dutch (legal) texts. Our dataset contains 4463 annotated sentences from 55 different Dutch laws, in which four roles are annotated by human annotators: action, actor, object and recipient. The inter-annotator agreement is substantial (κ=0.75). In experiments with a rule-based and a transformer-based method, results show that the transformer-based method performs quite well on the dataset (accuracy \&gt; 0.8). These results indicate that we can reliably predict actions, actors, objects and recipients in legal texts. This can help people equipped with the task of formal interpretation of legal texts.},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {van Drie, Romy A. N. and de Boer, Maaike H. T. and Bakker, Roos M. and Tolios, Ioannis and Vos, Daan},
	year = {2023},
	note = {event-place: Braga, Portugal},
	keywords = {natural language processing, datasets, legal interpretation, legal text, norms, semantic role labeling},
	pages = {316--322},
}

@article{garcia_semantics_2024,
	title = {Semantics and {Non}-fungible {Tokens} for {Copyright} {Management} on the {Metaverse} and {Beyond}},
	volume = {20},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3585387},
	doi = {10.1145/3585387},
	abstract = {Recent initiatives related to the Metaverse focus on better visualization, like augmented or virtual reality, but also persistent digital objects. To guarantee real ownership of these digital objects, open systems based on public blockchains and Non-Fungible Tokens (NFTs) are emerging together with a nascent decentralized and open creator economy. To manage this emerging economy in a more organized way, and fight the so common NFT plagiarism, we propose CopyrightLY, a decentralized application for authorship and copyright management. It provides means to claim content authorship, including supporting evidence. Content and metadata are stored in decentralized storage and registered on the blockchain. A token is used to curate these claims, and potential complaints, by staking it on them. Staking is incentivized by the fact that the token is minted using a bonding curve. The tokenomics include the resolution of complaints and enabling the monetization of curated claims. Monetization is achieved through licensing NFTs with metadata enhanced by semantic technologies. Semantic data makes explicit the reuse conditions transferred with the token while keeping the connection to the underlying copyright claims to improve the trustability of the NFTs. Moreover, the semantic metadata is flexible enough to enable licensing not just in the real world. Licenses can refer to reuses in specific locations in a metaverse, thus facilitating the emergence of creative economies in them.},
	number = {7},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {García, Roberto and Cediel, Ana and Teixidó, Mercè and Gil, Rosa},
	month = mar,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ontology, Metaverse, social media, blockchain, copyright, Non-Fungible Token},
}

@article{russo_meta-model_2018,
	title = {A {Meta}-{Model} for {Information} {Systems} {Quality}: {A} {Mixed} {Study} of the {Financial} {Sector}},
	volume = {9},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3230713},
	doi = {10.1145/3230713},
	abstract = {Information Systems Quality (ISQ) is a critical source of competitive advantages for organizations. In a scenario of increasing competition on digital services, ISQ is a competitive differentiation asset. In this regard, managing, maintaining, and evolving IT infrastructures have become a primary concern of organizations. Thus, a technical perspective on ISQ provides useful guidance to meet current challenges. The financial sector is paradigmatic, since it is a traditional business, with highly complex business-critical legacy systems, facing a tremendous change due to market and regulation drivers. We carried out a Mixed-Methods study, performing a Delphi-like study on the financial sector. We developed a specific research framework to pursue this vertical study. Data were collected in four phases starting with a high-level randomly stratified panel of 13 senior managers and then a target panel of 124 carefully selected and well-informed domain experts. We have identified and dealt with several quality factors; they were discussed in a comprehensive model inspired by the ISO 25010, 42010, and 12207 standards, corresponding to software quality, software architecture, and software process, respectively. Our results suggest that the relationship among quality, architecture, and process is a valuable technical perspective to explain the quality of an information system. Thus, we introduce and illustrate a novel meta-model, named SQuAP (Software Quality, Architecture, Process), which is intended to give a comprehensive picture of ISQ by abstracting and connecting detailed individual ISO models.},
	number = {3},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Russo, Daniel and Ciancarini, Paolo and Falasconi, Tommaso and Tomasi, Massimo},
	month = sep,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {software architecture, delphi study, Information systems quality, management information systems, mixed methods, software process, software quality},
}

@inproceedings{aicher_influence_2023,
	address = {New York, NY, USA},
	series = {{IVA} '23},
	title = {The {Influence} of {Avatar} {Interfaces} on {Argumentative} {Dialogues}},
	isbn = {978-1-4503-9994-4},
	url = {https://doi.org/10.1145/3570945.3607343},
	doi = {10.1145/3570945.3607343},
	abstract = {Humans form opinions and justify different points of view by exchanging arguments and knowledge. Likewise to human-human interaction, the way arguments are presented influence the user's willingness to engage into a critical reflection. Especially when interacting with conversational agents the user's engagement and motivation are important factors and highly influence the success or failure of such a mixed team. To maintain the users' trust and satisfaction, the users' perception of the respective system is an important indicator. Thus, this work investigates the design of a cooperative argumentative dialogue system using a virtual avatar compared to a non-avatar interface by evaluating a crowdsourcing study conducted with 84 participants. The results indicate, that the avatar system is perceived as significantly more appealing and natural and thus, engaging which also influences the acceptance and perception of the quality of presented arguments. Furthermore, we found that the presence of the avatar often led to an increase in the anticipated level of conversational proficiency similar to that of a human interlocutor. Therefore, this work provides important insights for the design of future cooperative argumentative virtual avatar interfaces.},
	booktitle = {Proceedings of the 23rd {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author = {Aicher, Annalena and Weber, Klaus and André, Elisabeth and Minker, Wolfgang and Ultes, Stefan},
	year = {2023},
	note = {event-place: Würzburg, Germany},
	keywords = {Human-Computer Interaction, Avatar Interface, Conversational Engagement, Crowdsourcing Study, User Trust},
}

@inproceedings{bertmark_artefacts_2025,
	address = {New York, NY, USA},
	series = {{TEI} '25},
	title = {Artefacts as {Pedagogy} for {Futuring}},
	isbn = {979-8-4007-1197-8},
	url = {https://doi.org/10.1145/3689050.3704801},
	doi = {10.1145/3689050.3704801},
	abstract = {This PhD explores the potential of interactive artefacts as tangible pedagogy for generating perspective shifts towards ecocentric thinking and doing. Research has highlighted how deliberate societal transformative change is required to reach the targets of sustainability goals and how perspective shifts for instigating these changes may be supported and achieved. The research investigates how design and HCI seek to challenge anthropocentric approaches and perspectives, motivated by the growing number of works in technology for the more-than-human and feminist post-humanism. Speculative, more-than-human design and sustainable HCI highlight ecosystem interdependence through transmedia narratives and interactive ecocentric artefacts. However, their inaccessibility and unexplored impact limit their potential to bridge the knowledge-action gap for local development to stay within necessary earth-system boundaries. This paper emphasises the necessity for perceiving how interactive artefacts generate epistemological and ontological shifts and how this may be utilised to advise future design towards ethics of care and regenerative development.},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Tangible}, {Embedded}, and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Bertmark, Anna My},
	year = {2025},
	keywords = {Design ontology, Embodied learning, Ecocentric design, Ecological HCI, Futuring, Interaction for sustainability},
}

@inproceedings{ishii_multimodal_2021,
	address = {New York, NY, USA},
	series = {{IVA} '21},
	title = {Multimodal and {Multitask} {Approach} to {Listener}'s {Backchannel} {Prediction}: {Can} {Prediction} of {Turn}-changing and {Turn}-management {Willingness} {Improve} {Backchannel} {Modeling}?},
	isbn = {978-1-4503-8619-7},
	url = {https://doi.org/10.1145/3472306.3478360},
	doi = {10.1145/3472306.3478360},
	abstract = {The listener's backchannel has the important function of encouraging a current speaker to hold their turn and continue to speak, which enables smooth conversation. The listener monitors the speaker's turn-management (a.k.a. speaking and listening) willingness and his/her own willingness to display backchannel behavior. Many studies have focused on predicting the appropriate timing of the backchannel so that conversational agents can display backchannel behavior in response to a user who is speaking. To the best of our knowledge, none of them added the prediction of turn-changing and participants' turn-management willingness to the backchannel prediction model in dyad interactions. In this paper, we proposed a novel backchannel prediction model that can jointly predict turn-changing and turn-management willingness. We investigated the impact of modeling turn-changing and willingness to improve backchannel prediction. Our proposed model is based on trimodal inputs, that is, acoustic, linguistic, and visual cues from conversations. Our results suggest that adding turn-management willingness as a prediction task improves the performance of backchannel prediction within the multi-modal multi-task learning approach, while adding turn-changing prediction is not useful for improving the performance of backchannel prediction.},
	booktitle = {Proceedings of the 21st {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author = {Ishii, Ryo and Ren, Xutong and Muszynski, Michal and Morency, Louis-Philippe},
	year = {2021},
	note = {event-place: Virtual Event, Japan},
	keywords = {multimodal signal processing, multitask learning, turn-management willingness, backchannel, turn-changing},
	pages = {131--138},
}

@article{anwar_social_2023,
	title = {Social {Relationship} {Analysis} {Using} {State}-of-the-art {Embeddings}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3539608},
	doi = {10.1145/3539608},
	abstract = {Detection of human relationships from their interactions on social media is a challenging problem with a wide range of applications in different areas, like targeted marketing, cyber-crime, fraud, defense, planning, and human resource, to name a few. All previous work in this area has only dealt with the most basic types of relationships. The proposed approach goes beyond the previous work to efficiently handle the hierarchy of social relationships. This article introduces a novel technique named Quantifiable Social Relationship (QSR) analysis for quantifying social relationships to analyze relationships between agents from their textual conversations. QSR uses cross-disciplinary techniques from computational linguistics and cognitive psychology to identify relationships. QSR utilizes sentiment and behavioral styles displayed in the conversations for mapping them onto level II relationship categories. Then, for identifying the level III relationship categories, QSR uses level II relationships, sentiments, interactions, and word embeddings as key features. QSR employs natural language processing techniques for feature engineering and state-of-the-art embeddings generated by word2vec, global vectors (glove), and bidirectional encoder representations from transformers (bert). QSR combines the intrinsic conversational features with word embeddings for classifying relationships. QSR achieves an accuracy of up to 89\% for classifying relationship subtypes. The evaluation shows that QSR can accurately identify the hierarchical relationships between agents by extracting intrinsic and extrinsic features from textual conversations between agents.},
	number = {5},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Anwar, Sibgha and Beg, Mirza Omer and Saleem, Kiran and Ahmed, Zeeshan and Javed, Abdul Rehman and Tariq, Usman},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {machine learning, Agents interaction model, behavioral model, hierarchical relationship analysis, quantifiable relationships, social relationship},
}

@inproceedings{kumarasinghe_semantic_2024,
	address = {New York, NY, USA},
	series = {{ASIA} {CCS} '24},
	title = {Semantic {Ranking} for {Automated} {Adversarial} {Technique} {Annotation} in {Security} {Text}},
	isbn = {979-8-4007-0482-6},
	url = {https://doi.org/10.1145/3634737.3645000},
	doi = {10.1145/3634737.3645000},
	abstract = {We introduce a novel approach for mapping attack behaviors described in threat analysis reports to entries in an adversarial techniques knowledge base. Our method leverages a multi-stage ranking architecture to efficiently rank the most related techniques based on their semantic relevance to the input text. Each ranker in our pipeline uses a distinct design for text representation. To enhance relevance modeling, we leverage pretrained language models, which we fine-tune for the technique annotation task. While generic large language models are not yet capable of fully addressing this challenge, we obtain very promising results. We achieve a recall rate improvement of +35\% compared to the previous state-of-the-art results. We further create new public benchmark datasets for training and validating methods in this domain, which we release to the research community aiming to promote future research in this important direction.},
	booktitle = {Proceedings of the 19th {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Kumarasinghe, Udesh and Lekssays, Ahmed and Sencar, Husrev Taha and Boughorbel, Sabri and Elvitigala, Charitha and Nakov, Preslav},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {threat intelligence, text attribution, text ranking, TTP annotation},
	pages = {49--62},
}

@inproceedings{de_berardinis_harmonic_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23},
	title = {The {Harmonic} {Memory}: a {Knowledge} {Graph} of harmonic patterns as a trustworthy framework for computational creativity},
	isbn = {978-1-4503-9416-1},
	url = {https://doi.org/10.1145/3543507.3587428},
	doi = {10.1145/3543507.3587428},
	abstract = {Computationally creative systems for music have recently achieved impressive results, fuelled by progress in generative machine learning. However, black-box approaches have raised fundamental concerns for ethics, accountability, explainability, and musical plausibility. To enable trustworthy machine creativity, we introduce the Harmonic Memory, a Knowledge Graph (KG) of harmonic patterns extracted from a large and heterogeneous musical corpus. By leveraging a cognitive model of tonal harmony, chord progressions are segmented into meaningful structures, and patterns emerge from their comparison via harmonic similarity. Akin to a music memory, the KG holds temporal connections between consecutive patterns, as well as salient similarity relationships. After demonstrating the validity of our choices, we provide examples of how this design enables novel pathways for combinational creativity. The memory provides a fully accountable and explainable framework to inspire and support creative professionals – allowing for the discovery of progressions consistent with given criteria, the recomposition of harmonic sections, but also the co-creation of new progressions.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {de Berardinis, Jacopo and Meroño-Peñuela, Albert and Poltronieri, Andrea and Presutti, Valentina},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {knowledge graphs, computational creativity, music technology},
	pages = {3873--3882},
}

@inproceedings{izo_intelligent_2023,
	address = {New York, NY, USA},
	series = {{SBSI} '23},
	title = {An {Intelligent} {Report} {Generator} for {Chemical} {Documents}},
	isbn = {979-8-4007-0759-9},
	url = {https://doi.org/10.1145/3592813.3592915},
	doi = {10.1145/3592813.3592915},
	abstract = {Context: Scientific articles and patents contain academic, industrial, and scientific information. Automatically retrieving information from these documents is necessary for supporting upcoming scientific research development. Problem: Difficulties in manually identifying and analyzing the chemical information in documents make it nearly impossible to access specific contents of chemical investigations and generate reports to support ongoing research. Solution: In this article, we present a system that recognizes chemical entities (elements, classes, compounds, methods, and equipment) and generates intelligent reports from free texts. IS Theory: We developed this work under the support of Soft Systems Theory. Method: This research was evaluated through proof of concept. We used 30 chemical patents from Brazilian National Institute of Industrial Property and 20 scientific articles from Revista Virtual de Química (RVq). For validation, we extracted the texts and recognized the named entities through, for instance, the hybrid method Conditional Random Field (CRF) + Local Grammar (LG). We then apply rules to generate intelligent reports. Summary of Results: The system can generate seven types of intelligent reports, two of which are customized by the user. For datasetPat our model obtained mean values of 98.96\% for Precision, 91.12\% for Recall, and 94.17\% for F-Score. The datasetArt reached average values of 97.31\%, 86.94\%, and 91.29\% for Precision, Recall, and F-Score, respectively. Contributions and Impact in the IS Area: This research presents as the main contribution the availability of an Information System for the generation of intelligent reports from documents based on the recognition of named entities in the chemical area. In addition the hybrid method CRF+LG can contribute to the evolution of Information Systems, helping people and organizations. The model is described throughout the paper and can be replicated in other contexts.},
	booktitle = {Proceedings of the {XIX} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Izo, Flavio and Vereau, Luis Enrique Santos Prado and Pirovani, Juliana Pinheiro Campos and Oliveira, Elias and Badue, Claudine},
	year = {2023},
	note = {event-place: Maceió, Brazil},
	keywords = {Natural Language Processing, Artificial Intelligence, Named Entity Recognition, Intelligent Report.},
	pages = {276--283},
}

@inproceedings{kanwal_attention-based_2022,
	address = {New York, NY, USA},
	series = {{SAC} '22},
	title = {Attention-based clinical note summarization},
	isbn = {978-1-4503-8713-2},
	url = {https://doi.org/10.1145/3477314.3507256},
	doi = {10.1145/3477314.3507256},
	abstract = {In recent years, the trend of deploying digital systems in numerous industries has hiked. The health sector has observed an extensive adoption of digital systems and services that generate significant medical records. Electronic health records contain valuable information for prospective and retrospective analysis that is often not entirely exploited because of the complicated dense information storage. The crude purpose of condensing health records is to select the information that holds most characteristics of the original documents based on a reported disease. These summaries may boost diagnosis and save a doctor's time during a saturated workload situation like the COVID-19 pandemic. In this paper, we are applying a multi-head attention-based mechanism to perform extractive summarization of meaningful phrases on clinical notes. Our method finds major sentences for a summary by correlating tokens, segments, and positional embeddings of sentences in a clinical note. The model outputs attention scores that are statistically transformed to extract critical phrases for visualization on the heat-mapping tool and for human use.},
	booktitle = {Proceedings of the 37th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Kanwal, Neel and Rizzo, Giuseppe},
	year = {2022},
	note = {event-place: Virtual Event},
	keywords = {natural language processing, deep learning, clinical notes, electronic health records, medical records, transformer models, MIMIC-III, information extraction, extractive summarization, ICD-9, multi-head attention},
	pages = {813--820},
}

@inproceedings{r_entailment_2021,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '21},
	title = {An {Entailment} {Analysis} {Based} {Entity} {Mapping} {To} {Improve} {Automatic} {Question} {Generation}},
	isbn = {978-1-4503-8817-7},
	url = {https://doi.org/10.1145/3430984.3431049},
	doi = {10.1145/3430984.3431049},
	abstract = {Automatic generation of assessment questions to enhance learning requires natural language understanding. To enhance the process, the entities extracted from the subject domain, may be mapped to form knowledge graphs. But entity mapping faces the challenge of analysing entailment. The proposed work analyses the semantic relevance in the process and aims at generating heuristics to map entities.},
	booktitle = {Proceedings of the 3rd {ACM} {India} {Joint} {International} {Conference} on {Data} {Science} \&amp; {Management} of {Data} (8th {ACM} {IKDD} {CODS} \&amp; 26th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {R, Tharaniya Sairaj and S. R., Balasundaram},
	year = {2021},
	note = {event-place: Bangalore, India},
	keywords = {Ontology, Natural Language Understanding, E-Assessment, Entailment Analysis, Entity Mapping},
	pages = {424},
}

@inproceedings{dang_efficient_2025,
	address = {New York, NY, USA},
	series = {{ICIIT} '25},
	title = {Efficient {Object} {Detection} {Using} {Total} {Energy} {Function}: {An} {Alternative} to {Anchor} {Box}},
	isbn = {979-8-4007-1084-1},
	url = {https://doi.org/10.1145/3731763.3731782},
	doi = {10.1145/3731763.3731782},
	abstract = {Bounding box regression is a popular technique for fine-tuning or predicting the localization boxes in object detection methods that are trained to regress from proposed regions or fixed anchor boxes to bounding boxes. But anchor-based methods face computational costs and challenges in optimal anchor configuration, which constrain their adaptability and performance. This paper proposes an anchor-free object detection method using multi-component energy optimization that integrates edge, region, size, and shape information to determine the optimal bounding box parameters. Experiments on PASCAL VOC 2012 show superior performance with 85.7\% loss reduction for small objects and 3-4 times faster convergence speed, demonstrating the effectiveness of the energy optimization method.},
	booktitle = {Proceedings of the 2025 10th {International} {Conference} on {Intelligent} {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Dang, Dung Thi and Nguyen, Khoi Tan and Huynh, Hiep Xuan},
	year = {2025},
	keywords = {Object Detection, Anchor-free Detection, Bounding Box Optimization, Energy-Based Models, Multi-component Energy Function},
	pages = {28--34},
}

@inproceedings{pinem_developing_2023,
	address = {New York, NY, USA},
	series = {{IC3INA} '22},
	title = {Developing {Semantic} {Annotation} {Representation} of {Social} {Media} {Sentiments} and {Metadata} as {Resource} {Description} {Framework}: {A} {Study} of {Indonesian} {New} {Capital} {Related} {Tweets} {Written} in {Bahasa}},
	isbn = {978-1-4503-9790-2},
	url = {https://doi.org/10.1145/3575882.3575926},
	doi = {10.1145/3575882.3575926},
	abstract = {Social Media has become a tool abiding the press in this modern society. Everyone can write their minds and build their mass media to publish opinions. Thus, in this manuscript, we develop a resource description framework scheme (RDFS) to enrich the information and metadata from Indonesian tweets regarding their New Capitol. This work focused on applying a popular method (i.e., the Tweetskb scheme) to construct the RDF of those tweets. We also developed the Schema to fulfill our need to contain all the information to RDF. RDF Triples were generated by connecting several established vocabularies to ensure the connection between its related nodes has meaning. The sentiment polarity (i.e., neutral, positive, and negative sentiment) is used in this manuscript. Thus, our proposal can be used as an initial work to make use of twitter's metadata to predict how reliable a user is, how the community interact with a certain topic, spam detection, clustering, and even implementing machine learning and deep learning sentiment analysis in a manner of knowledge graph.},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Computer}, {Control}, {Informatics} and {Its} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Pinem, Josua Geovani and Septiadi, Agung and Shaleha, Siti and Alfin, Muhammad Reza and Subekti, Aulia Haritsuddin Karisma Muhammad and Muliadi, Jemie and Wibowanto, Gembong and Santosa, Agung and Uliniansyah, M. Teduh and Jarin, Asril and Latief, Andi Djalal and {Gunarso} and Riza, Hammam},
	year = {2023},
	note = {event-place: Virtual Event, Indonesia},
	keywords = {Ontology, Natural language processing, Knowledge Graph, Twitter, Sentiment Analysis RDF/S},
	pages = {229--234},
}

@inproceedings{frezza_modelling_2018,
	address = {New York, NY, USA},
	series = {{ITiCSE} 2018 {Companion}},
	title = {Modelling competencies for computing education beyond 2020: a research based approach to defining competencies in the computing disciplines},
	isbn = {978-1-4503-6223-8},
	url = {https://doi.org/10.1145/3293881.3295782},
	doi = {10.1145/3293881.3295782},
	abstract = {How might the content and outcomes of tertiary education programmes be described and analysed in order to understand how they are structured and function? To address this question we develop a framework for modelling graduate competencies linked to tertiary degree programmes in the computing disciplines. While the focus of our work is computing the framework is applicable to education more broadly. The work presented here draws upon the pioneering curricular document for information technology (IT2017), curricular competency frameworks, other related documents such as the software engineering competency model (SWECOM), the Skills Framework for the Information Age (SFIA), current research in competency models, and elicitation workshop results from recent computing conferences. The aim is to inform the ongoing Computing Curricula (CC2020) project, an endeavour supported by the Association for Computing Machinery (ACM) and the IEEE Computer Society. We develop the Competency Learning Framework (CoLeaF), providing an internationally relevant tool for describing competencies. We argue that this competency based approach is well suited for constructing learning environments and assists degree programme architects in dealing with the challenge of developing, describing and including competencies relevant to computer and IT professionals. In this paper we demonstrate how the CoLeaF competency framework can be applied in practice, and though a series of case studies demonstrate its effectiveness and analytical power as a tool for describing and comparing degree programmes in the international higher education landscape.},
	booktitle = {Proceedings {Companion} of the 23rd {Annual} {ACM} {Conference} on {Innovation} and {Technology} in {Computer} {Science} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Frezza, Stephen and Daniels, Mats and Pears, Arnold and Cajander, Åsa and Kann, Viggo and Kapoor, Amanpreet and McDermott, Roger and Peters, Anne-Kathrin and Sabin, Mihaela and Wallace, Charles},
	year = {2018},
	note = {event-place: Larnaca, Cyprus},
	keywords = {CC2020, Computing competencies, curriculum guidelines, Professional competencies},
	pages = {148--174},
}

@inproceedings{benjamin_entoptic_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {The {Entoptic} {Field} {Camera} as {Metaphor}-{Driven} {Research}-through-{Design} with {AI} {Technologies}},
	isbn = {978-1-4503-9421-5},
	url = {https://doi.org/10.1145/3544548.3581175},
	doi = {10.1145/3544548.3581175},
	abstract = {Artificial intelligence (AI) technologies are widely deployed in smartphone photography; and prompt-based image synthesis models have rapidly become commonplace. In this paper, we describe a Research-through-Design (RtD) project which explores this shift in the means and modes of image production via the creation and use of the Entoptic Field Camera. Entoptic phenomena usually refer to perceptions of floaters or bright blue dots stemming from the physiological interplay of the eye and brain. We use the term entoptic as a metaphor to investigate how the material interplay of data and models in AI technologies shapes human experiences of reality. Through our case study using first-person design and a field study, we offer implications for critical, reflective, more-than-human and ludic design to engage AI technologies; the conceptualisation of an RtD research space which contributes to AI literacy discourses; and outline a research trajectory concerning materiality and design affordances of AI technologies.},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Benjamin, Jesse Josua and Biggs, Heidi and Berger, Arne and Rukanskaitundefined, Julija and Heidt, Michael B. and Merrill, Nick and Pierce, James and Lindley, Joseph},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {artificial intelligence, GAN, image synthesis, materiality, research through design, technological mediation},
}

@article{asprino_knowledge_2023,
	title = {Knowledge {Graph} {Construction} with a {Façade}: {A} {Unified} {Method} to {Access} {Heterogeneous} {Data} {Sources} on the {Web}},
	volume = {23},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3555312},
	doi = {10.1145/3555312},
	abstract = {Data integration is the dominant use case for RDF Knowledge Graphs. However, Web resources come in formats with weak semantics (for example, CSV and JSON), or formats specific to a given application (for example, BibTex, HTML, and Markdown). To solve this problem, Knowledge Graph Construction (KGC) is gaining momentum due to its focus on supporting users in transforming data into RDF. However, using existing KGC frameworks result in complex data processing pipelines, which mix structural and semantic mappings, whose development and maintenance constitute a significant bottleneck for KG engineers. Such frameworks force users to rely on different tools, sometimes based on heterogeneous languages, for inspecting sources, designing mappings, and generating triples, thus making the process unnecessarily complicated. We argue that it is possible and desirable to equip KG engineers with the ability of interacting with Web data formats by relying on their expertise in RDF and the well-established SPARQL query language\&nbsp;[2]. In this article, we study a unified method for data access to heterogeneous data sources with Facade-X, a meta-model implemented in a new data integration system called SPARQL Anything. We demonstrate that our approach is theoretically sound, since it allows a single meta-model, based on RDF, to represent data from (a) any file format expressible in BNF syntax, as well as (b) any relational database. We compare our method to state-of-the-art approaches in terms of usability (cognitive complexity of the mappings) and general performance. Finally, we discuss the benefits and challenges of this novel approach by engaging with the reference user community.},
	number = {1},
	journal = {ACM Trans. Internet Technol.},
	author = {Asprino, Luigi and Daga, Enrico and Gangemi, Aldo and Mulholland, Paul},
	month = feb,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {SPARQL, RDF, meta-model, re-engineering},
}

@inproceedings{ferrario_trustworthiness-based_2025,
	address = {New York, NY, USA},
	series = {{FAccT} '25},
	title = {A {Trustworthiness}-based {Metaphysics} of {Artificial} {Intelligence} {Systems}},
	isbn = {979-8-4007-1482-5},
	url = {https://doi.org/10.1145/3715275.3732091},
	doi = {10.1145/3715275.3732091},
	abstract = {Modern AI systems are man-made objects that leverage machine learning to support our lives across a myriad of contexts and applications. Despite extensive epistemological and ethical debates, their metaphysical foundations remain relatively under explored. The orthodox view simply suggests that AI systems, as artifacts, lack well-posed identity and persistence conditions—their metaphysical kinds are no real kinds. In this work, we challenge this perspective by introducing a theory of metaphysical identity of AI systems. We do so by characterizing their kinds and introducing identity criteria—formal rules that answer the questions “When are two AI systems the same?” and “When does an AI system persist, despite change?” Building on Carrara and Vermaas’ account of fine-grained artifact kinds, we argue that AI trustworthiness provides a lens to understand AI system kinds and formalize the identity of these artifacts by relating their functional requirements to their physical make-ups. The identity criteria of AI systems are determined by their trustworthiness profiles—the collection of capabilities that the systems must uphold over time throughout their artifact histories, and their effectiveness in maintaining these capabilities. Our approach suggests that the identity and persistence of AI systems is sensitive to the socio-technical context of their design and utilization via their trustworthiness, providing a solid metaphysical foundation to the epistemological, ethical, and legal discussions about these artifacts.},
	booktitle = {Proceedings of the 2025 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Ferrario, Andrea},
	year = {2025},
	keywords = {artificial intelligence, machine learning, ontology, deep learning, time, identity, metaphysics, change},
	pages = {1360--1370},
}

@inproceedings{skorupa_parolin_multi-coped_2022,
	address = {New York, NY, USA},
	series = {{AIES} '22},
	title = {Multi-{CoPED}: {A} {Multilingual} {Multi}-{Task} {Approach} for {Coding} {Political} {Event} {Data} on {Conflict} and {Mediation} {Domain}},
	isbn = {978-1-4503-9247-1},
	url = {https://doi.org/10.1145/3514094.3534178},
	doi = {10.1145/3514094.3534178},
	abstract = {Political and social scientists monitor, analyze and predict political unrest and violence, preventing (or mitigating) harm, and promoting the management of global conflict. They do so using event coder systems, which extract structured representations from news articles to design forecast models and event-driven continuous monitoring systems. Existing methods rely on expensive manual annotated dictionaries and do not support multilingual settings. To advance the global conflict management, we propose a novel model, Multi-CoPED (Multilingual Multi-Task Learning BERT for Coding Political Event Data), by exploiting multi-task learning and state-of-the-art language models for coding multilingual political events. This eliminates the need for expensive dictionaries by leveraging BERT models' contextual knowledge through transfer learning. The multilingual experiments demonstrate the superiority of Multi-CoPED over existing event coders, improving the absolute macro-averaged F1-scores by 23.3\% and 30.7\% for coding events in English and Spanish corpus, respectively. We believe that such expressive performance improvements can help to reduce harms to people at risk of violence.},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Skorupa Parolin, Erick and Hosseini, MohammadSaleh and Hu, Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio, Javier and D'Orazio, Vito},
	year = {2022},
	note = {event-place: Oxford, United Kingdom},
	keywords = {natural language processing, transfer learning, artificial intelligence and geopolitics, event coding, political conflict, social conflict},
	pages = {700--711},
}

@article{levy_semantic_2023,
	title = {Semantic computing with {IEML}},
	volume = {2},
	url = {https://doi.org/10.1177/26339137231207634},
	doi = {10.1177/26339137231207634},
	abstract = {This paper presents IEML, Information Economy MetaLanguage, a constructed language with the same expressive power as a natural language and with computable semantics. Distinguished from pragmatic and referential semantics, linguistic semantics have not yet been completely formalized. Only its syntagmatic dimension has been mathematized in the form of regular languages. Its paradigmatic dimension remained to be formalized. In order to complete the mathematizing of language, including its paradigmatic dimension, I have coded linguistic semantics with IEML. This article introduces its 3000-word dictionary, its formal grammar, and its integrated tools for building semantic graphs. For the future, IEML could become a vector for a fluid calculation and communication of meaning—semantic interoperability—capable of de-compartmentalizing the digital memory, and of advancing the progress of collective intelligence, artificial intelligence, and digital humanities. I conclude by indicating some research directions.},
	number = {4},
	journal = {Collective Intelligence},
	author = {Lévy, Pierre},
	month = nov,
	year = {2023},
	note = {Place: USA
Publisher: Sage Publications, Inc.},
	keywords = {artificial intelligence, semantics, Linguistics, collective intelligence, Ieml},
}

@inproceedings{zhan_socialdial_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {{SocialDial}: {A} {Benchmark} for {Socially}-{Aware} {Dialogue} {Systems}},
	isbn = {978-1-4503-9408-6},
	url = {https://doi.org/10.1145/3539618.3591877},
	doi = {10.1145/3539618.3591877},
	abstract = {Content Warning: this paper may contain content that is offensive or upsetting.Dialogue systems have been widely applied in many scenarios and are now more powerful and ubiquitous than ever before. With large neural models and massive available data, current dialogue systems have access to more knowledge than any people in their life. However, current dialogue systems still do not perform at a human level. One major gap between conversational agents and humans lies in their abilities to be aware of social norms. The development of socially-aware dialogue systems is impeded due to the lack of resources. In this paper, we present the first socially-aware dialogue corpus – SocialDial based on Chinese social culture. SocialDial consists of two parts: 1,563 multi-turn dialogues between two human speakers with fine-grained labels, and 4,870 synthetic conversations generated by ChatGPT. The human corpus covers five categories of social norms, which have 14 sub-categories in total. Specifically, it contains social factor annotations including social relation, context, social distance, and social norms. However, collecting sufficient socially-aware dialogues is costly. Thus, we harness the power of ChatGPT and devise an ontology-based synthetic data generation framework. This framework is able to generate synthetic data at scale. To ensure the quality of synthetic dialogues, we design several mechanisms for quality control during data collection. Finally, we evaluate our dataset using several pre-trained models, such as BERT and RoBERTa. Comprehensive empirical results based on state-of-the-art neural models demonstrate that modeling of social norms for dialogue systems is a promising research direction. To the best of our knowledge, SocialDial is the first socially-aware dialogue dataset that covers multiple social factors and has fine-grained labels.},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhan, Haolan and Li, Zhuang and Wang, Yufei and Luo, Linhao and Feng, Tao and Kang, Xiaoxi and Hua, Yuncheng and Qu, Lizhen and Soon, Lay-Ki and Sharma, Suraj and Zukerman, Ingrid and Semnani-Azad, Zhaleh and Haffari, Gholamreza},
	year = {2023},
	note = {event-place: Taipei, Taiwan},
	keywords = {datasets, social norms, socially-aware dialogue},
	pages = {2712--2722},
}

@article{debruyne_creating_2022,
	title = {Creating a {Knowledge} {Graph} for {Ireland}’s {Lost} {History}: {Knowledge} {Engineering} and {Curation} in the {Beyond} 2022 {Project}},
	volume = {15},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3474829},
	doi = {10.1145/3474829},
	abstract = {The Beyond 2022 project aims to create a virtual archive by digitally reconstructing and digitizing historical records lost in a catastrophic fire which consumed items in the Public Record Office of Ireland in 1922. The project is developing a knowledge graph (KG) to facilitate information retrieval and discovery over the reconstructed items. The project decided to adopt Semantic Web technologies to support its distributed KG and reasoning. In this article, we present our approach to KG generation and management. We elaborate on how we help historians contribute to the KG (via a suite of spreadsheets) and its ontology. We furthermore demonstrate how we use named graphs to store different versions of factoids and their provenance information and how these are serviced in two different endpoints. Modeling data in this manner allows us to acknowledge that history is, to some extent, subjective and different perspectives can exist in parallel. The construction of the KG is driven by competency questions elicited from subject matter experts within the consortium. We avail of CIDOC-CRM as our KG’s foundation, though we needed to extend this ontology with various qualifiers (types) and relations to support the competency questions. We illustrate how one can explore the KG to gain insights and answer questions. We conclude that CIDOC-CRM provides an adequate, albeit complex, foundation for the KG and that named graphs and Linked Data principles are a suitable mechanism to manage sets of factoids and their provenance.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Debruyne, Christophe and Munnelly, Gary and Kilgallon, Lynn and O’Sullivan, Declan and Crooks, Peter},
	month = apr,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Knowledge graph creation, digital humanities, knowledge graph management},
}

@article{gagnon_exploration_2024,
	title = {An {Exploration} of {IFLA} {LRM} for {Literature} {Data} {Representation}},
	volume = {17},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3687486},
	doi = {10.1145/3687486},
	abstract = {The digital humanities have witnessed a clear development in recent years due partly to their adoption of Semantic Web and linked data technologies and the creation of knowledge bases. In this work, we target the creation of an ontology and knowledge base for literature data representation based on the IFLA Library Reference Model (LRM). IFLA LRM is the main model for book-related data, allowing for a fine representation of the various layers that constitute a book. However, by design, it doesn’t deal with some aspects usually available in literature databases, such as information about authors, literary awards or book themes. As a result, LRM requires some extensions to be able to represent ancillary data. Another challenge is the querying of IFLA LRM knowledge bases, with a performance cost that comes with the fine-grained expressivity of the LRM model, which creates longer and therefore typically slower SPARQL queries. In this work, we propose an extension to the IFLA LRM ontology called IFLA LRM* that targets these limitations including a connection to the vocabulary Schema.org and to the taxonomies Thema and Dewey Decimal, and the representation of literary awards. We also present a practical case study on using our extended model to create a Quebec literature knowledge base, discussing the interest of our extensions.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Gagnon, Michel and Font, Ludovic and Zouaq, Amal},
	month = sep,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Linked Open Data, Cultural Heritage, IFLA LRM, Literature},
}

@inproceedings{huang_hykg-cf_2025,
	address = {New York, NY, USA},
	series = {{WSDM} '25},
	title = {{HyKG}-{CF}: {A} {Hybrid} {Approach} for {Counterfactual} {Prediction} using {Domain} {Knowledge}},
	isbn = {979-8-4007-1329-3},
	url = {https://doi.org/10.1145/3701551.3708813},
	doi = {10.1145/3701551.3708813},
	abstract = {Predictive models are gaining attention as powerful tools for aiding clinicians in diagnosis, prognosis, and treatment recommendations. However, their reliance on associative patterns may raise concerns about reliability of decision support, as association does not necessarily imply causation. To address this limit, we propose HyKG-CF, a hybrid approach to counterfactual prediction that leverages data and domain knowledge encoded in knowledge graph (KG). HyKG-CF integrates symbolic reasoning (on knowledge) with numerical learning (on data) using large language models (LLMs) and statistical models to learn causal Bayesian networks (CBNs) for accurate counterfactual prediction. Using data and knowledge, HyKG-CF improves the accuracy of causal discovery and counterfactual prediction. We evaluate HyKG-CF on a non-small cell lung cancer (NSCLC) KG, demonstrating that it outperforms other baselines. The results highlight the promise of combining domain knowledge with causal models to improve counterfactual prediction.},
	booktitle = {Proceedings of the {Eighteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Hao and Vidal, Maria-Esther},
	year = {2025},
	note = {event-place: Hannover, Germany},
	keywords = {knowledge graphs, causality, counterfactual prediction},
	pages = {1104--1105},
}

@article{tsakalakis_typology_2025,
	title = {A typology of explanations to support {Explainability}-by-{Design}},
	volume = {2},
	url = {https://doi.org/10.1145/3708504},
	doi = {10.1145/3708504},
	abstract = {As automated decision-making permeates almost all aspects of everyday life, capabilities to generate meaningful explanations for various stakeholders (i.e., decision-makers, addressees of decisions including individuals, auditors, and regulators) should be carefully deployed. This article presents a typology of explanations intended to support the first pillar of an explainability-by-design strategy. Its production has been achieved by pursuing a responsible innovation approach and introducing a new persona within the research and innovation process, i.e., a legal engineer, whose role is to work at the interface of two teams, the compliance and the engineering teams, and to oversee the process of requirement elicitation, which is often opinionated and narrowing. Once explanation requirements have been derived from applicable regulatory requirements, compliance rules, or business policies, they have been mapped to the dimensions of the typology to produce fine-grained explanation requirements, forming computable building blocks that can then be translated into system requirements during the technical design phase. The typology has been co-created with industry partners operating in two sectors: finance and education. Two pilot studies have thus been conducted to test both the feasibility of the generation and computation of explanations on the basis of the typology and the usefulness of the outputs in the light of the state-of-the-art. The typology comprises nine hierarchical dimensions. It can be leveraged to operate a stand-alone classifier of explanations that acts as detective controls within a broader partially automated compliance strategy. A machine-readable format of the typology is provided in the form of a light ontology.},
	number = {1},
	journal = {ACM J. Responsib. Comput.},
	author = {Tsakalakis, Niko and Stalla-Bourdillon, Sophie and Huynh, Dong and Moreau, Luc},
	month = feb,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Artificial intelligence, data protection, explainability, automated decisions, typology},
}

@inproceedings{cai_knowledge_2025,
	address = {New York, NY, USA},
	series = {{SIGIR} '25},
	title = {A {Knowledge} {Extraction} {Framework} on {Cyber} {Threat} {Reports} with {Enhanced} {Security} {Profiles}},
	isbn = {979-8-4007-1592-1},
	url = {https://doi.org/10.1145/3726302.3729880},
	doi = {10.1145/3726302.3729880},
	abstract = {Knowledge extraction on Cyber Threat Reports (CTRs) is critical for attack investigation and defenses. The granularity and the usability of the knowledge are key issues: the former is determined by entity recognition on CTRs, whereas the latter mainly depends on proper relation extraction. Nevertheless, in the state-of-the-art entity recognition methods on CTRs using span representation, the local semantics of behavior are not considered and the sequential features of entity labels within behavior descriptions are not utilized. Besides, domain-specific definitions/forms of the relation types and knowledge representations are also crucial for effective utilization of knowledge. In this paper, we propose a novel knowledge extraction framework on CTRs to address the above concerns. The framework is formed by the Enhanced Security Profiles (ESP) that can be directly utilized by security detection devices. In the ESP framework, we propose 3 modules to facilitate fine-grained and accurate knowledge extractions: (1) The entity recognition module utilizes a label-aware subsequence autoregressive algorithm to integrate local semantic and label sequence features, enabling accurate identification of cybersecurity entities; (2) The relation extraction module employs LLM-based strategies with shared partition representations to enhance semantic understanding and domain relevance; and (3) The security profile generation module leverages Chain-of-Thought reasoning and In-Context Learning to produce machine-readable rules executable in security detection systems. Extensive experiments on 6 datasets demonstrate that the ESP framework largely outperform the state-of-the-art solutions e.g., the Micro-Fl scores on entity recognition and relation extraction are at least 1.54\% and 13.12\% better, respectively. Our code can be found in https://github.com/YxinMiracle/ESP.},
	booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Cai, Yongxin and Qiu, Jing and Zhang, Fan and Li, Qiang and Chen, Lei},
	year = {2025},
	note = {event-place: Padua, Italy},
	keywords = {relation extraction, knowledge extraction, entity recognition},
	pages = {326--336},
}

@article{choudhury_duality_2025,
	title = {The {Duality} of λ-{Abstraction}},
	volume = {9},
	url = {https://doi.org/10.1145/3704848},
	doi = {10.1145/3704848},
	abstract = {In this paper, we develop and study the following perspective – just as higher-order functions give exponentials, higher-order continuations give coexponentials. From this, we design a language that combines exponentials and coexponentials, producing a duality of lambda abstraction. We formalise this language by giving an extension of a call-by-value simply-typed lambda-calculus with covalues, coabstraction, and coapplication. We develop the semantics of this language using the axiomatic structure of continuations, which we use to produce an equational theory, that gives a complete axiomatisation of control effects. We give a computational interpretation to this language using speculative execution and backtracking, and use this to derive the classical control operators and computational interpretation of classical logic, and encode common patterns of control flow using continuations. By dualising functional completeness, we further develop duals of first-order arrow languages using coexponentials. Finally, we discuss the implementation of this duality as control operators in programming, and develop some applications.},
	number = {POPL},
	journal = {Proc. ACM Program. Lang.},
	author = {Choudhury, Vikraman and Gay, Simon J.},
	month = jan,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {category theory, denotational semantics, classical logic, continuations, control effects, control operators, curry-howard, duality, equational theory, lambda-calculus, type theory},
}

@inproceedings{salminen_instilling_2021,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '21},
	title = {Instilling {Knowledge} {Claims} of {Personas} from 346 {Research} {Articles}},
	isbn = {978-1-4503-8095-9},
	url = {https://doi.org/10.1145/3411763.3451619},
	doi = {10.1145/3411763.3451619},
	abstract = {Our research goal is to summarize the body of persona knowledge by identifying knowledge claims. This can aid HCI researchers to (a) navigate persona knowledge to form an understanding of what is known about personas quickly, (b) identify central research gaps of what is not known (or said) about personas, and (c) identify claims that are not substantiated with strong empirical evidence and warrant future work. To this end, we use computational and manual techniques to extract 130 knowledge claims based on 9139 sentences from 346 persona articles and analyze whether the existing literature supports these claims. The results, clustered into four groups (“Definition”, “Creation”, “Evaluation”, and “Use”), indicate that claims regarding persona definition are characterized by a higher degree of consensus. In contrast, persona creation and use contain a high proportion of unverified claims. There are few claims concerning evaluation. Empirical research should address unverified claims and develop the ontological understanding on persona evaluation.},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Salminen, Joni and Jung, Soon-Gyo and Chhirang, Kamal and Jansen, Bernard},
	year = {2021},
	note = {event-place: Yokohama, Japan},
	keywords = {natural language processing, Personas, knowledge claims, summary},
}

@inproceedings{van_erp_unflattening_2023,
	address = {New York, NY, USA},
	series = {K-{CAP} '23},
	title = {Unflattening {Knowledge} {Graphs}},
	isbn = {979-8-4007-0141-2},
	url = {https://doi.org/10.1145/3587259.3630082},
	doi = {10.1145/3587259.3630082},
	abstract = {Large general-purpose knowledge graphs (KGs) are a critical component for knowledge-driven applications. However, most KGs represent only a limited view of the entities and concepts they describe. The concept coffee can, for example, refer to the plant that yields coffee seeds, the beverage ‘coffee’, and the activity of drinking the beverage. Moreover, it has a long history that is deeply connected to colonialism and status. All of these notions are an intricate part of national identities, have changed dramatically over time, and connect to many different narratives with different opinions on them. This complexity is not captured in current KGs. In this vision paper, I present the three crucial challenges for unflattening knowledge graphs and directions for future work.},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Van Erp, Marieke},
	year = {2023},
	note = {event-place: Pensacola, FL, USA},
	keywords = {knowledge graphs, digital humanities, language technology},
	pages = {223--224},
}

@inproceedings{tsay_aimmx_2020,
	address = {New York, NY, USA},
	series = {{MSR} '20},
	title = {{AIMMX}: {Artificial} {Intelligence} {Model} {Metadata} {Extractor}},
	isbn = {978-1-4503-7517-7},
	url = {https://doi.org/10.1145/3379597.3387448},
	doi = {10.1145/3379597.3387448},
	abstract = {Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87\% precision and 83\% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42\% of models in our sample citing their datasets, method reproducibility is more common at 72\% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd},
	year = {2020},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {Machine Learning, Artificial Intelligence, Metadata Extraction, Model Catalog, Model Metadata, Model Mining},
	pages = {81--92},
}

@article{gunasekara_overview_2024,
	title = {Overview of the {Ninth} {Dialog} {System} {Technology} {Challenge}: {DSTC9}},
	volume = {32},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2024.3426331},
	doi = {10.1109/TASLP.2024.3426331},
	abstract = {This paper introduces the Ninth Dialog System Technology Challenge (DSTC-9). This edition of the DSTC focuses on applying end-to-end dialog technologies for four distinct tasks in dialog systems, namely, 1. Task-oriented dialog Modeling with Unstructured Knowledge Access, 2. Multi-domain task-oriented dialog, 3. Interactive evaluation of dialog and 4. Situated interactive multimodal dialog. This paper describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Gunasekara, Chulaka and Kim, Seokhwan and D'Haro, Luis Fernando and Rastogi, Abhinav and Chen, Yun-Nung and Eric, Mihail and Hedayatnia, Behnam and Gopalakrishnan, Karthik and Liu, Yang and Huang, Chao-Wei and Hakkani-Tür, Dilek and Li, Jinchao and Zhu, Qi and Luo, Lingxiao and Liden, Lars and Huang, Kaili and Shayandeh, Shahin and Liang, Runze and Peng, Baolin and Zhang, Zheng and Shukla, Swadheen and Huang, Minlie and Gao, Jianfeng and Mehri, Shikib and Feng, Yulan and Gordon, Carla and Alavi, Seyed Hossein and Traum, David and Eskenazi, Maxine and Beirami, Ahmad and Cho, Eunjoon and Crook, Paul A. and De, Ankita and Geramifard, Alborz and Kottur, Satwik and Moon, Seungwhan and Poddar, Shivani and Subba, Rajen},
	month = jul,
	year = {2024},
	note = {Publisher: IEEE Press},
	pages = {4066--4076},
}

@inproceedings{huang_eco-dst_2022,
	address = {New York, NY, USA},
	series = {{MLMI} '21},
	title = {{ECO}-{DST}: {An} {Efficient} {Cross}-lingual {Dialogue} {State} {Tracking} {Framework}},
	isbn = {978-1-4503-8424-7},
	url = {https://doi.org/10.1145/3490725.3490737},
	doi = {10.1145/3490725.3490737},
	abstract = {Data efficiency is a critical challenge for cross-lingual task-oriented dialogue state tracking (DST) due to high cost of collecting large amount of task-related labeled training set for specific language. Therefore, we focus on adapting high-performance source language DST to target language by using only bilingual dictionary, without accessing labeled target data. We propose a novel data efficient cross-lingual DST framework (ECO-DST), which consists of cross-lingual encoder and language independent decoder. To support cross-lingual zero-shot adaptation, we leverage two advanced methods in encoder: 1) pre-trained cross-lingual model XLM-RoBERTa (XLM-R), 2) dynamic local phrase code-switching data augmentation for cross-lingual representation alignment. We evaluate the proposed method on The Ninth Dialogue System Technology Challenge (DSTC9) cross-lingual tasks. For target language DST, we compare our proposed framework with submitted systems in DSTC9, our model achieves state-of-the-art result on CrossWOZ dataset and promising result on MultiWOZ 2.1 dataset. Meanwhile on source language DST, the same model keeps competitive performance compared with original source DST model.},
	booktitle = {Proceedings of the 2021 4th {International} {Conference} on {Machine} {Learning} and {Machine} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Chao and Di, Hui and Wang, Lina and Ouchi, Kazushige},
	year = {2022},
	note = {event-place: Hangzhou, China},
	keywords = {Dialogue State Tracking, Cross-Lingual Transfer, Data Efficiency, Dynamic Local Phrase Code-Switching, Task-oriented Dialogue},
	pages = {77--82},
}

@article{malviya_experience_2022,
	title = {Experience {Replay}-based {Deep} {Reinforcement} {Learning} for {Dialogue} {Management} {Optimisation}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3539223},
	doi = {10.1145/3539223},
	abstract = {Dialogue policy is a crucial component in task-oriented Spoken Dialogue Systems (SDSs). As a decision function, it takes the current dialogue state as input and generates appropriate system’s response. In this paper, we explore the reinforcement learning approaches to solve this problem in an Indic language scenario. Recently, Deep Reinforcement Learning (DRL) has been used to optimise the dialogue policy. However, many DRL approaches are not sample-efficient. Hence, particular attention is given to actor-critic methods based on off-policy reinforcement learning that utilise the Experience Replay (ER) technique for reducing the bias and variance to achieve high sample efficiency. ER based actor-critic methods, such as Advantage Actor-Critic Experience Replay (A2CER) are proven to deliver competitive results in gaming environments that are fully observable and have a very small action-set. While, in SDSs, the states are not fully observable and often have to deal with the large action space. Describing the limitations of traditional methods, i.e., value-based and policy-based methods, such as high variance, low sample-efficiency, and often converging to local optima, we firstly explore the use of A2CER in dialogue policy learning. It is shown to beat the current state-of-the-art deep learning methods for SDS. Secondly, to handle the issues of early-stage performance, we utilise a demonstration corpus to pre-train the models prior to on-line policy learning. We thus experiment with the A2CER on a larger action space and find it significantly faster than the current state-of-the-art. Combining both approaches, we present a novel DRL based dialogue policy optimisation method, A2CER and its effectiveness for a task-oriented SDS in the Indic language.},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Malviya, Shrikant and Kumar, Piyush and Namasudra, Suyel and Tiwary, Uma Shanker},
	month = may,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {dialogue management, deep reinforcement learning, Spoken dialogue systems},
	annote = {Just Accepted},
}

@inproceedings{kume_extracting_2022,
	address = {New York, NY, USA},
	series = {{IJCKG} '21},
	title = {Extracting {Domain}-specific {Concepts} from {Large}-scale {Linked} {Open} {Data}},
	isbn = {978-1-4503-9565-6},
	url = {https://doi.org/10.1145/3502223.3502227},
	doi = {10.1145/3502223.3502227},
	abstract = {We propose a methodology for extracting concepts for a target domain from large-scale linked open data (LOD) to support the construction of domain ontologies providing field-specific knowledge and definitions. The proposed method defines search entities by linking the LOD vocabulary with technical terms related to the target domain. The search entities are then used as a starting point for obtaining upper-level concepts in the LOD, and the occurrences of common upper-level entities and the chain-of-path relationships are examined to determine the range of conceptual connections in the target domain. A technical dictionary index and natural language processing are used to evaluate whether the extracted concepts cover the domain. As an example of extracting a class hierarchy from LOD, we used Wikidata to construct a domain ontology for polymer materials and physical properties. The proposed method can be applied to general datasets with class hierarchies, and it allows ontology developers to create an initial model of the domain ontology for their own purposes.},
	booktitle = {Proceedings of the 10th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Kume, Satoshi and Kozaki, Kouji},
	year = {2022},
	note = {event-place: Virtual Event, Thailand},
	keywords = {Domain ontology, Wikidata, Linked open data, Ontology construction, Graph analysis},
	pages = {28--37},
}

@article{kostovska_representing_2025,
	title = {Representing and {Exploiting} {Benchmarking} {Data} for {Optimisation} and {Learning}},
	volume = {18},
	url = {https://doi.org/10.1145/3747321.3747323},
	doi = {10.1145/3747321.3747323},
	abstract = {The rapid advancements in Machine Learning (ML) and Black-Box Optimisation (BBO) have led to an increased reliance on benchmarking data for evaluating and comparing algorithms across diverse domain tasks. However, the effective exploitation of this data is hindered by challenges such as syntactic variability, semantic ambiguity, and lack of standardization. In this dissertation, we address these challenges by advocating for formal semantic representation of benchmarking data through the use of ontologies. By providing standardized vocabularies and ontologies, we improve knowledge sharing and promote data interoperability across studies in ML and BBO. In the ML domain, focusing on multi-label classification (MLC), we design an ontology-based framework for semantic annotation of benchmarking data, facilitating the creation of MLCBench - a semantic catalog that enhances data accessibility and reusability. In the BBO domain, we introduce the OPTION (OPTImization algorithm benchmarking ONtology) ontology to formally represent benchmarking data, including performance data, algorithm metadata, and problem landscapes. This ontology enables the automatic integration and interoperability of knowledge and data from diverse benchmarking studies.},
	number = {2},
	journal = {SIGEVOlution},
	author = {Kostovska, Ana},
	month = jul,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
}

@inproceedings{zhao_perennial_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Perennial {Semantic} {Data} {Terms} of {Use} for {Decentralized} {Web}},
	isbn = {979-8-4007-0171-9},
	url = {https://doi.org/10.1145/3589334.3645631},
	doi = {10.1145/3589334.3645631},
	abstract = {In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal 'Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a "perennial” DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Rui and Zhao, Jun},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {automated reasoning, usage control, data terms of use, decentralized web, formal modelling, notation 3},
	pages = {2238--2249},
}

@inproceedings{paul_nrityamanch_2023,
	address = {New York, NY, USA},
	series = {{FIRE} '22},
	title = {{NrityaManch}: {An} {Annotation} and {Retrieval} {System} for {Bharatanatyam} {Dance}},
	isbn = {979-8-4007-0023-1},
	url = {https://doi.org/10.1145/3574318.3574338},
	doi = {10.1145/3574318.3574338},
	abstract = {This paper presents an annotation and retrieval application named NrityaManch dedicated explicitly to the Indian classical dance. We primarily choose Bharatanatyam dance for the application development. We exploit ontology technique which captures dance image’s annotation details and structurally organizes the dance database. An OWL2 ontology is developed in Protégé 5.5.0 which is validated using HermiT 1.4.3.456 reasoner to maintain consistency. A user interface is provided for the manual annotation of dance images. Initially, we focus on dancer details, dance details, and elements of static dance posture like hasta mudra during the annotation. All annotation details are saved in RDF/XML file. A search window is provided, which facilitates two types of search - natural language query search and tight query search. Named Entity Recognition (NER) pipeline mechanism is utilized in this work which facilitates keyword extraction from natural language queries. A SPARQL query is automatically generated by the system which is applied to the RDF corpus in order to retrieve distinct images. The NER pipeline mechanism achieves an accuracy of 80\% for our dance dataset. The system achieves an average f-score value of 0.8547 for the retrieval functionality. The proposed system intends to help dance learners to find dance resources in a dedicated place and will also help in Indian classical dance preservation.},
	booktitle = {Proceedings of the 14th {Annual} {Meeting} of the {Forum} for {Information} {Retrieval} {Evaluation}},
	publisher = {Association for Computing Machinery},
	author = {Paul, Soumen and Saha, Rounak and Padhi, Swarup and Majumdar, Srijoni and Das, Partha Pratim and Rao, K Sreenivas},
	year = {2023},
	note = {event-place: Kolkata, India},
	keywords = {annotation, natural language query, Bharatanatyam, dance retrieval, search},
	pages = {65--73},
}

@article{min_network-regularized_2018,
	title = {Network-{Regularized} {Sparse} {Logistic} {Regression} {Models} for {Clinical} {Risk} {Prediction} and {Biomarker} {Discovery}},
	volume = {15},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2016.2640303},
	doi = {10.1109/TCBB.2016.2640303},
	abstract = {Molecular profiling data e.g., gene expression has been used for clinical risk prediction and biomarker discovery. However, it is necessary to integrate other prior knowledge like biological pathways or gene interaction networks to improve the predictive ability and biological interpretability of biomarkers. Here, we first introduce a general regularized Logistic Regression LR framework with regularized term lambda Vert boldsymbol wVert \_1 + eta boldsymbol w{\textasciicircum}Tboldsymbol Mboldsymbol w, which can reduce to different penalties, including Lasso, elastic net, and network-regularized terms with different boldsymbol M. This framework can be easily solved in a unified manner by a cyclic coordinate descent algorithm which can avoid inverse matrix operation and accelerate the computing speed. However, if those estimated boldsymbol w\_i and boldsymbol w\_j have opposite signs, then the traditional network-regularized penalty may not perform well. To address it, we introduce a novel network-regularized sparse LR model with a new penalty lambda Vert boldsymbol wVert \_1 + eta {\textbar}boldsymbol w{\textbar}{\textasciicircum}Tboldsymbol M{\textbar}boldsymbol w{\textbar} to consider the difference between the absolute values of the coefficients. We develop two efficient algorithms to solve it. Finally, we test our methods and compare them with the related ones using simulated and real data to show their efficiency.},
	number = {3},
	journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
	author = {Min, Wenwen and Liu, Juan and Zhang, Shihua},
	month = may,
	year = {2018},
	note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
	pages = {944--953},
}

@inproceedings{cabrera_self-sustaining_2024,
	address = {New York, NY, USA},
	series = {{SATrends} '24},
	title = {Self-sustaining {Software} {Systems} ({S4}): {Towards} {Improved} {Interpretability} and {Adaptation}},
	isbn = {979-8-4007-0560-1},
	url = {https://doi.org/10.1145/3643657.3643910},
	doi = {10.1145/3643657.3643910},
	abstract = {Software systems impact society at different levels as they pervasively solve real-world problems. Modern software systems are often so sophisticated that their complexity exceeds the limits of human comprehension. These systems must respond to changing goals, dynamic data, unexpected failures, and security threats, among other variable factors in real-world environments. Systems' complexity challenges their interpretability and requires autonomous responses to dynamic changes. Two main research areas explore autonomous systems' responses: evolutionary computing and autonomic computing. Evolutionary computing focuses on software improvement based on iterative modifications to the source code. Autonomic computing focuses on optimising systems' performance by changing their structure, behaviour, or environment variables. Approaches from both areas rely on feedback loops that accumulate knowledge from the system interactions to inform autonomous decision-making. However, this knowledge is often limited, constraining the systems' interpretability and adaptability. This paper proposes a new concept for interpretable and adaptable software systems: self-sustaining software systems (S4). S4 builds knowledge loops between all available knowledge sources that define modern software systems to improve their interpretability and adaptability. This paper introduces and discusses the S4 concept.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {New} {Trends} in {Software} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Cabrera, Christian and Paleyes, Andrei and Lawrence, Neil David},
	year = {2024},
	note = {event-place: Lisbon, Portugal},
	keywords = {knowledge graphs, large language models, autonomous systems, data-oriented architectures, software engineering},
	pages = {5--9},
}

@inproceedings{mcclatchey_deployment_2018,
	address = {New York, NY, USA},
	series = {{IDEAS} '18},
	title = {The {Deployment} of an {Enhanced} {Model}-{Driven} {Architecture} for {Business} {Process} {Management}},
	isbn = {978-1-4503-6527-7},
	url = {https://doi.org/10.1145/3216122.3216155},
	doi = {10.1145/3216122.3216155},
	abstract = {Business systems these days need to be agile to address the needs of a changing world. Business modelling requires process management to be highly adaptable with the ability to support dynamic workflows, inter-application integration (potentially between businesses) and process reconfiguration. Designing in the ability to cater for evolution is critical to success. To handle change, systems need the capability to adapt as and when necessary to changes in users' requirements. Using our implementation of a self-describing system, a so-called description-driven approach, new versions of data structures or processes can be created alongside older versions providing a log of changes to the underlying data schema and enabling the gathering of traceable ("provenance") data. The CRISTAL software, which originated at CERN for handling physics data, uses versions of stored descriptions to define data and workflows which can be evolved over time and thereby to handle evolving system needs. It has been customised for use in business as the Agilium-NG product. This paper reports on how the Agilium-NG software has enabled the deployment of an unique business process management solution that can be dynamically evolved to cater for changing user requirements.},
	booktitle = {Proceedings of the 22nd {International} {Database} {Engineering} \&amp; {Applications} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {McClatchey, Richard},
	year = {2018},
	note = {event-place: Villa San Giovanni, Italy},
	keywords = {business process management, business provenance, Description-driven systems, system evolution},
	pages = {217--225},
}

@inproceedings{wardani_sess_2023,
	address = {New York, NY, USA},
	series = {{IC3INA} '22},
	title = {{SESS}: {Utilization} of {SPIN} for {Ethnomedicine} {Semantic} {Search}},
	isbn = {978-1-4503-9790-2},
	url = {https://doi.org/10.1145/3575882.3575912},
	doi = {10.1145/3575882.3575912},
	abstract = {Indonesia has biodiversity which is very beneficial for human life. Existing applications for ethnomedicine have been developed using conventional methods that only utilized SPARQL Protocol and RDF Query Language (SPARQL), so they still have limitations in representing knowledge and its retrieval. Those conventional methods are which based of relational database and ontology that has not utilized inference in its query process. Therefore, this work proposed SPIN for Enthnomedicine Semantic Search (SESS), a framework of the semantic search for medicinal plants that were developed by using SPIN (SPARQL Inferencing Notation). SESS has two main parts, the ontology design included SPARQL Inferencing Notation (SPIN) library and query process. The experiments were assessed in terms of execution time, query variation and accuracy. The obtained results showed a ratio of precision at 1, recall at 0.98 and the average value of the f-measure was 0.99. Utilizing SPIN also decrease the time consuming to obtain the result by around .},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Computer}, {Control}, {Informatics} and {Its} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Wardani, Dewi and Susmawati, Mauluah},
	year = {2023},
	note = {event-place: Virtual Event, Indonesia},
	keywords = {ontology, sparql, semantic search, owl, ethnomedicine, spin},
	pages = {153--157},
}

@inproceedings{wang_knowledge_2023,
	address = {New York, NY, USA},
	series = {{ICISS} '23},
	title = {Knowledge {Graph} {Completion} {Using} {Multiple} {Embedding} {Representations} for {Intelligent} {Information} {Extraction} from {Technical} {Reports}},
	isbn = {979-8-4007-0820-6},
	url = {https://doi.org/10.1145/3625156.3625159},
	doi = {10.1145/3625156.3625159},
	abstract = {As a new data structure, knowledge graphs are widely used in search engines, recommendation systems, question and answer systems and other related fields. The knowledge graph is helpful to realize intelligent and digital knowledge service of nuclear power accidents, in which link prediction can solve the discovery and restoration of missing information in Knowledge graph. This is also one of the research hotspot in the field of knowledge graph applications. Currently, the text description of entity information is rarely considered in the completion of nuclear power Knowledge graph. In this paper, we propose MEK-ConvKB (Multi-Embedding Knowledge Graph Prediction based on ConvKB), a reasoning model combined with multi-embedding techniques to improve performance. The embedded expression of Knowledge graph is enhanced by text description in nuclear power accident reports, which improves the accuracy of link prediction and expands the Knowledge graph of nuclear power accidents. The results show that our model can effectively express the semantic association between entities, Our model achieved the best performance compared to the baseline methods., which can provide a research basis for solving the discovery and restoration of missing information in knowledge graphs.},
	booktitle = {Proceedings of the 2023 6th {International} {Conference} on {Information} {Science} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Hongwei and Zhang, Ziling and Liu, Xiuhua and Cao, Mengyuan},
	year = {2023},
	note = {event-place: Edinburgh, United Kingdom},
	pages = {15--21},
}

@inproceedings{anelli_fourth_2022,
	address = {New York, NY, USA},
	series = {{RecSys} '22},
	title = {Fourth {Knowledge}-aware and {Conversational} {Recommender} {Systems} {Workshop} ({KaRS})},
	isbn = {978-1-4503-9278-5},
	url = {https://doi.org/10.1145/3523227.3547412},
	doi = {10.1145/3523227.3547412},
	abstract = {In the last few years, a renewed interest of the research community in conversational recommender systems (CRSs) has been emerging. This is likely due to the massive proliferation of Digital Assistants (DAs) such as Amazon Alexa, Siri, or Google Assistant that are revolutionizing the way users interact with machines. DAs allow users to execute a wide range of actions through an interaction mostly based on natural language utterances. However, although DAs are able to complete tasks such as sending texts, making phone calls, or playing songs, they still remain at an early stage in terms of their recommendation capabilities via a conversation. In addition, we have been witnessing the advent of increasingly precise and powerful recommendation algorithms and techniques able to effectively assess users’ tastes and predict information that may be of interest to them. Most of these approaches rely on the collaborative paradigm (often exploiting machine learning techniques) and neglect the huge amount of knowledge, both structured and unstructured, describing the domain of interest of a recommendation engine. Although very effective in predicting relevant items, collaborative approaches miss some very interesting features that go beyond the accuracy of results and move in the direction of providing novel and diverse results as well as generating explanations for recommended items. Knowledge-aware side information becomes crucial when a conversational interaction is implemented, in particular for preference elicitation, explanation, and critiquing steps.},
	booktitle = {Proceedings of the 16th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Anelli, Vito Walter and Basile, Pierpaolo and de Melo, Gerard and Donini, Francesco Maria and Ferrara, Antonio and Musto, Cataldo and Narducci, Fedelucio and Ragone, Azzurra and Zanker, Markus},
	year = {2022},
	note = {event-place: Seattle, WA, USA},
	keywords = {Semantic Web, Knowledge Graphs, Natural Language Processing, Knowledge Representation, Recommender systems, Conversational Agents},
	pages = {663--666},
}

@inproceedings{weaver_simulating_2023,
	address = {Singapore, Singapore},
	series = {{WSC} '22},
	title = {Simulating {Energy} and {Security} {Interactions} in {Semiconductor} {Manufacturing}: {Insights} from the {Intel} {Minifab} {Model}},
	abstract = {Semiconductor manufacturing, particularly wafer fabrication, is a highly complex system of processes and workflows. Fabrication facilities must deal with re-entrant flows to support multiple types of wafers being produced simultaneously, each with their own deadlines and specifications. The manufacturing process itself depends upon the ability to control and programmatically adjust a variety of environmental conditions. In addition, wafer fabrication consumes large amounts of energy, particularly electricity. Emerging technologies including networked devices may help reduce the energy footprint but can introduce cybersecurity risks. Therefore, this paper presents its modeling and simulation framework to quantify tradeoffs between operational measures of performance, energy consumption, and cybersecurity risks. We augment the Intel Minifab model with an Industrial Control Systems (ICS) reference model based on the Purdue Enterprise Reference Architecture (PERA) as well as tool-level energy consumption data from a semiconductor manufacturing testbed.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Weaver, Gabriel A. and Shusko, Jacob and Hasenbein, John J. and Kutanoglu, Erhan and Martinez-Medina, Gonzalo and Castillo-Villar, Krystel K. and Costa, Paulo C. G.},
	year = {2023},
	pages = {3477--3488},
}

@article{zhang_acm_2018,
	title = {{ACM} {UMAP} 2018 - {User} {Modeling}, {Adaptation} and {Personalization}: 8-11 {July}, 2018 at {NTU}, {Singapore}},
	volume = {2018},
	issn = {1931-1745},
	url = {https://doi.org/10.1145/3183639.3183641},
	doi = {10.1145/3183639.3183641},
	abstract = {UMAP (User Modeling, Adaptation and Personalization) is the premier international conference for researchers and practitioners working on systems that adapt to individual users, to groups of users, and that collect, represent, and model user information. UMAP is the successor to the biennial User Modeling (UM) and Adaptive Hypermedia and Adaptive Web-based Systems (AH) conferences that were merged in 2009. It is sponsored by ACM SIGCHI and SIGWEB, and organized under the auspices of User Modeling Inc. The proceedings are published by ACM and will be part of the ACM Digital Library.},
	number = {Winter},
	journal = {SIGWEB Newsl.},
	author = {Zhang, Jie and Mitrovic, Tanja and Chin, David and Chen, Li},
	month = mar,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
}

@inproceedings{mitra_enhancing_2024,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '24},
	title = {Enhancing {Region}-{Based} {Geometric} {Embedding} for {Gene}-{Disease} {Associations}},
	isbn = {979-8-4007-1634-8},
	url = {https://doi.org/10.1145/3632410.3632489},
	doi = {10.1145/3632410.3632489},
	abstract = {In recent times, Geometric Knowledge Graph Embedding (KGE) methods that focus on regions have proven valuable for effectively representing structured knowledge, such as ontologies, in various reasoning tasks. Nevertheless, transforming the richly expressive semantics present in ontologies may require a transition to less expressive interim representations before initiating the actual embedding process. In our research, we explore current approaches and offer recommendations to enhance the state-of-the-art, drawing from extensive experiments conducted on the Human Phenotype Ontology (HPO) for predicting gene-disease (g-d) associations. Our findings indicate that incorporating these new suggestions can lead to results that outperform other leading KGE models.},
	booktitle = {Proceedings of the 7th {Joint} {International} {Conference} on {Data} {Science} \&amp; {Management} of {Data} (11th {ACM} {IKDD} {CODS} and 29th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Mitra, Aniket and Venugopal, Vinu},
	year = {2024},
	note = {event-place: Bangalore, India},
	keywords = {EL++ OWL, Knowledge Graph Embedding, n-ball Embedding},
	pages = {584--585},
}

@inproceedings{alperin_risk_2019,
	address = {New York, NY, USA},
	series = {{AISec}'19},
	title = {Risk {Prioritization} by {Leveraging} {Latent} {Vulnerability} {Features} in a {Contested} {Environment}},
	isbn = {978-1-4503-6833-9},
	url = {https://doi.org/10.1145/3338501.3357365},
	doi = {10.1145/3338501.3357365},
	abstract = {Cyber network defenders face an overwhelming volume of software vulnerabilities. Resource limitations preclude them mitigating all but a small number of vulnerabilities on an enterprise network, so proper prioritization of defensive actions are of paramount importance. Current methods of risk prioritization are predominantly expert-based, and many include leveraging Common Vulnerability Scoring System (CVSS) risk scores. These scores are assigned by subject matter experts according to conventional methods of qualifying risk. Vulnerability mitigation strategies are then often applied in CVSS score order. Our vulnerability assessment system, in contrast, takes a predominantly data-driven approach. In general, we associate a risk metric of vulnerabilities with existence of corresponding exploits. Our assumption is that if an entity has invested time and money to exploit a particular vulnerability, this is a critical gauge of that vulnerability's importance, and hence risk.Prior work presented a model that allows for the creation of prioritized vulnerabilities based on their association-likelihood with exploits, outperforming then-current methods. Because the initial approach only leveraged one vulnerability feature, we extended the vulnerability feature space by incorporating additional features derived from natural language processing. The importance metric is still given by a vulnerability-exploit relationship, but by processing text descriptions and other available information, our system became significantly more accurate and predictive. We next propose a mechanism that customizes vulnerability risks according to their exploitation likelihood in a contested environment given site-specific threat intelligence information, namely, attacks by an Advanced Persistent Threat (APT) group. Utilizing held-back data, we then demonstrate that latently similar vulnerabilities, which could be targeted by the same adversary, see higher risk ratings.},
	booktitle = {Proceedings of the 12th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Alperin, Kenneth and Wollaber, Allan and Ross, Dennis and Trepagnier, Pierre and Leonard, Leslie},
	year = {2019},
	note = {event-place: London, United Kingdom},
	keywords = {machine learning, natural language processing, exploit, risk model, vulnerability},
	pages = {49--57},
}

@inproceedings{quesnel_inbetweeny_2024,
	address = {New York, NY, USA},
	series = {{HttF} '24},
	title = {The {Inbetweeny} {Collective}: {Reflexive} {Dialogues} on the {Liminality} of {Researchers}' {Lived} {Experiences}},
	isbn = {979-8-4007-1042-1},
	url = {https://doi.org/10.1145/3686169.3686204},
	doi = {10.1145/3686169.3686204},
	abstract = {In this collaborative autoethnography, we critically explore our lived experiences within a wider context of HCI research and practice. We reflect on the epistemological ways of knowing through ‘insider’ lived experience of, and ‘outsider’ knowledge of our research topic(s) via the concept of “liminal space” as a process ontology. To embrace liminality entails inhabiting the space ‘in-between’ these ways of knowing, suspended on a threshold of uncertainty and transformative growth. All authors identify as ‘inbetweenies’, because we are neither just ‘insiders’ nor ‘outsiders’, and we collect our respective stories to share. Drawing from these stories and our dialogues, we discuss how ways of knowing have historically been dichotomously categorized with their associated subjective or objective characterizations, resulting in power hierarchies and tensions. We propose that for an ‘inbetweeny’ researcher, thoughtful approaches to navigating this liminal space could potentially bridge persistent tensions in HCI research and practices toward personal and systemic transformation. Five areas of reflection are discussed, with proposed learnings that can be applied towards sustained practices for individuals and collectives at any stage of their journey and development.},
	booktitle = {Proceedings of the {Halfway} to the {Future} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Quesnel, Denise T. and Losev, Tatiana and Stepanova, Ekaterina R. and Carpendale, Sheelagh and Riecke, Bernhard E.},
	year = {2024},
	note = {event-place: Santa Cruz, CA, USA},
	keywords = {Autoethnography, Philosophy, Human-Computer Interaction, Critical Reflexivity, Design Research, Experiential Knowledge, Insider and Outsider Research, Liminality, Lived Experience, Process Ontology, Research Methodology},
}

@inproceedings{bisbas_shared_2024,
	address = {New York, NY, USA},
	series = {{ASPLOS} '24},
	title = {A shared compilation stack for distributed-memory parallelism in stencil {DSLs}},
	isbn = {979-8-4007-0386-7},
	url = {https://doi.org/10.1145/3620666.3651344},
	doi = {10.1145/3620666.3651344},
	abstract = {Domain Specific Languages (DSLs) increase programmer productivity and provide high performance. Their targeted abstractions allow scientists to express problems at a high level, providing rich details that optimizing compilers can exploit to target current- and next-generation supercomputers. The convenience and performance of DSLs come with significant development and maintenance costs. The siloed design of DSL compilers and the resulting inability to benefit from shared infrastructure cause uncertainties around longevity and the adoption of DSLs at scale. By tailoring the broadly-adopted MLIR compiler framework to HPC, we bring the same synergies that the machine learning community already exploits across their DSLs (e.g. Tensorflow, PyTorch) to the finite-difference stencil HPC community. We introduce new HPC-specific abstractions for message passing targeting distributed stencil computations. We demonstrate the sharing of common components across three distinct HPC stencil-DSL compilers: Devito, PSyclone, and the Open Earth Compiler, showing that our framework generates high-performance executables based upon a shared compiler ecosystem.},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 3},
	publisher = {Association for Computing Machinery},
	author = {Bisbas, George and Lydike, Anton and Bauer, Emilien and Brown, Nick and Fehr, Mathieu and Mitchell, Lawrence and Rodriguez-Canal, Gabriel and Jamieson, Maurice and Kelly, Paul H. J. and Steuwer, Michel and Grosser, Tobias},
	year = {2024},
	note = {event-place: La Jolla, CA, USA},
	keywords = {domain-specific languages, intermediate representations, message passing, MLIR, MPI, SSA, stencil computations},
	pages = {38--56},
}

@inproceedings{fissore_simulating_2022,
	address = {New York, NY, USA},
	series = {{CIKM} '22},
	title = {Simulating {Complex} {Problems} {Inside} a {Database}},
	isbn = {978-1-4503-9236-5},
	url = {https://doi.org/10.1145/3511808.3557520},
	doi = {10.1145/3511808.3557520},
	abstract = {The standard way to store and interact with the large amount of data that are central to the functioning of any modern business is through the use of a relational Knowledge Graph Management System (KGMS). In this paper we show how the relational model can be successfully exploited to model complex analytic scenarios while enjoying the same characteristics of clarity and flexibility as when modeling the data themselves. Using the Rel language, we simulate the daily schedule of an airline company as an agentbased system, and we will show how modeling this system through a set of relationships and logical rules will let us focus directly on the inherent complexity of our model, taking away most of the incidental effort in actually implementing our simulation.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Fissore, Giancarlo and Vasiloglou, Nikolaos},
	year = {2022},
	note = {event-place: Atlanta, GA, USA},
	keywords = {simulation, database, agent based modelling},
	pages = {5086--5087},
}

@inproceedings{tabaza_binding_2024,
	address = {New York, NY, USA},
	series = {{AICCONF} '24},
	title = {Binding {Text}, {Images}, {Graphs}, and {Audio} for {Music} {Representation} {Learning}},
	isbn = {979-8-4007-1692-8},
	url = {https://doi.org/10.1145/3660853.3660886},
	doi = {10.1145/3660853.3660886},
	abstract = {Abstract In the field of Information Retrieval and Natural Language Processing, text embeddings play a significant role in tasks such as classification, clustering, and topic modeling. However, extending these embeddings to abstract concepts such as music, which involves multiple modalities, presents a unique challenge. Our work addresses this challenge by integrating rich multi-modal data into a unified joint embedding space. This space includes: (1) textual, (2) visual, (3) acoustic, and (4) graph-based modality features. By doing so, we mirror cognitive processes associated with music interaction and overcome the disjoint nature of individual modalities. The resulting joint low-dimensional vector space facilitates retrieval, clustering, embedding space arithmetic, and cross-modal retrieval tasks. Importantly, our approach carries implications for music information retrieval and recommendation systems. Furthermore, we propose a novel multi-modal model that integrates various data types—text, images, graphs, and audio—for music representation learning. Our model aims to capture the complex relationships between different modalities, enhancing the overall understanding of music. By combining textual descriptions, visual imagery, graph-based structures, and audio signals, we create a comprehensive representation that can be leveraged for a wide range of music-related tasks. Notably, our model demonstrates promising results in music classification, and recommendation systems. Code Availability: The source code for the multi-modal music representation model described in this paper is available on GitHub. Access and further details can be found at the following repository link: //github.com/a-tabaza/binding\_music/},
	booktitle = {Proceedings of the {Cognitive} {Models} and {Artificial} {Intelligence} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Tabaza, Abdulrahman and Quishawi, Omar and Yaghi, Abdelrahman and Qawasmeh, Omar},
	year = {2024},
	note = {event-place: undefinedstanbul, Turkiye},
	pages = {139--146},
}

@inproceedings{jeusfeld_deeptelos_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {{DeepTelos} and {DMLA}: a contribution to the {MULTI} 2022 collaborative comparison challenge},
	isbn = {978-1-4503-9467-3},
	url = {https://doi.org/10.1145/3550356.3561602},
	doi = {10.1145/3550356.3561602},
	abstract = {The MULTI 2022 Collaborative Comparison Challenge was created to promote in-depth discussion between multi-level modeling approaches. This paper presents a comparison of DeepTelos- and DMLA-based solutions in response to the challenge. We first present each approach and solution separately, and then list the similarities and differences between the two solutions, discussing their relative strengths and weaknesses.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Jeusfeld, Manfred and Mezei, Gergely and Bácsi, Sándor},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {multi-level modeling, collaborative challenge, DeepTelos, DMLA},
	pages = {414--423},
}

@inproceedings{nuipian_book_2024,
	address = {New York, NY, USA},
	series = {{NLPIR} '23},
	title = {Book {Recommendation} {System} based on {Course} {Descriptions} using {Cosine} {Similarity}},
	isbn = {979-8-4007-0922-7},
	url = {https://doi.org/10.1145/3639233.3639335},
	doi = {10.1145/3639233.3639335},
	abstract = {Ensuring the retrieval of books that match users' preferences is of paramount importance. A significant challenge users encounter is uncertainty regarding their choice of search terms, often stemming from a limited understanding of the content or exposure to new concepts. Offering users results that closely resemble their query represents one potential solution. This research aims to suggest books relevant to students' course topics, utilizing cosine similarity to compute similarity values within each document in the collection.Performance evaluation using a similarity threshold greater than 0.1 revealed that the retrieved book results achieved an average precision of 0.7 and a recall value of 0.73, indicating substantial alignment with the search terms. The anticipated benefits of the recommendation system encompass the elimination of the need for manual book suggestions by staff, the provision of personalized book recommendations tailored to readers' preferences, a deeper understanding of library user behavior, and the effective promotion of new books that align with users' interests.},
	booktitle = {Proceedings of the 2023 7th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Nuipian, Vatinee and Chuaykhun, Jirawat},
	year = {2024},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {Text mining, Cosine similarity, Book Recommendation, Course Descriptions},
	pages = {273--277},
}

@inproceedings{anagnostou_ten_2025,
	address = {Orlando, Florida, USA},
	series = {{WSC} '24},
	title = {Ten {Years} of the {Hybrid} {Simulation} {Track}: {Reflections} and {Vision} for the {Future}},
	isbn = {979-8-3315-3420-2},
	abstract = {The Hybrid Simulation (HS) track was included in the Winter Simulation Conference (WSC) proceedings as a full conference track for the first time in 2014. A decade has passed since that inaugural track, and HS research and practice has seen impressive advancements during this time. This paper, based on a high-level review of the published works in the last ten years of the HS track, reflects on its successes and challenges and sets the scene for the future of the field. The paper is authored by the HS track organizers, both past and present, who report on the track's history, the nature of HS applications, the modeling tools and software available, as well as implementation challenges and the users' perspective. Finally, the paper discusses the future of HS.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Anagnostou, Anastasia and Brailford, Sally and Eldabi, Tillal and Mustafee, Navonil and Tako, Antuela},
	year = {2025},
	pages = {1245--1259},
}

@inproceedings{nebbia_hypernymization_2023,
	address = {New York, NY, USA},
	series = {{ICMR} '23},
	title = {Hypernymization of named entity-rich captions for grounding-based multi-modal pretraining},
	isbn = {979-8-4007-0178-8},
	url = {https://doi.org/10.1145/3591106.3592223},
	doi = {10.1145/3591106.3592223},
	abstract = {Named entities are ubiquitous in text that naturally accompanies images, especially in domains such as news or Wikipedia articles. In previous work, named entities have been identified as a likely reason for low performance of image-text retrieval models pretrained on Wikipedia and evaluated on named entities-free benchmark datasets. Because they are rarely mentioned, named entities could be challenging to model. They also represent missed learning opportunities for self-supervised models: the link between named entity and object in the image may be missed by the model, but it would not be if the object were mentioned using a more common term. In this work, we investigate hypernymization as a way to deal with named entities for pretraining grounding-based multi-modal models and for fine-tuning on open-vocabulary detection. We propose two ways to perform hypernymization: (1) a “manual” pipeline relying on a comprehensive ontology of concepts, and (2) a “learned” approach where we train a language model to learn to perform hypernymization. We run experiments on data from Wikipedia and from The New York Times. We report improved pretraining performance on objects of interest following hypernymization, and we show the promise of hypernymization on open-vocabulary detection, specifically on classes not seen during training.},
	booktitle = {Proceedings of the 2023 {ACM} {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Nebbia, Giacomo and Kovashka, Adriana},
	year = {2023},
	note = {event-place: Thessaloniki, Greece},
	keywords = {Pre-training, Benchmarking, Wikipedia, Performance, Multi-modal, grounding, hypernymization, named entities, open-vocabulary detection, Natural language processing systems, News articles, Named entities, Hypernymization, Image texts, Open-vocabulary detection, Wikipedia articles},
	pages = {67--75},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{sarkar_ai_2025,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '25},
	title = {{AI} {Could} {Have} {Written} {This}: {Birth} of a {Classist} {Slur} in {Knowledge} {Work}},
	isbn = {979-8-4007-1395-8},
	url = {https://doi.org/10.1145/3706599.3716239},
	doi = {10.1145/3706599.3716239},
	abstract = {AI shaming is a social phenomenon in which negative judgements are associated with the use of Artificial Intelligence (AI). This includes comparing someone’s work with AI-generated work as a means of disparagement, voicing suspicion or alleging that someone has used AI to undermine their reputation, or blaming the poor quality of an artefact on AI use. Common justifications of AI shaming include recourse to AI’s societal harms, its technical limitations, and lack of creativity. I argue that, more fundamentally than any of these, AI shaming arises from a class anxiety induced in middle class knowledge workers, and is a form of boundary work to maintain class solidarity and limit mobility into knowledge work. I discuss the role of AI shaming in protecting the privileged class of knowledge work and its attendant harms.},
	booktitle = {Proceedings of the {Extended} {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sarkar, Advait},
	year = {2025},
	keywords = {colonialism, class identity, epistemic injustice, moral panic, protectionism, sociotechnical imaginaries, sumptuary laws},
}

@inproceedings{li_domain_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {Domain {Specific} {Knowledge} {Graphs} as a {Service} to the {Public}: {Powering} {Social}-{Impact} {Funding} in the {US}},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3403330},
	doi = {10.1145/3394486.3403330},
	abstract = {Web and mobile technologies enable ubiquitous access to information. Yet, it is getting harder, even for subject matter experts, to quickly identify quality, trustworthy, and reliable content available online through search engines powered by advanced knowledge graphs. This paper explores the practical applications of Domain Specific Knowledge Graphs that allow for the extraction of information from trusted published and unpublished sources, to map the extracted information to an ontology defined in collaboration with sector experts, and to enable the public to go from single queries into ongoing conversations meeting their knowledge needs reliably. We focused on Social-Impact Funding, an area of need for over one million nonprofit organizations, foundations, government entities, social entrepreneurs, impact investors, and academic institutions in the US.},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \&amp; {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Li, Ying and Zakhozhyi, Vitalii and Zhu, Daniel and Salazar, Luis J.},
	year = {2020},
	note = {event-place: Virtual Event, CA, USA},
	keywords = {domain ontology, domain specific knowledge graph, social-impact funding},
	pages = {2793--2801},
}

@inproceedings{oliveira_stereotype_2020,
	address = {Daegu, Republic of Korea},
	series = {{HRI} '19},
	title = {The stereotype content model applied to human-robot interactions in groups},
	isbn = {978-1-5386-8555-6},
	abstract = {In this paper we sought to understand how the display of different levels of warmth and competence, as well as, different roles (opponent versus partner) portrayed by a robot, affect the display of emotional responses towards robots and how they can be used to predict future intention to work. For this purpose we devised an entertainment card-game group scenario involving two humans and two robots (n=54). The results suggest that different levels of warmth and competence are associated with distinct emotional responses from users and that these variables are useful in predicting future intention to work, thus hinting at the importance of considering warmth and competence stereotypes in Human-Robot Interaction.},
	booktitle = {Proceedings of the 14th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {IEEE Press},
	author = {Oliveira, Raquel and Arriaga, Patrícia and Correia, Filipa and Paiva, Ana},
	year = {2020},
	keywords = {emotions, human-robot interaction, stereotypes, autonomous robots},
	pages = {123--132},
}

@inproceedings{mecharnia_approach_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {An {Approach} {Toward} a {Prediction} of the {Presence} of {Asbestos} in {Buildings} {Based} on {Incomplete} {Temporal} {Descriptions} of {Marketed} {Products}},
	isbn = {978-1-4503-7008-0},
	url = {https://doi.org/10.1145/3360901.3364428},
	doi = {10.1145/3360901.3364428},
	abstract = {Since 1997, the production, import and sale of asbestosfootnoteNaturally occurring mineral fibres which were used due to their insulating properties. have been banned in France. However, there are still millions of tons scattered in factories, buildings, or hospitals. In this paper we propose a method for predicting the presence of asbestos products in buildings based on temporal data that describes the probability of the presence of asbestos in marketed products.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Mecharnia, Thamer and Chibout Khelifa, Lydia and Pernelle, Nathalie and Hamdi, Fayçal},
	year = {2019},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {ontology, prediction, temporal data, uncertain information},
	pages = {239--242},
}

@inproceedings{habba_perfect_2023,
	address = {New York, NY, USA},
	series = {{ICAIL} '23},
	title = {The {Perfect} {Victim}: {Computational} {Analysis} of {Judicial} {Attitudes} towards {Victims} of {Sexual} {Violence}},
	isbn = {979-8-4007-0197-9},
	url = {https://doi.org/10.1145/3594536.3595168},
	doi = {10.1145/3594536.3595168},
	abstract = {We develop computational models to analyze court statements in order to assess judicial attitudes toward victims of sexual violence in the Israeli court system. The study examines the resonance of "rape myths" in the criminal justice system's response to sex crimes, in particular in judicial assessment of victim's credibility. We begin by formulating an ontology for evaluating judicial attitudes toward victim's credibility, with eight ordinal labels and binary categorizations. Second, we curate a manually annotated dataset for judicial assessments of victim's credibility in the Hebrew language, as well as a model that can extract credibility labels from court cases. The dataset consists of 855 verdict decision documents in sexual assault cases from 1990-2021, annotated with the help of legal experts and trained law students. The model uses a combined approach of syntactic and latent structures to find sentences that convey the judge's attitude towards the victim and classify them according to the credibility label set. Our ontology, data, and models will be made available upon request, in the hope they spur future progress in this judicial important task.},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Habba, Eliya and Keydar, Renana and Bareket, Dan and Stanovsky, Gabriel},
	year = {2023},
	note = {event-place: Braga, Portugal},
	keywords = {Judicial decision making, Rape myths, Sexual violence, Witness credibility},
	pages = {111--120},
}

@inproceedings{kone_using_2020,
	address = {San Diego, CA, USA},
	series = {{SummerSim} '20},
	title = {Using hills as a common concrete syntax for ses and devs: application to microscopic simulation of traffic},
	isbn = {978-1-7138-1429-0},
	abstract = {We propose to use the High Level Language for System Specification as a common visual language for DEVS-based simulation model specification and SES-based domain ontological description. In doing so, we ensure continuity in knowledge representation and therefore reduces accidental errors while compressing the time to obtain the simulation model from the domain knowledge. This approach also offers a documentation basis to objective-based selection of elements of the ontological representation. We use a traffic modeling case to illustrate the effectiveness of our conceptual proposal.},
	booktitle = {Proceedings of the 2020 {Summer} {Simulation} {Conference}},
	publisher = {Society for Computer Simulation International},
	author = {Koné, Youssouf and Maïga, Oumar and Traoré, Mamadou K.},
	year = {2020},
	note = {event-place: Virtual Event, Spain},
	keywords = {discrete event systems specification (DEVS), high level language for system specification (HiLLS), microscopic traffic modeling, system entity structure /model base (SES/MB)},
}

@inproceedings{bonisoli_dice_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {{DICE}: a {Dataset} of {Italian} {Crime} {Event} news},
	isbn = {978-1-4503-9408-6},
	url = {https://doi.org/10.1145/3539618.3591904},
	doi = {10.1145/3539618.3591904},
	abstract = {Extracting events from news stories as the aim of several Natural Language Processing (NLP) applications (e.g., question answering, news recommendation, news summarization) is not a trivial task, due to the complexity of natural language and the fact that news reporting is characterized by journalistic style and norms. Those aspects entail scattering an event description over several sentences within one document (or more documents), applying a mechanism of gradual specification of event-related information. This implies a widespread use of co-reference relations among the textual elements, conveying non-linear temporal information. In addition to this, despite the achievement of state-of-the-art results in several tasks, high-quality training datasets for non-English languages are rarely available.This paper presents our preliminary study to develop an annotated Dataset for Italian Crime Event news (DICE). The contribution of the paper are: (1) the creation of a corpus of 10,395 crime news; (2) the annotation schema; (3) a dataset of 10,395 news with automatic annotations; (4) a preliminary manual annotation using the proposed schema of 1000 documents. The first tests on DICE have compared the performance of a manual annotator with that of single-span and multi-span question answering models and shown there is still a gap in the models, especially when dealing with more complex annotation tasks and limited training data. This underscores the importance of investing in the creation of high-quality annotated datasets like DICE, which can provide a solid foundation for training and testing a wide range of NLP models.},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Bonisoli, Giovanni and Di Buono, Maria Pia and Po, Laura and Rollo, Federica},
	year = {2023},
	note = {event-place: Taipei, Taiwan},
	keywords = {event extraction, nlp, question answering, 5ws, crime news},
	pages = {2985--2995},
}

@inproceedings{querrec_model_2018,
	address = {New York, NY, USA},
	series = {{IVA} '18},
	title = {Model for {Verbal} {Interaction} between an {Embodied} {Tutor} and a {Learner} in {Virtual} {Environments}},
	isbn = {978-1-4503-6013-5},
	url = {https://doi.org/10.1145/3267851.3267895},
	doi = {10.1145/3267851.3267895},
	abstract = {This research work introduce virtual embodied tutors in Virtual Environments for learning devoted to learning of procedures for industrial systems. We present a communicative behavior which, integrated in pedagogical scenario, permits on the one hand to realize the pedagogical communicative actions at a semantic level (e.g., the tutor explains the goal of an action) and on the other hand to realize such actions through human-like communicative channels (i.e., the virtual tutor's voice, facial expressions and gestures). The communicative behavior relies on a taxonomy of questions in order to interpret the learner's communicative actions and to generate the tutor's own questions.},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author = {Querrec, Ronan and Taoum, Joanna and Nakhal, Bilal and Bevacqua, Elisabetta},
	year = {2018},
	note = {event-place: Sydney, NSW, Australia},
	keywords = {Virtual Learning Environment, Embodied Conversational Agent, Intelligent Tutoring System, Interface, Verbal Interaction},
	pages = {197--202},
}

@article{ahmad_ensemble_2018,
	title = {Ensemble of {Deep} {Models} for {Event} {Recognition}},
	volume = {14},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3199668},
	doi = {10.1145/3199668},
	abstract = {In this article, we address the problem of recognizing an event from a single related picture. Given the large number of event classes and the limited information contained in a single shot, the problem is known to be particularly hard. To achieve a reliable detection, we propose a combination of multiple classifiers, and we compare three alternative strategies to fuse the results of each classifier, namely: (i) induced order weighted averaging operators, (ii) genetic algorithms, and (iii) particle swarm optimization. Each method is aimed at determining the optimal weights to be assigned to the decision scores yielded by different deep models, according to the relevant optimization strategy. Experimental tests have been performed on three event recognition datasets, evaluating the performance of various deep models, both alone and selectively combined. Experimental results demonstrate that the proposed approach outperforms traditional multiple classifier solutions based on uniform weighting, and outperforms recent state-of-the-art approaches.},
	number = {2},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {Ahmad, Kashif and Mekhalfi, Mohamed Lamine and Conci, Nicola and Melgani, Farid and Natale, Francesco De},
	month = may,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {CNN, genetic algorithms, deep neural networks, Event recognition, fusion, IOWA, multimedia indexing and retrieval, multiple classifiers, PSO},
}

@inproceedings{choi_concept-driven_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Concept-{Driven} {Visual} {Analytics}: an {Exploratory} {Study} of {Model}- and {Hypothesis}-{Based} {Reasoning} with {Visualizations}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300298},
	doi = {10.1145/3290605.3300298},
	abstract = {Visualization tools facilitate exploratory data analysis, but fall short at supporting hypothesis-based reasoning. We conducted an exploratory study to investigate how visualizations might support a concept-driven analysis style, where users can optionally share their hypotheses and conceptual models in natural language, and receive customized plots depicting the fit of their models to the data. We report on how participants leveraged these unique affordances for visual analysis. We found that a majority of participants articulated meaningful models and predictions, utilizing them as entry points to sensemaking. We contribute an abstract typology representing the types of models participants held and externalized as data expectations. Our findings suggest ways for rearchitecting visual analytics tools to better support hypothesis- and model-based reasoning, in addition to their traditional role in exploratory analysis. We discuss the design implications and reflect on the potential benefits and challenges involved.},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Choi, In Kwon and Childers, Taylor and Raveendranath, Nirmal Kumar and Mishra, Swati and Harris, Kyle and Reda, Khairi},
	year = {2019},
	note = {event-place: Glasgow, Scotland Uk},
	keywords = {visual analytics, mental models, hypothesis- and model-based reasoning, sensemaking},
	pages = {1--14},
}

@article{chan_instagram_2023,
	title = {Instagram {Text} {Sentiment} {Analysis} {Combining} {Machine} {Learning} and {NLP}},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3606370},
	doi = {10.1145/3606370},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Chan, Chia-Pang and Yang, Jun-He},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {machine learning, natural language processing, deep learning, Instagram, word embedding technology},
	annote = {Just Accepted},
}

@inproceedings{chen_detection_2024,
	address = {New York, NY, USA},
	series = {{ICIAI} '24},
	title = {Detection of greenwashing in {ESG} reports of {Chinese} listed companies based on {Word2vec} and {TF}-{IDF}},
	isbn = {979-8-4007-0930-2},
	url = {https://doi.org/10.1145/3655497.3655513},
	doi = {10.1145/3655497.3655513},
	abstract = {With the growing emphasis on ESG (Environmental, Social, and Governance) issues, the mandatory disclosure of ESG reports is on the horizon. However, due to the lack of a regulatory framework and a unified international ESG evaluation, the phenomenon of greenwashing in corporate ESG reporting is prevalent. We collected social responsibility reports and actual ESG performance data from A-share companies from 2011 to 2021 and innovatively employed text mining techniques to quantitatively investigate the extent of greenwashing in ESG reports. Our study initially utilized the Word2Vec method, combined with Skip-gram and Continuous Bag of Words models to train word vectors, and built an ESG lexicon using seed words. ESG reports is subsequently segmented based on a defined sentence splitting function and TF-IDF algorithm is employed to extract keywords. By matching the keywords with the ESG lexicon, we precisely extracted the annual ESG discourse for each company and conducted sentiment analysis to derive a greenwashing score. Heterogeneity analysis reveals that firm ownership has no significant impact on the level of greenwashing, yet the industry and region in which the enterprise operates considerably influence the greenwashing level. This study holds implications for enhancing the quality of ESG reporting and optimizing investment decisions.},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Innovation} in {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Yan and Ma, Ding},
	year = {2024},
	note = {event-place: Tokyo, Japan},
	keywords = {Text mining, Word2vec, ESG, Greenwashing detection, TF-IDF algorithm},
	pages = {159--164},
}

@inproceedings{pritchard_capabilities2_2025,
	address = {Melbourne, Australia},
	series = {{HRI} '25},
	title = {Capabilities2 for {ROS2}: {Advanced} {Skill}-{Based} {Control} for {Human}-{Robot} {Interaction}},
	abstract = {In the early days of the Open Source Robotics Foundation, a lesser-known project aimed to design an ”app-able robot”, leading to the creation of the ”Capabilities” package for the Robot Operating System. Over a decade later, formulating robot capabilities remains a significant technical hurdle in bringing robots from the lab into everyday life. This paper introduces Capabilities2, a successor to the original Capabilities package, now reimagined for ROS2. Capabilities2 enhances the original design by enabling advancements in skill-based control techniques and offering a more efficient, extensible framework for defining and utilising robot capabilities. We delve into its application in new real-world scenarios, with a particular focus on human-robot interactions and the deployment of collaborative mobile robots in human-centric environments. Capabilities2 addresses challenges in implementing intuitive, collaborative robots by introducing an abstracted database handler, an object-relational mapping for capability models, and a plugin architecture for capability execution. These features support dynamic capability representation, runtime adaptability, and integration with modern AI techniques for skill-based task planning. By providing a standardised yet flexible framework, Capabilities2 reduces the integration effort required to develop top-level controls for real-world scenarios, facilitating rapid development and deployment. Our contributions include the reimplementation of the Capabilities package in ROS2, enhancements to support contemporary robotic applications, and demonstrations of new use cases enabled by Capabilities2. We believe that Capabilities2 significantly advances the field of robotics by equipping developers with tools to create more capable, adaptable, and interactive robots. Capabilities2 is available at https://github.com/CollaborativeRoboticsLab/capabilities2},
	booktitle = {Proceedings of the 2025 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {IEEE Press},
	author = {Pritchard, Michael and Ratnayake, Kalana and Gamage, Buddhi and Jayasuriya, Maleen and Herath, Damith},
	year = {2025},
	keywords = {interface, communication, capability, skill, api, hri, package, provider, ros2, service, task},
	pages = {1067--1071},
}

@inproceedings{konstantinidis_knowledge-driven_2022,
	address = {New York, NY, USA},
	series = {{SETN} '22},
	title = {Knowledge-driven {Unsupervised} {Skills} {Extraction} for {Graph}-based {Talent} {Matching}},
	isbn = {978-1-4503-9597-7},
	url = {https://doi.org/10.1145/3549737.3549769},
	doi = {10.1145/3549737.3549769},
	abstract = {In human resource management of large organisations, finding the best candidate for a job description requires an extensive examination of a large number of resume profiles. Even with the advent of Deep Information Retrieval and the supported semantic similarity search, identification of relevant skills within profiles requires thorough investigation over several aspects, including educational background, professional experience, achievements, etc. However, these techniques are based on the existence of domain-specific, human-annotated datasets, a laborious task that portrays high cost and a slow labeling progress. In this paper, we propose Resume2Skill-SE, an end-to-end architecture for interpretable skill-based talent matching. The solution consists of two components. The first module uses an unsupervised approach for skills extraction based on state-of-the-art text embeddings and efficient semantic similarity search. The second module creates a profile-skills bipartite graph and uses a proposed ranking formula for similar resume profiles, minimising the effect of potential errors from the skills extraction module. The optimal ranking formula was identified through an intuitive and automated evaluation method for getting relevance scores. The proposed technique delivers promising results while also including an interpretability layer by showing the common skills of a pair of resume profiles.},
	booktitle = {Proceedings of the 12th {Hellenic} {Conference} on {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Konstantinidis, Ioannis and Maragoudakis, Manolis and Magnisalis, Ioannis and Berberidis, Christos and Peristeras, Vassilios},
	year = {2022},
	note = {event-place: Corfu, Greece},
	keywords = {natural language processing, search engine, graph analytics, unsupervised skills extraction},
}

@inproceedings{tsiligkaridis_encoding_2024,
	address = {New York, NY, USA},
	series = {{GeoAI} '24},
	title = {Encoding {Agent} {Trajectories} as {Representations} with {Sequence} {Transformers}},
	isbn = {979-8-4007-1176-3},
	url = {https://doi.org/10.1145/3687123.3698284},
	doi = {10.1145/3687123.3698284},
	abstract = {Spatiotemporal data faces many analogous challenges to natural language text including the ordering of locations (words) in a sequence, long range dependencies between locations, and locations having multiple meanings. In this work, we propose a novel model for representing high dimensional spatiotemporal trajectories as sequences of discrete locations and encoding them with a Transformer-based neural network architecture. Similar to language models, our Sequence Transformer for Agent Representation Encodings (STARE) model can learn representations and structure in trajectory data through both supervisory tasks (e.g., classification), and self-supervisory tasks (e.g., masked modelling). We present experimental results on various synthetic and real trajectory datasets and show that our proposed model can learn meaningful encodings that are useful for many downstream tasks including discriminating between labels and indicating similarity between locations. Using these encodings, we also learn relationships between agents and locations present in spatiotemporal data.},
	booktitle = {Proceedings of the 7th {ACM} {SIGSPATIAL} {International} {Workshop} on {AI} for {Geographic} {Knowledge} {Discovery}},
	publisher = {Association for Computing Machinery},
	author = {Tsiligkaridis, Athanasios and Kalinowski, Nicholas and Li, Zhongheng and Hou, Elizabeth},
	year = {2024},
	note = {event-place: Atlanta, GA, USA},
	keywords = {Transformers, encoders, human mobility, spatiotemporal data, trajectory modeling},
	pages = {38--49},
}

@article{laatar_disambiguating_2020,
	title = {Disambiguating {Arabic} {Words} {According} to {Their} {Historical} {Appearance} in the {Document} {Based} on {Recurrent} {Neural} {Networks}},
	volume = {19},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3410569},
	doi = {10.1145/3410569},
	abstract = {How can we determine the semantic meaning of a word in relation to its context of appearance? We eventually have to grabble with this difficult question, as one of the paramount problems of Natural Language Processing (NLP). In other words, this issue is commonly defined as Word Sense Disambiguation (WSD). The latter is one of the crucial difficulties within the NLP field. In this respect, word vectors extracted from a neural network model have been successfully applied for resolving the WSD problem. Accordingly, this article presents an unprecedented method to disambiguate Arabic words according to both their contextual appearance in a source text and the era in which they emerged. In fact, in the few previous decades, many researchers have been grabbling with Arabic Word Sense Disambiguation.It should be noted that the Arabic language can be divided into three major historical periods: old Arabic, middle-age Arabic, and contemporary Arabic. Actually, contemporary Arabic has proved to be the greatest concern of many researchers. The main gist of our work is to disambiguate Arabic words according to the historical period in which they appeared. To perform such a task, we suggest a method that deploys contextualized word embeddings to better gather valid syntactic and semantic information of the same word by taking into account its contextual uses. The preponderant thing is to convert both the senses and the contextual uses of an ambiguous item to vectors, then determine which of the possible conceptual meanings of the target word is closer to the given context.},
	number = {6},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Laatar, Rim and Aloulou, Chafik and Belguith, Lamia Hadrich},
	month = oct,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural language processing, contemporary arabic, contextualized word embeddings, historical dictionary, middle-age arabic, old arabic, recurrent neural networks, word sense disambiguation},
}

@inproceedings{rabiser_industry_2024,
	address = {New York, NY, USA},
	series = {{SPLC} '24},
	title = {Industry {Adoption} of {UVL}: {What} {We} {Will} {Need}},
	isbn = {979-8-4007-0593-9},
	url = {https://doi.org/10.1145/3646548.3676597},
	doi = {10.1145/3646548.3676597},
	abstract = {Since 2018, in the Software Product Line community, the MODEVAR initiative is working on coming up with a simple, standard variability modeling language and has proposed the Universal Variability Language (UVL). UVL has already been integrated in multiple tools such as FeatureIDE and FLAMA and it has also already been adopted by other academics outside the MODEVAR initiative. This short position paper outlines the challenges for industry adoption of UVL the community needs to work on.},
	booktitle = {Proceedings of the 28th {ACM} {International} {Systems} and {Software} {Product} {Line} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Rabiser, Rick},
	year = {2024},
	note = {event-place: Dommeldange, Luxembourg},
	keywords = {Challenges, Industry Adoption, Variability Modeling},
	pages = {46--49},
}

@inproceedings{win_towards_2023,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {Towards {Automated} {Detection} of {Unethical} {Behavior} in {Open}-{Source} {Software} {Projects}},
	isbn = {979-8-4007-0327-0},
	url = {https://doi.org/10.1145/3611643.3616314},
	doi = {10.1145/3611643.3616314},
	abstract = {Given the rapid growth of Open-Source Software (OSS) projects, ethical considerations are becoming more important. Past studies focused on specific ethical issues (e.g., gender bias and fairness in OSS). There is little to no study on the different types of unethical behavior in OSS projects. We present the first study of unethical behavior in OSS projects from the stakeholders’ perspective. Our study of 316 GitHub issues provides a taxonomy of 15 types of unethical behavior guided by six ethical principles (e.g., autonomy). Examples of new unethical behavior include soft forking (copying a repository without forking) and self-promotion (promoting a repository without self-identifying as contributor to the repository). We also identify 18 types of software artifacts affected by the unethical behavior. The diverse types of unethical behavior identified in our study (1) call for attentions of developers and researchers when making contributions in GitHub, and (2) point to future research on automated detection of unethical behavior in OSS projects. From our study, we propose Etor, an approach that can automatically detect six types of unethical behavior by using ontological engineering and Semantic Web Rule Language (SWRL) rules to model GitHub attributes and software artifacts. Our evaluation on 195,621 GitHub issues (1,765 GitHub repositories) shows that Etor can automatically detect 548 unethical behavior with 74.8\% average true positive rate (up to 100\% true positive rate). This shows the feasibility of automated detection of unethical behavior in OSS projects.},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Win, Hsu Myat and Wang, Haibo and Tan, Shin Hwei},
	year = {2023},
	note = {event-place: San Francisco, CA, USA},
	keywords = {Ethics in Software Engineering, Open-source software projects},
	pages = {644--656},
}

@inproceedings{wagih_coronavirus_2021,
	address = {New York, NY, USA},
	series = {{ArabWIC} 2021},
	title = {Coronavirus: {A} {Curse} or {A} {Bless} ?},
	isbn = {978-1-4503-8418-6},
	url = {https://doi.org/10.1145/3485557.3485566},
	doi = {10.1145/3485557.3485566},
	abstract = {Nowadays crime is one of the major threats that affect human lives. The current pandemic has a great impact on changing the criminal landscape. Extensive investigations for crime and criminal behaviors have revealed new crime patterns and led to the generation of a large amount of data and relations that need to be presented in a proper model. In this paper, we conduct several experiments on different datasets representing some major cities in the USA to study the effect of the current pandemic on crime types, rates, and intensity which can be used in crime prediction and prevention. we also introduce an ontology model with its underlying description logics as the knowledge representation model to represent crime information.},
	booktitle = {The 7th {Annual} {International} {Conference} on {Arab} {Women} in {Computing} in {Conjunction} with the 2nd {Forum} of {Women} in {Research}},
	publisher = {Association for Computing Machinery},
	author = {Wagih, Heba M. and Mokhtar, Hoda M. O.},
	year = {2021},
	note = {event-place: Sharjah, United Arab Emirates},
	keywords = {ontology, description logic, knowledge representation model},
}

@inproceedings{ducatteeuw_developing_2021,
	address = {New York, NY, USA},
	series = {{GeoHumanities} '21},
	title = {Developing an {Urban} {Gazetteer}: {A} {Semantic} {Web} {Database} for {Humanities} {Data}},
	isbn = {978-1-4503-9102-3},
	url = {https://doi.org/10.1145/3486187.3490204},
	doi = {10.1145/3486187.3490204},
	abstract = {This talk discusses the development of a spatiotemporal data model for an urban gazetteer. The function of gazetteers is to obtain descriptions uniquely identifying places referred to in discourse. Often, they are lists of places containing place name, feature type and geographical extent. Contemporary digital gazetteers (e.g. World Historical Gazetteer and Pleiades) are valuable tools for geographical knowledge of the past and the structuring of humanities data. However, scholars and GLAM (Galleries, Libraries, Archives and Museums) specialists often require information about entities on an intra-city scale. This presentation explores the model and implementation of an urban gazetteer using CIDOC CRM as a top-level ontology. The model will closely follow international gazetteer standards (i.e. Linked Places Format) in order to ensure interoperability with other gazetteer datasets. To move towards a FAIR (Findable, Accessible, Interoperable, and Reusable) approach, humanities data from the urban gazetteer will be published as Linked Open Data (LOD) and searchable via (Geo)SPARQL.},
	booktitle = {Proceedings of the 5th {ACM} {SIGSPATIAL} {International} {Workshop} on {Geospatial} {Humanities}},
	publisher = {Association for Computing Machinery},
	author = {Ducatteeuw, Vincent},
	year = {2021},
	note = {event-place: Beijing, China},
	keywords = {semantic technologies, GeoSPARQL, digital humanities, CIDOC CRM, linked open data, gazetteer development, modelling geohistorical data, spatial history, spatial humanities, spatiotemporal analysis, urban gazetteer, urban history},
	pages = {36--39},
}

@inproceedings{bao_words_2025,
	address = {New York, NY, USA},
	series = {{IUI} '25},
	title = {Words as {Bridges}: {Exploring} {Computational} {Support} for {Cross}-{Disciplinary} {Translation} {Work}},
	isbn = {979-8-4007-1306-4},
	url = {https://doi.org/10.1145/3708359.3712110},
	doi = {10.1145/3708359.3712110},
	abstract = {Scholars often explore literature outside of their home community of study. This exploration process is frequently hampered by field-specific jargon. Past computational work often focuses on supporting translation work by removing jargon through simplification and summarization; here, we explore a different approach that preserves jargon as useful bridges to new conceptual spaces. Specifically, we cast different scholarly domains as different language-using communities, and explore how to adapt techniques from unsupervised cross-lingual alignment of word embeddings to explore conceptual alignments between domain-specific word embedding spaces.We developed a prototype cross-domain search engine that uses aligned domain-specific embeddings to support conceptual exploration, and tested this prototype in two case studies. We discuss qualitative insights into the promises and pitfalls of this approach to translation work, and suggest design insights for future interfaces that provide computational support for cross-domain information seeking.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Bao, Calvin and Shiue, Yow-Ting and Carpuat, Marine and Chan, Joel},
	year = {2025},
	keywords = {cross-domain information seeking, information foraging, scholarly term translation},
	pages = {1598--1623},
}

@inproceedings{mahaini_building_2019,
	address = {New York, NY, USA},
	series = {{ARES} '19},
	title = {Building {Taxonomies} based on {Human}-{Machine} {Teaming}: {Cyber} {Security} as an {Example}},
	isbn = {978-1-4503-7164-3},
	url = {https://doi.org/10.1145/3339252.3339282},
	doi = {10.1145/3339252.3339282},
	abstract = {Taxonomies and ontologies are handy tools in many application domains such as knowledge systematization and automatic reasoning. In the cyber security field, many researchers have proposed such taxonomies and ontologies, most of which were built based on manual work. Some researchers proposed the use of computing tools to automate the building process, but mainly on very narrow sub-areas of cyber security. Thus, there is a lack of general cyber security taxonomies and ontologies, possibly due to the difficulties of manually curating keywords and concepts for such a diverse, inter-disciplinary and dynamically evolving field.This paper presents a new human-machine teaming based process to build taxonomies, which allows human experts to work with automated natural language processing (NLP) and information retrieval (IR) tools to co-develop a taxonomy from a set of relevant textual documents. The proposed process could be generalized to support non-textual documents and to build (more complicated) ontologies as well. Using the cyber security as an example, we demonstrate how the proposed taxonomy building process has allowed us to build a general cyber security taxonomy covering a wide range of data-driven keywords (topics) with a reasonable amount of human effort.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Mahaini, Mohamad Imad and Li, Shujun and Sağlam, Rahime Belen},
	year = {2019},
	note = {event-place: Canterbury, CA, United Kingdom},
	keywords = {ontology, visualization, knowledge representation, taxonomy, Twitter, natural language processing (NLP), cyber security, information retrieval (IR), online social network (OSN)},
}

@inproceedings{shimizu_pattern_2022,
	address = {New York, NY, USA},
	series = {{IJCKG} '21},
	title = {A {Pattern} for {Features} on a {Hierarchical} {Spatial} {Grid}},
	isbn = {978-1-4503-9565-6},
	url = {https://doi.org/10.1145/3502223.3502236},
	doi = {10.1145/3502223.3502236},
	abstract = {The integration of data along a common spatial component remains an obstacle in many problem spaces. One promising method for integrating data in such a way is through the use of a common, underlying spatial reference system, such as a Discrete Global Grid (e.g., the S2 Grid System), and pre-computing spatial relations between features and the constituent components at a spatial resolution appropriate for the data and use case. That is, by emphasizing the notion of the cell, we can examine what is in a cell, predict contents of its parent and child cells, and quickly get an overview of spatially co-located features and regions of interest without having to directly compute spatial interactions. This paper provides an ontology design pattern, to be used as a structural template, for modeling how features or regions map onto a hierarchical grid system and addresses how the attributes of these features may be inherited upwards or downwards through the hierarchy. We furthermore provide a motivating example and implementation.},
	booktitle = {Proceedings of the 10th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Shimizu, Cogan and Zhu, Rui and Mai, Gengchen and Fisher, Colby and Cai, Ling and Schildhauer, Mark and Janowicz, Krzysztof and Hitzler, Pascal and Zhou, Lu and Stephen, Shirly},
	year = {2022},
	note = {event-place: Virtual Event, Thailand},
	keywords = {ontology engineering, geoinformation science, ontology design pattern},
	pages = {108--114},
}

@inproceedings{alobaid_depicting_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {Depicting {Vocabulary} {Summaries} with {Devos}},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587359},
	doi = {10.1145/3543873.3587359},
	abstract = {Communicating ontologies to potential users is still a difficult and time-consuming task. Even for small ones, users need to invest time to determine whether to reuse them. Providing diagrams together with the ontologies facilitates the task of understanding the model from a user perspective. While some tools are available for depicting ontologies, and the code could also be inspected using ontology editors’ graphical interfaces, in many cases, the diagrams are too big or complex. The main objective of this demo is to present Devos, a system to generate ontology diagrams based on different strategies for summarizing the ontology.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Alobaid, Ahmad and Toledo, Jhon and Corcho, Oscar and Poveda-Villalón, María},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {ontology engineering, ontology diagrams, ontology summarization},
	pages = {250--253},
}

@article{crovari_gecoagent_2021,
	title = {{GeCoAgent}: {A} {Conversational} {Agent} for {Empowering} {Genomic} {Data} {Extraction} and {Analysis}},
	volume = {3},
	url = {https://doi.org/10.1145/3464383},
	doi = {10.1145/3464383},
	abstract = {With the availability of reliable and low-cost DNA sequencing, human genomics is relevant to a growing number of end-users, including biologists and clinicians. Typical interactions require applying comparative data analysis to huge repositories of genomic information for building new knowledge, taking advantage of the latest findings in applied genomics for healthcare. Powerful technology for data extraction and analysis is available, but broad use of the technology is hampered by the complexity of accessing such methods and tools.This work presents GeCoAgent, a big-data service for clinicians and biologists. GeCoAgent uses a dialogic interface, animated by a chatbot, for supporting the end-users’ interaction with computational tools accompanied by multi-modal support. While the dialogue progresses, the user is accompanied in extracting the relevant data from repositories and then performing data analysis, which often requires the use of statistical methods or machine learning. Results are returned using simple representations (spreadsheets and graphics), while at the end of a session the dialogue is summarized in textual format. The innovation presented in this article is concerned with not only the delivery of a new tool but also our novel approach to conversational technologies, potentially extensible to other healthcare domains or to general data science.},
	number = {1},
	journal = {ACM Trans. Comput. Healthcare},
	author = {Crovari, Pietro and Pidò, Sara and Pinoli, Pietro and Bernasconi, Anna and Canakoglu, Arif and Garzotto, Franca and Ceri, Stefano},
	month = oct,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Conversational agents, natural language understanding, genomic computing},
}

@inproceedings{verbitskaia_parser_2018,
	address = {New York, NY, USA},
	series = {Scala 2018},
	title = {Parser combinators for context-free path querying},
	isbn = {978-1-4503-5836-1},
	url = {https://doi.org/10.1145/3241653.3241655},
	doi = {10.1145/3241653.3241655},
	abstract = {Transparent integration of a domain-specific language for specification of context-free path queries (CFPQs) into a general-purpose programming language as well as static checking of errors in queries may greatly simplify the development of applications using CFPQs. LINQ and ORM can be used for the integration, but they have issues with flexibility: query decomposition and reusing of subqueries are a challenge. Adaptation of parser combinators technique for paths querying may solve these problems. Conventional parser combinators process linear input, and only the Trails library is known to apply this technique for path querying. Trails suffers the common parser combinators issue: it does not support left-recursive grammars and also experiences problems in cycles handling. We demonstrate that it is possible to create general parser combinators for CFPQ which support arbitrary context-free grammars and arbitrary input graphs. We implement a library of such parser combinators and show that it is applicable for realistic tasks.},
	booktitle = {Proceedings of the 9th {ACM} {SIGPLAN} {International} {Symposium} on {Scala}},
	publisher = {Association for Computing Machinery},
	author = {Verbitskaia, Ekaterina and Kirillov, Ilya and Nozkin, Ilya and Grigorev, Semyon},
	year = {2018},
	note = {event-place: St. Louis, MO, USA},
	keywords = {Neo4j, Graph Databases, Context-Free Language Reachability, Context-Free Path Querying, Generalized LL, GLL, Language-Constrained Path Problem, Parser Combinators, Scala},
	pages = {13--23},
}

@article{banar_transfer_2023,
	title = {Transfer {Learning} for the {Visual} {Arts}: {The} {Multi}-modal {Retrieval} of {Iconclass} {Codes}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3575865},
	doi = {10.1145/3575865},
	abstract = {Iconclass is an iconographic thesaurus, which is widely used in the digital heritage domain to describe subjects depicted in artworks. Each subject is assigned a unique descriptive code, which has a corresponding textual definition. The assignment of Iconclass codes is a challenging task for computational systems, due to the large number of available labels in comparison to the limited amount of training data available. Transfer learning has become a common strategy to overcome such a data shortage. In deep learning, transfer learning consists in fine-tuning the weights of a deep neural network for a downstream task. In this work, we present a deep retrieval framework, which can be fully fine-tuned for the task under consideration. Our work is based on a recent approach to this task, which already yielded state-of-the-art performance, although it could not be fully fine-tuned yet. This approach exploits the multi-linguality and multi-modality that is inherent to digital heritage data. Our framework jointly processes multiple input modalities, namely, textual and visual features. We extract the textual features from the artwork titles in multiple languages, whereas the visual features are derived from photographic reproductions of the artworks. The definitions of the Iconclass codes, containing useful textual information, are used as target labels instead of the codes themselves. As our main contribution, we demonstrate that our approach outperforms the state-of-the-art by a large margin. In addition, our approach is superior to the M3P feature extractor and outperforms the multi-lingual CLIP in most experiments due to the better quality of the visual features. Our out-of-domain and zero-shot experiments show poor results and demonstrate that the Iconclass retrieval remains a challenging task. We make our source code and models publicly available to support heritage institutions in the further enrichment of their digital collections.},
	number = {2},
	journal = {J. Comput. Cult. Herit.},
	author = {Banar, Nikolay and Daelemans, Walter and Kestemont, Mike},
	month = jun,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language processing, deep learning, cultural heritage, transfer learning, Iconclass, multi-lingual retrieval, multi-modal retrieval},
}

@article{osman_exploiting_2023,
	title = {Exploiting {Functional} {Discourse} {Grammar} to {Enhance} {Complex} {Arabic} {Relation} {Extraction} using a {Hybrid} {Semantic} {Knowledge} {Base} - {Machine} {Learning} {Approach}},
	volume = {22},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3610581},
	doi = {10.1145/3610581},
	abstract = {Relation extraction from unstructured Arabic text is especially challenging due to the Arabic language complex morphology and the variation in word semantics and lexical categories. The research documented in this paper presents a hybrid Semantic Knowledge base - Machine Learning (SKML) approach for extracting complex Arabic relations from unstructured Arabic documents; the proposed approach exploits the principles of Functional Discourse Grammar (FDG) to emphasise the semantic and pragmatic properties of the language and facilitate the identification of relation elements. At the initial phase, the novel FDG-SKML relation extraction approach deploys a lexical-based mechanism that utilises a purposely built domain-specific Semantic Knowledge to encode the semantic association between the identified relations’ elements. The evaluation of the initial stage evidenced improved accuracy for extracting most complex Arabic relations. The initial relation extraction mechanism was further extended by integrating its output into a Machine Learning classifier that facilitated extracting especially complex relations with significant disparity in the relation elements’ presence, order, and correlation. Using Economics as the problem domain, experimental evaluation evidenced the high accuracy of our FDG-SKML approach in complex Arabic relation extraction task and demonstrated its further improvement upon integration with machine learning classifiers.},
	number = {8},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Osman, Taha and Khalil, Hussein and Miltan, Mohammed and Shaalan, Khaled and Alfrjani, Rowida},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Natural Language Processing, Arabic relation extraction, Functional Discourse Grammar, hybrid knowledge-based machine learning classification, semantic web base},
}

@inproceedings{lin_adaptive_2023,
	address = {New York, NY, USA},
	series = {{ICETT} '23},
	title = {Adaptive {Learning} {System} {Based} on {Knowledge} {Graph}},
	isbn = {978-1-4503-9959-3},
	url = {https://doi.org/10.1145/3599640.3599647},
	doi = {10.1145/3599640.3599647},
	abstract = {Since the rapid development of "Internet+Education" , various artificial intelligence technologies have been applied to teaching and learning. The development of technology has brought great impetus and potential to education. Based on the background of big data, how to use AI technology to mine valuable information in massive data to meet the adaptive learning needs of learners is an important topic that deserves attention and research. This paper builds learner ontology and course knowledge ontology, links learning resources to course knowledge ontology to build domain knowledge graph, designs and implements an adaptive learning system based on knowledge graph, including learner model, domain knowledge model, adaptive learning engine and interactive interface. Finally, Pellet inference engine is used to infer the designed SWRL rules. The result shows that the adaptive learning system proposed in this paper can recommend appropriate learning paths according to the learners' learning status, and present personalized learning resources.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Education} and {Training} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Longcheng and Wang, Fang},
	year = {2023},
	note = {event-place: Macau, China},
	keywords = {knowledge graph, recommendation, personalized learning, ontology construction, Adaptive learning system},
}

@inproceedings{rani_ttphunter_2023,
	address = {New York, NY, USA},
	series = {{ACSW} '23},
	title = {{TTPHunter}: {Automated} {Extraction} of {Actionable} {Intelligence} as {TTPs} from {Narrative} {Threat} {Reports}},
	isbn = {979-8-4007-0005-7},
	url = {https://doi.org/10.1145/3579375.3579391},
	doi = {10.1145/3579375.3579391},
	abstract = {With the proliferation of attacks from various Advanced Persistent Threats (APT) groups, it is essential to comprehend the threat actor’s attack patterns to accelerate threat detection and response. The MITRE ATT\&amp;CK framework’s Tactics, Techniques, and Procedures (TTPs) help to decipher attack patterns. The APT reports, published by security firms, contain rich information on tools and techniques used by threat actors. These reports are available in unstructured and natural language texts. There is a need for an automated tool to extract TTPs present in natural language text. However, there are few tools available in the literature, but their performance is not very satisfactory. In this work, we propose TTPHunter, to extract TTPs from APT reports by mapping sentence context to relevant TTPs. We fine-tune linear classifiers, which take input as BERT (Bidirectional Encoder Representations from Transformers) embeddings of sentences. We create two datasets: sentence-based (8,387 sentence samples) and document-based (50 threat reports) to validate TTPHunter. TTPHunter achieves the F1-score of 88\% and 75\% for both datasets, respectively. We compare the TTPHunter with rcATT and AttacKG baseline models, and it outperforms both baselines.},
	booktitle = {Proceedings of the 2023 {Australasian} {Computer} {Science} {Week}},
	publisher = {Association for Computing Machinery},
	author = {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla, Sandeep Kumar},
	year = {2023},
	note = {event-place: Melbourne, VIC, Australia},
	keywords = {Cybersecurity, Natural Language Processing, CK, MITRE ATT\&amp, Threat Intelligence, TTP Extraction},
	pages = {126--134},
}

@inproceedings{amsterdamer_declarative_2019,
	address = {New York, NY, USA},
	series = {{CIKM} '19},
	title = {Declarative {User} {Selection} with {Soft} {Constraints}},
	isbn = {978-1-4503-6976-3},
	url = {https://doi.org/10.1145/3357384.3358025},
	doi = {10.1145/3357384.3358025},
	abstract = {In applications with large userbases such as crowdsourcing, social networks or recommender systems, selecting users is a common and challenging task. Different applications require different policies for selecting users, and implementing such policies is applicationspecific and laborious. To this end, we introduce a novel declarative framework that abstracts common components of the user selection problem, while allowing for domain-specific tuning. The framework is based on an ontology view of user profiles, with respect to which we define a query language for policy specification. Our language extends SPARQL with means for capturing soft constraints which are essential for worker selection. At the core of our query engine is then a novel efficient algorithm for handling these constraints. Our experimental study on real-life data indicates the effectiveness and flexibility of our approach, showing in particular that it outperforms existing task-specific solutions in prominent user selection tasks.},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Amsterdamer, Yael and Milo, Tova and Somech, Amit and Youngmann, Brit},
	year = {2019},
	note = {event-place: Beijing, China},
	keywords = {semantic similarity, sparql, user selection},
	pages = {931--940},
}

@inproceedings{baughman_large_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Large {Scale} {Generative} {AI} {Text} {Applied} to {Sports} and {Music}},
	isbn = {979-8-4007-0490-1},
	url = {https://doi.org/10.1145/3637528.3671542},
	doi = {10.1145/3637528.3671542},
	abstract = {We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the GRAMMY awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforementioned events, supporting 90 million fans around the world with 8 billion page views, continuously pushing the bounds on what is possible at the intersection of sports, entertainment, and AI.},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Baughman, Aaron and Morales, Eduardo and Agarwal, Rahul and Akay, Gozde and Feris, Rogerio and Johnson, Tony and Hammer, Stephen and Karlinsky, Leonid},
	year = {2024},
	note = {event-place: Barcelona, Spain},
	keywords = {neural networks, generative ai, applied computing, large scale computing, sports and entertainment},
	pages = {4784--4792},
}

@article{aouachria_process_2024,
	title = {A {Process} {Mining} {Method} for {Inter}-organizational {Business} {Process} {Integration}},
	volume = {15},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3638062},
	doi = {10.1145/3638062},
	abstract = {Business process integration (BPI) allows organizations to connect and automate their business processes in order to deliver the right economic resources at the right time, place, and price. BPI requires the integration of business processes and their supporting systems across multiple autonomous organizations. However, such integration is complex and can face coordination complexities that occur during the resource exchanges between the partners’ processes. This article proposes a new method called Process Mining for Business Process Integration (PM4BPI) that helps process designers to perform BPI by creating new process models that cross the boundaries of multiple organizations from a collection of process event logs. PM4BPI uses federated process mining techniques to detect incompatibilities before the integration of the partners’ processes. Then, it applies process adaptation patterns to solve detected incompatibilities. Finally, organizations’ processes are merged to build a collaborative process model that crosses the organizations’ boundaries. AdaptWF\_Net, an extension of a Petri net, is used to design inter-organizational business processes and adaptation patterns. An integrated care pathway is used as a case study to assess the applicability and effectiveness of the proposed method.},
	number = {1},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Aouachria, Moufida and Leshob, Abderrahmane and Ghomari, Abdessamed Réda and Aouache, Mustapha},
	month = mar,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {process modelling, process mining, adaptation patterns, Business process integration, inter-organizational business process model},
}

@inproceedings{shan_research_2022,
	address = {New York, NY, USA},
	series = {{EITCE} '21},
	title = {Research on the {Construction} of {Domain} {Sentiment} {Lexicon} {Based} on {Label} {Propagation} {Algorithm}},
	isbn = {978-1-4503-8432-2},
	url = {https://doi.org/10.1145/3501409.3501592},
	doi = {10.1145/3501409.3501592},
	abstract = {With the rapid development of the Internet and the explosive growth of online information, timely monitoring and guidance of public opinion is the key to maintaining a safe online environment. Sentiment lexicon is an important corpus resource in fields such as opinion analysis, and when faced with tasks in different domains, generic sentiment lexicon has been difficult to meet the demand, so researchers have remembered to focus on domain sentiment lexicon. In this paper, we propose a method based on improved label propagation to achieve automatic construction of Chinese sentiment lexicon from the sentence-level text. The key idea is to use lexical rules and sentiment association corrections to take the computational approach of optimizing mutual information of points, both of which improve the algorithm's effectiveness in analyzing complex sentences and thus enhance the accuracy of sentiment word recognition. The experimental results show that the method can automatically build a sentiment lexicon according to the corpus, with high quality and certain domain adaptability.},
	booktitle = {Proceedings of the 2021 5th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shan, Ruikang and Jiang, Tao and Wang, Yetong},
	year = {2022},
	note = {event-place: Xiamen, China},
	keywords = {Sentiment analysis, Domain sentiment lexicon, Label propagation, Pointwise mutual information, Public opinion analysis},
	pages = {1024--1029},
}

@inproceedings{sierra-munera_efficient_2024,
	address = {Santa Fe, New Mexico, USA},
	series = {{JCDL} '23},
	title = {Efficient {Ultrafine} {Typing} of {Named} {Entities}},
	isbn = {979-8-3503-9931-8},
	url = {https://doi.org/10.1109/JCDL57899.2023.00038},
	doi = {10.1109/JCDL57899.2023.00038},
	abstract = {Ultrafine named entity typing (UFET) refers to the assignment of predefined labels to entity mentions in a given context. In contrast to traditional named entity typing, the number of potential labels is in the thousands and one mention can have more than one assigned type. Previous approaches either depend on large training datasets, or require inefficient encoding of all input-type combinations. Therefore, there is a need for investigating the efficiency during training and prediction of entity typing models in the ultrafine-grained setting, considering its distinctively bigger search space, compared to the coarse- and fine-grained tasks. To efficiently solve UFET, we propose Decent, a lightweight model that encodes, using a pretrained language model, the input sentences separately from the type labels. Additionally, we make use of negative oversampling to speed up the training while improving the generalization of unseen types. Using an openly available UFET dataset, we evaluated the classification and runtime performance of Decent and observed that training and prediction runtime is orders of magnitude faster than the current state-of-the-art approaches, while maintaining a competitive classification performance.},
	booktitle = {Proceedings of the 2023 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Sierra-Múnera, Alejandro and Westphal, Jan and Krestel, Ralf},
	year = {2024},
	note = {ISSN: 2575-8152},
	keywords = {Semantics, Vocabulary, named entity recognition, Training, Computational modeling, ultrafine enity typing, Limiting, Predictive models, Runtime},
	pages = {205--214},
}

@inproceedings{egami_vhakg_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {{VHAKG}: {A} {Multi}-modal {Knowledge} {Graph} {Based} on {Synchronized} {Multi}-view {Videos} of {Daily} {Activities}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679175},
	doi = {10.1145/3627673.3679175},
	abstract = {Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data (e.g., images and videos) into symbols, have attracted attention as resources enabling knowledge processing and machine learning across modalities. However, the construction of MMKGs for videos consisting of multiple events, such as daily activities, is still in the early stages. In this paper, we construct an MMKG based on synchronized multi-view simulated videos of daily activities. Besides representing the content of daily life videos as event-centric knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as bounding boxes within video frames. In addition, we provide support tools for querying our MMKG. As an application example, we demonstrate that our MMKG facilitates benchmarking vision-language models by providing the necessary vision-language datasets for a tailored task.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Egami, Shusaku and Ugai, Takanori and Htun, Swe Nwe Nwe and Fukuda, Ken},
	year = {2024},
	note = {event-place: Boise, ID, USA},
	keywords = {synthetic data, daily life video, event-centric knowledge graph, multi-modal knowledge graph, visual question answering},
	pages = {5360--5364},
}

@inproceedings{reda_towards_2018,
	address = {New York, NY, USA},
	series = {{DH} '18},
	title = {Towards {Consistent} {Data} {Representation} in the {IoT} {Healthcare} {Landscape}},
	isbn = {978-1-4503-6493-5},
	url = {https://doi.org/10.1145/3194658.3194668},
	doi = {10.1145/3194658.3194668},
	abstract = {Nowadays, the enormous volume of health and fitness data gathered from IoT wearable devices offers favourable opportunities to the research community. For instance, it can be exploited using sophisticated data analysis techniques, such as automatic reasoning, to find patterns and, extract information and new knowledge in order to enhance decision-making and deliver better healthcare. However, due to the high heterogeneity of data representation formats, the IoT healthcare landscape is characterised by an ubiquitous presence of data silos which prevents users and clinicians from obtaining a consistent representation of the whole knowledge. Semantic web technologies, such as ontologies and inference rules, have been shown as a promising way for the integration and exploitation of data from heterogeneous sources. In this paper, we present a semantic data model useful to: (1) consistently represent health and fitness data from heterogeneous IoT sources; (2) integrate and exchange them; and (3) enable automatic reasoning by inference engines.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Digital} {Health}},
	publisher = {Association for Computing Machinery},
	author = {Reda, Roberto and Piccinini, Filippo and Carbonaro, Antonella},
	year = {2018},
	note = {event-place: Lyon, France},
	keywords = {internet of things, health informatics, ontology-based data representation, semantic web technologies},
	pages = {5--10},
}

@inproceedings{robert_integrated_2023,
	address = {New York, NY, USA},
	series = {{IMX} '23},
	title = {An {Integrated} {Framework} for {Understanding} {Multimodal} {Embodied} {Experiences} in {Interactive} {Virtual} {Reality}},
	isbn = {979-8-4007-0028-6},
	url = {https://doi.org/10.1145/3573381.3596150},
	doi = {10.1145/3573381.3596150},
	abstract = {Virtual Reality (VR) technology enables “embodied interactions” in realistic environments where users can freely move and interact, with deep physical and emotional states. However, a comprehensive understanding of the embodied user experience is currently limited by the extent to which one can make relevant observations, and the accuracy at which observations can be interpreted. Paul Dourish proposed a way forward through the characterisation of embodied interactions in three senses: ontology, intersubjectivity, and intentionality. In a joint effort between computer and neuro-scientists, we built a framework to design studies that investigate multimodal embodied experiences in VR, and apply it to study the impact of simulated low-vision on user navigation. Our methodology involves the design of 3D scenarios annotated with an ontology, modelling intersubjective tasks, and correlating multimodal metrics such as gaze and physiology to derive intentions. We show how this framework enables a more fine-grained understanding of embodied interactions in behavioural research.},
	booktitle = {Proceedings of the 2023 {ACM} {International} {Conference} on {Interactive} {Media} {Experiences}},
	publisher = {Association for Computing Machinery},
	author = {Robert, Florent and Wu, Hui-Yin and Sassatelli, Lucile and Ramanoël, Stephen and Gros, Auriane and Winckler, Marco},
	year = {2023},
	note = {event-place: Nantes, France},
	keywords = {interaction, 3D environments, Embodied experiences, immersion, navigation, scene ontology, task modeling, user experience analysis},
	pages = {14--26},
}

@inproceedings{falcarin_building_2024,
	address = {New York, NY, USA},
	series = {{EnCyCriS}/{SVM} '24},
	title = {Building a {Cybersecurity} {Knowledge} {Graph} with {CyberGraph}},
	isbn = {979-8-4007-0565-6},
	url = {https://doi.org/10.1145/3643662.3643962},
	doi = {10.1145/3643662.3643962},
	abstract = {Software engineers and security professionals rely on a variety of sources of information, including known vulnerabilities, newly identified weaknesses, and threats, as well as attack patterns and current mitigations. Such information, spread across different places, results in an increased effort for developers in following all the cross-referenced data and finding appropriate solutions to their security issues in a timely manner. Software developers cannot have a good knowledge of the breadth of the different issues and vulnerabilities that are constantly increasing in time; the raising number of security issues to tackle cannot be matched by software developers which need more help from intelligent tools. Therefore, in this work, we present CyberGraph, a tool to automatically build and update a single, easily queryable cybersecurity knowledge graph by automatically linking heterogeneous data from different public repositories. The resulting unique integrated dataset, thanks to its magnitude, allows the execution of sophisticated queries that can quickly provide new insights and valuable perspectives.},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} 4th {International} {Workshop} on {Engineering} and {Cybersecurity} of {Critical} {Systems} ({EnCyCriS}) and 2024 {IEEE}/{ACM} {Second} {International} {Workshop} on {Software} {Vulnerability}},
	publisher = {Association for Computing Machinery},
	author = {Falcarin, Paolo and Dainese, Fabio},
	year = {2024},
	note = {event-place: Lisbon, Portugal},
	keywords = {visualization, knowledge graph, Neo4j, cybersecurity, MITRE, software vulnerabilities},
	pages = {29--36},
}

@inproceedings{guan_ca-wge_2024,
	address = {New York, NY, USA},
	series = {{AIPR} '23},
	title = {{CA}-{WGE}: {A} two-view graph neural network-based knowledge graph completion approach combining common sense perception},
	isbn = {979-8-4007-0767-4},
	url = {https://doi.org/10.1145/3641584.3641793},
	doi = {10.1145/3641584.3641793},
	abstract = {The knowledge graph completion algorithm can make the knowledge graph more complete and is currently a research hotspot in the field of artificial intelligence. The knowledge graph completion model is mainly defined in three aspects, the way of negative example generation, the design of scoring function and the design of loss function. The previous knowledge graph completion models only rely on factual view data to predict the missing links between entities and ignore the valuable common sense knowledge, and there is invalid negative sampling in knowledge graph embedding techniques; on the other hand, the existing graph neural network-based knowledge graph embedding models mainly consider capturing the graph structure around entities, and the relational representation is only used to update the entity embedding, which may miss the potentially useful information about the relational structure of potentially useful information. To address the above challenges, this paper proposes a two-view graph neural network-based knowledge graph completion model combined with common sense awareness. Common knowledge is first automatically extracted from fact triples with entity concepts to facilitate high-quality negative sampling, and then positive and weighted negative triples are fed into the two-view graph neural network-based knowledge graph embedding model to capture entity- and relationship-centric graph structures and learn vector representations of entities and relationships, and then the learned entity and relationship representations are fed into a weighted score function to return the final the final score. Extensive experimental and ablation studies on four datasets, FB15K, FB15K237, NELL995, and DBpedia-242, show that the model achieves better performance compared to the state-of-the-art models.},
	booktitle = {Proceedings of the 2023 6th {International} {Conference} on {Artificial} {Intelligence} and {Pattern} {Recognition}},
	publisher = {Association for Computing Machinery},
	author = {Guan, Wei and Lian, Xiaoru and Ma, Li},
	year = {2024},
	note = {event-place: Xiamen, China},
	keywords = {Graph Neural Network, Knowledge Graph Completion, Knowledge Graph Embedding, Common Sense Awareness, Negative Sampling},
	pages = {1382--1389},
}

@article{lian_vision_2025,
	title = {Vision to {Specification}: {Automating} the {Transition} from {Conceptual} {Features} to {Functional} {Requirements}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3754450},
	doi = {10.1145/3754450},
	abstract = {The translation of high-level abstract features into clear, and testable functional requirements (FRs) is a crucial step in software development, bridging the gap between user needs and technical specifications. In engineering practice, significant expert effort is needed for this translation. Our approach, EasyFR, streamlines the process by recommending Semantic Role Labeling (SRL) sequences for the given abstract features to guide Pre-trained Language Models (PLMs) in producing cohesive FR statements. By analyzing ten diverse datasets, we induce two variable SRL templates, each including two configurable parts. For concrete features, our proposed Key2Temp model can construct the appropriate variant of the SRL template by identifying a variable SRL template and placing the feature tokens in the appropriate slots. In this way, our approach reframes the process of requirement generation into a structured slot-filling activity. Experimental validation on four open datasets demonstrates that EasyFR outperforms three advanced Natural language generation (NLG) approaches, including GPT-4, particularly when existing FRs are available for training. The positive influence of our SRL template variant recommendations is further confirmed through an ablation study. We believe that our results indicate a notable step forward in the realm of automated requirements synthesis, holding potential to improve the process of requirements specification in future software projects.},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Lian, Xiaoli and Wu, Jiajun and Gao, Xiaoyun and Wang, Shuaisong and Zhang, Li},
	month = sep,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Features, Semantic Templates, Slot Filling, Software Requirements Synthesis},
	annote = {Just Accepted},
}

@inproceedings{alpizar-chacon_order_2020,
	address = {New York, NY, USA},
	series = {{DocEng} '20},
	title = {Order out of {Chaos}: {Construction} of {Knowledge} {Models} from {PDF} {Textbooks}},
	isbn = {978-1-4503-8000-3},
	url = {https://doi.org/10.1145/3395027.3419585},
	doi = {10.1145/3395027.3419585},
	abstract = {Textbooks are educational documents created, structured and formatted by domain experts with the main purpose to explain the knowledge in the domain to a novice. Authors use their understanding of the domain when structuring and formatting the content of a textbook to facilitate this explanation. As a result, the formatting and structural elements of textbooks carry the elements of domain knowledge implicitly encoded by their authors. Our paper presents an extendable approach towards automated extraction of this knowledge from textbooks taking into account their formatting rules and internal structure. We focus on PDF as the most common textbook representation format; however, the overall method is applicable to other formats as well. The evaluation experiments examine the accuracy of the approach, as well as the pragmatic quality of the obtained knowledge models using one of their possible applications – semantic linking of textbooks in the same domain. The results indicate high accuracy of model construction on symbolic, syntactic and structural levels across textbooks and domains, and demonstrate the added value of the extracted models on the semantic level.},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Document} {Engineering} 2020},
	publisher = {Association for Computing Machinery},
	author = {Alpizar-Chacon, Isaac and Sosnovsky, Sergey},
	year = {2020},
	note = {event-place: Virtual Event, CA, USA},
	keywords = {knowledge modeling, model extraction, PDF processing, textbook},
}

@inproceedings{guo_sovar_2024,
	address = {New York, NY, USA},
	series = {{ASE} '24},
	title = {{SoVAR}: {Build} {Generalizable} {Scenarios} from {Accident} {Reports} for {Autonomous} {Driving} {Testing}},
	isbn = {979-8-4007-1248-7},
	url = {https://doi.org/10.1145/3691620.3695037},
	doi = {10.1145/3691620.3695037},
	abstract = {Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications. However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved. Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving. Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds. However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction. Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges.In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports. SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model (LLM) in extracting accident information from textual data. Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories. Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs. We experiment with SoVAR, using the accident reports from the National Highway Traffic Safety Administration's (NHTSA) database to generate test scenarios for the industrial-grade ADS Apollo. The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures. Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo.},
	booktitle = {Proceedings of the 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Guo, An and Zhou, Yuan and Tian, Haoxiang and Fang, Chunrong and Sun, Yunjian and Sun, Weisong and Gao, Xinyu and Luu, Anh Tuan and Liu, Yang and Chen, Zhenyu},
	year = {2024},
	note = {event-place: Sacramento, CA, USA},
	keywords = {automatic test generation, autonomous driving system, constraint solving, software testing},
	pages = {268--280},
}

@inproceedings{babur_mocop_2019,
	address = {Montreal, Quebec, Canada},
	series = {{MiSE} '19},
	title = {{MoCoP}: towards a model clone portal},
	url = {https://doi.org/10.1109/MiSE.2019.00019},
	doi = {10.1109/MiSE.2019.00019},
	abstract = {Widespread and mature practice of model-driven engineering is leading to a growing number of modeling artifacts and challenges in their management. Model clone detection (MCD) is an important approach for managing and maintaining modeling artifacts. While its counterpart in traditional source code development, code clone detection, is enjoying popularity and more than two decades of development, MCD is still in its infancy in terms of research and tooling. We aim to develop a portal for model clone detection, MoCoP, as a central hub to mitigate adoption barriers and foster MCD research. In this short paper, we present our vision for MoCoP and its features and goals. We discuss MoCoP's key components that we plan on realizing in the short term including public tooling, curated data sets, and a body of MCD knowledge. Our longer term goals include a dedicated service-oriented infrastructure, contests, and forums. We believe MoCoP will strengthen MCD research, tooling, and the community, which in turn will lead to better quality, maintenance, and scalability for model-driven engineering practices.},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Modelling} in {Software} {Engineerings}},
	publisher = {IEEE Press},
	author = {Babur, Önder and Stephan, Matthew},
	year = {2019},
	keywords = {model management, model-driven engineering, model analytics, model clone detection, model repositories, software maintenance},
	pages = {78--81},
}

@article{lim_adaptive_2023,
	title = {Adaptive {Multi}-{Domain} {Dialogue} {State} {Tracking} on {Spoken} {Conversations}},
	volume = {32},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2023.3302232},
	doi = {10.1109/TASLP.2023.3302232},
	abstract = {The main objective of the task-oriented dialogue system is to identify the intent and needs of human dialogue. Many existing studies are conducted under the setting of written dialogue, but there always exists a difficulty in coping with real-world spoken dialogues. To this end, DSTC10 challenge organizers propose the task of building robust dialogue state tracking (DST) models on spoken dialogues. With the powerful existing DST model (i.e., MinTL), this article suggests integral components for building a dialogue state tracker; 1) Data augmentation effectively enhances the capability of the model to catch the entities that exist in the evaluation dataset. 2) Levenshtein post-processing aims to prevent the distortion in model prediction caused by automatic speech recognition errors. To validate the effectiveness of our methods, we evaluate our model on DSTC10 datasets and conduct qualitative analysis by ablating each component of the model. Experimental results show that our model significantly outperforms baselines in all evaluation metrics and took 3rd place in the challenge.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Lim, Jungwoo and Whang, Taesun and Lee, Dongyub and Lim, Heuiseok},
	month = aug,
	year = {2023},
	note = {Publisher: IEEE Press},
	pages = {727--732},
}

@inproceedings{zhao_drminer_2024,
	address = {New York, NY, USA},
	series = {{ASE} '24},
	title = {{DRMiner}: {Extracting} {Latent} {Design} {Rationale} from {Jira} {Issue} {Logs}},
	isbn = {979-8-4007-1248-7},
	url = {https://doi.org/10.1145/3691620.3695019},
	doi = {10.1145/3691620.3695019},
	abstract = {Software architectures are usually meticulously designed to address multiple quality concerns and support long-term maintenance. However, there may be a lack of motivation for developers to document design rationales (i.e., the design alternatives and the underlying arguments for making or rejecting decisions) when they will not gain immediate benefit, resulting in a lack of standard capture of these rationales. With the turnover of developers, the architecture inevitably becomes eroded. This issue has motivated a number of studies to extract design knowledge from open-source communities in recent years. Unfortunately, none of the existing research has successfully extracted solutions alone with their corresponding arguments due to challenges such as the intricate semantics of online discussions and the lack of benchmarks for design rationale extraction.In this paper, we propose a novel approach, named DRMiner, to automatically mine latent design rationales from developers' live discussion in open-source community (i.e., issue logs in Jira). To better identify solutions and their relevant arguments, DRMiner skillfully decomposes the problem into multiple text classification tasks and tackles them using prompt tuning of large language models (LLMs) and specific heuristic features. To evaluate DRMiner, we acquire issue logs from Cassandra, Flink, and Solr repositories in Jira and form a dataset for design rationale mining. Experimental results show that DRMiner outperforms all baselines and achieves F1 improvements of 24\%, 22\%, and 20\% for mining design rationales, solutions, and arguments, respectively, compared to the best baseline. Furthermore, we investigate the usefulness of the design rationales mined by DRMiner for automated program repair (APR) and find that advanced LLMs, when prompted with these extracted rationales, generate 10×-18× more full-match patches and achieve a 10\%-13\% gain in CodeBLEU scores.},
	booktitle = {Proceedings of the 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Jiuang and Yang, Zitian and Zhang, Li and Lian, Xiaoli and Yang, Donghao and Tan, Xin},
	year = {2024},
	note = {event-place: Sacramento, CA, USA},
	keywords = {design discussion, design rationale, design recovery, issue logs, program maintenance},
	pages = {468--480},
}

@article{nayak_experience_2021,
	title = {Experience: {Automated} {Prediction} of {Experimental} {Metadata} from {Scientific} {Publications}},
	volume = {13},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3451219},
	doi = {10.1145/3451219},
	abstract = {While there exists an abundance of open biomedical data, the lack of high-quality metadata makes it challenging for others to find relevant datasets and to reuse them for another purpose. In particular, metadata are useful to understand the nature and provenance of the data. A common approach to improving the quality of metadata relies on expensive human curation, which itself is time-consuming and also prone to error. Towards improving the quality of metadata, we use scientific publications to automatically predict metadata key:value pairs. For prediction, we use a Convolutional Neural Network (CNN) and a Bidirectional Long-short term memory network (BiLSTM). We focus our attention on the NCBI Disease Corpus, which is used for training the CNN and BiLSTM. We perform two different kinds of experiments with these two architectures: (1) we predict the disease names by using their unique ID in the MeSH ontology and (2) we use the tree structures of MeSH ontology to move up in the hierarchy of these disease terms, which reduces the number of labels. We also perform various multi-label classification techniques for the above-mentioned experiments. We find that in both cases CNN achieves the best results in predicting the superclasses for disease with an accuracy of 83\%.},
	number = {4},
	journal = {J. Data and Information Quality},
	author = {Nayak, Stuti and Zaveri, Amrapali and Serrano, Pedro Hernandez and Dumontier, Michel},
	month = aug,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language processing, metadata, Datasets, neural networks, quality},
}

@inproceedings{hasel_mehri_towards_2025,
	address = {New York, NY, USA},
	series = {{SACMAT} '25},
	title = {Towards {Explainable} {Access} {Control} [{BlueSky} {Paper}]},
	isbn = {979-8-4007-1503-7},
	url = {https://doi.org/10.1145/3734436.3734439},
	doi = {10.1145/3734436.3734439},
	abstract = {Access control (AC) systems play an important role in ensuring security by regulating how resources are accessed, protecting sensitive information, and maintaining system integrity. Their complexity arises not only from diverse policies and mechanisms but also from the involvement of multiple stakeholders, including resource owners, administrators, and end-users. Taking inspiration from explainable AI and explainable security, we define the first model of access control explainability, as a quality measure of the explanation graph constructed around the decisions made within the AC system. We then explore the literature to identify how existing work can be integrated as explanatory processes. Finally, we leverage our framework to articulate three open research challenges: the collection and interpretation of AC decisions, the effective construction of AC explanation graphs, and the definition of meaningful and computationally efficient explanation quality metrics.},
	booktitle = {Proceedings of the 30th {ACM} {Symposium} on {Access} {Control} {Models} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Hasel Mehri, Gelareh and Morisset, Charles and Zannone, Nicola},
	year = {2025},
	note = {event-place: USA},
	keywords = {explainability framework, explainable access control, policy comprehension},
	pages = {117--126},
}

@inproceedings{chatzipanagiotou_automated_2022,
	address = {New York, NY, USA},
	series = {{DHW} 2021},
	title = {Automated recognition of geographical named entities in titles of {Ukiyo}-e prints},
	isbn = {978-1-4503-8736-1},
	url = {https://doi.org/10.1145/3526242.3526254},
	doi = {10.1145/3526242.3526254},
	abstract = {This paper investigates the application of Natural Language Processing as a means to study the relationship between topography and its visual renderings in early modern Japanese ukiyo-e landscape prints. We introduce a new dataset with titles of landscape prints that have been annotated by an art historian for any included place-names. The prints are hosted by the digital database of the Art Research Center at the Ritsumeikan University, Kyoto, one of the hubs of Digital Humanities in Japan. By applying, calibrating and assessing a Named Entity Recognition (NER) tool, we argue that ‘distant viewing’ or macroanalysis of visual datasets can be facilitated, which is needed to assist art historical studies of this rich, complex and diverse research material. Experimental results indicated that the performance of NER can be improved by 30\% and reach 50\% precision, by using part of the introduced dataset.},
	booktitle = {Digital {Humanities} {Workshop}},
	publisher = {Association for Computing Machinery},
	author = {Chatzipanagiotou, Marita and Machotka, Ewa and Pavlopoulos, John},
	year = {2022},
	note = {event-place: Kyiv, Ukraine},
	keywords = {natural language processing, named entity recognition, art history, Ukiyo-e prints},
	pages = {70--77},
}

@inproceedings{zhu_research_2020,
	address = {New York, NY, USA},
	series = {{AIAM2020}},
	title = {Research on the {Labelling} {Technology} of {Morphology} and {Syntax}},
	isbn = {978-1-4503-7553-5},
	url = {https://doi.org/10.1145/3421766.3421812},
	doi = {10.1145/3421766.3421812},
	abstract = {This paper aimed the integration tagging and tree-bank transformation of morphology and syntax on the basis of phrase and syntax tree-bank, tagged the nested named entity in combination with the ontological linguistic clues. Finally, it integrates the named entity to carry out the integrative experimental analysis; according to the experimental results, both the accuracy rate and recall rate have been improved somewhat.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Artificial} {Intelligence} and {Advanced} {Manufacture}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Dengyun and Guo, Qi and Zhang, Dongjiao and Wan, Fucheng},
	year = {2020},
	note = {event-place: Manchester, United Kingdom},
	keywords = {Integration of morphology and syntax, named entity identification, tagging},
	pages = {181--184},
}

@inproceedings{peng_research_2024,
	address = {New York, NY, USA},
	series = {{CNSCT} '24},
	title = {Research on {Knowledge} {Graph} {Construction} for {Smart} {Grid} {Cybersecurity}},
	isbn = {979-8-4007-1695-9},
	url = {https://doi.org/10.1145/3673277.3673306},
	doi = {10.1145/3673277.3673306},
	abstract = {This paper proposes a construction method for smart grid cybersecurity knowledge graph and solves the difficulty of multilingual entity extraction with a small amount of labeled data. First, the construction method of smart grid cybersecurity knowledge graph is proposed with the multi-source heterogeneous data in the field of electric power cybersecurity collected by subject crawlers. Then, for the problems of insufficient labeled data and language mixing in the electric power cybersecurity domain, a DA-XLMR-BiLSTM-FC-CRF model based on a five-layer architecture is proposed to realize the entity extraction of multilingual unstructured text. Finally, comparative and ablation experiments are designed to prove the effectiveness of the proposed model, and the F1 value of the model reaches 94.04\% and the accuracy rate reaches 94.48\%.},
	booktitle = {Proceedings of the 2024 3rd {International} {Conference} on {Cryptography}, {Network} {Security} and {Communication} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Peng, Zhen and Du, Ye and Chen, Qifang and Zheng, Tianshuai},
	year = {2024},
	note = {event-place: Harbin, China},
	pages = {164--170},
}

@inproceedings{li_translating_2024,
	address = {New York, NY, USA},
	series = {{FDG} '24},
	title = {Translating {Between} {Game} {Generators} with {Asterism} and {Ceptre}},
	isbn = {979-8-4007-0955-5},
	url = {https://doi.org/10.1145/3649921.3659847},
	doi = {10.1145/3649921.3659847},
	abstract = {In this paper, we present in-progress work that converts games made with Ceptre, a genre-agnostic game description language, into graphical games using the framework of operational logics. Our preliminary code targets the translation of tilemap-based dungeon crawlers, but we present strategies for generalizing this process to other Ceptre games and Asterism engines. We gesture at the potential of operational logics and Asterism as a tool to communicate across the many frameworks surrounding game development and playing.},
	booktitle = {Proceedings of the 19th {International} {Conference} on the {Foundations} of {Digital} {Games}},
	publisher = {Association for Computing Machinery},
	author = {Li, Cynthia and Osborn, Joseph},
	year = {2024},
	note = {event-place: Worcester, MA, USA},
	keywords = {formal models, game generators, operational logics},
}

@inproceedings{ming_personalized_2025,
	address = {New York, NY, USA},
	series = {{ICAIE} '24},
	title = {The personalized university {English} learning system in computer-driven research},
	isbn = {979-8-4007-1269-2},
	url = {https://doi.org/10.1145/3722237.3722276},
	doi = {10.1145/3722237.3722276},
	abstract = {The aim of this study is to develop a computer-assisted personalised university English learning system to improve learning efficiency and effectiveness. Machine learning algorithms were used to analyse student data and build a personalised learning model. Experimental results show that the system significantly outperforms traditional methods in vocabulary acquisition (22\% improvement), listening comprehension (18\% improvement) and oral fluency (15\% improvement). The conclusion suggests that a personalised learning system based on artificial intelligence and big data can effectively improve university English teaching, and provide a new direction for the future development of educational technology. During the Eastern Han period, the production of everyday pottery, pottery sculptures, architectural bricks and tiles, a variety of ceramic sculptures, living statues, horses, acrobatics and other funerary objects, as well as new products such as large lofts, chariots and horses, became the main features of the industry.},
	booktitle = {Proceedings of the 2024 3rd {International} {Conference} on {Artificial} {Intelligence} and {Education}},
	publisher = {Association for Computing Machinery},
	author = {Ming, Jing},
	year = {2025},
	keywords = {Artificial Intelligence, Data Mining, Adaptive Learning, Educational Technology, Language Acquisition},
	pages = {222--225},
}

@inproceedings{tsiounis_goal_2023,
	address = {New York, NY, USA},
	series = {{ICSCA} '23},
	title = {Goal {Driven} {Code} {Generation} for {Smart} {Contract} {Assemblies}},
	isbn = {978-1-4503-9858-9},
	url = {https://doi.org/10.1145/3587828.3587846},
	doi = {10.1145/3587828.3587846},
	abstract = {We are currently witnessing the proliferation of blockchain environments to support a wide spectrum of corporate applications through the use of smart contracts. It is of no surprise that smart contract programming language technology constantly evolves to include not only specialized languages such as Solidity, but also general purpose languages such as GoLang and JavaScript. Furthermore, blockchain technology imposes unique challenges related to the monetary cost of deploying smart contracts, and handling roll-back issues when a smart contract fails. It is therefore evident that the complexity of systems involving smart contracts will only increase over time thus making the maintenance and evolution of such systems a very challenging task. One solution to these problems is to approach the implementation and deployment of such systems in a disciplined and automated way. In this paper, we propose a model-driven approach where the structure and inter-dependencies of smart contract, as well as stakeholder objectives, are denoted by extended goal models which can then be transformed to yield Solidity code that conforms with those models. More specifically, we present first a Domain Specific Language (DSL) to denote extended goal models and second, a transformation process which allows for the Abstract Syntax Trees of such a DSL program to be transformed into Solidity smart contact source code. The transformation process ensures that the generated smart contract skeleton code yields a system that is conformant with the model, which serves as a specification of said system so that subsequent analysis, understanding, and maintenance will be easier to achieve.},
	booktitle = {Proceedings of the 2023 12th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Tsiounis, Konstantinos and Kontogiannis, Kostas},
	year = {2023},
	note = {event-place: Kuantan, Malaysia},
	keywords = {Compliance, Code generation, Model-driven engineering, Smart contracts, Goal models, Modular Design},
	pages = {112--121},
}

@inproceedings{wilson_software_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {Software {Architecture} to {Generate} {Assistive} {Behaviors} for {Social} {Robots}},
	isbn = {979-8-4007-0323-2},
	url = {https://doi.org/10.1145/3610978.3640715},
	doi = {10.1145/3610978.3640715},
	abstract = {To facilitate the design of socially assistive robots (SARs), we present an architecture to generate assistive behavior for social robots given a high-level description of the intent of the assistance. Our approach features an ontology of assistive intents, a hierarchical task network planner, and robot middleware. We demonstrate the behaviors on two robot platforms and compare the behaviors. While many of the behaviors are similar, challenges remain in generating behaviors that will be presented consistently across multiple platforms.},
	booktitle = {Companion of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Wilson, Jason and Yang, Yuqi},
	year = {2024},
	note = {event-place: Boulder, CO, USA},
	keywords = {ontology, software architecture, behavior generation, HTN planning, socially assistive robot},
	pages = {1119--1123},
}

@inproceedings{smith_dialogue-based_2022,
	address = {New York, NY, USA},
	series = {{IUI} '22},
	title = {A {Dialogue}-{Based} {Interface} for {Active} {Learning} of {Activities} of {Daily} {Living}},
	isbn = {978-1-4503-9144-3},
	url = {https://doi.org/10.1145/3490099.3511130},
	doi = {10.1145/3490099.3511130},
	abstract = {While Human Activity Recognition (HAR) systems may benefit from Active Learning (AL) by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in HAR systems, which utilises a dataset of natural language descriptions of common activities (which we make publicly available) and semantic similarity measures. Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply our work to an existing CASAS dataset in an active learning scenario, to demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) technically, as an effective way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to use our approach and an established method, and to subsequently compare the two. Results show the potential of our approach as a user-friendly mechanism for annotation of sensor data as part of an active learning system.},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Smith, Ronnie and Dragone, Mauro},
	year = {2022},
	note = {event-place: Helsinki, Finland},
	keywords = {semantic similarity, natural language, Active Learning (AL), Human Activity Recognition (HAR) labelling, Human-in-the-Loop (HITL) annotation},
	pages = {820--831},
}

@article{liu_variational_2023,
	title = {Variational {Latent}-{State} {GPT} for {Semi}-{Supervised} {Task}-{Oriented} {Dialog} {Systems}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2023.3240661},
	doi = {10.1109/TASLP.2023.3240661},
	abstract = {Recently, two approaches, fine-tuning large pre-trained language models and variational training, have attracted significant interests, separately, for semi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper, we propose Variational Latent-State GPT model (VLS-GPT), which is the first to combine the strengths of the two approaches. Among many options of models, we propose the generative model and the inference model for variational learning of the end-to-end TOD system, both as auto-regressive language models based on GPT-2, which can be further trained over a mix of labeled and unlabeled dialog data in a semi-supervised manner. Variational training of VLS-GPT is both statistically and computationally more challenging than previous variational learning works for sequential latent variable models, which use turn-level first-order Markovian. The inference model in VLS-GPT is non-Markovian due to the use of the Transformer architecture. In this work, we establish Recursive Monte Carlo Approximation (RMCA) to the variational objective with non-Markovian inference model and prove its unbiasedness. Further, we develop the computational strategy of sampling-then-forward-computation to realize RMCA, which successfully overcomes the memory explosion issue of using GPT in variational learning and speeds up training. Semi-supervised TOD experiments are conducted on two benchmark multi-domain datasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to significantly outperform both supervised-only and semi-supervised self-training baselines.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Liu, Hong and Cai, Yucheng and Lin, Zhenru and Ou, Zhijian and Huang, Yi and Feng, Junlan},
	month = jan,
	year = {2023},
	note = {Publisher: IEEE Press},
	pages = {970--984},
}

@inproceedings{ramanathan_reasoning_2023,
	address = {New York, NY, USA},
	series = {{BuildSys} '23},
	title = {Reasoning about {Physical} {Processes} in {Buildings} through {Component} {Stereotypes}},
	isbn = {979-8-4007-0230-3},
	url = {https://doi.org/10.1145/3600100.3623737},
	doi = {10.1145/3600100.3623737},
	abstract = {Buildings employ an ensemble of technical systems like those for heating and ventilation and each of them orchestrate complex physical processes. Ontologies such as Brick, IFC, SSN/SOSA, and SAREF have been created to describe the technical systems in a machine-understandable manner. However, such ontologies focus largely on describing system topology, whereas several use cases, such as automated fault detection and diagnostics (AFDD), also need knowledge about the physical processes. Physical processes can be described using mathematical simulation models, but this is practically too expensive for building automation systems and their integration with mainstream technical systems ontologies is still under-explored. We propose to address these challenges by introducing the concept of component stereotypes that describe the effect of component actuation on the state its underlying physical mechanism. These stereotypes are then linked to actual component instances in the technical system description, thereby accomplishing an integration of structural description with knowledge about physical processes. We contribute an ontology for such stereotypes and evaluate it with respect to the coverage of HVAC components in Brick and its ability to automatically infer relationships between components in a real-world building. We show how the resulting knowledge graph can be queried by AFDD applications to know about expected consequences of an action, or conversely, identify components that may be responsible for an observed state of the process. While we are able to report a coverage of 100\% of Brick HVAC components, the automatic inference underreports component dependencies in real-world installations. This points at a group of concepts which we propose should be considered in future versions of the Brick ontology.},
	booktitle = {Proceedings of the 10th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {Ramanathan, Ganesh and Mayer, Simon},
	year = {2023},
	note = {event-place: Istanbul, Turkey},
	pages = {120--129},
}

@inproceedings{noh_joint_2021,
	address = {New York, NY, USA},
	series = {{BCB} '21},
	title = {Joint learning for biomedical {NER} and entity normalization: encoding schemes, counterfactual examples, and zero-shot evaluation},
	isbn = {978-1-4503-8450-6},
	url = {https://doi.org/10.1145/3459930.3469533},
	doi = {10.1145/3459930.3469533},
	abstract = {Named entity recognition (NER) and normalization (EN) form an indispensable first step to many biomedical natural language processing applications. In biomedical information science, recognizing entities (e.g., genes, diseases, or drugs) and normalizing them to concepts in standard terminologies or thesauri (e.g., Entrez, ICD-10, or RxNorm) is crucial for identifying more informative relations among them that drive disease etiology, progression, and treatment. In this effort we pursue two high level strategies to improve biomedical ER and EN. The first is to decouple standard entity encoding tags (e.g., "B-Drug" for the beginning of a drug) into type tags (e.g., "Drug") and positional tags (e.g., "B"). A second strategy is to use additional counterfactual training examples to handle the issue of models learning spurious correlations between surrounding context and normalized concepts in training data. We conduct elaborate experiments using the MedMentions dataset, the largest dataset of its kind for ER and EN in biomedicine. We find that our first strategy performs better in entity normalization when compared with the standard coding scheme. The second data augmentation strategy uniformly improves performance in span detection, typing, and normalization. The gains from counterfactual examples are more prominent when evaluating in zero-shot settings, for concepts that have never been encountered during training.},
	booktitle = {Proceedings of the 12th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Noh, Jiho and Kavuluru, Ramakanth},
	year = {2021},
	note = {event-place: Gainesville, Florida},
	keywords = {named entity recognition, entity normalization, biomedical natural language processing, information extraction, deep neural networks},
}

@article{damour_underspecification_2022,
	title = {Underspecification presents challenges for credibility in modern machine learning},
	volume = {23},
	issn = {1532-4435},
	abstract = {Machine learning (ML) systems often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification in ML pipelines as a key reason for these failures. An ML pipeline is the full procedure followed to train and validate a predictor. Such a pipeline is underspecified when it can return many distinct predictors with equivalently strong test performance. Underspecification is common in modern ML pipelines that primarily validate predictors on held-out data that follow the same distribution as the training data. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We provide evidence that underspecfication has substantive implications for practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	month = jan,
	year = {2022},
	note = {Publisher: JMLR.org},
	keywords = {natural language processing, electronic health records, genomics, fairness, computer vision, distribution shift, identifiability, medical imaging, spurious correlation},
}

@inproceedings{duck_finding_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Finding {Needles} in {Document} {Haystacks}: {Augmenting} {Serendipitous} {Claim} {Retrieval} {Workflows}},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713715},
	doi = {10.1145/3706598.3713715},
	abstract = {Preliminary exploration of vast text corpora for generating and validating hypotheses, typical in academic inquiry, requires flexible navigation and rapid validation of claims. Navigating the corpus by titles, summaries, and abstracts might neglect information, whereas identifying the relevant context-specific claims through in-depth reading is unfeasible with rapidly increasing publication numbers. Our paper identifies three typical user pathways for hypothesis exploration and operationalizes sentence-based retrieval combined with effective contextualization and provenance tracking in a unified workflow. We contribute an interface that augments the previously laborious tasks of claim identification and consistency checking using NLP techniques while balancing user control and serendipity. Use cases, expert interviews, and a user study with 10 participants demonstrate how the proposed workflow enables users to traverse literature corpora in novel and efficient ways. For the evaluation, we instantiate the tool within two independent domains, providing novel insights into the analysis of political discourse and medical research.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dück, Moritz and Holter, Steffen and Chan, Robin Shing Moon and Sevastjanova, Rita and El-Assady, Mennatallah},
	year = {2025},
	keywords = {natural language processing, provenance, text data, human-AI interaction, serendipity},
}

@inproceedings{guo_construction_2023,
	address = {New York, NY, USA},
	series = {{ISIA} '23},
	title = {Construction and {Application} of the {Knowledge} {Graph} {Method} in {Maintenance} of {Robot} in {Automotive} {Manufacturing} {Industry}},
	isbn = {979-8-4007-0940-1},
	url = {https://doi.org/10.1145/3632314.3632332},
	doi = {10.1145/3632314.3632332},
	abstract = {Based on the spare parts and structure data of industrial robots, the entity list of robot parts is established to form a query dictionary, and entity annotation is performed on the robot corpus by means of dictionary query, which reduces the cost of manual annotation and ensures the quality of annotation data. In the process of entity recognition training, Bert+Bilstm+CRF model structure is used to initially use 70\% of the dictionary data for annotation, and the model is trained by iteratively increasing the annotation data in a continuous cycle, so that the model can extract all the entities in the robot corpus as much as possible. In addition, the material number/model information and PM maintenance content/strategy of the entity have been used as attributes of the entity. Meanwhile, the experience summarized by the failure model and effect analysis of industrial robots is fully utilized to connect the phenomena, causes and measures through the entities in order to build the industrial robot knowledge graph relationships. The constructed knowledge graph relationship is stored in a Neo4j graphical database, making it convenient for content retrieval and inquiry of application systems.In the industrial robot knowledge graph application side, the field maintenance personnel requirements are collected through a questionnaire survey and the requirements are classified into intent. A Bert+TextCNN structure model is built to realize the intention recognition of user inquiries. By combining entity recognition models and intent classification models, the system is able to better understand user inquiry needs, leading to the implementation of an intelligent maintenance system for industrial robots.},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Intelligent} {Sensing} and {Industrial} {Automation}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Dongdong and Ma, Haitao and Zhao, Can and Peng, Hao and Du, Wenbo and Jiang, Zongrui and Zhang, Yan},
	year = {2023},
	note = {event-place: Virtual Event, China},
	keywords = {Knowledge Graph, Robot Maintenance},
}

@inproceedings{ruman_adversary_2024,
	address = {New York, NY, USA},
	series = {{ARES} '24},
	title = {Adversary {Tactic} {Driven} {Scenario} and {Terrain} {Generation} with {Partial} {Infrastructure} {Specification}},
	isbn = {979-8-4007-1718-5},
	url = {https://doi.org/10.1145/3664476.3664523},
	doi = {10.1145/3664476.3664523},
	abstract = {Diverse, accurate, and up-to-date training environments are essential for training cybersecurity experts and autonomous systems. However, preparation of their content is time-consuming and requires experts to provide detailed specifications. In this paper, we explore the challenges of automated generation of the content (composed of scenarios and terrains) for these environments. We propose new models to represent the cybersecurity domain and associated action spaces. These models are used to create sound and complex training content based on partial specifications provided by users. We compare the results with a real-world complex malware campaign to assess the realism of the synthesized content. To further evaluate the correctness and variability of the results, we utilize the kill-chain attack graph generation for the generated training content to asses the internal correspondence of its key components. Our results demonstrate that the proposed approach can create complex training content similar to advanced attack campaigns, which passes evaluation for soundness and practicality. Our proposed approach and its implementation significantly contribute to the state of the art, enabling novel approaches to cybersecurity training and autonomous system development.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Ruman, Ádám and Drašar, Martin and Sadlek, Lukáš and Yang, Shanchieh Jay and Celeda, Pavel},
	year = {2024},
	note = {event-place: Vienna, Austria},
	keywords = {adversary framework, attack scenario generation, cyber terrain generation, cybersecurity model},
}

@article{finkel_analyzing_2022,
	title = {Analyzing {Code}-mixing in {Linguistic} {Corpora} {Using} {Kratylos}},
	volume = {15},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3480238},
	doi = {10.1145/3480238},
	abstract = {Code-switching, code-mixing, and, more generally, multilingualism pose technological challenges for language documentation, the sub-discipline of linguistics that deals with the annotation and basic analysis of field recordings and other primary data. We focus here on a case study involving code-mixing in the endangered Koda language, which poses special problems for morphosyntactic analysis. We offer a robust approach to multilingual annotations that involves a combination of the popular open source software FieldWorks Language Explorer (FLEx) with Kratylos, a web-based corpus tool for display and query. Kratylos exposes linguistic data from various formats to powerful regular-expression queries that can exploit tier structure and other aspects of interlinear glossed text. We show how Kratylos can target mixed structures in our FLEx database of Koda that cannot be easily identified within the original FLEx software itself.},
	number = {1},
	journal = {J. Comput. Cult. Herit.},
	author = {Finkel, Raphael and Kaufman, Daniel and Shamim, Ahmed},
	month = jan,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {linguistics, interlinear glossed texts, Language archives, lexicons},
}

@inproceedings{bou_nassar_unsolvable_2025,
	address = {New York, NY, USA},
	series = {{COMPASS} '25},
	title = {‘{Unsolvable} within existing regimes’: {Using} a {Systems} {Thinking} {Approach} to {Co}-design for {Data} {Governance} in {Cities}},
	isbn = {979-8-4007-1484-9},
	url = {https://doi.org/10.1145/3715335.3735456},
	doi = {10.1145/3715335.3735456},
	abstract = {Despite people’s significant role in generating data in cities, their involvement in data governance (DG) remains limited, failing to address the inherent complexity of DG and undermining their ’right to the city’. We propose a collaborative systems thinking approach as a scoping tool for co-design, enabling researchers and designers to involve people in co-creating an understanding of the systemic structures underpinning DG in cities and developing prototypes and solutions informed by these structures. Using causal loop diagrams, we facilitated the development of a conceptual model of DG. Participants, representing diverse perspectives, created individual causal loop diagrams that were merged into a collaborative causal loop diagram (C-CLD). This C-CLD was employed in an interactive workshop to identify intervention points and develop targeted solutions. Our findings demonstrate how C-CLDs can accommodate multiplicity, foster agonism, and enable participants to challenge political dimensions and existing systemic structures. Moreover, the engagement process revealed the complexity of DG in the city, as perceived by the collective of participants, resulting in three key submodules that highlight tensions between citizen sensitisation to data collection, the private sector’s role in fulfilling citizens’ needs, and the struggles faced by local governments. This work draws on and extends HCI research that engages with systems thinking ontologies, contributing to an HCI that includes the political, moves beyond solutionism, and advances social justice-oriented approaches.},
	booktitle = {Proceedings of the 2025 {ACM} {SIGCAS}/{SIGCHI} {Conference} on {Computing} and {Sustainable} {Societies}},
	publisher = {Association for Computing Machinery},
	author = {Bou Nassar, Jessica and Anwar, Misita and Bartram, Lyn and Sharp, Darren and Goodwin, Sarah},
	year = {2025},
	keywords = {Systems thinking, Co-design, Cities, Causal loop diagram, Data governance},
	pages = {48--67},
}

@inproceedings{zargham_designing_2024,
	address = {New York, NY, USA},
	series = {{MUM} '24},
	title = {Designing {AI} {Personalities}: {Enhancing} {Human}-{Agent} {Interaction} {Through} {Thoughtful} {Persona} {Design}},
	isbn = {979-8-4007-1283-8},
	url = {https://doi.org/10.1145/3701571.3701608},
	doi = {10.1145/3701571.3701608},
	abstract = {In the rapidly evolving field of artificial intelligence (AI) agents, designing the agent’s characteristics is crucial for shaping user experience. This workshop aims to establish a research community focused on AI agent persona design for various contexts, such as in-car assistants, educational tools, and smart home environments. We will explore critical aspects of persona design, such as voice, embodiment, and demographics, and their impact on user satisfaction and engagement. Through discussions and hands-on activities, we aim to propose practices and standards that enhance the ecological validity of agent personas. Topics include the design of conversational interfaces, the influence of agent personas on user experience, and approaches for creating contextually appropriate AI agents. This workshop will provide a platform for building a community dedicated to developing AI agent personas that better fit diverse, everyday interactions.},
	booktitle = {Proceedings of the {International} {Conference} on {Mobile} and {Ubiquitous} {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Zargham, Nima and Dubiel, Mateusz and Desai, Smit and Mildner, Thomas and Belz, Hanz-Joachim},
	year = {2024},
	keywords = {Conversational Agents, Personas, conversational user interfaces, AI Agents, Speech Interfaces},
	pages = {490--494},
}

@article{lisboa_malaquias_towards_2023,
	title = {Towards a {Methodology} to {Design} {Provably} {Secure} {Cyber}-physical {Systems}},
	volume = {43},
	issn = {1094-3641},
	url = {https://doi.org/10.1145/3631483.3631499},
	doi = {10.1145/3631483.3631499},
	abstract = {The inordinate financial cost of mitigating post-production cybersecurity vulnerabilities in cyber-physical systems (CPS) is forcing the industry to rethink systems design cycles: greater attention is being given to the design phase - with the goal of reducing the attack surface of systems at an early stage (i.e., before silicon tape out). Fortunately, formal methods have advanced to the point that they can address such needs and contribute towards achieving security certification. However, new methods and tools focusing on industrial scalability and usability for systems engineers are required. In this ongoing research paper, we describe a framework that will help systems engineers to: a) design cyber-assured CPS using a Model Based Engineering (MBE) approach; b) formally map security requirements to different hardware and software blocks in the model; and c) formally verify security requirements. Based on the nature of each requirement, our framework collects formal correctness evidence from different tools: while high-level architectural properties are suitable for a contract- or ontology-based reasoning, more complex properties with rich semantics require the use of model checking or theorem proving techniques.},
	number = {1},
	journal = {Ada Lett.},
	author = {Lisboa Malaquias, Felipe and Giantamidis, Georgios and Basagiannis, Stylianos and Fulvio Rollini, Simone and Amundson, Isaac},
	month = oct,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {94--99},
}

@inproceedings{jain_knowledge_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Knowledge {Enabled} {Relation} {Extraction}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3651263},
	doi = {10.1145/3589335.3651263},
	abstract = {Relation extraction is the task of extracting relationships from input text, where input can be a sentence, document, or multiple documents. This task has been popular for decades and is still of keen interest. Various techniques have been proposed to solve the relation extraction problem, among which the most popular are using distant supervision, deep learning-based models, reasoning-based models, and transformer-based models. We propose three approaches (named ReOnto, DocRE-CLip, and KDocRE) for relation extraction from text at three levels of granularity (sentence, document and across documents). These approaches embed knowledge in a deep learning based model to improve performance. ReOnto and DocRE-CLip have been evaluated and the source code is publicly available. We are currently implementing and evaluating KDocRE.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Jain, Monika},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {ontology, knowledge graph, relation extraction, graph neural network, neurosymbolic ai},
	pages = {1210--1213},
}

@inproceedings{mcquaigue_data-driven_2023,
	address = {New York, NY, USA},
	series = {{SC}-{W} '23},
	title = {Data-{Driven} {Discovery} of {Anchor} {Points} for {PDC} {Content}},
	isbn = {979-8-4007-0785-8},
	url = {https://doi.org/10.1145/3624062.3624099},
	doi = {10.1145/3624062.3624099},
	abstract = {The Parallel and Distributed Computing community has been interested in integrating PDC content into early CS curriculum to prime the students for more advanced materials and build a workforce able to leverage advanced computing infrastructure. To deploy this strategy at scale, it is important to identify anchor points in early CS courses where we can insert PDC content. We present an analysis of CS courses that primarily focuses on CS1 and Data Structure courses. We collected data on course content through in-person workshops, where instructors of courses classified their course materials against standard curriculum guidelines. By using these classification, we make sense of how Computer Science is being taught. We highlight different types of CS1 and Data Structure courses. And we provide reflection on how that knowledge can be used by PDC experts to identify anchoring points for PDC content, while being sensitive to the needs of instructors.},
	booktitle = {Proceedings of the {SC} '23 {Workshops} of the {International} {Conference} on {High} {Performance} {Computing}, {Network}, {Storage}, and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Mcquaigue, Matthew and Saule, Erik and Subramanian, Kalpathi and Payton, Jamie},
	year = {2023},
	note = {event-place: Denver, CO, USA},
	keywords = {Course Model, CS Education, Curriculum Guidelines, Integrating PDC in Early CS},
	pages = {335--342},
}

@inproceedings{bystrov_information_2024,
	address = {New York, NY, USA},
	series = {{NISS} '24},
	title = {Information {Retrieval} {Multi}-{Agent} {System} {Established} on the {Metaphysics} {Lexical} {Database}},
	isbn = {979-8-4007-0929-6},
	url = {https://doi.org/10.1145/3659677.3659827},
	doi = {10.1145/3659677.3659827},
	abstract = {The system, retrieving information from heterogeneous sources is discussed. Architecture of the system is based on multi-agent approach. The user's queries could be presented in the native language. Retrieving process based on using knowledge, representing via ontology scheme. For this purpose the wordnet ontology is used. The development process of the ontology is discussed. The presented system works on a distributed environment, where component agents collaborate via XML web services and SOAP protocols.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Networking}, {Intelligent} {Systems} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Bystrov, Dmitriy},
	year = {2024},
	note = {event-place: Meknes, AA, Morocco},
}

@inproceedings{huang_study_2023,
	address = {New York, NY, USA},
	series = {{CNIOT} '23},
	title = {A {Study} of {Sentence}-{BERT} {Based} {Essay} {Off}-topic {Detection}},
	isbn = {979-8-4007-0070-5},
	url = {https://doi.org/10.1145/3603781.3603871},
	doi = {10.1145/3603781.3603871},
	abstract = {Automated essay scoring systems are widely used in education, and essay off-topic detection is an integral part of this. Traditionally off-topic essay detection is based on text features represented as spatial vectors, however, this approach only addresses the structure of essay statements and requires the use of manual features. This paper proposed to use the Sentence-BERT model to detect off-topic essays, the method first obtains a large amount of high-quality data to build a corpus of off-topic essays, and two Siamese twin pre-trained models are used to embed sentences in the essay topic, and the body of the essay, generate semantically rich sentence vectors and then use cosine similarity to calculate the similarity between the topic and the body of the essay after averaging the pooled sentence vectors, and select the optimal threshold to determine off-topic essays through continuous training. The experimental results show that the proposed method improves the accuracy, recall, and F1 values by 9.5\%, 11.2\%, and 10.4\% respectively over the C-BGRU (Convolutional-Bidirectional Gate Recurrent Unit) based Siamese twin network and also has an excellent performance in topics with different degrees of divergence.},
	booktitle = {Proceedings of the 2023 4th {International} {Conference} on {Computing}, {Networks} and {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Pengcheng and Li, Li and Wu, Chunyan and Zhang, Xiaoqian and Liu, Zhigui},
	year = {2023},
	note = {event-place: Xiamen, China},
	keywords = {Cosine similarity, Off-topic essay detection, Pre-training models, Siamese network},
	pages = {515--519},
}

@article{benedict_report_2024,
	title = {Report on the 1st {Workshop} on {Generative} {Information} {Retrieval} ({Gen}-{IR} 2023) at {SIGIR} 2023},
	volume = {57},
	issn = {0163-5840},
	url = {https://doi.org/10.1145/3642979.3642995},
	doi = {10.1145/3642979.3642995},
	abstract = {The first edition of the workshop on Generative Information Retrieval (Gen-IR 2023) took place in July 2023 in a hybrid fashion, co-located with the ACM SIGIR Conference 2023 in Taipei (SIGIR 2023). The aim was to bring information retrieval researchers together around the topic of generative AI that gathered attention in 2022 and 2023 with large language models and diffusion models. Given the novelty of the topic, the workshop was focused around multi-sided discussions, namely panels and poster sessions of the accepted proceedings papers. Two main research outcomes are the proceedings of the workshop1 and the potential research directions discussed in this report.Date: 27 July 2023.Website: https://coda.io/@sigir/gen-ir.},
	number = {2},
	journal = {SIGIR Forum},
	author = {Bénédict, Gabriel and Zhang, Ruqing and Metzler, Donald and Yates, Andrew and Deffayet, Romain and Hager, Philipp and Jullien, Sami},
	month = jan,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
}

@inproceedings{mcdermott_determining_2025,
	address = {New York, NY, USA},
	series = {{ITiCSE} 2025},
	title = {Determining the {Scope} of the {Philosophy} of {Computing} {Education}},
	isbn = {979-8-4007-1567-9},
	url = {https://doi.org/10.1145/3724363.3729049},
	doi = {10.1145/3724363.3729049},
	abstract = {There are a number of different approaches to the investigation of teaching and learning within the subject of Computing Education. Many of the advances in pedagogy that have taken place over the past thirty years have been due to careful statistical analysis of empirical data, enhancing the reputation of the subject within the broader Computing discipline. Empirical, qualitative methodologies, of the kinds used extensively in the Social Sciences, have also appeared in the Computing Education literature, often investigating the socio-cultural aspects of the subject. More recently, there has been a proposal to develop a role for philosophical inquiry in Computing Education, which mirrors similar historical developments in Engineering Education. Rather than focus on the quantitative or qualitative analysis of the student experience, philosophical investigation instead relies on the use of conceptual analysis to investigate the detailed semantic content of ideas raised in the practice of computing education, careful analysis of the methodologies used to do such work, and a critique of the assumptions that underlie the subject.In this paper, we investigate ways in which an understanding of the Philosophy of Computing Education can assist research within the subject. We consider how it emerges from basic questions about nature of the subject, its scope, and how it can be applied fruitfully within the discipline.},
	booktitle = {Proceedings of the 30th {ACM} {Conference} on {Innovation} and {Technology} in {Computer} {Science} {Education} {V}. 1},
	publisher = {Association for Computing Machinery},
	author = {McDermott, Roger and Daniels, Mats and Brown, John N.A. and Cajander, Åsa},
	year = {2025},
	note = {event-place: Nijmegen, Netherlands},
	keywords = {ontology, methodology, epistemology, axiology, conceptual analysis, philosophy of computing education},
	pages = {403--409},
}

@inproceedings{nizamis_introducing_2024,
	address = {New York, NY, USA},
	series = {{eSAAM} '24},
	title = {Introducing an {Enhanced} {Metadata} {Broker} for {Manufacturing} {Data} {Spaces}},
	isbn = {979-8-4007-0984-5},
	url = {https://doi.org/10.1145/3685651.3686699},
	doi = {10.1145/3685651.3686699},
	abstract = {Nowadays, collaborative ecosystems and value networks have been established based on data sharing mechanisms and principles coming from concepts like Data Spaces. This data-centric approach has also increased the need for effective metadata management that enables entities participating in data sharing scenarios to find and trust available data. In this paper, a metadata broker for manufacturing related Data Spaces is introduced. It is based on an ontology that has been implemented to describe data related to Industries 4.0 and 5.0 implementations. The proposed broker is based on Data Spaces principles and artefacts that it extends by enabling semantic-based modeling and search capabilities.},
	booktitle = {Proceedings of the 4th {Eclipse} {Security}, {AI}, {Architecture} and {Modelling} {Conference} on {Data} {Space}},
	publisher = {Association for Computing Machinery},
	author = {Nizamis, Alexandros and Ioannidis, Dimosthenis and Gkonis, Panagiotis and Trakadas, Panagiotis},
	year = {2024},
	note = {event-place: Mainz, Germany},
	keywords = {Data Spaces, Industry 4.0 / 5.0, Metadata Broker, Metadata Registry},
	pages = {37--40},
}

@article{yu_sentiment_2024,
	title = {Sentiment {Analysis} {Method} of {Epidemic}-related {Microblog} {Based} on {Hesitation} {Theory}},
	volume = {23},
	issn = {2375-4699},
	url = {https://doi.org/10.1145/3648360},
	doi = {10.1145/3648360},
	abstract = {The COVID-19 pandemic in 2020 brought an unprecedented global crisis. After two years of control efforts, life gradually returned to the pre-pandemic state, but localized outbreaks continued to occur. Toward the end of 2022, COVID-19 resurged in China, leading to another disruption of people’s lives and work. Many pieces of information on social media reflected people’s views and emotions toward the second outbreak, which showed distinct differences compared to the first outbreak in 2020. To explore people’s emotional attitudes toward the pandemic at different stages and the underlying reasons, this study collected microblog data from November 2022 to January 2023 and from January to June 2020, encompassing Chinese reactions to the COVID-19 pandemic. Based on hesitancy and the Fuzzy Intuition theory, we proposed a hypothesis: hesitancy can be integrated into machine learning models to select suitable corpora for training, which not only improves accuracy but also enhances model efficiency. Based on this hypothesis, we designed a hesitancy-integrated model. The experimental results demonstrated the model’s positive performance on a self-constructed database. By applying this model to analyze people’s attitudes toward the pandemic, we obtained their sentiments in different months. We found that the most negative emotions appeared at the beginning of the pandemic, followed by emotional fluctuations influenced by social events, ultimately showing an overall positive trend. Combining word cloud techniques and the Latent Dirichlet Allocation (LDA) model effectively helped explore the reasons behind the changes in pandemic attitude.},
	number = {4},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Yu, Yang and Qiu, Dong and Wan, Huanyu},
	month = apr,
	year = {2024},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {machine learning, COVID-19, sentiment analysis, fuzzy intuition theory, hesitancy},
}

@inproceedings{zufarova_branding_2025,
	address = {New York, NY, USA},
	series = {{ICFNDS} '24},
	title = {Branding {Strategies} for {Education} {Using} {NLP} and {Knowledge} {Based} {Systems}},
	isbn = {979-8-4007-1170-1},
	url = {https://doi.org/10.1145/3726122.3726123},
	doi = {10.1145/3726122.3726123},
	abstract = {Knowledge-based systems can provide educational institutions with tailored branding strategies based on their data-driven insights, so it is necessary to integrate advanced computational methods. People are accustomed to using informal and dynamic language on the internet to express their preferences and expectations. In order to obtain actionable insights from large-scale textual data, it is necessary to employ natural language processing techniques to extract meaningful patterns and trends. Therefore, the study introduced TF-IDF analysis and Topic Modeling for textual data analysis. At the same time, the study introduced TF-IDF analysis and Topic Modeling for identifying key branding themes and introduced a knowledge-based system combining rule-based decision-making models and ontological frameworks based on branding domain knowledge, thereby constructing a comprehensive branding framework. The innovation of the research lies in the synergistic combination of traditional frequency-based analysis and coherence-driven topic modeling feature weight calculation methods with knowledge-based systems to formulate adaptive branding strategies, thereby improving the efficiency and precision of branding methodologies. The outcomes indicated that the accuracy of the classification model combining Topic Modeling and TF-IDF based on educational branding datasets was 94\%, the coherence score was as high as 0.724, and the term frequency recall was 89\%. Meanwhile, the classification error rate of the model was only 6\%. In addition, the proposed branding system, which adopted a knowledge-driven framework, had an average of 2,500 daily visits per person, with an effective browsing time of 12 minutes per person, and a daily browsing page count of 15 pages per person. This indicates that the knowledge-based branding framework has strong practical application effects and provides reliable technical support for current research in the field of educational branding.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Future} {Networks} \&amp; {Distributed} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zufarova, Nozima and Kasimova, Zilola and Aripkhodjaev, Saidamir},
	year = {2025},
	pages = {1--7},
}

@inproceedings{ferrini_vdb-based_2025,
	address = {Melbourne, Australia},
	series = {{HRI} '25},
	title = {{VDB}-based {Spatially} {Grounded} {Semantics} for {Interactive} {Robots}},
	abstract = {This paper presents a new approach for representing spatially-grounded semantics in interactive robots. The method combines spatial and symbolic data to improve robot interactions in human-occupied environments. A key feature is a voxel-based data structure optimized for dynamic and sparse information, along with a global lookup table to manage and track spatially-grounded entities and their relationships. The implementation, which is integrated into a ROS 2-based framework, allows for seamless querying through semantic web APIs such as SPARQL. Initial tests demonstrate the efficiency of this system in supporting advanced scenarios in human-robot interaction. All the repositories developed as part of this contribution can be found at github.com/RepresentationMaps.},
	booktitle = {Proceedings of the 2025 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {IEEE Press},
	author = {Ferrini, Lorenzo and Lemaignan, Séverin},
	year = {2025},
	keywords = {knowledge representation, human-robot interaction, semantic mapping, interactive robots},
	pages = {1005--1009},
}

@inproceedings{alghamdi_upwardly_2021,
	address = {New York, NY, USA},
	series = {K-{CAP} '21},
	title = {Upwardly {Abstracted} {Definition}-{Based} {Subontologies}},
	isbn = {978-1-4503-8457-5},
	url = {https://doi.org/10.1145/3460210.3493564},
	doi = {10.1145/3460210.3493564},
	abstract = {In this paper, we present a method for extracting subontologies from mathcalELH ontologies for a set of symbols. The approach is focused on the generation of upwardly abstracted definitions of concepts, which is a technique for computing definitions expressed using closest primitive ancestors. The subontologies returned by the method are evaluated for quality and compared to extracts computed with locality-based modularisation and uniform interpolation. Our subontology generation method produces promising results in terms of size and relevance to the needs of domain experts.},
	booktitle = {Proceedings of the 11th {Knowledge} {Capture} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Alghamdi, Ghadah and Schmidt, Renate A. and Del-Pinto, Warren and Gao, Yongsheng},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {ontology engineering, ontology modularisation, ontology summarisation, snomed ct, subontologies},
	pages = {209--216},
}

@inproceedings{bogacheva_named_2020,
	address = {New York, NY, USA},
	series = {{CSAI} '19},
	title = {Named {Entity} {Recognition} for {Russian} {Historical} {Texts}},
	isbn = {978-1-4503-7627-3},
	url = {https://doi.org/10.1145/3374587.3374637},
	doi = {10.1145/3374587.3374637},
	abstract = {With the raise of big data, machine learning and crowdsourcing, the volume of existing datasets for different machine learning problems have greatly increased. The natural language processing field is not an exception; so, as a result, most of the researches have transitioned into investigating and applying different deep architectures for it. One of the main issues of this trend is as follows: it is hard to adopt such approaches for somewhat poorly studied languages, which do not have training data enough as for natural language processing perspective. In this paper, we investigate some modern approaches to named entity recognition as for Russian language and show that for historical texts their results are much lower than for general ones. In addition, we propose our own algorithm that improves the results of for these historical texts.},
	booktitle = {Proceedings of the 2019 3rd {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Bogacheva, Evgenia and Puchkovskaia, Antonina and Smetannikov, Ivan},
	year = {2020},
	note = {event-place: Normal, IL, USA},
	keywords = {Natural language processing, named entity recognition, historical texts, Russian language},
	pages = {13--17},
}

@inproceedings{yang_dynamic_2025,
	address = {New York, NY, USA},
	series = {{BDCTA} '25},
	title = {Dynamic {Construction} and {Application} of {Electric} {Power} {Thesaurus} {Based} on {Semantic} {Analysis}},
	isbn = {979-8-4007-1362-0},
	url = {https://doi.org/10.1145/3727505.3727544},
	doi = {10.1145/3727505.3727544},
	abstract = {In order to improve the efficiency of knowledge management in the electric power industry, a lexicon of electric power topics based on semantic parsing is established in response to the dynamic changes of terminology and domain characteristics. It also integrates the semantic network with the ontology, constructs a multi-dimensional semantic association model, and utilizes deep learning, natural language processing, and other technologies to realize the automatic identification and classification of the vocabulary. Experiments prove that the method can effectively improve the learning effect of the lexicon. In particular, the coverage increases from 72\% to 92\% and from 12 to 16 months. In addition, the number of correctly recognized words has increased to 200 with a correction accuracy of 95\%. This series of improvements not only reflects the significant improvement of the new method in terms of accuracy and applicability, but also enhances the robustness of the new method in application-specific domain-oriented semantic analysis of power systems.},
	booktitle = {Proceedings of the 2025 {International} {Conference} on {Big} {Data}, {Communication} {Technology} and {Computer} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Yan and Yao, Wenxu and Zhu, Yuangeng and Gao, Xiaoxin and Zheng, Yanrong and Liu, Yanan},
	year = {2025},
	keywords = {semantic analysis, electric power subject thesaurus, semantic relations},
	pages = {226--232},
}

@article{wang_dltinline-formulagtlttex-math_2023,
	title = {D\&lt;inline-formula\&gt;\&lt;tex-math notation="{LaTeX}"\&gt;$^{\textrm{2}}$\&lt;/tex-math\&gt;\&lt;/inline-formula\&gt;{PSG}: {Multi}-{Party} {Dialogue} {Discourse} {Parsing} as {Sequence} {Generation}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2023.3313415},
	doi = {10.1109/TASLP.2023.3313415},
	abstract = {Conversational discourse analysis aims to extract the interactions between dialogue turns, which is crucial for modeling complex multi-party dialogues. As the benchmarks are still limited in size and human annotations are costly, the current standard approaches apply pretrained language models, but they still require randomly initialized classifiers to make predictions. These classifiers usually require massive data to work smoothly with the pretrained encoder, causing severe data hunger issue. We propose two convenient strategies to formulate this task as a sequence generation problem, where classifier decisions are carefully converted into sequence of tokens. We then adopt a pretrained T5 [C. Raffel et al., 2020] model to solve this task so that no parameters are randomly initialized. We also leverage the descriptions of the discourse relations to help model understand their meanings. Experiments on two popular benchmarks show that our approach outperforms previous state-of-the-art models by a large margin, and it is also more robust in zero-shot and few-shot settings.\&lt;sup\&gt;1\&lt;/sup\&gt;},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Wang, Ante and Song, Linfeng and Jin, Lifeng and Yao, Junfeng and Mi, Haitao and Lin, Chen and Su, Jinsong and Yu, Dong},
	month = sep,
	year = {2023},
	note = {Publisher: IEEE Press},
	pages = {4004--4013},
}

@inproceedings{basik_dbpal_2018,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {{DBPal}: {A} {Learned} {NL}-{Interface} for {Databases}},
	isbn = {978-1-4503-4703-7},
	url = {https://doi.org/10.1145/3183713.3193562},
	doi = {10.1145/3183713.3193562},
	abstract = {In this demo, we present DBPal, a novel data exploration tool with a natural language interface. DBPal leverages recent advances in deep models to make query understanding more robust in the following ways: First, DBPal uses novel machine translation models to translate natural language statements to SQL, making the translation process more robust to paraphrasing and linguistic variations. Second, to support the users in phrasing questions without knowing the database schema and the query features, DBPal provides a learned auto-completion model that suggests to users partial query extensions during query formulation and thus helps to write complex queries.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Basik, Fuat and Hättasch, Benjamin and Ilkhechi, Amir and Usta, Arif and Ramaswamy, Shekar and Utama, Prasetya and Weir, Nathaniel and Binnig, Carsten and Cetintemel, Ugur},
	year = {2018},
	note = {event-place: Houston, TX, USA},
	keywords = {relational database, natural language to sql, nlidb, robust natural language interface},
	pages = {1765--1768},
}

@inproceedings{xie_ai_2025,
	address = {New York, NY, USA},
	series = {{ICAIES} '25},
	title = {{AI} + {Art} and {Design} {Education}: {Research} on {Intelligent} {Algorithm}-{Driven} {Educational} {Content} {Generation} {Mechanism}},
	isbn = {979-8-4007-1506-8},
	url = {https://doi.org/10.1145/3744367.3744383},
	doi = {10.1145/3744367.3744383},
	abstract = {Targeting at building up a framework for producing educational contents, this study provides a thorough investigation of cutting-edge applications of artificial intelligence technologies in art and design education with the assistance of intelligent algorithms. Generative AI, deep learning, natural language processing (NLP), computer vision (CV) and other significant tools are employed in this study to deal with varied issues in related areas such as outdated content updates, insufficient personalized teaching, and excessive teacher workload. In this study, various technologies including domain knowledge graph construction, AI-generated content optimization, personalized learning path recommendation, and human-AI collaborative creation mechanisms are all combined to form a novel approach. Based on controlled experimental analysis, this framework is proven to deliver great improvements in teaching quality (p\&lt;0.05), learning efficiency by 32.7\%, and help build up the expression abilities. Besides acting as the theoretical foundation for the combined AI, art, and design education, the research can also contribute to the intelligent transformation of education, holding important value for advancing educational paradigms toward greater precision and personalization.},
	booktitle = {Proceedings of the 2025 {International} {Conference} on {Artificial} {Intelligence} and {Educational} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Xie, Zhixian and Hong, Yina},
	year = {2025},
	keywords = {Adaptive Learning, AI + Art and Design Education, AI Collaborative Creativity, Intelligent Algorithm-Driven Content Generation},
	pages = {88--94},
}

@inproceedings{dwyer_friend_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {Friend or {Foe}? {Navigating} and {Re}-configuring “{Snipers}' {Alley}“},
	isbn = {979-8-4007-1394-1},
	url = {https://doi.org/10.1145/3706598.3713317},
	doi = {10.1145/3706598.3713317},
	abstract = {In a ‘digital by default’ society, essential services must be accessed online. This opens users to digital deception not only from criminal fraudsters but from a range of actors in a marketised digital economy. Using grounded empirical research from northern England, we show how supposedly ‘trusted’ actors, such as governments, (re)produce the insecurities and harms that they seek to prevent. Enhanced by a weakening of social institutions amid a drive for efficiency and scale, this has built a constricted, unpredictable digital channel. We conceptualise this as a “snipers’ alley”. Four key snipers articulated by participants’ lived experiences are examined: 1) Governments; 2) Business; 3) Criminal Fraudsters; and 4) Friends and Family to explore how snipers are differentially experienced and transfigure through this constricted digital channel. We discuss strategies to re-configure the alley, and how crafting and adopting opportunity models can enable more equitable forms of security for all.},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dwyer, Andrew C and Coles-Kemp, Lizzie and Heath, Claude P R and Crivellaro, Clara},
	year = {2025},
	keywords = {Dark Patterns, Digital Access, Digital Economy, Security Models, Threat Models},
}

@inproceedings{vella_hello_2025,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '25},
	title = {"{Hello}, {Mr} {Tree}": {Toying} with {Playful} {Conversational} {AI} in the {Early} {Years}},
	isbn = {979-8-4007-1395-8},
	url = {https://doi.org/10.1145/3706599.3719878},
	doi = {10.1145/3706599.3719878},
	abstract = {The use of generative AI is increasingly integrated into childhood education, primarily with text-to-image generation and older age ranges. This late-breaking work looks at the use of a prototype technology using voiced conversational AI (CAI) to engage young children’s interest in nature, through the role-play of a character: the ‘Talking Tree’. Using research-through-design, we conducted six interactive sessions with children aged 3 to 5 years. These drove the iterative development of the device and provided insight into how CAI might be applied within the context of early learning. We found that the device operated within children’s performative social interactions and within their imagination to prompt recollections of nature and fantastic diversions. We contribute insight into the use of conversational AI for learning in the busy environments of early childhood education centres and the use of CAI-performed fictional characters to build children’s connection with nature.},
	booktitle = {Proceedings of the {Extended} {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Vella, Kellie and Dobson, Madeleine and Brereton, Margot},
	year = {2025},
	keywords = {LLM, AI, design, Conversational agent, Child-computer interaction, Digital play, Early childhood education, Play-based learning, Voice recognition},
}

@inproceedings{bilidas_fire_2023,
	address = {New York, NY, USA},
	series = {{SIGSPATIAL} '23},
	title = {Fire {Risk} {Management} using {Data} {Cubes}, {Machine} {Learning} and {OBDA} systems},
	isbn = {979-8-4007-0168-9},
	url = {https://doi.org/10.1145/3589132.3625615},
	doi = {10.1145/3589132.3625615},
	abstract = {We present a fire risk management system which takes input data from various sources (e.g., meteorological data, satellite indicators for vegetation, historical burned areas), produces a harmonized spatio-temporal data cube to compute fire risk and enables semantic querying to assist fire risk management. The distinguishing implementation features of the system is the use of data cubes, machine learning algorithms and, most importantly, geospatial ontology-based data access technologies. The system has been implemented in the European project DeepCube for the geographic area of Greece and can be used operationally to assist authorities to determine fire risk during the summer fire season.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bilidas, Dimitris and Mantas, Anastasios and Yfantis, Filippos and Stamoulis, George and Koubarakis, Manolis and Kondylatos, Spyros and Prapas, Ioannis and Papoutsis, Ioannis},
	year = {2023},
	note = {event-place: Hamburg, Germany},
	keywords = {machine learning, ontology based data access, data cubes, fire risk},
}

@inproceedings{subramanian_semantic_2019,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '19},
	title = {Semantic {Harmonisation} of {Numeric} {Data} from {Open} {Government} {Data}},
	isbn = {978-1-4503-6207-8},
	url = {https://doi.org/10.1145/3297001.3297032},
	doi = {10.1145/3297001.3297032},
	abstract = {Open tabular data published as part of the open government initiatives typically contain a spatial dimension, a temporal dimension and the actual numeric data capturing information such as health indicators, pollution readings, sanitation status etc. "Semantic Harmonisation" of numeric data entails linking numeric data columns with web-accessible semantic entities from an ontology - a machine readable knowledge representation. These semantic entities are embedded in a knowledge graph, allowing integration of information from disparate sources under common semantic definitions across spatial and temporal dimensions. Multiple research efforts have contributed to recovering semantics of numeric columns in tables, however they are either restricted to a single domain or rely on the existence of numeric data as linked data tuples in known ontologies. We present a novel yet simple approach using a supervised machine learning classifier (Random Forests) and semantic web techniques to generate semantics for numeric columns in tabular data. This approach has been tested with encouraging results for over 100 tabular datasets from data.gov.in (Indian Open Government Data Portal) downloaded from multiple domains such as "Health and Family Welfare", "Agriculture", "Environment" etc. We also present a use case for this work, being implemented in collaboration with the ministries of the Government of Karnataka for knowledge aggregation and dissemination of sustainable development data.},
	booktitle = {Proceedings of the {ACM} {India} {Joint} {International} {Conference} on {Data} {Science} and {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Subramanian, Asha and RR, Pavan Kumar and Vikkurthi, Manikanta and Buttigieg, Pier Luigi},
	year = {2019},
	note = {event-place: Kolkata, India},
	keywords = {Ontologies, Government Data, Semantic Harmonisation},
	pages = {238--244},
}

@article{cornut_annotations_2023,
	title = {Annotations as {Knowledge} {Practices} in {Image} {Archives}: {Application} of {Linked} {Open} {Usable} {Data} and {Machine} {Learning}},
	volume = {16},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3625301},
	doi = {10.1145/3625301},
	abstract = {We reflect on some of the preliminary findings of the Participatory Knowledge Practices in Analogue and Digital Image Archives (PIA) research project around annotations of photographic archives from the Swiss Society for Folklore Studies (SSFS) as knowledge practices, the underlying technological decisions, and their impact. The aim is not only to seek more information but to find new approaches of understanding the way in which people’s memory relate to the collective, public form of archival memory and ultimately how users figure in and shape the digital archive.We provide a proof-of-concept workflow based on automatically generated annotations comprising 53,481 photos that were subjected to object detection using Faster R-CNN Inception ResNet V2. Of the detected objects, 184,609 have a detection score greater than 0.5, 123,529 have a score greater than 0.75, and 88,442 have a score greater than 0.9. A threshold of 0.75 was set for the dissemination of our annotations, compatible with the W3C Web Annotation Data Model (WADM) and embedded in our IIIF Manifests.In the near future, the workflow will be upgraded to allow for the co-existence of various, and occasionally conflicting, assertions made by both human and machine users. We believe that Linked Open Usable Data (LOUD) standards should be used to improve the sustainability of such an ecosystem and to foster collaboration between actors in cultural heritage.},
	number = {4},
	journal = {J. Comput. Cult. Herit.},
	author = {Cornut, Murielle and Raemy, Julien Antoine and Spiess, Florian},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {machine learning, memory, Citizen science, cultural heritage, cultural anthropology, digital materiality, international image interoperability framework, linked art, linked open usable data, object detection, participatory knowledge practices in analogue and digital image archives, web annotation data model},
}

@article{van_der_linden_medicospace_2023,
	title = {{MediCoSpace}: {Visual} {Decision}-{Support} for {Doctor}-{Patient} {Consultations} using {Medical} {Concept} {Spaces} from {EHRs}},
	volume = {14},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3564275},
	doi = {10.1145/3564275},
	abstract = {Healthcare systems are under pressure from an aging population, rising costs, and increasingly complex conditions and treatments. Although data are determined to play a bigger role in how doctors diagnose and prescribe treatments, they struggle due to a lack of time and an abundance of structured and unstructured information. To address this challenge, we introduce MediCoSpace, a visual decision-support tool for more efficient doctor-patient consultations. The tool links patient reports to past and present diagnoses, diseases, drugs, and treatments, both for the current patient and other patients in comparable situations. MediCoSpace uses textual medical data, deep-learning supported text analysis and concept spaces to facilitate a visual discovery process. The tool is evaluated by five medical doctors. The results show that MediCoSpace facilitates a promising, yet complex way to discover unlikely relations and thus suggests a path toward the development of interactive visual tools to provide physicians with more holistic diagnoses and personalized, dynamic treatments for patients.},
	number = {2},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {van der Linden, Sanne and Sevastjanova, Rita and Funk, Mathias and El-Assady, Mennatallah},
	month = jan,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language processing, electronic health records, Visual analytics, interaction design},
}

@inproceedings{chua_classifying_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Classifying {Multimodal} {Data} {Using} {Transformers}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3542634},
	doi = {10.1145/3534678.3542634},
	abstract = {The increasing prevalence of multimodal data in our society has led to the increased need for machines to make sense of such data holistically. However, data scientists and machine learning engineers aspiring to work on such data face challenges fusing the knowledge from existing tutorials which often deal with each mode separately. Drawing on our experience in classifying multimodal municipal issue feedback in the Singapore government, we conduct a hands-on tutorial to help flatten the learning curve for practitioners who want to apply machine learning to multimodal data.},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Chua, Watson W.K. and Li, Lu and Goh, Alvina},
	year = {2022},
	note = {event-place: Washington DC, USA},
	keywords = {natural language processing, deep learning, transformers, multimodal learning, computer vision, vision-language representation},
	pages = {4780--4781},
}

@inproceedings{bowles_datamod2020_2020,
	address = {New York, NY, USA},
	series = {{CIKM} '20},
	title = {{DataMod2020}: 9th {International} {Symposium} "{From} {Data} to {Models} and {Back}"},
	isbn = {978-1-4503-6859-9},
	url = {https://doi.org/10.1145/3340531.3414073},
	doi = {10.1145/3340531.3414073},
	abstract = {DataMod 2020 aims to bring together practitioners and researchers from academia, industry and research institutions interested in the combined application of computational modelling methods with data-driven techniques from the areas of knowledge management, data mining and machine learning. Modelling methodologies of interest include automata, agents, Petri nets, process algebras and rewriting systems. Application domains include social systems, ecology, biology, medicine, smart cities, governance, security, education, software engineering, and any other field that deals with complex systems and large amounts of data. Papers can present research results in any of the themes of interest for the symposium as well as application experiences, tools and promising preliminary ideas. Papers dealing with synergistic approaches that integrate modelling and knowledge management/discovery or that exploit knowledge management/discovery to develop/syntesise system models are especially welcome.},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \&amp; {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Bowles, Juliana and Broccia, Giovanna and Nanni, Mirco},
	year = {2020},
	note = {event-place: Virtual Event, Ireland},
	keywords = {machine learning, text mining, deep learning, formal methods, big data analytics, process calculi, processing mining},
	pages = {3531--3532},
}

@article{long_decoding_2025,
	title = {Decoding {Digital} {Risk} {From} {Corporate} {Disclosure}: {A} {Neural} {Network} {Approach}},
	volume = {16},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3728365},
	doi = {10.1145/3728365},
	abstract = {Digital risk—or the likelihood of losses from key digital activities (i.e., information system [IS] sourcing, digital infrastructure, data management, IS applications, IS use, and digital product offerings)—constitutes a key consideration in firm valuation. Firms’ public disclosures (e.g., 10-K reports, earnings conference calls) are a key source of data to learn about digital risks. Although text analytics approaches (e.g., word frequency, topic modeling, and sentiment analysis) have been applied to a firm's public disclosures to assess various types of risk (e.g., political risk, tax risk, cybersecurity), they do not consider the structural linguistic relations embedded in the text that are potentially relevant in measuring risk.We apply a neural network approach to address this gap and extract linguistic relations from a firm's 10-K disclosure (Section “Item 1A”). We develop novel firm-level digital risk measures based on these linguistic relations. Specifically, we measure firm-level digital risk from three perspectives: (1) presence (whether digital risk is mentioned or not), (2) intensity (text coverage of digital risk relative to other issues), and (3) diversity (the types of digital risk mentioned).We validate our digital risk measures by demonstrating their significant correlation with firm risk, proxied by stock market volatility. Our research reveals that investors’ perceptions of digital risk diversity and digital risk intensity differ between IT and non-IT companies. First, across all firms, digital risk intensity is negatively associated with firm risk, indicating that investors do not incorporate intensity of digital risk when assessing firm risk. Second, in non-IT firms, digital risk diversity is positively associated with firm risk, suggesting that managers in these firms may influence investor perceptions through strategic disclosure of digital risk types. Overall, our findings suggest that text-based digital risk measurement is practically feasible, scalable, and economically meaningful.},
	number = {3},
	journal = {ACM Trans. Manage. Inf. Syst.},
	author = {Long, Yuan and Rai, Arun},
	month = may,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Deep learning, Textual analysis, Corporate disclosure, Digital risk, Linguistic structure},
}

@inproceedings{perez-martinez_towards_2025,
	address = {New York, NY, USA},
	series = {{UMAP} {Adjunct} '25},
	title = {Towards {Cultural} {Preservation} of {Traditional} {Motion} {Knowledge} through {Automated} {Annotations} with {MoR}℡aban},
	isbn = {979-8-4007-1399-6},
	url = {https://doi.org/10.1145/3708319.3733657},
	doi = {10.1145/3708319.3733657},
	abstract = {Movement disciplines like dance or martial arts are carriers of cultural knowledge, identity, and tradition. However, oral traditions and video recordings make the preservation of this knowledge susceptible to being lost. Expert movement notation, in turn, holds the potential for precise capture and knowledge inheritance. However, motion notation approaches are not widespread, the process is often time-consuming, and the movements are hard to visualize without expert knowledge. In this work, we use Labanotation and Laban Movement Analysis (LMA), a notation system and method originally developed for dance, as a symbolic, interpretable framework for motion representation and preservation. Our contribution resides in the expansion of an existing annotation system, the LabanEditor, to handle full-body motion and data from multiple sources, and support the work of experts in annotating the movements. Our development, called MoR℡aban, supports motion-to-notation and inverse mapping from notation to keyframes, enabling exchange between video, motion capture, and Labanotation formats. This allows for the documentation and reconstruction of traditional motion practices using expert-readable scores and 3D skeletons.},
	booktitle = {Adjunct {Proceedings} of the 33rd {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Perez-Martinez, Roberto and Casas-Ortiz, Alberto and Santos, Olga C.},
	year = {2025},
	keywords = {knowledge representation, expert systems, Labanotation, motion modeling, movement modeling, Laban Movement Analysis (LMA)},
	pages = {459--463},
}

@inproceedings{zhao_ceil_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23},
	title = {{CEIL}: {A} {General} {Classification}-{Enhanced} {Iterative} {Learning} {Framework} for {Text} {Clustering}},
	isbn = {978-1-4503-9416-1},
	url = {https://doi.org/10.1145/3543507.3583457},
	doi = {10.1145/3543507.3583457},
	abstract = {Text clustering, as one of the most fundamental challenges in unsupervised learning, aims at grouping semantically similar text segments without relying on human annotations. With the rapid development of deep learning, deep clustering has achieved significant advantages over traditional clustering methods. Despite the effectiveness, most existing deep text clustering methods rely heavily on representations pre-trained in general domains, which may not be the most suitable solution for clustering in specific target domains. To address this issue, we propose CEIL, a novel Classification-Enhanced Iterative Learning framework for short text clustering, which aims at generally promoting the clustering performance by introducing a classification objective to iteratively improve feature representations. In each iteration, we first adopt a language model to retrieve the initial text representations, from which the clustering results are collected using our proposed Category Disentangled Contrastive Clustering (CDCC) algorithm. After strict data filtering and aggregation processes, samples with clean category labels are retrieved, which serve as supervision information to update the language model with the classification objective via a prompt learning approach. Finally, the updated language model with improved representation ability is used to enhance clustering in the next iteration. Extensive experiments demonstrate that the CEIL framework significantly improves the clustering performance over iterations, and is generally effective on various clustering algorithms. Moreover, by incorporating CEIL on CDCC, we achieve the state-of-the-art clustering performance on a wide range of short text clustering benchmarks outperforming other strong baseline methods.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Mingjun and Wang, Mengzhen and Ma, Yinglong and Niu, Di and Wu, Haijiang},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	keywords = {Classification-enhanced Clustering, Iterative Framework, Text Clustering},
	pages = {1784--1792},
}

@inproceedings{gao_research_2025-1,
	address = {New York, NY, USA},
	series = {{MLNN} '25},
	title = {Research on construction technology and application of knowledge graph in equipment fault {Diagnosis} domain},
	isbn = {979-8-4007-1438-2},
	url = {https://doi.org/10.1145/3747227.3747237},
	doi = {10.1145/3747227.3747237},
	abstract = {In recent years, the construction technology of general knowledge graph (KG) has been developing continuously. The medical industry, manufacturing industry, financial industry and other industries have constructed domain KGs. The research of KG in the equipment field is mainly focused on general equipment knowledge, and the field of equipment fault diagnosis also needs to build its own domain KG. Combined with the definition of the general KG, the definition and construction process of the equipment fault diagnosis domain KG are expounded, the specific technical methods of each link of the construction process are summarized, and the application of the equipment fault diagnosis domain KG is explained, and some positive exploration is carried out for the subsequent construction of the equipment fault diagnosis domain KG.},
	booktitle = {Proceedings of the 2025 {International} {Conference} on {Machine} {Learning} and {Neural} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Gao, Feifei and Zhang, Lin and Zhang, Bo and Wang, Wenfeng and Liu, Wei and Zhang, Jingyi and Liu, Han and Qiu, Shi and Huang, Kai and Zhang, Mingang},
	year = {2025},
	keywords = {Knowledge extraction, Ontology building, Domain knowledge graph, Data acquisition, Equipment fault diagnosis, Knowledge processing, Knowledge storage},
	pages = {61--69},
}

@inproceedings{mannekote_exploring_2023,
	address = {New York, NY, USA},
	series = {{CUI} '23},
	title = {Exploring {Usability} {Issues} in {Instruction}-{Based} and {Schema}-{Based} {Authoring} of {Task}-{Oriented} {Dialogue} {Agents}},
	isbn = {979-8-4007-0014-9},
	url = {https://doi.org/10.1145/3571884.3604310},
	doi = {10.1145/3571884.3604310},
	abstract = {Platforms such as Google DialogFlow and Amazon Lex have enabled easier development of conversational agents. The standard approach to training these agents involve collecting and annotating in-domain data in the form of labelled utterances. However, obtaining in-domain data for training machine learning models remains a bottleneck. Schema-based dialogue, which involves laying out a structured representation of the flow of a “typical” dialogue, and prompt-based methods, which involve writing instructions in natural language to large language models such as GPT-3, are promising ways to tackle this problem. However, usability issues when translating these methods into practice are less explored. Our study takes a first step towards addressing this gap by having 23 students who had finished a graduate-level course on spoken dialogue systems report their experiences as they defined structured schemas and composed instruction-based prompts for two task-oriented dialogue scenarios. Through inductive coding and subsequent thematic analysis of the survey data, we explored users’ authoring experiences with schema and prompt-based methods. The findings provide insights for future data collection and authoring tool design for dialogue systems.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Mannekote, Amogh and Celepkolu, Mehmet and Wiggins, Joseph B. and Boyer, Kristy Elizabeth},
	year = {2023},
	note = {event-place: Eindhoven, Netherlands},
	keywords = {dialogue systems, schema-based dialogue, user studies, zero-shot prompting},
}

@inproceedings{balwani_approach_2024,
	address = {New York, NY, USA},
	series = {{ISEC} '24},
	title = {An {Approach} for {Providing} {Recommendation} for {Requirements} {Non}-{Conformant} with {Requirement} {Templates} ({RTs})},
	isbn = {979-8-4007-1767-3},
	url = {https://doi.org/10.1145/3641399.3641412},
	doi = {10.1145/3641399.3641412},
	abstract = {RTs generally possess a fixed syntactic structure and comprise pre-defined slots, and requirements written in the format of RTs must conform with the template structure. Suppose the requirements do not conform to the RT. In that case, manually verifying the conformity of requirements to RTs becomes a tedious task due to the large size of industry requirement documents and introduces the possibility of errors. Furthermore, rewriting requirements to conform to the template structure when they initially do not conform presents a significant challenge. This paper proposes a tool-based approach that automatically verifies whether Functional Requirements (FRs) conform to RTs. It recommends a Template Conformance (TC) requirement by generating a semantically identical requirement that Conforms to the template structure. Our study focused on two well-known RTs, Easy Approach to Requirements Syntax (EARS) and RUPPs, for checking conformance and making recommendations. We utilized Natural Language Processing (NLP) techniques and applied our approach to industrial and publicly available case studies. Our results demonstrate that the proposed tool-based approach facilitates requirement analysis and aids in recommending requirements based on their conformity with RTs. Our results show an accuracy of 83.9\% for providing recommendations to non-conformant requirements with RTs.},
	booktitle = {Proceedings of the 17th {Innovations} in {Software} {Engineering} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Balwani, Shivani and Tiwari, Saurabh and Dasgupta, Sourish and Sharma, Akhilesh},
	year = {2024},
	note = {event-place: Bangalore, India},
	keywords = {Quality, Analysis, Natural Language Processing (NLP), Recommendation, Requirement Templates (RTs)},
}

@inproceedings{anelli_fifth_2023,
	address = {New York, NY, USA},
	series = {{RecSys} '23},
	title = {Fifth {Knowledge}-aware and {Conversational} {Recommender} {Systems} {Workshop} ({KaRS})},
	isbn = {979-8-4007-0241-9},
	url = {https://doi.org/10.1145/3604915.3608759},
	doi = {10.1145/3604915.3608759},
	abstract = {Recommender systems have become ubiquitous in daily life, but their limitations in interacting with human users have become evident. Deep learning approaches have led to the development of data-driven algorithms that identify connections between users and items, but they often miss a critical actor in the loop - the end-user. Knowledge-based approaches are gaining attention due to the availability of knowledge-graphs, such as DBpedia and Wikidata, which provide semantics-aware information on different knowledge domains. These approaches are being used for recommendation and challenges such as knowledge graph embeddings, hybrid recommendation, and interpretable recommendation. Moreover, the emergence of neural-symbolic systems, which combine data-driven and symbolic methods, can significantly improve recommendation systems. A growing number of research papers on such topics demonstrate the growing interest and research potential of these systems. Furthermore, content features become crucial when interaction requires it. The development of conversational recommender systems presents new challenges, as they require multi-turn dialogues between users and systems, blurring the line between recommendation and retrieval. Evaluation of these systems goes beyond simple accuracy metrics and is hampered by the limited availability of datasets. While research and development into conversational recommender systems has been less prominent in the past, recent literature shows growing interest and potential for these systems.},
	booktitle = {Proceedings of the 17th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Anelli, Vito Walter and Basile, Pierpaolo and De Melo, Gerard and Donini, Francesco M and Ferrara, Antonio and Musto, Cataldo and Narducci, Fedelucio and Ragone, Azzurra and Zanker, Markus},
	year = {2023},
	note = {event-place: Singapore, Singapore},
	keywords = {Semantic Web, Knowledge Graphs, Natural Language Processing, Knowledge Representation, Recommender systems, Conversational Agents, Neural-Symbolic Reasoning},
	pages = {1259--1262},
}

@article{ustalov_watset_2019,
	title = {Watset: {Local}-{Global} {Graph} {Clustering} with {Applications} in {Sense} and {Frame} {Induction}},
	volume = {45},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00354},
	doi = {10.1162/coli_a_00354},
	abstract = {We present a detailed theoretical and computational analysis of the Watset meta-algorithm for fuzzy graph clustering, which has been found to be widely applicable in a variety of domains. This algorithm creates an intermediate representation of the input graph, which reflects the “ambiguity” of its nodes. Then, it uses hard clustering to discover clusters in this “disambiguated” intermediate graph. After outlining the approach and analyzing its computational complexity, we demonstrate that Watset shows competitive results in three applications: unsupervised synset induction from a synonymy graph, unsupervised semantic frame induction from dependency triples, and unsupervised semantic class induction from a distributional thesaurus. Our algorithm is generic and can also be applied to other networks of linguistic data.},
	number = {3},
	journal = {Comput. Linguist.},
	author = {Ustalov, Dmitry and Panchenko, Alexander and Biemann, Chris and Ponzetto, Simone Paolo},
	month = sep,
	year = {2019},
	note = {Place: Cambridge, MA, USA
Publisher: MIT Press},
	pages = {423--479},
}

@inproceedings{bendeck_slopeseeker_2024,
	address = {New York, NY, USA},
	series = {{IUI} '24},
	title = {{SlopeSeeker}: {A} {Search} {Tool} for {Exploring} a {Dataset} of {Quantifiable} {Trends}},
	isbn = {979-8-4007-0508-3},
	url = {https://doi.org/10.1145/3640543.3645208},
	doi = {10.1145/3640543.3645208},
	abstract = {Natural language and search interfaces intuitively facilitate data exploration and provide visualization responses to diverse analytical queries based on the underlying datasets. However, these interfaces often fail to interpret more complex analytical intents, such as discerning subtleties and quantifiable differences between terms like “bump’’ and “spike’’ in the context of COVID cases, for example. We address this gap by extending the capabilities of a data exploration search interface for interpreting semantic concepts in time series trends. We first create a comprehensive dataset of semantic concepts by mapping quantifiable univariate data trends such as slope and angle to crowdsourced, semantically meaningful trend labels. The dataset contains quantifiable properties that capture the slope-scalar effect of semantic modifiers like “sharply” and “gradually,” as well as multi-line trends (e.g., “peak,” “valley”). We demonstrate the utility of this dataset in SlopeSeeker, a tool that supports natural language querying of quantifiable trends, such as “show me stocks that tanked in 2010.” The tool incorporates novel scoring and ranking techniques based on semantic relevance and visual prominence to present relevant trend chart responses containing these semantic trend concepts. In addition, SlopeSeeker provides a faceted search interface for users to navigate a semantic hierarchy of concepts from general trends (e.g., “increase’’) to more specific ones (e.g., “sharp increase’’). A preliminary user evaluation of the tool demonstrates that the search interface supports greater expressivity of queries containing concepts that describe data trends. We identify potential future directions for leveraging our publicly available quantitative semantics dataset in other data domains and for novel visual analytics interfaces.},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Bendeck, Alexander and Bromley, Dennis and Setlur, Vidya},
	year = {2024},
	note = {event-place: Greenville, SC, USA},
	keywords = {Semantics, search, quantifiable metadata, trends, visual analysis.},
	pages = {817--836},
}

@inproceedings{al_rahat_authsaber_2024,
	address = {New York, NY, USA},
	series = {{CCS} '24},
	title = {{AuthSaber}: {Automated} {Safety} {Verification} of {OpenID} {Connect} {Programs}},
	isbn = {979-8-4007-0636-3},
	url = {https://doi.org/10.1145/3658644.3670318},
	doi = {10.1145/3658644.3670318},
	abstract = {Single Sign-On (SSO)-based authentication protocols, like OpenID Connect (OIDC), play a crucial role in enhancing security and privacy in today's interconnected digital world, gaining widespread adoption among the majority of prominent authentication service providers. These protocols establish a structured framework for verifying and authenticating the identities of individuals, organizations, and devices, while avoiding the necessity of sharing sensitive credentials (e.g., passwords) with external entities. However, the security guarantees of these protocols rely on their proper implementation, and real-world implementations can, and indeed often do, contain logical programming errors leading to severe attacks, including authentication bypass and user account takeover. In response to this challenge, we present AuthSaber, an automated verifier designed to assess the real-world OIDC protocol implementations against their standard safety specifications in a scalable manner. AuthSaber addresses the challenges of expressiveness for OIDC properties, modeling multi-party interactions, and automation by first designing a novel specification language based on linear temporal logic, leveraging an automaton-based approach to constrain the space of possible interactions between OIDC entities, and incorporating several domain-specific transformations to obtain programs and properties that can be directly reasoned about by software model checkers. We evaluate AuthSaber on the 15 most popular and widely used OIDC libraries and discover 16 previously unknown vulnerabilities, all of which are responsively disclosed to the developers. Five categories of these vulnerabilities also led to new CVEs.},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Al Rahat, Tamjid and Feng, Yu and Tian, Yuan},
	year = {2024},
	note = {event-place: Salt Lake City, UT, USA},
	keywords = {authentication, authorization, automated analysis, openid connect security, safety verification, single sign-on},
	pages = {2949--2962},
}

@inproceedings{wang_gsr_2020,
	address = {New York, NY, USA},
	series = {{ICIAI} '20},
	title = {{GSR}: {A} {Resource} {Model} and {Semantics}-based {API} {Recommendation} {Algorithm}},
	isbn = {978-1-4503-7658-7},
	url = {https://doi.org/10.1145/3390557.3394128},
	doi = {10.1145/3390557.3394128},
	abstract = {With the rapid development of Web services, more and more Web services are published on the Internet. A Mashup application that aggregates multiple Web APIs is also becoming more popular. But it also brings a problem that is how to find a suitable API among a wide variety of APIs has become a challenge. To this end, this paper proposes a web service recommendation algorithm that combines graph databases and semantics. In this algorithm, we propose to use graph database to build a two-layer structure resource model. First, we use LDA for topic classification and classify Mashup and API of the same classification into the same category respectively. This helps reduce the number of searches for Mashup and API. When a user enters a requirement document, Word2vec and WMD algorithms are used to find similar Web API description text. Finally, we use similarity and API history invokes to propose a ranking algorithm to generate a recommendation list. Through real-world data, this experiment has a better-recommended performance.},
	booktitle = {Proceedings of the 2020 the 4th {International} {Conference} on {Innovation} in {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Jiawei and Cui, Guorong and Zhu, Xiaoke and Liu, Huijian and Liu, Junsong and Jia, Xuebin},
	year = {2020},
	note = {event-place: Xiamen, China},
	keywords = {LDA, API recommendation, Resource Model, Word Mover's Distance},
	pages = {184--188},
}

@inproceedings{conrad_compositional_2022,
	address = {New York, NY, USA},
	series = {{CPP} 2022},
	title = {A compositional proof framework for {FRETish} requirements},
	isbn = {978-1-4503-9182-5},
	url = {https://doi.org/10.1145/3497775.3503685},
	doi = {10.1145/3497775.3503685},
	abstract = {Structured natural languages provide a trade space between ambiguous natural languages that make up most written requirements, and mathematical formal specifications such as Linear Temporal Logic. FRETish is a structured natural language for the elicitation of system requirements developed at NASA. The related open-source tool Fret provides support for translating FRETish requirements into temporal logic formulas that can be input to several verification and analysis tools. In the context of safety-critical systems, it is crucial to ensure that a generated formula captures the semantics of the corresponding FRETish requirement precisely. This paper presents a rigorous formalization of the FRETish language including a new denotational semantics and a proof of semantic equivalence between FRETish specifications and their temporal logic counterparts computed by Fret. The complete formalization and the proof have been developed in the Prototype Verification System (PVS) theorem prover.},
	booktitle = {Proceedings of the 11th {ACM} {SIGPLAN} {International} {Conference} on {Certified} {Programs} and {Proofs}},
	publisher = {Association for Computing Machinery},
	author = {Conrad, Esther and Titolo, Laura and Giannakopoulou, Dimitra and Pressburger, Thomas and Dutle, Aaron},
	year = {2022},
	note = {event-place: Philadelphia, PA, USA},
	keywords = {Requirements, Formal Proofs, Metric Temporal Logic, PVS, Structured Natural Language},
	pages = {68--81},
}

@article{costa_evaluation_2022,
	title = {An {Evaluation} of {Graph} {Databases} and {Object}-{Graph} {Mappers} in {CIDOC} {CRM}-{Compliant} {Digital} {Archives}},
	volume = {15},
	issn = {1556-4673},
	url = {https://doi.org/10.1145/3485847},
	doi = {10.1145/3485847},
	abstract = {The Portuguese General Directorate for Book, Archives and Libraries (DGLAB) has selected CIDOC CRM as the basis for its next-generation digital archive management software. Given the ontological foundations of the Conceptual Reference Model (CRM), a graph database or a triplestore was seen as the best candidate to represent a CRM-based data model for the new software. We thus decided to compare several of these databases, based on their maturity, features, performance in standard tasks and, most importantly, the Object-Graph Mappers (OGM) available to interact with each database in an object-oriented way. Our conclusions are drawn not only from a systematic review of related works but from an experimental scenario. For our experiment, we designed a simple CRM-compliant graph designed to test the ability of each OGM/database combination to tackle the so-called “diamond-problem” in Object-Oriented Programming (OOP) to ensure that property instances follow domain and range constraints.\&nbsp;\&nbsp;Our results show that (1) ontological consistency enforcement in graph databases and triplestores is much harder to achieve than in a relational database, making them more suited to an analytical rather than a transactional role; (2) OGMs are still rather immature solutions; and (3) neomodel, an OGM for the Neo4j graph database, is the most mature solution in the study as it satisfies all requirements, although it is also the least performing.},
	number = {3},
	journal = {J. Comput. Cult. Herit.},
	author = {Costa, Lázaro and Freitas, Nuno and da Silva, João Rocha},
	month = sep,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {CIDOC CRM, comparison, graph databases, digital archives, Object-graph mapping},
}

@inproceedings{li_lisum_2024,
	address = {Echternach, Luxembourg},
	series = {{ASE} '23},
	title = {{LiSum}: {Open} {Source} {Software} {License} {Summarization} with {Multi}-{Task} {Learning}},
	isbn = {979-8-3503-2996-4},
	url = {https://doi.org/10.1109/ASE56229.2023.00150},
	doi = {10.1109/ASE56229.2023.00150},
	abstract = {Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13\% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.},
	booktitle = {Proceedings of the 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai, Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
	year = {2024},
	note = {ISSN: 2643-1572},
	keywords = {Formal languages, Training, Surveys, Measurement, multi-task learning, license comprehension, open source software licenses, Learning systems, License comprehension, Licenses, Multi-Task Learning, Multitasking, Open Source Software Licenses},
	pages = {787--799},
}

@article{li_reinforcement_2022,
	title = {Reinforcement {Learning}-{Based} {Dialogue} {Guided} {Event} {Extraction} to {Exploit} {Argument} {Relations}},
	volume = {30},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2021.3138670},
	doi = {10.1109/TASLP.2021.3138670},
	abstract = {Event extraction is a ftask for natural language processing. Finding the roles of event arguments like event participants is essential for event extraction. However, doing so for real-life event descriptions is challenging because an argument’s role often varies in different contexts. While the relationship and interactions between multiple arguments are useful for settling the argument roles, such information is largely ignored by existing approaches. This paper presents a better approach for event extraction by explicitly utilizing the relationships of event arguments. We achieve this through a carefully designed task-oriented dialogue system. To model the argument relation, we employ reinforcement learning and incremental learning to extract multiple arguments via a multi-turned, iterative process. Our approach leverages knowledge of the already extracted arguments of the same sentence to determine the role of arguments that would be difficult to decide individually. It then uses the newly obtained information to improve the decisions of previously extracted arguments. This two-way feedback process allows us to exploit the argument relations to effectively settle argument roles, leading to better sentence understanding and event extraction. Experimental results show that our approach consistently outperforms seven state-of-the-art event extraction methods for the classification of events and argument role and argument identification.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Li, Qian and Peng, Hao and Li, Jianxin and Wu, Jia and Ning, Yuanxing and Wang, Lihong and Yu, Philip S. and Wang, Zheng},
	month = dec,
	year = {2022},
	note = {Publisher: IEEE Press},
	pages = {520--533},
}

@inproceedings{engel_semantification_2024,
	address = {Santa Fe, New Mexico, USA},
	series = {{JCDL} '23},
	title = {Semantification of {Space} {Data} - {A} {Feasibility} {Study}},
	isbn = {979-8-3503-9931-8},
	url = {https://doi.org/10.1109/JCDL57899.2023.00065},
	doi = {10.1109/JCDL57899.2023.00065},
	abstract = {This paper presents a new approach to the semantic representation of NASA planetary mission data. The Open Archival Information Systems (OAIS) Information Model (IM) is used as a model to represent this data in a Knowledge Graph (KG). To prepare the data for this requirement, a machine learning approach is used to extract information (entities) and connect to Wikidata. A demo shows the advantages of using federated SPARQL queries over the created KG and Wikidata.},
	booktitle = {Proceedings of the 2023 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Engel, Felix and Krdzavac, Nenad and Tuncay, Erhun Giray and Klinger, Axel and Hughes, John},
	year = {2024},
	keywords = {natural language processing, knowledge graph, extracting semantics},
	pages = {295--296},
}

@inproceedings{kang_praxes_2023,
	address = {New York, NY, USA},
	series = {{FAccT} '23},
	title = {On the {Praxes} and {Politics} of {AI} {Speech} {Emotion} {Recognition}},
	isbn = {979-8-4007-0192-4},
	url = {https://doi.org/10.1145/3593013.3594011},
	doi = {10.1145/3593013.3594011},
	abstract = {There is no scientific consensus on what is meant by “emotion” – researchers have examined various phenomena spanning brain modes, feelings, sensations, and cognitive structures, among others, in their study of emotional experiences. For the purposes of developing an AI speech emotion recognition (SER) system, however, emotion must be defined, bounded, and instantiated as ground truth in the training data. This means practical choices must be made in which particular emotional ontologies are prioritized over others in the construction of SER datasets. In this paper, I explore these tensions around fairness, accountability, and transparency by analyzing open-source datasets used for SER applications along with their accompanying methodology papers. Specifically, I critique the centrality of discrete emotion theory in SER applications as a contestable emotional framework that is invoked primarily for its practical utility and alignment – as opposed to scientific rigor – with machine learning epistemologies. In so doing, I also shed light on the role of the dataset creators as emotional designers in their attempt to produce, elicit, record, and index emotional expressions for the purposes of crafting SER training datasets. Ultimately, by further querying SER through the aperture of Critical Disability Studies, I use this empirical work to examine the sociopolitical stakes of SER as a normative and regulatory technology that siphons emotion into a broader agenda of capitalistic productivity in the context of call center optimization.},
	booktitle = {Proceedings of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Kang, Edward B.},
	year = {2023},
	note = {event-place: Chicago, IL, USA},
	pages = {455--466},
}

@inproceedings{peri_towards_2020,
	address = {New York, NY, USA},
	series = {{LAK} '20},
	title = {Towards understanding the lifespan and spread of ideas: epidemiological modeling of participation on {Twitter}},
	isbn = {978-1-4503-7712-6},
	url = {https://doi.org/10.1145/3375462.3375515},
	doi = {10.1145/3375462.3375515},
	abstract = {How ideas develop and evolve is a topic of interest for educators. By understanding this process, designers and educators are better able to support and guide collaborative learning activities. This paper presents an application of our Lifespan of an Idea framework to measure engagement patterns among individuals in communal socio-technical spaces like Twitter. We correlated engagement with social participation, enabling the process of idea expression, spread, and evolution. Social participation leads to transmission of ideas from one individual to another and can be gauged in the same way as evaluating diseases. The temporal dynamics of the social participation can be modeled through the lens of epidemiological modeling. To test the plausibility of this framework, we investigated social participation on Twitter using the tweet posting patterns of individuals in three academic conferences and one long term chat space. We used a basic SIR epidemiological model, where the rate parameters were estimated through Euler's solutions to SIR model and non-linear least squares optimization technique. We discuss the differences in the social participation among individuals in these spaces based on their transition behavior into different categories of the SIR model. We also made inferences on how the total lifetime of these different twitter spaces affects the engagement among individuals. We conclude by discussing implications of this study and planned future research of refining the Lifespan of an Idea Framework.},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Learning} {Analytics} \&amp; {Knowledge}},
	publisher = {Association for Computing Machinery},
	author = {Peri, Sai Santosh Sasank and Chen, Bodong and Dougall, Angela Liegey and Siemens, George},
	year = {2020},
	note = {event-place: Frankfurt, Germany},
	keywords = {epidemiology, connectivism, engagement patterns, ideas, knowledge creation, networked learning},
	pages = {197--202},
}

@inproceedings{timmer_nasa_2023,
	address = {New York, NY, USA},
	series = {{WWW} '23 {Companion}},
	title = {{NASA} {Science} {Mission} {Directorate} {Knowledge} {Graph} {Discovery}},
	isbn = {978-1-4503-9419-2},
	url = {https://doi.org/10.1145/3543873.3587585},
	doi = {10.1145/3543873.3587585},
	abstract = {The size of the National Aeronautics and Space Administration (NASA) Science Mission Directorate (SMD) data catalog is growing exponentially, allowing researchers to make discoveries. However, making discoveries is challenging and time-consuming due to the size of the data catalogs, and as many concepts and data are indirectly connected. This paper proposes a pipeline to generate knowledge graphs (KGs) representing different NASA SMD domains. These KGs can be used as the basis for dataset search engines, saving researchers time and supporting them in finding new connections. We collected textual data and used several modern natural language processing (NLP) methods to create the nodes and the edges of the KGs. We explore the cross-domain connections, discuss our challenges, and provide future directions to inspire researchers working on similar challenges.},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {Association for Computing Machinery},
	author = {Timmer, Roelien C. and Mark, Megan and Khoo, Fech Scen and Ribeiro Martins, Marcella Scoczynski and Berea, Anamaria and Renard, Gregory and Bugbee, Kaylin},
	year = {2023},
	note = {event-place: Austin, TX, USA},
	pages = {795--799},
}

@inproceedings{kuhne_fmmlx_2024,
	address = {New York, NY, USA},
	series = {{MODELS} {Companion} '24},
	title = {{FMMLx} and {DLM} – {A} {Contribution} to the {MULTI} {Collaborative} {Comparison} {Challenge}},
	isbn = {979-8-4007-0622-6},
	url = {https://doi.org/10.1145/3652620.3688212},
	doi = {10.1145/3652620.3688212},
	abstract = {This paper is a response to the MULTI 2022 Collaborative Comparison Challenge [23]. We compare FMMLx- and DLM-based solutions. We first present each approach and solution separately, and then discuss trade-offs of both the solutions and the approaches.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 27th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kühne, Thomas and Maier, Pierre},
	year = {2024},
	note = {event-place: Linz, Austria},
	keywords = {DLM, FMMLx, MLM, modeling challenge},
	pages = {800--809},
}

@inproceedings{viola_playsoundspace_2018,
	address = {New York, NY, USA},
	series = {{SAAM} '18},
	title = {Playsound.space: enhancing a live music performance tool with semantic recommendations},
	isbn = {978-1-4503-6495-9},
	url = {https://doi.org/10.1145/3243907.3243908},
	doi = {10.1145/3243907.3243908},
	abstract = {Playsound is a simple and intuitive web-based tool for music composition based on sounds from Freesound, an online repository of diverse audio content with Creative Commons licenses. In this paper, we present an approach based on Semantic Web technologies to provide recommendations to Playsound users. A Semantic Web of Things architecture is outlined, showing loosely coupled, independent software agents interoperating by means of a semantic publish/subscribe platform and a set of ontologies to describe agents, audio contents, input/output of audio analytics tools and recommendations. Preliminary tests confirm that the designed architecture adapts well to environments where services can be discovered and seamlessly orchestrated on the fly, resulting in a dynamic workflow.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Semantic} {Applications} for {Audio} and {Music}},
	publisher = {Association for Computing Machinery},
	author = {Viola, Fabio and Stolfi, Ariane and Milo, Alessia and Ceriani, Miguel and Barthet, Mathieu and Fazekas, György},
	year = {2018},
	note = {event-place: Monterey, CA, USA},
	keywords = {Ontologies, Semantic Web, Recommendations, Web of Things},
	pages = {46--53},
}

@inproceedings{kourtiche_internet_2022,
	address = {New York, NY, USA},
	series = {{EATIS} '22},
	title = {Internet of {Things} under a semantic perspective with user profiles},
	isbn = {978-1-4503-9738-4},
	url = {https://doi.org/10.1145/3544538.3544668},
	doi = {10.1145/3544538.3544668},
	abstract = {Internet of Things (IoT) is a network made up of different types of devices integrated into the Internet. For the use of IoT in cities to assist humans beings in their daily activities, it is necessary to extract and process as much information as possible and therefore, semantic web technologies are a key point. This new environment is known as the Semantic Web of Things (SWoT). The application of semantic techniques to IoT can improve interoperability, effective access to data, discovery of integration resources, reasoning and processing of knowledge extraction from data, opening up opportunities for new applications. We show an overview of the most relevant semantic technologies, focusing on SWoT and well-accepted ontologies for IoT and we stress that considering users’ preferences, interests and needs are of vital importance, as a complement to the semantic information extracted. This new scenario enables the development of advanced applications and services to meet user requirements and needs, in particular for smart city applications.},
	booktitle = {Proceedings of the 11th {Euro} {American} {Conference} on {Telematics} and {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kourtiche, Ali and Felici-Castell, S. and Perez Solano, J. J. and Segura-Garcia, J. and Soriano-Asensi, A. and Navarro-Camba, E. and Pinto, J.},
	year = {2022},
	note = {event-place: Aveiro, Portugal},
	keywords = {ontology, semantic web, IoT, smart cities, web of things, user profiles, citizens, e-public},
}

@inproceedings{ragab_unlocking_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Unlocking the {Potential} of {Health} {Data} with {Decentralised} {Search} in {Personal} {Health} {Datastores}},
	isbn = {979-8-4007-0172-6},
	url = {https://doi.org/10.1145/3589335.3651454},
	doi = {10.1145/3589335.3651454},
	abstract = {In the digital age, where health data and digital lives converge, data privacy and control are crucial. The advent of AI and Large Language Models (LLMs) brings advanced data analysis and healthcare predictions, but also privacy concerns. The ESPRESSO project 1 asserts that for AI to be trustworthy and effective in healthcare, it must prioritize user control over corporate interests. The shift towards decentralized personal online datastores (pods) and Solid 2 principles represents a new era of private, controllable Web interactions, balancing AI data protection and machine intelligence. This balance is particularly important for applications involving health data. However, decentralization poses challenges, particularly in secure, efficient data search and data retrieval, that need to be addressed first. We argue that a decentralized search system that provides a large-scale search across Solid pods, while considering data owners' control of their data and users' different access rights, is crucial for this new paradigm. In this paper, we describe how our current decentralized search system's prototype (ESPRESSO) helps to query structured and unstructured personal health data in Solid servers. The paper also describes a search scenario that shows how ESPRESSO can search health data combined with fitness personal data stored in different personal datastores},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Ragab, Mohamed and Savateev, Yury and Oliver, Helen and Tiropanis, Thanassis and Poulovassilis, Alexandra and Chapman, Adriane and Roussos, George},
	year = {2024},
	note = {event-place: Singapore, Singapore},
	keywords = {linked data, decentralized web search, health and well-being data, personal online datastores, solid framework},
	pages = {1154--1157},
}

@inproceedings{wang_construction_2024,
	address = {New York, NY, USA},
	series = {{ICCSMT} '23},
	title = {Construction of {Gesar} {Epic} {Event} {Graph} {Based} on {Event} {Extraction}},
	isbn = {979-8-4007-0951-7},
	url = {https://doi.org/10.1145/3644523.3644601},
	doi = {10.1145/3644523.3644601},
	abstract = {The Biography of King Gesar is the largest heroic epic in world history and also one of the world's intangible cultural heritage sites. The Tibetan culture it carries is an important component of Chinese civilization. In order to better showcase and protect this intangible cultural heritage, and provide data support for research on digital retrieval, intelligent Q\&amp;A, and reading comprehension of Gesar cultural resources, the construction of Gesar event graph was studied. Firstly, the BTCNN event federation model is used to extract event trigger words and event elements from semi structured and unstructured event texts, assisting in manually customized event relationships, and completing the preliminary construction of the Gesar event graph. Based on the constructed event graph, a visualization system for the graph was built, and the graph retrieval function was designed and implemented, supporting entity query, relationship query, and entity relationship hybrid query.},
	booktitle = {Proceedings of the 2023 4th {International} {Conference} on {Computer} {Science} and {Management} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Kaijie and Wang, Tiejun and Lu, Ziling},
	year = {2024},
	note = {event-place: Xi'an, China},
	pages = {431--436},
}

@inproceedings{nurmikko-fuller_swinging_2018,
	address = {New York, NY, USA},
	series = {{SAAM} '18},
	title = {Swinging {Triples}: {Bridging} {Jazz} {Performance} {Datasets} using {Linked} {Data}},
	isbn = {978-1-4503-6495-9},
	url = {https://doi.org/10.1145/3243907.3243914},
	doi = {10.1145/3243907.3243914},
	abstract = {The jazz performance metadata prototype JazzCats:Jazz Collection of Aggregated Triples uses Linked Data to bridge four discrete jazz music datasets: Linked Jazz, with prosopographical and interpersonal information about musicians; the Weimar Jazz Database (WJazzD), containing musicological metadata; a discography of the jazz standard Body\&amp;Soul; and J-DISC, a fourth independent but complementary and extensive discographic project. Through the use of custom-built ontological structures the data, originally stored in various different information structures, has been converted to RDF and merged together in a single triplestore. The result is a new digital resource that can be used to support and enrich scholarship and research in musicology and performance studies.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Semantic} {Applications} for {Audio} and {Music}},
	publisher = {Association for Computing Machinery},
	author = {Nurmikko-Fuller, Terhi and Bangert, Daniel and Hao, Yun and Downie, J. Stephen},
	year = {2018},
	note = {event-place: Monterey, CA, USA},
	keywords = {SPARQL, ontologies, semantic web, metadata, Linked Data, performance, digital musicology, jazz},
	pages = {42--45},
}

@inproceedings{yousaf_how_2019,
	address = {New York, NY, USA},
	series = {{GIR} '19},
	title = {How to identify appropriate key-value pairs for querying {OSM}},
	isbn = {978-1-4503-7260-2},
	url = {https://doi.org/10.1145/3371140.3371147},
	doi = {10.1145/3371140.3371147},
	abstract = {This paper presents a study on how natural language words that designate types of spatial entities (metropolis, city, creek, etc.) can automatically be translated to the entity classification used in OpenStreetMap (OSM) that assigns key-value tags to entities. The problem of identifying key-value pairs for querying OSM occurs in geographic information retrieval based on natural language text and is difficult for three reasons: Conceptualisation of entities in natural language text and in OSM often differs. Even classification of a single entity type is subject to variations throughout the OSM database. Language is rich and offers many words to communicate nuances of a single entity type. The contribution of this paper is to analyse the contribution of semantic word similarity using Word-Net to identify a mapping from natural language to OSM tags. We present a strategy to identify key-value pairs for natural language words using WordNet and analyse its effectiveness.},
	booktitle = {Proceedings of the 13th {Workshop} on {Geographic} {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Yousaf, Madiha and Wolter, Diedrich},
	year = {2019},
	note = {event-place: Lyon, France},
	keywords = {geo-referencing, OpenStreetMap (OSM), semantics of spatial language},
}

@inproceedings{gavran_t_2018,
	address = {New York, NY, USA},
	series = {Onward! 2018},
	title = {Tᴏᴏʟ: accessible automated reasoning for human robot collaboration},
	isbn = {978-1-4503-6031-9},
	url = {https://doi.org/10.1145/3276954.3276961},
	doi = {10.1145/3276954.3276961},
	abstract = {We present an expressive, concise, and extendable domain specific language for planning of assembly systems, such as industrial human robot cooperation. Increased flexibility requirements in manufacturing processes call for more automation at the description and planning stages of manufacturing. Procedural models are good candidates to meet this demand as programs offer a high degree of flexibility and are easily composed. Furthermore, we aim to make our programs close to declarative specification and integrate automatic reasoning tools to help the users. The constraints come both from specific programs and preexisting knowledge base from the target domain. The case of human robot collaboration is interesting as there is a number of constraints and regulations around this domain. Unfortunately, automated reasoners are often too unpredictable and cannot be used directly by non-experts. In this paper, we present our domain specific language “Tool Ontology and Optimization Language” (Tool) and describe how we integrated automated reasoners and planners in a way that makes them accessible to users which have little programming knowledge, but expertise in manufacturing domain and no previous experience with or knowledge about the underlying reasoners. We present encouraging results by applying Tool to a case study from the automotive and aerospace industry.},
	booktitle = {Proceedings of the 2018 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {Association for Computing Machinery},
	author = {Gavran, Ivan and Mailahn, Ortwin and Müller, Rainer and Peifer, Richard and Zufferey, Damien},
	year = {2018},
	note = {event-place: Boston, MA, USA},
	keywords = {cyber-physical systems, assembly planning, industry 4.0, knowledge integration, domain specific language, automated reasoning, human-robot cooperation, robotics and automation},
	pages = {44--56},
}

@article{dumitrache_crowdsourcing_2018,
	title = {Crowdsourcing {Ground} {Truth} for {Medical} {Relation} {Extraction}},
	volume = {8},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/3152889},
	doi = {10.1145/3152889},
	abstract = {Cognitive computing systems require human labeled data for evaluation and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, which reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the cause and treat relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, which account for ambiguity in both human and machine performance on this task.},
	number = {2},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Dumitrache, Anca and Aroyo, Lora and Welty, Chris},
	month = jul,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {relation extraction, clinical natural language processing, crowd truth, crowdtruth, Ground truth, inter-annotator disagreement, natural language ambiguity},
}

@inproceedings{mandic_semantic_2018,
	address = {New York, NY, USA},
	series = {{WIMS} '18},
	title = {Semantic {Web} based software platform for curriculum harmonization},
	isbn = {978-1-4503-5489-9},
	url = {https://doi.org/10.1145/3227609.3227654},
	doi = {10.1145/3227609.3227654},
	abstract = {This paper presents a software platform for comparing informatics teacher education curricula. The ontological model of the chosen informatics teachers' curriculum from the Republic of Serbia and the reference informatics teachers' curriculum model were created. The semi-automatic software platform is based on the standard techniques and methods of ontology matching. The created ontological models of the teacher education curricula are compared using the developed software. Analysis of the results of comparison includes consideration of classes' matching, obtained system evaluation (by the expert team) and a harmonization of the Revised Bloom's taxonomy categories. Also, the obtained results are compared with the results obtained for other combinations of input ontological models (secondary informatics and informatics teacher education curricula models). The analysis of the results revealed the need to improve the content and structure of the observed model curricula as well as the limits of the developed system and possibilities of improving the software platform.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Web} {Intelligence}, {Mining} and {Semantics}},
	publisher = {Association for Computing Machinery},
	author = {Mandić, Milinko},
	year = {2018},
	note = {event-place: Novi Sad, Serbia},
	keywords = {ontology, informatics, alignment, matching, teacher education curriculum},
}

@article{steimann_containerless_2022,
	title = {Containerless {Plurals}: {Separating} {Number} from {Type} in {Object}-{Oriented} {Programming}},
	volume = {44},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/3527635},
	doi = {10.1145/3527635},
	abstract = {To let expressions evaluate to no or many objects, most object-oriented programming languages require the use of special constructs that encode these cases as single objects or values. While the requirement to treat these standard situations idiomatically seems to be broadly accepted, I argue that its alternative, letting expressions evaluate to any number of objects directly, has several advantages that make it worthy of consideration. As a proof of concept, I present a core object-oriented programming language, dubbed Num, which separates number from type so that the type of an expression is independent of the number of objects it may evaluate to, thus removing one major obstacle to using no, one, and many objects uniformly. Furthermore, Num abandons null references, replaces the nullability of reference types with the more general notion of countability, and allows methods to be invoked on any number of objects, including no object. To be able to adapt behavior to the actual number of receivers, Num complements instance methods with plural methods, that is, with methods that operate on a number of objects jointly and that replace static methods known from other languages. An implementation of Num in Prolog and accompanying type and number safety proofs are presented.},
	number = {4},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Steimann, Friedrich},
	month = sep,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {bunches, collections, Multiplicities in programming, null-safety, object-relational programming},
}

@inproceedings{katwe_evaluating_2025,
	address = {New York, NY, USA},
	series = {{FIRE} '24},
	title = {Evaluating {Relation} {Hallucination} in {Text} {Summarization}: {An} {Introduction} to the {Relation} {Hallucination} {Index}},
	isbn = {979-8-4007-1318-7},
	url = {https://doi.org/10.1145/3734947.3734958},
	doi = {10.1145/3734947.3734958},
	abstract = {Text summarization involves generating a concise, precise, and coherent summary that captures the essence of a longer document. Relation Hallucination refers to the generation of summary sentences that imply or state relationships between entities or events that were not present or implied in the original document. Relation Hallucination is a pivotal concern in abstractive text summarization, where models fabricate or exaggerate connections, leading to misleading summaries. This paper delves deep into the evaluation of Relation Hallucination in abstractive summarization. We introduce a novel metric, the Relation Hallucination Index, designed to evaluate and quantify Relation hallucinations across various state-of-the-art models. Emphasizing the paramount importance of maintaining accurate relational context in summaries, this article showcases the efficacy of the Relation hallucination index in discerning fabricated relationships. The Relation hallucination index provides a quantitative measure of the various levels of Relation Hallucination present in generated summaries, enabling practitioners from industry as well as Academia to select models aligned with desired hallucination parameters for their tailored applications.},
	booktitle = {Proceedings of the 16th {Annual} {Meeting} of the {Forum} for {Information} {Retrieval} {Evaluation}},
	publisher = {Association for Computing Machinery},
	author = {Katwe, Praveen Kumar and Balabantaray, Rakesh Chandra and Vittala, Kali Prasad},
	year = {2025},
	keywords = {Abstractive summarization, Hallucination, Extractiveness Factor, Lost Focus, Negative Hallucination, Over Focus, Positive Hallucination, Relation Tuples},
	pages = {88--94},
}

@inproceedings{chen_web_2021,
	address = {New York, NY, USA},
	series = {{PLDI} 2021},
	title = {Web question answering with neurosymbolic program synthesis},
	isbn = {978-1-4503-8391-2},
	url = {https://doi.org/10.1145/3453483.3454047},
	doi = {10.1145/3453483.3454047},
	abstract = {In this paper, we propose a new technique based on program synthesis for extracting information from webpages. Given a natural language query and a few labeled webpages, our method synthesizes a program that can be used to extract similar types of information from other unlabeled webpages. To handle websites with diverse structure, our approach employs a neurosymbolic DSL that incorporates both neural NLP models as well as standard language constructs for tree navigation and string manipulation. We also propose an optimal synthesis algorithm that generates all DSL programs that achieve optimal F1 score on the training examples. Our synthesis technique is compositional, prunes the search space by exploiting a monotonicity property of the DSL, and uses transductive learning to select programs with good generalization power. We have implemented these ideas in a new tool called WebQA and evaluate it on 25 different tasks across multiple domains. Our experiments show that WebQA significantly outperforms existing tools such as state-of-the-art question answering models and wrapper induction systems.},
	booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Qiaochu and Lamoreaux, Aaron and Wang, Xinyu and Durrett, Greg and Bastani, Osbert and Dillig, Isil},
	year = {2021},
	note = {event-place: Virtual, Canada},
	keywords = {Program Synthesis, Programming by Example, Web Information Extraction},
	pages = {328--343},
}

@inproceedings{wang_named_2024,
	address = {New York, NY, USA},
	series = {{CAICE} '24},
	title = {Named entity recognition method for mine electromechanical equipment field},
	isbn = {979-8-4007-1694-2},
	url = {https://doi.org/10.1145/3672758.3672868},
	doi = {10.1145/3672758.3672868},
	abstract = {Aiming at the lack of annotated corpus in the field of mine electromechanical equipment, the high similarity between different categories of entities and the long names of some entities, this paper proposed an entity extraction method of mine electromechanical equipment based on the fusion of technical words and comparative learning. Firstly, an ontology library was constructed according to the characteristics of mine electromechanical equipment, Word2Vec was used to obtain the vector representation of characters and words, and the term words were obtained by matching in the ontology library. The multi-term multi-head attention mechanism was used to assign larger weights to these term words and then they were fused with the character vector. Then, the Bi-LSTM model was used for feature extraction, and a contrastive learning strategy based on R-drop was used to reduce the recognition bias error caused by the similarity of entity names. We also improved the loss function by using the relative entropy loss calculated by the Bi-LSTM layer as the regularization term of the loss in the CRF layer to form a constraint on the loss function and enhance the robustness of the model to Dropout. Finally, we used the CRF model to decode to obtain the optimal label. Experimental results show that compared with the existing mainstream baseline method Lattice-LSTM, our proposed method achieves better results on the self-constructed mine electromechanical equipment corpus. The precision, recall and F1 value are improved by 4.73, 5.5 and 5.09 percentage points, respectively.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Computer}, {Artificial} {Intelligence} and {Control} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Yi Long and Wen, Ying Zi and Wang, Yao Hui and She, Xin Peng and Sun, Xiao Hu and Lyu, Xue Qiang and Hao, Qiang},
	year = {2024},
	note = {event-place: Xi' an, China},
	pages = {660--664},
}

@inproceedings{rabut_multi-class_2019,
	address = {New York, NY, USA},
	series = {{ICCBD} '19},
	title = {Multi-class {Document} {Classification} {Using} {Improved} {Word} {Embeddings}},
	isbn = {978-1-4503-7290-9},
	url = {https://doi.org/10.1145/3366650.3366661},
	doi = {10.1145/3366650.3366661},
	abstract = {In this paper, we conducted an experiment to build a classification model that combines different techniques in most of the Natural Language Processing Tasks. We used the word embedding method to transform every word in the dataset and to obtain the custom-built word embedding vectors. This is in contrast to the approaches in the previous literature that implement word embedding using the pre-trained word embedding vectors. We enriched the custom-built word embedding vectors by incorporating Part-of-Speech (POS) tag vectors to provide additional semantic information about the word to be used in training our proposed classification model. The proposed model was built using the neural network approach, which is considered to be more efficient and reliable in solving real problems for document classification tasks. We fine-tuned the parameters during the training of our neural network classification model with our aim to increase the performance in terms of classification accuracy. The experimental result demonstrates that our model performs remarkably well and increase the percentage accuracy up to 1.7\% compared to the accuracy results obtained by the previous baseline word embedding methods using the same dataset. It was also observed that our model outperforms some other traditional classification models implemented using different techniques and machine learning algorithms.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Computing} and {Big} {Data}},
	publisher = {Association for Computing Machinery},
	author = {Rabut, Benedict A. and Fajardo, Arnel C. and Medina, Ruji P.},
	year = {2019},
	note = {event-place: Taichung, Taiwan},
	keywords = {Natural Language Processing, Word Embeddings, Document Classification},
	pages = {42--46},
}

@inproceedings{wilsdorf_validation_2024,
	address = {San Antonio, Texas, USA},
	series = {{WSC} '23},
	title = {Validation {Without} {Data} - {Formalizing} {Stylized} {Facts} of {Time} {Series}},
	isbn = {979-8-3503-6966-3},
	abstract = {A stylized fact is a simplified presentation of an empirical finding. When modeling and simulating complex systems and real data are sparse, stylized facts have become a key instrument for building trust in a model as they represent important requirements regarding the model's behavior. However, automatically validating stylized facts has remained limited as they are usually expressed in natural language. Therefore, we develop a formal language with a custom syntax and tailored predicates allowing modelers to unambiguously and succinctly describe important (temporal) characteristics of simulation traces or relationships between multiple traces via statistical tests. The proposed formal language is able to express numerous facts from the literature in different application domains, as well as to automatically check stylized facts. If stylized facts are defined at the beginning of a simulation study, formally expressing and checking them can streamline and guide the development of simulation models and their successive revisions.},
	booktitle = {Proceedings of the {Winter} {Simulation} {Conference}},
	publisher = {IEEE Press},
	author = {Wilsdorf, Pia and Zuska, Marian and Andelfinger, Philipp and Uhrmacher, Adelinde M. and Peters, Florian},
	year = {2024},
	pages = {2674--2685},
}

@inproceedings{fiallos_semi-automatic_2019,
	address = {New York, NY, USA},
	series = {{LAK19}},
	title = {Semi-{Automatic} {Generation} of {Intelligent} {Curricula} to {Facilitate} {Learning} {Analytics}},
	isbn = {978-1-4503-6256-6},
	url = {https://doi.org/10.1145/3303772.3303834},
	doi = {10.1145/3303772.3303834},
	abstract = {Several Learning Analytics applications are limited by the cost of generating a computer understandable description of the course domain, what is called an Intelligent Curriculum. The following work contributes a novel approach to (semi-)automatically generate Intelligent Curriculum through ontologies extracted from existing learning materials such as digital books or web content. Through a series of natural language processing steps, the semi-structured information present in existing content is transformed into a concept-graph. This work also evaluates the proposed methodology by applying it to learning content for two different courses and measuring the quality of the extracted ontologies against manually generated ones. The results obtained suggest that the technique can be readily used to provide domain information to other Learning Analytics tools.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Learning} {Analytics} \&amp; {Knowledge}},
	publisher = {Association for Computing Machinery},
	author = {Fiallos, Angel and Ochoa, Xavier},
	year = {2019},
	note = {event-place: Tempe, AZ, USA},
	keywords = {ontologies, NLP, intelligent curriculum},
	pages = {46--50},
}

@inproceedings{jokinen_towards_2025,
	address = {Melbourne, Australia},
	series = {{HRI} '25},
	title = {Towards {Domain} {Graphs} and {Dialogue} {Graphs} for {Conversational} {Grounding} in {HRI}},
	abstract = {Knowledge graphs have been used to improve robot dialogues by providing more sophisticated world knowledge. We now propose a new role for knowledge graphs in GenAI-based HRI that aims to reduce dialogue errors by better conversational grounding. This approach uses both domain knowledge graphs and dialogue history graphs, constructing shared knowledge via entity linking. We present first steps towards these aims, and also address sustainability by supporting the use of smaller models.},
	booktitle = {Proceedings of the 2025 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {IEEE Press},
	author = {Jokinen, Kristiina and Wilcock, Graham},
	year = {2025},
	keywords = {knowledge graphs, sustainability, conversational grounding, human-robot dialogues},
	pages = {1373--1377},
}

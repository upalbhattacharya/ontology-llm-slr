
@inproceedings{gritz_towards_2018,
	address = {Sofia, Bulgaria},
	title = {Towards {Lexical} {Meaning} {Formal} {Representation} by virtue of the {NL}-{DL} {Definition} {Transformation} {Method}},
	url = {https://aclanthology.org/2018.clib-1.5/},
	abstract = {The paper represents a part of an extensive study devoted to the issues of lexical meaning formal representation in OWL 2 DL notation. Both theoretical and methodological aspects of lexical meaning formalization within the framework of an ontology are observed in the paper. Model-theoretic semantics paradigm and Kripke model are considered to form a theoretical background for formalization of lexical meaning, whereas the NL-DL definition transformation method is investigated as a method designed to provide us with acceptable formal definitions in OWL 2 DL notation with natural language definitions given at the input. A brief critical study of the method has allowed to reveal particular problematic cases of the method application, which arise due to syntactic peculiarities of natural language definitions given at the input.},
	booktitle = {Proceedings of the {Third} {International} {Conference} on {Computational} {Linguistics} in {Bulgaria} ({CLIB} 2018)},
	publisher = {Department of Computational Linguistics, Institute for Bulgarian Language, Bulgarian Academy of Sciences},
	author = {Gritz, Maria},
	month = may,
	year = {2018},
	pages = {23--33},
}

@inproceedings{maziarz_towards_2018,
	address = {Nanyang Technological University (NTU), Singapore},
	title = {Towards {Mapping} {Thesauri} onto {plWordNet}},
	url = {https://aclanthology.org/2018.gwc-1.6/},
	abstract = {plWordNet, the wordnet of Polish, has become a very comprehensive description of the Polish lexical system. This paper presents a plan of its semi-automated integration with thesauri, terminological databases and ontologies, as a further necessary step in its development. This will improve linking of plWordNet into Linked Open Data, and facilitate applications in, e.g., WSD, keyword extraction or automated metadata generation. We present an overview of resources relevant to Polish and a plan for their linking to plWordNet.},
	booktitle = {Proceedings of the 9th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Maziarz, Marek and Piasecki, Maciej},
	editor = {Bond, Francis and Vossen, Piek and Fellbaum, Christiane},
	month = jan,
	year = {2018},
	pages = {44--52},
}

@inproceedings{pease_toward_2018,
	address = {Nanyang Technological University (NTU), Singapore},
	title = {Toward a {Semantic} {Concordancer}},
	url = {https://aclanthology.org/2018.gwc-1.12/},
	abstract = {Concordancers are an accepted and valuable part of the tool set of linguists and lexicographers. They allow the user to see the context of use of a word or phrase in a corpus. A large enough corpus, such as the Corpus Of Contemporary American English, provides the data needed to enumerate all common uses or meanings. One challenge is that there may be too many results for short search phrases or common words when only a specific context is desired. However, finding meaningful groupings of usage may be impractical if it entails enumerating long lists of possible values, such as city names. If a tool existed that could create some semantic abstractions, it would free the lexicographer from the need to resort to customized development of analysis software. To address this need, we have developed a Semantic Concordancer that uses dependency parsing and the Suggested Upper Merged Ontology (SUMO) to support linguistic analysis at a level of semantic abstraction above the original textual elements. We show how this facility can be employed to analyze the use of English prepositions by non-native speakers. We briefly introduce condordancers and then describe the corpora on which we applied this work. Next we provide a detailed description of the NLP pipeline followed by how this captures detailed semantics. We show how the semantics can be used to analyze errors in the use of English prepositions by non-native speakers of English. Then we provide a description of a tool that allows users to build semantic search specifications from a set of English examples and how those results can be employed to build rules that translate sentences into logical forms. Finally, we summarize our conclusions and mention future work.},
	booktitle = {Proceedings of the 9th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Pease, Adam and Cheung, Andrew},
	editor = {Bond, Francis and Vossen, Piek and Fellbaum, Christiane},
	month = jan,
	year = {2018},
	pages = {97--104},
}

@inproceedings{muniz_extending_2018,
	address = {Nanyang Technological University (NTU), Singapore},
	title = {Extending {Wordnet} to {Geological} {Times}},
	url = {https://aclanthology.org/2018.gwc-1.17/},
	abstract = {This paper describes work extending Princeton WordNet to the domain of geological texts, associated with the time periods of the geological eras of the Earth History. We intend this extension to be considered as an example for any other domain extension that we might want to pursue. To provide this extension, we first produce a textual version of Princeton WordNet. Then we map a fragment of the International Commission on Stratigraphy (ICS) ontologies to WordNet and create the appropriate new synsets. We check the extended ontology on a small corpus of sentences from Gas and Oil technical reports and realize that more work needs to be done, as we need new words, new senses and new compounds in our extended WordNet.},
	booktitle = {Proceedings of the 9th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Muniz, Henrique and Chalub, Fabricio and Rademaker, Alexandre and Paiva, Valeria De},
	editor = {Bond, Francis and Vossen, Piek and Fellbaum, Christiane},
	month = jan,
	year = {2018},
	pages = {145--152},
}

@inproceedings{pedersen_towards_2018,
	address = {Nanyang Technological University (NTU), Singapore},
	title = {Towards a principled approach to sense clustering – a case study of wordnet and dictionary senses in {Danish}},
	url = {https://aclanthology.org/2018.gwc-1.21/},
	abstract = {Our aim is to develop principled methods for sense clustering which can make existing lexical resources practically useful in NLP – not too fine-grained to be operational and yet finegrained enough to be worth the trouble. Where traditional dictionaries have a highly structured sense inventory typically describing the vocabulary by means of mainand subsenses, wordnets are generally fine-grained and unstructured. We present a series of clustering and annotation experiments with 10 of the most polysemous nouns in Danish. We combine the structured information of a traditional Danish dictionary with the ontological types found in the Danish wordnet, DanNet. This constellation enables us to automatically cluster senses in a principled way and improve inter-annotator agreement and wsd performance.},
	booktitle = {Proceedings of the 9th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Pedersen, Bolette and Agirrezabal, Manex and Nimb, Sanni and Olsen, Ida and Olsen, Sussi},
	editor = {Bond, Francis and Vossen, Piek and Fellbaum, Christiane},
	month = jan,
	year = {2018},
	pages = {182--189},
}

@inproceedings{campagna_zero-shot_2020,
	address = {Online},
	title = {Zero-{Shot} {Transfer} {Learning} with {Synthesized} {Data} for {Multi}-{Domain} {Dialogue} {State} {Tracking}},
	url = {https://aclanthology.org/2020.acl-main.12/},
	doi = {10.18653/v1/2020.acl-main.12},
	abstract = {Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21\%.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Campagna, Giovanni and Foryciarz, Agata and Moradshahi, Mehrad and Lam, Monica},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {122--132},
}

@inproceedings{sotudeh_gharebagh_attend_2020,
	address = {Online},
	title = {Attend to {Medical} {Ontologies}: {Content} {Selection} for {Clinical} {Abstractive} {Summarization}},
	url = {https://aclanthology.org/2020.acl-main.172/},
	doi = {10.18653/v1/2020.acl-main.172},
	abstract = {Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9\% RG-1, 2.5\% RG-2, 1.9\% RG-L), in the healthcare domain where any range of improvement impacts patients' welfare.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sotudeh Gharebagh, Sajad and Goharian, Nazli and Filice, Ross},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {1899--1905},
}

@inproceedings{xu_generate-and-rank_2020,
	address = {Online},
	title = {A {Generate}-and-{Rank} {Framework} with {Semantic} {Type} {Regularization} for {Biomedical} {Concept} {Normalization}},
	url = {https://aclanthology.org/2020.acl-main.748/},
	doi = {10.18653/v1/2020.acl-main.748},
	abstract = {Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Dongfang and Zhang, Zeyu and Bethard, Steven},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {8452--8464},
}

@inproceedings{schumacher_clinical_2020,
	address = {Online},
	title = {Clinical {Concept} {Linking} with {Contextualized} {Neural} {Representations}},
	url = {https://aclanthology.org/2020.acl-main.760/},
	doi = {10.18653/v1/2020.acl-main.760},
	abstract = {In traditional approaches to entity linking, linking decisions are based on three sources of information – the similarity of the mention string to an entity's name, the similarity of the context of the document to the entity, and broader information about the knowledge base (KB). In some domains, there is little contextual information present in the KB and thus we rely more heavily on mention string similarity. We consider one example of this, concept linking, which seeks to link mentions of medical concepts to a medical concept ontology. We propose an approach to concept linking that leverages recent work in contextualized neural models, such as ELMo (Peters et al. 2018), which create a token representation that integrates the surrounding context of the mention and concept name. We find a neural ranking approach paired with contextualized embeddings provides gains over a competitive baseline (Leaman et al. 2013). Additionally, we find that a pre-training step using synonyms from the ontology offers a useful initialization for the ranker.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Schumacher, Elliot and Mulyar, Andriy and Dredze, Mark},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {8585--8592},
}

@inproceedings{li_gaia_2020,
	address = {Online},
	title = {{GAIA}: {A} {Fine}-grained {Multimedia} {Knowledge} {Extraction} {System}},
	url = {https://aclanthology.org/2020.acl-demos.11/},
	doi = {10.18653/v1/2020.acl-demos.11},
	abstract = {We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation. The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Manling and Zareian, Alireza and Lin, Ying and Pan, Xiaoman and Whitehead, Spencer and Chen, Brian and Wu, Bo and Ji, Heng and Chang, Shih-Fu and Voss, Clare and Napierski, Daniel and Freedman, Marjorie},
	editor = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
	month = jul,
	year = {2020},
	pages = {77--86},
}

@inproceedings{dong_multi-modal_2020,
	address = {Online},
	title = {Multi-modal {Information} {Extraction} from {Text}, {Semi}-structured, and {Tabular} {Data} on the {Web}},
	url = {https://aclanthology.org/2020.acl-tutorials.6/},
	doi = {10.18653/v1/2020.acl-tutorials.6},
	abstract = {The World Wide Web contains vast quantities of textual information in several forms: unstructured text, template-based semi-structured webpages (which present data in key-value pairs and lists), and tables. Methods for extracting information from these sources and converting it to a structured form have been a target of research from the natural language processing (NLP), data mining, and database communities. While these researchers have largely separated extraction from web data into different problems based on the modality of the data, they have faced similar problems such as learning with limited labeled data, defining (or avoiding defining) ontologies, making use of prior knowledge, and scaling solutions to deal with the size of the Web. In this tutorial we take a holistic view toward information extraction, exploring the commonalities in the challenges and solutions developed to address these different forms of text. We will explore the approaches targeted at unstructured text that largely rely on learning syntactic or semantic textual patterns, approaches targeted at semi-structured documents that learn to identify structural patterns in the template, and approaches targeting web tables which rely heavily on entity linking and type information. While these different data modalities have largely been considered separately in the past, recent research has started taking a more inclusive approach toward textual extraction, in which the multiple signals offered by textual, layout, and visual clues are combined into a single extraction model made possible by new deep learning approaches. At the same time, trends within purely textual extraction have shifted toward full-document understanding rather than considering sentences as independent units. With this in mind, it is worth considering the information extraction problem as a whole to motivate solutions that harness textual semantics along with visual and semi-structured layout information. We will discuss these approaches and suggest avenues for future work.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Tutorial} {Abstracts}},
	publisher = {Association for Computational Linguistics},
	author = {Dong, Xin Luna and Hajishirzi, Hannaneh and Lockard, Colin and Shiralkar, Prashant},
	editor = {Savary, Agata and Zhang, Yue},
	month = jul,
	year = {2020},
	pages = {23--26},
}

@inproceedings{arnold_enriching_2020,
	address = {Marseille, France},
	title = {Enriching {Historic} {Photography} with {Structured} {Data} using {Image} {Region} {Segmentation}},
	isbn = {979-10-95546-63-4},
	url = {https://aclanthology.org/2020.ai4hi-1.1/},
	abstract = {Cultural institutions such as galleries, libraries, archives and museums continue to make commitments to large scale digitization of collections. An ongoing challenge is how to increase discovery and access through structured data and the semantic web. In this paper we describe a method for using computer vision algorithms that automatically detect regions of “stuff” — such as the sky, water, and roads — to produce rich and accurate structured data triples for describing the content of historic photography. We apply our method to a collection of 1610 documentary photographs produced in the 1930s and 1940 by the FSA-OWI division of the U.S. federal government. Manual verification of the extracted annotations yields an accuracy rate of 97.5\%, compared to 70.7\% for relations extracted from object detection and 31.5\% for automatically generated captions. Our method also produces a rich set of features, providing more unique labels (1170) than either the captions (1040) or object detection (178) methods. We conclude by describing directions for a linguistically-focused ontology of region categories that can better enrich historical image data. Open source code and the extracted metadata from our corpus are made available as external resources.},
	language = {eng},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Artificial} {Intelligence} for {Historical} {Image} {Enrichment} and {Access}},
	publisher = {European Language Resources Association (ELRA)},
	author = {Arnold, Taylor and Tilton, Lauren},
	editor = {Abgaz, Yalemisew and Dorn, Amelie and Diaz, Jose Luis Preza and Koch, Gerda},
	month = may,
	year = {2020},
	pages = {1--10},
}

@inproceedings{breit_interlinking_2020,
	address = {Marseille, France},
	title = {Interlinking {Iconclass} {Data} with {Concepts} of {Art} \& {Architecture} {Thesaurus}},
	isbn = {979-10-95546-63-4},
	url = {https://aclanthology.org/2020.ai4hi-1.2/},
	abstract = {Iconclass, being a a well established classification system, could benefit from interconnections with other ontologies in order to semantically enrich its content. This work presents a disambiguating and interlinking approach which is used to map Iconclass Subjects to concepts of the Art and Architecture Thesaurus. In a preliminary evaluation, the system is able to produce promising predictions, though the task is highly challenging due to conceptual and schema heterogeneity. Several algorithmic improvements for this specific interlinking task, as well as and future research directions are suggestions. The produced mappings, as well as the source code and additional information can be found at https://github.com/annabreit/taxonomy-interlinking.},
	language = {eng},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Artificial} {Intelligence} for {Historical} {Image} {Enrichment} and {Access}},
	publisher = {European Language Resources Association (ELRA)},
	author = {Breit, Anna},
	editor = {Abgaz, Yalemisew and Dorn, Amelie and Diaz, Jose Luis Preza and Koch, Gerda},
	month = may,
	year = {2020},
	pages = {11--15},
}

@inproceedings{roberto_toward_2020,
	address = {Marseille, France},
	title = {Toward the {Automatic} {Retrieval} and {Annotation} of {Outsider} {Art} images: {A} {Preliminary} {Statement}},
	isbn = {979-10-95546-63-4},
	url = {https://aclanthology.org/2020.ai4hi-1.3/},
	abstract = {The aim of this position paper is to establish an initial approach to the automatic classification of digital images about the Outsider Art style of painting. Specifically, we explore whether is it possible to classify non-traditional artistic styles by using the same features that are used for classifying traditional styles? Our research question is motivated by two facts. First, art historians state that non-traditional styles are influenced by factors “outside” of the world of art. Second, some studies have shown that several artistic styles confound certain classification techniques. Following current approaches to style prediction, this paper utilises Deep Learning methods to encode image features. Our preliminary experiments have provided motivation to think that, as is the case with traditional styles, Outsider Art can be computationally modelled with objective means by using training datasets and CNN models. Nevertheless, our results are not conclusive due to the lack of a large available dataset on Outsider Art. Therefore, at the end of the paper, we have mapped future lines of action, which include the compilation of a large dataset of Outsider Art images and the creation of an ontology of Outsider Art.},
	language = {eng},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Artificial} {Intelligence} for {Historical} {Image} {Enrichment} and {Access}},
	publisher = {European Language Resources Association (ELRA)},
	author = {Roberto, John and Ortego, Diego and Davis, Brian},
	editor = {Abgaz, Yalemisew and Dorn, Amelie and Diaz, Jose Luis Preza and Koch, Gerda},
	month = may,
	year = {2020},
	pages = {16--22},
}

@inproceedings{das_sequence--set_2020,
	address = {Online},
	title = {Sequence-to-{Set} {Semantic} {Tagging} for {Complex} {Query} {Reformulation} and {Automated} {Text} {Categorization} in {Biomedical} {IR} using {Self}-{Attention}},
	url = {https://aclanthology.org/2020.bionlp-1.2/},
	doi = {10.18653/v1/2020.bionlp-1.2},
	abstract = {Novel contexts, comprising a set of terms referring to one or more concepts, may often arise in complex querying scenarios such as in evidence-based medicine (EBM) involving biomedical literature. These may not explicitly refer to entities or canonical concept forms occurring in a fact-based knowledge source, e.g. the UMLS ontology. Moreover, hidden associations between related concepts meaningful in the current context, may not exist within a single document, but across documents in the collection. Predicting semantic concept tags of documents can therefore serve to associate documents related in unseen contexts, or categorize them, in information filtering or retrieval scenarios. Thus, inspired by the success of sequence-to-sequence neural models, we develop a novel sequence-to-set framework with attention, for learning document representations in a unique unsupervised setting, using no human-annotated document labels or external knowledge resources and only corpus-derived term statistics to drive the training, that can effect term transfer within a corpus for semantically tagging a large collection of documents. Our sequence-to-set modeling approach to predict semantic tags, gives to the best of our knowledge, the state-of-the-art for both, an unsupervised query expansion (QE) task for the TREC CDS 2016 challenge dataset when evaluated on an Okapi BM25–based document retrieval system; and also over the MLTM system baseline baseline (Soleimani and Miller, 2016), for both supervised and semi-supervised multi-label prediction tasks on the del.icio.us and Ohsumed datasets. We make our code and data publicly available.},
	booktitle = {Proceedings of the 19th {SIGBioMed} {Workshop} on {Biomedical} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Das, Manirupa and Li, Juanxi and Fosler-Lussier, Eric and Lin, Simon and Rust, Steve and Huang, Yungui and Ramnath, Rajiv},
	editor = {Demner-Fushman, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
	month = jul,
	year = {2020},
	pages = {14--27},
}

@inproceedings{chang_benchmark_2020,
	address = {Online},
	title = {Benchmark and {Best} {Practices} for {Biomedical} {Knowledge} {Graph} {Embeddings}},
	url = {https://aclanthology.org/2020.bionlp-1.18/},
	doi = {10.18653/v1/2020.bionlp-1.18},
	abstract = {Much of biomedical and healthcare data is encoded in discrete, symbolic form such as text and medical codes. There is a wealth of expert-curated biomedical domain knowledge stored in knowledge bases and ontologies, but the lack of reliable methods for learning knowledge representation has limited their usefulness in machine learning applications. While text-based representation learning has significantly improved in recent years through advances in natural language processing, attempts to learn biomedical concept embeddings so far have been lacking. A recent family of models called knowledge graph embeddings have shown promising results on general domain knowledge graphs, and we explore their capabilities in the biomedical domain. We train several state-of-the-art knowledge graph embedding models on the SNOMED-CT knowledge graph, provide a benchmark with comparison to existing methods and in-depth discussion on best practices, and make a case for the importance of leveraging the multi-relational nature of knowledge graphs for learning biomedical knowledge representation. The embeddings, code, and materials will be made available to the community.},
	booktitle = {Proceedings of the 19th {SIGBioMed} {Workshop} on {Biomedical} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Chang, David and Balažević, Ivana and Allen, Carl and Chawla, Daniel and Brandt, Cynthia and Taylor, Andrew},
	editor = {Demner-Fushman, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
	month = jul,
	year = {2020},
	pages = {167--176},
}

@inproceedings{peng_automatic_2020,
	address = {Online},
	title = {Automatic recognition of abdominal lymph nodes from clinical text},
	url = {https://aclanthology.org/2020.clinicalnlp-1.12/},
	doi = {10.18653/v1/2020.clinicalnlp-1.12},
	abstract = {Lymph node status plays a pivotal role in the treatment of cancer. The extraction of lymph nodes from radiology text reports enables large-scale training of lymph node detection on MRI. In this work, we first propose an ontology of 41 types of abdominal lymph nodes with a hierarchical relationship. We then introduce an end-to-end approach based on the combination of rules and transformer-based methods to detect these abdominal lymph node mentions and classify their types from the MRI radiology reports. We demonstrate the superior performance of a model fine-tuned on MRI reports using BlueBERT, called MriBERT. We find that MriBERT outperforms the rule-based labeler (0.957 vs 0.644 in micro weighted F1-score) as well as other BERT-based variations (0.913 - 0.928). We make the code and MriBERT publicly available at https://github.com/ncbi-nlp/bluebert, with the hope that this method can facilitate the development of medical report annotators to produce labels from scratch at scale.},
	booktitle = {Proceedings of the 3rd {Clinical} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Peng, Yifan and Lee, Sungwon and Elton, Daniel C. and Shen, Thomas and Tang, Yu-xing and Chen, Qingyu and Wang, Shuai and Zhu, Yingying and Summers, Ronald and Lu, Zhiyong},
	editor = {Rumshisky, Anna and Roberts, Kirk and Bethard, Steven and Naumann, Tristan},
	month = nov,
	year = {2020},
	pages = {101--110},
}

@inproceedings{sen_schema_2020,
	address = {Barcelona, Spain (Online)},
	title = {Schema {Aware} {Semantic} {Reasoning} for {Interpreting} {Natural} {Language} {Queries} in {Enterprise} {Settings}},
	url = {https://aclanthology.org/2020.coling-main.115/},
	doi = {10.18653/v1/2020.coling-main.115},
	abstract = {Natural Language Query interfaces allow the end-users to access the desired information without the need to know any specialized query language, data storage, or schema details. Even with the recent advances in NLP research space, the state-of-the-art QA systems fall short of understanding implicit intents of real-world Business Intelligence (BI) queries in enterprise systems, since Natural Language Understanding still remains an AI-hard problem. We posit that deploying ontology reasoning over domain semantics can help in achieving better natural language understanding for QA systems. In this paper, we specifically focus on building a Schema Aware Semantic Reasoning Framework that translates natural language interpretation as a sequence of solvable tasks by an ontology reasoner. We apply our framework on top of an ontology based, state-of-the-art natural language question-answering system ATHENA, and experiment with 4 benchmarks focused on BI queries. Our experimental numbers empirically show that the Schema Aware Semantic Reasoning indeed helps in achieving significantly better results for handling BI queries with an average accuracy improvement of {\textbackslash}textasciitilde30\%},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Sen, Jaydeep and Babtiwale, Tanaya and Saxena, Kanishk and Butala, Yash and Bhatia, Sumit and Sankaranarayanan, Karthik},
	editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
	month = dec,
	year = {2020},
	pages = {1334--1345},
}

@inproceedings{wan_modality_2020,
	address = {Barcelona, Spain (Online)},
	title = {Modality {Enriched} {Neural} {Network} for {Metaphor} {Detection}},
	url = {https://aclanthology.org/2020.coling-main.270/},
	doi = {10.18653/v1/2020.coling-main.270},
	abstract = {Metaphor as a cognitive mechanism in human's conceptual system manifests itself an effective way for language communication. Although being intuitively sensible for human, metaphor detection is still a challenging task due to the subtle ontological differences between metaphorical and non-metaphorical expressions. This work proposes a modality enriched deep learning model for tackling this unsolved issue. It provides a new perspective for understanding metaphor as a modality shift, as in `sweet voice'. It also attempts to enhance metaphor detection by combining deep learning with effective linguistic insight. Extending the work at Wan et al. (2020), we concatenate word sensorimotor scores (Lynott et al., 2019) with word vectors as the input of attention-based Bi-LSTM using a benchmark dataset–the VUA corpus. The experimental results show great F1 improvement (above 0.5\%) of the proposed model over other methods in record, demonstrating the usefulness of leveraging modality norms for metaphor detection.},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Wan, Mingyu and Xing, Baixi},
	editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
	month = dec,
	year = {2020},
	pages = {3036--3042},
}

@inproceedings{betti_expert_2020,
	address = {Barcelona, Spain (Online)},
	title = {Expert {Concept}-{Modeling} {Ground} {Truth} {Construction} for {Word} {Embeddings} {Evaluation} in {Concept}-{Focused} {Domains}},
	url = {https://aclanthology.org/2020.coling-main.586/},
	doi = {10.18653/v1/2020.coling-main.586},
	abstract = {We present a novel, domain expert-controlled, replicable procedure for the construction of concept-modeling ground truths with the aim of evaluating the application of word embeddings. In particular, our method is designed to evaluate the application of word and paragraph embeddings in concept-focused textual domains, where a generic ontology does not provide enough information. We illustrate the procedure, and validate it by describing the construction of an expert ground truth, QuiNE-GT. QuiNE-GT is built to answer research questions concerning the concept of naturalized epistemology in QUINE, a 2-million-token, single-author, 20th-century English philosophy corpus of outstanding quality, cleaned up and enriched for the purpose. To the best of our ken, expert concept-modeling ground truths are extremely rare in current literature, nor has the theoretical methodology behind their construction ever been explicitly conceptualised and properly systematised. Expert-controlled concept-modeling ground truths are however essential to allow proper evaluation of word embeddings techniques, and increase their trustworthiness in specialised domains in which the detection of concepts through their expression in texts is important. We highlight challenges, requirements, and prospects for future work.},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Betti, Arianna and Reynaert, Martin and Ossenkoppele, Thijs and Oortwijn, Yvette and Salway, Andrew and Bloem, Jelke},
	editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
	month = dec,
	year = {2020},
	pages = {6690--6702},
}

@inproceedings{giacomini_computational_2020,
	address = {Marseille, France},
	title = {Computational {Aspects} of {Frame}-based {Meaning} {Representation} in {Terminology}},
	isbn = {979-10-95546-57-3},
	url = {https://aclanthology.org/2020.computerm-1.11/},
	abstract = {Our contribution is part of a wider research project on term variation in German and concentrates on the computational aspects of a frame-based model for term meaning representation in the technical field. We focus on the role of frames (in the sense of Frame-Based Terminology) as the semantic interface between concepts covered by a domain ontology and domain-specific terminology. In particular, we describe methods for performing frame-based corpus annotation and frame-based term extraction. The aim of the contribution is to discuss the capacity of the model to automatically acquire semantic knowledge suitable for terminographic information tools such as specialised dictionaries, and its applicability to further specialised languages.},
	language = {eng},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Computational} {Terminology}},
	publisher = {European Language Resources Association},
	author = {Giacomini, Laura and Schäfer, Johannes},
	editor = {Daille, Béatrice and Kageura, Kyo and Terryn, Ayla Rigouts},
	month = may,
	year = {2020},
	pages = {80--84},
}

@inproceedings{wang_slot_2020,
	address = {Online},
	title = {Slot {Attention} with {Value} {Normalization} for {Multi}-{Domain} {Dialogue} {State} {Tracking}},
	url = {https://aclanthology.org/2020.emnlp-main.243/},
	doi = {10.18653/v1/2020.emnlp-main.243},
	abstract = {Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of ontology, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52\% on MultiWOZ 2.0 and 54.86\% on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30\% ontology is used, VN can also contribute to our model.},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yexiang and Guo, Yi and Zhu, Siqi},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {3019--3028},
}

@inproceedings{epure_modeling_2020,
	address = {Online},
	title = {Modeling the {Music} {Genre} {Perception} across {Language}-{Bound} {Cultures}},
	url = {https://aclanthology.org/2020.emnlp-main.386/},
	doi = {10.18653/v1/2020.emnlp-main.386},
	abstract = {The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we study the feasibility of obtaining relevant cross-lingual, culture-specific music genre annotations based only on language-specific semantic representations, namely distributed concept embeddings and ontologies. Our study, focused on six languages, shows that unsupervised cross-lingual music genre annotation is feasible with high accuracy, especially when combining both types of representations. This approach of studying music genres is the most extensive to date and has many implications in musicology and music information retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus to benchmark state of the art multilingual pre-trained embedding models.},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Epure, Elena V. and Salha, Guillaume and Moussallam, Manuel and Hennequin, Romain},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {4765--4779},
}

@inproceedings{moradshahi_localizing_2020,
	address = {Online},
	title = {Localizing {Open}-{Ontology} {QA} {Semantic} {Parsers} in a {Day} {Using} {Machine} {Translation}},
	url = {https://aclanthology.org/2020.emnlp-main.481/},
	doi = {10.18653/v1/2020.emnlp-main.481},
	abstract = {We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators. We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our model achieves an overall test accuracy ranging between 61\% and 69\% for the hotels domain and between 64\% and 78\% for restaurants domain, which compares favorably to 69\% and 80\% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30\% for hotels and 40\% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours. Our code is released open-source.},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Moradshahi, Mehrad and Campagna, Giovanni and Semnani, Sina and Xu, Silei and Lam, Monica},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {5970--5983},
}

@inproceedings{michael_asking_2020,
	address = {Online},
	title = {Asking without {Telling}: {Exploring} {Latent} {Ontologies} in {Contextual} {Representations}},
	url = {https://aclanthology.org/2020.emnlp-main.552/},
	doi = {10.18653/v1/2020.emnlp-main.552},
	abstract = {The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Michael, Julian and Botha, Jan A. and Tenney, Ian},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {6792--6812},
}

@inproceedings{joshi_dr_2020,
	address = {Online},
	title = {Dr. {Summarize}: {Global} {Summarization} of {Medical} {Dialogue} by {Exploiting} {Local} {Structures}.},
	url = {https://aclanthology.org/2020.findings-emnlp.335/},
	doi = {10.18653/v1/2020.findings-emnlp.335},
	abstract = {Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice: capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient's medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The model also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80\% of the conversations making it a realistic alternative to costly manual summarization by medical experts.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Anirudh and Katariya, Namit and Amatriain, Xavier and Kannan, Anitha},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {3755--3763},
}

@inproceedings{zhang_automatic_2020,
	address = {Online},
	title = {Automatic {Term} {Name} {Generation} for {Gene} {Ontology}: {Task} and {Dataset}},
	url = {https://aclanthology.org/2020.findings-emnlp.422/},
	doi = {10.18653/v1/2020.findings-emnlp.422},
	abstract = {Terms contained in Gene Ontology (GO) have been widely used in biology and bio-medicine. Most previous research focuses on inferring new GO terms, while the term names that reflect the gene function are still named by the experts. To fill this gap, we propose a novel task, namely term name generation for GO, and build a large-scale benchmark dataset. Furthermore, we present a graph-based generative model that incorporates the relations between genes, words and terms for term name generation, which exhibits great advantages over the strong baselines.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yanjian and Chen, Qin and Zhang, Yiteng and Wei, Zhongyu and Gao, Yixu and Peng, Jiajie and Huang, Zengfeng and Sun, Weijian and Huang, Xuanjing},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {4705--4710},
}

@inproceedings{uresova_synsemclass_2020,
	address = {Marseille, France},
	title = {{SynSemClass} {Linked} {Lexicon}: {Mapping} {Synonymy} between {Languages}},
	isbn = {979-10-95546-46-7},
	url = {https://aclanthology.org/2020.globalex-1.2/},
	abstract = {This paper reports on an extended version of a synonym verb class lexicon, newly called SynSemClass (formerly CzEngClass). This lexicon stores cross-lingual semantically similar verb senses in synonym classes extracted from a richly annotated parallel corpus, the Prague Czech-English Dependency Treebank. When building the lexicon, we make use of predicate-argument relations (valency) and link them to semantic roles; in addition, each entry is linked to several external lexicons of more or less “semantic” nature, namely FrameNet, WordNet, VerbNet, OntoNotes and PropBank, and Czech VALLEX. The aim is to provide a linguistic resource that can be used to compare semantic roles and their syntactic properties and features across languages within and across synonym groups (classes, or `synsets'), as well as gold standard data for automatic NLP experiments with such synonyms, such as synonym discovery, feature mapping, etc. However, perhaps the most important goal is to eventually build an event type ontology that can be referenced and used as a human-readable and human-understandable “database” for all types of events, processes and states. While the current paper describes primarily the content of the lexicon, we are also presenting a preliminary design of a format compatible with Linked Data, on which we are hoping to get feedback during discussions at the workshop. Once the resource (in whichever form) is applied to corpus annotation, deep analysis will be possible using such combined resources as training data.},
	language = {eng},
	booktitle = {Proceedings of the 2020 {Globalex} {Workshop} on {Linked} {Lexicography}},
	publisher = {European Language Resources Association},
	author = {Uresova, Zdenka and Fucikova, Eva and Hajicova, Eva and Hajic, Jan},
	editor = {Kernerman, Ilan and Krek, Simon and McCrae, John P. and Gracia, Jorge and Ahmadi, Sina and Kabashi, Besim},
	month = may,
	year = {2020},
	pages = {10--19},
}

@inproceedings{mambrini_representing_2020,
	address = {Marseille, France},
	title = {Representing {Etymology} in the {LiLa} {Knowledge} {Base} of {Linguistic} {Resources} for {Latin}},
	isbn = {979-10-95546-46-7},
	url = {https://aclanthology.org/2020.globalex-1.3/},
	abstract = {In this paper we describe the process of inclusion of etymological information in a knowledge base of interoperable Latin linguistic resources developed in the context of the LiLa: Linking Latin project. Interoperability is obtained by applying the Linked Open Data principles. Particularly, an extensive collection of Latin lemmas is used to link the (distributed) resources. For the etymology, we rely on the Ontolex-lemon ontology and the lemonEty extension to model the information, while the source data are taken from a recent etymological dictionary of Latin. As a result, the collection of lemmas LiLa is built around now includes 1,465 Proto-Italic and 1,393 Proto-Indo-European reconstructed forms that are used to explain the history of 1,400 Latin words. We discuss the motivation, methodology and modeling strategies of the work, as well as its possible applications and potential future developments.},
	language = {eng},
	booktitle = {Proceedings of the 2020 {Globalex} {Workshop} on {Linked} {Lexicography}},
	publisher = {European Language Resources Association},
	author = {Mambrini, Francesco and Passarotti, Marco},
	editor = {Kernerman, Ilan and Krek, Simon and McCrae, John P. and Gracia, Jorge and Ahmadi, Sina and Kabashi, Besim},
	month = may,
	year = {2020},
	pages = {20--28},
}

@inproceedings{sanjurjo-gonzalez_increasing_2020,
	address = {Indian Institute of Technology Patna, Patna, India},
	title = {Increasing accuracy of a semantic word labelling tool based on a small lexicon},
	url = {https://aclanthology.org/2020.icon-main.2/},
	abstract = {Semantic annotation has become an important piece of information within corpus linguistics. This information is usually included for every lexical unit of the corpus providing a more exhaustive analysis of language. There are some resources such as lexicons or ontologies that allow this type of annotation. However, expanding these resources is a time-consuming task. This paper describes a simple NLP baseline for increasing accuracy of the existing semantic resources of the UCREL Semantic Analysis System (USAS). In our experiments, Spanish token accuracy is improved by up to 30\% using this method.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Natural} {Language} {Processing} ({ICON})},
	publisher = {NLP Association of India (NLPAI)},
	author = {Sanjurjo-González, Hugo},
	editor = {Bhattacharyya, Pushpak and Sharma, Dipti Misra and Sangal, Rajeev},
	month = dec,
	year = {2020},
	pages = {10--14},
}

@inproceedings{reed_demonstration_2020,
	address = {Patna, India},
	title = {Demonstration of a {Literature} {Based} {Discovery} {System} based on {Ontologies}, {Semantic} {Filters} and {Word} {Embeddings} for the {Raynaud} {Disease}-{Fish} {Oil} {Rediscovery}},
	url = {https://aclanthology.org/2020.icon-demos.1/},
	abstract = {A novel literature-based discovery system based on UMLS Ontologies, Semantic Filters, Statistics, and Word Embed-dings was developed and validated against the well-established Raynaud's disease – Fish Oil discovery by min-ing different size and specificity corpora of Pubmed titles and abstracts. Results show an `inverse effect' between open ver-sus closed discovery search modes. In open discovery, a more general and bigger corpus (Vascular disease or Peri-vascular disease) produces better results than a more specific and smaller in size corpus (Raynaud disease), whereas in closed discovery, the exact opposite is true.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Natural} {Language} {Processing} ({ICON}): {System} {Demonstrations}},
	publisher = {NLP Association of India (NLPAI)},
	author = {Reed, Toby and Cutsuridis, Vassilis},
	editor = {Goyal, Vishal and Ekbal, Asif},
	month = dec,
	year = {2020},
	pages = {1--3},
}

@inproceedings{moneglia_annotation_2020,
	address = {Marseille},
	title = {The {Annotation} of {Thematic} {Structure} and {Alternations} face to the {Semantic} {Variation} of {Action} {Verbs}. {Current} {Trends} in the {IMAGACT} {Ontology}},
	isbn = {979-10-95546-48-1},
	url = {https://aclanthology.org/2020.isa-1.8/},
	abstract = {We present some issues in the development of the semantic annotation of IMAGACT, a multimodal and multilingual ontology of actions. The resource is structured on action concepts that are meant to be cognitive entities and to which a linguistic caption is attached. For each of these concepts, we annotate the minimal thematic structure of the caption and the possible argument alternations allowed. We present some insights on this process with regards to the notion of thematic structure and the relationship between action concepts and linguistic expressions. From the empirical evidence provided by the annotation, we discuss on the very nature of thematic structure, arguing that it is neither a property of the verb itself nor a property of action concepts. We further show what is the relation between thematic structure and 1- the semantic variation of action verbs; 2- the lexical variation of action concepts.},
	language = {eng},
	booktitle = {Proceedings of the 16th {Joint} {ACL}-{ISO} {Workshop} on {Interoperable} {Semantic} {Annotation}},
	publisher = {European Language Resources Association},
	author = {Moneglia, Massimo and Varvara, Rossella},
	editor = {Bunt, Harry},
	month = may,
	year = {2020},
	pages = {67--74},
}

@inproceedings{roberto_towards_2020,
	address = {Marseille},
	title = {Towards the {Ontologization} of the {Outsider} {Art} {Domain}: {Position} {Paper}},
	isbn = {979-10-95546-48-1},
	url = {https://aclanthology.org/2020.isa-1.11/},
	abstract = {The purpose of this paper is to present a prospective and interdisciplinary research project seeking to ontologize knowledge of the domain of Outsider Art, that is, the art created outside the boundaries of official culture. The goal is to combine ontology engineering methodologies to develop a knowledge base which i) examines the relation between social exclusion and cultural productions, ii) standardizes the terminology of Outsider Art and iii) enables semantic interoperability between cultural metadata relevant to Outsider Art. The Outsider Art ontology will integrate some existing ontologies and terminologies, such as the CIDOC - Conceptual Reference Model (CRM), the Art \& Architecture Thesaurus and the Getty Union List of Artist Names, among other resources. Natural Language Processing and Machine Learning techniques will be fundamental instruments for knowledge acquisition and elicitation. NLP techniques will be used to annotate bibliographies of relevant outsider artists and descriptions of outsider artworks with linguistic information. Machine Learning techniques will be leveraged to acquire knowledge from linguistic features embedded in both types of texts.},
	language = {eng},
	booktitle = {Proceedings of the 16th {Joint} {ACL}-{ISO} {Workshop} on {Interoperable} {Semantic} {Annotation}},
	publisher = {European Language Resources Association},
	author = {Roberto, John and Davis, Brian},
	editor = {Bunt, Harry},
	month = may,
	year = {2020},
	pages = {94--101},
}

@inproceedings{sheremetyeva_towards_2020,
	address = {Marseille},
	title = {Towards {Creating} {Interoperable} {Resources} for {Conceptual} {Annotation} of {Multilingual} {Domain} {Corpora}},
	isbn = {979-10-95546-48-1},
	url = {https://aclanthology.org/2020.isa-1.12/},
	abstract = {In this paper we focus on creation of interoperable annotation resources that make up a significant proportion of an on-going project on the development of conceptually annotated multilingual corpora for the domain of terrorist attacks in three languages (English, French and Russian) that can be used for comparative linguistic research, intelligent content and trend analysis, summarization, machine translation, etc. Conceptual annotation is understood as a type of task-oriented domain-specific semantic annotation. The annotation process in our project relies on ontological analysis. The paper details on the issues of the development of both static and dynamic resources such as a universal conceptual annotation scheme, multilingual domain ontology and multipurpose annotation platform with flexible settings, which can be used for the automation of the conceptual resource acquisition and of the annotation process, as well as for the documentation of the annotated corpora specificities. The resources constructed in the course of the research are also to be used for developing concept disambiguation metrics by means of qualitative and quantitative analysis of the golden portion of the conceptually annotated multilingual corpora and of the annotation platform linguistic knowledge.},
	language = {eng},
	booktitle = {Proceedings of the 16th {Joint} {ACL}-{ISO} {Workshop} on {Interoperable} {Semantic} {Annotation}},
	publisher = {European Language Resources Association},
	author = {Sheremetyeva, Svetlana},
	editor = {Bunt, Harry},
	month = may,
	year = {2020},
	pages = {102--109},
}

@inproceedings{tittel_towards_2020,
	address = {Marseille, France},
	title = {Towards an {Ontology} {Based} on {Hallig}-{Wartburg}'s {Begriffssystem} for {Historical} {Linguistic} {Linked} {Data}},
	isbn = {979-10-95546-36-8},
	url = {https://aclanthology.org/2020.ldl-1.1/},
	abstract = {To empower end users in searching for historical linguistic content with a performance that far exceeds the research functions offered by websites of, e.g., historical dictionaries, is undoubtedly a major advantage of (Linguistic) Linked Open Data ([L]LOD). An important aim of lexicography is to enable a language-independent, onomasiological approach, and the modelling of linguistic resources following the LOD paradigm facilitates the semantic mapping to ontologies making this approach possible. Hallig-Wartburg's Begriffssystem (HW) is a well-known extra-linguistic conceptual system used as an onomasiological framework by many historical lexicographical and lexicological works. Published in 1952, HW has meanwhile been digitised. With proprietary XML data as the starting point, our goal is the transformation of HW into Linked Open Data in order to facilitate its use by linguistic resources modelled as LOD. In this paper, we describe the particularities of the HW conceptual model and the method of converting HW: We discuss two approaches, (i) the representation of HW in RDF using SKOS, the SKOS thesaurus extension, and XKOS, and (ii) the creation of a lightweight ontology expressed in OWL, based on the RDF/SKOS model. The outcome is illustrated with use cases of medieval Gascon, and Italian.},
	language = {eng},
	booktitle = {Proceedings of the 7th {Workshop} on {Linked} {Data} in {Linguistics} ({LDL}-2020)},
	publisher = {European Language Resources Association},
	author = {Tittel, Sabine and Gillis-Webber, Frances and Nannini, Alessandro A.},
	editor = {Ionov, Maxim and McCrae, John P. and Chiarcos, Christian and Declerck, Thierry and Bosque-Gil, Julia and Gracia, Jorge},
	month = may,
	year = {2020},
	pages = {1--10},
}

@inproceedings{fiorelli_lime-flavored_2020,
	address = {Marseille, France},
	title = {A {Lime}-{Flavored} {REST} {API} for {Alignment} {Services}},
	isbn = {979-10-95546-36-8},
	url = {https://aclanthology.org/2020.ldl-1.8/},
	abstract = {A practical alignment service should be flexible enough to handle the varied alignment scenarios that arise in the real world, while minimizing the need for manual configuration. MAPLE, an orchestration framework for ontology alignment, supports this goal by coordinating a few loosely coupled actors, which communicate and cooperate to solve a matching task using explicit metadata about the input ontologies, other available resources and the task itself. The alignment task is thus summarized by a report listing its characteristics and suggesting alignment strategies. The schema of the report is based on several metadata vocabularies, among which the Lime module of the OntoLex-Lemon model is particularly important, summarizing the lexical content of the input ontologies and describing external language resources that may be exploited for performing the alignment. In this paper, we propose a REST API that enables the participation of downstream alignment services in the process orchestrated by MAPLE, helping them self-adapt in order to handle heterogeneous alignment tasks and scenarios. The realization of this alignment orchestration effort has been performed through two main phases: we first described its API as an OpenAPI specification (a la API-first), which we then exploited to generate server stubs and compliant client libraries. Finally, we switched our focus to the integration of existing alignment systems, with one fully integrated system and an additional one being worked on, in the effort to propose the API as a valuable addendum to any system being developed.},
	language = {eng},
	booktitle = {Proceedings of the 7th {Workshop} on {Linked} {Data} in {Linguistics} ({LDL}-2020)},
	publisher = {European Language Resources Association},
	author = {Fiorelli, Manuel and Stellato, Armando},
	editor = {Ionov, Maxim and McCrae, John P. and Chiarcos, Christian and Declerck, Thierry and Bosque-Gil, Julia and Gracia, Jorge},
	month = may,
	year = {2020},
	pages = {52--60},
}

@inproceedings{eric_multiwoz_2020,
	address = {Marseille, France},
	title = {{MultiWOZ} 2.1: {A} {Consolidated} {Multi}-{Domain} {Dialogue} {Dataset} with {State} {Corrections} and {State} {Tracking} {Baselines}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.53/},
	abstract = {MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there are substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original dataset with user dialogue acts. This leads to multiple co-existent versions of the same dataset with minor modifications. In this work we tackle the aforementioned issues by introducing MultiWOZ 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32\% of state annotations across 40\% of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the MultiWOZ 2.1 dataset and show the joint state tracking performance on the corrected state annotations. We are publicly releasing MultiWOZ 2.1 to the community, hoping that this dataset resource will allow for more effective models across various dialogue subproblems to be built in the future.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Eric, Mihail and Goel, Rahul and Paul, Shachi and Sethi, Abhishek and Agarwal, Sanchit and Gao, Shuyang and Kumar, Adarsh and Goyal, Anuj and Ku, Peter and Hakkani-Tur, Dilek},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {422--428},
}

@inproceedings{min_towards_2020,
	address = {Marseille, France},
	title = {Towards {Few}-{Shot} {Event} {Mention} {Retrieval}: {An} {Evaluation} {Framework} and {A} {Siamese} {Network} {Approach}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.216/},
	abstract = {Automatically analyzing events in a large amount of text is crucial for situation awareness and decision making. Previous approaches treat event extraction as “one size fits all” with an ontology defined a priori. The resulted extraction models are built just for extracting those types in the ontology. These approaches cannot be easily adapted to new event types nor new domains of interest. To accommodate personalized event-centric information needs, this paper introduces the few-shot Event Mention Retrieval (EMR) task: given a user-supplied query consisting of a handful of event mentions, return relevant event mentions found in a corpus. This formulation enables “query by example”, which drastically lowers the bar of specifying event-centric information needs. The retrieval setting also enables fuzzy search. We present an evaluation framework leveraging existing event datasets such as ACE. We also develop a Siamese Network approach, and show that it performs better than ad-hoc retrieval models in the few-shot EMR setting.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Min, Bonan and Chan, Yee Seng and Zhao, Lingjun},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {1747--1752},
}

@inproceedings{ferre_handling_2020,
	address = {Marseille, France},
	title = {Handling {Entity} {Normalization} with no {Annotated} {Corpus}: {Weakly} {Supervised} {Methods} {Based} on {Distributional} {Representation} and {Ontological} {Information}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.241/},
	abstract = {Entity normalization (or entity linking) is an important subtask of information extraction that links entity mentions in text to categories or concepts in a reference vocabulary. Machine learning based normalization methods have good adaptability as long as they have enough training data per reference with a sufficient quality. Distributional representations are commonly used because of their capacity to handle different expressions with similar meanings. However, in specific technical and scientific domains, the small amount of training data and the relatively small size of specialized corpora remain major challenges. Recently, the machine learning-based CONTES method has addressed these challenges for reference vocabularies that are ontologies, as is often the case in life sciences and biomedical domains. And yet, its performance is dependent on manually annotated corpus. Furthermore, like other machine learning based methods, parametrization remains tricky. We propose a new approach to address the scarcity of training data that extends the CONTES method by corpus selection, pre-processing and weak supervision strategies, which can yield high-performance results without any manually annotated examples. We also study which hyperparameters are most influential, with sometimes different patterns compared to previous work. The results show that our approach significantly improves accuracy and outperforms previous state-of-the-art algorithms.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Ferré, Arnaud and Bossy, Robert and Ba, Mouhamadou and Deléger, Louise and Lavergne, Thomas and Zweigenbaum, Pierre and Nédellec, Claire},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {1959--1966},
}

@inproceedings{shafran_medical_2020,
	address = {Marseille, France},
	title = {The {Medical} {Scribe}: {Corpus} {Development} and {Model} {Performance} {Analyses}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.250/},
	abstract = {There is a growing interest in creating tools to assist in clinical note generation using the audio of provider-patient encounters. Motivated by this goal and with the help of providers and medical scribes, we developed an annotation scheme to extract relevant clinical concepts. We used this annotation scheme to label a corpus of about 6k clinical encounters. This was used to train a state-of-the-art tagging model. We report ontologies, labeling results, model performances, and detailed analyses of the results. Our results show that the entities related to medications can be extracted with a relatively high accuracy of 0.90 F-score, followed by symptoms at 0.72 F-score, and conditions at 0.57 F-score. In our task, we not only identify where the symptoms are mentioned but also map them to canonical forms as they appear in the clinical notes. Of the different types of errors, in about 19-38\% of the cases, we find that the model output was correct, and about 17-32\% of the errors do not impact the clinical note. Taken together, the models developed in this work are more useful than the F-scores reflect, making it a promising approach for practical applications.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Shafran, Izhak and Du, Nan and Tran, Linh and Perry, Amanda and Keyes, Lauren and Knichel, Mark and Domin, Ashley and Huang, Lei and Chen, Yu-hui and Li, Gang and Wang, Mingqiu and El Shafey, Laurent and Soltau, Hagen and Paul, Justin Stuart},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {2036--2044},
}

@inproceedings{vanetik_automated_2020,
	address = {Marseille, France},
	title = {Automated {Discovery} of {Mathematical} {Definitions} in {Text}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.256/},
	abstract = {Automatic definition extraction from texts is an important task that has numerous applications in several natural language processing fields such as summarization, analysis of scientific texts, automatic taxonomy generation, ontology generation, concept identification, and question answering. For definitions that are contained within a single sentence, this problem can be viewed as a binary classification of sentences into definitions and non-definitions. Definitions in scientific literature can be generic (Wikipedia) or more formal (mathematical articles). In this paper, we focus on automatic detection of one-sentence definitions in mathematical texts, which are difficult to separate from surrounding text. We experiment with several data representations, which include sentence syntactic structure and word embeddings, and apply deep learning methods such as convolutional neural network (CNN) and recurrent neural network (RNN), in order to identify mathematical definitions. Our experiments demonstrate the superiority of CNN and its combination with RNN, applied on the syntactically-enriched input representation. We also present a new dataset for definition extraction from mathematical texts. We demonstrate that the use of this dataset for training learning models improves the quality of definition extraction when these models are then used for other definition datasets. Our experiments with different domains approve that mathematical definitions require special treatment, and that using cross-domain learning is inefficient.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Vanetik, Natalia and Litvak, Marina and Shevchuk, Sergey and Reznik, Lior},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {2086--2094},
}

@inproceedings{humphreys_populating_2020,
	address = {Marseille, France},
	title = {Populating {Legal} {Ontologies} using {Semantic} {Role} {Labeling}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.264/},
	abstract = {This paper is concerned with the goal of maintaining legal information and compliance systems: the `resource consumption bottleneck' of creating semantic technologies manually. The use of automated information extraction techniques could significantly reduce this bottleneck. The research question of this paper is: How to address the resource bottleneck problem of creating specialist knowledge management systems? In particular, how to semi-automate the extraction of norms and their elements to populate legal ontologies? This paper shows that the acquisition paradox can be addressed by combining state-of-the-art general-purpose NLP modules with pre- and post-processing using rules based on domain knowledge. It describes a Semantic Role Labeling based information extraction system to extract norms from legislation and represent them as structured norms in legal ontologies. The output is intended to help make laws more accessible, understandable, and searchable in legal document management systems such as Eunomos (Boella et al., 2016).},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Humphreys, Llio and Boella, Guido and Di Caro, Luigi and Robaldo, Livio and van der Torre, Leon and Ghanavati, Sepideh and Muthuri, Robert},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {2157--2166},
}

@inproceedings{de_boer_towards_2020,
	address = {Marseille, France},
	title = {Towards {Data}-driven {Ontologies}: a {Filtering} {Approach} using {Keywords} and {Natural} {Language} {Constructs}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.278/},
	abstract = {Creating ontologies is an expensive task. Our vision is that we can automatically generate ontologies based on a set of relevant documents to create a kick-start in ontology creating sessions. In this paper, we focus on enhancing two often used methods, OpenIE and co-occurrences. We evaluate the methods on two document sets, one about pizza and one about the agriculture domain. The methods are evaluated using two types of F1-score (objective, quantitative) and through a human assessment (subjective, qualitative). The results show that 1) Cooc performs both objectively and subjectively better than OpenIE; 2) the filtering methods based on keywords and on Word2vec perform similarly; 3) the filtering methods both perform better compared to OpenIE and similar to Cooc; 4) Cooc-NVP performs best, especially considering the subjective evaluation. Although, the investigated methods provide a good start for extracting an ontology out of a set of domain documents, various improvements are still possible, especially in the natural language based methods.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {de Boer, Maaike and Verhoosel, Jack P. C.},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {2285--2292},
}

@inproceedings{jabbari_french_2020,
	address = {Marseille, France},
	title = {A {French} {Corpus} and {Annotation} {Schema} for {Named} {Entity} {Recognition} and {Relation} {Extraction} of {Financial} {News}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.279/},
	abstract = {In financial services industry, compliance involves a series of practices and controls in order to meet key regulatory standards which aim to reduce financial risk and crime, e.g. money laundering and financing of terrorism. Faced with the growing risks, it is imperative for financial institutions to seek automated information extraction techniques for monitoring financial activities of their customers. This work describes an ontology of compliance-related concepts and relationships along with a corpus annotated according to it. The presented corpus consists of financial news articles in French and allows for training and evaluating domain-specific named entity recognition and relation extraction algorithms. We present some of our experimental results on named entity recognition and relation extraction using our annotated corpus. We aim to furthermore use the the proposed ontology towards construction of a knowledge base of financial relations.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Jabbari, Ali and Sauvage, Olivier and Zeine, Hamada and Chergui, Hamza},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {2293--2299},
}

@inproceedings{himoro_towards_2020,
	address = {Marseille, France},
	title = {Towards a {Spell} {Checker} for {Zamboanga} {Chavacano} {Orthography}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.327/},
	abstract = {Zamboanga Chabacano (ZC) is the most vibrant variety of Philippine Creole Spanish, with over 400,000 native speakers in the Philippines (as of 2010). Following its introduction as a subject and a medium of instruction in the public schools of Zamboanga City from Grade 1 to 3 in 2012, an official orthography for this variety - the so-called “Zamboanga Chavacano Orthography” - has been approved in 2014. Its complexity, however, is a barrier to most speakers, since it does not necessarily reflect the particular phonetic evolution in ZC, but favours etymology instead. The distance between the correct spelling and the different spelling variations is often so great that delivering acceptable performance with the current de facto spell checking technologies may be challenging. The goals of this research have been to propose i) a spelling error taxonomy for ZC, formalised as an ontology and ii) an adaptive spell checking approach using Character-Based Statistical Machine Translation to correct spelling errors in ZC. Our results show that this approach is suitable for the goals mentioned and that it could be combined with other current spell checking technologies to achieve even higher performance.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Himoro, Marcelo Yuji and Pareja-Lora, Antonio},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {2685--2697},
}

@inproceedings{li_ca-ehn_2020,
	address = {Marseille, France},
	title = {{CA}-{EHN}: {Commonsense} {Analogy} from {E}-{HowNet}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.365/},
	abstract = {Embedding commonsense knowledge is crucial for end-to-end models to generalize inference beyond training corpora. However, existing word analogy datasets have tended to be handcrafted, involving permutations of hundreds of words with only dozens of pre-defined relations, mostly morphological relations and named entities. In this work, we model commonsense knowledge down to word-level analogical reasoning by leveraging E-HowNet, an ontology that annotates 88K Chinese words with their structured sense definitions and English translations. We present CA-EHN, the first commonsense word analogy dataset containing 90,505 analogies covering 5,656 words and 763 relations. Experiments show that CA-EHN stands out as a great indicator of how well word representations embed commonsense knowledge. The dataset is publicly available at https://github.com/ckiplab/CA-EHN.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Li, Peng-Hsuan and Yang, Tsan-Yu and Ma, Wei-Yun},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {2984--2990},
}

@inproceedings{allen_broad-coverage_2020,
	address = {Marseille, France},
	title = {A {Broad}-{Coverage} {Deep} {Semantic} {Lexicon} for {Verbs}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.396/},
	abstract = {Progress on deep language understanding is inhibited by the lack of a broad coverage lexicon that connects linguistic behavior to ontological concepts and axioms. We have developed COLLIE-V, a deep lexical resource for verbs, with the coverage of WordNet and syntactic and semantic details that meet or exceed existing resources. Bootstrapping from a hand-built lexicon and ontology, new ontological concepts and lexical entries, together with semantic role preferences and entailment axioms, are automatically derived by combining multiple constraints from parsing dictionary definitions and examples. We evaluated the accuracy of the technique along a number of different dimensions and were able to obtain high accuracy in deriving new concepts and lexical entries. COLLIE-V is publicly available.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Allen, James and An, Hannah and Bose, Ritwik and de Beaumont, Will and Teng, Choh Man},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {3243--3251},
}

@inproceedings{evert_corpus_2020,
	address = {Marseille, France},
	title = {Corpus {Query} {Lingua} {Franca} part {II}: {Ontology}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.410/},
	abstract = {The present paper outlines the projected second part of the Corpus Query Lingua Franca (CQLF) family of standards: CQLF Ontology, which is currently in the process of standardization at the International Standards Organization (ISO), in its Technical Committee 37, Subcommittee 4 (TC37SC4) and its national mirrors. The first part of the family, ISO 24623-1 (henceforth CQLF Metamodel), was successfully adopted as an international standard at the beginning of 2018. The present paper reflects the state of the CQLF Ontology at the moment of submission for the Committee Draft ballot. We provide a brief overview of the CQLF Metamodel, present the assumptions and aims of the CQLF Ontology, its basic structure, and its potential extended applications. The full ontology is expected to emerge from a community process, starting from an initial version created by the authors of the present paper.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Evert, Stefan and Harlamov, Oleg and Heinrich, Philipp and Banski, Piotr},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {3346--3352},
}

@inproceedings{labropoulou_making_2020,
	address = {Marseille, France},
	title = {Making {Metadata} {Fit} for {Next} {Generation} {Language} {Technology} {Platforms}: {The} {Metadata} {Schema} of the {European} {Language} {Grid}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.420/},
	abstract = {The current scientific and technological landscape is characterised by the increasing availability of data resources and processing tools and services. In this setting, metadata have emerged as a key factor facilitating management, sharing and usage of such digital assets. In this paper we present ELG-SHARE, a rich metadata schema catering for the description of Language Resources and Technologies (processing and generation services and tools, models, corpora, term lists, etc.), as well as related entities (e.g., organizations, projects, supporting documents, etc.). The schema powers the European Language Grid platform that aims to be the primary hub and marketplace for industry-relevant Language Technology in Europe. ELG-SHARE has been based on various metadata schemas, vocabularies, and ontologies, as well as related recommendations and guidelines.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Labropoulou, Penny and Gkirtzou, Katerina and Gavriilidou, Maria and Deligiannis, Miltos and Galanis, Dimitris and Piperidis, Stelios and Rehm, Georg and Berger, Maria and Mapelli, Valérie and Rigault, Michael and Arranz, Victoria and Choukri, Khalid and Backfried, Gerhard and Gómez-Pérez, José Manuel and Garcia-Silva, Andres},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {3428--3437},
}

@inproceedings{loffler_tag_2020,
	address = {Marseille, France},
	title = {Tag {Me} {If} {You} {Can}! {Semantic} {Annotation} of {Biodiversity} {Metadata} with the {QEMP} {Corpus} and the {BiodivTagger}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.560/},
	abstract = {Dataset Retrieval is gaining importance due to a large amount of research data and the great demand for reusing scientific data. Dataset Retrieval is mostly based on metadata, structured information about the primary data. Enriching these metadata with semantic annotations based on Linked Open Data (LOD) enables datasets, publications and authors to be connected and expands the search on semantically related terms. In this work, we introduce the BiodivTagger, an ontology-based Information Extraction pipeline, developed for metadata from biodiversity research. The system recognizes biological, physical and chemical processes, environmental terms, data parameters and phenotypes as well as materials and chemical compounds and links them to concepts in dedicated ontologies. To evaluate our pipeline, we created a gold standard of 50 metadata files (QEMP corpus) selected from five different data repositories in biodiversity research. To the best of our knowledge, this is the first annotated metadata corpus for biodiversity research data. The results reveal a mixed picture. While materials and data parameters are properly matched to ontological concepts in most cases, some ontological issues occurred for processes and environmental terms.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Löffler, Felicitas and Abdelmageed, Nora and Babalou, Samira and Kaur, Pawandeep and König-Ries, Birgitta},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {4557--4564},
}

@inproceedings{alsudias_developing_2020,
	address = {Marseille, France},
	title = {Developing an {Arabic} {Infectious} {Disease} {Ontology} to {Include} {Non}-{Standard} {Terminology}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.596/},
	abstract = {Building ontologies is a crucial part of the semantic web endeavour. In recent years, research interest has grown rapidly in supporting languages such as Arabic in NLP in general but there has been very little research on medical ontologies for Arabic. We present a new Arabic ontology in the infectious disease domain to support various important applications including the monitoring of infectious disease spread via social media. This ontology meaningfully integrates the scientific vocabularies of infectious diseases with their informal equivalents. We use ontology learning strategies with manual checking to build the ontology. We applied three statistical methods for term extraction from selected Arabic infectious diseases articles: TF-IDF, C-value, and YAKE. We also conducted a study, by consulting around 100 individuals, to discover the informal terms related to infectious diseases in Arabic. In future work, we will automatically extract the relations for infectious disease concepts but for now these are manually created. We report two complementary experiments to evaluate the ontology. First, a quantitative evaluation of the term extraction results and an additional qualitative evaluation by a domain expert.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Alsudias, Lama and Rayson, Paul},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {4842--4850},
}

@inproceedings{bou_ontology-style_2020,
	address = {Marseille, France},
	title = {Ontology-{Style} {Relation} {Annotation}: {A} {Case} {Study}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.599/},
	abstract = {This paper proposes an Ontology-Style Relation (OSR) annotation approach. In conventional Relation Extraction (RE) datasets, relations are annotated as links between entity mentions. In contrast, in our OSR annotation, a relation is annotated as a relation mention (i.e., not a link but a node) and domain and range links are annotated from the relation mention to its argument entity mentions. We expect the following benefits: (1) the relation annotations can be easily converted to Resource Description Framework (RDF) triples to populate an Ontology, (2) some part of conventional RE tasks can be tackled as Named Entity Recognition (NER) tasks. The relation classes are limited to several RDF properties such as domain, range, and subClassOf, and (3) OSR annotations can be clear documentations of Ontology contents. As a case study, we converted an in-house corpus of Japanese traffic rules in conventional annotations into the OSR annotations and built a novel OSR-RoR (Rules of the Road) corpus. The inter-annotator agreements of the conversion were 85-87\%. We evaluated the performance of neural NER and RE tools on the conventional and OSR annotations. The experimental results showed that the OSR annotations make the RE task easier while introducing slight complexity into the NER task.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Bou, Savong and Suzuki, Naoki and Miwa, Makoto and Sasaki, Yutaka},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {4867--4876},
}

@inproceedings{dekova_ontology_2020,
	address = {Marseille, France},
	title = {The {Ontology} of {Bulgarian} {Dialects} – {Architecture} and {Information} {Retrieval}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.600/},
	abstract = {Following a concise description of the structure, the paper focuses on the potential of the Ontology of the Bulgarian Dialects, which demonstrates a novel usage of the ontological modelling for the purposes of dialect digital archiving and information processing. The ontology incorporates information on the dialects of the Bulgarian language and includes data from 84 dialects, spoken not only on the territory of the Republic of Bulgaria, but also abroad. It encodes both their geographical distribution and some of their main diagnostic features, such as the different mutations (also referred to as reflexes) of some of the Old Bulgarian vowels. The mutations modelled so far in the ontology include the reflex of the back nasal vowel /{\textbackslash}cyrbyus/ under stress, the reflex of the back er vowel /{\textbackslash}cyrhrdsn/ under stress, and the reflex of the yat vowel /{\textbackslash}cyryat/ under stress when it precedes a syllable with a back vowel. Besides the opportunity for formal structuring of the considerable amount of data gathered through the years by dialectologists, the ontology also provides numerous possibilities for information retrieval – searches by dialect, country, dialect region, city or village, various combinations of diagnostic features.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Dekova, Rositsa},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {4877--4882},
}

@inproceedings{bick_syntax_2020,
	address = {Marseille, France},
	title = {Syntax and {Semantics} in a {Treebank} for {Esperanto}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.630/},
	abstract = {In this paper we describe and evaluate syntactic and semantic aspects of Arbobanko, a treebank for the artificial language Esperanto, as well as tools and methods used in the production of the treebank. In addition to classical morphosyntax and dependency structure, the treebank was enriched with a lexical-semantic layer covering named entities, a semantic type ontology for nouns and adjectives and a framenet-inspired semantic classification of verbs. For an under-resourced language, the quality of automatic syntactic and semantic pre-annotation is of obvious importance, and by evaluating the underlying parser and the coverage of its semantic ontologies, we try to answer the question whether the language's extremely regular morphology and transparent semantic affixes translate into a more regular syntax and higher parsing accuracy. On the linguistic side, the treebank allows us to address and quantify typological issues such as the question of word order, auxiliary constructions, lexical transparency and semantic type ambiguity in Esperanto.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Bick, Eckhard},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {5120--5127},
}

@inproceedings{bento_ontology_2020,
	address = {Marseille, France},
	title = {Ontology {Matching} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.693/},
	abstract = {In order to achieve interoperability of information in the context of the Semantic Web, it is necessary to find effective ways to align different ontologies. As the number of ontologies grows for a given domain, and as overlap between ontologies grows proportionally, it is becoming more and more crucial to develop accurate and reliable techniques to perform this task automatically. While traditional approaches to address this challenge are based on string metrics and structure analysis, in this paper we present a methodology to align ontologies automatically using machine learning techniques. Specifically, we use convolutional neural networks to perform string matching between class labels using character embeddings. We also rely on the set of superclasses to perform the best alignment. Our results show that we obtain state-of-the-art performance on ontologies from the Ontology Alignment Evaluation Initiative (OAEI). Our model also maintains good performance when tested on a different domain, which could lead to potential cross-domain applications.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Bento, Alexandre and Zouaq, Amal and Gagnon, Michel},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {5648--5653},
}

@inproceedings{el-haj_infrastructure_2020,
	address = {Marseille, France},
	title = {Infrastructure for {Semantic} {Annotation} in the {Genomics} {Domain}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.855/},
	abstract = {We describe a novel super-infrastructure for biomedical text mining which incorporates an end-to-end pipeline for the collection, annotation, storage, retrieval and analysis of biomedical and life sciences literature, combining NLP and corpus linguistics methods. The infrastructure permits extreme-scale research on the open access PubMed Central archive. It combines an updatable Gene Ontology Semantic Tagger (GOST) for entity identification and semantic markup in the literature, with a NLP pipeline scheduler (Buster) to collect and process the corpus, and a bespoke columnar corpus database (LexiDB) for indexing. The corpus database is distributed to permit fast indexing, and provides a simple web front-end with corpus linguistics methods for sub-corpus comparison and retrieval. GOST is also connected as a service in the Language Application (LAPPS) Grid, in which context it is interoperable with other NLP tools and data in the Grid and can be combined with them in more complex workflows. In a literature based discovery setting, we have created an annotated corpus of 9,776 papers with 5,481,543 words.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {El-Haj, Mahmoud and Rutherford, Nathan and Coole, Matthew and Ezeani, Ignatius and Prentice, Sheryl and Ide, Nancy and Knight, Jo and Piao, Scott and Mariani, John and Rayson, Paul and Suderman, Keith},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {6921--6929},
}

@inproceedings{fiorelli_editing_2020,
	address = {Marseille, France},
	title = {Editing {OntoLex}-{Lemon} in {VocBench} 3},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.889/},
	abstract = {OntoLex-Lemon is a collection of RDF vocabularies for specifying the verbalization of ontologies in natural language. Beyond its original scope, OntoLex-Lemon, as well as its predecessor Monnet lemon, found application in the Linguistic Linked Open Data cloud to represent and interlink language resources on the Semantic Web. Unfortunately, generic ontology and RDF editors were considered inconvenient to use with OntoLex-Lemon because of its complex design patterns and other peculiarities, including indirection, reification and subtle integrity constraints. This perception led to the development of dedicated editors, trading the flexibility of RDF in combining different models (and the features already available in existing RDF editors) for a more direct and streamlined editing of OntoLex-Lemon patterns. In this paper, we investigate on the benefits gained by extending an already existing RDF editor, VocBench 3, with capabilities closely tailored to OntoLex-Lemon and on the challenges that such extension implies. The outcome of such investigation is twofold: a vertical assessment of a new editor for OntoLex-Lemon and, in the broader scope of RDF editor design, a new perspective on which flexibility and extensibility characteristics an editor should meet in order to cover new core modeling vocabularies, for which OntoLex-Lemon represents a use case.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Fiorelli, Manuel and Stellato, Armando and Lorenzetti, Tiziano and Turbati, Andrea and Schmitz, Peter and Francesconi, Enrico and Hajlaoui, Najeh and Batouche, Brahim},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {7194--7203},
}

@inproceedings{fath_fintan_2020,
	address = {Marseille, France},
	title = {Fintan - {Flexible}, {Integrated} {Transformation} and {Annotation} {eNgineering}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.891/},
	abstract = {We introduce the Flexible and Integrated Transformation and Annotation eNgeneering (Fintan) platform for converting heterogeneous linguistic resources to RDF. With its modular architecture, workflow management and visualization features, Fintan facilitates the development of complex transformation pipelines by integrating generic RDF converters and augmenting them with extended graph processing capabilities: Existing converters can be easily deployed to the system by means of an ontological data structure which renders their properties and the dependencies between transformation steps. Development of subsequent graph transformation steps for resource transformation, annotation engineering or entity linking is further facilitated by a novel visual rendering of SPARQL queries. A graphical workflow manager allows to easily manage the converter modules and combine them to new transformation pipelines. Employing the stream-based graph processing approach first implemented with CoNLL-RDF, we address common challenges and scalability issues when transforming resources and showcase the performance of Fintan by means of a purely graph-based transformation of the Universal Morphology data to RDF.},
	language = {eng},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Fäth, Christian and Chiarcos, Christian and Ebbrecht, Björn and Ionov, Maxim},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {7212--7221},
}

@inproceedings{gonzalez-dios_towards_2020,
	address = {Marseille, France},
	title = {Towards modelling {SUMO} attributes through {WordNet} adjectives: a {Case} {Study} on {Qualities}},
	isbn = {979-10-95546-41-2},
	url = {https://aclanthology.org/2020.mmw-1.1/},
	abstract = {Previous studies have shown that the knowledge about attributes and properties in the SUMO ontology and its mapping to WordNet adjectives lacks of an accurate and complete characterization. A proper characterization of this type of knowledge is required to perform formal commonsense reasoning based on the SUMO properties, for instance to distinguish one concept from another based on their properties. In this context, we propose a new semi-automatic approach to model the knowledge about properties and attributes in SUMO by exploiting the information encoded in WordNet adjectives and its mapping to SUMO. To that end, we considered clusters of semantically related groups of WordNet adjectival and nominal synsets. Based on these clusters, we propose a new semi-automatic model for SUMO attributes and their mapping to WordNet, which also includes polarity information. In this paper, as an exploratory approach, we focus on qualities.},
	language = {eng},
	booktitle = {Proceedings of the {LREC} 2020 {Workshop} on {Multimodal} {Wordnets} ({MMW2020})},
	publisher = {The European Language Resources Association (ELRA)},
	author = {Gonzalez-Dios, Itziar and Alvez, Javier and Rigau, German},
	editor = {Declerk, Thierry and Gonzalez-Dios, Itziar and Rigau, German},
	month = may,
	year = {2020},
	pages = {1--6},
}

@inproceedings{marciniak_wordnet_2020,
	address = {Marseille, France},
	title = {Wordnet {As} a {Backbone} of {Domain} and {Application} {Conceptualizations} in {Systems} with {Multimodal} {Data}},
	isbn = {979-10-95546-41-2},
	url = {https://aclanthology.org/2020.mmw-1.5/},
	abstract = {Information systems gathering big amounts of resources growing with time containing distinct modalities (text, audio, video, images, GIS) and aggregating content in various ways (modular e-learning modules, Web systems presenting cultural artefacts) require tools supporting content description. The subject of the description may be the topic and the characteristics of the content expressed by sets of attributes. To describe such resources one can just use some of existing indexing languages like thesauri, classification systems, domain and upper ontologies, terminologies or dictionaries. When appropriate language does not exist, it is necessary to build a new system, which will have to serve both experts who describe resources and non-experts who search through them. The solution presented in this paper used to resource description, allows experts to freely select words and expressions, which are organized in hierarchies of various nature, including that of domain and application character. This is based on the wordnet structure, which introduces a clear order for each of these groups due to its lexical nature. The paper presents two systems where such approach was applied: the E-archaeology.org e-learning content repository in which domain knowledge was integrated to describe content topics and the Hatch system gathering multimodal information about the archaeological site targeted at a wide audience, where application conceptualization was applied to describe the content by a set of attributes.},
	language = {eng},
	booktitle = {Proceedings of the {LREC} 2020 {Workshop} on {Multimodal} {Wordnets} ({MMW2020})},
	publisher = {The European Language Resources Association (ELRA)},
	author = {Marciniak, Jacek},
	editor = {Declerk, Thierry and Gonzalez-Dios, Itziar and Rigau, German},
	month = may,
	year = {2020},
	pages = {25--32},
}

@inproceedings{bouscarrat_multilingual_2020,
	address = {Marseille, France},
	title = {Multilingual enrichment of disease biomedical ontologies},
	isbn = {979-10-95546-65-8},
	url = {https://aclanthology.org/2020.multilingualbio-1.4/},
	abstract = {Translating biomedical ontologies is an important challenge, but doing it manually requires much time and money. We study the possibility to use open-source knowledge bases to translate biomedical ontologies. We focus on two aspects: coverage and quality. We look at the coverage of two biomedical ontologies focusing on diseases with respect to Wikidata for 9 European languages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese and Spanish) for both, plus Arabic, Chinese and Russian for the second. We first use direct links between Wikidata and the studied ontologies and then use second-order links by going through other intermediate ontologies. We then compare the quality of the translations obtained thanks to Wikidata with a commercial machine translation tool, here Google Cloud Translation.},
	language = {eng},
	booktitle = {Proceedings of the {LREC} 2020 {Workshop} on {Multilingual} {Biomedical} {Text} {Processing} ({MultilingualBIO} 2020)},
	publisher = {European Language Resources Association},
	author = {Bouscarrat, Léo and Bonnefoy, Antoine and Capponi, Cécile and Ramisch, Carlos},
	editor = {Melero, Maite},
	month = may,
	year = {2020},
	pages = {21--28},
}

@inproceedings{bhutani_answering_2020,
	address = {Online},
	title = {Answering {Complex} {Questions} by {Combining} {Information} from {Curated} and {Extracted} {Knowledge} {Bases}},
	url = {https://aclanthology.org/2020.nli-1.1/},
	doi = {10.18653/v1/2020.nli-1.1},
	abstract = {Knowledge-based question answering (KB\_QA) has long focused on simple questions that can be answered from a single knowledge source, a manually curated or an automatically extracted KB. In this work, we look at answering complex questions which often require combining information from multiple sources. We present a novel KB-QA system, Multique, which can map a complex question to a complex query pattern using a sequence of simple queries each targeted at a specific KB. It finds simple queries using a neural-network based model capable of collective inference over textual relations in extracted KB and ontological relations in curated KB. Experiments show that our proposed system outperforms previous KB-QA systems on benchmark datasets, ComplexWebQuestions and WebQuestionsSP.},
	booktitle = {Proceedings of the {First} {Workshop} on {Natural} {Language} {Interfaces}},
	publisher = {Association for Computational Linguistics},
	author = {Bhutani, Nikita and Zheng, Xinyi and Qian, Kun and Li, Yunyao and Jagadish, H.},
	editor = {Awadallah, Ahmed Hassan and Su, Yu and Sun, Huan and Yih, Scott Wen-tau},
	month = jul,
	year = {2020},
	pages = {1--10},
}

@inproceedings{zang_multiwoz_2020,
	address = {Online},
	title = {{MultiWOZ} 2.2 : {A} {Dialogue} {Dataset} with {Additional} {Annotation} {Corrections} and {State} {Tracking} {Baselines}},
	url = {https://aclanthology.org/2020.nlp4convai-1.13/},
	doi = {10.18653/v1/2020.nlp4convai-1.13},
	abstract = {MultiWOZ is a well-known task-oriented dialogue dataset containing over 10,000 annotated dialogues spanning 8 domains. It is extensively used as a benchmark for dialogue state tracking. However, recent works have reported presence of substantial noise in the dialogue state annotations. MultiWOZ 2.1 identified and fixed many of these erroneous annotations and user utterances, resulting in an improved version of this dataset. This work introduces MultiWOZ 2.2, which is a yet another improved version of this dataset. Firstly, we identify and fix dialogue state annotation errors across 17.3\% of the utterances on top of MultiWOZ 2.1. Secondly, we redefine the ontology by disallowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking). In addition, we introduce slot span annotations for these slots to standardize them across recent models, which previously used custom string matching heuristics to generate them. We also benchmark a few state of the art dialogue state tracking models on the corrected dataset to facilitate comparison for future work. In the end, we discuss best practices for dialogue data collection that can help avoid annotation errors.},
	booktitle = {Proceedings of the 2nd {Workshop} on {Natural} {Language} {Processing} for {Conversational} {AI}},
	publisher = {Association for Computational Linguistics},
	author = {Zang, Xiaoxue and Rastogi, Abhinav and Sunkara, Srinivas and Gupta, Raghav and Zhang, Jianguo and Chen, Jindong},
	editor = {Wen, Tsung-Hsien and Celikyilmaz, Asli and Yu, Zhou and Papangelis, Alexandros and Eric, Mihail and Kumar, Anuj and Casanueva, Iñigo and Shah, Rushin},
	month = jul,
	year = {2020},
	pages = {109--117},
}

@inproceedings{barros_covid-19_2020,
	address = {Online},
	title = {{COVID}-19: {A} {Semantic}-{Based} {Pipeline} for {Recommending} {Biomedical} {Entities}},
	url = {https://aclanthology.org/2020.nlpcovid19-2.20/},
	doi = {10.18653/v1/2020.nlpcovid19-2.20},
	abstract = {With the increasing number of publications about COVID-19, it is a challenge to extract personalized knowledge suitable for each researcher. This work aims to build a new semantic-based pipeline for recommending biomedical entities to scientific researchers. To this end, we developed a pipeline that creates an implicit feedback matrix based on Named Entity Recognition (NER) on a corpus of documents, using multidisciplinary ontologies for recognizing and linking the entities. Our hypothesis is that by using ontologies from different fields in the NER phase, we can improve the results for state-of-the-art collaborative-filtering recommender systems applied to the dataset created. The tests performed using the COVID-19 Open Research Dataset (CORD-19) dataset show that when using four ontologies, the results for precision@k, for example, reach the 80\%, whereas when using only one ontology, the results for precision@k drops to 20\%, for the same users. Furthermore, the use of multi-fields entities may help in the discovery of new items, even if the researchers do not have items from that field in their set of preferences.},
	booktitle = {Proceedings of the 1st {Workshop} on {NLP} for {COVID}-19 ({Part} 2) at {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Barros, Marcia Afonso and Lamurias, Andre and Sousa, Diana and Ruas, Pedro and Couto, Francisco M.},
	editor = {Verspoor, Karin and Cohen, Kevin Bretonnel and Conway, Michael and de Bruijn, Berry and Dredze, Mark and Mihalcea, Rada and Wallace, Byron},
	month = dec,
	year = {2020},
}

@inproceedings{amith_towards_2020,
	address = {Online},
	title = {Towards an {Ontology}-based {Medication} {Conversational} {Agent} for {PrEP} and {PEP}},
	url = {https://aclanthology.org/2020.nlpmc-1.5/},
	doi = {10.18653/v1/2020.nlpmc-1.5},
	abstract = {ABSTRACT: HIV (human immunodeficiency virus) can damage a human's immune system and cause Acquired Immunodeficiency Syndrome (AIDS) which could lead to severe outcomes, including death. While HIV infections have decreased over the last decade, there is still a significant population where the infection permeates. PrEP and PEP are two proven preventive measures introduced that involve periodic dosage to stop the onset of HIV infection. However, the adherence rates for this medication is low in part due to the lack of information about the medication. There exist several communication barriers that prevent patient-provider communication from happening. In this work, we present our ontology-based method for automating the communication of this medication that can be deployed for live conversational agents for PrEP and PEP. This method facilitates a model of automated conversation between the machine and user can also answer relevant questions.},
	booktitle = {Proceedings of the {First} {Workshop} on {Natural} {Language} {Processing} for {Medical} {Conversations}},
	publisher = {Association for Computational Linguistics},
	author = {Amith, Muhammad and Cui, Licong and Roberts, Kirk and Tao, Cui},
	editor = {Bhatia, Parminder and Lin, Steven and Gangadharaiah, Rashmi and Wallace, Byron and Shafran, Izhak and Shivade, Chaitanya and Du, Nan and Diab, Mona},
	month = jul,
	year = {2020},
	pages = {31--40},
}

@inproceedings{zad_systematic_2020,
	address = {Online},
	title = {Systematic {Evaluation} of a {Framework} for {Unsupervised} {Emotion} {Recognition} for {Narrative} {Text}},
	url = {https://aclanthology.org/2020.nuse-1.4/},
	doi = {10.18653/v1/2020.nuse-1.4},
	abstract = {Identifying emotions as expressed in text (a.k.a. text emotion recognition) has received a lot of attention over the past decade. Narratives often involve a great deal of emotional expression, and so emotion recognition on narrative text is of great interest to computational approaches to narrative understanding. Prior work by Kim et al. 2010 was the work with the highest reported emotion detection performance, on a corpus of fairy tales texts. Close inspection of that work, however, revealed significant reproducibility problems, and we were unable to reimplement Kim's approach as described. As a consequence, we implemented a framework inspired by Kim's approach, where we carefully evaluated the major design choices. We identify the highest-performing combination, which outperforms Kim's reported performance by 7.6 F\_1 points on average. Close inspection of the annotated data revealed numerous missing and incorrect emotion terms in the relevant lexicon, WordNetAffect (WNA; Strapparava and Valitutti, 2004), which allowed us to augment it in a useful way. More generally, this showed that numerous clearly emotive words and phrases are missing from WNA, which suggests that effort invested in augmenting or refining emotion ontologies could be useful for improving the performance of emotion recognition systems. We release our code and data to definitely enable future reproducibility of this work.},
	booktitle = {Proceedings of the {First} {Joint} {Workshop} on {Narrative} {Understanding}, {Storylines}, and {Events}},
	publisher = {Association for Computational Linguistics},
	author = {Zad, Samira and Finlayson, Mark},
	editor = {Bonial, Claire and Caselli, Tommaso and Chaturvedi, Snigdha and Clark, Elizabeth and Huang, Ruihong and Iyyer, Mohit and Jaimes, Alejandro and Ji, Heng and Martin, Lara J. and Miller, Ben and Mitamura, Teruko and Peng, Nanyun and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {26--37},
}

@inproceedings{ogrodniczuk_new_2020,
	address = {Marseille, France},
	title = {New {Developments} in the {Polish} {Parliamentary} {Corpus}},
	isbn = {979-10-95546-47-4},
	url = {https://aclanthology.org/2020.parlaclarin-1.1/},
	abstract = {This short paper presents the current (as of February 2020) state of preparation of the Polish Parliamentary Corpus (PPC)—an extensive collection of transcripts of Polish parliamentary proceedings dating from 1919 to present. The most evident developments as compared to the 2018 version is harmonization of metadata, standardization of document identifiers, uploading contents of all documents and metadata to the database (to enable easier modification, maintenance and future development of the corpus), linking utterances to the political ontology, linking corpus texts to source data and processing historical documents.},
	language = {eng},
	booktitle = {Proceedings of the {Second} {ParlaCLARIN} {Workshop}},
	publisher = {European Language Resources Association},
	author = {Ogrodniczuk, Maciej and Nitoń, Bartłomiej},
	editor = {Fišer, Darja and Eskevich, Maria and de Jong, Franciska},
	month = may,
	year = {2020},
	pages = {1--4},
}

@inproceedings{reed_learning_2020,
	address = {1st virtual meeting},
	title = {Learning from {Mistakes}: {Combining} {Ontologies} via {Self}-{Training} for {Dialogue} {Generation}},
	url = {https://aclanthology.org/2020.sigdial-1.3/},
	doi = {10.18653/v1/2020.sigdial-1.3},
	abstract = {Natural language generators (NLGs) for task-oriented dialogue typically take a meaning representation (MR) as input, and are trained end-to-end with a corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies. Here we explore, for the first time, whether it is possible to train an NLG for a new larger ontology using existing training sets for the restaurant domain, where each set is based on a different ontology. We create a new, larger combined ontology, and then train an NLG to produce utterances covering it. For example, if one dataset has attributes for family friendly and rating information, and the other has attributes for decor and service, our aim is an NLG for the combined ontology that can produce utterances that realize values for family friendly, rating, decor and service. Initial experiments with a baseline neural sequence-to-sequence model show that this task is surprisingly challenging. We then develop a novel self-training method that identifies (errorful) model outputs, automatically constructs a corrected MR input to form a new (MR, utterance) training pair, and then repeatedly adds these new instances back into the training data. We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4\% improvement over the baseline model. We also report a human qualitative evaluation of the final model showing that it achieves high naturalness, semantic coherence and grammaticality.},
	booktitle = {Proceedings of the 21th {Annual} {Meeting} of the {Special} {Interest} {Group} on {Discourse} and {Dialogue}},
	publisher = {Association for Computational Linguistics},
	author = {Reed, Lena and Harrison, Vrindavan and Oraby, Shereen and Hakkani-Tur, Dilek and Walker, Marilyn},
	editor = {Pietquin, Olivier and Muresan, Smaranda and Chen, Vivian and Kennington, Casey and Vandyke, David and Dethlefs, Nina and Inoue, Koji and Ekstedt, Erik and Ultes, Stefan},
	month = jul,
	year = {2020},
	pages = {21--34},
}

@inproceedings{chen_reading_2020,
	address = {Online},
	title = {Reading the {Manual}: {Event} {Extraction} as {Definition} {Comprehension}},
	url = {https://aclanthology.org/2020.spnlp-1.9/},
	doi = {10.18653/v1/2020.spnlp-1.9},
	abstract = {We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension of an extraction framework by intended end-users through declarations such as, “Some person was born in some location at some time.” We introduce an example of a model that employs such statements, with experiments illustrating we can extract events under closed ontologies and generalize to unseen event types simply by reading new definitions.},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Structured} {Prediction} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Yunmo and Chen, Tongfei and Ebner, Seth and White, Aaron Steven and Van Durme, Benjamin},
	editor = {Agrawal, Priyanka and Kozareva, Zornitsa and Kreutzer, Julia and Lampouras, Gerasimos and Martins, André and Ravi, Sujith and Vlachos, Andreas},
	month = nov,
	year = {2020},
	pages = {74--83},
}

@inproceedings{zhang_find_2020,
	address = {Barcelona, Spain (Online)},
	title = {Find or {Classify}? {Dual} {Strategy} for {Slot}-{Value} {Predictions} on {Multi}-{Domain} {Dialog} {State} {Tracking}},
	url = {https://aclanthology.org/2020.starsem-1.17/},
	abstract = {Dialog state tracking (DST) is a core component in task-oriented dialog systems. Existing approaches for DST mainly fall into one of two categories, namely, ontology-based and ontology-free methods. An ontology-based method selects a value from a candidate-value list for each target slot, while an ontology-free method extracts spans from dialog contexts. Recent work introduced a BERT-based model to strike a balance between the two methods by pre-defining categorical and non-categorical slots. However, it is not clear enough which slots are better handled by either of the two slot types, and the way to use the pre-trained model has not been well investigated. In this paper, we propose a simple yet effective dual-strategy model for DST, by adapting a single BERT-style reading comprehension model to jointly handle both the categorical and non-categorical slots. Our experiments on the MultiWOZ datasets show that our method significantly outperforms the BERT-based counterpart, finding that the key is a deep interaction between the domain-slot and context information. When evaluated on noisy (MultiWOZ 2.0) and cleaner (MultiWOZ 2.1) settings, our method performs competitively and robustly across the two different settings. Our method sets the new state of the art in the noisy setting, while performing more robustly than the best model in the cleaner setting. We also conduct a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research.},
	booktitle = {Proceedings of the {Ninth} {Joint} {Conference} on {Lexical} and {Computational} {Semantics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Jianguo and Hashimoto, Kazuma and Wu, Chien-Sheng and Wang, Yao and Yu, Philip and Socher, Richard and Xiong, Caiming},
	editor = {Gurevych, Iryna and Apidianaki, Marianna and Faruqui, Manaal},
	month = dec,
	year = {2020},
	pages = {154--167},
}

@inproceedings{mahlaza_owlsiz_2020,
	address = {Dublin, Ireland (Virtual)},
	title = {{OWLSIZ}: {An} {isiZulu} {CNL} for structured knowledge validation},
	url = {https://aclanthology.org/2020.webnlg-1.2/},
	abstract = {In iterative knowledge elicitation, engineers are expected to be directly involved in validating the already captured knowledge and obtaining new knowledge increments, thus making the process time consuming. Languages such as English have controlled natural languages than can be repurposed to generate natural language questions from an ontology in order to allow a domain expert to independently validate the contents of an ontology without understanding a ontology authoring language such as OWL. IsiZulu, South Africa's main L1 language by number speakers, does not have such a resource, hence, it is not possible to build a verbaliser to generate such questions. Therefore, we propose an isiZulu controlled natural language, called OWL Simplified isiZulu (OWLSIZ), for producing grammatical and fluent questions from an ontology. Human evaluation of the generated questions showed that participants' judgements agree that most (83\%) questions are positive for grammaticality or understandability.},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Natural} {Language} {Generation} from the {Semantic} {Web} ({WebNLG}+)},
	publisher = {Association for Computational Linguistics},
	author = {Mahlaza, Zola and Keet, C. Maria},
	editor = {Castro Ferreira, Thiago and Gardent, Claire and Ilinykh, Nikolai and van der Lee, Chris and Mille, Simon and Moussallem, Diego and Shimorina, Anastasia},
	month = dec,
	year = {2020},
	pages = {15--25},
}

@inproceedings{woldemariyam_embedding_2020,
	address = {Seattle, USA},
	title = {Embedding {Oriented} {Adaptable} {Semantic} {Annotation} {Framework} for {Amharic} {Web} {Documents}},
	url = {https://aclanthology.org/2020.winlp-1.3/},
	doi = {10.18653/v1/2020.winlp-1.3},
	abstract = {The Web has become a source of information, where information is provided by humans for humans and its growth has increased necessity to get solutions that intelligently extract valuable knowledge from existing and newly added web documents with no (minimal) supervisions. However, due to the unstructured nature of existing data on the Web, effective extraction of this knowledge is limited for both human beings and software agents. Thus, this research work designed generic and embedding oriented framework that automatically annotates semantically Amharic web documents using ontology. This framework significantly reduces manual annotation and learning cost used for semantic annotation of Amharic web documents with its nature of adaptability with minimal modification. The results have also implied that neural network techniques are promising for semantic annotation, especially for less resourced languages like Amharic in comparison to language dependent techniques that have cost of speed and challenge of adaptation into new domains and languages. We experiment the feasibility of the proposed approach using Amharic news collected from WALTA news agency and Amharic Wikipedia. Our results show that the proposed solution exhibits 70.68\% of precision, 66.89\% of recall and 68.53\% of f-measure in semantic annotation for a morphologically complex Amharic language with limited size dataset.},
	booktitle = {Proceedings of the {Fourth} {Widening} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Woldemariyam, Kidane and Getahun, Dr. Fekade},
	editor = {Cunha, Rossana and Shaikh, Samira and Varis, Erika and Georgi, Ryan and Tsai, Alicia and Anastasopoulos, Antonios and Chandu, Khyathi Raghavi},
	month = jul,
	year = {2020},
	pages = {7},
}

@inproceedings{onoe_modeling_2021,
	address = {Online},
	title = {Modeling {Fine}-{Grained} {Entity} {Types} with {Box} {Embeddings}},
	url = {https://aclanthology.org/2021.acl-long.160/},
	doi = {10.18653/v1/2021.acl-long.160},
	abstract = {Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types' complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. Our model represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Onoe, Yasumasa and Boratko, Michael and McCallum, Andrew and Durrett, Greg},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {2051--2064},
}

@inproceedings{deng_ontoed_2021,
	address = {Online},
	title = {{OntoED}: {Low}-resource {Event} {Detection} with {Ontology} {Embedding}},
	url = {https://aclanthology.org/2021.acl-long.220/},
	doi = {10.18653/v1/2021.acl-long.220},
	abstract = {Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type. Most current methods to ED rely heavily on training instances, and almost ignore the correlation of event types. Hence, they tend to suffer from data scarcity and fail to handle new unseen event types. To address these problems, we formulate ED as a process of event ontology population: linking event instances to pre-defined event types in event ontology, and propose a novel ED framework entitled OntoED with ontology embedding. We enrich event ontology with linkages among event types, and further induce more event-event correlations. Based on the event ontology, OntoED can leverage and propagate correlation knowledge, particularly from data-rich to data-poor event types. Furthermore, OntoED can be applied to new unseen event types, by establishing linkages to existing ones. Experiments indicate that OntoED is more predominant and robust than previous approaches to ED, especially in data-scarce scenarios.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Deng, Shumin and Zhang, Ningyu and Li, Luoqiu and Hui, Chen and Huaixiao, Tou and Chen, Mosha and Huang, Fei and Chen, Huajun},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {2828--2839},
}

@inproceedings{zhang_fine-grained_2021,
	address = {Online},
	title = {Fine-grained {Information} {Extraction} from {Biomedical} {Literature} based on {Knowledge}-enriched {Abstract} {Meaning} {Representation}},
	url = {https://aclanthology.org/2021.acl-long.489/},
	doi = {10.18653/v1/2021.acl-long.489},
	abstract = {Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers. We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence. Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model's understanding of complex scientific concepts. We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks. Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8\% and 3.0\% absolute F-score gains respectively. In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Zixuan and Parulian, Nikolaus and Ji, Heng and Elsayed, Ahmed and Myers, Skatje and Palmer, Martha},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {6261--6270},
}

@inproceedings{cao_controllable_2021,
	address = {Online},
	title = {Controllable {Open}-ended {Question} {Generation} with {A} {New} {Question} {Type} {Ontology}},
	url = {https://aclanthology.org/2021.acl-long.502/},
	doi = {10.18653/v1/2021.acl-long.502},
	abstract = {We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences. We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words. A new dataset with 4,959 questions is labeled based on the new ontology. We then propose a novel question type-aware question generation framework, augmented by a semantic graph representation, to jointly predict question focuses and produce the question. Based on this framework, we further use both exemplars and automatically generated templates to improve controllability and diversity. Experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics. Human judges also rate our model outputs highly in answerability, coverage of scope, and overall quality. Finally, our model variants with templates can produce questions with enhanced controllability and diversity.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Shuyang and Wang, Lu},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {6424--6439},
}

@inproceedings{lyu_zero-shot_2021,
	address = {Online},
	title = {Zero-shot {Event} {Extraction} via {Transfer} {Learning}: {Challenges} and {Insights}},
	url = {https://aclanthology.org/2021.acl-short.42/},
	doi = {10.18653/v1/2021.acl-short.42},
	abstract = {Event extraction has long been a challenging task, addressed mostly with supervised methods that require expensive annotation and are not extensible to new event ontologies. In this work, we explore the possibility of zero-shot event extraction by formulating it as a set of Textual Entailment (TE) and/or Question Answering (QA) queries (e.g. “A city was attacked” entails “There is an attack”), exploiting pretrained TE/QA models for direct transfer. On ACE-2005 and ERE, our system achieves acceptable results, yet there is still a large gap from supervised approaches, showing that current QA and TE technologies fail in transferring to a different domain. To investigate the reasons behind the gap, we analyze the remaining key challenges, their respective impact, and possible improvement directions.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lyu, Qing and Zhang, Hongming and Sulem, Elior and Roth, Dan},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {322--332},
}

@inproceedings{harrigan_leveraging_2021,
	address = {Online},
	title = {Leveraging {English} {Word} {Embeddings} for {Semi}-{Automatic} {Semantic} {Classification} in {Nêhiyawêwin} ({Plains} {Cree})},
	url = {https://aclanthology.org/2021.americasnlp-1.12/},
	doi = {10.18653/v1/2021.americasnlp-1.12},
	abstract = {This paper details a semi-automatic method of word clustering for the Algonquian language, Nêhiyawêwin (Plains Cree). Although this method worked well, particularly for nouns, it required some amount of manual postprocessing. The main benefit of this approach over implementing an existing classification ontology is that this method approaches the language from an endogenous point of view, while performing classification quicker than in a fully manual context.},
	booktitle = {Proceedings of the {First} {Workshop} on {Natural} {Language} {Processing} for {Indigenous} {Languages} of the {Americas}},
	publisher = {Association for Computational Linguistics},
	author = {Harrigan, Atticus and Arppe, Antti},
	editor = {Mager, Manuel and Oncevay, Arturo and Rios, Annette and Ruiz, Ivan Vladimir Meza and Palmer, Alexis and Neubig, Graham and Kann, Katharina},
	month = jun,
	year = {2021},
	pages = {113--121},
}

@inproceedings{dacanay_more_2021,
	address = {Online},
	title = {The {More} {Detail}, the {Better}? – {Investigating} the {Effects} of {Semantic} {Ontology} {Specificity} on {Vector} {Semantic} {Classification} with a {Plains} {Cree} / nêhiyawêwin {Dictionary}},
	url = {https://aclanthology.org/2021.americasnlp-1.15/},
	doi = {10.18653/v1/2021.americasnlp-1.15},
	abstract = {One problem in the task of automatic semantic classification is the problem of determining the level on which to group lexical items. This is often accomplished using pre-made, hierarchical semantic ontologies. The following investigation explores the computational assignment of semantic classifications on the contents of a dictionary of nêhiyawêwin / Plains Cree (ISO: crk, Algonquian, Western Canada and United States), using a semantic vector space model, and following two semantic ontologies, WordNet and SIL's Rapid Words, and compares how these computational results compare to manual classifications with the same two ontologies.},
	booktitle = {Proceedings of the {First} {Workshop} on {Natural} {Language} {Processing} for {Indigenous} {Languages} of the {Americas}},
	publisher = {Association for Computational Linguistics},
	author = {Dacanay, Daniel and Harrigan, Atticus and Wolvengrey, Arok and Arppe, Antti},
	editor = {Mager, Manuel and Oncevay, Arturo and Rios, Annette and Ruiz, Ivan Vladimir Meza and Palmer, Alexis and Neubig, Graham and Kann, Katharina},
	month = jun,
	year = {2021},
	pages = {143--152},
}

@inproceedings{boschee_keynote_2021,
	address = {Online},
	title = {Keynote {Abstract}: {Events} on a {Global} {Scale}: {Towards} {Language}-{Agnostic} {Event} {Extraction}},
	url = {https://aclanthology.org/2021.case-1.2/},
	doi = {10.18653/v1/2021.case-1.2},
	abstract = {Event extraction is a challenging and exciting task in the world of machine learning \& natural language processing. The breadth of events of possible interest, the speed at which surrounding socio-political event contexts evolve, and the complexities involved in generating representative annotated data all contribute to this challenge. One particular dimension of difficulty is the intrinsically global nature of events: many downstream use cases for event extraction involve reporting not just in a few major languages but in a much broader context. The languages of interest for even a fixed task may still shift from day to day, e.g. when a disease emerges in an unexpected location. Early approaches to multi-lingual event extraction (e.g. ACE) relied wholly on supervised data provided in each language of interest. Later approaches leveraged the success of machine translation to side-step the issue, simply translating foreign-language content to English and deploying English models on the result (often leaving some significant portion of the original content behind). Most recently, however, the community has begun to shown significant progress applying zero-shot transfer techniques to the problem, developing models using supervised English data but decoding in a foreign language without translation, typically using embedding spaces specifically designed to capture multi-lingual semantic content. In this talk I will discuss multiple dimensions of these promising new approaches and the linguistic representations that underlie them. I will compare them with approaches based on machine translation (as well as with models trained using in-language training data, where available), and discuss their strengths and weaknesses in different contexts, including the amount of English/foreign bitext available and the nature of the target event ontology. I will also discuss possible future directions with an eye to improving the quality of event extraction no matter its source around the globe.},
	booktitle = {Proceedings of the 4th {Workshop} on {Challenges} and {Applications} of {Automated} {Extraction} of {Socio}-political {Events} from {Text} ({CASE} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Boschee, Elizabeth},
	editor = {Hürriyetoğlu, Ali},
	month = aug,
	year = {2021},
	pages = {10},
}

@inproceedings{eck_keynote_2021,
	address = {Online},
	title = {Keynote {Abstract}: {Machine} {Learning} in {Conflict} {Studies}: {Reflections} on {Ethics}, {Collaboration}, and {Ongoing} {Challenges}},
	url = {https://aclanthology.org/2021.case-1.3/},
	doi = {10.18653/v1/2021.case-1.3},
	abstract = {Advances in machine learning are nothing short of revolutionary in their potential to analyze massive amounts of data and in doing so, create new knowledge bases. But there is a responsibility in wielding the power to analyze these data since the public attributes a high degree of confidence to results which are based on big datasets. In this keynote, I will first address our ethical imperative as scholars to “get it right.” This imperative relates not only to model precision but also to the quality of the underlying data, and to whether the models inadvertently reproduce or obscure political biases in the source material. In considering the ethical imperative to get it right, it is also important to define what is “right”: what is considered an acceptable threshold for classification success needs to be understood in light of the project's objectives. I then reflect on the different topics and data which are sourced in this field. Much of the existing research has focused on identifying conflict events (e.g. battles), but scholars are also increasingly turning to ML approaches to address other facets of the conflict environment. Conflict event extraction has long been a challenge for the natural language processing (NLP) community because it requires sophisticated methods for defining event ontologies, creating language resources, and developing algorithmic approaches. NLP machine-learning tools are ill-adapted to the complex, often messy, and diverse data generated during conflicts. Relative to other types of NLP text corpora, conflicts tend to generate less textual data, and texts are generated non-systematically. Conflict-related texts are often lexically idiosyncratic and tend to be written differently across actors, periods, and conflicts. Event definition and adjudication present tough challenges in the context of conflict corpora. Topics which rely on other types of data may be better-suited to NLP and machine learning methods. For example, Twitter and other social media data lend themselves well to studying hate speech, public opinion, social polarization, or discursive aspects of conflictual environments. Likewise, government-produced policy documents have typically been analyzed with historical, qualitative methods but their standardized formats and quantity suggest that ML methods can provide new traction. ML approaches may also allow scholars to exploit local sources and multi-language sources to a greater degree than has been possible. Many challenges remain, and these are best addressed in collaborative projects which build on interdisciplinary expertise. Classification projects need to be anchored in the theoretical interests of scholars of political violence if the data they produce are to be put to analytical use. There are few ontologies for classification that adequately reflect conflict researchers' interests, which highlights the need for conceptual as well as technical development.},
	booktitle = {Proceedings of the 4th {Workshop} on {Challenges} and {Applications} of {Automated} {Extraction} of {Socio}-political {Events} from {Text} ({CASE} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Eck, Kristine},
	editor = {Hürriyetoğlu, Ali},
	month = aug,
	year = {2021},
	pages = {11},
}

@article{kouris_abstractive_2021,
	title = {Abstractive {Text} {Summarization}: {Enhancing} {Sequence}-to-{Sequence} {Models} {Using} {Word} {Sense} {Disambiguation} and {Semantic} {Content} {Generalization}},
	volume = {47},
	url = {https://aclanthology.org/2021.cl-4.27/},
	doi = {10.1162/coli_a_00417},
	abstract = {Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.},
	number = {4},
	journal = {Computational Linguistics},
	author = {Kouris, Panagiotis and Alexandridis, Georgios and Stafylopatis, Andreas},
	month = dec,
	year = {2021},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {813--859},
}

@inproceedings{saxena_leveraging_2021,
	address = {Online},
	title = {Leveraging {Wikipedia} {Navigational} {Templates} for {Curating} {Domain}-{Specific} {Fuzzy} {Conceptual} {Bases}},
	url = {https://aclanthology.org/2021.dash-1.1/},
	doi = {10.18653/v1/2021.dash-1.1},
	abstract = {Domain-specific conceptual bases use key concepts to capture domain scope and relevant information. Conceptual bases serve as a foundation for various downstream tasks, including ontology construction, information mapping, and analysis. However, building conceptual bases necessitates domain awareness and takes time. Wikipedia navigational templates offer multiple articles on the same/similar domain. It is possible to use the templates to recognize fundamental concepts that shape the domain. Earlier work in this domain used Wikipedia's structured and unstructured data to construct open-domain ontologies, domain terminologies, and knowledge bases. We present a novel method for leveraging navigational templates to create domain-specific fuzzy conceptual bases in this work. Our system generates knowledge graphs from the articles mentioned in the template, which we then process using Wikidata and machine learning algorithms. We filter important concepts using fuzzy logic on network metrics to create a crude conceptual base. Finally, the expert helps by refining the conceptual base. We demonstrate our system using an example of RNA virus antiviral drugs.},
	booktitle = {Proceedings of the {Second} {Workshop} on {Data} {Science} with {Human} in the {Loop}: {Language} {Advances}},
	publisher = {Association for Computational Linguistics},
	author = {Saxena, Krati and Singh, Tushita and Patil, Ashwini and Sunkle, Sagar and Kulkarni, Vinay},
	editor = {Dragut, Eduard and Li, Yunyao and Popa, Lucian and Vucetic, Slobodan},
	month = jun,
	year = {2021},
	pages = {1--7},
}

@inproceedings{li_zero-shot_2021,
	address = {Online},
	title = {Zero-shot {Generalization} in {Dialog} {State} {Tracking} through {Generative} {Question} {Answering}},
	url = {https://aclanthology.org/2021.eacl-main.91/},
	doi = {10.18653/v1/2021.eacl-main.91},
	abstract = {Dialog State Tracking (DST), an integral part of modern dialog systems, aims to track user preferences and constraints (slots) in task-oriented dialogs. In real-world settings with constantly changing services, DST systems must generalize to new domains and unseen slot types. Existing methods for DST do not generalize well to new slot names and many require known ontologies of slot types and values for inference. We introduce a novel ontology-free framework that supports natural language queries for unseen constraints and slots in multi-domain task-oriented dialogs. Our approach is based on generative question-answering using a conditional language model pre-trained on substantive English sentences. Our model improves joint goal accuracy in zero-shot domain adaptation settings by up to 9\% (absolute) over the previous state-of-the-art on the MultiWOZ 2.1 dataset.},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Shuyang and Cao, Jin and Sridhar, Mukund and Zhu, Henghui and Li, Shang-Wen and Hamza, Wael and McAuley, Julian},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	month = apr,
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Computational linguistics, Question Answering, Domain adaptation, Natural language processing systems, State of the art, Natural language queries, English sentences, Multi-domain tasks, Real world setting},
	pages = {1063--1074},
	annote = {Cited by: 32},
}

@inproceedings{falis_cophe_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{CoPHE}: {A} {Count}-{Preserving} {Hierarchical} {Evaluation} {Metric} in {Large}-{Scale} {Multi}-{Label} {Text} {Classification}},
	url = {https://aclanthology.org/2021.emnlp-main.69/},
	doi = {10.18653/v1/2021.emnlp-main.69},
	abstract = {Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Falis, Matúš and Dong, Hang and Birch, Alexandra and Alex, Beatrice},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {907--912},
}

@inproceedings{moghe_cross-lingual_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Cross-lingual {Intermediate} {Fine}-tuning improves {Dialogue} {State} {Tracking}},
	url = {https://aclanthology.org/2021.emnlp-main.87/},
	doi = {10.18653/v1/2021.emnlp-main.87},
	abstract = {Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English -{\textbackslash}ensuremath{\textgreater} Chinese, Chinese -{\textbackslash}ensuremath{\textgreater} English) and Multilingual WoZ (English -{\textbackslash}ensuremath{\textgreater} German, English -{\textbackslash}ensuremath{\textgreater} Italian) datasets. We achieve impressive improvements ({\textbackslash}ensuremath{\textgreater} 20\% on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10\% of the target language task data and zero-shot setup respectively.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Moghe, Nikita and Steedman, Mark and Birch, Alexandra},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {1137--1150},
}

@inproceedings{pyatkin_asking_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Asking {It} {All}: {Generating} {Contextualized} {Questions} for any {Semantic} {Role}},
	url = {https://aclanthology.org/2021.emnlp-main.108/},
	doi = {10.18653/v1/2021.emnlp-main.108},
	abstract = {Asking questions about a situation is an inherent step towards understanding it. To this end, we introduce the task of role question generation, which, given a predicate mention and a passage, requires producing a set of questions asking about all possible semantic roles of the predicate. We develop a two-stage model for this task, which first produces a context-independent question prototype for each role and then revises it to be contextually appropriate for the passage. Unlike most existing approaches to question generation, our approach does not require conditioning on existing answers in the text. Instead, we condition on the type of information to inquire about, regardless of whether the answer appears explicitly in the text, could be inferred from it, or should be sought elsewhere. Our evaluation demonstrates that we generate diverse and well-formed questions for a large, broad-coverage ontology of predicates and roles.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Pyatkin, Valentina and Roit, Paul and Michael, Julian and Goldberg, Yoav and Tsarfaty, Reut and Dagan, Ido},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {1429--1441},
}

@inproceedings{li_generation_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Generation and {Extraction} {Combined} {Dialogue} {State} {Tracking} with {Hierarchical} {Ontology} {Integration}},
	url = {https://aclanthology.org/2021.emnlp-main.171/},
	doi = {10.18653/v1/2021.emnlp-main.171},
	abstract = {Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes severer. Current models are not satisfactory for solving the challenges of ontology integration between domains and out-of-vocabulary problems. To address the problem, we explore the hierarchical semantic of ontology and enhance the interrelation between slots with masked hierarchical attention. In state value decoding stage, we solve the out-of-vocabulary problem by combining generation method and extraction method together. We evaluate the performance of our model on two representative datasets, MultiWOZ in English and CrossWOZ in Chinese. The results show that our model yields a significant performance gain over current state-of-the-art state tracking model and it is more robust to out-of-vocabulary problem compared with other methods.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Xinmeng and Li, Qian and Wu, Wansen and Yin, Quanjun},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {2241--2249},
}

@inproceedings{wang_chemner_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{ChemNER}: {Fine}-{Grained} {Chemistry} {Named} {Entity} {Recognition} with {Ontology}-{Guided} {Distant} {Supervision}},
	url = {https://aclanthology.org/2021.emnlp-main.424/},
	doi = {10.18653/v1/2021.emnlp-main.424},
	abstract = {Scientific literature analysis needs fine-grained named entity recognition (NER) to provide a wide range of information for scientific discovery. For example, chemistry research needs to study dozens to hundreds of distinct, fine-grained entity types, making consistent and accurate annotation difficult even for crowds of domain experts. On the other hand, domain-specific ontologies and knowledge bases (KBs) can be easily accessed, constructed, or integrated, which makes distant supervision realistic for fine-grained chemistry NER. In distant supervision, training labels are generated by matching mentions in a document with the concepts in the knowledge bases (KBs). However, this kind of KB-matching suffers from two major challenges: incomplete annotation and noisy annotation. We propose ChemNER, an ontology-guided, distantly-supervised method for fine-grained chemistry NER to tackle these challenges. It leverages the chemistry type ontology structure to generate distant labels with novel methods of flexible KB-matching and ontology-guided multi-type disambiguation. It significantly improves the distant label generation for the subsequent sequence labeling model training. We also provide an expert-labeled, chemistry NER dataset with 62 fine-grained chemistry types (e.g., chemical compounds and chemical reactions). Experimental results show that ChemNER is highly effective, outperforming substantially the state-of-the-art NER methods (with .25 absolute F1 score improvement).},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Xuan and Hu, Vivian and Song, Xiangchen and Garg, Shweta and Xiao, Jinfeng and Han, Jiawei},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {5227--5240},
}

@inproceedings{yu_lifelong_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Lifelong {Event} {Detection} with {Knowledge} {Transfer}},
	url = {https://aclanthology.org/2021.emnlp-main.428/},
	doi = {10.18653/v1/2021.emnlp-main.428},
	abstract = {Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from unstructured data, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE tasks. In event detection and more general IE tasks, rich correlations or semantic relatedness exist among hierarchical knowledge element types. In our proposed framework, knowledge is being transferred between learned old event types and new event types. Specifically, we update old knowledge with new event types' mentions using a self-training loss. In addition, we aggregate old event types' representations based on their similarities with new event types to initialize the new event types' representations. Experimental results show that our framework outperforms competitive baselines with a 5.1\% absolute gain in the F1 score. Moreover, our proposed framework can boost the F1 score for over 30\% absolute gain on some new long-tail rare event types with few training instances. Our knowledge transfer module improves performance on both learned event types and new event types under the lifelong learning setting, showing that it helps consolidate old knowledge and improve novel knowledge acquisition.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Pengfei and Ji, Heng and Natarajan, Prem},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {5278--5290},
}

@inproceedings{iyer_veealign_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{VeeAlign}: {Multifaceted} {Context} {Representation} {Using} {Dual} {Attention} for {Ontology} {Alignment}},
	url = {https://aclanthology.org/2021.emnlp-main.842/},
	doi = {10.18653/v1/2021.emnlp-main.842},
	abstract = {Ontology Alignment is an important research problem applied to various fields such as data integration, data transfer, data preparation, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domain-specific architectures, making them unscalable and inefficient. In this work, we propose VeeAlign, a Deep Learning based model that uses a novel dual-attention mechanism to compute the contextualized representation of a concept which, in turn, is used to discover alignments. By doing this, not only is our approach able to exploit both syntactic and semantic information encoded in ontologies, it is also, by design, flexible and scalable to different domains with minimal effort. We evaluate our model on four different datasets from different domains and languages, and establish its superiority through these results as well as detailed ablation studies. The code and datasets used are available at https://github.com/Remorax/VeeAlign.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Iyer, Vivek and Agarwal, Arvind and Kumar, Harshit},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {10780--10792},
}

@inproceedings{shrivastava_span_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Span {Pointer} {Networks} for {Non}-{Autoregressive} {Task}-{Oriented} {Semantic} {Parsing}},
	url = {https://aclanthology.org/2021.findings-emnlp.161/},
	doi = {10.18653/v1/2021.findings-emnlp.161},
	abstract = {An effective recipe for building seq2seq, non-autoregressive, task-oriented parsers to map utterances to semantic frames proceeds in three steps: encoding an utterance x, predicting a frame's length {\textbar}y{\textbar}, and decoding a {\textbar}y{\textbar}-sized frame with utterance and ontology tokens. Though empirically strong, these models are typically bottlenecked by length prediction, as even small inaccuracies change the syntactic and semantic characteristics of resulting frames. In our work, we propose span pointer networks, non-autoregressive parsers which shift the decoding task from text generation to span prediction; that is, when imputing utterance spans into frame slots, our model produces endpoints (e.g., [i, j]) as opposed to text (e.g., “6pm”). This natural quantization of the output space reduces the variability of gold frames, therefore improving length prediction and, ultimately, exact match. Furthermore, length prediction is now responsible for frame syntax and the decoder is responsible for frame semantics, resulting in a coarse-to-fine model. We evaluate our approach on several task-oriented semantic parsing datasets. Notably, we bridge the quality gap between non-autogressive and autoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020). Furthermore, due to our more consistent gold frames, we show strong improvements in model generalization in both cross-domain and cross-lingual transfer in low-resource settings. Finally, due to our diminished output vocabulary, we observe 70\% reduction in latency and 83\% reduction in memory at beam size 5 compared to prior non-autoregressive parsers.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Shrivastava, Akshat and Chuang, Pierce and Babu, Arun and Desai, Shrey and Arora, Abhinav and Zotov, Alexander and Aly, Ahmed},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {1873--1886},
}

@inproceedings{chen_nuanced_2021,
	address = {Punta Cana, Dominican Republic},
	title = {{NUANCED}: {Natural} {Utterance} {Annotation} for {Nuanced} {Conversation} with {Estimated} {Distributions}},
	url = {https://aclanthology.org/2021.findings-emnlp.337/},
	doi = {10.18653/v1/2021.findings-emnlp.337},
	abstract = {Existing conversational systems are mostly agent-centric, which assumes the user utterances will closely follow the system ontology. However, in real-world scenarios, it is highly desirable that users can speak freely and naturally. In this work, we attempt to build a user-centric dialogue system for conversational recommendation. As there is no clean mapping for a user's free form utterance to an ontology, we first model the user preferences as estimated distributions over the system ontology and map the user's utterances to such distributions. Learning such a mapping poses new challenges on reasoning over various types of knowledge, ranging from factoid knowledge, commonsense knowledge to the users' own situations. To this end, we build a new dataset named NUANCED that focuses on such realistic settings, with 5.1k dialogues, 26k turns of high-quality user responses. We conduct experiments, showing both the usefulness and challenges of our problem setting. We believe NUANCED can serve as a valuable resource to push existing research from the agent-centric system to the user-centric system. The code and data are publicly available.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Zhiyu and Liu, Honglei and Xu, Hu and Moon, Seungwhan and Zhou, Hao and Liu, Bing},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {4016--4024},
}

@inproceedings{svetla_towards_2021,
	address = {University of South Africa (UNISA)},
	title = {Towards {Expanding} {WordNet} with {Conceptual} {Frames}},
	url = {https://aclanthology.org/2021.gwc-1.21/},
	abstract = {The paper presents the project Semantic Network with a Wide Range of Semantic Relations and its main achievements. The ultimate objective of the project is to expand Princeton WordNet with conceptual frames that define the syntagmatic relations of verb synsets and the semantic classes of nouns felicitous to combine with particular verbs. At this stage of the work: a) over 5,000 WordNet verb synsets have been supplied with manually evaluated FrameNet semantic frames, b) 253 semantic types have been manually mapped to the appropriate WordNet concepts providing detailed ontological representation of the semantic classes of nouns.},
	booktitle = {Proceedings of the 11th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Svetla, Koeva},
	editor = {Vossen, Piek and Fellbaum, Christiane},
	month = jan,
	year = {2021},
	pages = {182--191},
}

@inproceedings{weber_zero-shot_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Zero-{Shot} {Cross}-{Lingual} {Transfer} is a {Hard} {Baseline} to {Beat} in {German} {Fine}-{Grained} {Entity} {Typing}},
	url = {https://aclanthology.org/2021.insights-1.7/},
	doi = {10.18653/v1/2021.insights-1.7},
	abstract = {The training of NLP models often requires large amounts of labelled training data, which makes it difficult to expand existing models to new languages. While zero-shot cross-lingual transfer relies on multilingual word embeddings to apply a model trained on one language to another, Yarowski and Ngai (2001) propose the method of annotation projection to generate training data without manual annotation. This method was successfully used for the tasks of named entity recognition and coarse-grained entity typing, but we show that it is outperformed by zero-shot cross-lingual transfer when applied to the similar task of fine-grained entity typing. In our study of fine-grained entity typing with the FIGER type ontology for German, we show that annotation projection amplifies the English model's tendency to underpredict level 2 labels and is beaten by zero-shot cross-lingual transfer on three novel test sets.},
	booktitle = {Proceedings of the {Second} {Workshop} on {Insights} from {Negative} {Results} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Weber, Sabine and Steedman, Mark},
	editor = {Sedoc, João and Rogers, Anna and Rumshisky, Anna and Tafreshi, Shabnam},
	month = nov,
	year = {2021},
	pages = {42--48},
}

@inproceedings{stowe_semlink_2021,
	address = {Groningen, The Netherlands (online)},
	title = {{SemLink} 2.0: {Chasing} {Lexical} {Resources}},
	url = {https://aclanthology.org/2021.iwcs-1.21/},
	abstract = {The SemLink resource provides mappings between a variety of lexical semantic ontologies, each with their strengths and weaknesses. To take advantage of these differences, the ability to move between resources is essential. This work describes advances made to improve the usability of the SemLink resource: the automatic addition of new instances and mappings, manual corrections, sense-based vectors and collocation information, and architecture built to automatically update the resource when versions of the underlying resources change. These updates improve coverage, provide new tools to leverage the capabilities of these resources, and facilitate seamless updates, ensuring the consistency and applicability of these mappings in the future.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Computational} {Semantics} ({IWCS})},
	publisher = {Association for Computational Linguistics},
	author = {Stowe, Kevin and Preciado, Jenette and Conger, Kathryn and Brown, Susan Windisch and Kazeminejad, Ghazaleh and Gung, James and Palmer, Martha},
	editor = {Zarrieß, Sina and Bos, Johan and van Noord, Rik and Abzianidze, Lasha},
	month = jun,
	year = {2021},
	pages = {222--227},
}

@inproceedings{schleider_zero-shot_2021,
	address = {Punta Cana, Dominican Republic (online)},
	title = {Zero-{Shot} {Information} {Extraction} to {Enhance} a {Knowledge} {Graph} {Describing} {Silk} {Textiles}},
	url = {https://aclanthology.org/2021.latechclfl-1.16/},
	doi = {10.18653/v1/2021.latechclfl-1.16},
	abstract = {The knowledge of the European silk textile production is a typical case for which the information collected is heterogeneous, spread across many museums and sparse since rarely complete. Knowledge Graphs for this cultural heritage domain, when being developed with appropriate ontologies and vocabularies, enable to integrate and reconcile this diverse information. However, many of these original museum records still have some metadata gaps. In this paper, we present a zero-shot learning approach that leverages the ConceptNet common sense knowledge graph to predict categorical metadata informing about the silk objects production. We compared the performance of our approach with traditional supervised deep learning-based methods that do require training data. We demonstrate promising and competitive performance for similar datasets and circumstances and the ability to predict sometimes more fine-grained information. Our results can be reproduced using the code and datasets published at https://github.com/silknow/ZSL-KG-silk.},
	booktitle = {Proceedings of the 5th {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	publisher = {Association for Computational Linguistics},
	author = {Schleider, Thomas and Troncy, Raphael},
	editor = {Degaetano-Ortlieb, Stefania and Kazantseva, Anna and Reiter, Nils and Szpakowicz, Stan},
	month = nov,
	year = {2021},
	pages = {138--146},
}

@inproceedings{nan_dart_2021,
	address = {Online},
	title = {{DART}: {Open}-{Domain} {Structured} {Data} {Record} to {Text} {Generation}},
	url = {https://aclanthology.org/2021.naacl-main.37/},
	doi = {10.18653/v1/2021.naacl-main.37},
	abstract = {We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Nan, Linyong and Radev, Dragomir and Zhang, Rui and Rau, Amrit and Sivaprasad, Abhinand and Hsieh, Chiachun and Tang, Xiangru and Vyas, Aadit and Verma, Neha and Krishna, Pranav and Liu, Yangxiaokang and Irwanto, Nadia and Pan, Jessica and Rahman, Faiaz and Zaidi, Ahmad and Mutuma, Mutethia and Tarabar, Yasin and Gupta, Ankit and Yu, Tao and Tan, Yi Chern and Lin, Xi Victoria and Xiong, Caiming and Socher, Richard and Rajani, Nazneen Fatema},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {432--447},
}

@inproceedings{prabhumoye_case_2021,
	address = {Online},
	title = {Case {Study}: {Deontological} {Ethics} in {NLP}},
	url = {https://aclanthology.org/2021.naacl-main.297/},
	doi = {10.18653/v1/2021.naacl-main.297},
	abstract = {Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Prabhumoye, Shrimai and Boldt, Brendon and Salakhutdinov, Ruslan and Black, Alan W},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {3784--3798},
}

@inproceedings{liu_self-alignment_2021,
	address = {Online},
	title = {Self-{Alignment} {Pretraining} for {Biomedical} {Entity} {Representations}},
	url = {https://aclanthology.org/2021.naacl-main.334/},
	doi = {10.18653/v1/2021.naacl-main.334},
	abstract = {Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Fangyu and Shareghi, Ehsan and Meng, Zaiqiao and Basaldella, Marco and Collier, Nigel},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Language model, Semantics, Pre-training, Semantic relationships, Computational linguistics, Biomedical domain, Fine grained, State of the art, Entity-level, Hybrid systems, Model entities, Self-align, Self-alignment},
	pages = {4228--4238},
	annote = {Cited by: 206; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{pham_legal_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Legal {Terminology} {Extraction} with the {Termolator}},
	url = {https://aclanthology.org/2021.nllp-1.16/},
	doi = {10.18653/v1/2021.nllp-1.16},
	abstract = {Domain-specific terminology is ubiquitous in legal documents. Despite potential utility in populating glossaries and ontologies or as arguments in information extraction and document classification tasks, there has been limited work done for legal terminology extraction. This paper describes some work to remedy this omission. In the described research, we make some modifications to the Termolator, a high-performing, open-source terminology extractor which has been tuned to scientific articles. Our changes are designed to improve the Termolator's results when applied to United States Supreme Court decisions. Unaltered and using the recommended settings, the original Termolator provides a list of terminology with a precision of 23\% and 25\% for the categories of economic activity (development set) and criminal procedures (test set) respectively. These were the most frequently occurring broad issues in Washington University in St. Louis Database corpus, a database of Supreme Court decisions that have been manually classified by topic. Our contribution includes the introduction of several legal domain-specific filtration steps and changes to the web search relevance score; each incrementally improved precision culminating in a combined precision of 63\% and 65\%. We also evaluated the baseline version of the Termolator on more specific subcategories and on broad issues with fewer cases. Our results show that a narrowed scope as well as smaller document numbers significantly lower the precision. In both cases, the modifications to the Termolator improve precision.},
	booktitle = {Proceedings of the {Natural} {Legal} {Language} {Processing} {Workshop} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Pham, Nhi and Pham, Lachlan and Meyers, Adam L.},
	editor = {Aletras, Nikolaos and Androutsopoulos, Ion and Barrett, Leslie and Goanta, Catalina and Preotiuc-Pietro, Daniel},
	month = nov,
	year = {2021},
	pages = {155--162},
}

@inproceedings{aceta_ontology_2021,
	address = {Held Online},
	title = {Ontology {Population} {Reusing} {Resources} for {Dialogue} {Intent} {Detection}: {Generic} and {Multilingual} {Approach}},
	url = {https://aclanthology.org/2021.ranlp-1.2/},
	abstract = {This work presents a generic semi-automatic strategy to populate the domain ontology of an ontology-driven task-oriented dialogue system, with the aim of performing successful intent detection in the dialogue process, reusing already existing multilingual resources. This semi-automatic approach allows ontology engineers to exploit available resources so as to associate the potential situations in the use case to FrameNet frames and obtain the relevant lexical units associated to them in the target language, following lexical and semantic criteria, without linguistic expert knowledge. This strategy has been validated and evaluated in two use cases, from industrial scenarios, for interaction in Spanish with a guide robot and with a Computerized Maintenance Management System (CMMS). In both cases, this method has allowed the ontology engineer to instantiate the domain ontology with the intent-relevant information with quality data in a simple and low-resource-consuming manner.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Aceta, Cristina and Fernández, Izaskun and Soroa, Aitor},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2021},
	pages = {10--18},
}

@inproceedings{hristov_application_2021,
	address = {Held Online},
	title = {Application of {Deep} {Learning} {Methods} to {SNOMED} {CT} {Encoding} of {Clinical} {Texts}: {From} {Data} {Collection} to {Extreme} {Multi}-{Label} {Text}-{Based} {Classification}},
	url = {https://aclanthology.org/2021.ranlp-1.63/},
	abstract = {Concept normalization of clinical texts to standard medical classifications and ontologies is a task with high importance for healthcare and medical research. We attempt to solve this problem through automatic SNOMED CT encoding, where SNOMED CT is one of the most widely used and comprehensive clinical term ontologies. Applying basic Deep Learning models, however, leads to undesirable results due to the unbalanced nature of the data and the extreme number of classes. We propose a classification procedure that features a multiple-step workflow consisting of label clustering, multi-cluster classification, and clusters-to-labels mapping. For multi-cluster classification, BioBERT is fine-tuned over our custom dataset. The clusters-to-labels mapping is carried out by a one-vs-all classifier (SVC) applied to every single cluster. We also present the steps for automatic dataset generation of textual descriptions annotated with SNOMED CT codes based on public data and linked open data. In order to cope with the problem that our dataset is highly unbalanced, some data augmentation methods are applied. The results from the conducted experiments show high accuracy and reliability of our approach for prediction of SNOMED CT codes relevant to a clinical text.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Hristov, Anton and Tahchiev, Aleksandar and Papazov, Hristo and Tulechki, Nikola and Primov, Todor and Boytcheva, Svetla},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2021},
	pages = {557--565},
}

@inproceedings{koeva_multilingual_2021,
	address = {Held Online},
	title = {Multilingual {Image} {Corpus}: {Annotation} {Protocol}},
	url = {https://aclanthology.org/2021.ranlp-1.80/},
	abstract = {In this paper, we present work in progress aimed at the development of a new image dataset with annotated objects. The Multilingual Image Corpus consists of an ontology of visual objects (based on WordNet) and a collection of thematically related images annotated with segmentation masks and object classes. We identified 277 dominant classes and 1,037 parent and attribute classes, and grouped them into 10 thematic domains such as sport, medicine, education, food, security, etc. For the selected classes a large-scale web image search is being conducted in order to compile a substantial collection of high-quality copyright free images. The focus of the paper is the annotation protocol which we established to facilitate the annotation process: the Ontology of visual objects and the conventions for image selection and for object segmentation. The dataset is designed both for image classification and object detection and for semantic segmentation. In addition, the object annotations will be supplied with multilingual descriptions by using freely available wordnets.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Koeva, Svetla},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2021},
	pages = {701--707},
}

@inproceedings{sahnoun_multilingual_2021,
	address = {Held Online},
	title = {Multilingual {Epidemic} {Event} {Extraction} : {From} {Simple} {Classification} {Methods} to {Open} {Information} {Extraction} ({OIE}) and {Ontology}},
	url = {https://aclanthology.org/2021.ranlp-1.138/},
	abstract = {There is an incredible amount of information available in the form of textual documents due to the growth of information sources. In order to get the information into an actionable way, it is common to use information extraction and more specifically the event extraction, it became crucial in various domains even in public health. In this paper, we address the problem of the epidemic event extraction in potentially any language, so that we tested different corpuses on an existed multilingual system for tele-epidemiology: the Data Analysis for Information Extraction in any Language(DANIEL) system. We focused on the influence of the number of documents on the performance of the system, on average results show that it is able to achieve a precision and recall around 82\%, but when we resorted to the evaluation by event by checking whether it has been really detected or not, the results are not satisfactory according to this paper's evaluation. Our idea is to propose a system that uses an ontology which includes information in different languages and covers specific epidemiological concepts, it is also based on the multilingual open information extraction for the relation extraction step to reduce the expert intervention and to restrict the content for each text. We describe a methodology of five main stages: Pre-processing, relation extraction, named entity recognition (NER), event recognition and the matching between the information extracted and the ontology.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Sahnoun, Sihem and Lejeune, Gaël},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2021},
	pages = {1227--1233},
}

@inproceedings{velichkov_comparative_2021,
	address = {Held Online},
	title = {Comparative {Analysis} of {Fine}-tuned {Deep} {Learning} {Language} {Models} for {ICD}-10 {Classification} {Task} for {Bulgarian} {Language}},
	url = {https://aclanthology.org/2021.ranlp-1.162/},
	doi = {10.26615/978-954-452-072-4_162},
	abstract = {The task of automatic diagnosis encoding into standard medical classifications and ontologies, is of great importance in medicine - both to support the daily tasks of physicians in the preparation and reporting of clinical documentation, and for automatic processing of clinical reports. In this paper we investigate the application and performance of different deep learning transformers for automatic encoding in ICD-10 of clinical texts in Bulgarian. The comparative analysis attempts to find which approach is more efficient to be used for fine-tuning of pretrained BERT family transformer to deal with a specific domain terminology on a rare language as Bulgarian. On the one side are used SlavicBERT and MultiligualBERT, that are pretrained for common vocabulary in Bulgarian, but lack medical terminology. On the other hand in the analysis are used BioBERT, ClinicalBERT, SapBERT, BlueBERT, that are pretrained for medical terminology in English, but lack training for language models in Bulgarian, and more over for vocabulary in Cyrillic. In our research study all BERT models are fine-tuned with additional medical texts in Bulgarian and then applied to the classification task for encoding medical diagnoses in Bulgarian into ICD-10 codes. Big corpora of diagnosis in Bulgarian annotated with ICD-10 codes is used for the classification task. Such an analysis gives a good idea of which of the models would be suitable for tasks of a similar type and domain. The experiments and evaluation results show that both approaches have comparable accuracy.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Velichkov, Boris and Vassileva, Sylvia and Gerginov, Simeon and Kraychev, Boris and Ivanov, Ivaylo and Ivanov, Philip and Koychev, Ivan and Boytcheva, Svetla},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Terminology, Language model, Deep learning, Medical ontology, Diagnosis, Clinical documentation, Computational linguistics, Medical classification, Medical terminologies, Signal encoding, Encoding (symbols), Classification tasks, Comparative analyzes, Automatic diagnosis, Daily tasks, Learning languages},
	pages = {1448--1454},
	annote = {Cited by: 1; All Open Access; Bronze Open Access},
}

@inproceedings{dayanik_using_2021,
	address = {Online},
	title = {Using {Hierarchical} {Class} {Structure} to {Improve} {Fine}-{Grained} {Claim} {Classification}},
	url = {https://aclanthology.org/2021.spnlp-1.6/},
	doi = {10.18653/v1/2021.spnlp-1.6},
	abstract = {The analysis of public debates crucially requires the classification of political demands according to hierarchical \textit{claim ontologies} (e.g. for immigration, a supercategory “Controlling Migration” might have subcategories “Asylum limit” or “Border installations”). A major challenge for automatic claim classification is the large number and low frequency of such subclasses. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding soft constraints in the claim classifier and (b) imposing hard constraints via Integer Linear Programming. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches.},
	booktitle = {Proceedings of the 5th {Workshop} on {Structured} {Prediction} for {NLP} ({SPNLP} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Dayanik, Erenay and Blessing, Andre and Blokker, Nico and Haunss, Sebastian and Kuhn, Jonas and Lapesa, Gabriella and Padó, Sebastian},
	editor = {Kozareva, Zornitsa and Ravi, Sujith and Vlachos, Andreas and Agrawal, Priyanka and Martins, André},
	month = aug,
	year = {2021},
	pages = {53--60},
}

@inproceedings{xu_how_2022,
	address = {Dublin, Ireland},
	title = {How {Do} {We} {Answer} {Complex} {Questions}: {Discourse} {Structure} of {Long}-form {Answers}},
	url = {https://aclanthology.org/2022.acl-long.249/},
	doi = {10.18653/v1/2022.acl-long.249},
	abstract = {Long-form answers, consisting of multiple sentences, can provide nuanced and comprehensive answers to a broader set of questions. To better understand this complex and understudied task, we study the functional structure of long-form answers collected from three datasets, ELI5, WebGPT and Natural Questions. Our main goal is to understand how humans organize information to craft complex answers. We develop an ontology of six sentence-level functional roles for long-form answers, and annotate 3.9k sentences in 640 answer paragraphs. Different answer collection methods manifest in different discourse structures. We further analyze model-generated answers – finding that annotators agree less with each other when annotating model-generated answers compared to annotating human-written answers. Our annotated data enables training a strong classifier that can be used for automatic analysis. We hope our work can inspire future research on discourse-level modeling and evaluation of long-form QA systems.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Fangyuan and Li, Junyi Jessy and Choi, Eunsol},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {3556--3572},
}

@inproceedings{hu_graph_2022,
	address = {Dublin, Ireland},
	title = {Graph {Enhanced} {Contrastive} {Learning} for {Radiology} {Findings} {Summarization}},
	url = {https://aclanthology.org/2022.acl-long.320/},
	doi = {10.18653/v1/2022.acl-long.320},
	abstract = {The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians. Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists, and thus automatic impression generation has attracted substantial attention. With the encoder-decoder framework, most previous studies explore incorporating extra knowledge (e.g., static pre-defined clinical ontologies or extra background information). Yet, they encode such knowledge by a separate encoder to treat it as an extra input to their models, which is limited in leveraging their relations with the original findings. To address the limitation, we propose a unified framework for exploiting both extra knowledge and the original findings in an integrated way so that the critical information (i.e., key words and their relations) can be extracted in an appropriate way to facilitate impression generation. In detail, for each input findings, it is encoded by a text encoder and a graph is constructed through its entities and dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is adopted to model relation information in the constructed graph. Finally, to emphasize the key words in the findings, contrastive learning is introduced to map positive samples (constructed by masking non-key words) closer and push apart negative ones (constructed by masking key words). The experimental results on two datasets, OpenI and MIMIC-CXR, confirm the effectiveness of our proposed method, where the state-of-the-art results are achieved.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Jinpeng and Li, Zhuo and Chen, Zhihong and Li, Zhen and Wan, Xiang and Chang, Tsung-Hui},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {4677--4688},
}

@inproceedings{dou_is_2022,
	address = {Dublin, Ireland},
	title = {Is {GPT}-3 {Text} {Indistinguishable} from {Human} {Text}? {Scarecrow}: {A} {Framework} for {Scrutinizing} {Machine} {Text}},
	volume = {1},
	url = {https://aclanthology.org/2022.acl-long.501/},
	doi = {10.18653/v1/2022.acl-long.501},
	abstract = {Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation. We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow—such as redundancy, commonsense errors, and incoherence—are identified through several rounds of crowd annotation experiments without a predefined ontology. We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dou, Yao and Forbes, Maxwell and Koncel-Kedziorski, Rik and Smith, Noah A. and Choi, Yejin},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Quality control, Decoding, Computational linguistics, Training data, Ontology's, Research communities, Large models, Errors, English languages, Fluents, Machine errors, Text evaluation, Time configuration},
	pages = {7250--7274},
	annote = {Cited by: 81; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{cao_program_2022,
	address = {Dublin, Ireland},
	title = {Program {Transfer} for {Answering} {Complex} {Questions} over {Knowledge} {Bases}},
	url = {https://aclanthology.org/2022.acl-long.559/},
	doi = {10.18653/v1/2022.acl-long.559},
	abstract = {Program induction for answering complex questions over knowledge bases (KBs) aims to decompose a question into a multi-step program, whose execution against the KB produces the final answer. Learning to induce programs relies on a large number of parallel question-program pairs for the given KB. However, for most KBs, the gold program annotations are usually lacking, making learning difficult. In this paper, we propose the approach of program transfer, which aims to leverage the valuable program annotations on the rich-resourced KBs as external supervision signals to aid program induction for the low-resourced KBs that lack program annotations. For program transfer, we design a novel two-stage parsing framework with an efficient ontology-guided pruning strategy. First, a sketch parser translates the question into a high-level program sketch, which is the composition of functions. Second, given the question and sketch, an argument parser searches the detailed arguments from the KB for functions. During the searching, we incorporate the KB ontology to prune the search space. The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly, demonstrating the effectiveness of program transfer and our framework. Our codes and datasets can be obtained from https://github.com/THU-KEG/ProgramTransfer.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Shulin and Shi, Jiaxin and Yao, Zijun and Lv, Xin and Yu, Jifan and Hou, Lei and Li, Juanzi and Liu, Zhiyuan and Xiao, Jinghui},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {8128--8140},
}

@inproceedings{socrates_extraction_2022,
	address = {Dublin, Ireland},
	title = {Extraction of {Diagnostic} {Reasoning} {Relations} for {Clinical} {Knowledge} {Graphs}},
	url = {https://aclanthology.org/2022.acl-srw.33/},
	doi = {10.18653/v1/2022.acl-srw.33},
	abstract = {Clinical knowledge graphs lack meaningful diagnostic relations (e.g. comorbidities, sign/symptoms), limiting their ability to represent real-world diagnostic processes. Previous methods in biomedical relation extraction have focused on concept relations, such as gene-disease and disease-drug, and largely ignored clinical processes. In this thesis, we leverage a clinical reasoning ontology and propose methods to extract such relations from a physician-facing point-of-care reference wiki and consumer health resource texts. Given the lack of data labeled with diagnostic relations, we also propose new methods of evaluating the correctness of extracted triples in the zero-shot setting. We describe a process for the intrinsic evaluation of new facts by triple confidence filtering and clinician manual review, as well extrinsic evaluation in the form of a differential diagnosis prediction task.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Student} {Research} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Socrates, Vimig},
	editor = {Louvan, Samuel and Madotto, Andrea and Madureira, Brielen},
	month = may,
	year = {2022},
	pages = {413--421},
}

@inproceedings{falis_horses_2022,
	address = {Dublin, Ireland},
	title = {Horses to {Zebras}: {Ontology}-{Guided} {Data} {Augmentation} and {Synthesis} for {ICD}-9 {Coding}},
	url = {https://aclanthology.org/2022.bionlp-1.39/},
	doi = {10.18653/v1/2022.bionlp-1.39},
	abstract = {Medical document coding is the process of assigning labels from a structured label space (ontology – e.g., ICD-9) to medical documents. This process is laborious, costly, and error-prone. In recent years, efforts have been made to automate this process with neural models. The label spaces are large (in the order of thousands of labels) and follow a big-head long-tail label distribution, giving rise to few-shot and zero-shot scenarios. Previous efforts tried to address these scenarios within the model, leading to improvements on rare labels, but worse results on frequent ones. We propose data augmentation and synthesis techniques in order to address these scenarios. We further introduce an analysis technique for this setting inspired by confusion matrices. This analysis technique points to the positive impact of data augmentation and synthesis, but also highlights more general issues of confusion within families of codes, and underprediction.},
	booktitle = {Proceedings of the 21st {Workshop} on {Biomedical} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Falis, Matúš and Dong, Hang and Birch, Alexandra and Alex, Beatrice},
	editor = {Demner-Fushman, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
	month = may,
	year = {2022},
	pages = {389--401},
}

@inproceedings{kim_snu-causality_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {{SNU}-{Causality} {Lab} @ {Causal} {News} {Corpus} 2022: {Detecting} {Causality} by {Data} {Augmentation} via {Part}-of-{Speech} tagging},
	url = {https://aclanthology.org/2022.case-1.6/},
	doi = {10.18653/v1/2022.case-1.6},
	abstract = {Finding causal relations in texts has been a challenge since it requires methods ranging from defining event ontologies to developing proper algorithmic approaches. In this paper, we developed a framework which classifies whether a given sentence contains a causal event. As our approach, we exploited an external corpus that has causal labels to overcome the small size of the original corpus (Causal News Corpus) provided by task organizers. Further, we employed a data augmentation technique utilizing Part-Of-Speech (POS) based on our observation that some parts of speech are more (or less) relevant to causality. Our approach especially improved the recall of detecting causal events in sentences.},
	booktitle = {Proceedings of the 5th {Workshop} on {Challenges} and {Applications} of {Automated} {Extraction} of {Socio}-political {Events} from {Text} ({CASE})},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Juhyeon and Choe, Yesong and Lee, Sanghack},
	editor = {Hürriyetoğlu, Ali and Tanev, Hristo and Zavarella, Vanni and Yörük, Erdem},
	month = dec,
	year = {2022},
	pages = {44--49},
}

@inproceedings{kralev_image_2022,
	address = {Sofia, Bulgaria},
	title = {Image {Models} for large-scale {Object} {Detection} and {Classification}},
	url = {https://aclanthology.org/2022.clib-1.22/},
	abstract = {Recent developments in computer vision applications that are based on machine learning models allow real-time object detection, segmentation and captioning in image or video streams. The paper presents the development of an extension of the 80 COCO categories into a novel ontology with more than 700 classes covering 130 thematic subdomains related to Sport, Transport, Arts and Security. The development of an image dataset of object segmentation was accelerated by machine learning for automatic generation of objects' boundaries and classes. The Multilingual image dataset contains over 20,000 images and 200,000 annotations. It was used to pre-train 130 models for object detection and classification. We show the established approach for the development of the new models and their integration into an application and evaluation framework.},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Computational} {Linguistics} in {Bulgaria} ({CLIB} 2022)},
	publisher = {Department of Computational Linguistics, IBL – BAS},
	author = {Kralev, Jordan and Koeva, Svetla},
	month = sep,
	year = {2022},
	pages = {190--201},
}

@inproceedings{song_decoupling_2022,
	address = {Gyeongju, Republic of Korea},
	title = {Decoupling {Mixture}-of-{Graphs}: {Unseen} {Relational} {Learning} for {Knowledge} {Graph} {Completion} by {Fusing} {Ontology} and {Textual} {Experts}},
	url = {https://aclanthology.org/2022.coling-1.196/},
	abstract = {Knowledge Graph Embedding (KGE) has been proposed and successfully utilized to knowledge Graph Completion (KGC). But classic KGE paradigm often fail in unseen relation representations. Previous studies mainly utilize the textual descriptions of relations and its neighbor relations to represent unseen relations. In fact, the semantics of a relation can be expressed by three kinds of graphs: factual graph, ontology graph, textual description graph, and they can complement each other. A more common scenario in the real world is that seen and unseen relations appear at the same time. In this setting, the training set (only seen relations) and testing set (both seen and unseen relations) own different distributions. And the train-test inconsistency problem will make KGE methods easiy overfit on seen relations and under-performance on unseen relations. In this paper, we propose decoupling mixture-of-graph experts (DMoG) for unseen relations learning, which could represent the unseen relations in the factual graph by fusing ontology and textual graphs, and decouple fusing space and reasoning space to alleviate overfitting for seen relations. The experiments on two unseen only public datasets and a mixture dataset verify the effectiveness of the proposed method, which improves the state-of-the-art methods by 6.84\% in Hits@10 on average.},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Song, Ran and He, Shizhu and Zheng, Suncong and Gao, Shengxiang and Liu, Kang and Yu, Zhengtao and Zhao, Jun},
	editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
	month = oct,
	year = {2022},
	pages = {2237--2246},
}

@inproceedings{yu_building_2022,
	address = {Hybrid},
	title = {Building an {Event} {Extractor} with {Only} a {Few} {Examples}},
	url = {https://aclanthology.org/2022.deeplo-1.11/},
	doi = {10.18653/v1/2022.deeplo-1.11},
	abstract = {Supervised event extraction models require a substantial amount of training data to perform well. However, event annotation requires a lot of human effort and costs much time, which limits the application of existing supervised approaches to new event types. In order to reduce manual labor and shorten the time to build an event extraction system for an arbitrary event ontology, we present a new framework to train such systems much more efficiently without large annotations. Our event trigger labeling model uses a weak supervision approach, which only requires a set of keywords, a small number of examples and an unlabeled corpus, on which our approach automatically collects weakly supervised annotations. Our argument role labeling component performs zero-shot learning, which only requires the names of the argument roles of new event types. The source codes of our event trigger detection1 and event argument extraction2 models are publicly available for research purposes. We also release a dockerized system connecting the two models into an unified event extraction pipeline.},
	booktitle = {Proceedings of the {Third} {Workshop} on {Deep} {Learning} for {Low}-{Resource} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Pengfei and Zhang, Zixuan and Voss, Clare and May, Jonathan and Ji, Heng},
	editor = {Cherry, Colin and Fan, Angela and Foster, George and Haffari, Gholamreza (Reza) and Khadivi, Shahram and Peng, Nanyun (Violet) and Ren, Xiang and Shareghi, Ehsan and Swayamdipta, Swabha},
	month = jul,
	year = {2022},
	pages = {102--109},
}

@inproceedings{mahapatra_entity_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Entity {Extraction} in {Low} {Resource} {Domains} with {Selective} {Pre}-training of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2022.emnlp-main.61/},
	doi = {10.18653/v1/2022.emnlp-main.61},
	abstract = {Transformer-based language models trained on large natural language corpora have been very useful in downstream entity extraction tasks. However, they often result in poor performances when applied to domains that are different from those they are pretrained on. Continued pretraining using unlabeled data from target domains can help improve the performances of these language models on the downstream tasks. However, using all of the available unlabeled data for pretraining can be time-intensive; also, it can be detrimental to the performance of the downstream tasks, if the unlabeled data is not aligned with the data distribution for the target tasks. Previous works employed external supervision in the form of ontologies for selecting appropriate data samples for pretraining, but external supervision can be quite hard to obtain in low-resource domains. In this paper, we introduce effective ways to select data from unlabeled corpora of target domains for language model pretraining to improve the performances in target entity extraction tasks. Our data selection strategies do not require any external supervision. We conduct extensive experiments for the task of named entity recognition (NER) on seven different domains and show that language models pretrained on target domain unlabeled data obtained using our data selection strategies achieve better performances compared to those using data selection strategies in previous works that use external supervision. We also show that these pretrained language models using our data selection strategies outperform those pretrained on all of the available unlabeled target domain data.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mahapatra, Aniruddha and Nangi, Sharmila Reddy and Garimella, Aparna and N, Anandhavelu},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Pre-training, Performance, Computational linguistics, Extraction, Natural languages, Entity extractions, Data reduction, Down-stream, Target domain, Unlabeled data, Data selection strategies, Resource domains},
	pages = {942--951},
	annote = {Cited by: 5; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{reid_m2d2_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{M2D2}: {A} {Massively} {Multi}-{Domain} {Language} {Modeling} {Dataset}},
	url = {https://aclanthology.org/2022.emnlp-main.63/},
	doi = {10.18653/v1/2022.emnlp-main.63},
	abstract = {We present M2D2, a fine-grained, massively multi-domain corpus for studying domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and spans 145 domains extracted from Wikipedia and Semantic Scholar. Using ontologies derived from Wikipedia and ArXiv categories, we organize the domains in each data source into 22 groups. This two-level hierarchy enables the study of relationships between domains and their effects on in- and out-of-domain performance after adaptation. We also present a number of insights into the nature of effective domain adaptation in LMs, as examples of the new types of studies M2D2 enables. To improve in-domain performance, we show the benefits of adapting the LM along a domain hierarchy; adapting to smaller amounts of fine-grained domain-specific data can lead to larger in-domain performance gains than larger amounts of weakly relevant data. We further demonstrate a trade-off between in-domain specialization and out-of-domain generalization within and across ontologies, as well as a strong correlation between out-of-domain performance and lexical overlap between domains.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Reid, Machel and Zhong, Victor and Gururangan, Suchin and Zettlemoyer, Luke},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Semantics, Modeling languages, Wikipedia, Performance, Computational linguistics, Domain adaptation, Domain language, Ontology's, Fine grained, Data-source, Multi-domains, Effective domains},
	pages = {964--975},
	annote = {Cited by: 13; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{lal_using_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Using {Commonsense} {Knowledge} to {Answer} {Why}-{Questions}},
	url = {https://aclanthology.org/2022.emnlp-main.79/},
	doi = {10.18653/v1/2022.emnlp-main.79},
	abstract = {Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the context of answering questions in the TellMeWhy dataset using COMET as a source of relevant commonsense relations. We analyze the effects of model size (T5 and GPT3) along with methods of injecting knowledge (COMET) into these models. Results show that the largest models, as expected, yield substantial improvements over base models. Injecting external knowledge helps models of various sizes, but the amount of improvement decreases with larger model size. We also find that the format in which knowledge is provided is critical, and that smaller models benefit more from larger amounts of knowledge. Finally, we develop an ontology of knowledge types and analyze the relative coverage of the models across these categories.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lal, Yash Kumar and Tandon, Niket and Aggarwal, Tanvi and Liu, Horace and Chambers, Nathanael and Mooney, Raymond and Balasubramanian, Niranjan},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Commonsense knowledge, Ontology's, External knowledge, Large models, Base models, Large amounts, HELP model, Knowledge analysis, Model size},
	pages = {1204--1219},
	annote = {Cited by: 13; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{li_open_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Open {Relation} and {Event} {Type} {Discovery} with {Type} {Abstraction}},
	url = {https://aclanthology.org/2022.emnlp-main.461/},
	doi = {10.18653/v1/2022.emnlp-main.461},
	abstract = {Conventional “closed-world” information extraction (IE) approaches rely on human ontologies to define the scope for extraction. As a result, such approaches fall short when applied to new domains. This calls for systems that can automatically infer new types from given corpora, a task which we refer to as type discovery.To tackle this problem, we introduce the idea of type abstraction, where the model is prompted to generalize and name the type. Then we use the similarity between inferred names to induce clusters. Observing that this abstraction-based representation is often complementary to the entity/trigger token representation, we set up these two representations as two views and design our model as a co-training framework. Our experiments on multiple relation extraction and event extraction datasets consistently show the advantage of our type abstraction approach.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Sha and Ji, Heng and Han, Jiawei},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {6864--6877},
}

@inproceedings{portelli_generalizing_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Generalizing over {Long} {Tail} {Concepts} for {Medical} {Term} {Normalization}},
	url = {https://aclanthology.org/2022.emnlp-main.588/},
	doi = {10.18653/v1/2022.emnlp-main.588},
	abstract = {Medical term normalization consists in mapping a piece of text to a large number of output classes.Given the small size of the annotated datasets and the extremely long tail distribution of the concepts, it is of utmost importance to develop models that are capable to generalize to scarce or unseen concepts.An important attribute of most target ontologies is their hierarchical structure. In this paper we introduce a simple and effective learning strategy that leverages such information to enhance the generalizability of both discriminative and generative models.The evaluation shows that the proposed strategy produces state-of-the-art performance on seen concepts and consistent improvements on unseen ones, allowing also for efficient zero-shot knowledge transfer across text typologies and datasets.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Portelli, Beatrice and Scaboro, Simone and Santus, Enrico and Sedghamiz, Hooman and Chersoni, Emmanuele and Serra, Giuseppe},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {8580--8591},
}

@inproceedings{tittel_towards_2022,
	address = {Marseille, France},
	title = {Towards an {Ontology} for {Toponyms} in {Nepalese} {Historical} {Documents}},
	url = {https://aclanthology.org/2022.eurali-1.2/},
	abstract = {Nepalese historical legal documents contain a plethora of valuable information on the history of what is today Nepal. An empirical study based on such documents enables a deep understanding of religion and ritual, legal practice, rulership, and many other aspects of the society through time. The aim of the research project `Documents on the History of Religion and Law of Pre-modern Nepal' is to make accessible a text corpus with 18 th to 20 th century documents both through cataloging and digital text editions, building a database called Documenta Nepalica. However, the lack of interoperability with other resources hampers its seamless integration into broader research contexts. To address this problem, we target the modeling of the Documenta Nepalica as Linked Data. This paper presents one module of this larger endeavour: It describes a proof of concept for an ontology for Nepalese toponyms that provides the means to classify toponyms attested in the documents and to model their entanglement with other toponyms, persons, events, and time. The ontology integrates and extends standard ontologies and increases interoperability through aligning the ontology individuals to the respective entries of geographic authority files such as GeoNames. Also, we establish a mapping of the individuals to DBpedia entities.},
	booktitle = {Proceedings of the {Workshop} on {Resources} and {Technologies} for {Indigenous}, {Endangered} and {Lesser}-resourced {Languages} in {Eurasia} within the 13th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Tittel, Sabine},
	editor = {Ojha, Atul Kr. and Ahmadi, Sina and Liu, Chao-Hong and McCrae, John P.},
	month = jun,
	year = {2022},
	pages = {7--16},
}

@inproceedings{wang_query_2022,
	address = {Dublin, Ireland},
	title = {Query and {Extract}: {Refining} {Event} {Extraction} as {Type}-oriented {Binary} {Decoding}},
	url = {https://aclanthology.org/2022.findings-acl.16/},
	doi = {10.18653/v1/2022.findings-acl.16},
	abstract = {Event extraction is typically modeled as a multi-class classification problem where event types and argument roles are treated as atomic symbols. These approaches are usually limited to a set of pre-defined types. We propose a novel event extraction framework that uses event types and argument roles as natural language queries to extract candidate triggers and arguments from the input text. With the rich semantics in the queries, our framework benefits from the attention mechanisms to better capture the semantic correlation between the event types or argument roles and the input text. Furthermore, the query-and-extract formulation allows our approach to leverage all available event annotations from various ontologies as a unified model. Experiments on ACE and ERE demonstrate that our approach achieves state-of-the-art performance on each dataset and significantly outperforms existing methods on zero-shot event extraction.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Sijia and Yu, Mo and Chang, Shiyu and Sun, Lichao and Huang, Lifu},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {169--182},
}

@inproceedings{nesterov_ruccon_2022,
	address = {Dublin, Ireland},
	title = {{RuCCoN}: {Clinical} {Concept} {Normalization} in {Russian}},
	url = {https://aclanthology.org/2022.findings-acl.21/},
	doi = {10.18653/v1/2022.findings-acl.21},
	abstract = {We present RuCCoN, a new dataset for clinical concept normalization in Russian manually annotated by medical professionals. It contains over 16,028 entity mentions manually linked to over 2,409 unique concepts from the Russian language part of the UMLS ontology. We provide train/test splits for different settings (stratified, zero-shot, and CUI-less) and present strong baselines obtained with state-of-the-art models such as SapBERT. At present, Russian medical NLP is lacking in both datasets and trained models, and we view this work as an important step towards filling this gap. Our dataset and annotation guidelines are available at https://github.com/AIRI-Institute/RuCCoN.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Nesterov, Alexandr and Zubkova, Galina and Miftahutdinov, Zulfat and Kokh, Vladimir and Tutubalina, Elena and Shelmanov, Artem and Alekseev, Anton and Avetisian, Manvel and Chertok, Andrey and Nikolenko, Sergey},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {239--245},
}

@inproceedings{li_cross-lingual_2022,
	address = {Dublin, Ireland},
	title = {Cross-lingual {Inference} with {A} {Chinese} {Entailment} {Graph}},
	url = {https://aclanthology.org/2022.findings-acl.96/},
	doi = {10.18653/v1/2022.findings-acl.96},
	abstract = {Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triples. In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the first Chinese fine-grained entity typing dataset under the FIGER type ontology. Through experiments on the Levy-Holt dataset, we verify the strength of our Chinese entailment graph, and reveal the cross-lingual complementarity: on the parallel Levy-Holt dataset, an ensemble of Chinese and English entailment graphs outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC points.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Li, Tianyi and Weber, Sabine and Hosseini, Mohammad Javad and Guillou, Liane and Steedman, Mark},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {1214--1233},
}

@inproceedings{dayanik_improving_2022,
	address = {Dublin, Ireland},
	title = {Improving {Neural} {Political} {Statement} {Classification} with {Class} {Hierarchical} {Information}},
	url = {https://aclanthology.org/2022.findings-acl.186/},
	doi = {10.18653/v1/2022.findings-acl.186},
	abstract = {Many tasks in text-based computational social science (CSS) involve the classification of political statements into categories based on a domain-specific codebook. In order to be useful for CSS analysis, these categories must be fine-grained. The typically skewed distribution of fine-grained categories, however, results in a challenging classification problem on the NLP side. This paper proposes to make use of the hierarchical relations among categories typically present in such codebooks:e.g., markets and taxation are both subcategories of economy, while borders is a subcategory of security. We use these ontological relations as prior knowledge to establish additional constraints on the learned model, thusimproving performance overall and in particular for infrequent categories. We evaluate several lightweight variants of this intuition by extending state-of-the-art transformer-based textclassifiers on two datasets and multiple languages. We find the most consistent improvement for an approach based on regularization.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Dayanik, Erenay and Blessing, Andre and Blokker, Nico and Haunss, Sebastian and Kuhn, Jonas and Lapesa, Gabriella and Pado, Sebastian},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {2367--2382},
}

@inproceedings{casanueva_nlu_2022,
	address = {Seattle, United States},
	title = {{NLU}++: {A} {Multi}-{Label}, {Slot}-{Rich}, {Generalisable} {Dataset} for {Natural} {Language} {Understanding} in {Task}-{Oriented} {Dialogue}},
	url = {https://aclanthology.org/2022.findings-naacl.154/},
	doi = {10.18653/v1/2022.findings-naacl.154},
	abstract = {We present NLU++, a novel dataset for natural language understanding (NLU) in task-oriented dialogue (ToD) systems, with the aim to provide a much more challenging evaluation environment for dialogue NLU models, up to date with the current application and industry requirements. NLU++ is divided into two domains (BANKING and HO℡S) and brings several crucial improvements over current commonly used NLU datasets. 1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets. 2) The ontology is divided into domain-specific and generic (i.e., domain-universal) intents that overlap across domains, promoting cross-domain reusability of annotated examples. 3) The dataset design has been inspired by the problems observed in industrial ToD systems, and 4) it has been collected, filtered and carefully annotated by dialogue NLU experts, yielding high-quality annotated data. Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Casanueva, Inigo and Vulić, Ivan and Spithourakis, Georgios and Budzianowski, Paweł},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {1998--2013},
}

@inproceedings{zhang_knowledge-rich_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Knowledge-{Rich} {Self}-{Supervision} for {Biomedical} {Entity} {Linking}},
	url = {https://aclanthology.org/2022.findings-emnlp.61/},
	doi = {10.18653/v1/2022.findings-emnlp.61},
	abstract = {Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia. In this paper, we explore Knowledge-RIch Self-Supervision (KRISS) for biomedical entity linking, by leveraging readily available domain knowledge. In training, it generates self-supervised mention examples on unlabeled text using a domain ontology and trains a contextual encoder using contrastive learning. For inference, it samples self-supervised mentions as prototypes for each entity and conducts linking by mapping the test mention to the most similar prototype. Our approach can easily incorporate entity descriptions and gold mention labels if available. We conducted extensive experiments on seven standard datasets spanning biomedical literature and clinical notes. Without using any labeled information, our method produces KRISSBERT, a universal entity linker for four million UMLS entities that attains new state of the art, outperforming prior self-supervised methods by as much as 20 absolute points in accuracy. We released KRISSBERT at https://aka.ms/krissbert.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Sheng and Cheng, Hao and Vashishth, Shikhar and Wong, Cliff and Xiao, Jinfeng and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {868--880},
}

@inproceedings{remy_biolord_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{BioLORD}: {Learning} {Ontological} {Representations} from {Definitions} for {Biomedical} {Concepts} and their {Textual} {Descriptions}},
	url = {https://aclanthology.org/2022.findings-emnlp.104/},
	doi = {10.18653/v1/2022.findings-emnlp.104},
	abstract = {This work introduces BioLORD, a new pre-training strategy for producing meaningful representations for clinical sentences and biomedical concepts. State-of-the-art methodologies operate by maximizing the similarity in representation of names referring to the same concept, and preventing collapse through contrastive learning. However, because biomedical names are not always self-explanatory, it sometimes results in non-semantic representations. BioLORD overcomes this issue by grounding its concept representations using definitions, as well as short descriptions derived from a multi-relational knowledge graph consisting of biomedical ontologies. Thanks to this grounding, our model produces more semantic concept representations that match more closely the hierarchical structure of ontologies. BioLORD establishes a new state of the art for text similarity on both clinical sentences (MedSTS) and biomedical concepts (MayoSRS).},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Remy, François and Demuynck, Kris and Demeester, Thomas},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {1454--1465},
}

@inproceedings{yu_knowledge-grounded_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Knowledge-grounded {Dialog} {State} {Tracking}},
	url = {https://aclanthology.org/2022.findings-emnlp.250/},
	doi = {10.18653/v1/2022.findings-emnlp.250},
	abstract = {Knowledge (including structured knowledge such as schema and ontology and unstructured knowledge such as web corpus) is a critical part of dialog understanding, especially for unseen tasks and domains. Traditionally, such domain-specific knowledge is encoded implicitly into model parameters for the execution of downstream tasks, which makes training inefficient. In addition , such models are not easily transferable to new tasks with different schemas. In this work, we propose to perform dialog state tracking grounded on knowledge encoded externally. We query relevant knowledge of various forms based on the dialog context where such information can grounds the prediction of dialog states. We demonstrate superior performance of our proposed method over strong baselines, especially in the few-shot learning setting.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Dian and Wang, Mingqiu and Cao, Yuan and El Shafey, Laurent and Shafran, Izhak and Soltau, Hagen},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {3428--3435},
}

@inproceedings{zhang_efficient_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Efficient {Zero}-shot {Event} {Extraction} with {Context}-{Definition} {Alignment}},
	url = {https://aclanthology.org/2022.findings-emnlp.531/},
	doi = {10.18653/v1/2022.findings-emnlp.531},
	abstract = {Event extraction (EE) is the task of identifying interested event mentions from text.Conventional efforts mainly focus on the supervised setting. However, these supervised models cannot generalize to event types out of the pre-defined ontology. To fill this gap, many efforts have been devoted to the zero-shot EE problem. This paper follows the trend of modeling event-type semantics but moves one step further. We argue that using the static embedding of the event type name might not be enough because a single word could be ambiguous, and we need a sentence to define the type semantics accurately. To model the definition semantics, we use two separate transformer models to project the contextualized event mentions and corresponding definitions into the same embedding space and then minimize their embedding distance via contrastive learning. On top of that, we also propose a warming phase to help the model learn the minor difference between similar definitions. We name our approach Zero-shot Event extraction with Definition (ZED). Experiments on the MAVEN dataset show that our model significantly outperforms all previous zero-shot EE methods with fast inference speed due to the disjoint design. Further experiments also show that can be easily applied to the few-shot setting when the annotation is available and consistently outperforms baseline supervised methods.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Hongming and Yao, Wenlin and Yu, Dong},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {7169--7179},
}

@inproceedings{declerck_towards_2022,
	address = {Marseille, France},
	title = {Towards the {Linking} of a {Sign} {Language} {Ontology} with {Lexical} {Data}},
	url = {https://aclanthology.org/2022.gwll-1.2/},
	abstract = {We describe our current work for linking a new ontology for representing constitutive elements of Sign Languages with lexical data encoded within the OntoLex-Lemon framework. We first present very briefly the current state of the ontology, and show how transcriptions of signs can be represented in OntoLex-Lemon, in a minimalist manner, before addressing the challenges of linking the elements of the ontology to full lexical descriptions of the spoken languages.},
	booktitle = {Proceedings of {Globalex} {Workshop} on {Linked} {Lexicography} within the 13th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Declerck, Thierry},
	editor = {Kernerman, Ilan and Krek, Simon},
	month = jun,
	year = {2022},
	pages = {6--9},
}

@inproceedings{varvara_annotating_2022,
	address = {Marseille, France},
	title = {Annotating complex words to investigate the semantics of derivational processes},
	url = {https://aclanthology.org/2022.isa-1.18/},
	abstract = {In this paper, we present and test an annotation scheme designed to analyse the semantic properties of derived nouns in context. Aiming at a general semantic comparison of morphological processes, we use a descriptive model that seeks to capture semantic regularities among lexemes and affixes, rather than match occurrences to word sense inventories. We annotate two distinct features of target words: the ontological type of the entity they denote and their semantic relationship with the word they derive from. As illustrated through an annotation experiment on French corpus data, this procedure allows us to highlight semantic differences and similarities between affixes by investigating the number and frequency of their semantic functions, as well as the relation between affix polyfunctionality and lexical ambiguity.},
	booktitle = {Proceedings of the 18th {Joint} {ACL} - {ISO} {Workshop} on {Interoperable} {Semantic} {Annotation} within {LREC2022}},
	publisher = {European Language Resources Association},
	author = {Varvara, Rossella and Salvadori, Justine and Huyghe, Richard},
	editor = {Bunt, Harry},
	month = jun,
	year = {2022},
	pages = {133--141},
}

@inproceedings{ricchiardi_annotating_2022,
	address = {Marseille, France},
	title = {Annotating {Propositional} {Attitude} {Verbs} and their {Arguments}},
	url = {https://aclanthology.org/2022.isa-1.19/},
	abstract = {This paper describes the results of an empirical study on attitude verbs and propositional attitude reports in Italian. Within the framework of a project aiming at acquiring argument structures for Italian verbs from corpora, we carried out a systematic annotation that aims at individuating which verbs are actually attitude verbs in Italian. The result is a list of 179 argument structures based on corpus-derived pattern of use for 126 verbs that behave as attitude verbs. The distribution of these verbs in the corpus suggests that not only the canonical that-clauses, i.e. subordinates introduced by the complementizerte che, but also direct speech, infinitives introduced by the complementizer di, and some nominals are good candidates to express propositional contents in propositional attitude reports. The annotation also enlightens some issues between semantics and ontology, concerning the relation between events and propositions.},
	booktitle = {Proceedings of the 18th {Joint} {ACL} - {ISO} {Workshop} on {Interoperable} {Semantic} {Annotation} within {LREC2022}},
	publisher = {European Language Resources Association},
	author = {Ricchiardi, Marta and Jezek, Elisabetta},
	editor = {Bunt, Harry},
	month = jun,
	year = {2022},
	pages = {142--149},
}

@inproceedings{fantoli_linking_2022,
	address = {Marseille, France},
	title = {Linking the {LASLA} {Corpus} in the {LiLa} {Knowledge} {Base} of {Interoperable} {Linguistic} {Resources} for {Latin}},
	url = {https://aclanthology.org/2022.ldl-1.4/},
	abstract = {This paper describes the process of interlinking the 130 Classical Latin texts provided by an annotated corpus developed at the LASLA laboratory with the LiLa Knowledge Base, which makes linguistic resources for Latin interoperable by following the principles of the Linked Data paradigm and making reference to classes and properties of widely adopted ontologies to model the relevant information. After introducing the overall architecture of the LiLa Knowledge Base and the LASLA corpus, the paper details the phases of the process of linking the corpus with the collection of lemmas of LiLa and presents a federated query to exemplify the added value of interoperability of LASLA's texts with other resources for Latin.},
	booktitle = {Proceedings of the 8th {Workshop} on {Linked} {Data} in {Linguistics} within the 13th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Fantoli, Margherita and Passarotti, Marco and Mambrini, Francesco and Moretti, Giovanni and Ruffolo, Paolo},
	editor = {Declerck, Thierry and McCrae, John P. and Montiel, Elena and Chiarcos, Christian and Ionov, Maxim},
	month = jun,
	year = {2022},
	pages = {26--34},
}

@inproceedings{bobillo_fuzzy_2022,
	address = {Marseille, France},
	title = {Fuzzy {Lemon}: {Making} {Lexical} {Semantic} {Relations} {More} {Juicy}},
	url = {https://aclanthology.org/2022.ldl-1.6/},
	abstract = {The OntoLex-Lemon model provides a vocabulary to enrich ontologies with linguistic information that can be exploited by Natural Language Processing applications. The increasing uptake of Lemon illustrates the growing interest in combining linguistic information and Semantic Web technologies. In this paper, we present Fuzzy Lemon, an extension of Lemon that allows to assign an uncertainty degree to lexical semantic relations. Our approach is based on an OWL ontology that defines a hierarchy of data properties encoding different types of uncertainty. We also illustrate the usefulness of Fuzzy Lemon by showing that it can be used to represent the confidence degrees of automatically discovered translations between pairs of bilingual dictionaries from the Apertium family.},
	booktitle = {Proceedings of the 8th {Workshop} on {Linked} {Data} in {Linguistics} within the 13th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Bobillo, Fernando and Bosque-Gil, Julia and Gracia, Jorge and Lanau-Coronas, Marta},
	editor = {Declerck, Thierry and McCrae, John P. and Montiel, Elena and Chiarcos, Christian and Ionov, Maxim},
	month = jun,
	year = {2022},
	pages = {45--51},
}

@inproceedings{fath_spicy_2022,
	address = {Marseille, France},
	title = {Spicy {Salmon}: {Converting} between 50+ {Annotation} {Formats} with {Fintan}, {Pepper}, {Salt} and {Powla}},
	url = {https://aclanthology.org/2022.ldl-1.8/},
	abstract = {Heterogeneity of formats, models and annotations has always been a primary hindrance for exploiting the ever increasing amount of existing linguistic resources for real world applications in and beyond NLP. Fintan - the Flexible INtegrated Transformation and Annotation eNgineering platform introduced in 2020 is designed to rapidly convert, combine and manipulate language resources both in and outside the Semantic Web by transforming it into segmented RDF representations which can be processed in parallel on a multithreaded environment and integrating it with ontologies and taxonomies. Fintan has recently been extended with a set of additional modules increasing the amount of supported non-RDF formats and the interoperability with existing non-JAVA conversion tools, and parts of this work are demonstrated in this paper. In particular, we focus on a novel recipe for resource transformation in which Fintan works in tandem with the Pepper toolset to allow computational linguists to transform their data between over 50 linguistic corpus formats with a graphical workflow manager.},
	booktitle = {Proceedings of the 8th {Workshop} on {Linked} {Data} in {Linguistics} within the 13th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Fäth, Christian and Chiarcos, Christian},
	editor = {Declerck, Thierry and McCrae, John P. and Montiel, Elena and Chiarcos, Christian and Ionov, Maxim},
	month = jun,
	year = {2022},
	pages = {61--68},
}

@inproceedings{feder_building_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Building a {Clinically}-{Focused} {Problem} {List} {From} {Medical} {Notes}},
	url = {https://aclanthology.org/2022.louhi-1.8/},
	doi = {10.18653/v1/2022.louhi-1.8},
	abstract = {Clinical notes often contain useful information not documented in structured data, but their unstructured nature can lead to critical patient-related information being missed. To increase the likelihood that this valuable information is utilized for patient care, algorithms that summarize notes into a problem list have been proposed. Focused on identifying medically-relevant entities in the free-form text, these solutions are often detached from a canonical ontology and do not allow downstream use of the detected text-spans. Mitigating these issues, we present here a system for generating a canonical problem list from medical notes, consisting of two major stages. At the first stage, annotation, we use a transformer model to detect all clinical conditions which are mentioned in a single note. These clinical conditions are then grounded to a predefined ontology, and are linked to spans in the text. At the second stage, summarization, we develop a novel algorithm that aggregates over the set of clinical conditions detected on all of the patient's notes, and produce a concise patient summary that organizes their most important conditions.},
	booktitle = {Proceedings of the 13th {International} {Workshop} on {Health} {Text} {Mining} and {Information} {Analysis} ({LOUHI})},
	publisher = {Association for Computational Linguistics},
	author = {Feder, Amir and Laish, Itay and Agarwal, Shashank and Lerner, Uri and Atias, Avel and Cheung, Cathy and Clardy, Peter and Peled-Cohen, Alon and Fellinger, Rachana and Liu, Hengrui and Huong Nguyen, Lan and Patel, Birju and Potikha, Natan and Taubenfeld, Amir and Xu, Liwen and Yang, Seung Doo and Benjamini, Ayelet and Hassidim, Avinatan},
	editor = {Lavelli, Alberto and Holderness, Eben and Jimeno Yepes, Antonio and Minard, Anne-Lyse and Pustejovsky, James and Rinaldi, Fabio},
	month = dec,
	year = {2022},
	pages = {60--68},
}

@inproceedings{barros_divide_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Divide and {Conquer}: {An} {Extreme} {Multi}-{Label} {Classification} {Approach} for {Coding} {Diseases} and {Procedures} in {Spanish}},
	url = {https://aclanthology.org/2022.louhi-1.16/},
	doi = {10.18653/v1/2022.louhi-1.16},
	abstract = {Clinical coding is the task of transforming medical documents into structured codes following a standard ontology. Since these terminologies are composed of hundreds of codes, this problem can be considered an Extreme Multi-label Classification task. This paper proposes a novel neural network-based architecture for clinical coding. First, we take full advantage of the hierarchical nature of ontologies to create clusters based on semantic relations. Then, we use a Matcher module to assign the probability of documents belonging to each cluster. Finally, the Ranker calculates the probability of each code considering only the documents in the cluster. This division allows a fine-grained differentiation within the cluster, which cannot be addressed using a single classifier. In addition, since most of the previous work has focused on solving this task in English, we conducted our experiments on three clinical coding corpora in Spanish. The experimental results demonstrate the effectiveness of our model, achieving state-of-the-art results on two of the three datasets. Specifically, we outperformed previous models on two subtasks of the CodiEsp shared task: CodiEsp-D (diseases) and CodiEsp-P (procedures). Automatic coding can profoundly impact healthcare by structuring critical information written in free text in electronic health records.},
	booktitle = {Proceedings of the 13th {International} {Workshop} on {Health} {Text} {Mining} and {Information} {Analysis} ({LOUHI})},
	publisher = {Association for Computational Linguistics},
	author = {Barros, Jose and Rojas, Matias and Dunstan, Jocelyn and Abeliuk, Andres},
	editor = {Lavelli, Alberto and Holderness, Eben and Jimeno Yepes, Antonio and Minard, Anne-Lyse and Pustejovsky, James and Rinaldi, Fabio},
	month = dec,
	year = {2022},
	pages = {138--147},
}

@inproceedings{bagherzadeh_integration_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Integration of {Heterogeneous} {Knowledge} {Sources} for {Biomedical} {Text} {Processing}},
	url = {https://aclanthology.org/2022.louhi-1.25/},
	doi = {10.18653/v1/2022.louhi-1.25},
	abstract = {Recently, research into bringing outside knowledge sources into current neural NLP models has been increasing. Most approaches that leverage external knowledge sources require laborious and non-trivial designs, as well as tailoring the system through intensive ablation of different knowledge sources, an effort that discourages users to use quality ontological resources. In this paper, we show that multiple large heterogeneous KSs can be easily integrated using a decoupled approach, allowing for an automatic ablation of irrelevant KSs, while keeping the overall parameter space tractable. We experiment with BERT and pre-trained graph embeddings, and show that they interoperate well without performance degradation, even when some do not contribute to the task.},
	booktitle = {Proceedings of the 13th {International} {Workshop} on {Health} {Text} {Mining} and {Information} {Analysis} ({LOUHI})},
	publisher = {Association for Computational Linguistics},
	author = {Bagherzadeh, Parsa and Bergler, Sabine},
	editor = {Lavelli, Alberto and Holderness, Eben and Jimeno Yepes, Antonio and Minard, Anne-Lyse and Pustejovsky, James and Rinaldi, Fabio},
	month = dec,
	year = {2022},
	pages = {229--238},
}

@inproceedings{winter_kimera_2022,
	address = {Marseille, France},
	title = {{KIMERA}: {Injecting} {Domain} {Knowledge} into {Vacant} {Transformer} {Heads}},
	url = {https://aclanthology.org/2022.lrec-1.38/},
	abstract = {Training transformer language models requires vast amounts of text and computational resources. This drastically limits the usage of these models in niche domains for which they are not optimized, or where domain-specific training data is scarce. We focus here on the clinical domain because of its limited access to training data in common tasks, while structured ontological data is often readily available. Recent observations in model compression of transformer models show optimization potential in improving the representation capacity of attention heads. We propose KIMERA (Knowledge Injection via Mask Enforced Retraining of Attention) for detecting, retraining and instilling attention heads with complementary structured domain knowledge. Our novel multi-task training scheme effectively identifies and targets individual attention heads that are least useful for a given downstream task and optimizes their representation with information from structured data. KIMERA generalizes well, thereby building the basis for an efficient fine-tuning. KIMERA achieves significant performance boosts on seven datasets in the medical domain in Information Retrieval and Clinical Outcome Prediction settings. We apply KIMERA to BERT-base to evaluate the extent of the domain transfer and also improve on the already strong results of BioBERT in the clinical domain.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Winter, Benjamin and Rosero, Alexei Figueroa and Löser, Alexander and Gers, Felix Alexander and Siu, Amy},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Language model, Modeling languages, Domain knowledge, Computational linguistics, Supervised learning, Training data, Domain Knowledge, Representation model, Semi-supervised, Machine learning methods, Neural language representation model, Statistical learning methods, Supervised and unsupervised learning, Weakly-supervised and unsupervised learning},
	pages = {363--373},
	annote = {Cited by: 5},
}

@inproceedings{hudecek_diaser_2022,
	address = {Marseille, France},
	title = {{DIASER}: {A} {Unifying} {View} {On} {Task}-oriented {Dialogue} {Annotation}},
	url = {https://aclanthology.org/2022.lrec-1.137/},
	abstract = {Every model is only as strong as the data that it is trained on. In this paper, we present a new dataset, obtained by merging four publicly available annotated corpora for task-oriented dialogues in several domains (MultiWOZ 2.2, CamRest676, DSTC2 and Schema-Guided Dialogue Dataset). This way, we assess the feasibility of providing a unified ontology and annotation schema covering several domains with a relatively limited effort. We analyze the characteristics of the resulting dataset along three main dimensions: language, information content and performance. We focus on aspects likely to be pertinent for improving dialogue success, e.g. dialogue consistency. Furthermore, to assess the usability of this new corpus, we thoroughly evaluate dialogue generation performance under various conditions with the help of two prominent recent end-to-end dialogue models: MarCo and GPT-2. These models were selected as popular open implementations representative of the two main dimensions of dialogue modelling. While we did not observe a significant gain for dialogue state tracking performance, we show that using more training data from different sources can improve language modelling capabilities and positively impact dialogue flow (consistency). In addition, we provide the community with one of the largest open dataset for machine learning experiments.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Hudeček, Vojtěch and Schaub, Léon-Paul and Stancl, Daniel and Paroubek, Patrick and Dušek, Ondřej},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Modeling languages, Performance, Ontology's, Information contents, Task-oriented, Dialogue models, Annotated corpus, Dialogue generations, Language informations, Merging, Resource merging, Task oriented dialog},
	pages = {1286--1296},
	annote = {Cited by: 1},
}

@inproceedings{uresova_making_2022,
	address = {Marseille, France},
	title = {Making a {Semantic} {Event}-type {Ontology} {Multilingual}},
	url = {https://aclanthology.org/2022.lrec-1.142/},
	abstract = {We present an extension of the SynSemClass Event-type Ontology, originally conceived as a bilingual Czech-English resource. We added German entries to the classes representing the concepts of the ontology. Having a different starting point than the original work (unannotated parallel corpus without links to a valency lexicon and, of course, different existing lexical resources), it was a challenge to adapt the annotation guidelines, the data model and the tools used for the original version. We describe the process and results of working in such a setup. We also show the next steps to adapt the annotation process, data structures and formats and tools necessary to make the addition of a new language in the future more smooth and efficient, and possibly to allow for various teams to work on SynSemClass extensions to many languages concurrently. We also present the latest release which contains the results of adding German, freely available for download as well as for online access.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Uresova, Zdenka and Zaczynska, Karolina and Bourgonje, Peter and Fučíková, Eva and Rehm, Georg and Hajic, Jan},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {1332--1343},
}

@inproceedings{koeva_multilingual_2022,
	address = {Marseille, France},
	title = {Multilingual {Image} {Corpus} – {Towards} a {Multimodal} and {Multilingual} {Dataset}},
	url = {https://aclanthology.org/2022.lrec-1.162/},
	abstract = {One of the processing tasks for large multimodal data streams is automatic image description (image classification, object segmentation and classification). Although the number and the diversity of image datasets is constantly expanding, still there is a huge demand for more datasets in terms of variety of domains and object classes covered. The goal of the project Multilingual Image Corpus (MIC 21) is to provide a large image dataset with annotated objects and object descriptions in 24 languages. The Multilingual Image Corpus consists of an Ontology of visual objects (based on WordNet) and a collection of thematically related images whose objects are annotated with segmentation masks and labels describing the ontology classes. The dataset is designed both for image classification and object detection and for semantic segmentation. The main contributions of our work are: a) the provision of large collection of high quality copyright-free images; b) the formulation of the Ontology of visual objects based on WordNet noun hierarchies; c) the precise manual correction of automatic object segmentation within the images and the annotation of object classes; and d) the association of objects and images with extended multilingual descriptions based on WordNet inner- and interlingual relations. The dataset can be used also for multilingual image caption generation, image-to-text alignment and automatic question answering for images and videos.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Koeva, Svetla and Stoyanova, Ivelina and Kralev, Jordan},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {1509--1518},
}

@inproceedings{meisinger_increasing_2022,
	address = {Marseille, France},
	title = {Increasing {CMDI}'s {Semantic} {Interoperability} with schema.org},
	url = {https://aclanthology.org/2022.lrec-1.290/},
	abstract = {The CLARIN Concept Registry (CCR) is the common semantic ground for most CMDI-based profiles to describe language-related resources in the CLARIN universe. While the CCR supports semantic interoperability within this universe, it does not extend beyond it. The flexibility of CMDI, however, allows users to use other term or concept registries when defining their metadata components. In this paper, we describe our use of schema.org, a light ontology used by many parties across disciplines.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Meisinger, Nino and Trippel, Thorsten and Zinn, Claus},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {2714--2720},
}

@inproceedings{osenova_bulgarian_2022,
	address = {Marseille, France},
	title = {The {Bulgarian} {Event} {Corpus}: {Overview} and {Initial} {NER} {Experiments}},
	url = {https://aclanthology.org/2022.lrec-1.374/},
	abstract = {The paper describes the Bulgarian Event Corpus (BEC). The annotation scheme is based on CIDOC-CRM ontology and on the English Framenet, adjusted for our task. It includes two main layers: named entities and events with their roles. The corpus is multi-domain and mainly oriented towards Social Sciences and Humanities (SSH). It will be used for: extracting knowledge and making it available through the Bulgaria-centric Knowledge Graph; further developing an annotation scheme that handles multiple domains in SSH; training automatic modules for the most important knowledge-based tasks, such as domain-specific and nested NER, NEL, event detection and profiling. Initial experiments were conducted on standard NER task due to complexity of the dataset and the rich NE annotation scheme. The results are promising with respect to some labels and give insights on handling better other ones. These experiments serve also as error detection modules that would help us in scheme re-design. They are a basis for further and more complex tasks, such as nested NER, NEL and event detection.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Osenova, Petya and Simov, Kiril and Marinova, Iva and Berbatova, Melania},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {3491--3499},
}

@inproceedings{zotova_clinidmap_2022,
	address = {Marseille, France},
	title = {{ClinIDMap}: {Towards} a {Clinical} {IDs} {Mapping} for {Data} {Interoperability}},
	url = {https://aclanthology.org/2022.lrec-1.390/},
	abstract = {This paper presents ClinIDMap, a tool for mapping identifiers between clinical ontologies and lexical resources. ClinIDMap interlinks identifiers from UMLS, SMOMED-CT, ICD-10 and the corresponding Wikipedia articles for concepts from the UMLS Metathesaurus. Our main goal is to provide semantic interoperability across the clinical concepts from various knowledge bases. As a side effect, the mapping enriches already annotated corpora in multiple languages with new labels. For instance, spans manually annotated with IDs from UMLS can be annotated with Semantic Types and Groups, and its corresponding SNOMED CT and ICD-10 IDs. We also experiment with sequence labelling models for detecting Diagnosis and Procedures concepts and for detecting UMLS Semantic Groups trained on Spanish, English, and bilingual corpora obtained with the new mapping procedure. The ClinIDMap tool is publicly available.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Zotova, Elena and Cuadros, Montse and Rigau, German},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {3661--3669},
}

@inproceedings{declerck_towards_2022-1,
	address = {Marseille, France},
	title = {Towards a new {Ontology} for {Sign} {Languages}},
	url = {https://aclanthology.org/2022.lrec-1.423/},
	abstract = {We present the current status of a new ontology for representing constitutive elements of Sign Languages (SL). This development emerged from investigations on how to represent multimodal lexical data in the OntoLex-Lemon framework, with the goal to publish such data in the Linguistic Linked Open Data (LLOD) cloud. While studying the literature and various sites dealing with sign languages, we saw the need to harmonise all the data categories (or features) defined and used in those sources, and to organise them in an ontology to which lexical descriptions in OntoLex-Lemon could be linked. We make the code of the first version of this ontology available, so that it can be further developed collaboratively by both the Linked Data and the SL communities},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Declerck, Thierry},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {3977--3983},
}

@inproceedings{kuhn_grhoot_2022,
	address = {Marseille, France},
	title = {{GRhOOT}: {Ontology} of {Rhetorical} {Figures} in {German}},
	url = {https://aclanthology.org/2022.lrec-1.426/},
	abstract = {GRhOOT, the German RhetOrical OnTology, is a domain ontology of 110 rhetorical figures in the German language. The overall goal of building an ontology of rhetorical figures in German is not only the formal representation of different rhetorical figures, but also allowing for their easier detection, thus improving sentiment analysis, argument mining, detection of hate speech and fake news, machine translation, and many other tasks in which recognition of non-literal language plays an important role. The challenge of building such ontologies lies in classifying the figures and assigning adequate characteristics to group them, while considering their distinctive features. The ontology of rhetorical figures in the Serbian language was used as a basis for our work. Besides transferring and extending the concepts of the Serbian ontology, we ensured completeness and consistency by using description logic and SPARQL queries. Furthermore, we show a decision tree to identify figures and suggest a usage scenario on how the ontology can be utilized to collect and annotate data.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Kühn, Ramona and Mitrović, Jelena and Granitzer, Michael},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Information extraction, Ontology, Language model, Semantic Web, Domain ontologies, Modeling languages, Information retrieval, Sentiment analysis, Computational linguistics, Speech recognition, Decision trees, Ontology's, Semantic-Web, Formal representations, Machine translations, Data description, German language, Knowledge discovery/representation},
	pages = {4001--4010},
	annote = {Cited by: 6},
}

@inproceedings{parmar_hyperbox_2022,
	address = {Marseille, France},
	title = {{HyperBox}: {A} {Supervised} {Approach} for {Hypernym} {Discovery} using {Box} {Embeddings}},
	url = {https://aclanthology.org/2022.lrec-1.652/},
	abstract = {Hypernymy plays a fundamental role in many AI tasks like taxonomy learning, ontology learning, etc. This has motivated the development of many automatic identification methods for extracting this relation, most of which rely on word distribution. We present a novel model HyperBox to learn box embeddings for hypernym discovery. Given an input term, HyperBox retrieves its suitable hypernym from a target corpus. For this task, we use the dataset published for SemEval 2018 Shared Task on Hypernym Discovery. We compare the performance of our model on two specific domains of knowledge: medical and music. Experimentally, we show that our model outperforms existing methods on the majority of the evaluation metrics. Moreover, our model generalize well over unseen hypernymy pairs using only a small set of training data.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Parmar, Maulik and Narayan, Apurva},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {6069--6076},
}

@inproceedings{faralli_large_2022,
	address = {Marseille, France},
	title = {A {Large} {Interlinked} {Knowledge} {Graph} of the {Italian} {Cultural} {Heritage}},
	url = {https://aclanthology.org/2022.lrec-1.675/},
	abstract = {Knowledge is the lifeblood for a plethora of applications such as search, recommender systems and natural language understanding. Thanks to the efforts in the fields of Semantic Web and Linked Open Data a growing number of interlinked knowledge bases are supporting the development of advanced knowledge-based applications. Unfortunately, for a large number of domain-specific applications, these knowledge bases are unavailable. In this paper, we present a resource consisting of a large knowledge graph linking the Italian cultural heritage entities (defined in the ArCo ontology) with the concepts defined on well-known knowledge bases (i.e., DBpedia and the Getty GVP ontology). We describe the methodologies adopted for the semi-automatic resource creation and provide an in-depth analysis of the resulting interlinked graph.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Faralli, Stefano and Lenzi, Andrea and Velardi, Paola},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {6280--6289},
}

@inproceedings{weinzierl_vaccinelies_2022,
	address = {Marseille, France},
	title = {{VaccineLies}: {A} {Natural} {Language} {Resource} for {Learning} to {Recognize} {Misinformation} about the {COVID}-19 and {HPV} {Vaccines}},
	url = {https://aclanthology.org/2022.lrec-1.753/},
	abstract = {Billions of COVID-19 vaccines have been administered, but many remain hesitant. Misinformation about the COVID-19 vaccines and other vaccines, propagating on social media, is believed to drive hesitancy towards vaccination. The ability to automatically recognize misinformation targeting vaccines on Twitter depends on the availability of data resources. In this paper we present VaccineLies, a large collection of tweets propagating misinformation about two vaccines: the COVID-19 vaccines and the Human Papillomavirus (HPV) vaccines. Misinformation targets are organized in vaccine-specific taxonomies, which reveal the misinformation themes and concerns. The ontological commitments of the misinformation taxonomies provide an understanding of which misinformation themes and concerns dominate the discourse about the two vaccines covered in VaccineLies. The organization into training, testing and development sets of VaccineLies invites the development of novel supervised methods for detecting misinformation on Twitter and identifying the stance towards it. Furthermore, VaccineLies can be a stepping stone for the development of datasets focusing on misinformation targeting additional vaccines.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Weinzierl, Maxwell and Harabagiu, Sanda},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {6967--6975},
}

@inproceedings{sajjad_analyzing_2022,
	address = {Seattle, United States},
	title = {Analyzing {Encoded} {Concepts} in {Transformer} {Language} {Models}},
	url = {https://aclanthology.org/2022.naacl-main.225/},
	doi = {10.18653/v1/2022.naacl-main.225},
	abstract = {We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation) and linguistic ontologies (e.g. Word-Net), whereas the core-linguistic concepts (e.g., morphology, syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts.},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Alam, Firoj and Khan, Abdul and Xu, Jia},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {3082--3101},
}

@inproceedings{jose_constraining_2022,
	address = {Hybrid: Seattle, Washington + Online},
	title = {Constraining word alignments with posterior regularization for label transfer},
	url = {https://aclanthology.org/2022.naacl-industry.15/},
	doi = {10.18653/v1/2022.naacl-industry.15},
	abstract = {Unsupervised word alignments offer a lightweight and interpretable method to transfer labels from high- to low-resource languages, as long as semantically related words have the same label across languages. But such an assumption is often not true in industrial NLP pipelines, where multilingual annotation guidelines are complex and deviate from semantic consistency due to various factors (such as annotation difficulty, conflicting ontology, upcoming feature launches etc.);We address this difficulty by constraining the alignments models to remain consistent with both source and target annotation guidelines , leveraging posterior regularization and labeled examples. We illustrate the overall approach using IBM 2 (fast\_align) as a base model, and report results on both internal and external annotated datasets. We measure consistent accuracy improvements on the MultiATIS++ dataset over AWESoME, a popular transformer-based alignment model, in the label projection task (+2.7\% at word-level and +15\% at sentence-level), and show how even a small amount of target language annotations help substantially.},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Industry} {Track}},
	publisher = {Association for Computational Linguistics},
	author = {Jose, Kevin and Gueudre, Thomas},
	editor = {Loukina, Anastassia and Gangadharaiah, Rashmi and Min, Bonan},
	month = jul,
	year = {2022},
	pages = {121--129},
}

@inproceedings{percin_combining_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Combining {WordNet} and {Word} {Embeddings} in {Data} {Augmentation} for {Legal} {Texts}},
	url = {https://aclanthology.org/2022.nllp-1.4/},
	doi = {10.18653/v1/2022.nllp-1.4},
	abstract = {Creating balanced labeled textual corpora for complex tasks, like legal analysis, is a challenging and expensive process that often requires the collaboration of domain experts. To address this problem, we propose a data augmentation method based on the combination of GloVe word embeddings and the WordNet ontology. We present an example of application in the legal domain, specifically on decisions of the Court of Justice of the European Union.Our evaluation with human experts confirms that our method is more robust than the alternatives.},
	booktitle = {Proceedings of the {Natural} {Legal} {Language} {Processing} {Workshop} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Perçin, Sezen and Galassi, Andrea and Lagioia, Francesca and Ruggeri, Federico and Santin, Piera and Sartor, Giovanni and Torroni, Paolo},
	editor = {Aletras, Nikolaos and Chalkidis, Ilias and Barrett, Leslie and Goanță, Cătălina and Preoțiuc-Pietro, Daniel},
	month = dec,
	year = {2022},
	pages = {47--52},
}

@inproceedings{gnehm_fine-grained_2022,
	address = {Abu Dhabi, UAE},
	title = {Fine-{Grained} {Extraction} and {Classification} of {Skill} {Requirements} in {German}-{Speaking} {Job} {Ads}},
	url = {https://aclanthology.org/2022.nlpcss-1.2/},
	doi = {10.18653/v1/2022.nlpcss-1.2},
	abstract = {Monitoring the development of labor market skill requirements is an information need that is more and more approached by applying text mining methods to job advertisement data. We present an approach for fine-grained extraction and classification of skill requirements from German-speaking job advertisements. We adapt pre-trained transformer-based language models to the domain and task of computing meaningful representations of sentences or spans. By using context from job advertisements and the large ESCO domain ontology we improve our similarity-based unsupervised multi-label classification results. Our best model achieves a mean average precision of 0.969 on the skill class level.},
	booktitle = {Proceedings of the {Fifth} {Workshop} on {Natural} {Language} {Processing} and {Computational} {Social} {Science} ({NLP}+{CSS})},
	publisher = {Association for Computational Linguistics},
	author = {Gnehm, Ann-sophie and Bühlmann, Eva and Buchs, Helen and Clematide, Simon},
	editor = {Bamman, David and Hovy, Dirk and Jurgens, David and Keith, Katherine and O'Connor, Brendan and Volkova, Svitlana},
	month = nov,
	year = {2022},
	pages = {14--24},
}

@inproceedings{stranisci_o-dang_2022,
	address = {Marseille, France},
	title = {O-{Dang}! {The} {Ontology} of {Dangerous} {Speech} {Messages}},
	url = {https://aclanthology.org/2022.salld-1.2/},
	abstract = {Inside the NLP community there is a considerable amount of language resources created, annotated and released every day with the aim of studying specific linguistic phenomena. Despite a variety of attempts in order to organize such resources has been carried on, a lack of systematic methods and of possible interoperability between resources are still present. Furthermore, when storing linguistic information, still nowadays, the most common practice is the concept of “gold standard”, which is in contrast with recent trends in NLP that aim at stressing the importance of different subjectivities and points of view when training machine learning and deep learning methods. In this paper we present O-Dang!: The Ontology of Dangerous Speech Messages, a systematic and interoperable Knowledge Graph (KG) for the collection of linguistic annotated data. O-Dang! is designed to gather and organize Italian datasets into a structured KG, according to the principles shared within the Linguistic Linked Open Data community. The ontology has also been designed to account a perspectivist approach, since it provides a model for encoding both gold standard and single-annotator labels in the KG. The paper is structured as follows. In Section 1 the motivations of our work are outlined. Section 2 describes the O-Dang! Ontology, that provides a common semantic model for the integration of datasets in the KG. The Ontology Population stage with information about corpora, users, and annotations is presented in Section 3. Finally, in Section 4 an analysis of offensiveness across corpora is provided as a first case study for the resource.},
	booktitle = {Proceedings of the 2nd {Workshop} on {Sentiment} {Analysis} and {Linguistic} {Linked} {Data}},
	publisher = {European Language Resources Association},
	author = {Stranisci, Marco Antonio and Frenda, Simona and Lai, Mirko and Araque, Oscar and Cignarella, Alessandra Teresa and Basile, Valerio and Bosco, Cristina and Patti, Viviana},
	editor = {Kernerman, Ilan and Carvalho, Sara and Iglesias, Carlos A. and Sprugnoli, Rachele},
	month = jun,
	year = {2022},
	pages = {2--8},
}

@inproceedings{lin_gentus_2022,
	address = {Edinburgh, UK},
	title = {{GenTUS}: {Simulating} {User} {Behaviour} and {Language} in {Task}-oriented {Dialogues} with {Generative} {Transformers}},
	url = {https://aclanthology.org/2022.sigdial-1.28/},
	doi = {10.18653/v1/2022.sigdial-1.28},
	abstract = {User simulators (USs) are commonly used to train task-oriented dialogue systems via reinforcement learning. The interactions often take place on semantic level for efficiency, but there is still a gap from semantic actions to natural language, which causes a mismatch between training and deployment environment. Incorporating a natural language generation (NLG) module with USs during training can partly deal with this problem. However, since the policy and NLG of USs are optimised separately, these simulated user utterances may not be natural enough in a given context. In this work, we propose a generative transformer-based user simulator (GenTUS). GenTUS consists of an encoder-decoder structure, which means it can optimise both the user policy and natural language generation jointly. GenTUS generates both semantic actions and natural language utterances, preserving interpretability and enhancing language variation. In addition, by representing the inputs and outputs as word sequences and by using a large pre-trained language model we can achieve generalisability in feature representation. We evaluate GenTUS with automatic metrics and human evaluation. Our results show that GenTUS generates more natural language and is able to transfer to an unseen ontology in a zero-shot fashion. In addition, its behaviour can be further shaped with reinforcement learning opening the door to training specialised user simulators.},
	booktitle = {Proceedings of the 23rd {Annual} {Meeting} of the {Special} {Interest} {Group} on {Discourse} and {Dialogue}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Hsien-chin and Geishauser, Christian and Feng, Shutong and Lubis, Nurul and van Niekerk, Carel and Heck, Michael and Gasic, Milica},
	editor = {Lemon, Oliver and Hakkani-Tur, Dilek and Li, Junyi Jessy and Ashrafzadeh, Arash and Garcia, Daniel Hernández and Alikhani, Malihe and Vandyke, David and Dušek, Ondřej},
	month = sep,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Semantics, Natural language generation, Reinforcement learning, Computational linguistics, Zero-shot learning, Dialogue systems, Natural languages, Learning systems, Speech processing, Natural language processing systems, Behavioral research, Reinforcement learnings, User behaviors, Task-oriented, Encoder-decoder, Policy language, Semantic action, Semantic levels},
	pages = {270--282},
	annote = {Cited by: 15},
}

@inproceedings{vukovic_dialogue_2022,
	address = {Edinburgh, UK},
	title = {Dialogue {Term} {Extraction} using {Transfer} {Learning} and {Topological} {Data} {Analysis}},
	url = {https://aclanthology.org/2022.sigdial-1.53/},
	doi = {10.18653/v1/2022.sigdial-1.53},
	abstract = {Goal oriented dialogue systems were originally designed as a natural language interface to a fixed data-set of entities that users might inquire about, further described by domain, slots and values. As we move towards adaptable dialogue systems where knowledge about domains, slots and values may change, there is an increasing need to automatically extract these terms from raw dialogues or related non-dialogue data on a large scale. In this paper, we take an important step in this direction by exploring different features that can enable systems to discover realisations of domains, slots and values in dialogues in a purely data-driven fashion. The features that we examine stem from word embeddings, language modelling features, as well as topological features of the word embedding space. To examine the utility of each feature set, we train a seed model based on the widely used MultiWOZ data-set. Then, we apply this model to a different corpus, the Schema-guided dialogue data-set. Our method outperforms the previously proposed approach that relies solely on word embeddings. We also demonstrate that each of the features is responsible for discovering different kinds of content. We believe our results warrant further research towards ontology induction, and continued harnessing of topological data analysis for dialogue and natural language processing research.},
	booktitle = {Proceedings of the 23rd {Annual} {Meeting} of the {Special} {Interest} {Group} on {Discourse} and {Dialogue}},
	publisher = {Association for Computational Linguistics},
	author = {Vukovic, Renato and Heck, Michael and Ruppik, Benjamin and van Niekerk, Carel and Zibrowius, Marcus and Gasic, Milica},
	editor = {Lemon, Oliver and Hakkani-Tur, Dilek and Li, Junyi Jessy and Ashrafzadeh, Arash and Garcia, Daniel Hernández and Alikhani, Malihe and Vandyke, David and Dušek, Ondřej},
	month = sep,
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Modeling languages, Term extraction, Embeddings, Data mining, Transfer learning, Computational linguistics, Natural language interfaces, Dialogue systems, Data set, Topology, Speech processing, Natural language processing systems, Data handling, Large-scales, Goal-oriented, Learning data, Topological data analysis},
	pages = {564--581},
	annote = {Cited by: 6},
}

@inproceedings{ortega-martin_dezzaismm4h22_2022,
	address = {Gyeongju, Republic of Korea},
	title = {dezzai@{SMM4H}'22: {Tasks} 5 \& 10 - {Hybrid} models everywhere},
	url = {https://aclanthology.org/2022.smm4h-1.3/},
	abstract = {This paper presents our approaches to SMM4H'22 task 5 - Classification of tweets of self-reported COVID-19 symptoms in Spanish, and task 10 - Detection of disease mentions in tweets – SocialDisNER (in Spanish). We have presented hybrid systems that combine Deep Learning techniques with linguistic rules and medical ontologies, which have allowed us to achieve outstanding results in both tasks.},
	booktitle = {Proceedings of the {Seventh} {Workshop} on {Social} {Media} {Mining} for {Health} {Applications}, {Workshop} \& {Shared} {Task}},
	publisher = {Association for Computational Linguistics},
	author = {Ortega-Martín, Miguel and Ardoiz, Alfonso and Garcia, Oscar and Álvarez, Jorge and Alonso, Adrián},
	editor = {Gonzalez-Hernandez, Graciela and Weissenbacher, Davy},
	month = oct,
	year = {2022},
	pages = {7--10},
}

@article{heck_robust_2022,
	title = {Robust {Dialogue} {State} {Tracking} with {Weak} {Supervision} and {Sparse} {Data}},
	volume = {10},
	url = {https://aclanthology.org/2022.tacl-1.68/},
	doi = {10.1162/tacl_a_00513},
	abstract = {Generalizing dialogue state tracking (DST) to new data is especially challenging due to the strong reliance on abundant and fine-grained supervision during training. Sample sparsity, distributional shift, and the occurrence of new concepts and topics frequently lead to severe performance degradation during inference. In this paper we propose a training strategy to build extractive DST models without the need for fine-grained manual span labels. Two novel input-level dropout methods mitigate the negative impact of sample sparsity. We propose a new model architecture with a unified encoder that supports value as well as slot independence by leveraging the attention mechanism. We combine the strengths of triple copy strategy DST and value matching to benefit from complementary predictions without violating the principle of ontology independence. Our experiments demonstrate that an extractive DST model can be trained without manual span labels. Our architecture and training strategies improve robustness towards sample sparsity, new concepts, and topics, leading to state-of-the-art performance on a range of benchmarks. We further highlight our model's ability to effectively learn from non-dialogue data.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Heck, Michael and Lubis, Nurul and van Niekerk, Carel and Feng, Shutong and Geishauser, Christian and Lin, Hsien-Chin and Gašić, Milica},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2022},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {1175--1192},
}

@inproceedings{han_examining_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Examining {Large} {Pre}-{Trained} {Language} {Models} for {Machine} {Translation}: {What} {You} {Don}'t {Know} about {It}},
	url = {https://aclanthology.org/2022.wmt-1.84/},
	abstract = {Pre-trained language models (PLMs) often take advantage of the monolingual and multilingual dataset that is freely available online to acquire general or mixed domain knowledge before deployment into specific tasks. Extra-large PLMs (xLPLMs) are proposed very recently to claim supreme performances over smaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs include Meta-AI's wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this work, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in fine-tuning toward domain-specific MTs. We use two different in-domain data of different sizes: commercial automotive in-house data and clinical shared task data from the ClinSpEn2022 challenge at WMT2022. We choose the popular Marian Helsinki as smaller sized PLM and two massive-sized Mega-Transformers from Meta-AI as xLPLMs.Our experimental investigation shows that 1) on smaller-sized in-domain commercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much better evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized Marian, even though its score increase rate is lower than Marian after fine-tuning; 2) on relatively larger-size well prepared clinical data fine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized Marian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn offered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on Task-1 (clinical cases) on all official metrics including SacreBLEU and BLEU; 3) metrics do not always agree with each other on the same tasks using the same model outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SacreBLEU/BLEU) and Task-3 (via METEOR and ROUGE) among all submissions.},
	booktitle = {Proceedings of the {Seventh} {Conference} on {Machine} {Translation} ({WMT})},
	publisher = {Association for Computational Linguistics},
	author = {Han, Lifeng and Erofeev, Gleb and Sorokina, Irina and Gladkoff, Serge and Nenadic, Goran},
	editor = {Koehn, Philipp and Barrault, Loïc and Bojar, Ondřej and Bougares, Fethi and Chatterjee, Rajen and Costa-jussà, Marta R. and Federmann, Christian and Fishel, Mark and Fraser, Alexander and Freitag, Markus and Graham, Yvette and Grundkiewicz, Roman and Guzman, Paco and Haddow, Barry and Huck, Matthias and Jimeno Yepes, Antonio and Kocmi, Tom and Martins, André and Morishita, Makoto and Monz, Christof and Nagata, Masaaki and Nakazawa, Toshiaki and Negri, Matteo and Névéol, Aurélie and Neves, Mariana and Popel, Martin and Turchi, Marco and Zampieri, Marcos},
	month = dec,
	year = {2022},
	pages = {908--919},
}

@inproceedings{manchanda_optums_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Optum's {Submission} to {WMT22} {Biomedical} {Translation} {Tasks}},
	url = {https://aclanthology.org/2022.wmt-1.86/},
	abstract = {This paper describes Optum's submission to the Biomedical Translation task of the seventh conference on Machine Translation (WMT22). The task aims at promoting the development and evaluation of machine translation systems in their ability to handle challenging domain-specific biomedical data. We made submissions to two sub-tracks of ClinSpEn 2022, namely, ClinSpEn-CC (clinical cases) and ClinSpEn-OC (ontology concepts). These sub-tasks aim to test translation from English to Spanish. Our approach involves fine-tuning a pre-trained transformer model using in-house clinical domain data and the biomedical data provided by WMT. The fine-tuned model results in a test BLEU score of 38.12 in the ClinSpEn-CC (clinical cases) subtask, which is a gain of 1.23 BLEU compared to the pre-trained model.},
	booktitle = {Proceedings of the {Seventh} {Conference} on {Machine} {Translation} ({WMT})},
	publisher = {Association for Computational Linguistics},
	author = {Manchanda, Sahil and Bhagwat, Saurabh},
	editor = {Koehn, Philipp and Barrault, Loïc and Bojar, Ondřej and Bougares, Fethi and Chatterjee, Rajen and Costa-jussà, Marta R. and Federmann, Christian and Fishel, Mark and Fraser, Alexander and Freitag, Markus and Graham, Yvette and Grundkiewicz, Roman and Guzman, Paco and Haddow, Barry and Huck, Matthias and Jimeno Yepes, Antonio and Kocmi, Tom and Martins, André and Morishita, Makoto and Monz, Christof and Nagata, Masaaki and Nakazawa, Toshiaki and Negri, Matteo and Névéol, Aurélie and Neves, Mariana and Popel, Martin and Turchi, Marco and Zampieri, Marcos},
	month = dec,
	year = {2022},
	pages = {925--929},
}

@inproceedings{wu_plms_2023,
	address = {Toronto, Canada},
	title = {Do {PLMs} {Know} and {Understand} {Ontological} {Knowledge}?},
	volume = {1},
	url = {https://aclanthology.org/2023.acl-long.173/},
	doi = {10.18653/v1/2023.acl-long.173},
	abstract = {Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a system- atic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic un- derstanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ontological knowledge beyond memorization, we comprehensively study whether they can reliably perform logical reasoning with given knowledge according to ontological entailment rules. Our probing results show that PLMs can memorize certain ontological knowledge and utilize implicit knowledge in reasoning. How- ever, both the memorizing and reasoning per- formances are less than perfect, indicating in- complete knowledge and understanding.},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Weiqi and Jiang, Chengyue and Jiang, Yong and Xie, Pengjun and Tu, Kewei},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Semantics, Factual knowledge, Computational linguistics, Logical reasoning, Sports, Property, Implicit knowledge, Semantics understanding, Surface forms, World knowledge, Domain constraint, Range constraints},
	pages = {3080--3101},
	annote = {Cited by: 16; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{parekh_geneva_2023,
	address = {Toronto, Canada},
	title = {{GENEVA}: {Benchmarking} {Generalizability} for {Event} {Argument} {Extraction} with {Hundreds} of {Event} {Types} and {Argument} {Roles}},
	url = {https://aclanthology.org/2023.acl-long.203/},
	doi = {10.18653/v1/2023.acl-long.203},
	abstract = {Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites aimed at evaluating models' ability to handle limited data and unseen event type generalization. We benchmark six EAE models from various families. The results show that owing to non-entity argument roles, even the best-performing model can only achieve 39\% F1 score, indicating how GENEVA provides new challenges for generalization in EAE. Overall, our large and diverse EAE ontology can aid in creating more comprehensive future resources, while GENEVA is a challenging benchmarking dataset encouraging further research for improving generalizability in EAE. The code and data can be found at https://github.com/PlusLabNLP/GENEVA.},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Parekh, Tanmay and Hsu, I-Hung and Huang, Kuan-Hao and Chang, Kai-Wei and Peng, Nanyun},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {3664--3686},
}

@inproceedings{wei_guide_2023,
	address = {Toronto, Canada},
	title = {Guide the {Many}-to-{One} {Assignment}: {Open} {Information} {Extraction} via {IoU}-aware {Optimal} {Transport}},
	url = {https://aclanthology.org/2023.acl-long.272/},
	doi = {10.18653/v1/2023.acl-long.272},
	abstract = {Open Information Extraction (OIE) seeks to extract structured information from raw text without the limitations of close ontology. Recently, the detection-based OIE methods have received great attention from the community due to their parallelism. However, as the essential step of those models, how to assign ground truth labels to the parallelly generated tuple proposals remains under-exploited. The commonly utilized Hungarian algorithm for this procedure is restricted to handling one-to-one assignment among the desired tuples and tuple proposals, which ignores the correlation between proposals and affects the recall of the models. To solve this problem, we propose a dynamic many-to-one label assignment strategy named IOT. Concretely, the label assignment process in OIE is formulated as an Optimal Transport (OT) problem. We leverage the intersection-over-union (IoU) as the assignment quality measurement, and convert the problem of finding the best assignment solution to the one of solving the optimal transport plan by maximizing the IoU values. To further utilize the knowledge from the assignment, we design an Assignment-guided Multi-granularity loss (AM) by simultaneously considering word-level and tuple-level information. Experiment results show the proposed method outperforms the state-of-the-art models on three benchmarks.},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wei, Kaiwen and Yang, Yiran and Jin, Li and Sun, Xian and Zhang, Zequn and Zhang, Jingyuan and Li, Xiao and Zhang, Linhao and Liu, Jintao and Zhi, Guo},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {4971--4984},
}

@inproceedings{stengel-eskin_why_2023,
	address = {Toronto, Canada},
	title = {Why {Did} the {Chicken} {Cross} the {Road}? {Rephrasing} and {Analyzing} {Ambiguous} {Questions} in {VQA}},
	url = {https://aclanthology.org/2023.acl-long.569/},
	doi = {10.18653/v1/2023.acl-long.569},
	abstract = {Natural language is ambiguous. Resolving ambiguous questions is key to successfully answering them. Focusing on questions about images, we create a dataset of ambiguous examples. We annotate these, grouping answers by the underlying question they address and rephrasing the question for each group to reduce ambiguity. Our analysis reveals a linguistically-aligned ontology of reasons for ambiguity in visual questions. We then develop an English question-generation model which we demonstrate via automatic and human evaluation produces less ambiguous questions. We further show that the question generation objective we use allows the model to integrate answer group information without any direct supervision.},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Stengel-Eskin, Elias and Guallar-Blasco, Jimena and Zhou, Yi and Van Durme, Benjamin},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {10220--10237},
}

@inproceedings{sharma_automated_2023,
	address = {Toronto, Canada},
	title = {Automated {Digitization} of {Unstructured} {Medical} {Prescriptions}},
	url = {https://aclanthology.org/2023.acl-industry.76/},
	doi = {10.18653/v1/2023.acl-industry.76},
	abstract = {Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to place orders. In this paper, we present prescription digitization system for online medicine ordering built with minimal supervision. Our system uses a modular pipeline comprising a mix of ML and rule-based components for (a) image to text extraction, (b) segmentation into blocks and medication items, (c) medication attribute extraction, (d) matching against medicine catalog, and (e) shopping cart building. Our approach efficiently utilizes multiple signals like layout, medical ontologies, and semantic embeddings via LayoutLMv2 model to yield substantial improvement relative to strong baselines on medication attribute extraction. Our pipeline achieves +5.9\% gain in precision@3 and +5.6\% in recall@3 over catalog-based fuzzy matching baseline for shopping cart building for printed prescriptions.},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 5: {Industry} {Track})},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Megha and Vatsal, Tushar and Merugu, Srujana and Rajan, Aruna},
	editor = {Sitaram, Sunayana and Beigman Klebanov, Beata and Williams, Jason D},
	month = jul,
	year = {2023},
	pages = {794--805},
}

@inproceedings{remy_automatic_2023,
	address = {Toronto, Canada},
	title = {Automatic {Glossary} of {Clinical} {Terminology}: a {Large}-{Scale} {Dictionary} of {Biomedical} {Definitions} {Generated} from {Ontological} {Knowledge}},
	url = {https://aclanthology.org/2023.bionlp-1.23/},
	doi = {10.18653/v1/2023.bionlp-1.23},
	abstract = {Background: More than 400.000 biomedical concepts and some of their relationships are contained in SnomedCT, a comprehensive biomedical ontology. However, their concept names are not always readily interpretable by non-experts, or patients looking at their own electronic health records (EHR). Clear definitions or descriptions in understandable language or often not available. Therefore, generating human-readable definitions for biomedical concepts might help make the information they encode more accessible and understandable to a wider public. Objective: In this article, we introduce the Automatic Glossary of Clinical Terminology (AGCT), a large-scale biomedical dictionary of clinical concepts generated using high-quality information extracted from the biomedical knowledge contained in SnomedCT.Methods: We generate a novel definition for every SnomedCT concept, after prompting the OpenAI Turbo model, a variant of GPT 3.5, using a high-quality verbalization of the SnomedCT relationships of the to-be-defined concept. A significant subset of the generated definitions was subsequently evaluated by NLP researchers with biomedical expertise on 5-point scales along the following three axes: factuality, insight, and fluency. Results: AGCT contains 422,070 computer-generated definitions for SnomedCT concepts, covering various domains such as diseases, procedures, drugs, and anatomy. The average length of the definitions is 49 words. The definitions were assigned average scores of over 4.5 out of 5 on all three axes, indicating a majority of factual, insightful, and fluent definitions. Conclusion: AGCT is a novel and valuable resource for biomedical tasks that require human-readable definitions for SnomedCT concepts. It can also serve as a base for developing robust biomedical retrieval models or other applications that leverage natural language understanding of biomedical knowledge.},
	booktitle = {Proceedings of the 22nd {Workshop} on {Biomedical} {Natural} {Language} {Processing} and {BioNLP} {Shared} {Tasks}},
	publisher = {Association for Computational Linguistics},
	author = {Remy, François and Demuynck, Kris and Demeester, Thomas},
	editor = {Demner-fushman, Dina and Ananiadou, Sophia and Cohen, Kevin},
	month = jul,
	year = {2023},
	pages = {265--272},
}

@inproceedings{slavcheva_road_2023,
	address = {Varna, Bulgaria},
	title = {On the {Road} to a {Protest} {Event} {Ontology} for {Bulgarian}: {Conceptual} {Structures} and {Representation} {Design}},
	url = {https://aclanthology.org/2023.case-1.13/},
	abstract = {The paper presents a semantic model of protest events, called Semantic Interpretations of Protest Events (SemInPE). The analytical framework used for building the semantic representations is inspired by the object-oriented paradigm in computer science and a cognitive approach to the linguistic analysis. The model is a practical application of the Unified Eventity Representation (UER) formalism, which is based on the Unified Modeling Language (UML). The multi-layered architecture of the model provides flexible means for building the semantic representations of the language objects along a scale of generality and specificity. Thus, it is a suitable environment for creating the elements of ontologies on various topics and for different languages.},
	booktitle = {Proceedings of the 6th {Workshop} on {Challenges} and {Applications} of {Automated} {Extraction} of {Socio}-political {Events} from {Text}},
	publisher = {INCOMA Ltd., Shoumen, Bulgaria},
	author = {Slavcheva, Milena and Tanev, Hristo and Uca, Onur},
	editor = {Hürriyetoğlu, Ali and Tanev, Hristo and Zavarella, Vanni and Yeniterzi, Reyyan and Yörük, Erdem and Slavcheva, Milena},
	month = sep,
	year = {2023},
	pages = {92--100},
}

@inproceedings{xiong_enhancing_2023,
	address = {Harbin, China},
	title = {Enhancing {Ontology} {Knowledge} for {Domain}-{Specific} {Joint} {Entity} and {Relation} {Extraction}},
	url = {https://aclanthology.org/2023.ccl-1.61/},
	abstract = {“Pre-trained language models (PLMs) have been widely used in entity and relation extractionmethods in recent years. However, due to the semantic gap between general-domain text usedfor pre-training and domain-specific text, these methods encounter semantic redundancy anddomain semantics insufficiency when it comes to domain-specific tasks. To mitigate this issue,we propose a low-cost and effective knowledge-enhanced method to facilitate domain-specificsemantics modeling in joint entity and relation extraction. Precisely, we use ontology and entitytype descriptions as domain knowledge sources, which are encoded and incorporated into thedownstream entity and relation extraction model to improve its understanding of domain-specificinformation. We construct a dataset called SSUIE-RE for Chinese entity and relation extractionin space science and utilization domain of China Manned Space Engineering, which contains awealth of domain-specific knowledge. The experimental results on SSUIE-RE demonstrate theeffectiveness of our method, achieving a 1.4\% absolute improvement in relation F1 score overprevious best approach. Introduction”},
	language = {eng},
	booktitle = {Proceedings of the 22nd {Chinese} {National} {Conference} on {Computational} {Linguistics}},
	publisher = {Chinese Information Processing Society of China},
	author = {Xiong, Xiong and Chen, Wang and Yunfei, Liu and Shengyang, Li},
	editor = {Sun, Maosong and Qin, Bing and Qiu, Xipeng and Jiang, Jing and Han, Xianpei},
	month = aug,
	year = {2023},
	pages = {713--725},
}

@inproceedings{han_investigating_2023,
	address = {Toronto, Canada},
	title = {Investigating {Massive} {Multilingual} {Pre}-{Trained} {Machine} {Translation} {Models} for {Clinical} {Domain} via {Transfer} {Learning}},
	url = {https://aclanthology.org/2023.clinicalnlp-1.5/},
	doi = {10.18653/v1/2023.clinicalnlp-1.5},
	abstract = {Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI's MMPLMs “wmt21-dense-24-wide-en-X and X-en (WMT21fb)” which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language pair which \textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly.We prepare carefully aligned \textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge.Our experimental result shows that the fine-tuning is very successful using just 250k well-aligned in-domain EN-ES segments for three sub-task translation testings: clinical cases, clinical terms, and ontology concepts. It achieves very close evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as a high-resource setting in the pre-training.To the best of our knowledge, this is the first work on using MMPLMs towards \textit{clinical domain transfer-learning NMT} successfully for totally unseen languages during pre-training.},
	booktitle = {Proceedings of the 5th {Clinical} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Han, Lifeng and Erofeev, Gleb and Sorokina, Irina and Gladkoff, Serge and Nenadic, Goran},
	editor = {Naumann, Tristan and Ben Abacha, Asma and Bethard, Steven and Roberts, Kirk and Rumshisky, Anna},
	month = jul,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Language model, Pre-training, Machine translation, Transfer learning, Computational linguistics, Learning systems, Domain Knowledge, Natural language processing systems, Computer aided language translation, Fine tuning, Down-stream, Machine translations, Domain machines, Experimental investigations, Language pairs, Machine translation models, Well testing},
	pages = {31--40},
	annote = {Cited by: 2},
}

@inproceedings{blankemeier_interactive_2023,
	address = {Toronto, Canada},
	title = {Interactive {Span} {Recommendation} for {Biomedical} {Text}},
	url = {https://aclanthology.org/2023.clinicalnlp-1.40/},
	doi = {10.18653/v1/2023.clinicalnlp-1.40},
	abstract = {Motivated by the scarcity of high-quality labeled biomedical text, as well as the success of data programming, we introduce KRISS-Search. By leveraging the Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an interactive few-shot span recommendation task that we propose. We first introduce unsupervised KRISS-Search and show that our method outperforms existing methods in identifying spans that are semantically similar to a given span of interest, with {\textbackslash}ensuremath{\textgreater}50\% AUPRC improvement relative to PubMedBERT. We then introduce supervised KRISS-Search, which leverages human interaction to improve the notion of similarity used by unsupervised KRISS-Search. Through simulated human feedback, we demonstrate an enhanced F1 score of 0.68 in classifying spans as semantically similar or different in the low-label setting, outperforming PubMedBERT by 2 F1 points. Finally, supervised KRISS-Search demonstrates competitive or superior performance compared to PubMedBERT in few-shot biomedical named entity recognition (NER) across five benchmark datasets, with an average improvement of 5.6 F1 points. We envision KRISS-Search increasing the efficiency of programmatic data labeling and also providing broader utility as an interactive biomedical search engine.},
	booktitle = {Proceedings of the 5th {Clinical} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Blankemeier, Louis and Zhao, Theodore and Tinn, Robert and Kiblawi, Sid and Gu, Yu and Chaudhari, Akshay and Poon, Hoifung and Zhang, Sheng and Wei, Mu and Preston, J.},
	editor = {Naumann, Tristan and Ben Abacha, Asma and Bethard, Steven and Roberts, Kirk and Rumshisky, Anna},
	month = jul,
	year = {2023},
	pages = {373--384},
}

@inproceedings{karlgren_high-dimensional_2023,
	address = {Washington, D.C.},
	title = {High-dimensional vector spaces can accommodate constructional features quite conveniently},
	url = {https://aclanthology.org/2023.cxgsnlp-1.4/},
	abstract = {Current language processing tools presuppose input in the form of a sequence of high-dimensional vectors with continuous values. Lexical items can be converted to such vectors with standard methodology and subsequent processing is assumed to handle structural features of the string. Constructional features do typically not fit in that processing pipeline: they are not as clearly sequential, they overlap with other items, and the fact that they are combinations of lexical items obscures their ontological status as observable linguistic items in their own right. Constructional grammar frameworks allow for a more general view on how to understand lexical items and their configurations in a common framework. This paper introduces an approach to accommodate that understanding in a vector symbolic architecture, a processing framework which allows for combinations of continuous vectors and discrete items, convenient for various downstream processing using e.g. neural processing or other tools which expect input in vector form.},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {Construction} {Grammars} and {NLP} ({CxGs}+{NLP}, {GURT}/{SyntaxFest} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Karlgren, Jussi},
	editor = {Bonial, Claire and Tayyar Madabushi, Harish},
	month = mar,
	year = {2023},
	pages = {31--35},
}

@inproceedings{jiang_combo_2023,
	address = {Dubrovnik, Croatia},
	title = {{COMBO}: {A} {Complete} {Benchmark} for {Open} {KG} {Canonicalization}},
	url = {https://aclanthology.org/2023.eacl-main.26/},
	doi = {10.18653/v1/2023.eacl-main.26},
	abstract = {Open knowledge graph (KG) consists of (subject, relation, object) triples extracted from millions of raw text. The subject and object noun phrases and the relation in open KG have severe redundancy and ambiguity and need to be canonicalized. Existing datasets for open KG canonicalization only provide gold entity-level canonicalization for noun phrases. In this paper, we present COMBO, a Complete Benchmark for Open KG canonicalization. Compared with existing datasets, we additionally provide gold canonicalization for relation phrases, gold ontology-level canonicalization for noun phrases, as well as source sentences from which triples are extracted. We also propose metrics for evaluating each type of canonicalization. On the COMBO dataset, we empirically compare previously proposed canonicalization methods as well as a few simple baseline methods based on pretrained language models. We find that properly encoding the phrases in a triple using pretrained language models results in better relation canonicalization and ontology-level canonicalization of the noun phrase. We release our dataset, baselines, and evaluation scripts at path/to/url.},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Chengyue and Jiang, Yong and Wu, Weiqi and Zheng, Yuting and Xie, Pengjun and Tu, Kewei},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Ontology, Language model, Computational linguistics, Canonicalization, Ontology's, Simple++, Encodings, Baseline methods, Entity-level, Gold, Modeling results, Noun phrase},
	pages = {340--357},
	annote = {Cited by: 2},
}

@inproceedings{shrivastava_retrieve-and-fill_2023,
	address = {Dubrovnik, Croatia},
	title = {Retrieve-and-{Fill} for {Scenario}-based {Task}-{Oriented} {Semantic} {Parsing}},
	url = {https://aclanthology.org/2023.eacl-main.32/},
	doi = {10.18653/v1/2023.eacl-main.32},
	abstract = {Task-oriented semantic parsing models have achieved strong results in recent years, but unfortunately do not strike an appealing balance between model size, runtime latency, and cross-domain generalizability. We tackle this problem by introducing scenario-based semantic parsing: a variant of the original task which first requires disambiguating an utterance's “scenario” (an intent-slot template with variable leaf spans) before generating its frame, complete with ontology and utterance tokens. This formulation enables us to isolate coarse-grained and fine-grained aspects of the task, each of which we solve with off-the-shelf neural modules, also optimizing for the axes outlined above. Concretely, we create a Retrieve-and-Fill (RAF) architecture comprised of (1) a retrieval module which ranks the best scenario given an utterance and (2) a filling module which imputes spans into the scenario to create the frame. Our model is modular, differentiable, interpretable, and allows us to garner extra supervision from scenarios. RAF achieves strong results in high-resource, low-resource, and multilingual settings, outperforming recent approaches by wide margins despite, using base pre-trained encoders, small sequence lengths, and parallel decoding.},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shrivastava, Akshat and Desai, Shrey and Gupta, Anchit and Elkahky, Ali and Livshits, Aleksandr and Zotov, Alexander and Aly, Ahmed},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {430--447},
}

@inproceedings{edwards_semi-supervised_2023,
	address = {Dubrovnik, Croatia},
	title = {Semi-supervised {New} {Event} {Type} {Induction} and {Description} via {Contrastive} {Loss}-{Enforced} {Batch} {Attention}},
	url = {https://aclanthology.org/2023.eacl-main.275/},
	doi = {10.18653/v1/2023.eacl-main.275},
	abstract = {Most event extraction methods have traditionally relied on an annotated set of event types. However, creating event ontologies and annotating supervised training data are expensive and time-consuming. Previous work has proposed semi-supervised approaches which leverage seen (annotated) types to learn how to automatically discover new event types. State-of-the-art methods, both semi-supervised or fully unsupervised, use a form of reconstruction loss on specific tokens in a context. In contrast, we present a novel approach to semi-supervised new event type induction using a masked contrastive loss, which learns similarities between event mentions by enforcing an attention mechanism over the data minibatch. We further disentangle the discovered clusters by approximating the underlying manifolds in the data, which allows us to achieve an adjusted rand index score of 48.85\%. Building on these clustering results, we extend our approach to two new tasks: predicting the type name of the discovered clusters and linking them to FrameNet frames.},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Edwards, Carl and Ji, Heng},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {3805--3827},
}

@inproceedings{cao_event_2023,
	address = {Singapore},
	title = {Event {Ontology} {Completion} with {Hierarchical} {Structure} {Evolution} {Networks}},
	url = {https://aclanthology.org/2023.emnlp-main.21/},
	doi = {10.18653/v1/2023.emnlp-main.21},
	abstract = {Traditional event detection methods require predefined event schemas. However, manually defining event schemas is expensive and the coverage of schemas is limited. To this end, some works study the event type induction (ETI) task, which discovers new event types via clustering. However, the setting of ETI suffers from two limitations: event types are not linked into the existing hierarchy and have no semantic names. In this paper, we propose a new research task named Event Ontology Completion (EOC), which aims to simultaneously achieve event clustering, hierarchy expansion and type naming. Furthermore, we develop a Hierarchical Structure Evolution Network (HalTon) for this new task. Specifically, we first devise a Neighborhood Contrastive Clustering module to cluster unlabeled event instances. Then, we propose a Hierarchy-Aware Linking module to incorporate the hierarchical information for event expansion. Finally, we generate meaningful names for new types via an In-Context Learning-based Naming module. Extensive experiments indicate that our method achieves the best performance, outperforming the baselines by 8.23\%, 8.79\% and 8.10\% of ARI score on three datasets.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Pengfei and Hao, Yupu and Chen, Yubo and Liu, Kang and Xu, Jiexin and Li, Huaijun and Jiang, Xiaojian and Zhao, Jun},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {306--320},
}

@inproceedings{makhervaks_clinical_2023,
	address = {Singapore},
	title = {Clinical {Contradiction} {Detection}},
	url = {https://aclanthology.org/2023.emnlp-main.80/},
	doi = {10.18653/v1/2023.emnlp-main.80},
	abstract = {Detecting contradictions in text is essential in determining the validity of the literature and sources that we consume. Medical corpora are riddled with conflicting statements. This is due to the large throughput of new studies and the difficulty in replicating experiments, such as clinical trials. Detecting contradictions in this domain is hard since it requires clinical expertise. We present a distant supervision approach that leverages a medical ontology to build a seed of potential clinical contradictions over 22 million medical abstracts. We automatically build a labeled training dataset consisting of paired clinical sentences that are grounded in an ontology and represent potential medical contradiction. The dataset is used to weakly-supervise state-of-the-art deep learning models showing significant empirical improvements across multiple medical contradiction datasets.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Makhervaks, Dave and Gillis, Plia and Radinsky, Kira},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {1248--1263},
}

@inproceedings{li_glen_2023,
	address = {Singapore},
	title = {{GLEN}: {General}-{Purpose} {Event} {Detection} for {Thousands} of {Types}},
	url = {https://aclanthology.org/2023.emnlp-main.170/},
	doi = {10.18653/v1/2023.emnlp-main.170},
	abstract = {The progress of event extraction research has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 205K event mentions with 3,465 different types, making it more than 20x larger in ontology than today's largest event dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model specifically designed to handle the large ontology size in GLEN. We show that our model exhibits superior performance compared to a range of baselines including InstructGPT. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance for this new dataset.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Sha and Zhan, Qiusi and Conger, Kathryn and Palmer, Martha and Ji, Heng and Han, Jiawei},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {2823--2838},
}

@inproceedings{schulhoff_ignore_2023,
	address = {Singapore},
	title = {Ignore {This} {Title} and {HackAPrompt}: {Exposing} {Systemic} {Vulnerabilities} of {LLMs} {Through} a {Global} {Prompt} {Hacking} {Competition}},
	url = {https://aclanthology.org/2023.emnlp-main.302/},
	doi = {10.18653/v1/2023.emnlp-main.302},
	abstract = {Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource and quantitative study on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive ontology of the types of adversarial prompts.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Schulhoff, Sander and Pinto, Jeremy and Khan, Anaum and Bouchard, Louis-François and Si, Chenglei and Anati, Svetlina and Tagliabue, Valen and Kost, Anson and Carnahan, Christopher and Boyd-Graber, Jordan},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Language model, Computational linguistics, Chatbots, Natural language processing systems, Large-scales, State of the art, 'lacuna', Freeforms, Global scale, Personal computing, Quantitative study, Security threats, User engagement},
	pages = {4945--4977},
	annote = {Cited by: 31; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{han_log-fgaer_2023,
	address = {Singapore},
	title = {Log-{FGAER}: {Logic}-{Guided} {Fine}-{Grained} {Address} {Entity} {Recognition} from {Multi}-{Turn} {Spoken} {Dialogue}},
	url = {https://aclanthology.org/2023.emnlp-main.432/},
	doi = {10.18653/v1/2023.emnlp-main.432},
	abstract = {Fine-grained address entity recognition (FGAER) from multi-turn spoken dialogues is particularly challenging. The major reason lies in that a full address is often formed through a conversation process. Different parts of an address are distributed through multiple turns of a dialogue with spoken noises. It is nontrivial to extract by turn and combine them. This challenge has not been well emphasized by main-stream entity extraction algorithms. To address this issue, we propose in this paper a logic-guided fine-grained address recognition method (Log-FGAER), where we formulate the address hierarchy relationship as the logic rule and softly apply it in a probabilistic manner to improve the accuracy of FGAER. In addition, we provide an ontology-based data augmentation methodology that employs ChatGPT to augment a spoken dialogue dataset with labeled address entities. Experiments are conducted using datasets generated by the proposed data augmentation technique and derived from real-world scenarios. The results of the experiment demonstrate the efficacy of our proposal.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Han, Xue and Wang, Yitong and Hu, Qian and Hu, Pengwei and Deng, Chao and Feng, Junlan},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {6988--6997},
}

@inproceedings{shavarani_spel_2023,
	address = {Singapore},
	title = {{SpEL}: {Structured} {Prediction} for {Entity} {Linking}},
	url = {https://aclanthology.org/2023.emnlp-main.686/},
	doi = {10.18653/v1/2023.emnlp-main.686},
	abstract = {Entity linking is a prominent thread of research focused on structured data creation by linking spans of text to an ontology or knowledge source. We revisit the use of structured prediction for entity linking which classifies each individual input token as an entity, and aggregates the token predictions. Our system, called SpEL (Structured prediction for Entity Linking) is a state-of-the-art entity linking system that uses some new ideas to apply structured prediction to the task of entity linking including: two refined fine-tuning steps; a context sensitive prediction aggregation strategy; reduction of the size of the model's output vocabulary, and; we address a common problem in entity-linking systems where there is a training vs. inference tokenization mismatch. Our experiments show that we can outperform the state-of-the-art on the commonly used AIDA benchmark dataset for entity linking to Wikipedia. Our method is also very compute efficient in terms of number of parameters and speed of inference.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Shavarani, Hassan and Sarkar, Anoop},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {11123--11137},
}

@inproceedings{kartchner_comprehensive_2023,
	address = {Singapore},
	title = {A {Comprehensive} {Evaluation} of {Biomedical} {Entity} {Linking} {Models}},
	url = {https://aclanthology.org/2023.emnlp-main.893/},
	doi = {10.18653/v1/2023.emnlp-main.893},
	abstract = {Biomedical entity linking (BioEL) is the process of connecting entities referenced in documents to entries in biomedical databases such as the Unified Medical Language System (UMLS) or Medical Subject Headings (MeSH). The study objective was to comprehensively evaluate nine recent state-of-the-art biomedical entity linking models under a unified framework. We compare these models along axes of (1) accuracy, (2) speed, (3) ease of use, (4) generalization, and (5) adaptability to new ontologies and datasets. We additionally quantify the impact of various preprocessing choices such as abbreviation detection. Systematic evaluation reveals several notable gaps in current methods. In particular, current methods struggle to correctly link genes and proteins and often have difficulty effectively incorporating context into linking decisions. To expedite future development and baseline testing, we release our unified evaluation framework and all included models on GitHub at https://github.com/davidkartchner/biomedical-entity-linking},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kartchner, David and Deng, Jennifer and Lohiya, Shubham and Kopparthi, Tejasri and Bathala, Prasanth and Domingo-Fernández, Daniel and Mitchell, Cassie},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {14462--14478},
}

@inproceedings{zhao_anytod_2023,
	address = {Singapore},
	title = {{AnyTOD}: {A} {Programmable} {Task}-{Oriented} {Dialog} {System}},
	url = {https://aclanthology.org/2023.emnlp-main.1006/},
	doi = {10.18653/v1/2023.emnlp-main.1006},
	abstract = {We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, AnyTOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing dialog policy is executed to recommend actions AnyTOD should take. This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. We demonstrate state-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ. In addition, we release STARv2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jeffrey and Cao, Yuan and Gupta, Raghav and Lee, Harrison and Rastogi, Abhinav and Wang, Mingqiu and Soltau, Hagen and Shafran, Izhak and Wu, Yonghui},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Language model, Data annotation, Computational linguistics, Zero-shot learning, Dialogue systems, Ontology's, End to end, Generalisation, Task-oriented, Keep track of, Model training, Program logic, Stars},
	pages = {16189--16204},
	annote = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{yu_documentnet_2023,
	address = {Singapore},
	title = {{DocumentNet}: {Bridging} the {Data} {Gap} in {Document} {Pre}-training},
	url = {https://aclanthology.org/2023.emnlp-industry.66/},
	doi = {10.18653/v1/2023.emnlp-industry.66},
	abstract = {Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multimodal capabilities for VDER.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Industry} {Track}},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Lijun and Miao, Jin and Sun, Xiaoyu and Chen, Jiayi and Hauptmann, Alexander and Dai, Hanjun and Wei, Wei},
	editor = {Wang, Mingxuan and Zitouni, Imed},
	month = dec,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Document understanding, Knowledge management, Pre-training, Computational linguistics, Knowledge transfer, 'current, Labeled data, Retrieval models, Entity retrieval, Broad application, Data gap, Privacy constraints},
	pages = {707--722},
	annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{lefebvre_rethinking_2023,
	address = {Dubrovnik, Croatia},
	title = {Rethinking the {Event} {Coding} {Pipeline} with {Prompt} {Entailment}},
	url = {https://aclanthology.org/2023.fever-1.1/},
	doi = {10.18653/v1/2023.fever-1.1},
	abstract = {For monitoring crises, political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable, particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types, a task referred to as event coding. Typically, domain experts craft an event type ontology, annotators label a large dataset and technical experts develop a supervised coding system. In this work, we propose PR-ENT, a new event coding approach that is more flexible and resource-efficient, while maintaining competitive accuracy: first, we extend an event description such as “Military injured two civilians” by a template, e.g. “People were [Z]” and prompt a pre-trained (cloze) language model to fill the slot Z. Second, we select suitable answer candidates Zstar = “injured”, “hurt”... by treating the event description as premise and the filled templates as hypothesis in a textual entailment task. In a final step, the selected answer candidate can be mapped to its corresponding event type. This allows domain experts to draft the codebook directly as labeled prompts and interpretable answer candidates. This human-in-the-loop process is guided by our codebook design tool. We show that our approach is robust through several checks: perturbing the event description and prompt template, restricting the vocabulary and removing contextual information.},
	booktitle = {Proceedings of the {Sixth} {Fact} {Extraction} and {VERification} {Workshop} ({FEVER})},
	publisher = {Association for Computational Linguistics},
	author = {Lefebvre, Clément and Stoehr, Niklas},
	editor = {Akhtar, Mubashara and Aly, Rami and Christodoulopoulos, Christos and Cocarascu, Oana and Guo, Zhijiang and Mittal, Arpit and Schlichtkrull, Michael and Thorne, James and Vlachos, Andreas},
	month = may,
	year = {2023},
	pages = {1--16},
}

@inproceedings{ajjour_topic_2023,
	address = {Dubrovnik, Croatia},
	title = {Topic {Ontologies} for {Arguments}},
	url = {https://aclanthology.org/2023.findings-eacl.104/},
	doi = {10.18653/v1/2023.findings-eacl.104},
	abstract = {Many computational argumentation tasks, such as stance classification, are topic-dependent: The effectiveness of approaches to these tasks depends largely on whether they are trained with arguments on the same topics as those on which they are tested. The key question is: What are these training topics? To answer this question, we take the first step of mapping the argumentation landscape with The Argument Ontology (TAO). TAO draws on three authoritative sources for argument topics: the World Economic Forum, Wikipedia's list of controversial topics, and Debatepedia. By comparing the topics in our ontology with those in 59 argument corpora, we perform the first comprehensive assessment of their topic coverage. While TAO already covers most of the corpus topics, the corpus topics barely cover all the topics in TAO. This points to a new goal for corpus construction to achieve a broad topic coverage and thus better generalizability of computational argumentation approaches.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Ajjour, Yamen and Kiesel, Johannes and Stein, Benno and Potthast, Martin},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {1411--1427},
}

@inproceedings{coca_more_2023,
	address = {Dubrovnik, Croatia},
	title = {More {Robust} {Schema}-{Guided} {Dialogue} {State} {Tracking} via {Tree}-{Based} {Paraphrase} {Ranking}},
	url = {https://aclanthology.org/2023.findings-eacl.106/},
	doi = {10.18653/v1/2023.findings-eacl.106},
	abstract = {The schema-guided paradigm overcomes scalability issues inherent in building task-oriented dialogue (TOD) agents with static ontologies. Rather than operating on dialogue context alone, agents have access to hierarchical schemas containing task-relevant natural language descriptions. Fine-tuned language models excel at schema-guided dialogue state tracking (DST) but are sensitive to the writing style of the schemas. We explore methods for improving the robustness of DST models. We propose a framework for generating synthetic schemas which uses tree-based ranking to jointly optimise lexical diversity and semantic faithfulness. The robust generalisation of strong baselines is improved when augmenting their training data with prompts generated by our framework, as demonstrated by marked improvements in average Joint Goal Accuracy (JGA) and schema sensitivity (SS) on the SGD-X benchmark.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Coca, Alexandru and Tseng, Bo-Hsiang and Lin, Weizhe and Byrne, Bill},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Semantics, Computational linguistics, Natural languages, Ontology's, In-buildings, State tracking, Language description, Task-oriented, Hierarchical schema, Scalability issue, Task relevant, Tree-based},
	pages = {1443--1454},
	annote = {Cited by: 2},
}

@inproceedings{weinstein_unsupervised_2023,
	address = {Toronto, Canada},
	title = {Unsupervised {Mapping} of {Arguments} of {Deverbal} {Nouns} to {Their} {Corresponding} {Verbal} {Labels}},
	url = {https://aclanthology.org/2023.findings-acl.184/},
	doi = {10.18653/v1/2023.findings-acl.184},
	abstract = {Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling arguments of nominalized constructions are based on semantic annotation and require semantic ontologies, making their applications restricted to a small set of nouns. We propose to adopt instead a more syntactic approach, which maps the arguments of deverbal nouns to the universal-dependency relations of the corresponding verbal construction. We present an unsupervised mechanism—based on contextualized word representations—which allows to enrich universal-dependency trees with dependency arcs denoting arguments of deverbal nouns, using the same labels as the corresponding verbal cases. By sharing the same label set as in the verbal case, patterns that were developed for verbs can be applied without modification but with high accuracy also to the nominal constructions.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Weinstein, Aviv and Goldberg, Yoav},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {2921--2935},
}

@inproceedings{he_language_2023,
	address = {Toronto, Canada},
	title = {Language {Model} {Analysis} for {Ontology} {Subsumption} {Inference}},
	url = {https://aclanthology.org/2023.findings-acl.213/},
	doi = {10.18653/v1/2023.findings-acl.213},
	abstract = {Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {He, Yuan and Chen, Jiaoyan and Jimenez-Ruiz, Ernesto and Dong, Hang and Horrocks, Ian},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Background knowledge, Computational linguistics, Language inference, Natural languages, Ontology's, Modeling analyzes, Open systems, Different domains, Simple++, OWL ontologies, Open source software, Research interests},
	pages = {3439--3453},
	annote = {Cited by: 13; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{guo_eventoa_2023,
	address = {Toronto, Canada},
	title = {{EventOA}: {An} {Event} {Ontology} {Alignment} {Benchmark} {Based} on {FrameNet} and {Wikidata}},
	url = {https://aclanthology.org/2023.findings-acl.637/},
	doi = {10.18653/v1/2023.findings-acl.637},
	abstract = {Event ontology provides a shared and formal specification about what happens in the real world and can benefit many natural language understanding tasks. However, the independent development of event ontologies often results in heterogeneous representations that raise the need for establishing alignments between semantically related events. There exists a series of works about ontology alignment (OA), but they only focus on the entity-based OA, and neglect the event-based OA. To fill the gap, we construct an Event Ontology Alignment (EventOA) dataset based on FrameNet and Wikidata, which consists of 900+ event type alignments and 8,000+ event argument alignments. Furthermore, we propose a multi-view event ontology alignment (MEOA) method, which utilizes description information (i.e., name, alias and definition) and neighbor information (i.e., subclass and superclass) to obtain richer representation of the event ontologies. Extensive experiments show that our MEOA outperforms the existing entity-based OA methods and can serve as a strong baseline for EventOA research.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Guo, Shaoru and Wang, Chenhao and Chen, Yubo and Liu, Kang and Li, Ru and Zhao, Jun},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {10038--10052},
}

@inproceedings{huang_concept2box_2023,
	address = {Toronto, Canada},
	title = {{Concept2Box}: {Joint} {Geometric} {Embeddings} for {Learning} {Two}-{View} {Knowledge} {Graphs}},
	url = {https://aclanthology.org/2023.findings-acl.642/},
	doi = {10.18653/v1/2023.findings-acl.642},
	abstract = {Knowledge graph embeddings (KGE) have been extensively studied to embed large-scale relational data for many real-world applications. Existing methods have long ignored the fact many KGs contain two fundamentally different views: high-level ontology-view concepts and fine-grained instance-view entities. They usually embed all nodes as vectors in one latent space. However, a single geometric representation fails to capture the structural differences between two views and lacks probabilistic semantics towards concepts' granularity. We propose Concept2Box, a novel approach that jointly embeds the two views of a KG using dual geometric representations. We model concepts with box embeddings, which learn the hierarchy structure and complex relations such as overlap and disjoint among them. Box volumes can be interpreted as concepts' granularity. Different from concepts, we model entities as vectors. To bridge the gap between concept box embeddings and entity vector embeddings, we propose a novel vector-to-box distance metric and learn both embeddings jointly. Experiments on both the public DBpedia KG and a newly-created industrial KG showed the effectiveness of Concept2Box.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Zijie and Wang, Daheng and Huang, Binxuan and Zhang, Chenwei and Shang, Jingbo and Liang, Yan and Wang, Zhengyang and Li, Xian and Faloutsos, Christos and Sun, Yizhou and Wang, Wei},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {10105--10118},
}

@inproceedings{zhou_inductive_2023,
	address = {Singapore},
	title = {Inductive {Relation} {Inference} of {Knowledge} {Graph} {Enhanced} by {Ontology} {Information}},
	url = {https://aclanthology.org/2023.findings-emnlp.431/},
	doi = {10.18653/v1/2023.findings-emnlp.431},
	abstract = {The inductive inference of the knowledge graph aims to complete the potential relations between the new unknown entities in the graph. Most existing methods are based on entity-independent features such as graph structure information and relationship information to inference. However, the neighborhood of these new entities is often too sparse to obtain enough information to build these features effectively. In this work, we propose a knowledge graph inductive inference method that fuses ontology information. Based on the enclosing subgraph, we bring in feature embeddings of concepts corresponding to entities to learn the semantic information implicit in the ontology. Considering that the ontology information of entities may be missing, we build a type constraint regular loss to explicitly model the semantic connections between entities and concepts, and thus capture the missing concepts of entities. Experimental results show that our approach significantly outperforms large language models like ChatGPT on two benchmark datasets, YAGO21K-610 and DB45K-165, and improves the MRR metrics by 15.4\% and 44.1\%, respectively, when compared with the state-of-the-art methods.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Wentao and Zhao, Jun and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Ontology, Semantics, Computational linguistics, Inductive inference, Graphic methods, Ontology's, Large datasets, Graph structures, Unknown entities, Structure information, Subgraphs, Neighbourhood, AS graph, Inference methods},
	pages = {6491--6502},
	annote = {Cited by: 4; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{chen_mixture_2023,
	address = {Singapore},
	title = {Mixture of {Soft} {Prompts} for {Controllable} {Data} {Generation}},
	url = {https://aclanthology.org/2023.findings-emnlp.988/},
	doi = {10.18653/v1/2023.findings-emnlp.988},
	abstract = {Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating multi-attribute data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Derek and Lee, Celine and Lu, Yunan and Rosati, Domenic and Yu, Zhou},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Language model, Semantics, Forecasting, Computational linguistics, Structured prediction, Ontology's, Natural language processing systems, Large models, Prediction tasks, Data generation, Direct prediction, Fluents, Natural language patterns, Output formats},
	pages = {14815--14833},
	annote = {Cited by: 6; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{lu_pivoine_2023,
	address = {Singapore},
	title = {{PIVOINE}: {Instruction} {Tuning} for {Open}-world {Entity} {Profiling}},
	url = {https://aclanthology.org/2023.findings-emnlp.1009/},
	doi = {10.18653/v1/2023.findings-emnlp.1009},
	abstract = {This work considers the problem of Open-world Entity Profiling, a sub-domain of Open-world Information Extraction (Open-world IE). Unlike the conventional closed-world IE, Open-world IE is considered a more general situation where entities and relations could be beyond a predefined ontology. We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction-tuning dataset for Open-world Entity Profiling enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world Entity Profiling with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional methods and ChatGPT-based baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge of entity profiling.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Lu, Keming and Pan, Xiaoman and Song, Kaiqiang and Zhang, Hongming and Yu, Dong and Chen, Jianshu},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Computational linguistics, Natural languages, Ontology's, Fine grained, General situation, Generalization capability, Open world, Subdomain},
	pages = {15108--15127},
	annote = {Cited by: 5; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{alvez_towards_2023,
	address = {University of the Basque Country, Donostia - San Sebastian, Basque Country},
	title = {Towards {Effective} {Correction} {Methods} {Using} {WordNet} {Meronymy} {Relations}},
	url = {https://aclanthology.org/2023.gwc-1.4/},
	abstract = {In this paper, we analyse and compare several correction methods of knowledge resources with the purpose of improving the abilities of systems that require commonsense reasoning with the least possible human-effort. To this end, we cross-check the WordNet meronymy relation member against the knowledge encoded in a SUMO-based first-order logic ontology on the basis of the mapping between WordNet and SUMO. In particular, we focus on the knowledge in WordNet regarding the taxonomy of animals and plants. Despite being created manually, these knowledge resources — WordNet, SUMO and their mapping — are not free of errors and discrepancies. Thus, we propose three correction methods by semi-automatically improving the alignment between WordNet and SUMO, by performing some few corrections in SUMO and by combining the above two strategies. The evaluation of each method includes the required human-effort and the achieved improvement on unseen data from the WebChild project, that is tested using first-order logic automated theorem provers.},
	booktitle = {Proceedings of the 12th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Álvez, Javier and Gonzalez-Dios, Itziar and Rigau, German},
	editor = {Rigau, German and Bond, Francis and Rademaker, Alexandre},
	month = jan,
	year = {2023},
	pages = {31--40},
}

@inproceedings{biagetti_linking_2023,
	address = {University of the Basque Country, Donostia - San Sebastian, Basque Country},
	title = {Linking the {Sanskrit} {WordNet} to the {Vedic} {Dependency} {Treebank}: a pilot study},
	url = {https://aclanthology.org/2023.gwc-1.9/},
	abstract = {The Sanskrit WordNet is a resource currently under development, whose core was induced from a Vedic text sample semantically annotated by means of an ontology mapped on the Princeton WordNet synsets. Building on a previous case study on Ancient Greek (Zanchi et al. 2021), we show how sentence frames can be extracted from morphosyntactically parsed corpora by linking an existing dependency treebank of Vedic Sanskrit to verbal synsets in the Sanskrit WordNet. Our case study focuses on two verbs of asking, yāc- and prach-, featuring a high degree of variability in sentence frames. Treebanks enhanced with WordNet-based semantic information revealed to be of crucial help in motivating sentence frame alternations.},
	booktitle = {Proceedings of the 12th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Biagetti, Erica and Zanchi, Chiara and Luraghi, Silvia},
	editor = {Rigau, German and Bond, Francis and Rademaker, Alexandre},
	month = jan,
	year = {2023},
	pages = {77--83},
}

@inproceedings{zinn_mapping_2023,
	address = {University of the Basque Country, Donostia - San Sebastian, Basque Country},
	title = {Mapping {GermaNet} for the {Semantic} {Web} using {OntoLex}-{Lemon}},
	url = {https://aclanthology.org/2023.gwc-1.19/},
	abstract = {GermaNet is a large lexical-semantic net that relates German nouns, verbs, and adjectives semantically. The word net has been manually constructed over the last 25 years and hence presents a high-quality, valuable resource for German. While GermaNet is maintained in a Postgres database, all its content can be exported as an XML-based serialisation. Recently, this XML representation has been converted into RDF, largely by staying close to GermaNet's principle of arrangement where lexunits that share the same meaning are grouped together into so-called synsets. With each lexical unit and synset now globally addressable via a unique resource identifier, it has become much easier to link together GermaNet entries with other lexical and semantic resources. In terms of semantic interoperability, however, the RDF variant of GermaNet leaves much to be desired. In this paper, we describe yet another conversion from GermaNet's XML representation to RDF. The new conversion makes use of the OntoLex-Lemon ontology, and therefore, presents a decisive step toward a GermaNet representation with a much higher level of semantic interoperability, and which makes it possible to use GermaNet with other wordnets that already support this conceptualisation of lexica.},
	booktitle = {Proceedings of the 12th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Zinn, Claus and Hinrichs, Marie and Hinrichs, Erhard},
	editor = {Rigau, German and Bond, Francis and Rademaker, Alexandre},
	month = jan,
	year = {2023},
	pages = {158--166},
}

@inproceedings{pedersen_reusing_2023,
	address = {University of the Basque Country, Donostia - San Sebastian, Basque Country},
	title = {Reusing the {Danish} {WordNet} for a {New} {Central} {Word} {Register} for {Danish} - a {Project} {Report}},
	url = {https://aclanthology.org/2023.gwc-1.26/},
	abstract = {In this paper we report on a new Danish lexical initiative, the Central Word Register for Danish, (COR), which aims at providing an open-source, well curated and large-coverage lexicon for AI purposes. The semantic part of the lexicon (COR-S) relies to a large extent on the lexical-semantic information provided in the Danish wordnet, DanNet. However, we have taken the opportunity to evaluate and curate the wordnet information while compiling the new resource. Some information types have been simplified and more systematically curated. This is the case for the hyponymy relations, the ontological typing, and the sense inventory, i.e. the treatment of polysemy, including systematic polysemy.},
	booktitle = {Proceedings of the 12th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Pedersen, Bolette and Nimb, Sanni and Sørensen, Nathalie and Olsen, Sussi and Flörke, Ida and Troelsgård, Thomas},
	editor = {Rigau, German and Bond, Francis and Rademaker, Alexandre},
	month = jan,
	year = {2023},
	pages = {214--219},
}

@inproceedings{simov_recent_2023,
	address = {University of the Basque Country, Donostia - San Sebastian, Basque Country},
	title = {Recent {Developments} in {BTB}-{WordNet}},
	url = {https://aclanthology.org/2023.gwc-1.27/},
	abstract = {The paper reports on recent developments in Bulgarian BTB-WordNet (BTB-WN). This resource is viewed as playing a central role with respect to the integration and interlinking of various language resources such as: e-dictionaries (morphological, terminological, bilingual, orthographic, etymological and explanatory, etc., including editions from previous periods); corpora (coming from outside or being internal - like the corpus of definitions as well as the corpus of examples to synset meanings); ontologies (such as CIDOC-CRM, DBpedia, etc.); sources of world knowledge (such as information from the Bulgarian Encyclopedia, Wikipedia, etc.). The paper also gives information about a number of applications built on BTB-WN. These are: the Bulgaria-centered knowledge graph, the All about word application as well as some education-oriented exercises.},
	booktitle = {Proceedings of the 12th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Simov, Kiril and Osenova, Petya},
	editor = {Rigau, German and Bond, Francis and Rademaker, Alexandre},
	month = jan,
	year = {2023},
	pages = {220--227},
}

@inproceedings{rademaker_semantic_2023,
	address = {University of the Basque Country, Donostia - San Sebastian, Basque Country},
	title = {Semantic {Parsing} and {Sense} {Tagging} the {Princeton} {WordNet} {Gloss} {Corpus}},
	url = {https://aclanthology.org/2023.gwc-1.30/},
	abstract = {In 2008, the Princeton team released the last version of the “Princeton Annotated Gloss Corpus”. In this corpus. The word forms from the definitions and examples (glosses) of Princeton WordNet are manually linked to the context-appropriate sense in WordNet. However, the annotation was not complete, and the dataset was never officially released as part of WordNet 3.0, remaining as one of the standoff files available for download. Eleven years later, in 2019, one of the authors of this paper restarted the project aiming to complete the sense annotation of the approximately 200 thousand word forms not yet annotated. Here, we provide additional motivations to complete this dataset and report the progress in the work and evaluations. Intending to provide an extra level of consistency in the sense annotation and a deep semantic representation of the definitions and examples promoting WordNet from a lexical resource to a lightweight ontology, we now employ the English Resource Grammar (ERG), a broad-coverage HPSG grammar of English to parse the sentences and project the sense annotations from the surface words to the ERG predicates. We also report some initial steps on upgrading the corpus to WordNet 3.1 to facilitate mapping the data to other lexical resources.},
	booktitle = {Proceedings of the 12th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Rademaker, Alexandre and Basu, Abhishek and Veluri, Rajkiran},
	editor = {Rigau, German and Bond, Francis and Rademaker, Alexandre},
	month = jan,
	year = {2023},
	pages = {243--253},
}

@inproceedings{zotova_towards_2023,
	address = {University of the Basque Country, Donostia - San Sebastian, Basque Country},
	title = {Towards the integration of {WordNet} into {ClinIDMap}},
	url = {https://aclanthology.org/2023.gwc-1.42/},
	abstract = {This paper presents the integration of WordNet knowledge resource into ClinIDMap tool, which aims to map identifiers between clinical ontologies and lexical resources. ClinIDMap interlinks identifiers from UMLS, SMOMED-CT, ICD-10 and the corresponding Wikidata and Wikipedia articles for concepts from the UMLS Metathesaurus. The main goal of the tool is to provide semantic interoperability across the clinical concepts from various knowledge bases. As a side effect, the mapping enriches already annotated medical corpora in multiple languages with new labels. In this new release, we add WordNet 3.0 and 3.1 synsets using the available mappings through Wikidata. Thanks to cross-lingual links in MCR we also include the corresponding synsets in other languages and also, extend further ClinIDMap with different domain information. Finally, the final resource helps in the task of enriching of already annotated clinical corpora with additional semantic annotations.},
	booktitle = {Proceedings of the 12th {Global} {Wordnet} {Conference}},
	publisher = {Global Wordnet Association},
	author = {Zotova, Elena and Cuadros, Montse and Rigau, German},
	editor = {Rigau, German and Bond, Francis and Rademaker, Alexandre},
	month = jan,
	year = {2023},
	pages = {352--362},
}

@inproceedings{mustafa_annotating_2023,
	address = {Dubrovnik, Croatia},
	title = {Annotating {PubMed} {Abstracts} with {MeSH} {Headings} using {Graph} {Neural} {Network}},
	url = {https://aclanthology.org/2023.insights-1.9/},
	doi = {10.18653/v1/2023.insights-1.9},
	abstract = {The number of scientific publications in the biomedical domain is continuously increasing with time. An efficient system for indexing these publications is required to make the information accessible according to the user's information needs. Task 10a of the BioASQ challenge aims to classify PubMed articles according to the MeSH ontology so that new publications can be grouped with similar preexisting publications in the field without the assistance of time-consuming and costly annotations by human annotators. In this work, we use Graph Neural Network (GNN) in the link prediction setting to exploit potential graph-structured information present in the dataset which could otherwise be neglected by transformer-based models. Additionally, we provide error analysis and a plausible reason for the substandard performance achieved by GNN.},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Insights} from {Negative} {Results} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Mustafa, Faizan E and Boutalbi, Rafika and Iurshina, Anastasiia},
	editor = {Tafreshi, Shabnam and Akula, Arjun and Sedoc, João and Drozd, Aleksandr and Rogers, Anna and Rumshisky, Anna},
	month = may,
	year = {2023},
	pages = {75--81},
}

@inproceedings{spaulding_darpa_2023,
	address = {Nancy, France},
	title = {The {DARPA} {Wikidata} {Overlay}: {Wikidata} as an ontology for natural language processing},
	url = {https://aclanthology.org/2023.isa-1.1/},
	abstract = {With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, \textit{events} and \textit{actions} are often labeled with eventive nouns (e.g., the process of diagnosing a person's illness is labeled “diagnosis”), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.},
	booktitle = {Proceedings of the 19th {Joint} {ACL}-{ISO} {Workshop} on {Interoperable} {Semantics} ({ISA}-19)},
	publisher = {Association for Computational Linguistics},
	author = {Spaulding, Elizabeth and Conger, Kathryn and Gershman, Anatole and Uceda-Sosa, Rosario and Brown, Susan Windisch and Pustejovsky, James and Anick, Peter and Palmer, Martha},
	editor = {Bunt, Harry},
	month = jun,
	year = {2023},
	pages = {1--10},
}

@inproceedings{saba_towards_2023,
	address = {Nancy, France},
	title = {Towards {Ontologically} {Grounded} and {Language}-{Agnostic} {Knowledge} {Graphs}},
	url = {https://aclanthology.org/2023.iwcs-1.11/},
	abstract = {Knowledge graphs (KGs) have become the standard technology for the representation of factual information in applications such as recommendation engines, search, and question-answering systems. However, the continual updating of KGs, as well as the integration of KGs from different domains and KGs in different languages, remains to be a major challenge. What we suggest here is that by a reification of abstract objects and by acknowledging the ontological distinction between concepts and types, we arrive at an ontologically grounded and language-agnostic representation that can alleviate the difficulties in KG integration.},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Computational} {Semantics}},
	publisher = {Association for Computational Linguistics},
	author = {Saba, Walid},
	editor = {Amblard, Maxime and Breitholtz, Ellen},
	month = jun,
	year = {2023},
	pages = {94--98},
}

@inproceedings{strakova_extending_2023,
	address = {Toronto, Canada},
	title = {Extending an {Event}-type {Ontology}: {Adding} {Verbs} and {Classes} {Using} {Fine}-tuned {LLMs} {Suggestions}},
	url = {https://aclanthology.org/2023.law-1.9/},
	doi = {10.18653/v1/2023.law-1.9},
	abstract = {In this project, we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task, namely adding descriptive words (verbs) to an existing (but incomplete, as of yet) ontology of event types. Several research questions have been focused on, from the investigation of a possible heuristics to provide at least hints to annotators which verbs to include and which are outside the current version of the ontology, to the possible use of the automatic scores to help the annotators to be more efficient in finding a threshold for identifying verbs that cannot be assigned to any existing class and therefore they are to be used as seeds for a new class. We have also carefully examined the correlation of the automatic scores with the human annotation. While the correlation turned out to be strong, its influence on the annotation proper is modest due to its near linearity, even though the mere fact of such pre-annotation leads to relatively short annotation times.},
	booktitle = {Proceedings of the 17th {Linguistic} {Annotation} {Workshop} ({LAW}-{XVII})},
	publisher = {Association for Computational Linguistics},
	author = {Straková, Jana and Fučíková, Eva and Hajič, Jan and Urešová, Zdeňka},
	editor = {Prange, Jakob and Friedrich, Annemarie},
	month = jul,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Learning systems, Ontology's, 'current, Research questions, Event Types, Human annotations, Machine learning methods},
	pages = {85--95},
	annote = {Cited by: 2},
}

@inproceedings{pannach_modeling_2023,
	address = {Vienna, Austria},
	title = {Modeling and {Comparison} of {Narrative} {Domains} with {Shallow} {Ontologies}},
	url = {https://aclanthology.org/2023.ldk-1.26/},
	booktitle = {Proceedings of the 4th {Conference} on {Language}, {Data} and {Knowledge}},
	publisher = {NOVA CLUNL, Portugal},
	author = {Pannach, Franziska and Blaschke, Theresa},
	editor = {Carvalho, Sara and Khan, Anas Fahad and Anić, Ana Ostroški and Spahiu, Blerina and Gracia, Jorge and McCrae, John P. and Gromann, Dagmar and Heinisch, Barbara and Salgado, Ana},
	month = sep,
	year = {2023},
	pages = {274--280},
}

@inproceedings{remy_detecting_2023,
	address = {Dubrovnik, Croatia},
	title = {Detecting {Idiomatic} {Multiword} {Expressions} in {Clinical} {Terminology} using {Definition}-{Based} {Representation} {Learning}},
	url = {https://aclanthology.org/2023.mwe-1.11/},
	doi = {10.18653/v1/2023.mwe-1.11},
	abstract = {This paper shines a light on the potential of definition-based semantic models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs) in clinical terminology. Our study focuses on biomedical entities defined in the UMLS ontology and aims to help prioritize the translation efforts of these entities. In particular, we develop an effective tool for scoring the idiomaticity of biomedical MWEs based on the degree of similarity between the semantic representations of those MWEs and a weighted average of the representation of their constituents. We achieve this using a biomedical language model trained to produce similar representations for entity names and their definitions, called BioLORD. The importance of this definition-based approach is highlighted by comparing the BioLORD model to two other state-of-the-art biomedical language models based on Transformer: SapBERT and CODER. Our results show that the BioLORD model has a strong ability to identify idiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity estimation helps ontology translators to focus on more challenging MWEs.},
	booktitle = {Proceedings of the 19th {Workshop} on {Multiword} {Expressions} ({MWE} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Remy, François and Khabibullina, Alfiya and Demeester, Thomas},
	editor = {Bhatia, Archna and Evang, Kilian and Garcia, Marcos and Giouli, Voula and Han, Lifeng and Taslimipoor, Shiva},
	month = may,
	year = {2023},
	pages = {73--80},
}

@inproceedings{yang_dialogue_2023,
	address = {Toronto, Canada},
	title = {Dialogue {State} {Tracking} with {Sparse} {Local} {Slot} {Attention}},
	url = {https://aclanthology.org/2023.nlp4convai-1.4/},
	doi = {10.18653/v1/2023.nlp4convai-1.4},
	abstract = {Dialogue state tracking (DST) is designed to track the dialogue state during the conversations between users and systems, which is the core of task-oriented dialogue systems. Mainstream models predict the values for each slot with fully token-wise slot attention from dialogue history. However, such operations may result in overlooking the neighboring relationship. Moreover, it may lead the model to assign probability mass to irrelevant parts, while these parts contribute little. It becomes severe with the increase in dialogue length. Therefore, we investigate sparse local slot attention for DST in this work. Slot-specific local semantic information is obtained at a sub-sampled temporal resolution capturing local dependencies for each slot. Then these local representations are attended with sparse attention weights to guide the model to pay attention to relevant parts of local information for subsequent state value prediction. The experimental results on MultiWOZ 2.0 and 2.4 datasets show that the proposed approach effectively improves the performance of ontology-based dialogue state tracking, and performs better than token-wise attention for long dialogues.},
	booktitle = {Proceedings of the 5th {Workshop} on {NLP} for {Conversational} {AI} ({NLP4ConvAI} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Longfei and Li, Jiyi and Li, Sheng and Shinozaki, Takahiro},
	editor = {Chen, Yun-Nung and Rastogi, Abhinav},
	month = jul,
	year = {2023},
	pages = {39--46},
}

@inproceedings{olstad_generation_2023,
	address = {Tórshavn, Faroe Islands},
	title = {Generation of {Replacement} {Options} in {Text} {Sanitization}},
	url = {https://aclanthology.org/2023.nodalida-1.30/},
	abstract = {The purpose of text sanitization is to edit text documents to mask text spans that may directly or indirectly reveal personal information. An important problem in text sanitization is to find less specific, yet still informative replacements for each text span to mask. We present an approach to generate possible replacements using a combination of heuristic rules and an ontology derived from Wikidata. Those replacement options are hierarchically structured and cover various types of personal identifiers. Using this approach, we extend a recently released text sanitization dataset with manually selected replacements. The outcome of this data collection shows that the approach is able to suggest appropriate replacement options for most text spans.},
	booktitle = {Proceedings of the 24th {Nordic} {Conference} on {Computational} {Linguistics} ({NoDaLiDa})},
	publisher = {University of Tartu Library},
	author = {Olstad, Annika Willoch and Papadopoulou, Anthi and Lison, Pierre},
	editor = {Alumäe, Tanel and Fishel, Mark},
	month = may,
	year = {2023},
	pages = {292--300},
}

@inproceedings{mille_generating_2023,
	address = {Singapore},
	title = {Generating {Irish} {Text} with a {Flexible} {Plug}-and-{Play} {Architecture}},
	url = {https://aclanthology.org/2023.pandl-1.4/},
	doi = {10.18653/v1/2023.pandl-1.4},
	abstract = {In this paper, we describe M-FleNS, a multilingual flexible plug-and-play architecture designed to accommodate neural and symbolic modules, and initially instantiated with rule-based modules. We focus on using M-FleNS for the specific purpose of building new resources for Irish, a language currently under-represented in the NLP landscape. We present the general M-FleNS framework and how we use it to build an Irish Natural Language Generation system for verbalising part of the DBpedia ontology and building a multilayered dataset with rich linguistic annotations. Via automatic and human assessments of the output texts we show that with very limited resources we are able to create a system that reaches high levels of fluency and semantic accuracy, while having very low energy and memory requirements.},
	booktitle = {Proceedings of the 2nd {Workshop} on {Pattern}-based {Approaches} to {NLP} in the {Age} of {Deep} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Mille, Simon and Uí Dhonnchadha, Elaine and Cassidy, Lauren and Davis, Brian and Dasiopoulou, Stamatia and Belz, Anya},
	editor = {Surdeanu, Mihai and Riloff, Ellen and Chiticariu, Laura and Frietag, Dayne and Hahn-Powell, Gus and Morrison, Clayton T. and Noriega-Atala, Enrique and Sharp, Rebecca and Valenzuela-Escarcega, Marco},
	month = dec,
	year = {2023},
	pages = {25--42},
}

@inproceedings{aleksic_lexicon-driven_2023,
	address = {Varna, Bulgaria},
	title = {Lexicon-{Driven} {Automatic} {Sentence} {Generation} for the {Skills} {Section} in a {Job} {Posting}},
	url = {https://aclanthology.org/2023.ranlp-1.4/},
	abstract = {This paper presents a sentence generation pipeline as implemented on the online job board Stepstone. The goal is to automatically create a set of sentences for the candidate profile and the task description sections in a job ad, related to a given input skill. They must cover two different “tone of voice” variants in German (Du, Sie), three experience levels (junior, mid, senior), and two optionality values (skill is mandatory or optional/nice to have). The generation process considers the difference between soft skills, natural language competencies and hard skills, as well as more specific sub-categories such as IT skills, programming languages and similar. To create grammatically consistent text, morphosyntactic features from the proprietary skill ontology and lexicon are consulted. The approach is a lexicon-driven generation process that compares all lexical features of the new input skills with the ones already added to the sentence database and creates new sentences according to the corresponding templates.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing}},
	publisher = {INCOMA Ltd., Shoumen, Bulgaria},
	author = {Aleksic, Vera and Brems, Mona and Mathes, Anna and Bertele, Theresa},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2023},
	pages = {32--40},
}

@inproceedings{hristov_clinical_2023,
	address = {Varna, Bulgaria},
	title = {Clinical {Text} {Classification} to {SNOMED} {CT} {Codes} {Using} {Transformers} {Trained} on {Linked} {Open} {Medical} {Ontologies}},
	url = {https://aclanthology.org/2023.ranlp-1.57/},
	doi = {10.26615/978-954-452-092-2_057},
	abstract = {We present an approach for medical text coding with SNOMED CT. Our approach uses publicly available linked open data from terminologies and ontologies as training data for the algorithms. We claim that even small training corpora made of short text snippets can be used to train models for the given task. We propose a method based on transformers enhanced with clustering and filtering of the candidates. Further, we adopt a classical machine learning approach - support vector classification (SVC) using transformer embeddings. The resulting approach proves to be more accurate than the predictions given by Large Language Models. We evaluate on a dataset generated from linked open data for SNOMED codes related to morphology and topography for four use cases. Our transformers-based approach achieves an F1-score of 0.82 for morphology and 0.99 for topography codes. Further, we validate the applicability of our approach in a clinical context using labelled real clinical data that are not used for model training.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing}},
	publisher = {INCOMA Ltd., Shoumen, Bulgaria},
	author = {Hristov, Anton and Ivanov, Petar and Aksenova, Anna and Asamov, Tsvetan and Gyurov, Pavlin and Primov, Todor and Boytcheva, Svetla},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Ontology, Linked data, Medical ontology, Open Data, SNOMED-CT, Training data, Text processing, Ontology's, Classification (of information), Short texts, Clinical text classifications, Linked open datum, Small training, Text snippets, Topography, Training corpus},
	pages = {519--526},
	annote = {Cited by: 3; All Open Access; Bronze Open Access},
}

@inproceedings{conceicao_lasigebiotm_2023,
	address = {Toronto, Canada},
	title = {{lasigeBioTM} at {SemEval}-2023 {Task} 7: {Improving} {Natural} {Language} {Inference} {Baseline} {Systems} with {Domain} {Ontologies}},
	url = {https://aclanthology.org/2023.semeval-1.2/},
	doi = {10.18653/v1/2023.semeval-1.2},
	abstract = {Clinical Trials Reports (CTRs) contain highly valuable health information from which Natural Language Inference (NLI) techniques determine if a given hypothesis can be inferred from a given premise. CTRs are abundant with domain terminology with particular terms that are difficult to understand without prior knowledge. Thus, we proposed to use domain ontologies as a source of external knowledge that could help with the inference process in theSemEval-2023 Task 7: Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT). This document describes our participation in subtask 1: Textual Entailment, where Ontologies, NLP techniques, such as tokenization and named-entity recognition, and rule-based approaches are all combined in our approach. We were able to show that inputting annotations from domain ontologies improved the baseline systems.},
	booktitle = {Proceedings of the 17th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2023)},
	publisher = {Association for Computational Linguistics},
	author = {Conceição, Sofia I. R. and F. Sousa, Diana and Silvestre, Pedro and Couto, Francisco M},
	editor = {Ojha, Atul Kr. and Doğruöz, A. Seza and Da San Martino, Giovanni and Tayyar Madabushi, Harish and Kumar, Ritesh and Sartori, Elisa},
	month = jul,
	year = {2023},
	pages = {10--15},
}

@inproceedings{roy_dipta_semantically-informed_2023,
	address = {Toronto, Canada},
	title = {Semantically-informed {Hierarchical} {Event} {Modeling}},
	url = {https://aclanthology.org/2023.starsem-1.31/},
	doi = {10.18653/v1/2023.starsem-1.31},
	abstract = {Prior work has shown that coupling sequential latent variable models with semantic ontological knowledge can improve the representational capabilities of event modeling approaches. In this work, we present a novel, doubly hierarchical, semi-supervised event modeling framework that provides structural hierarchy while also accounting for ontological hierarchy. Our approach consistsof multiple layers of structured latent variables, where each successive layer compresses and abstracts the previous layers. We guide this compression through the injection of structured ontological knowledge that is defined at the type level of events: importantly, our model allows for partial injection of semantic knowledge and it does not depend on observing instances at any particular level of the semantic ontology. Across two different datasets and four different evaluation metrics, we demonstrate that our approach is able to out-perform the previous state-of-the-art approaches by up to 8.5\%, demonstrating the benefits of structured and semantic hierarchical knowledge for event modeling.},
	booktitle = {Proceedings of the 12th {Joint} {Conference} on {Lexical} and {Computational} {Semantics} (*{SEM} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Roy Dipta, Shubhashis and Rezaee, Mehdi and Ferraro, Francis},
	editor = {Palmer, Alexis and Camacho-collados, Jose},
	month = jul,
	year = {2023},
	pages = {353--369},
}

@article{chen_opal_2023,
	title = {{OPAL}: {Ontology}-{Aware} {Pretrained} {Language} {Model} for {End}-to-{End} {Task}-{Oriented} {Dialogue}},
	volume = {11},
	url = {https://aclanthology.org/2023.tacl-1.5/},
	doi = {10.1162/tacl_a_00534},
	abstract = {This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user's constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and obtains competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Chen, Zhi and Liu, Yuncong and Chen, Lu and Zhu, Su and Wu, Mengyue and Yu, Kai},
	year = {2023},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	keywords = {Ontology, Language model, Pre-training, Benchmarking, Computational linguistics, Bridges, Ontology's, Large-scales, Simple++, Task-oriented, Dialogue models, End-to-end task, Modeling task, Task-specific modules},
	pages = {68--84},
	annote = {Cited by: 5; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{fucikova_corpus-based_2023,
	address = {Washington, D.C.},
	title = {Corpus-{Based} {Multilingual} {Event}-type {Ontology}: {Annotation} {Tools} and {Principles}},
	url = {https://aclanthology.org/2023.tlt-1.1/},
	abstract = {In the course of building a multilingual Event-type Ontology resource called SynSemClass, it was necessary to provide the maintainers and the annotators with a set of tools to facilitate their job, achieve data format consistency, and in general obtain high-quality data. We have adapted a previously existing tool (Urešová et al., 2018b), developed to assist the work in capturing bilingual synonymy. This tool needed to be both substantially expanded with some new features and fundamentally changed in the context of developing the resource for more languages, which necessarily is to be done in parallel. We are thus presenting here the tool, the new data structure design which had to change at the same time, and the associated workflow.},
	booktitle = {Proceedings of the 21st {International} {Workshop} on {Treebanks} and {Linguistic} {Theories} ({TLT}, {GURT}/{SyntaxFest} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Fučíková, Eva and Hajič, Jan and Urešová, Zdeňka},
	editor = {Dakota, Daniel and Evang, Kilian and Kübler, Sandra and Levin, Lori},
	month = mar,
	year = {2023},
	pages = {1--10},
}

@inproceedings{fernandez-alcaina_spanish_2023,
	address = {Washington, D.C.},
	title = {Spanish {Verbal} {Synonyms} in the {SynSemClass} {Ontology}},
	url = {https://aclanthology.org/2023.tlt-1.2/},
	abstract = {This paper presents ongoing work in the expansion of the multilingual semantic event-type ontology SynSemClass (Czech-English-German) to include Spanish. As in previous versions of the lexicon, Spanish verbal synonyms have been collected from a sentence-aligned parallel corpus and classified into classes based on their syntactic-semantic properties. Each class member is linked to a number of syntactic and/or semantic resources specific to each language, thus enriching the annotation and enabling interoperability. This paper describes the procedure for the data extraction and annotation of Spanish verbal synonyms in the lexicon.},
	booktitle = {Proceedings of the 21st {International} {Workshop} on {Treebanks} and {Linguistic} {Theories} ({TLT}, {GURT}/{SyntaxFest} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Fernández-Alcaina, Cristina and Fučíková, Eva and Hajič, Jan and Urešová, Zdeňka},
	editor = {Dakota, Daniel and Evang, Kilian and Kübler, Sandra and Levin, Lori},
	month = mar,
	year = {2023},
	pages = {11--20},
}

@inproceedings{fischer_reconstructing_2023,
	address = {Dubrovnik, Croatia},
	title = {Reconstructing {Language} {History} by {Using} a {Phonological} {Ontology}. {An} {Analysis} of {German} {Surnames}},
	url = {https://aclanthology.org/2023.vardial-1.10/},
	doi = {10.18653/v1/2023.vardial-1.10},
	abstract = {This paper applies the ontology-baseddialectometric technique of Engsterhold(2020) to surnames. The method wasoriginally developed for phonetic analyses. However, as will be shown, it is also suitedfor the study of graphemic representations. Based on data from the German SurnameAtlas (DFA), the method is optimized forgraphemic analysis and illustrated with anexample case.},
	booktitle = {Tenth {Workshop} on {NLP} for {Similar} {Languages}, {Varieties} and {Dialects} ({VarDial} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Fischer, Hanna and Engsterhold, Robert},
	editor = {Scherrer, Yves and Jauhiainen, Tommi and Ljubešić, Nikola and Nakov, Preslav and Tiedemann, Jörg and Zampieri, Marcos},
	month = may,
	year = {2023},
	pages = {104--112},
}

@inproceedings{hu_leveraging_2024,
	address = {Bangkok, Thailand},
	title = {Leveraging {Codebook} {Knowledge} with {NLI} and {ChatGPT} for {Zero}-{Shot} {Political} {Relation} {Classification}},
	url = {https://aclanthology.org/2024.acl-long.35/},
	doi = {10.18653/v1/2024.acl-long.35},
	abstract = {Is it possible accurately classify political relations within evolving event ontologies without extensive annotations? This study investigates zero-shot learning methods that use expert knowledge from existing annotation codebook, and evaluates the performance of advanced ChatGPT (GPT-3.5/4) and a natural language inference (NLI)-based model called ZSP. ChatGPT uses codebook's labeled summaries as prompts, whereas ZSP breaks down the classification task into context, event mode, and class disambiguation to refine task-specific hypotheses. This decomposition enhances interpretability, efficiency, and adaptability to schema changes. The experiments reveal ChatGPT's strengths and limitations, and crucially show ZSP's outperformance of dictionary-based methods and its competitive edge over some supervised models. These findings affirm the value of ZSP for validating event records and advancing ontology development. Our study underscores the efficacy of leveraging transfer learning and existing domain expertise to enhance research efficiency and scalability.},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Yibo and Skorupa Parolin, Erick and Khan, Latifur and Brandt, Patrick and Osorio, Javier and D'Orazio, Vito},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {583--603},
}

@inproceedings{cai_improving_2024,
	address = {Bangkok, Thailand},
	title = {Improving {Event} {Definition} {Following} {For} {Zero}-{Shot} {Event} {Detection}},
	volume = {1},
	url = {https://aclanthology.org/2024.acl-long.157/},
	doi = {10.18653/v1/2024.acl-long.157},
	abstract = {Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations.In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type.Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection.},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Cai, Zefan and Kung, Po-Nien and Suvarna, Ashima and Ma, Mingyu and Bansal, Hritik and Chang, Baobao and Brantingham, P. Jeffrey and Wang, Wei and Peng, Nanyun},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Modeling languages, Performance, Computational linguistics, Zero-shot learning, Large datasets, High quality, Learn+, Event Types, Events extractions, Events detection, Training model, Set of events, Sporadics, Train model},
	pages = {2842--2863},
	annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{wen_mindmap_2024,
	address = {Bangkok, Thailand},
	title = {{MindMap}: {Knowledge} {Graph} {Prompting} {Sparks} {Graph} of {Thoughts} in {Large} {Language} {Models}},
	volume = {1},
	url = {https://aclanthology.org/2024.acl-long.558/},
	doi = {10.18653/v1/2024.acl-long.558},
	abstract = {Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named MindMap, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \& answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference.},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wen, Yilin and Wang, Zifeng and Sun, Jimeng},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Language model, Natural language generation, Performance, Computational linguistics, Natural language understanding, Natural language processing systems, Implicit knowledge, Mind maps, Model inference, Model transparency, Reasoning process},
	pages = {10370--10388},
	annote = {Cited by: 17; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{qian_automating_2024,
	address = {Chicago, USA},
	title = {Automating {Idiom} {Translation} with {Cross}-{Lingual} {Natural} {Language} {Generation} {Grounded} {In} {Semantic} {Analyses} {Using} {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.amta-presentations.7/},
	abstract = {Idioms exhibit varying degrees of semantic transparency, making their translation challenging. Cross-language differences in idiom usage and connotations add complexity. Using a large language modeling (LLM) approach, we automate Chinese-to-English idiom translation in three steps: (1) Semantic analysis of Chinese idioms using ontology or FrameNet to identify key concepts/relationships like action, purpose, outcome, and context. (2) Generation of multi-word English expressions reflecting these concepts. (3) Selection of the top English idiom candidate that closely matches the Chinese idiom's meaning. Applied to examples like `破釜沉舟', `刀山火海', and `抛砖引玉', our method performs on par with human experts. The semantic reasoning approach enhances transparency in LLM decisions, simulating logical inferences over the semantic framework.},
	booktitle = {Proceedings of the 16th {Conference} of the {Association} for {Machine} {Translation} in the {Americas} ({Volume} 2: {Presentations})},
	publisher = {Association for Machine Translation in the Americas},
	author = {Qian, Ming},
	editor = {Martindale, Marianna and Campbell, Janice and Savenkov, Konstantin and Goel, Shivali},
	month = sep,
	year = {2024},
	pages = {95--115},
}

@inproceedings{shlyk_real_2024,
	address = {Bangkok, Thailand},
	title = {{REAL}: {A} {Retrieval}-{Augmented} {Entity} {Linking} {Approach} for {Biomedical} {Concept} {Recognition}},
	url = {https://aclanthology.org/2024.bionlp-1.29/},
	doi = {10.18653/v1/2024.bionlp-1.29},
	abstract = {Large Language Models (LLMs) offer an appealing alternative to training dedicated models for many Natural Language Processing (NLP) tasks. However, outdated knowledge and hallucination issues can be major obstacles in their application in knowledge-intensive biomedical scenarios. In this study, we consider the task of biomedical concept recognition (CR) from unstructured scientific literature and explore the use of Retrieval Augmented Generation (RAG) to improve accuracy and reliability of the LLM-based biomedical CR. Our approach, named REAL (Retrieval Augmented Entity Linking), combines the generative capabilities of LLMs with curated knowledge bases to automatically annotate natural language texts with concepts from bio-ontologies. By applying REAL to benchmark corpora on phenotype concept recognition, we show its effectiveness in improving LLM-based CR performance. This research highlights the potential of combining LLMs with external knowledge sources to advance biomedical text processing.},
	booktitle = {Proceedings of the 23rd {Workshop} on {Biomedical} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Shlyk, Darya and Groza, Tudor and Mesiti, Marco and Montanelli, Stefano and Cavalleri, Emanuele},
	editor = {Demner-Fushman, Dina and Ananiadou, Sophia and Miwa, Makoto and Roberts, Kirk and Tsujii, Junichi},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Language model, Concept recognition, Benchmarking, Performance, Computational linguistics, Language processing, Natural languages, Natural language processing systems, Model-based OPC, External knowledge, Natural languages texts, Scientific literature, Bio-ontologies},
	pages = {380--389},
	annote = {Cited by: 7},
}

@inproceedings{el_khettari_mention-agnostic_2024,
	address = {Bangkok, Thailand},
	title = {Mention-{Agnostic} {Information} {Extraction} for {Ontological} {Annotation} of {Biomedical} {Articles}},
	url = {https://aclanthology.org/2024.bionlp-1.37/},
	doi = {10.18653/v1/2024.bionlp-1.37},
	abstract = {Biomedical information extraction is crucial for advancing research, enhancing healthcare, and discovering treatments by efficiently analyzing extensive data. Given the extensive amount of biomedical data available, automated information extraction methods are necessary due to manual extraction's labor-intensive, expertise-dependent, and costly nature. In this paper, we propose a novel two-stage system for information extraction where we annotate biomedical articles based on a specific ontology (HOIP). The major challenge is annotating relation between biomedical processes often not explicitly mentioned in text articles. Here, we first predict the candidate processes and then determine the relationships between these processes. The experimental results show promising outcomes in mention-agnostic process identification using Large Language Models (LLMs). In relation classification, BERT-based supervised models still outperform LLMs significantly. The end-to-end evaluation results suggest the difficulty of this task and room for improvement in both process identification and relation classification.},
	booktitle = {Proceedings of the 23rd {Workshop} on {Biomedical} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {El Khettari, Oumaima and Nishida, Noriki and Liu, Shanshan and Munne, Rumana Ferdous and Yamagata, Yuki and Quiniou, Solen and Chaffron, Samuel and Matsumoto, Yuji},
	editor = {Demner-Fushman, Dina and Ananiadou, Sophia and Miwa, Makoto and Roberts, Kirk and Tsujii, Junichi},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Information retrieval, Computational linguistics, Biomedical data, Ontology's, Relation classifications, Classification (of information), Information extraction methods, Labour-intensive, Automated information, Biomedical information extractions, Process identification, Two stage system},
	pages = {457--473},
	annote = {Cited by: 2},
}

@inproceedings{achara_efficient_2024,
	address = {Bangkok, Thailand},
	title = {Efficient {Biomedical} {Entity} {Linking}: {Clinical} {Text} {Standardization} with {Low}-{Resource} {Techniques}},
	url = {https://aclanthology.org/2024.bionlp-1.40/},
	doi = {10.18653/v1/2024.bionlp-1.40},
	abstract = {Clinical text is rich in information, with mentions of treatment, medication and anatomy among many other clinical terms. Multiple terms can refer to the same core concepts which can be referred as a clinical entity. Ontologies like the Unified Medical Language System (UMLS) are developed and maintained to store millions of clinical entities including the definitions, relations and other corresponding information. These ontologies are used for standardization of clinical text by normalizing varying surface forms of a clinical term through Biomedical entity linking. With the introduction of transformer-based language models, there has been significant progress in Biomedical entity linking. In this work, we focus on learning through synonym pairs associated with the entities. As compared to the existing approaches, our approach significantly reduces the training data and resource consumption. Moreover, we propose a suite of context-based and context-less reranking techniques for performing the entity disambiguation. Overall, we achieve similar performance to the state-of-the-art zero-shot and distant supervised entity linking techniques on the Medmentions dataset, the largest annotated dataset on UMLS, without any domain-based training. Finally, we show that retrieval performance alone might not be sufficient as an evaluation metric and introduce an article level quantitative and qualitative analysis to reveal further insights on the performance of entity linking methods.},
	booktitle = {Proceedings of the 23rd {Workshop} on {Biomedical} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Achara, Akshit and Sasidharan, Sanand and N, Gagan},
	editor = {Demner-Fushman, Dina and Ananiadou, Sophia and Miwa, Makoto and Roberts, Kirk and Tsujii, Junichi},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Clinical terms, Performance, Computational linguistics, Unified Modeling Language, Training data, Ontology's, Unified medical language systems, Context-based, Re-ranking techniques, Resources consumption, Surface forms},
	pages = {493--505},
	annote = {Cited by: 0},
}

@inproceedings{frei_creating_2024,
	address = {Bangkok, Thailand},
	title = {Creating {Ontology}-annotated {Corpora} from {Wikipedia} for {Medical} {Named}-entity {Recognition}},
	url = {https://aclanthology.org/2024.bionlp-1.47/},
	doi = {10.18653/v1/2024.bionlp-1.47},
	abstract = {Acquiring annotated corpora for medical NLP is challenging due to legal and privacy constraints and costly annotation efforts, and using annotated public datasets may do not align well to the desired target application in terms of annotation style or language. We investigate the approach of utilizing Wikipedia and WikiData jointly to acquire an unsupervised annotated corpus for named-entity recognition (NER). By controlling the annotation ruleset through WikiData's ontology, we extract custom-defined annotations and dynamically impute weak annotations by an adaptive loss scaling. Our validation on German medication detection datasets yields competitive results. The entire pipeline only relies on open models and data resources, enabling reproducibility and open sharing of models and corpora. All relevant assets are shared on GitHub.},
	booktitle = {Proceedings of the 23rd {Workshop} on {Biomedical} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Frei, Johann and Kramer, Frank},
	editor = {Demner-Fushman, Dina and Ananiadou, Sophia and Miwa, Makoto and Roberts, Kirk and Tsujii, Junichi},
	month = aug,
	year = {2024},
	pages = {570--579},
}

@inproceedings{torri_structuring_2024,
	address = {Torino, Italia},
	title = {Structuring {Clinical} {Notes} of {Italian} {ST}-elevation {Myocardial} {Infarction} {Patients}},
	url = {https://aclanthology.org/2024.cl4health-1.5/},
	abstract = {In recent years, it has become common for patients to get full access to their Electronic Health Records (EHRs), thanks to the advancements in the EHRs systems of many healthcare providers. While this access empowers patients and doctors with comprehensive and real-time health information, it also introduces new challenges, in particular due to the unstructured nature of much of the information within EHRs. To address this, we propose a pipeline to structure clinical notes, providing them with a clear and concise overview of their health data and its longitudinal evolution, also allowing clinicians to focus more on patient care during consultations. In this paper, we present preliminary results on extracting structured information from anamneses of patients diagnosed with ST-Elevation Myocardial Infarction from an Italian hospital. Our pipeline exploits text classification models to extract relevant clinical variables, comparing rule-based, recurrent neural network and BERT-based models. While various approaches utilized ontologies or knowledge graphs for Italian data, our work represents the first attempt to develop this type of pipeline. The results for the extraction of most variables are satisfactory (f1-score {\textbackslash}ensuremath{\textgreater} 0.80), with the exception of the most rare values of certain variables, for which we propose future research directions to investigate.},
	booktitle = {Proceedings of the {First} {Workshop} on {Patient}-{Oriented} {Language} {Processing} ({CL4Health}) @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Torri, Vittorio and Mazzucato, Sara and Dalmiani, Stefano and Paradossi, Umberto and Passino, Claudio and Moccia, Sara and Micera, Silvestro and Ieva, Francesca},
	editor = {Demner-Fushman, Dina and Ananiadou, Sophia and Thompson, Paul and Ondov, Brian},
	month = may,
	year = {2024},
	pages = {37--43},
}

@inproceedings{liu_meddialog-fr_2024,
	address = {Torino, Italia},
	title = {{MedDialog}-{FR}: {A} {French} {Version} of the {MedDialog} {Corpus} for {Multi}-label {Classification} and {Response} {Generation} {Related} to {Women}'s {Intimate} {Health}},
	url = {https://aclanthology.org/2024.cl4health-1.21/},
	abstract = {This article presents MedDialog-FR, a large publicly available corpus of French medical conversations for the medical domain. Motivated by the lack of French dialogue corpora for data-driven dialogue systems and the paucity of available information related to women's intimate health, we introduce an annotated corpus of question-and-answer dialogues between a real patient and a real doctor concerning women's intimate health. The corpus is composed of about 20,000 dialogues automatically translated from the English version of MedDialog-EN. The corpus test set is composed of 1,400 dialogues that have been manually post-edited and annotated with 22 categories from the UMLS ontology. We also fine-tuned state-of-the-art reference models to automatically perform multi-label classification and response generation to give an initial performance benchmark and highlight the difficulty of the tasks.},
	booktitle = {Proceedings of the {First} {Workshop} on {Patient}-{Oriented} {Language} {Processing} ({CL4Health}) @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Liu, Xingyu and Segonne, Vincent and Mannion, Aidan and Schwab, Didier and Goeuriot, Lorraine and Portet, François},
	editor = {Demner-Fushman, Dina and Ananiadou, Sophia and Thompson, Paul and Ondov, Brian},
	month = may,
	year = {2024},
	pages = {173--183},
}

@inproceedings{hartendorp_biomedical_2024,
	address = {Torino, Italia},
	title = {Biomedical {Entity} {Linking} for {Dutch}: {Fine}-tuning a {Self}-alignment {BERT} {Model} on an {Automatically} {Generated} {Wikipedia} {Corpus}},
	url = {https://aclanthology.org/2024.cl4health-1.31/},
	abstract = {Biomedical entity linking, a main component in automatic information extraction from health-related texts, plays a pivotal role in connecting textual entities (such as diseases, drugs and body parts mentioned by patients) to their corresponding concepts in a structured biomedical knowledge base. The task remains challenging despite recent developments in natural language processing. This report presents the first evaluated biomedical entity linking model for the Dutch language. We use MedRoBERTa.nl as basemodel and perform second-phase pretraining through self-alignment on a Dutch biomedical ontology extracted from the UMLS and Dutch SNOMED. We derive a corpus from Wikipedia of ontology-linked Dutch biomedical entities in context and fine-tune our model on this dataset. We evaluate our model on the Dutch portion of the Mantra GSC-corpus and achieve 54.7\% classification accuracy and 69.8\% 1-distance accuracy. We then perform a case study on a collection of unlabeled, patient-support forum data and show that our model is hampered by the limited quality of the preceding entity recognition step. Manual evaluation of small sample indicates that of the correctly extracted entities, around 65\% is linked to the correct concept in the ontology. Our results indicate that biomedical entity linking in a language other than English remains challenging, but our Dutch model can be used to for high-level analysis of patient-generated text.},
	booktitle = {Proceedings of the {First} {Workshop} on {Patient}-{Oriented} {Language} {Processing} ({CL4Health}) @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Hartendorp, Fons and Seinen, Tom and van Mulligen, Erik and Verberne, Suzan},
	editor = {Demner-Fushman, Dina and Ananiadou, Sophia and Thompson, Paul and Ondov, Brian},
	month = may,
	year = {2024},
	pages = {253--263},
}

@inproceedings{penkov_mitigating_2024,
	address = {Sofia, Bulgaria},
	title = {Mitigating {Hallucinations} in {Large} {Language} {Models} via {Semantic} {Enrichment} of {Prompts}: {Insights} from {BioBERT} and {Ontological} {Integration}},
	url = {https://aclanthology.org/2024.clib-1.30/},
	abstract = {The advent of Large Language Models (LLMs) has been transformative for natural language processing, yet their tendency to produce “hallucinations”—outputs that are factually incorrect or entirely fabricated— remains a significant hurdle. This paper introduces a proactive methodology for reducing hallucinations by strategically enriching LLM prompts. This involves identifying key entities and contextual cues from varied domains and integrating this information into the LLM prompts to guide the model towards more accurate and relevant responses. Leveraging examples from BioBERT for biomedical entity recognition and ChEBI for chemical ontology, we illustrate a broader approach that encompasses semantic prompt enrichment as a versatile tool for enhancing LLM output accuracy. By examining the potential of semantic and ontological enrichment in diverse contexts, we aim to present a scalable strategy for improving the reliability of AI-generated content, thereby contributing to the ongoing efforts to refine LLMs for a wide range of applications.},
	booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Computational} {Linguistics} in {Bulgaria} ({CLIB} 2024)},
	publisher = {Department of Computational Linguistics, Institute for Bulgarian Language, Bulgarian Academy of Sciences},
	author = {Penkov, Stanislav},
	month = sep,
	year = {2024},
	note = {Type: Conference paper},
	pages = {272--276},
	annote = {Cited by: 0},
}

@inproceedings{usmanova_structuring_2024,
	address = {Bangkok, Thailand},
	title = {Structuring {Sustainability} {Reports} for {Environmental} {Standards} with {LLMs} guided by {Ontology}},
	url = {https://aclanthology.org/2024.climatenlp-1.13/},
	doi = {10.18653/v1/2024.climatenlp-1.13},
	abstract = {Following the introduction of the European Sustainability Reporting Standard (ESRS), companies will have to adapt to a new policy and provide mandatory sustainability reports. However, implementing such reports entails a challenge, such as the comprehension of a large number of textual information from various sources. This task can be accelerated by employing Large Language Models (LLMs) and ontologies to effectively model the domain knowledge. In this study, we extended an existing ontology to model ESRS Topical Standard for disclosure. The developed ontology would enable automated reasoning over the data and assist in constructing Knowledge Graphs (KGs). Moreover, the proposed ontology extension would also help to identify gaps in companies' sustainability reports with regard to the ESRS requirements.Additionally, we extracted knowledge from corporate sustainability reports via LLMs guided with a proposed ontology and developed their KG representation.},
	booktitle = {Proceedings of the 1st {Workshop} on {Natural} {Language} {Processing} {Meets} {Climate} {Change} ({ClimateNLP} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Usmanova, Aida and Usbeck, Ricardo},
	editor = {Stammbach, Dominik and Ni, Jingwei and Schimanski, Tobias and Dutia, Kalyan and Singh, Alok and Bingler, Julia and Christiaen, Christophe and Kushwaha, Neetu and Muccione, Veruska and A. Vaghefi, Saeid and Leippold, Markus},
	month = aug,
	year = {2024},
	pages = {168--177},
}

@inproceedings{de_deyne_can_2024,
	address = {Torino, Italia},
	title = {Can {GPT}-4 {Recover} {Latent} {Semantic} {Relational} {Information} from {Word} {Associations}? {A} {Detailed} {Analysis} of {Agreement} with {Human}-annotated {Semantic} {Ontologies}.},
	url = {https://aclanthology.org/2024.cogalex-1.8/},
	abstract = {Word associations, i.e., spontaneous responses to a cue word, provide not only a window into the human mental lexicon but have also been shown to be a repository of common-sense knowledge and can underpin efforts in lexicography and the construction of dictionaries. Especially the latter tasks require knowledge about the relations underlying the associations (e.g., Taxonomic vs. Situational); however, to date, there is neither an established ontology of relations nor an effective labelling paradigm. Here, we test GPT-4's ability to infer semantic relations for human-produced word associations. We use four human-labelled data sets of word associations and semantic features, with differing relation inventories and various levels of annotator agreement. We directly prompt GPT-4 with detailed relation definitions without further fine-tuning or training. Our results show that while GPT-4 provided a good account of higher-level classifications (e.g. Taxonomic vs Situational), prompting instructions alone cannot obtain similar performance for detailed classifications (e.g. superordinate, subordinate or coordinate relations) despite high agreement among human annotators. This suggests that latent relations can at least be partially recovered from word associations and highlights ways in which LLMs could be improved and human annotation protocols could adapted to reduce coding ambiguity.},
	booktitle = {Proceedings of the {Workshop} on {Cognitive} {Aspects} of the {Lexicon} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {De Deyne, Simon and Liu, Chunhua and Frermann, Lea},
	editor = {Zock, Michael and Chersoni, Emmanuele and Hsu, Yu-Yin and de Deyne, Simon},
	month = may,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Large language model, Ontology, Language model, Semantics, Semantic ontology, Semantic relations, Latent semantics, Commonsense knowledge, Ontology's, Labeled data, Labelings, Word-association},
	pages = {68--78},
	annote = {Cited by: 0},
}

@inproceedings{cartier_combining_2024,
	address = {Torino, Italia},
	title = {Combining {Deep} {Learning} {Models} and {Lexical} {Linked} {Data}: {Some} {Insights} from the {Development} of a {Multilingual} {News} {Named} {Entity} {Recognition} and {Linking} {Dataset}},
	url = {https://aclanthology.org/2024.dlnld-1.3/},
	abstract = {This paper presents the methodology and outcomes of a Named Entity Recognition and Linking multilingual news benchmark that leverages both Deep learning approaches by using a fine-tuned transformer model to detect mentions of persons, locations and organisations in text, and Linguistic Linked Open Data, through the use of Wikidata to disambiguate mentions and link them to ontology entries. It shows all the advantages of combining both approaches, not only for building the benchmark but also for fine-tuning detection models. We also insist on several perspectives of research to improve the accuracy of a combining system and go further on leveraging the complementary approaches.},
	booktitle = {Proceedings of the {Workshop} on {Deep} {Learning} and {Linked} {Data} ({DLnLD}) @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Cartier, Emmanuel and Peetermans, Emile},
	editor = {Sérasset, Gilles and Oliveira, Hugo Gonçalo and Oleskeviciene, Giedre Valunaite},
	month = may,
	year = {2024},
	pages = {31--44},
}

@inproceedings{shichman_propbank-powered_2024,
	address = {Torino, Italia},
	title = {{PropBank}-{Powered} {Data} {Creation}: {Utilizing} {Sense}-{Role} {Labelling} to {Generate} {Disaster} {Scenario} {Data}},
	url = {https://aclanthology.org/2024.dmr-1.1/},
	abstract = {For human-robot dialogue in a search-and-rescue scenario, a strong knowledge of the conditions and objects a robot will face is essential for effective interpretation of natural language instructions. In order to utilize the power of large language models without overwhelming the limited storage capacity of a robot, we propose PropBank-Powered Data Creation. PropBank-Powered Data Creation is an expert-in-the-loop data generation pipeline which creates training data for disaster-specific language models. We leverage semantic role labeling and Rich Event Ontology resources to efficiently develop seed sentences for fine-tuning a smaller, targeted model that could operate onboard a robot for disaster relief. We developed 32 sentence templates, which we used to make 2 seed datasets of 175 instructions for earthquake search and rescue and train derailment response. We further leverage our seed datasets as evaluation data to test our baseline fine-tuned models.},
	booktitle = {Proceedings of the {Fifth} {International} {Workshop} on {Designing} {Meaning} {Representations} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Shichman, Mollie Frances and Bonial, Claire and Hudson, Taylor A. and Blodgett, Austin and Ferraro, Francis and Rudinger, Rachel},
	editor = {Bonial, Claire and Bonn, Julia and Hwang, Jena D.},
	month = may,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Language model, Semantics, Computational linguistics, Affordances, Disasters, Robots, Synthetic data, Fine tuning, Search and rescue, Labelings, Digital storage, Data creation, Disaster prevention, Object affordance, Propbank, Synthetic data creation},
	pages = {1--10},
	annote = {Cited by: 0},
}

@inproceedings{moneglia_aspect_2024,
	address = {Torino, Italia},
	title = {Aspect {Variability} and the {Annotation} of {Aspect} in the {IMAGACT} {Ontology} of {Action}},
	url = {https://aclanthology.org/2024.dmr-1.2/},
	abstract = {This paper highlights some theoretical and quantitative issues related to the representation and annotation of aspectual meaning in the IMAGACT corpus-based multimodal ontology of action. Given the multimodal nature of this ontology, in which actions are represented through both prototypical visual scenes and linguistic captions, the annotation of aspect in this resource allows us to draw some important considerations about the relation between aspectual meaning and eventualities. The annotation procedure is reported and quantitative data show that, both in the English and Italian corpora, many verbs present aspectual variation, and many eventualities can be represented by locally equivalent verbs with different aspect. The reason why verb aspectual class may vary is investigated. Our analysis makes once more evident that verbs may vary their aspectual properties with respect not only to their argument structure but, more precisely, to the inner qualities of the eventualities they express. Crucially, when eventualities are expressed by equivalent verbs with different aspectual properties, the verbs put on focus different parts of the structure of the eventuality.},
	booktitle = {Proceedings of the {Fifth} {International} {Workshop} on {Designing} {Meaning} {Representations} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Moneglia, Massimo and Varvara, Rossella},
	editor = {Bonial, Claire and Bonn, Julia and Hwang, Jena D.},
	month = may,
	year = {2024},
	pages = {11--19},
}

@inproceedings{winiwarter_volare_2024,
	address = {Torino, Italia},
	title = {{VOLARE} - {Visual} {Ontological} {LAnguage} {REpresentation}},
	url = {https://aclanthology.org/2024.dmr-1.7/},
	abstract = {In this paper, we introduce a novel meaning representation, which is based on AMR but extends it towards a visual ontological representation. We visualize concepts by representative images, and roles by emojis. All concepts are identified either by PropBank rolesets, Wikipedia page titles, WordNet synsets, or Wikidata lexeme senses. We have developed a Web-based annotation environment enabled by augmented browsing and interactive diagramming. As first application, we have implemented a multilingual annotation solution by using English as anchor language and comparing it with French and Japanese language versions. Therefore, we have extended our representation by a translation deviation annotation to document the differences between the language versions. The intended user groups are, besides professional translators and interpreters, students of translation, language, and literary studies. We describe a first use case in which we use novels by French authors and compare them with their English and Japanese translations. The main motivation for choosing Japanese is the soaring popularity of Japanese courses at our university and the particular challenges involved with trying to master this language.},
	booktitle = {Proceedings of the {Fifth} {International} {Workshop} on {Designing} {Meaning} {Representations} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Winiwarter, Werner},
	editor = {Bonial, Claire and Bonn, Julia and Hwang, Jena D.},
	month = may,
	year = {2024},
	pages = {54--65},
}

@inproceedings{zhou_usage-centric_2024,
	address = {Miami, Florida, USA},
	title = {A {Usage}-centric {Take} on {Intent} {Understanding} in {E}-{Commerce}},
	url = {https://aclanthology.org/2024.emnlp-main.14/},
	doi = {10.18653/v1/2024.emnlp-main.14},
	abstract = {Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its essential role in product recommendation and business user profiling analysis, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as “how a customer uses a product”, and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph: category-rigidity and property-ambiguity. They limit its ability to strongly align user intents with products having the most desirable property, and to recommend useful products across diverse categories. Following these observations, we introduce a Product Recovery Benchmark featuring a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark. Our code and dataset are available at https://github.com/stayones/Usgae-Centric-Intent-Understanding.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Wendi and Li, Tianyi and Vougiouklis, Pavlos and Steedman, Mark and Pan, Jeff Z.},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {228--236},
}

@inproceedings{weir_ontologically_2024,
	address = {Miami, Florida, USA},
	title = {Ontologically {Faithful} {Generation} of {Non}-{Player} {Character} {Dialogues}},
	url = {https://aclanthology.org/2024.emnlp-main.520/},
	doi = {10.18653/v1/2024.emnlp-main.520},
	abstract = {We introduce a language generation dataset grounded in a popular video game. KNUDGE (**KN**owledge Constrained **U**ser-NPC **D**ialogue **GE**neration) requires models to produce trees of dialogue between video game characters that accurately reflect quest and entity specifications stated in natural language. KNUDGE is constructed from side quest dialogues drawn directly from game data of Obsidian Entertainment's \_The Outer Worlds\_, leading to real-world complexities in generation: (1) utterances must remain faithful to the game lore, including character personas and backstories; (2) a dialogue must accurately reveal new quest details to the human player; and (3) dialogues are large trees as opposed to linear chains of utterances. We report results for a set of neural generation models using supervised and in-context learning techniques; we find competent performance but room for future work addressing the challenges of creating realistic, game-quality dialogues.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Weir, Nathaniel and Thomas, Ryan and d'Amore, Randolph and Hill, Kellie and Van Durme, Benjamin and Jhamtani, Harsh},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {9212--9242},
}

@inproceedings{parekh_speed_2024,
	address = {Miami, Florida, USA},
	title = {{SPEED}++: {A} {Multilingual} {Event} {Extraction} {Framework} for {Epidemic} {Prediction} and {Preparedness}},
	url = {https://aclanthology.org/2024.emnlp-main.720/},
	doi = {10.18653/v1/2024.emnlp-main.720},
	abstract = {Social media is often the first place where communities discuss the latest societal trends. Prior works have utilized this platform to extract epidemic-related information (e.g. infections, preventive measures) to provide early warnings for epidemic prediction. However, these works only focused on English posts, while epidemics can occur anywhere in the world, and early discussions are often in the local, non-English languages. In this work, we introduce the first multilingual Event Extraction (EE) framework SPEED++ for extracting epidemic event information for any disease and language. To this end, we extend a previous epidemic ontology with 20 argument roles; and curate our multilingual EE dataset SPEED++ comprising 5.1K tweets in four languages for four diseases. Annotating data in every language is infeasible; thus we develop zero-shot cross-lingual cross-disease models (i.e., training only on English COVID data) utilizing multilingual pre-training and show their efficacy in extracting epidemic-related events for 65 diverse languages across different diseases. Experiments demonstrate that our framework can provide epidemic warnings for COVID-19 in its earliest stages in Dec 2019 (3 weeks before global discussions) from Chinese Weibo posts without any training in Chinese. Furthermore, we exploit our framework's argument extraction capabilities to aggregate community epidemic discussions like symptoms and cure measures, aiding misinformation detection and public attention monitoring. Overall, we lay a strong foundation for multilingual epidemic preparedness.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Parekh, Tanmay and Kwan, Jeffrey and Yu, Jiarui and Johri, Sparsh and Ahn, Hyosang and Muppalla, Sreya and Chang, Kai-Wei and Wang, Wei and Peng, Nanyun},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {12936--12965},
}

@inproceedings{sun_tools_2024,
	address = {Miami, Florida, USA},
	title = {Tools {Fail}: {Detecting} {Silent} {Errors} in {Faulty} {Tools}},
	url = {https://aclanthology.org/2024.emnlp-main.790/},
	doi = {10.18653/v1/2024.emnlp-main.790},
	abstract = {Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not in their weights, to perform tasks on the web, and even to control robots. However, most ontologies and surveys of tool-use have assumed the core challenge for LLMs is choosing the tool. Instead, we introduce a framework for tools more broadly which guides us to explore a model's ability to detect “silent” tool errors, and reflect on how to plan. This more directly aligns with the increasingly popular use of models as tools. We provide an initial approach to failure recovery with promising results both on a controlled calculator setting and embodied agent planning.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Jimin and Min, So Yeon and Chang, Yingshan and Bisk, Yonatan},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Computational linguistics, Ontology's, Control robots, Embodied agent, Failure recovery, Modeling abilities, Setting agents, Tool error, Tool use},
	pages = {14272--14289},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{koo_next_2024,
	address = {St. Julian's, Malta},
	title = {Next {Visit} {Diagnosis} {Prediction} via {Medical} {Code}-{Centric} {Multimodal} {Contrastive} {EHR} {Modelling} with {Hierarchical} {Regularisation}},
	url = {https://aclanthology.org/2024.findings-eacl.3/},
	abstract = {Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical codes representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Koo, Heejoon},
	editor = {Graham, Yvette and Purver, Matthew},
	month = mar,
	year = {2024},
	pages = {41--55},
}

@inproceedings{park_morality_2024,
	address = {St. Julian's, Malta},
	title = {Morality is {Non}-{Binary}: {Building} a {Pluralist} {Moral} {Sentence} {Embedding} {Space} using {Contrastive} {Learning}},
	url = {https://aclanthology.org/2024.findings-eacl.45/},
	abstract = {Recent advances in NLP show that language models retain a discernible level of knowledge in deontological ethics and moral norms. However, existing works often treat morality as binary, ranging from right to wrong. This simplistic view does not capture the nuances of moral judgment. Pluralist moral philosophers argue that human morality can be deconstructed into a finite number of elements, respecting individual differences in moral judgment. In line with this view, we build a pluralist moral sentence embedding space via a state-of-the-art contrastive learning approach. We systematically investigate the embedding space by studying the emergence of relationships among moral elements, both quantitatively and qualitatively. Our results show that a pluralist approach to morality can be captured in an embedding space. However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Park, Jeongwoo and Liscio, Enrico and Murukannaiah, Pradeep K.},
	editor = {Graham, Yvette and Purver, Matthew},
	month = mar,
	year = {2024},
	pages = {654--673},
}

@inproceedings{jiao_text2db_2024,
	address = {Bangkok, Thailand},
	title = {{Text2DB}: {Integration}-{Aware} {Information} {Extraction} with {Large} {Language} {Model} {Agents}},
	url = {https://aclanthology.org/2024.findings-acl.12/},
	doi = {10.18653/v1/2024.findings-acl.12},
	abstract = {The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE, Text2DB, that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for \textit{what to extract} and adapting to the given DB/KB schema for \textit{how to extract} on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-Plan-Analyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Jiao, Yizhu and Li, Sha and Zhou, Sizhe and Ji, Heng and Han, Jiawei},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Language model, Computational linguistics, Database systems, Structured knowledge, Model agents, Extraction modeling, Extraction ontologies, Document sets, Downstream applications, Infilling, On-the-fly, Target database},
	pages = {185--205},
	annote = {Cited by: 1},
}

@inproceedings{kteich_modelling_2024,
	address = {Bangkok, Thailand},
	title = {Modelling {Commonsense} {Commonalities} with {Multi}-{Facet} {Concept} {Embeddings}},
	url = {https://aclanthology.org/2024.findings-acl.86/},
	doi = {10.18653/v1/2024.findings-acl.86},
	abstract = {Concept embeddings offer a practical and efficient mechanism for injecting commonsense knowledge into downstream tasks. Their core purpose is often not to predict the commonsense properties of concepts themselves, but rather to identify commonalities, i.e. sets of concepts which share some property of interest. Such commonalities are the basis for inductive generalisation, hence high-quality concept embeddings can make learning easier and more robust. Unfortunately, standard embeddings primarily reflect basic taxonomic categories, making them unsuitable for finding commonalities that refer to more specific aspects (e.g. the colour of objects or the materials they are made of). In this paper, we address this limitation by explicitly modelling the different facets of interest when learning concept embeddings. We show that this leads to embeddings which capture a more diverse range of commonsense properties, and consistently improves results in downstream tasks such as ultra-fine entity typing and ontology completion.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Kteich, Hanane and Li, Na and Chatterjee, Usashi and Bouraoui, Zied and Schockaert, Steven},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {1467--1480},
}

@inproceedings{wang_can_2024,
	address = {Miami, Florida, USA},
	title = {Can {Large} {Language} {Models} {Understand} {DL}-{Lite} {Ontologies}? {An} {Empirical} {Study}},
	url = {https://aclanthology.org/2024.findings-emnlp.141/},
	doi = {10.18653/v1/2024.findings-emnlp.141},
	abstract = {Large language models (LLMs) have shown significant achievements in solving a wide range of tasks. Recently, LLMs' capability to store, retrieve and infer with symbolic knowledge has drawn a great deal of attention, showing their potential to understand structured information. However, it is not yet known whether LLMs can understand Description Logic (DL) ontologies. In this work, we empirically analyze the LLMs' capability of understanding DL-Lite ontologies covering 6 representative tasks from syntactic and semantic aspects. With extensive experiments, we demonstrate both the effectiveness and limitations of LLMs in understanding DL-Lite ontologies. We find that LLMs can understand formal syntax and model-theoretic semantics of concepts and roles. However, LLMs struggle with understanding TBox NI transitivity and handling ontologies with large ABoxes. We hope that our experiments and analyses provide more insights into LLMs and inspire to build more faithful knowledge engineering solutions.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Keyu and Qi, Guilin and Li, Jiaqi and Zhai, Songlin},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontology, Language model, Semantics, Knowledge engineering, Description logic, Empirical studies, Computational linguistics, Latent semantic analysis, Formal modeling, Syntactics, Ontology's, Structured information, Experiment and analysis, Formal syntaxes, Model-theoretic semantics, Symbolic knowledge},
	pages = {2503--2519},
	annote = {Cited by: 0},
}

@inproceedings{seeberger_mmutf_2024,
	address = {Miami, Florida, USA},
	title = {{MMUTF}: {Multimodal} {Multimedia} {Event} {Argument} {Extraction} with {Unified} {Template} {Filling}},
	url = {https://aclanthology.org/2024.findings-emnlp.381/},
	doi = {10.18653/v1/2024.findings-emnlp.381},
	abstract = {With the advancement of multimedia technologies, news documents and user-generated content are often represented as multiple modalities, making Multimedia Event Extraction (MEE) an increasingly important challenge. However, recent MEE methods employ weak alignment strategies and data augmentation with simple classification models, which ignore the capabilities of natural language-formulated event templates for the challenging Event Argument Extraction (EAE) task. In this work, we focus on EAE and address this issue by introducing a unified template filling model that connects the textual and visual modalities via textual prompts. This approach enables the exploitation of cross-ontology transfer and the incorporation of event-specific semantics. Experiments on the M2E2 benchmark demonstrate the effectiveness of our approach. Our system surpasses the current SOTA on textual EAE by +7\% F1, and performs generally better than the second-best systems for multimedia EAE.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Seeberger, Philipp and Wagner, Dominik and Riedhammer, Korbinian},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {6539--6548},
}

@inproceedings{li_contor_2024,
	address = {Miami, Florida, USA},
	title = {{CONTOR}: {Benchmarking} {Strategies} for {Completing} {Ontologies} with {Plausible} {Missing} {Rules}},
	url = {https://aclanthology.org/2024.findings-emnlp.488/},
	doi = {10.18653/v1/2024.findings-emnlp.488},
	abstract = {We consider the problem of finding plausible rules that are missing from a given ontology. A number of strategies for this problem have already been considered in the literature. Little is known about the relative performance of these strategies, however, as they have thus far been evaluated on different ontologies. Moreover, existing evaluations have focused on distinguishing held-out ontology rules from randomly corrupted ones, which often makes the task unrealistically easy and leads to the presence of incorrectly labelled negative examples. To address these concerns, we introduce a benchmark with manually annotated hard negatives and use this benchmark to evaluate ontology completion models. In addition to previously proposed models, we test the effectiveness of several approaches that have not yet been considered for this task, including LLMs and simple but effective hybrid strategies.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Li, Na and Bailleux, Thomas and Bouraoui, Zied and Schockaert, Steven},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ontology, Computational linguistics, Ontology's, Simple++, Hybrid strategies, Negative examples, Relative performance},
	pages = {8316--8334},
	annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{rajan_knowledge-based_2024,
	address = {Miami, Florida, USA},
	title = {Knowledge-based {Consistency} {Testing} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.findings-emnlp.596/},
	doi = {10.18653/v1/2024.findings-emnlp.596},
	abstract = {In this work, we systematically expose and measure the inconsistency and knowledge gaps of Large Language Models (LLMs). Specifically, we propose an automated testing framework (called KONTEST) which leverages a knowledge graph to construct test cases. KONTEST probes and measures the inconsistencies in the LLM's knowledge of the world via a combination of semantically-equivalent queries and test oracles (metamorphic or ontological oracle). KONTEST further mitigates knowledge gaps via a weighted LLM model ensemble. Using four state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that KONTEST generates 19.2\% error inducing inputs (1917 errors from 9979 test inputs). It also reveals a 16.5\% knowledge gap across all tested LLMs. A mitigation method informed by KONTEST's test suite reduces LLM knowledge gap by 32.48\%. Our ablation study further shows that GPT3.5 is not suitable for knowledge-based consistency testing because it is only 60\%-68\% effective in knowledge construction.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Rajan, Sai Sathiesh and Soremekun, Ezekiel and Chattopadhyay, Sudipta},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Language model, Semantics, Computational linguistics, Structured Query Language, Knowledge gaps, State of the art, Test case, Automated testing, Knowledge based, Model ensembles, Test oracles, Testing framework},
	pages = {10185--10196},
	annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{zhang_extractive_2024,
	address = {Miami, Florida, USA},
	title = {Extractive {Medical} {Entity} {Disambiguation} with {Memory} {Mechanism} and {Memorized} {Entity} {Information}},
	url = {https://aclanthology.org/2024.findings-emnlp.810/},
	doi = {10.18653/v1/2024.findings-emnlp.810},
	abstract = {Medical entity disambiguation (MED) aims to ground medical mentions in text with ontological entities in knowledge bases (KBs). A notable challenge of MED is the long medical text usually contains multiple entities' mentions with intricate correlations. However, limited by computation overhead, many existing methods consider only a single candidate entity mention during the disambiguation process. As such, they focus only on local MED optimal while ignoring the sole-mention disambiguation possibly boosted by richer context from other mentions' disambiguating processes – missing global optimal on entity combination in the text. Motivated by this, we propose a new approach called Extractive Medical Entity Disambiguation with Memory Mechanism and Memorized Entity Information (M3E). Specifically, we reformulate MED as a text extraction task, which simultaneously accepts the context of medical mentions, all possible candidate entities, and entity definitions, and it is then trained to extract the text span corresponding to the correct entity. Upon our new formulation, 1) to alleviate the computation overhead from the enriched context, we devise a memory mechanism module that performs memory caching, retrieval, fusion and cross-network residual; and 2) to utilize the disambiguation clues from other mentions, we design an auxiliary disambiguation module that employs a gating mechanism to assist the disambiguation of remaining mentions. Extensive experiments on two benchmark datasets demonstrate the superiority of M3E over the state-of-the-art MED methods on all metrics.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Guobiao and Peng, Xueping and Shen, Tao and Long, Guodong and Si, Jiasheng and Qin, Libo and Lu, Wenpeng},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {13811--13822},
}

@inproceedings{ashury_tahan_data-driven_2024,
	address = {Miami, Florida, USA},
	title = {Data-driven {Coreference}-based {Ontology} {Building}},
	url = {https://aclanthology.org/2024.findings-emnlp.834/},
	doi = {10.18653/v1/2024.findings-emnlp.834},
	abstract = {While coreference resolution is traditionally used as a component in individual document understanding, in this work we take a more global view and explore what can we learn about a domain from the set of all document-level coreference relations that are present in a large corpus. We derive coreference chains from a corpus of 30 million biomedical abstracts and construct a graph based on the string phrases within these chains, establishing connections between phrases if they co-occur within the same coreference chain. We then use the graph structure and the betweeness centrality measure to distinguish between edges denoting hierarchy, identity and noise, assign directionality to edges denoting hierarchy, and split nodes (strings) that correspond to multiple distinct concepts. The result is a rich, data-driven ontology over concepts in the biomedical domain, parts of which overlaps significantly with human-authored ontologies. We release the coreference chains and resulting ontology under a creative-commons license.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Ashury Tahan, Shir and Cohen, Amir David Nissan and Cohen, Nadav and Louzoun, Yoram and Goldberg, Yoav},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {14290--14300},
}

@inproceedings{alshammari_linking_2024,
	address = {Trento},
	title = {Linking {Quran} and {Hadith} {Topics} in an {Ontology} using {Word} {Embeddings} and {Cellfie} {Plugin}},
	url = {https://aclanthology.org/2024.icnlsp-1.46/},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Natural} {Language} and {Speech} {Processing} ({ICNLSP} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Alshammari, Ibtisam Khalaf and Atwell, Eric and Alsalka, Mohammad Ammar},
	editor = {Abbas, Mourad and Freihat, Abed Alhakim},
	month = oct,
	year = {2024},
	pages = {449--455},
}

@inproceedings{prakash_loc_2024,
	address = {AU-KBC Research Centre, Chennai, India},
	title = {{LOC}: {Livestock} {Ontology} {Construction} {Approach} {From} {Domain} based {Text} {Documents}},
	url = {https://aclanthology.org/2024.icon-1.53/},
	abstract = {Livestock plays an irreplaceable role in rural and global economies and as a part of its progression livestock ontology would unlock its potential of cross - domain applications of Natural Language Processing (NLP). Domain data is essential for the retrieval of semantic and syntactic understanding of the input text data given to the model. The paper presents a Livestock based Ontology Construction (LOC) is proposed. The input data endures anaphora resolution employing semantic methods based on rules then the pre-trained BERT model with Regular expression are utilized for retrieving terms (entities) from the data. Now the Graph Neural Network (GNN) is constructed with Regular Expressions for extricating relationships from the input documents for designing the livestock ontology. The efficaciousness of the proposed LOC based on the BERT model with regular expressions and GNN method with Regular expressions depicts noteworthy results when compared to existing methods, showing a precision and recall of 97.56\% and 95.24\%.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Natural} {Language} {Processing} ({ICON})},
	publisher = {NLP Association of India (NLPAI)},
	author = {Prakash, Nandhana and A, Amudhan and R, Nithish and Saravanan, Krithikha Sanju},
	editor = {Lalitha Devi, Sobha and Arora, Karunesh},
	month = dec,
	year = {2024},
	pages = {454--461},
}

@inproceedings{petliak_search_2024,
	address = {Torino, Italia},
	title = {Search tool for {An} {Event}-{Type} {Ontology}},
	url = {https://aclanthology.org/2024.isa-1.9/},
	abstract = {This short demo description paper presents a new tool designed for searching an event-type ontology with rich information, demonstrated on the SynSemClass ontology resource. The tool complements a web browser, created by the authors of the SynSemClass ontology previously. Due to the complexity of the resource, the search tool offers possibilities both for a linguistically-oriented researcher as well as for teams working with the resource from a technical point of view, such as building role labeling tools, automatic annotation tools, etc.},
	booktitle = {Proceedings of the 20th {Joint} {ACL} - {ISO} {Workshop} on {Interoperable} {Semantic} {Annotation} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Petliak, Nataliia and Alcaina, Cristina Fernandéz and Fučíková, Eva and Hajič, Jan and Urešová, Zdeňka},
	editor = {Bunt, Harry and Ide, Nancy and Lee, Kiyong and Petukhova, Volha and Pustejovsky, James and Romary, Laurent},
	month = may,
	year = {2024},
	pages = {66--70},
}

@inproceedings{salman_tiny_2024,
	address = {Torino, Italia},
	title = {Tiny {But} {Mighty}: {A} {Crowdsourced} {Benchmark} {Dataset} for {Triple} {Extraction} from {Unstructured} {Text}},
	url = {https://aclanthology.org/2024.isa-1.10/},
	abstract = {In the context of Natural Language Processing (NLP) and Semantic Web applications, constructing Knowledge Graphs (KGs) from unstructured text plays a vital role. Several techniques have been developed for KG construction from text, but the lack of standardized datasets hinders the evaluation of triple extraction methods. The evaluation of existing KG construction approaches is based on structured data or manual investigations. To overcome this limitation, this work introduces a novel dataset specifically designed to evaluate KG construction techniques from unstructured text. Our dataset consists of a diverse collection of compound and complex sentences meticulously annotated by human annotators with potential triples (subject, verb, object). The annotations underwent further scrutiny by expert ontologists to ensure accuracy and consistency. For evaluation purposes, the proposed F-measure criterion offers a robust approach to quantify the relatedness and assess the alignment between extracted triples and the ground-truth triples, providing a valuable tool for evaluating the performance of triple extraction systems. By providing a diverse collection of high-quality triples, our proposed benchmark dataset offers a comprehensive training and evaluation set for refining the performance of state-of-the-art language models on a triple extraction task. Furthermore, this dataset encompasses various KG-related tasks, such as named entity recognition, relation extraction, and entity linking.},
	booktitle = {Proceedings of the 20th {Joint} {ACL} - {ISO} {Workshop} on {Interoperable} {Semantic} {Annotation} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Salman, Muhammad and Haller, Armin and Rodriguez Mendez, Sergio J. and Naseem, Usman},
	editor = {Bunt, Harry and Ide, Nancy and Lee, Kiyong and Petukhova, Volha and Pustejovsky, James and Romary, Laurent},
	month = may,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Large language model, Natural language processing, Language model, Quality control, Crowdsourcing, Benchmarking, Computational linguistics, Extraction, Language processing, Natural languages, Natural language processing systems, Large datasets, Unstructured texts, Text annotations, Triple},
	pages = {71--81},
	annote = {Cited by: 0},
}

@inproceedings{gurgurov_adapting_2024,
	address = {Bangkok, Thailand},
	title = {Adapting {Multilingual} {LLMs} to {Low}-{Resource} {Languages} with {Knowledge} {Graphs} via {Adapters}},
	url = {https://aclanthology.org/2024.kallm-1.7/},
	doi = {10.18653/v1/2024.kallm-1.7},
	abstract = {This paper explores the integration of graph knowledge from linguistic ontologies into multilingual Large Language Models (LLMs) using adapters to improve performance for low-resource languages (LRLs) in sentiment analysis (SA) and named entity recognition (NER). Building upon successful parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we propose a similar approach for incorporating knowledge from multilingual graphs, connecting concepts in various languages with each other through linguistic relationships, into multilingual LLMs for LRLs. Specifically, we focus on eight LRLs — Maltese, Bulgarian, Indonesian, Nepali, Javanese, Uyghur, Tibetan, and Sinhala — and employ language-specific adapters fine-tuned on data extracted from the language-specific section of ConceptNet, aiming to enable knowledge transfer across the languages covered by the knowledge graph. We compare various fine-tuning objectives, including standard Masked Language Modeling (MLM), MLM with full-word masking, and MLM with targeted masking, to analyze their effectiveness in learning and integrating the extracted graph data. Through empirical evaluation on language-specific tasks, we assess how structured graph knowledge affects the performance of multilingual LLMs for LRLs in SA and NER, providing insights into the potential benefits of adapting language models for low-resource scenarios.},
	booktitle = {Proceedings of the 1st {Workshop} on {Knowledge} {Graphs} and {Large} {Language} {Models} ({KaLLM} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Gurgurov, Daniil and Hartmann, Mareike and Ostermann, Simon},
	editor = {Biswas, Russa and Kaffee, Lucie-Aimée and Agarwal, Oshin and Minervini, Pasquale and Singh, Sameer and de Melo, Gerard},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Language model, Named entity recognition, Sentiment analysis, Computational linguistics, ConceptNet, Fine tuning, Low resource languages, Improve performance, Linguistic ontology, Tibetans},
	pages = {63--74},
	annote = {Cited by: 4},
}

@inproceedings{van_cauter_ontology-guided_2024,
	address = {Bangkok, Thailand},
	title = {Ontology-guided {Knowledge} {Graph} {Construction} from {Maintenance} {Short} {Texts}},
	url = {https://aclanthology.org/2024.kallm-1.8/},
	doi = {10.18653/v1/2024.kallm-1.8},
	abstract = {Large-scale knowledge graph construction remains infeasible since it requires significant human-expert involvement. Further complications arise when building graphs from domain-specific data due to their unique vocabularies and associated contexts. In this work, we demonstrate the ability of open-source large language models (LLMs), such as Llama-2 and Llama-3, to extract facts from domain-specific Maintenance Short Texts (MSTs). We employ an approach which combines ontology-guided triplet extraction and in-context learning. By using only 20 semantically similar examples with the Llama-3-70B-Instruct model, we achieve performance comparable to previous methods that relied on fine-tuning techniques like SpERT and REBEL. This indicates that domain-specific fact extraction can be accomplished through inference alone, requiring minimal labeled data. This opens up possibilities for effective and efficient semi-automated knowledge graph construction for domain-specific data.},
	booktitle = {Proceedings of the 1st {Workshop} on {Knowledge} {Graphs} and {Large} {Language} {Models} ({KaLLM} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {van Cauter, Zeno and Yakovets, Nikolay},
	editor = {Biswas, Russa and Kaffee, Lucie-Aimée and Agarwal, Oshin and Minervini, Pasquale and Singh, Sameer and de Melo, Gerard},
	month = aug,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Ontology, Language model, Semantics, Computational linguistics, Adversarial machine learning, Graph construction, Domain Knowledge, Ontology's, In contexts, Contrastive Learning, Large-scales, Domain specific, Human expert, Open-source, Short texts},
	pages = {75--84},
	annote = {Cited by: 4},
}

@inproceedings{spaulding_propbank_2024,
	address = {St. Julians, Malta},
	title = {{PropBank} goes {Public}: {Incorporation} into {Wikidata}},
	url = {https://aclanthology.org/2024.law-1.16/},
	abstract = {This paper presents the first integration of PropBank role information into Wikidata, in order to provide a novel resource for information extraction, one combining Wikidata's ontological metadata with PropBank's rich argument structure encoding for event classes. We discuss a technique for PropBank augmentation to existing eventive Wikidata items, as well as identification of gaps in Wikidata's coverage based on manual examination of over 11,300 PropBank rolesets. We propose five new Wikidata properties to integrate PropBank structure into Wikidata so that the annotated mappings can be added en masse. We then outline the methodology and challenges of this integration, including annotation with the combined resources.},
	booktitle = {Proceedings of the 18th {Linguistic} {Annotation} {Workshop} ({LAW}-{XVIII})},
	publisher = {Association for Computational Linguistics},
	author = {Spaulding, Elizabeth and Conger, Kathryn and Gershman, Anatole and Morshed, Mahir and Brown, Susan Windisch and Pustejovsky, James and Uceda-Sosa, Rosario and Ge, Sijia and Palmer, Martha},
	editor = {Henning, Sophie and Stede, Manfred},
	month = mar,
	year = {2024},
	pages = {166--175},
}

@inproceedings{banerjee_cross-lingual_2024,
	address = {Torino, Italia},
	title = {Cross-{Lingual} {Ontology} {Matching} using {Structural} and {Semantic} {Similarity}},
	url = {https://aclanthology.org/2024.ldl-1.2/},
	abstract = {The development of ontologies in various languages is attracting attention as the amount of multilingual data available on the web increases. Cross-lingual ontology matching facilitates interoperability amongst ontologies in different languages. Although supervised machine learning-based methods have shown good performance on ontology matching, their application to the cross-lingual setting is limited by the availability of training data. Current state-of-the-art unsupervised methods for cross-lingual ontology matching focus on lexical similarity between entities. These approaches follow a two-stage pipeline where the entities are translated into a common language using a translation service in the first step followed by computation of lexical similarity between the translations to match the entities in the second step. In this paper we introduce a novel ontology matching method based on the fusion of structural similarity and cross-lingual semantic similarity. We carry out experiments using 3 language pairs and report substantial improvements on the performance of the lexical methods thus showing the effectiveness of our proposed approach. To the best of our knowledge this is the first work which tackles the problem of unsupervised ontology matching in the cross-lingual setting by leveraging both structural and semantic embeddings.},
	booktitle = {Proceedings of the 9th {Workshop} on {Linked} {Data} in {Linguistics} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Banerjee, Shubhanker and Chakravarthi, Bharathi Raja and McCrae, John Philip},
	editor = {Chiarcos, Christian and Gkirtzou, Katerina and Ionov, Maxim and Khan, Fahad and McCrae, John P. and Ponsoda, Elena Montiel and Chozas, Patricia Martín},
	month = may,
	year = {2024},
	pages = {11--21},
}

@inproceedings{boano_querying_2024,
	address = {Torino, Italia},
	title = {Querying the {Lexicon} der indogermanischen {Verben} in the {LiLa} {Knowledge} {Base}: {Two} {Use} {Cases}},
	url = {https://aclanthology.org/2024.ldl-1.3/},
	abstract = {This paper presents two use cases of the etymological data provided by the *Lexicon der indogermanischen Verben* (LIV) after their publication as Linked Open Data and their linking to the LiLa Knowledge Base (KB) of interoperable linguistic resources for Latin. The first part of the paper briefly describes the LiLa KB and its structure. Then, the LIV and the information it contains are introduced, followed by a short description of the ontologies and the extensions used for modelling the LIV's data and interlinking them to the LiLa ecosystem. The last section details the two use cases. The first case concerns the inflection types of the Latin verbs that reflect Proto-Indo-European stems, while the second one focusses on the Latin derivatives of the inherited stems. The results of the investigations are put in relation to current research topics in Historical Linguistics, demonstrating their relevance to the discipline.},
	booktitle = {Proceedings of the 9th {Workshop} on {Linked} {Data} in {Linguistics} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Boano, Valeria Irene and Passarotti, Marco and Ginevra, Riccardo},
	editor = {Chiarcos, Christian and Gkirtzou, Katerina and Ionov, Maxim and Khan, Fahad and McCrae, John P. and Ponsoda, Elena Montiel and Chozas, Patricia Martín},
	month = may,
	year = {2024},
	pages = {22--31},
}

@inproceedings{canning_defining_2024,
	address = {Torino, Italia},
	title = {Defining an {Ontology} for {Museum} {Critical} {Cataloguing} {Terminology} {Guidelines}},
	url = {https://aclanthology.org/2024.ldl-1.4/},
	abstract = {Submission type: Short paper This paper presents the proposed ontology for the project Computational Approaches for Addressing Problematic Terminology (CAAPT). This schema seeks to represent contents and structure of language guideline documents produced by cultural heritage institutions seeking to engage with critical cataloguing or reparative description work, known as terminology guidance documents. It takes the Victoria \& Albert Museum's Terminology Guidance Document as a source for the initial modelling work. Ultimately, CAAPT seeks to expand the knowledge graph beyond the V\&A Museum context to incorporate additional terminology guidance documents and linked open data vocabularies. The ontology seeks to bring together scholarly communities in areas relevant to this project, most notably those in cultural heritage and linguistics linked open data, by leveraging existing linked data resources in these areas: as such, OntoLex, CIDOC CRM, and SKOS are used as a foundation for this work, along with a proposed schema from a related project, CULCO. As the CAAPT project is in early stages, this paper presents the preliminary results of work undertaken thus far in order to seek feedback from the linguistics linked open data community.},
	booktitle = {Proceedings of the 9th {Workshop} on {Linked} {Data} in {Linguistics} @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Canning, Erin},
	editor = {Chiarcos, Christian and Gkirtzou, Katerina and Ionov, Maxim and Khan, Fahad and McCrae, John P. and Ponsoda, Elena Montiel and Chozas, Patricia Martín},
	month = may,
	year = {2024},
	pages = {32--36},
}

@inproceedings{gerlach_concept_2024,
	address = {Torino, Italia},
	title = {A {Concept} {Based} {Approach} for {Translation} of {Medical} {Dialogues} into {Pictographs}},
	url = {https://aclanthology.org/2024.lrec-main.21/},
	abstract = {Pictographs have been found to improve patient comprehension of medical information or instructions. However, tools to produce pictograph representations from natural language are still scarce. In this contribution we describe a system that automatically translates French speech into pictographs to enable diagnostic interviews in emergency settings, thereby providing a tool to overcome the language barrier or provide support in Augmentative and Alternative Communication (AAC) contexts. Our approach is based on a semantic gloss that serves as pivot between spontaneous language and pictographs, with medical concepts represented using the UMLS ontology. In this study we evaluate different available pre-trained models fine-tuned on artificial data to translate French into this semantic gloss. On unseen data collected in real settings, consisting of questions and instructions by physicians, the best model achieves an F0.5 score of 86.7. A complementary human evaluation of the semantic glosses differing from the reference shows that 71\% of these would be usable to transmit the intended meaning. Finally, a human evaluation of the pictograph sequences derived from the gloss reveals very few additions, omissions or order issues ({\textbackslash}ensuremath{\textless}3\%), suggesting that the gloss as designed is well suited as a pivot for translation into pictographs.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Gerlach, Johanna and Bouillon, Pierrette and Mutal, Jonathan and Spechbach, Hervé},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {233--242},
}

@inproceedings{gajbhiye_amended_2024,
	address = {Torino, Italia},
	title = {{AMenDeD}: {Modelling} {Concepts} by {Aligning} {Mentions}, {Definitions} and {Decontextualised} {Embeddings}},
	url = {https://aclanthology.org/2024.lrec-main.72/},
	abstract = {Contextualised Language Models (LM) improve on traditional word embeddings by encoding the meaning of words in context. However, such models have also made it possible to learn high-quality decontextualised concept embeddings. Three main strategies for learning such embeddings have thus far been considered: (i) fine-tuning the LM to directly predict concept embeddings from the name of the concept itself, (ii) averaging contextualised representations of mentions of the concept in a corpus, and (iii) encoding definitions of the concept. As these strategies have complementary strengths and weaknesses, we propose to learn a unified embedding space in which all three types of representations can be integrated. We show that this allows us to outperform existing approaches in tasks such as ontology completion, which heavily depends on access to high-quality concept embeddings. We furthermore find that mentions and definitions are well-aligned in the resulting space, enabling tasks such as target sense verification, even without the need for any fine-tuning.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Gajbhiye, Amit and Bouraoui, Zied and Espinosa Anke, Luis and Schockaert, Steven},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Language model, Semantics, Modeling languages, Embeddings, Definition, Computational linguistics, Lexical semantics, Signal encoding, High quality, Encoding (symbols), Learn+, Fine tuning, Modeling concepts, Encodings, Concept embedding},
	pages = {801--811},
	annote = {Cited by: 1},
}

@inproceedings{loukachevitch_biomedical_2024,
	address = {Torino, Italia},
	title = {Biomedical {Concept} {Normalization} over {Nested} {Entities} with {Partial} {UMLS} {Terminology} in {Russian}},
	url = {https://aclanthology.org/2024.lrec-main.213/},
	abstract = {We present a new manually annotated dataset of PubMed abstracts for concept normalization in Russian. It contains over 23,641 entity mentions in 756 documents linked to 4,544 unique concepts from the UMLS ontology. Compared to existing corpora, we explore two novel annotation characteristics: the nestedness of named entities and the incompleteness of the Russian medical terminology in UMLS. 4,424 entity mentions are linked to 1,535 unique English concepts absent in the Russian part of the UMLS ontology. We present several baselines for normalization over nested named entities obtained with state-of-the-art models such as SapBERT. Our experimental results show that models pre-trained on graph structural data from UMLS achieve superior performance in a zero-shot setting on bilingual terminology.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Loukachevitch, Natalia and Sakhovskiy, Andrey and Tutubalina, Elena},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {2383--2389},
}

@inproceedings{mousavi_construction_2024,
	address = {Torino, Italia},
	title = {Construction of {Paired} {Knowledge} {Graph} - {Text} {Datasets} {Informed} by {Cyclic} {Evaluation}},
	url = {https://aclanthology.org/2024.lrec-main.335/},
	abstract = {Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Informed by these observations, we construct a new, improved dataset called \textbf{LAGRANGE} using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Mousavi, Ali and Zhan, Xin and Bai, He and Shi, Peng and Rekatsinas, Theodoros and Han, Benjamin and Li, Yunyao and Pound, Jeffrey and Susskind, Joshua M. and Schluter, Natalie and Ilyas, Ihab F. and Jaitly, Navdeep},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Knowledge graphs, Knowledge graph, Language model, Ontology's, Large datasets, Synthetic datasets, Neural modelling, Forward modeling, Noisy datasets, Reverse Modeling},
	pages = {3782--3803},
	annote = {Cited by: 1},
}

@inproceedings{agarwal_ethical_2024,
	address = {Torino, Italia},
	title = {Ethical {Reasoning} and {Moral} {Value} {Alignment} of {LLMs} {Depend} on the {Language} {We} {Prompt} {Them} in},
	url = {https://aclanthology.org/2024.lrec-main.560/},
	abstract = {Ethical reasoning is a crucial skill for Large Language Models (LLMs). However, moral values are not universal, but rather influenced by language and culture. This paper explores how three prominent LLMs – GPT-4, ChatGPT, and Llama2Chat-70B – perform ethical reasoning in different languages and if their moral judgement depend on the language in which they are prompted. We extend the study of ethical reasoning of LLMs by (CITATION) to a multilingual setup following their framework of probing LLMs with ethical dilemmas and policies from three branches of normative ethics: deontology, virtue, and consequentialism. We experiment with six languages: English, Spanish, Russian, Chinese, Hindi, and Swahili. We find that GPT-4 is the most consistent and unbiased ethical reasoner across languages, while ChatGPT and Llama2Chat-70B show significant moral value bias when we move to languages other than English. Interestingly, the nature of this bias significantly vary across languages for all LLMs, including GPT-4.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Agarwal, Utkarsh and Tanmay, Kumar and Khandelwal, Aditi and Choudhury, Monojit},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {6330--6340},
}

@inproceedings{nie_know-adapter_2024,
	address = {Torino, Italia},
	title = {Know-{Adapter}: {Towards} {Knowledge}-{Aware} {Parameter}-{Efficient} {Transfer} {Learning} for {Few}-shot {Named} {Entity} {Recognition}},
	url = {https://aclanthology.org/2024.lrec-main.854/},
	abstract = {Parameter-Efficient Fine-Tuning (PEFT) is a promising approach to mitigate the challenges about the model adaptation of pretrained language models (PLMs) for the named entity recognition (NER) task. Recent studies have highlighted the improvements that can be made to the quality of information retrieved from PLMs by adding explicit knowledge from external source like KGs to otherwise naive PEFTs. In this paper, we propose a novel knowledgeable adapter, Know-adapter, to incorporate structure and semantic knowledge of knowledge graphs into PLMs for few-shot NER. First, we construct a related KG entity type sequence for each sentence using a knowledge retriever. However, the type system of a domain-specific NER task is typically independent of that of current KGs and thus exhibits heterogeneity issue inevitably, which makes matching between the original NER and KG types (e.g. Person in NER potentially matches President in KBs) less likely, or introduces unintended noises. Thus, then we design a unified taxonomy based on KG ontology for KG entity types and NER labels. This taxonomy is used to build a learnable shared representation module, which provides shared representations for both KG entity type sequences and NER labels. Based on these shared representations, our Know-adapter introduces high semantic relevance knowledge and structure knowledge from KGs as inductive bias to guide the updating process of the adapter. Additionally, the shared representations guide the learnable representation module to reduce noise in the unsupervised expansion of label words. Extensive experiments on multiple NER datasets show the superiority of Know-Adapter over other state-of-the-art methods in both full-resource and low-resource settings.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Nie, Binling and Shao, Yiming and Wang, Yigang},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {9777--9786},
}

@inproceedings{gnehm_mapping_2024,
	address = {Torino, Italia},
	title = {Mapping {Work} {Task} {Descriptions} from {German} {Job} {Ads} on the {O}*{NET} {Work} {Activities} {Ontology}},
	url = {https://aclanthology.org/2024.lrec-main.963/},
	abstract = {This work addresses the challenge of extracting job tasks from German job postings and mapping them to the fine-grained work activities classification in the O*NET labor market ontology. By utilizing ontological data with a Multiple Negatives Ranking loss and integrating a modest volume of labeled job advertisement data into the training process, our top configuration achieved a notable precision of 70\% for the best mapping on the test set, representing a substantial improvement compared to the 33\% baseline delivered by a general-domain SBERT. In our experiments the following factors proved to be most effective for improving SBERT models: First, the incorporation of subspan markup, both during training and inference, supports accurate classification, by streamlining varied job ad task formats with structured, uniform ontological work activities. Second, the inclusion of additional occupational information from O*NET into training supported learning by contextualizing hierarchical ontological relationships. Third, the most significant performance improvement was achieved by updating SBERT models with labeled job ad data specifically addressing challenging cases encountered during pre-finetuning, effectively bridging the semantic gap between O*NET and job ad data.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Gnehm, Ann-Sophie and Clematide, Simon},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {11049--11059},
}

@inproceedings{liu_mhgrl_2024,
	address = {Torino, Italia},
	title = {{MHGRL}: {An} {Effective} {Representation} {Learning} {Model} for {Electronic} {Health} {Records}},
	url = {https://aclanthology.org/2024.lrec-main.985/},
	abstract = {Electronic health records (EHRs) serve as a digital repository storing comprehensive medical information about patients. Representation learning for EHRs plays a crucial role in healthcare applications. In this paper, we propose a Multimodal Heterogeneous Graph-enhanced Representation Learning, denoted as MHGRL, aimed at learning effective EHR representations. To address the challenge posed by data insufficiency of EHRs, MHGRL utilizes a multimodal heterogeneous graph to model an EHR. Specifically, we construct a heterogeneous graph for each EHR and enrich it by incorporating multimodal information with medical ontology and textual notes. With the integration of pre-trained model, graph neural network, and attention mechanism, MHGRL effectively incorporates both node attributes and structural information across a multimodal heterogeneous graph. Moreover, we employ contrastive learning to ensure the consistency of representations for similar EHRs and improve the model robustness. The experimental results show that MHGRL outperforms all baselines on two real clinical datasets in downstream tasks, including EHR clustering and disease prediction. The code is available at https://github.com/emmali808/MHGRL.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Liu, Feiyan and Li, Liangzhi and Wang, Xiaoli and Luo, Feng and Liu, Chang and Su, Jinsong and Qian, Yiming},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {11272--11282},
}

@inproceedings{dezotti_modelling_2024,
	address = {Torino, Italia},
	title = {Modelling and {Linking} an {Old} {Latin}-{Portuguese} {Dictionary} to the {LiLa} {Knowledge} {Base}},
	url = {https://aclanthology.org/2024.lrec-main.1008/},
	abstract = {This paper describes the steps undertaken to include data from Antonio Velez's bilingual Latin-Portuguese dictionary (Index Totius Artis, 1744) into the LiLa Knowledge Base of interoperable linguistic resources for Latin. The paper focuses on how the lexical and lexicographic information of the source dictionary was modelled by using respectively the Lexicon Model for Ontologies (OntoLex-lemon) and its lexicog module. The linking process of the dictionary entries with those of the LiLa collection of Latin lemmas is detailed, discussing issues in dealing with ambiguities and typographical errors found in the source. The result is the first Latin-Portuguese lexical resource made interoperable with the (meta)data of the other linguistic resources for Latin interlinked in the LiLa Knowledge Base, providing new ways of assessing the dictionary information or using its content as starting point to explore the connections with other interlinked linguistic resources. A couple of use case scenarios illustrate those possibilities.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Dezotti, Lucas Consolin and Passarotti, Marco and Mambrini, Francesco},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {11537--11547},
}

@inproceedings{park_multi-dimensional_2024,
	address = {Torino, Italia},
	title = {Multi-{Dimensional} {Machine} {Translation} {Evaluation}: {Model} {Evaluation} and {Resource} for {Korean}},
	url = {https://aclanthology.org/2024.lrec-main.1024/},
	abstract = {Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperforms its counterpart in the style dimension while reference-based models retain an edge regarding accuracy. Overall, RemBERT emerges as the most promising model. Through our evaluation, we offer an insight into the translation quality in a more fine-grained, interpretable manner.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Park, Dojun and Padó, Sebastian},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Quality control, Machine translation, Benchmarking, Explainability, Computational linguistics, Multilinguality, Corpus, Fine grained, Computer aided language translation, Evaluation methodologies, Machine translations, Metric scores, MT evaluations, Quality metrices, Reference-free},
	pages = {11723--11744},
	annote = {Cited by: 1},
}

@inproceedings{phi_polynere_2024,
	address = {Torino, Italia},
	title = {{PolyNERE}: {A} {Novel} {Ontology} and {Corpus} for {Named} {Entity} {Recognition} and {Relation} {Extraction} in {Polymer} {Science} {Domain}},
	url = {https://aclanthology.org/2024.lrec-main.1126/},
	abstract = {Polymers are widely used in diverse fields, and the demand for efficient methods to extract and organize information about them is increasing. An automated approach that utilizes machine learning can accurately extract relevant information from scientific papers, providing a promising solution for automating information extraction using annotated training data. In this paper, we introduce a polymer-relevant ontology featuring crucial entities and relations to enhance information extraction in the polymer science field. Our ontology is customizable to adapt to specific research needs. We present PolyNERE, a high-quality named entity recognition (NER) and relation extraction (RE) corpus comprising 750 polymer abstracts annotated using our ontology. Distinctive features of PolyNERE include multiple entity types, relation categories, support for various NER settings, and the ability to assert entities and relations at different levels. PolyNERE also facilitates reasoning in the RE task through supporting evidence. While our experiments with recent advanced methods achieved promising results, challenges persist in adapting NER and RE from abstracts to full-text paragraphs. This emphasizes the need for robust information extraction systems in the polymer domain, making our corpus a valuable benchmark for future developments.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Phi, Van-Thuy and Teranishi, Hiroki and Matsumoto, Yuji and Oka, Hiroyuki and Ishii, Masashi},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {12856--12866},
}

@inproceedings{liu_primo_2024,
	address = {Torino, Italia},
	title = {{PRIMO}: {Progressive} {Induction} for {Multi}-hop {Open} {Rule} {Generation}},
	url = {https://aclanthology.org/2024.lrec-main.1137/},
	abstract = {Open rules refer to the implication from premise atoms to hypothesis atoms, which captures various relationships between instances in the real world. Injecting open rule knowledge into the machine helps to improve the performance of downstream tasks such as dialogue and relation extraction. Existing approaches focus on single-hop open rule generation, ignoring scenarios involving multiple hops, leading to logical inconsistencies between premise and hypothesis atoms, as well as semantic duplication of generated rule atoms. To address these issues, we propose a progressive multi-stage open rule generation method called PRIMO. We introduce ontology information during the rule generation stage to reduce ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure consisting of generation, extraction, and rank modules to fully leverage the latent knowledge within the language model across multiple dimensions. Furthermore, we employ reinforcement learning from human feedback to further optimize model, enhancing the model's understanding of commonsense knowledge. Experimental results demonstrate that compared to baseline models, PRIMO significantly enhances rule quality and diversity while reducing the repetition rate of rule atoms.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Liu, Jianyu and Bi, Sheng and Qi, Guilin},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Language model, Pre-trained language model, Semantics, Knowledge management, Reinforcement learning, Computational linguistics, Extraction, Learning systems, Multi-hops, Real-world, Reinforcement learnings, Atoms, Multi-stages, Open rule, Reinforcement learning from human feedback, Rule generation, Rule knowledge},
	pages = {12988--12998},
	annote = {Cited by: 0},
}

@inproceedings{konan_automating_2024,
	address = {Mexico City, Mexico},
	title = {Automating the {Generation} of a {Functional} {Semantic} {Types} {Ontology} with {Foundational} {Models}},
	url = {https://aclanthology.org/2024.naacl-industry.21/},
	doi = {10.18653/v1/2024.naacl-industry.21},
	abstract = {The rise of data science, the inherent dirtiness of data, and the proliferation of vast data providers have increased the value proposition of Semantic Types. Semantic Types are a way of encoding contextual information onto a data schema that informs the user about the definitional meaning of data, its broader context, and relationships to other types. We increasingly see a world where providing structure to this information, attached directly to data, will enable both people and systems to better understand the content of a dataset and the ability to efficiently automate data tasks such as validation, mapping/joins, and eventually machine learning. While ontological systems exist, they have not had widespread adoption due to challenges in mapping to operational datasets and lack of specificity of entity-types. Additionally, the validation checks associated with data are stored in code bases separate from the datasets that are distributed. In this paper, we address both challenges holistically by proposing a system that efficiently maps and encodes functional meaning on Semantic Types.},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 6: {Industry} {Track})},
	publisher = {Association for Computational Linguistics},
	author = {Konan, Sachin and Rudolph, Larry and Affens, Scott},
	editor = {Yang, Yi and Davani, Aida and Sil, Avi and Kumar, Anoop},
	month = jun,
	year = {2024},
	pages = {248--265},
}

@inproceedings{chowdhury_cross_2024,
	address = {Miami, FL, USA},
	title = {Cross {Examine}: {An} {Ensemble}-based approach to leverage {Large} {Language} {Models} for {Legal} {Text} {Analytics}},
	url = {https://aclanthology.org/2024.nllp-1.16/},
	doi = {10.18653/v1/2024.nllp-1.16},
	abstract = {Legal documents are complex in nature, describing a course of argumentative reasoning that is followed to settle a case. Churning through large volumes of legal documents is a daily requirement for a large number of professionals who need access to the information embedded in them. Natural language processing methods that help in document summarization with key information components, insight extraction and question answering play a crucial role in legal text processing. Most of the existing document analysis systems use supervised machine learning, which require large volumes of annotated training data for every different application and are expensive to build. In this paper we propose a legal text analytics pipeline using Large Language Models (LLM), which can work with little or no training data. For document summarization, we propose an iterative pipeline using retrieval augmented generation to ensure that the generated text remains contextually relevant. For question answering, we propose a novel ontology-driven ensemble approach similar to cross-examination that exploits questioning and verification principles. A knowledge graph, created with the extracted information, stores the key entities and relationships reflecting the repository content structure. A new dataset is created with Indian court documents related to bail applications for cases filed under Protection of Children from Sexual Offences (POCSO) Act, 2012 an Indian law to protect children from sexual abuse and offences. Analysis of insights extracted from the answers reveal patterns of crime and social conditions leading to those crimes, which are important inputs for social scientists as well as legal system.},
	booktitle = {Proceedings of the {Natural} {Legal} {Language} {Processing} {Workshop} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Chowdhury, Saurav and Joshi, Suyog and Dey, Lipika},
	editor = {Aletras, Nikolaos and Chalkidis, Ilias and Barrett, Leslie and Goanță, Cătălina and Preoțiuc-Pietro, Daniel and Spanakis, Gerasimos},
	month = nov,
	year = {2024},
	pages = {194--204},
}

@inproceedings{vasilakis_evaluation_2024,
	address = {Oakland, USA},
	title = {Evaluation of {Pretrained} {Language} {Models} on {Music} {Understanding}},
	url = {https://aclanthology.org/2024.nlp4musa-1.16/},
	abstract = {Music-text multimodal systems have enabled new approaches to Music Information Research (MIR) applications. Despite the reported success, there has been little effort in evaluating the musical knowledge of Large Language Models (LLM). We demonstrate that LLMs suffer from prompt sensitivity, inability to model negation and sensitivity towards specific words. We quantified these properties as a triplet-based accuracy, evaluating the ability to model the relative similarity of labels in a hierarchical ontology. We leveraged Audioset ontology to generate triplets consisting of anchor, positive and negative label for genre/instruments sub-tree and use six general-purpose Transformer-based models. Triplets required filtering, as some were difficult to judge and therefore relatively uninformative for evaluation purposes. Despite the relatively high accuracy reported, inconsistencies are evident in all six models, suggesting that off-the-shelf LLMs need adaptation to music before use.},
	booktitle = {Proceedings of the 3rd {Workshop} on {NLP} for {Music} and {Audio} ({NLP4MusA})},
	publisher = {Association for Computational Lingustics},
	author = {Vasilakis, Yannis and Bittner, Rachel and Pauwels, Johan},
	editor = {Kruspe, Anna and Oramas, Sergio and Epure, Elena V. and Sordo, Mohamed and Weck, Benno and Doh, SeungHeon and Won, Minz and Manco, Ilaria and Meseguer-Brocal, Gabriel},
	month = nov,
	year = {2024},
	pages = {98--106},
}

@inproceedings{cominetti_impaqts_2024,
	address = {Torino, Italia},
	title = {{IMPAQTS}: a multimodal corpus of parliamentary and other political speeches in {Italy} (1946-2023), annotated with implicit strategies},
	url = {https://aclanthology.org/2024.parlaclarin-1.15/},
	abstract = {The paper introduces the IMPAQTS corpus of Italian political discourse, a multimodal corpus of around 2.65 million tokens including 1,500 speeches uttered by 150 prominent politicians spanning from 1946 to 2023. Covering the entire history of the Italian Republic, the collection exhibits a non-homogeneous consistency that progressively increases in quantity towards the present. The corpus is balanced according to textual and socio-linguistic criteria and includes different types of speeches. The sociolinguistic features of the speakers are carefully considered to ensure representation of Republican Italian politicians. For each speaker, the corpus contains 4 parliamentary speeches, 2 rallies, 1 party assembly, and 3 statements (in person or broadcasted). Parliamentary speeches therefore constitute the largest section of the corpus (40\% of the total), enabling direct comparison with other types of political speeches. The collection procedure, including details relevant to the transcription protocols, and the processing pipeline are described. The corpus has been pragmatically annotated to include information about the implicitly conveyed questionable contents, paired with their explicit paraphrasis, providing the largest Italian collection of ecologic examples of linguistic implicit strategies. The adopted ontology of linguistic implicitness and the fine-grained annotation scheme are presented in detail.},
	booktitle = {Proceedings of the {IV} {Workshop} on {Creating}, {Analysing}, and {Increasing} {Accessibility} of {Parliamentary} {Corpora} ({ParlaCLARIN}) @ {LREC}-{COLING} 2024},
	publisher = {ELRA and ICCL},
	author = {Cominetti, Federica and Gregori, Lorenzo and Lombardi Vallauri, Edoardo and Panunzi, Alessandro},
	editor = {Fiser, Darja and Eskevich, Maria and Bordon, David},
	month = may,
	year = {2024},
	pages = {101--109},
}

@inproceedings{wu_ehdchat_2024,
	address = {Miami, Florida, USA},
	title = {{EHDChat}: {A} {Knowledge}-{Grounded}, {Empathy}-{Enhanced} {Language} {Model} for {Healthcare} {Interactions}},
	url = {https://aclanthology.org/2024.sicon-1.10/},
	doi = {10.18653/v1/2024.sicon-1.10},
	abstract = {Large Language Models (LLMs) excel at a range of tasks but often struggle with issues like hallucination and inadequate empathy support. To address hallucinations, we ground our dialogues in medical knowledge sourced from external repositories such as Disease Ontology and DrugBank. To improve empathy support, we develop the Empathetic Healthcare Dialogues dataset, which utilizes multiple dialogue strategies in each response. This dataset is then used to fine-tune an LLM, and we introduce a lightweight, adaptable method called Strategy Combination Guidance to enhance the emotional support capabilities of the fine-tuned model, named EHDChat. Our evaluations show that EHDChat significantly outperforms existing models in providing emotional support and medical accuracy, demonstrating the effectiveness of our approach in enhancing empathetic and informed AI interactions in healthcare.},
	booktitle = {Proceedings of the {Second} {Workshop} on {Social} {Influence} in {Conversations} ({SICon} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Shenghan and Hsu, Wynne and Lee, Mong Li},
	editor = {Hale, James and Chawla, Kushal and Garg, Muskan},
	month = nov,
	year = {2024},
	pages = {141--151},
}

@inproceedings{vukovic_dialogue_2024,
	address = {Kyoto, Japan},
	title = {Dialogue {Ontology} {Relation} {Extraction} via {Constrained} {Chain}-of-{Thought} {Decoding}},
	url = {https://aclanthology.org/2024.sigdial-1.33/},
	doi = {10.18653/v1/2024.sigdial-1.33},
	abstract = {State-of-the-art task-oriented dialogue systems typically rely on task-specific ontologies for fulfilling user queries. The majority of task-oriented dialogue data, such as customer service recordings, comes without ontology and annotation. Such ontologies are normally built manually, limiting the application of specialised systems. Dialogue ontology construction is an approach for automating that process and typically consists of two steps: term extraction and relation extraction. In this work, we focus on relation extraction in a transfer learning set-up. To improve the generalisation, we propose an extension to the decoding mechanism of large language models. We adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning problems, to generative relation extraction. Here, we generate multiple branches in the decoding space and select the relations based on a confidence threshold. By constraining the decoding to ontology terms and relations, we aim to decrease the risk of hallucination. We conduct extensive experimentation on two widely used datasets and find improvements in performance on target ontology for source fine-tuned and one-shot prompted large language models.},
	booktitle = {Proceedings of the 25th {Annual} {Meeting} of the {Special} {Interest} {Group} on {Discourse} and {Dialogue}},
	publisher = {Association for Computational Linguistics},
	author = {Vukovic, Renato and Arps, David and van Niekerk, Carel and Ruppik, Benjamin Matthias and Lin, Hsien-chin and Heck, Michael and Gasic, Milica},
	editor = {Kawahara, Tatsuya and Demberg, Vera and Ultes, Stefan and Inoue, Koji and Mehri, Shikib and Howcroft, David and Komatani, Kazunori},
	month = sep,
	year = {2024},
	pages = {370--384},
}

@inproceedings{bacciu_handling_2024,
	address = {Mexico City, Mexico},
	title = {Handling {Ontology} {Gaps} in {Semantic} {Parsing}},
	url = {https://aclanthology.org/2024.starsem-1.28/},
	doi = {10.18653/v1/2024.starsem-1.28},
	abstract = {The majority of Neural Semantic Parsing (NSP) models are developed with the assumption that there are no concepts outside the ones such models can represent with their target symbols (closed-world assumption). This assumption leads to generate hallucinated outputs rather than admitting their lack of knowledge. Hallucinations can lead to wrong or potentially offensive responses to users. Hence, a mechanism to prevent this behavior is crucial to build trusted NSP-based Question Answering agents. To that end, we propose the Hallucination Simulation Framework (HSF), a general setting for stimulating and analyzing NSP model hallucinations. The framework can be applied to any NSP task with a closed-ontology. Using the proposed framework and KQA Pro as the benchmark dataset, we assess state-of-the-art techniques for hallucination detection. We then present a novel hallucination detection strategy that exploits the computational graph of the NSP model to detect the NSP hallucinations in the presence of ontology gaps, out-of-domain utterances, and to recognize NSP errors, improving the F1-Score respectively by {\textbackslash}textasciitilde21\%, {\textbackslash}textasciitilde24\% and {\textbackslash}textasciitilde1\%. This is the first work in closed-ontology NSP that addresses the problem of recognizing ontology gaps. We release our code and checkpoints at https://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.},
	booktitle = {Proceedings of the 13th {Joint} {Conference} on {Lexical} and {Computational} {Semantics} (*{SEM} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Bacciu, Andrea and Damonte, Marco and Basaldella, Marco and Monti, Emilio},
	editor = {Bollegala, Danushka and Shwartz, Vered},
	month = jun,
	year = {2024},
	pages = {345--359},
}

@inproceedings{saetia_financial_2024,
	address = {Bangkok, Thailand},
	title = {Financial {Product} {Ontology} {Population} with {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.textgraphs-1.4/},
	abstract = {Ontology population, which aims to extract structured data to enrich domain-specific ontologies from unstructured text, typically faces challenges in terms of data scarcity and linguistic complexity, particularly in specialized fields such as retail banking. In this study, we investigate the application of large language models (LLMs) to populate domain-specific ontologies of retail banking products from Thai corporate documents. We compare traditional span-based approaches to LLMs-based generative methods, with different prompting techniques. Our findings reveal that while span-based methods struggle with data scarcity and the complex linguistic structure, LLMs-based generative approaches substantially outperform, achieving a 61.05\% F1 score, with the most improvement coming from providing examples in the prompts. This improvement highlights the potential of LLMs for ontology population tasks, offering a scalable and efficient solution for structured information extraction in especially in low-resource language settings.},
	booktitle = {Proceedings of {TextGraphs}-17: {Graph}-based {Methods} for {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Saetia, Chanatip and Phruetthiset, Jiratha and Chalothorn, Tawunrat and Lertsutthiwong, Monchai and Taerungruang, Supawat and Buabthong, Pakpoom},
	editor = {Ustalov, Dmitry and Gao, Yanjun and Panchenko, Alexander and Tutubalina, Elena and Nikishina, Irina and Ramesh, Arti and Sakhovskiy, Andrey and Usbeck, Ricardo and Penn, Gerald and Valentino, Marco},
	month = aug,
	year = {2024},
	pages = {53--60},
}

@inproceedings{heddaya_causal_2024,
	address = {Miami, Florida, USA},
	title = {Causal {Micro}-{Narratives}},
	url = {https://aclanthology.org/2024.wnu-1.12/},
	doi = {10.18653/v1/2024.wnu-1.12},
	abstract = {We present a novel approach to classify causal micro-narratives from text. These narratives are sentence-level explanations of the cause(s) and/or effect(s) of a target subject. The approach requires only a subject-specific ontology of causes and effects, and we demonstrate it with an application to inflation narratives. Using a human-annotated dataset spanning historical and contemporary US news articles for training, we evaluate several large language models (LLMs) on this multi-label classification task. The best-performing model—a fine-tuned Llama 3.1 8B—achieves F1 scores of 0.87 on narrative detection and 0.71 on narrative classification. Comprehensive error analysis reveals challenges arising from linguistic ambiguity and highlights how model errors often mirror human annotator disagreements. This research establishes a framework for extracting causal micro-narratives from real-world data, with wide-ranging applications to social science research.},
	booktitle = {Proceedings of the 6th {Workshop} on {Narrative} {Understanding}},
	publisher = {Association for Computational Linguistics},
	author = {Heddaya, Mourad and Zeng, Qingcheng and Zentefis, Alexander and Voigt, Rob and Tan, Chenhao},
	editor = {Lal, Yash Kumar and Clark, Elizabeth and Iyyer, Mohit and Chaturvedi, Snigdha and Brei, Anneliese and Brahman, Faeze and Chandu, Khyathi Raghavi},
	month = nov,
	year = {2024},
	pages = {67--84},
}

@inproceedings{vukovic_ontology_2024,
	address = {Kyoto, Japan},
	title = {Ontology {Construction} for {Task}-oriented {Dialogue}},
	url = {https://aclanthology.org/2024.yrrsds-1.20/},
	doi = {10.18653/v1/2024.yrrsds-1.20},
	abstract = {My research interests lie generally in dialogue ontology construction, that uses techniques from information extraction to extract relevant terms from task-oriented dialogue data and order them by finding hierarchical relations between them.},
	booktitle = {Proceedings of the 20th {Workshop} of {Young} {Researchers}' {Roundtable} on {Spoken} {Dialogue} {Systems}},
	publisher = {Association for Computational Linguistics},
	author = {Vukovic, Renato},
	editor = {Inoue, Koji and Fu, Yahui and Axelsson, Agnes and Ohashi, Atsumoto and Madureira, Brielen and Zenimoto, Yuki and Mohapatra, Biswesh and Stricker, Armand and Khosla, Sopan},
	month = sep,
	year = {2024},
	pages = {53--56},
}

@inproceedings{park_deontological_2025,
	address = {Vienna, Austria},
	title = {Deontological {Keyword} {Bias}: {The} {Impact} of {Modal} {Expressions} on {Normative} {Judgments} of {Language} {Models}},
	isbn = {979-8-89176-251-0},
	url = {https://aclanthology.org/2025.acl-long.360/},
	doi = {10.18653/v1/2025.acl-long.360},
	abstract = {Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, where criteria for judgment are often unclear, even for humans. While LLM alignment studies cover many areas, one important yet underexplored area is how LLMs make judgments about obligations. This work reveals a strong tendency in LLMs to judge non-obligatory contexts as obligations when prompts are augmented with modal expressions such as \textit{must} or \textit{ought to}. We introduce this phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge over 90\% of commonsense scenarios as obligations when modal expressions are present. This tendency is consist across various LLM families, question types, and answer formats. To mitigate DKB, we propose a judgment strategy that integrates few-shot examples with reasoning prompts. This study sheds light on how modal expressions, as a form of linguistic framing, influence the normative decisions of LLMs and underscores the importance of addressing such biases to ensure judgment alignment.},
	booktitle = {Proceedings of the 63rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Park, Bumjin and Leejinsil, Leejinsil and Choi, Jaesik},
	editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
	month = jul,
	year = {2025},
	pages = {7277--7296},
}

@inproceedings{cui_uncertainty_2025,
	address = {Vienna, Austria},
	title = {Uncertainty in {Causality}: {A} {New} {Frontier}},
	isbn = {979-8-89176-251-0},
	url = {https://aclanthology.org/2025.acl-long.396/},
	doi = {10.18653/v1/2025.acl-long.396},
	abstract = {Understanding uncertainty in causality is vital in various domains, including core NLP tasks like event causality extraction, commonsense reasoning, and counterfactual text generation. However, existing literature lacks a comprehensive examination of this area. This survey aims to fill this gap by thoroughly reviewing uncertainty in causality. We first introduce a novel trichotomy, categorizing causal uncertainty into aleatoric (inherent randomness in causal data), epistemic (causal model limitations), and ontological (existence of causal links) uncertainty. We then survey methods for quantifying uncertainty in causal analysis and highlight the complementary relationship between causal uncertainty and causal strength. Furthermore, we examine the challenges that large language models (LLMs) face in handling causal uncertainty, such as hallucinations and inconsistencies, and propose key traits for an optimal causal LLM. Our paper reviews current approaches and outlines future research directions, aiming to serve as a practical guide for researchers and practitioners in this emerging field.},
	booktitle = {Proceedings of the 63rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Cui, Shaobo and Mouchel, Luca and Faltings, Boi},
	editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
	month = jul,
	year = {2025},
	pages = {8022--8044},
}

@inproceedings{liu_ontology-guided_2025,
	address = {Vienna, Austria},
	title = {Ontology-{Guided} {Reverse} {Thinking} {Makes} {Large} {Language} {Models} {Stronger} on {Knowledge} {Graph} {Question} {Answering}},
	isbn = {979-8-89176-251-0},
	url = {https://aclanthology.org/2025.acl-long.741/},
	doi = {10.18653/v1/2025.acl-long.741},
	abstract = {Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.},
	booktitle = {Proceedings of the 63rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Runxuan and Luobei, Luobei and Li, Jiaqi and Wang, Baoxin and Liu, Ming and Wu, Dayong and Wang, Shijin and Qin, Bing},
	editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
	month = jul,
	year = {2025},
	pages = {15269--15284},
}

@inproceedings{ha_synthia_2025,
	address = {Vienna, Austria},
	title = {{SYNTHIA}: {Novel} {Concept} {Design} with {Affordance} {Composition}},
	isbn = {979-8-89176-251-0},
	url = {https://aclanthology.org/2025.acl-long.1020/},
	doi = {10.18653/v1/2025.acl-long.1020},
	abstract = {Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, –the integration of multiple affordances into a single coherent concept–remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1\% and 14.7\% for novelty and functional coherence in human evaluation, respectively.},
	booktitle = {Proceedings of the 63rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ha, Hyeonjeong and Jin, Xiaomeng and Kim, Jeonghwan and Liu, Jiateng and Wang, Zhenhailong and Nguyen, Khanh Duy and Blume, Ansel and Peng, Nanyun and Chang, Kai-Wei and Ji, Heng},
	editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
	month = jul,
	year = {2025},
	pages = {20939--20958},
}

@inproceedings{liu_ma-coir_2025,
	address = {Vienna, Austria},
	title = {{MA}-{COIR}: {Leveraging} {Semantic} {Search} {Index} and {Generative} {Models} for {Ontology}-{Driven} {Biomedical} {Concept} {Recognition}},
	isbn = {979-8-89176-254-1},
	url = {https://aclanthology.org/2025.acl-srw.39/},
	doi = {10.18653/v1/2025.acl-srw.39},
	abstract = {Recognizing biomedical concepts in the text is vital for ontology refinement, knowledge graph construction, and concept relationship discovery. However, traditional concept recognition methods, relying on explicit mention identification, often fail to capture complex concepts not explicitly stated in the text. To overcome this limitation, we introduce MA-COIR, a framework that reformulates concept recognition as an indexing-recognition task. By assigning semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in ontology entries and enhances recognition efficiency. Using a pretrained BART-based model fine-tuned on small datasets, our approach reduces computational requirements to facilitate adoption by domain experts. Furthermore, we incorporate large language model (LLM)-generated queries and synthetic data to improve recognition in low-resource settings. Experimental results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of MA-COIR in recognizing both explicit and implicit concepts without the need for mention-level annotations during inference, advancing ontology-driven concept recognition in biomedical domain applications. Our code and constructed data are available at https://github.com/sl-633/macoir-master.},
	booktitle = {Proceedings of the 63rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 4: {Student} {Research} {Workshop})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Shanshan and Nishida, Noriki and Munne, Rumana Ferdous and Tokunaga, Narumi and Yamagata, Yuki and Kozaki, Kouji and Matsumoto, Yuji},
	editor = {Zhao, Jin and Wang, Mingyang and Liu, Zhu},
	month = jul,
	year = {2025},
	pages = {596--607},
}

@inproceedings{caporusso_computational_2025,
	address = {Vienna, Austria},
	title = {A {Computational} {Framework} to {Identify} {Self}-{Aspects} in {Text}},
	isbn = {979-8-89176-254-1},
	url = {https://aclanthology.org/2025.acl-srw.47/},
	doi = {10.18653/v1/2025.acl-srw.47},
	abstract = {This Ph.D. proposal introduces a plan to develop a computational framework to identify Self-aspects in text. The Self is a multifaceted construct and it is reflected in language. While it is described across disciplines like cognitive science and phenomenology, it remains underexplored in natural language processing (NLP). Many of the aspects of the Self align with psychological and other well-researched phenomena (e.g., those related to mental health), highlighting the need for systematic NLP-based analysis. In line with this, we plan to introduce an ontology of Self-aspects and a gold-standard annotated dataset. Using this foundation, we will develop and evaluate conventional discriminative models, generative large language models, and embedding-based retrieval approaches against four main criteria: interpretability, ground-truth adherence, accuracy, and computational efficiency. Top-performing models will be applied in case studies in mental health and empirical phenomenology.},
	booktitle = {Proceedings of the 63rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 4: {Student} {Research} {Workshop})},
	publisher = {Association for Computational Linguistics},
	author = {Caporusso, Jaya and Purver, Matthew and Pollak, Senja},
	editor = {Zhao, Jin and Wang, Mingyang and Liu, Zhu},
	month = jul,
	year = {2025},
	pages = {725--739},
}

@inproceedings{dong_tablecoder_2025,
	address = {Vienna, Austria},
	title = {{TableCoder}: {Table} {Extraction} from {Text} via {Reliable} {Code} {Generation}},
	isbn = {979-8-89176-288-6},
	url = {https://aclanthology.org/2025.acl-industry.98/},
	doi = {10.18653/v1/2025.acl-industry.98},
	abstract = {This paper introduces a task aimed at extracting structured tables from text using natural language (NL) instructions. We present TableCoder, an approach that leverages the symbolic nature of code to enhance the robustness of table structure construction and content extraction. TableCoder first generates Python classes or SQL statements to explicitly construct table structures, capturing semantic ontology, computational dependencies, numerical properties, and format strings. This approach reliably mitigates issues such as structural errors, erroneous computations, and mismatched value types. Subsequently, TableCoder proposes grounded content extraction, populating table cells sequentially and maintaining the exact order in which they are mentioned in the source text. By simulating a grounded “translation” from text to code, this method reduces the likelihood of omissions and hallucinations.Experimental results demonstrate that TableCoder significantly improves F1 scores and mitigates hallucination and computational errors, crucial for high-stakes applications like government data analytics and financial compliance reporting. Moreover, the code-generation-based method naturally integrates with standard SQL databases and Python workflows, ensuring seamless deployment in existing enterprise data pipelines.},
	booktitle = {Proceedings of the 63rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 6: {Industry} {Track})},
	publisher = {Association for Computational Linguistics},
	author = {Dong, Haoyu and Hu, Yue and Peng, Huailiang and Cao, Yanan},
	editor = {Rehm, Georg and Li, Yunyao},
	month = jul,
	year = {2025},
	pages = {1399--1412},
}

@inproceedings{yoo_enhancing_2025,
	address = {Viena, Austria},
	title = {Enhancing {Antimicrobial} {Drug} {Resistance} {Classification} by {Integrating} {Sequence}-{Based} and {Text}-{Based} {Representations}},
	isbn = {979-8-89176-275-6},
	url = {https://aclanthology.org/2025.bionlp-1.23/},
	doi = {10.18653/v1/2025.bionlp-1.23},
	abstract = {Antibiotic resistance identification is essential for public health, medical treatment, and drug development. Traditional sequence-based models struggle with accurate resistance prediction due to the lack of biological context. To address this, we propose an NLP-based model that integrates genetic sequences with structured textual annotations, including gene family classifications and resistance mechanisms. Our approach leverages pretrained language models for both genetic sequences and biomedical text, aligning biological metadata with sequence-based embeddings. We construct a novel dataset based on the Antibiotic Resistance Ontology (ARO), consolidating gene sequences with resistance-related textual information. Experiments show that incorporating domain knowledge significantly improves classification accuracy over sequence-only models, reducing reliance on exhaustive laboratory testing. By integrating genetic sequence processing with biomedical text understanding, our approach provides a scalable and interpretable solution for antibiotic resistance prediction.},
	booktitle = {Proceedings of the 24th {Workshop} on {Biomedical} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yoo, Hyunwoo and Sokhansanj, Bahrad and Brown, James},
	editor = {Demner-Fushman, Dina and Ananiadou, Sophia and Miwa, Makoto and Tsujii, Junichi},
	month = aug,
	year = {2025},
	pages = {263--273},
}

@article{zhang_neural_2025,
	title = {Neural {Semantic} {Parsing} with {Extremely} {Rich} {Symbolic} {Meaning} {Representations}},
	volume = {51},
	url = {https://aclanthology.org/2025.cl-1.7/},
	doi = {10.1162/coli_a_00542},
	abstract = {Current open-domain neural semantics parsers show impressive performance. However, closer inspection of the symbolic meaning representations they produce reveals significant weaknesses: Sometimes they tend to merely copy character sequences from the source text to form symbolic concepts, defaulting to the most frequent word sense based in the training distribution. By leveraging the hierarchical structure of a lexical ontology, we introduce a novel compositional symbolic representation for concepts based on their position in the taxonomical hierarchy. This representation provides richer semantic information and enhances interpretability. We introduce a neural “taxonomical” semantic parser to utilize this new representation system of predicates, and compare it with a standard neural semantic parser trained on the traditional meaning representation format, employing a novel challenge set and evaluation metric for evaluation. Our experimental findings demonstrate that the taxonomical model, trained on much richer and complex meaning representations, is slightly subordinate in performance to the traditional model using the standard metrics for evaluation, but outperforms it when dealing with out-of-vocabulary concepts. We further show through neural model probing that training on a taxonomic representation enhances the model's ability to learn the taxonomical hierarchy. This finding is encouraging for research in computational semantics that aims to combine data-driven distributional meanings with knowledge-based symbolic representations.},
	number = {1},
	journal = {Computational Linguistics},
	author = {Zhang, Xiao and Bouma, Gosse and Bos, Johan},
	month = mar,
	year = {2025},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {235--274},
}

@inproceedings{munne_zero-shot_2025,
	address = {Abu Dhabi, UAE},
	title = {Zero-{Shot} {Entailment} {Learning} for {Ontology}-{Based} {Biomedical} {Annotation} {Without} {Explicit} {Mentions}},
	url = {https://aclanthology.org/2025.coling-main.542/},
	abstract = {Automatic biomedical annotation is essential for advancing medical research, diagnosis, and treatment. However, it presents significant challenges, especially when entities are not explicitly mentioned in the text, leading to difficulties in extraction of relevant information. These challenges are intensified by unclear terminology, implicit background knowledge, and the lack of labeled training data. Annotating with a specific ontology adds another layer of complexity, as it requires aligning text with a predefined set of concepts and relationships. Manual annotation is time-consuming and expensive, highlighting the need for automated systems to handle large volumes of biomedical data efficiently. In this paper, we propose an entailment-based zero-shot text classification approach to annotate biomedical text passages using the Homeostasis Imbalance Process (HOIP) ontology. Our method reformulates the annotation task as a multi-class, multi-label classification problem and uses natural language inference to classify text into related HOIP processes. Experimental results show promising performance, especially when processes are not explicitly mentioned, highlighting the effectiveness of our approach for ontological annotation of biomedical literature.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Munne, Rumana Ferdous and Nishida, Noriki and Liu, Shanshan and Tokunaga, Narumi and Yamagata, Yuki and Kozaki, Kouji and Matsumoto, Yuji},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	month = jan,
	year = {2025},
	pages = {8148--8159},
}

@inproceedings{kuhn_enhancing_2025,
	address = {Abu Dhabi, UAE},
	title = {Enhancing {Rhetorical} {Figure} {Annotation}: {An} {Ontology}-{Based} {Web} {Application} with {RAG} {Integration}},
	url = {https://aclanthology.org/2025.coling-main.587/},
	abstract = {Rhetorical figures play an important role in our communication. They are used to convey subtle, implicit meaning, or to emphasize statements. We notice them in hate speech, fake news, and propaganda. By improving the systems for computational detection of rhetorical figures, we can also improve tasks such as hate speech and fake news detection, sentiment analysis, opinion mining, or argument mining. Unfortunately, there is a lack of annotated data, as well as qualified annotators that would help us build large corpora to train machine learning models for the detection of rhetorical figures. The situation is particularly difficult in languages other than English, and for rhetorical figures other than metaphor, sarcasm, and irony. To overcome this issue, we develop a web application called “Find your Figure” that facilitates the identification and annotation of German rhetorical figures. The application is based on the German Rhetorical ontology GRhOOT which we have specially adapted for this purpose. In addition, we improve the user experience with Retrieval Augmented Generation (RAG). In this paper, we present the restructuring of the ontology, the development of the web application, and the built-in RAG pipeline. We also identify the optimal RAG settings for our application. Our approach is one of the first to practically use rhetorical ontologies in combination with RAG and shows promising results.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kühn, Ramona and Mitrović, Jelena and Granitzer, Michael},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	month = jan,
	year = {2025},
	pages = {8774--8786},
}

@inproceedings{zuo_knowcoder-x_2025,
	address = {Vienna, Austria},
	title = {{KnowCoder}-{X}: {Boosting} {Multilingual} {Information} {Extraction} via {Code}},
	isbn = {979-8-89176-256-5},
	url = {https://aclanthology.org/2025.findings-acl.748/},
	doi = {10.18653/v1/2025.findings-acl.748},
	abstract = {Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model's cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17\% and SoTA by 20.03\%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Zuo, Yuxin and Jiang, Wenxuan and Liu, Wenxuan and Li, Zixuan and Bai, Long and Wang, Hanbin and Zeng, Yutao and Jin, Xiaolong and Guo, Jiafeng and Cheng, Xueqi},
	editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
	month = jul,
	year = {2025},
	pages = {14486--14509},
}

@inproceedings{hong_evaluating_2025,
	address = {Vienna, Austria},
	title = {Evaluating {LLMs}' {Mathematical} and {Coding} {Competency} through {Ontology}-guided {Interventions}},
	isbn = {979-8-89176-256-5},
	url = {https://aclanthology.org/2025.findings-acl.1172/},
	doi = {10.18653/v1/2025.findings-acl.1172},
	abstract = {Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce (i) a general ontology of perturbations for math and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, GSMore and HumanEval-Core, respectively, of perturbed math and coding problems to probe LLM capabilities in numeric reasoning and coding tasks.Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust problem solving skills and structured reasoning abilities in many areas, as defined by our ontology.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Hong, Pengfei and Majumder, Navonil and Ghosal, Deepanway and Aditya, Somak and Mihalcea, Rada and Poria, Soujanya},
	editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
	month = jul,
	year = {2025},
	pages = {22811--22849},
}

@inproceedings{lin_gems_2025,
	address = {Vienna, Austria},
	title = {{GEMS}: {Generation}-{Based} {Event} {Argument} {Extraction} via {Multi}-perspective {Prompts} and {Ontology} {Steering}},
	isbn = {979-8-89176-256-5},
	url = {https://aclanthology.org/2025.findings-acl.1353/},
	doi = {10.18653/v1/2025.findings-acl.1353},
	abstract = {Generative methods significantly advance event argument extraction by probabilistically generating event argument sequences in a structured format. However, existing approaches primarily rely on a single prompt to generate event arguments in a fixed, predetermined order. Such a rigid approach overlooks the complex structural and dynamic interdependencies among event arguments. In this work, we present GEMS, a multi-prompt learning framework that Generates Event arguments via Multi-perspective prompts and ontology Steering. Specifically, GEMS utilizes multiple unfilled prompts for each sentence, predicting event arguments in varying sequences to explicitly capture the interrelationships between arguments. These predictions are subsequently aggregated using a voting mechanism. Furthermore, an ontology-driven steering mechanism is proposed to ensure that the generated arguments are contextually appropriate and consistent with event-specific knowledge. Extensive experiments on two benchmark datasets demonstrate that GEMS achieves state-of-the-art performance, particularly in low-resource settings. The source code is available at: https://github.com/AONE-NLP/EAE-GEMS},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Run and Liu, Yao and Gan, Yanglei and Cai, Yuxiang and Lan, Tian and Liu, Qiao},
	editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
	month = jul,
	year = {2025},
	pages = {26392--26409},
}

@inproceedings{abdo_amwal_2025,
	address = {Abu Dhabi, UAE},
	title = {{AMWAL}: {Named} {Entity} {Recognition} for {Arabic} {Financial} {News}},
	url = {https://aclanthology.org/2025.finnlp-1.20/},
	abstract = {Financial Named Entity Recognition (NER) presents a pivotal task in extracting structured information from unstructured financial data, especially when extending its application to languages beyond English. In this paper, we present AMWAL, a named entity recognition system for Arabic financial news. Our approach centered on building a specialized corpus compiled from three major Arabic financial newspapers spanning from 2000 to 2023. Entities were extracted from this corpus using a semi-automatic process that included manual annotation and review to ensure accuracy. The total number of entities identified amounts to 17.1k tokens, distributed across 20 categories, providing a comprehensive coverage of financial entities. To standardize the identified entities, we adopt financial concepts from the Financial Industry Business Ontology (FIBO, 2020), aligning our framework with industry standards. The significance of our work lies not only in the creation of the first customized NER system for Arabic financial data but also in its potential to streamline information extraction processes in the financial domain. Our NER system achieves a Precision score of 96.08, a Recall score of 95.87, and an F1 score of 95.97, which outperforms state-of-the-art general Arabic NER systems as well as other systems for financial NER in other languages.},
	booktitle = {Proceedings of the {Joint} {Workshop} of the 9th {Financial} {Technology} and {Natural} {Language} {Processing} ({FinNLP}), the 6th {Financial} {Narrative} {Processing} ({FNP}), and the 1st {Workshop} on {Large} {Language} {Models} for {Finance} and {Legal} ({LLMFinLegal})},
	publisher = {Association for Computational Linguistics},
	author = {Abdo, Muhammad S. and Hatekar, Yash and Cavar, Damir},
	editor = {Chen, Chung-Chi and Moreno-Sandoval, Antonio and Huang, Jimin and Xie, Qianqian and Ananiadou, Sophia and Chen, Hsin-Hsi},
	month = jan,
	year = {2025},
	pages = {207--213},
}

@inproceedings{christou_artificial_2025,
	address = {Albuquerque, New Mexico},
	title = {Artificial {Relationships} in {Fiction}: {A} {Dataset} for {Advancing} {NLP} in {Literary} {Domains}},
	isbn = {979-8-89176-241-1},
	url = {https://aclanthology.org/2025.latechclfl-1.13/},
	doi = {10.18653/v1/2025.latechclfl-1.13},
	abstract = {Relation extraction (RE) in fiction presents unique NLP challenges due to implicit, narrative-driven relationships. Unlike factual texts, fiction weaves complex connections, yet existing RE datasets focus on non-fiction. To address this, we introduce Artificial Relationships in Fiction (ARF), a synthetically annotated dataset for literary RE. Built from diverse Project Gutenberg fiction, ARF considers author demographics, publication periods, and themes. We curated an ontology for fiction-specific entities and relations, and using GPT-4o, generated artificial relationships to capture narrative complexity. Our analysis demonstrates its value for finetuning RE models and advancing computational literary studies. By bridging a critical RE gap, ARF enables deeper exploration of fictional relationships, enriching NLP research at the intersection of storytelling and AI-driven literary analysis.},
	booktitle = {Proceedings of the 9th {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature} ({LaTeCH}-{CLfL} 2025)},
	publisher = {Association for Computational Linguistics},
	author = {Christou, Despina and Tsoumakas, Grigorios},
	editor = {Kazantseva, Anna and Szpakowicz, Stan and Degaetano-Ortlieb, Stefania and Bizzoni, Yuri and Pagel, Janis},
	month = may,
	year = {2025},
	pages = {130--147},
}

@inproceedings{uresova_creating_2025,
	address = {Vienna, Austria},
	title = {Creating {Hierarchical} {Relations} in a {Multilingual} {Event}-type {Ontology}},
	isbn = {979-8-89176-262-6},
	url = {https://aclanthology.org/2025.law-1.19/},
	doi = {10.18653/v1/2025.law-1.19},
	abstract = {This paper describes the work on hierarchization of the SynSemClass event-type ontology. The original resource has been extended by a hierarchical structure to model specialization and generalization relations between classes that are formally and technically unrelated in the original ontology. The goal is to enable one to use the ontology enriched by the hierarchical concepts for annotation of running texts in symbolic meaning representations, such as UMR or PDT. The hierarchy is in principle built bottom-up, based on existing SSC classes (concepts). This approach differs from other approaches to semantic classes, such as in WordNet or VerbNet. Although the hierarchical relations are similar, the underlying nodes in the hierarchy are not. In this paper, we describe the challenges related to the principles chosen: single-tree constraint and finding features for the definitions of specificity/generality. Also, a pilot inter-annotator experiment is described that shows the difficulty of the hierarchization task.},
	booktitle = {Proceedings of the 19th {Linguistic} {Annotation} {Workshop} ({LAW}-{XIX}-2025)},
	publisher = {Association for Computational Linguistics},
	author = {Urešová, Zdeňka and Fučíková, Eva and Hajič, Jan},
	editor = {Peng, Siyao and Rehbein, Ines},
	month = jul,
	year = {2025},
	pages = {240--249},
}

@inproceedings{li_privacy_2025,
	address = {Albuquerque, New Mexico},
	title = {Privacy {Checklist}: {Privacy} {Violation} {Detection} {Grounding} on {Contextual} {Integrity} {Theory}},
	isbn = {979-8-89176-189-6},
	url = {https://aclanthology.org/2025.naacl-long.86/},
	doi = {10.18653/v1/2025.naacl-long.86},
	abstract = {Privacy research has attracted wide attention as individuals worry that their private data can be easily leaked during interactions with smart devices, social platforms, and AI applications. Existing works mostly consider privacy attacks and defenses on various sub-fields. Within each field, various privacy attacks and defenses are studied to address patterns of personally identifiable information (PII). In this paper, we argue that privacy is not solely about PII patterns. We ground on the Contextual Integrity (CI) theory which posits that people's perceptions of privacy are highly correlated with the corresponding social context. Based on such an assumption, we formulate privacy as a reasoning problem rather than naive PII matching. We develop the first comprehensive checklist that covers social identities, private attributes, and existing privacy regulations. Unlike prior works on CI that either cover limited expert annotated norms or model incomplete social context, our proposed privacy checklist uses the whole Health Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to show that we can resort to large language models (LLMs) to completely cover the HIPAA's regulations. Additionally, our checklist also gathers expert annotations across multiple ontologies to determine private information including but not limited to PII. We use our preliminary results on the HIPAA to shed light on future context-centric privacy research to cover more privacy regulations, social norms and standards. We will release the reproducible code and data.},
	booktitle = {Proceedings of the 2025 {Conference} of the {Nations} of the {Americas} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Haoran and Fan, Wei and Chen, Yulin and Jiayang, Cheng and Chu, Tianshu and Zhou, Xuebing and Hu, Peizhao and Song, Yangqiu},
	editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
	month = apr,
	year = {2025},
	pages = {1748--1766},
}

@inproceedings{wysocka_syllobio-nli_2025,
	address = {Albuquerque, New Mexico},
	title = {{SylloBio}-{NLI}: {Evaluating} {Large} {Language} {Models} on {Biomedical} {Syllogistic} {Reasoning}},
	isbn = {979-8-89176-189-6},
	url = {https://aclanthology.org/2025.naacl-long.371/},
	doi = {10.18653/v1/2025.naacl-long.371},
	abstract = {Syllogistic reasoning is crucial for Natural Language Inference (NLI). This capability is particularly significant in specialized domains such as biomedicine, where it can support automatic evidence interpretation and scientific discovery. This paper presents SylloBio-NLI, a novel framework that leverages external ontologies to systematically instantiate diverse syllogistic arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language Models (LLMs) on identifying valid conclusions and extracting supporting evidence across 28 syllogistic schemes instantiated with human genome pathways. Extensive experiments reveal that biomedical syllogistic reasoning is particularly challenging for zero-shot LLMs, which achieve an average accuracy between 70\% on generalized modus ponens and 23\% on disjunctive syllogism. At the same time, we found that few-shot prompting can boost the performance of different LLMs, including Gemma (+14\%) and LLama-3 (+43\%). However, a deeper analysis shows that both techniques exhibit high sensitivity to superficial lexical variations, highlighting a dependency between reliability, models' architecture, and pre-training regime. Overall, our results indicate that, while in-context examples have the potential to elicit syllogistic reasoning in LLMs, existing models are still far from achieving the robustness and consistency required for safe biomedical NLI applications.},
	booktitle = {Proceedings of the 2025 {Conference} of the {Nations} of the {Americas} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wysocka, Magdalena and Carvalho, Danilo and Wysocki, Oskar and Valentino, Marco and Freitas, Andre},
	editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
	month = apr,
	year = {2025},
	pages = {7235--7258},
}

@inproceedings{safa_zero-shot_2025,
	address = {Albuquerque, New Mexico},
	title = {A {Zero}-{Shot} {Open}-{Vocabulary} {Pipeline} for {Dialogue} {Understanding}},
	isbn = {979-8-89176-189-6},
	url = {https://aclanthology.org/2025.naacl-long.387/},
	doi = {10.18653/v1/2025.naacl-long.387},
	abstract = {Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing fully-trained systems, limiting their practicality. To address these limitations, we propose a zero-shot, open-vocabulary system that integrates domain classification and DST in a single pipeline. Our approach includes reformulating DST as a question-answering task for less capable models and employing self-refining prompts for more adaptable ones. Our system does not rely on fixed slot values defined in the ontology allowing the system to adapt dynamically. We compare our approach with existing SOTA, and show that it provides up to 20\% better Joint Goal Accuracy (JGA) over previous methods on datasets like MultiWOZ 2.1, with up to 90\% fewer requests to the LLM API.},
	booktitle = {Proceedings of the 2025 {Conference} of the {Nations} of the {Americas} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Safa, Abdulfattah and Şahin, Gözde Gül},
	editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
	month = apr,
	year = {2025},
	pages = {7562--7579},
}

@inproceedings{chen_zero-shot_2025,
	address = {Albuquerque, New Mexico},
	title = {Zero-{Shot} {ATC} {Coding} with {Large} {Language} {Models} for {Clinical} {Assessments}},
	isbn = {979-8-89176-194-0},
	url = {https://aclanthology.org/2025.naacl-industry.19/},
	doi = {10.18653/v1/2025.naacl-industry.19},
	abstract = {Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to prescription records is a significant bottleneck in healthcare research and operations at Ontario Health and InterRAI Canada, requiring extensive expert time and effort. To automate this process while maintaining data privacy, we develop a practical approach using locally deployable large language models (LLMs). Inspired by recent advances in automatic International Classification of Diseases (ICD) coding, our method frames ATC coding as a hierarchical information extraction task, guiding LLMs through the ATC ontology level by level. We evaluate our approach using GPT-4o as an accuracy ceiling and focus development on open-source Llama models suitable for privacy-sensitive deployment. Testing across Health Canada drug product data, the RABBITS benchmark, and real clinical notes from Ontario Health, our method achieves 78\% exact match accuracy with GPT-4o and 60\% with Llama 3.1 70B. We investigate knowledge grounding through drug definitions, finding modest improvements in accuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama 3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller models. Our results demonstrate the feasibility of automatic ATC coding in privacy-sensitive healthcare environments, providing a foundation for future deployments.},
	booktitle = {Proceedings of the 2025 {Conference} of the {Nations} of the {Americas} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 3: {Industry} {Track})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Zijian and Gamble, John-Michael and Jantzi, Micaela and Hirdes, John P. and Lin, Jimmy},
	editor = {Chen, Weizhu and Yang, Yi and Kachuee, Mohammad and Fu, Xue-Yong},
	month = apr,
	year = {2025},
	pages = {226--232},
}

@inproceedings{beyer_linguistic_2025,
	address = {Albuquerque, USA},
	title = {Linguistic {Features} in {German} {BERT}: {The} {Role} of {Morphology}, {Syntax}, and {Semantics} in {Multi}-{Class} {Text} {Classification}},
	isbn = {979-8-89176-192-6},
	url = {https://aclanthology.org/2025.naacl-srw.3/},
	doi = {10.18653/v1/2025.naacl-srw.3},
	abstract = {Most studies on the linguistic information encoded by BERT primarily focus on English. Our study examines a monolingual German BERT model using a semantic classification task on newspaper articles, analysing the linguistic features influencing classification decisions through SHAP values. We use the TüBa-D/Z corpus, a resource with gold-standard annotations for a set of linguistic features, including POS, inflectional morphology, phrasal, clausal, and dependency structures. Semantic features of nouns are evaluated via the GermaNet ontology using shared hypernyms. Our results indicate that the features identified in English also affect classification in German but suggests important language- and task-specific features as well.},
	booktitle = {Proceedings of the 2025 {Conference} of the {Nations} of the {Americas} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 4: {Student} {Research} {Workshop})},
	publisher = {Association for Computational Linguistics},
	author = {Beyer, Henrike and Frassinelli, Diego},
	editor = {Ebrahimi, Abteen and Haider, Samar and Liu, Emmy and Haider, Sammar and Leonor Pacheco, Maria and Wein, Shira},
	month = apr,
	year = {2025},
	pages = {28--39},
}

@inproceedings{wu_what_2025,
	address = {Vienna, Austria},
	title = {What {Counts} {Underlying} {LLMs}' {Moral} {Dilemma} {Judgments}?},
	isbn = {978-1-959429-19-7},
	url = {https://aclanthology.org/2025.nlp4pi-1.12/},
	doi = {10.18653/v1/2025.nlp4pi-1.12},
	abstract = {Moral judgments in LLMs increasingly capture the attention of researchers in AI ethics domain. This study explores moral judgments of three open-source large language models (LLMs)—Qwen-1.5-14B, Llama3-8B, and DeepSeek-R1 in plausible moral dilemmas, examining their sensitivity to social exposure and collaborative decision-making. Using a dual-process framework grounded in deontology and utilitarianism, we evaluate LLMs' responses to moral dilemmas under varying social contexts. Results reveal that all models are significantly influenced by moral norms rather than consequences, with DeepSeek-R1 exhibiting a stronger action tendency compared to Qwen-1.5-14B and Llama3-8B, which show higher inaction preferences. Social exposure and collaboration impact LLMs differently: Qwen-1.5-14B becomes less aligned with moral norms under observation, while DeepSeek-R1's action tendency is moderated by social collaboration. These findings highlight the nuanced moral reasoning capabilities of LLMs and their varying sensitivity to social cues, providing insights into the ethical alignment of AI systems in socially embedded contexts.},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {NLP} for {Positive} {Impact} ({NLP4PI})},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Wenya and Deng, Weihong},
	editor = {Atwell, Katherine and Biester, Laura and Borah, Angana and Dementieva, Daryna and Ignat, Oana and Kotonya, Neema and Liu, Ziyi and Wan, Ruyuan and Wilson, Steven and Zhao, Jieyu},
	month = jul,
	year = {2025},
	pages = {144--150},
}

@inproceedings{bayrami_asl_tekanlou_homa_2025,
	address = {Vienna, Austria},
	title = {Homa at {SemEval}-2025 {Task} 5: {Aligning} {Librarian} {Records} with {OntoAligner} for {Subject} {Tagging}},
	isbn = {979-8-89176-273-2},
	url = {https://aclanthology.org/2025.semeval-1.312/},
	abstract = {This paper presents our system, Homa, for SemEval-2025 Task 5: Subject Tagging, which focuses on automatically assigning subject labels to technical records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner, a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented generation (RAG) techniques. Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity. We evaluate OntoAligner's adaptability for subject indexing and analyze its effectiveness in handling multilingual records. Experimental results demonstrate the strengths and limitations of this method, highlighting the potential of alignment techniques for improving subject tagging in digital libraries.},
	booktitle = {Proceedings of the 19th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2025)},
	publisher = {Association for Computational Linguistics},
	author = {Bayrami Asl Tekanlou, Hadi and Razmara, Jafar and Sanaei, Mahsa and Rahgouy, Mostafa and Babaei Giglou, Hamed},
	editor = {Rosenthal, Sara and Rosá, Aiala and Ghosh, Debanjan and Zampieri, Marcos},
	month = jul,
	year = {2025},
	pages = {2400--2406},
}

@inproceedings{salfinger_if_2025,
	address = {Vienna, Austria},
	title = {{LA}²{I}²{F} at {SemEval}-2025 {Task} 5: {Reasoning} in {Embedding} {Space} – {Fusing} {Analogical} and {Ontology}-based {Reasoning} for {Document} {Subject} {Tagging}},
	isbn = {979-8-89176-273-2},
	url = {https://aclanthology.org/2025.semeval-1.314/},
	abstract = {The LLMs4Subjects shared task invited system contributions that leverage a technical library's tagged document corpus to learn document subject tagging, i.e., proposing adequate subjects given a document's title and abstract. To address the imbalance of this training corpus, team LA²I²F devised a semantic retrieval-based system fusing the results of ontological and analogical reasoning in embedding vector space. Our results outperformed a naive baseline of prompting a llama 3.1-based model, whilst being computationally more efficient and competitive with the state of the art.},
	booktitle = {Proceedings of the 19th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2025)},
	publisher = {Association for Computational Linguistics},
	author = {Salfinger, Andrea and Zaccagna, Luca and Incitti, Francesca and De Nardi, Gianluca and Dal Fabbro, Lorenzo and Snidaro, Lauro},
	editor = {Rosenthal, Sara and Rosá, Aiala and Ghosh, Debanjan and Zampieri, Marcos},
	month = jul,
	year = {2025},
	pages = {2413--2423},
}

@article{ventura_navigating_2025,
	title = {Navigating {Cultural} {Chasms}: {Exploring} and {Unlocking} the {Cultural} {POV} of {Text}-{To}-{Image} {Models}},
	volume = {13},
	url = {https://aclanthology.org/2025.tacl-1.10/},
	doi = {10.1162/tacl_a_00732},
	abstract = {Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have demonstrated remarkable prompt-based image generation capabilities. Multilingual encoders may have a substantial impact on the cultural agency of these models, as language is a conduit of culture. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three tiers: cultural dimensions, cultural domains, and cultural concepts. Based on this ontology, we derive prompt templates to unlock the cultural knowledge in TTI models, and propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer models and human assessments, to evaluate the cultural content of TTI-generated images. To bolster our research, we introduce the CulText2I dataset, based on six diverse TTI models and spanning ten languages. Our experiments provide insights regarding Do, What, Which, and How research questions about the nature of cultural encoding in TTI models, paving the way for cross-cultural applications of these models.1},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Ventura, Mor and Ben-David, Eyal and Korhonen, Anna and Reichart, Roi},
	year = {2025},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {142--166},
}

@inproceedings{mehryar_resolution-alignment-completion_2025,
	address = {Vienna, Austria},
	title = {Resolution-{Alignment}-{Completion} of {Tabular} {Electronic} {Health} {Records} via {Meta}-{Path} {Generative} {Sampling}},
	isbn = {979-8-89176-268-8},
	url = {https://aclanthology.org/2025.trl-1.17/},
	doi = {10.18653/v1/2025.trl-1.17},
	abstract = {The increasing availability of electronic health records (EHR) offers significant opportunities in data-driven healthcare, yet much of this data remains fragmented, semantically inconsistent, or incomplete. These issues are particularly evident in tabular patient records where important contextual information are lacking from the input for effective modeling. In this work, we introduce a system that performs ontology-based entity alignment to resolve and complete tabular data used in real-world clinical units. We transform patient records into a knowledge graph and capture its hidden structures through graph embeddings. We further propose a meta-path sample generation approach for completing the missing information. Our experiments demonstrate the system's ability to augment cardiovascular disease (CVD) data for lab event detection, diagnosis prediction, and drug recommendation, enabling more robust and precise predictive models in clinical decision-making.},
	booktitle = {Proceedings of the 4th {Table} {Representation} {Learning} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Mehryar, S},
	editor = {Chang, Shuaichen and Hulsebos, Madelon and Liu, Qian and Chen, Wenhu and Sun, Huan},
	month = jul,
	year = {2025},
	pages = {200--207},
}

@inproceedings{pagano_treen_2025,
	address = {Ljubljana, Slovenia},
	title = {{TreEn}: {A} {Multilingual} {Treebank} {Project} on {Environmental} {Discourse}},
	isbn = {979-8-89176-292-3},
	url = {https://aclanthology.org/2025.udw-1.9/},
	abstract = {The increasing complexity of environmental discourse is directly proportional to the growing complexity of environmental debates present today in all communication media. While linguistic and communication studies have been pursued on this discourse, the development of computational linguistic tools and resources dedicated to support its analysis and interpretation is still very incipient. For one, no morphosyntactic resources specific to the environmental domain can be found on major platforms and repositories. This paper introduces TreEn, a multilingual treebank project in progress which compiles texts on environmental discourse produced in different conversational and communication contexts. In particular, it reports on the parallel component of the project and discusses issues faced during sentence-level alignment between original and translated texts, annotation of texts following UD guidelines, and labeling entities drawing on an ontology of environmental-related topics. This novel resource is expected to support environmental discourse analysis by providing morphological and syntactical data to enable cross-language and cross-cultural comparison based on the semantics of the entities annotated in the treebank.},
	booktitle = {Proceedings of the {Eighth} {Workshop} on {Universal} {Dependencies} ({UDW}, {SyntaxFest} 2025)},
	publisher = {Association for Computational Linguistics},
	author = {Pagano, Adriana Silvina and Chiril, Patricia and Chierchiello, Elisa and Bosco, Cristina},
	editor = {Bouma, Gosse and Çöltekin, Çağrı},
	month = aug,
	year = {2025},
	pages = {80--96},
}

@inproceedings{montiel-ponsoda_modelling_2008,
	address = {Manchester, UK},
	title = {Modelling {Multilinguality} in {Ontologies}},
	url = {https://aclanthology.org/C08-2017/},
	booktitle = {Coling 2008: {Companion} volume: {Posters}},
	publisher = {Coling 2008 Organizing Committee},
	author = {Montiel-Ponsoda, Elena and Aguado de Cea, Guadalupe and Gómez-Pérez, Asunción and Peters, Wim},
	editor = {Scott, Donia and Uszkoreit, Hans},
	month = aug,
	year = {2008},
	pages = {67--70},
}

@inproceedings{chen_structured_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Structured {Dialogue} {Policy} with {Graph} {Neural} {Networks}},
	url = {https://aclanthology.org/C18-1107/},
	abstract = {Recently, deep reinforcement learning (DRL) has been used for dialogue policy optimization. However, many DRL-based policies are not sample-efficient. Most recent advances focus on improving DRL optimization algorithms to address this issue. Here, we take an alternative route of designing neural network structure that is better suited for DRL-based dialogue management. The proposed structured deep reinforcement learning is based on graph neural networks (GNN), which consists of some sub-networks, each one for a node on a directed graph. The graph is defined according to the domain ontology and each node can be considered as a sub-agent. During decision making, these sub-agents have internal message exchange between neighbors on the graph. We also propose an approach to jointly optimize the graph structure as well as the parameters of GNN. Experiments show that structured DRL significantly outperforms previous state-of-the-art approaches in almost all of the 18 tasks of the PyDial benchmark.},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Lu and Tan, Bowen and Long, Sishan and Yu, Kai},
	editor = {Bender, Emily M. and Derczynski, Leon and Isabelle, Pierre},
	month = aug,
	year = {2018},
	pages = {1257--1268},
}

@inproceedings{kazeminejad_automatically_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Automatically {Extracting} {Qualia} {Relations} for the {Rich} {Event} {Ontology}},
	url = {https://aclanthology.org/C18-1224/},
	abstract = {Commonsense, real-world knowledge about the events that entities or “things in the world” are typically involved in, as well as part-whole relationships, is valuable for allowing computational systems to draw everyday inferences about the world. Here, we focus on automatically extracting information about (1) the events that typically bring about certain entities (origins), (2) the events that are the typical functions of entities, and (3) part-whole relationships in entities. These correspond to the agentive, telic and constitutive qualia central to the Generative Lexicon. We describe our motivations and methods for extracting these qualia relations from the Suggested Upper Merged Ontology (SUMO) and show that human annotators overwhelmingly find the information extracted to be reasonable. Because ontologies provide a way of structuring this information and making it accessible to agents and computational systems generally, efforts are underway to incorporate the extracted information to an ontology hub of Natural Language Processing semantic role labeling resources, the Rich Event Ontology.},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kazeminejad, Ghazaleh and Bonial, Claire and Brown, Susan Windisch and Palmer, Martha},
	editor = {Bender, Emily M. and Derczynski, Leon and Isabelle, Pierre},
	month = aug,
	year = {2018},
	pages = {2644--2652},
}

@inproceedings{dasgupta_automatic_2018,
	address = {Santa Fe, New Mexico},
	title = {Automatic {Curation} and {Visualization} of {Crime} {Related} {Information} from {Incrementally} {Crawled} {Multi}-source {News} {Reports}},
	url = {https://aclanthology.org/C18-2023/},
	abstract = {In this paper, we demonstrate a system for the automatic extraction and curation of crime-related information from multi-source digitally published News articles collected over a period of five years. We have leveraged the use of deep convolution recurrent neural network model to analyze crime articles to extract different crime related entities and events. The proposed methods are not restricted to detecting known crimes only but contribute actively towards maintaining an updated crime ontology. We have done experiments with a collection of 5000 crime-reporting News articles span over time, and multiple sources. The end-product of our experiments is a crime-register that contains details of crime committed across geographies and time. This register can be further utilized for analytical and reporting purposes.},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Dasgupta, Tirthankar and Dey, Lipika and Saha, Rupsa and Naskar, Abir},
	editor = {Zhao, Dongyan},
	month = aug,
	year = {2018},
	pages = {103--107},
}

@inproceedings{xiang_ersom_2015,
	address = {Lisbon, Portugal},
	title = {{ERSOM}: {A} {Structural} {Ontology} {Matching} {Approach} {Using} {Automatically} {Learned} {Entity} {Representation}},
	url = {https://aclanthology.org/D15-1289/},
	doi = {10.18653/v1/D15-1289},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xiang, Chuncheng and Jiang, Tingsong and Chang, Baobao and Sui, Zhifang},
	editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian},
	month = sep,
	year = {2015},
	pages = {2419--2429},
}

@inproceedings{pinter_predicting_2018,
	address = {Brussels, Belgium},
	title = {Predicting {Semantic} {Relations} using {Global} {Graph} {Properties}},
	url = {https://aclanthology.org/D18-1201/},
	doi = {10.18653/v1/D18-1201},
	abstract = {Semantic graphs, such as WordNet, are resources which curate natural language on two distinguishable layers. On the local level, individual relations between synsets (semantic building blocks) such as hypernymy and meronymy enhance our understanding of the words used to express their meanings. Globally, analysis of graph-theoretic properties of the entire net sheds light on the structure of human language as a whole. In this paper, we combine global and local properties of semantic graphs through the framework of Max-Margin Markov Graph Models (M3GM), a novel extension of Exponential Random Graph Model (ERGM) that scales to large multi-relational graphs. We demonstrate how such global modeling improves performance on the local task of predicting semantic relations between synsets, yielding new state-of-the-art results on the WN18RR dataset, a challenging version of WordNet link prediction in which “easy” reciprocal cases are removed. In addition, the M3GM model identifies multirelational motifs that are characteristic of well-formed lexical semantic ontologies.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Pinter, Yuval and Eisenstein, Jacob},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {1741--1751},
}

@inproceedings{shen_learning_2018,
	address = {Brussels, Belgium},
	title = {Learning {Context}-{Sensitive} {Convolutional} {Filters} for {Text} {Processing}},
	url = {https://aclanthology.org/D18-1210/},
	doi = {10.18653/v1/D18-1210},
	abstract = {Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-sensitive convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-sensitive filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-sensitive filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-sensitive filters, we further validate and rationalize the effectiveness of proposed framework.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Dinghan and Min, Martin Renqiang and Li, Yitong and Carin, Lawrence},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {1839--1848},
}

@inproceedings{ren_towards_2018,
	address = {Brussels, Belgium},
	title = {Towards {Universal} {Dialogue} {State} {Tracking}},
	url = {https://aclanthology.org/D18-1299/},
	doi = {10.18653/v1/D18-1299},
	abstract = {Dialogue state tracker is the core part of a spoken dialogue system. It estimates the beliefs of possible user's goals at every dialogue turn. However, for most current approaches, it's difficult to scale to large dialogue domains. They have one or more of following limitations: (a) Some models don't work in the situation where slot values in ontology changes dynamically; (b) The number of model parameters is proportional to the number of slots; (c) Some models extract features based on hand-crafted lexicons. To tackle these challenges, we propose StateNet, a universal dialogue state tracker. It is independent of the number of values, shares parameters across all slots, and uses pre-trained word vectors instead of explicit semantic dictionaries. Our experiments on two datasets show that our approach not only overcomes the limitations, but also significantly outperforms the performance of state-of-the-art approaches.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ren, Liliang and Xie, Kaige and Chen, Lu and Yu, Kai},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {2780--2786},
}

@inproceedings{wang_teacher-student_2018,
	address = {Brussels, Belgium},
	title = {A {Teacher}-{Student} {Framework} for {Maintainable} {Dialog} {Manager}},
	url = {https://aclanthology.org/D18-1415/},
	doi = {10.18653/v1/D18-1415},
	abstract = {Reinforcement learning (RL) is an attractive solution for task-oriented dialog systems. However, extending RL-based systems to handle new intents and slots requires a system redesign. The high maintenance cost makes it difficult to apply RL methods to practical systems on a large scale. To address this issue, we propose a practical teacher-student framework to extend RL-based dialog systems without retraining from scratch. Specifically, the “student” is an extended dialog manager based on a new ontology, and the “teacher” is existing resources used for guiding the learning process of the “student”. By specifying constraints held in the new dialog manager, we transfer knowledge of the “teacher” to the “student” without additional resources. Experiments show that the performance of the extended system is comparable to the system trained from scratch. More importantly, the proposed framework makes no assumption about the unsupported intents and slots, which makes it possible to improve RL-based systems incrementally.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Weikang and Zhang, Jiajun and Zhang, Han and Hwang, Mei-Yuh and Zong, Chengqing and Li, Zhifei},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {3803--3812},
}

@inproceedings{tanveer_syntaviz_2018,
	address = {Brussels, Belgium},
	title = {{SyntaViz}: {Visualizing} {Voice} {Queries} through a {Syntax}-{Driven} {Hierarchical} {Ontology}},
	url = {https://aclanthology.org/D18-2001/},
	doi = {10.18653/v1/D18-2001},
	abstract = {This paper describes SyntaViz, a visualization interface specifically designed for analyzing natural-language queries that were created by users of a voice-enabled product. SyntaViz provides a platform for browsing the ontology of user queries from a syntax-driven perspective, providing quick access to high-impact failure points of the existing intent understanding system and evidence for data-driven decisions in the development cycle. A case study on Xfinity X1 (a voice-enabled entertainment platform from Comcast) reveals that SyntaViz helps developers identify multiple action items in a short amount of time without any special training. SyntaViz has been open-sourced for the benefit of the community.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Tanveer, Md Iftekhar and Ture, Ferhan},
	editor = {Blanco, Eduardo and Lu, Wei},
	month = nov,
	year = {2018},
	pages = {1--6},
}

@inproceedings{labutov_lia_2018,
	address = {Brussels, Belgium},
	title = {{LIA}: {A} {Natural} {Language} {Programmable} {Personal} {Assistant}},
	url = {https://aclanthology.org/D18-2025/},
	doi = {10.18653/v1/D18-2025},
	abstract = {We present LIA, an intelligent personal assistant that can be programmed using natural language. Our system demonstrates multiple competencies towards learning from human-like interactions. These include the ability to be taught reusable conditional procedures, the ability to be taught new knowledge about the world (concepts in an ontology) and the ability to be taught how to ground that knowledge in a set of sensors and effectors. Building such a system highlights design questions regarding the overall architecture that such an agent should have, as well as questions about parsing and grounding language in situational contexts. We outline key properties of this architecture, and demonstrate a prototype that embodies them in the form of a personal assistant on an Android device.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Labutov, Igor and Srivastava, Shashank and Mitchell, Tom},
	editor = {Blanco, Eduardo and Lu, Wei},
	month = nov,
	year = {2018},
	pages = {145--150},
}

@inproceedings{gupta_care_2019,
	address = {Hong Kong, China},
	title = {{CaRe}: {Open} {Knowledge} {Graph} {Embeddings}},
	url = {https://aclanthology.org/D19-1036/},
	doi = {10.18653/v1/D19-1036},
	abstract = {Open Information Extraction (OpenIE) methods are effective at extracting (noun phrase, relation phrase, noun phrase) triples from text, e.g., (Barack Obama, took birth in, Honolulu). Organization of such triples in the form of a graph with noun phrases (NPs) as nodes and relation phrases (RPs) as edges results in the construction of Open Knowledge Graphs (OpenKGs). In order to use such OpenKGs in downstream tasks, it is often desirable to learn embeddings of the NPs and RPs present in the graph. Even though several Knowledge Graph (KG) embedding methods have been recently proposed, all of those methods have targeted Ontological KGs, as opposed to OpenKGs. Straightforward application of existing Ontological KG embedding methods to OpenKGs is challenging, as unlike Ontological KGs, OpenKGs are not canonicalized, i.e., a real-world entity may be represented using multiple nodes in the OpenKG, with each node corresponding to a different NP referring to the entity. For example, nodes with labels Barack Obama, Obama, and President Obama may refer to the same real-world entity Barack Obama. Even though canonicalization of OpenKGs has received some attention lately, output of such methods has not been used to improve OpenKG embed- dings. We fill this gap in the paper and propose Canonicalization-infused Representations (CaRe) for OpenKGs. Through extensive experiments, we observe that CaRe enables existing models to adapt to the challenges in OpenKGs and achieve substantial improvements for the link prediction task.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Gupta, Swapnil and Kenkre, Sreyash and Talukdar, Partha},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {378--388},
}

@inproceedings{ren_scalable_2019,
	address = {Hong Kong, China},
	title = {Scalable and {Accurate} {Dialogue} {State} {Tracking} via {Hierarchical} {Sequence} {Generation}},
	url = {https://aclanthology.org/D19-1196/},
	doi = {10.18653/v1/D19-1196},
	abstract = {Existing approaches to dialogue state tracking rely on pre-defined ontologies consisting of a set of all possible slot types and values. Though such approaches exhibit promising performance on single-domain benchmarks, they suffer from computational complexity that increases proportionally to the number of pre-defined slots that need tracking. This issue becomes more severe when it comes to multi-domain dialogues which include larger numbers of slots. In this paper, we investigate how to approach DST using a generation framework without the pre-defined ontology list. Given each turn of user utterance and system response, we directly generate a sequence of belief states by applying a hierarchical encoder-decoder structure. In this way, the computational complexity of our model will be a constant regardless of the number of pre-defined slots. Experiments on both the multi-domain and the single domain dialogue state tracking dataset show that our model not only scales easily with the increasing number of pre-defined domains and slots but also reaches the state-of-the-art performance.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Ren, Liliang and Ni, Jianmo and McAuley, Julian},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {1876--1885},
}

@inproceedings{henderson_polyresponse_2019,
	address = {Hong Kong, China},
	title = {{PolyResponse}: {A} {Rank}-based {Approach} to {Task}-{Oriented} {Dialogue} with {Application} in {Restaurant} {Search} and {Booking}},
	url = {https://aclanthology.org/D19-3031/},
	doi = {10.18653/v1/D19-3031},
	abstract = {We present PolyResponse, a conversational search engine that supports task-oriented dialogue. It is a retrieval-based approach that bypasses the complex multi-component design of traditional task-oriented dialogue systems and the use of explicit semantics in the form of task-specific ontologies. The PolyResponse engine is trained on hundreds of millions of examples extracted from real conversations: it learns what responses are appropriate in different conversational contexts. It then ranks a large index of text and visual responses according to their similarity to the given context, and narrows down the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the PolyResponse engine, currently available in 8 different languages.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP}): {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Henderson, Matthew and Vulić, Ivan and Casanueva, Iñigo and Budzianowski, Paweł and Gerz, Daniela and Coope, Sam and Spithourakis, Georgios and Wen, Tsung-Hsien and Mrkšić, Nikola and Su, Pei-Hao},
	editor = {Padó, Sebastian and Huang, Ruihong},
	month = nov,
	year = {2019},
	pages = {181--186},
}

@inproceedings{khalife_scalable_2019,
	address = {Hong Kong},
	title = {Scalable graph-based method for individual named entity identification},
	url = {https://aclanthology.org/D19-5303/},
	doi = {10.18653/v1/D19-5303},
	abstract = {In this paper, we consider the named entity linking (NEL) problem. We assume a set of queries, named entities, that have to be identified within a knowledge base. This knowledge base is represented by a text database paired with a semantic graph, endowed with a classification of entities (ontology). We present state-of-the-art methods in NEL, and propose a new method for individual identification requiring few annotated data samples. We demonstrate its scalability and performance over standard datasets, for several ontology configurations. Our approach is well-motivated for integration in real systems. Indeed, recent deep learning methods, despite their capacity to improve experimental precision, require lots of parameter tuning along with large volume of annotated data.},
	booktitle = {Proceedings of the {Thirteenth} {Workshop} on {Graph}-{Based} {Methods} for {Natural} {Language} {Processing} ({TextGraphs}-13)},
	publisher = {Association for Computational Linguistics},
	author = {Khalife, Sammy and Vazirgiannis, Michalis},
	editor = {Ustalov, Dmitry and Somasundaran, Swapna and Jansen, Peter and Glavaš, Goran and Riedl, Martin and Surdeanu, Mihai and Vazirgiannis, Michalis},
	month = nov,
	year = {2019},
	pages = {17--25},
}

@inproceedings{bossy_bacteria_2019,
	address = {Hong Kong, China},
	title = {Bacteria {Biotope} at {BioNLP} {Open} {Shared} {Tasks} 2019},
	url = {https://aclanthology.org/D19-5719/},
	doi = {10.18653/v1/D19-5719},
	abstract = {This paper presents the fourth edition of the Bacteria Biotope task at BioNLP Open Shared Tasks 2019. The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and full-text excerpts, and the characterization of these entities with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology). The task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology. The paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization. We also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016.},
	booktitle = {Proceedings of the 5th {Workshop} on {BioNLP} {Open} {Shared} {Tasks}},
	publisher = {Association for Computational Linguistics},
	author = {Bossy, Robert and Deléger, Louise and Chaix, Estelle and Ba, Mouhamadou and Nédellec, Claire},
	editor = {Jin-Dong, Kim and Claire, Nédellec and Robert, Bossy and Louise, Deléger},
	month = nov,
	year = {2019},
	pages = {121--131},
}

@inproceedings{baumgartner_craft_2019,
	address = {Hong Kong, China},
	title = {{CRAFT} {Shared} {Tasks} 2019 {Overview} — {Integrated} {Structure}, {Semantics}, and {Coreference}},
	url = {https://aclanthology.org/D19-5725/},
	doi = {10.18653/v1/D19-5725},
	abstract = {As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks — dependency parse construction, coreference resolution, and ontology concept identification — over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task focuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each task, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three tasks.},
	booktitle = {Proceedings of the 5th {Workshop} on {BioNLP} {Open} {Shared} {Tasks}},
	publisher = {Association for Computational Linguistics},
	author = {Baumgartner, William and Bada, Michael and Pyysalo, Sampo and Ciosici, Manuel R. and Hailu, Negacy and Pielke-Lombardo, Harrison and Regan, Michael and Hunter, Lawrence},
	editor = {Jin-Dong, Kim and Claire, Nédellec and Robert, Bossy and Louise, Deléger},
	month = nov,
	year = {2019},
	pages = {174--184},
}

@inproceedings{nooralahzadeh_reinforcement-based_2019,
	address = {Hong Kong, China},
	title = {Reinforcement-based denoising of distantly supervised {NER} with partial annotation},
	url = {https://aclanthology.org/D19-6125/},
	doi = {10.18653/v1/D19-6125},
	abstract = {Existing named entity recognition (NER) systems rely on large amounts of human-labeled data for supervision. However, obtaining large-scale annotated data is challenging particularly in specific domains like health-care, e-commerce and so on. Given the availability of domain specific knowledge resources, (e.g., ontologies, dictionaries), distant supervision is a solution to generate automatically labeled training data to reduce human effort. The outcome of distant supervision for NER, however, is often noisy. False positive and false negative instances are the main issues that reduce performance on this kind of auto-generated data. In this paper, we explore distant supervision in a supervised setup. We adopt a technique of partial annotation to address false negative cases and implement a reinforcement learning strategy with a neural network policy to identify false positive instances. Our results establish a new state-of-the-art on four benchmark datasets taken from different domains and different languages. We then go on to show that our model reduces the amount of manually annotated data required to perform NER in a new domain.},
	booktitle = {Proceedings of the 2nd {Workshop} on {Deep} {Learning} {Approaches} for {Low}-{Resource} {NLP} ({DeepLo} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Nooralahzadeh, Farhad and Lønning, Jan Tore and Øvrelid, Lilja},
	editor = {Cherry, Colin and Durrett, Greg and Foster, George and Haffari, Reza and Khadivi, Shahram and Peng, Nanyun and Ren, Xiang and Swayamdipta, Swabha},
	month = nov,
	year = {2019},
	pages = {225--233},
}

@inproceedings{falis_ontological_2019,
	address = {Hong Kong},
	title = {Ontological attention ensembles for capturing semantic concepts in {ICD} code prediction from clinical text},
	url = {https://aclanthology.org/D19-6220/},
	doi = {10.18653/v1/D19-6220},
	abstract = {We present a semantically interpretable system for automated ICD coding of clinical text documents. Our contribution is an ontological attention mechanism which matches the structure of the ICD ontology, in which shared attention vectors are learned at each level of the hierarchy, and combined into label-dependent ensembles. Analysis of the attention heads shows that shared concepts are learned by the lowest common denominator node. This allows child nodes to focus on the differentiating concepts, leading to efficient learning and memory usage. Visualisation of the multi-level attention on the original text allows explanation of the code predictions according to the semantics of the ICD ontology. On the MIMIC-III dataset we achieve a 2.7\% absolute (11\% relative) improvement from 0.218 to 0.245 macro-F1 score compared to the previous state of the art across 3,912 codes. Finally, we analyse the labelling inconsistencies arising from different coding practices which limit performance on this task.},
	booktitle = {Proceedings of the {Tenth} {International} {Workshop} on {Health} {Text} {Mining} and {Information} {Analysis} ({LOUHI} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Falis, Matus and Pajak, Maciej and Lisowska, Aneta and Schrempf, Patrick and Deckers, Lucas and Mikhael, Shadia and Tsaftaris, Sotirios and O'Neil, Alison},
	editor = {Holderness, Eben and Jimeno Yepes, Antonio and Lavelli, Alberto and Minard, Anne-Lyse and Pustejovsky, James and Rinaldi, Fabio},
	month = nov,
	year = {2019},
	pages = {168--177},
}

@inproceedings{sadoun_peuplement_2012,
	address = {Grenoble, France},
	title = {Peuplement d'une ontologie modélisant le fonctionnement d'un environnement intelligent guidée par l'extraction d'instances de relations ({Population} of an {Ontology} {Modeling} the {Behavior} of an {Intelligent} {Environment} {Guided} by {Instance} {Relation} {Extractions}) [in {French}]},
	url = {https://aclanthology.org/F12-3022/},
	booktitle = {Proceedings of the {Joint} {Conference} {JEP}-{TALN}-{RECITAL} 2012, volume 3: {RECITAL}},
	publisher = {ATALA/AFCP},
	author = {Sadoun, Driss},
	editor = {Molina Mejia, Jorge Mauricio and Schwab, Didier and Sérasset, Gilles},
	month = jun,
	year = {2012},
	pages = {281--294},
}

@inproceedings{zheng_hybrid_2005,
	title = {A {Hybrid} {Chinese} {Language} {Model} based on a {Combination} of {Ontology} with {Statistical} {Method}},
	url = {https://aclanthology.org/I05-2003/},
	booktitle = {Companion {Volume} to the {Proceedings} of {Conference} including {Posters}/{Demos} and tutorial abstracts},
	author = {Zheng, Dequan and Zhao, Tiejun and Li, Sheng and Yu, Hao},
	year = {2005},
}

@inproceedings{haralambous_semantic_2011,
	address = {Chiang Mai, Thailand},
	title = {A {Semantic} {Relatedness} {Measure} {Based} on {Combined} {Encyclopedic}, {Ontological} and {Collocational} {Knowledge}},
	url = {https://aclanthology.org/I11-1161/},
	booktitle = {Proceedings of 5th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Asian Federation of Natural Language Processing},
	author = {Haralambous, Yannis and Klyuev, Vitaly},
	editor = {Wang, Haifeng and Yarowsky, David},
	month = nov,
	year = {2011},
	pages = {1397--1402},
}

@inproceedings{mahgoub_simvecs_2019,
	address = {Hong Kong, China},
	title = {{SimVecs}: {Similarity}-{Based} {Vectors} for {Utterance} {Representation} in {Conversational} {AI} {Systems}},
	url = {https://aclanthology.org/K19-1066/},
	doi = {10.18653/v1/K19-1066},
	abstract = {Conversational AI systems are gaining a lot of attention recently in both industrial and scientific domains, providing a natural way of interaction between customers and adaptive intelligent systems. A key requirement in these systems is the ability to understand the user's intent and provide adequate responses to them. One of the greatest challenges of language understanding (LU) services is efficient utterance (sentence) representation in vector space, which is an essential step for most ML tasks. In this paper, we propose a novel approach for generating vector space representations of utterances using pair-wise similarity metrics. The proposed approach uses only a few corpora to tune the weights of the similarity metric without relying on external general purpose ontologies. Our experiments confirm that the generated vectors can improve the performance of LU services in unsupervised, semi-supervised and supervised learning tasks.},
	booktitle = {Proceedings of the 23rd {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Mahgoub, Ashraf and Shahin, Youssef and Mansour, Riham and Bagchi, Saurabh},
	editor = {Bansal, Mohit and Villavicencio, Aline},
	month = nov,
	year = {2019},
	pages = {708--717},
}

@inproceedings{kiyota_domain_2006,
	address = {Genoa, Italy},
	title = {A {Domain} {Ontology} {Production} {Tool} {Kit} {Based} on {Automatically} {Constructed} {Case} {Frames}},
	url = {https://aclanthology.org/L06-1135/},
	abstract = {This paper proposes a tool kit to produce a domain ontology for text mining, based on case frames automatically constructed from a raw corpus of a specific domain. Since case frames are strongly related to implicit facts hidden in large domain-specific corpora, we can say that case frames are a promising device for text mining. The aim of the tool kit is to enable automatic analysis of event reports, from which implicit factors of the events are to be extracted. The tool kit enables us to produce a domain ontology by iterating associative retrieval of case frames and manual refinement. In this study, the tool kit is applied to the Japan Airlines pilot report collection, and a domain ontology of contributing factors in the civil aviation domain is experimentally produced. A lot of interesting examples are found in the ontology. In addition, a brief examination of the production process shows the efficiency of the tool kit.},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'06)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Kiyota, Yoji and Nakagawa, Hiroshi},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
	month = may,
	year = {2006},
}

@inproceedings{declerck_generic_2006,
	address = {Genoa, Italy},
	title = {Generic {NLP} {Tools} for {Supporting} {Shallow} {Ontology} {Building}},
	url = {https://aclanthology.org/L06-1216/},
	abstract = {In this paper we present on-going investigations on how complex syntactic annotation, combined with linguistic semantics, can possibly help in supporting the semi-automatic building of (shallow) ontologies from text by proposing an automated extraction of (possibly underspecified) semantic relations from linguistically annotated text.},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'06)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Declerck, Thierry and Vela, Mihaela},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
	month = may,
	year = {2006},
}

@inproceedings{tablan_user-friendly_2006,
	address = {Genoa, Italy},
	title = {User-friendly ontology authoring using a controlled language},
	url = {https://aclanthology.org/L06-1324/},
	abstract = {In recent years, following the rapid development in the Semantic Web and Knowledge Management research, ontologies have become more in demand in Natural Language Processing. An increasing number of systems use ontologies either internally, for modelling the domain of the application, or as data structures that hold the output resulting from the work of the system, in the form of knowledge bases. While there are many ontology editing tools aimed at expert users, there are very few which are accessible to users wishing to create simple structures without delving into the intricacies of knowledge representation languages. The approach described in this paper allows users to create and edit ontologies simply by using a restricted version of the English language. The controlled language described within is based on an open vocabulary and a restricted set of grammatical constructs. Sentences written in this language unambiguously map into a number of knowledge representation formats including OWL and RDF-S to allow round-trip ontology management.},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'06)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Tablan, Valentin and Polajnar, Tamara and Cunningham, Hamish and Bontcheva, Kalina},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
	month = may,
	year = {2006},
}

@inproceedings{suktarachan_workbench_2008,
	address = {Marrakech, Morocco},
	title = {Workbench with {Authoring} {Tools} for {Collaborative} {Multi}-lingual {Ontological} {Knowledge} {Construction} and {Maintenance}},
	url = {https://aclanthology.org/L08-1271/},
	abstract = {An ontological knowledge management system requires dynamic and encapsulating operation in order to share knowledge among communities. The key to success of knowledge sharing in the field of agriculture is using and sharing agreed terminologies such as ontological knowledge especially in multiple languages. This paper proposes a workbench with three authoring tools for collaborative multilingual ontological knowledge construction and maintenance, in order to add value and support communities in the field of food and agriculture. The framework consists of the multilingual ontological knowledge construction and maintenance workbench platform, which composes of ontological knowledge management and user management, and three ontological knowledge authoring tools. The authoring tools used are two ontology extraction tools, ATOM and KULEX, and one ontology integration tool.},
	booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'08)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Suktarachan, Mukda and Thamvijit, Dussadee and Noikongka, Daoyos and Yongyuth, Panita and Mahasarakham, Puwarat Pavaputanont Na and Kawtrakul, Asanee and Kawtrakul, Asanee and Sini, Margherita},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios and Tapias, Daniel},
	month = may,
	year = {2008},
}

@inproceedings{davis_linguistically_2008,
	address = {Marrakech, Morocco},
	title = {Linguistically {Light} {Lexical} {Extensions} for {Ontologies}},
	url = {https://aclanthology.org/L08-1558/},
	abstract = {The identification of class instances within unstructured text for either the purposes of Ontology population or semantic annotation are usually limited to term mentions of Proper Noun and Personal Noun or fixed Key Phrases within Text Analytics or Ontology based Information Extraction(OBIE) applications. These systems do not generalize to cope with compound nominal classes of multi word expressions. Computational Linguistics approaches involving deep analysis tend to suffer from idiomaticity and overgeneration problems while the shallower words with spaces approach frequently employed in Information Extraction(IE) and Industrial Text Analytics systems lacks flexibility and is prone to lexical proliferation. We outline a representation for encoding light linguistic features of Compound Nominal term mentions of Concepts within an Ontology as well as a lightweight semantic annotator which complies the above linguistic information into efficient Dictionary formats to drive large scale identification and semantic annotation of the aforementioned concepts.},
	booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'08)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Davis, Brian and Handschuh, Siegfried and Troussov, Alexander and Judge, John and Sogrin, Mikhail},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios and Tapias, Daniel},
	month = may,
	year = {2008},
}

@inproceedings{monachesi_socially_2010,
	address = {Valletta, Malta},
	title = {Socially {Driven} {Ontology} {Enrichment} for {eLearning}},
	url = {https://aclanthology.org/L10-1570/},
	abstract = {One of the objectives of the Language Technologies for Life-Long Learning (LTfLL) project, is to develop a knowledge sharing system that connects learners to resources and learners to other learners. To this end, we complement the formal knowledge represented by existing domain ontologies with the informal knowledge emerging from social tagging. More specifically, we crawl data from social media applications such as Delicious, Slideshare and YouTube. Similarity measures are employed to select possible lexicalizations of concepts that are related to the ones present in the given ontology and which are assumed to be socially relevant with respect to the input lexicalisation. In order to identify the appropriate relationships which exist between the extracted related terms and the existing domain ontology, we employ several heuristics that rely on the use of a large background knowledge base, such as DBpedia. An evaluation of the resulting ontology has been carried out. The methodology proposed allows for an appropriate enrichment process and produces a complementary vocabulary to that of a domain expert.},
	booktitle = {Proceedings of the {Seventh} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'10)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Monachesi, Paola and Markus, Thomas},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios and Rosner, Mike and Tapias, Daniel},
	month = may,
	year = {2010},
}

@inproceedings{jauhar_ontologically_2015,
	address = {Denver, Colorado},
	title = {Ontologically {Grounded} {Multi}-sense {Representation} {Learning} for {Semantic} {Vector} {Space} {Models}},
	url = {https://aclanthology.org/N15-1070/},
	doi = {10.3115/v1/N15-1070},
	booktitle = {Proceedings of the 2015 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Jauhar, Sujay Kumar and Dyer, Chris and Hovy, Eduard},
	editor = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
	month = may,
	year = {2015},
	pages = {683--693},
}

@inproceedings{kolyvakis_deepalignment_2018,
	address = {New Orleans, Louisiana},
	title = {{DeepAlignment}: {Unsupervised} {Ontology} {Matching} with {Refined} {Word} {Vectors}},
	url = {https://aclanthology.org/N18-1072/},
	doi = {10.18653/v1/N18-1072},
	abstract = {Ontologies compartmentalize types and relations in a target domain and provide the semantic backbone needed for a plethora of practical applications. Very often different ontologies are developed independently for the same domain. Such “parallel” ontologies raise the need for a process that will establish alignments between their entities in order to unify and extend the existing knowledge. In this work, we present a novel entity alignment method which we dub DeepAlignment. DeepAlignment refines pre-trained word vectors aiming at deriving ontological entity descriptions which are tailored to the ontology matching task. The absence of explicit information relevant to the ontology matching task during the refinement process makes DeepAlignment completely unsupervised. We empirically evaluate our method using standard ontology matching benchmarks. We present significant performance improvements over the current state-of-the-art, demonstrating the advantages that representation learning techniques bring to ontology matching.},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Kolyvakis, Prodromos and Kalousis, Alexandros and Kiritsis, Dimitris},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {787--798},
}

@inproceedings{kollar_alexa_2018,
	address = {New Orleans - Louisiana},
	title = {The {Alexa} {Meaning} {Representation} {Language}},
	url = {https://aclanthology.org/N18-3022/},
	doi = {10.18653/v1/N18-3022},
	abstract = {This paper introduces a meaning representation for spoken language understanding. The Alexa meaning representation language (AMRL), unlike previous approaches, which factor spoken utterances into domains, provides a common representation for how people communicate in spoken language. AMRL is a rooted graph, links to a large-scale ontology, supports cross-domain queries, fine-grained types, complex utterances and composition. A spoken language dataset has been collected for Alexa, which contains {\textbackslash}ensuremath{\textbackslash}sim20k examples across eight domains. A version of this meaning representation was released to developers at a trade show in 2016.},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 3 ({Industry} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Kollar, Thomas and Berry, Danielle and Stuart, Lauren and Owczarzak, Karolina and Chung, Tagyoung and Mathias, Lambert and Kayser, Michael and Snow, Bradford and Matsoukas, Spyros},
	editor = {Bangalore, Srinivas and Chu-Carroll, Jennifer and Li, Yunyao},
	month = jun,
	year = {2018},
	pages = {177--184},
}

@inproceedings{li_biomedical_2019,
	address = {Minneapolis, Minnesota},
	title = {Biomedical {Event} {Extraction} based on {Knowledge}-driven {Tree}-{LSTM}},
	url = {https://aclanthology.org/N19-1145/},
	doi = {10.18653/v1/N19-1145},
	abstract = {Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features: (1) dependency structures to capture wide contexts; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Diya and Huang, Lifu and Ji, Heng and Han, Jiawei},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {1421--1430},
}

@inproceedings{prokhorov_generating_2019,
	address = {Minneapolis, Minnesota},
	title = {Generating {Knowledge} {Graph} {Paths} from {Textual} {Definitions} using {Sequence}-to-{Sequence} {Models}},
	url = {https://aclanthology.org/N19-1196/},
	doi = {10.18653/v1/N19-1196},
	abstract = {We present a novel method for mapping unrestricted text to knowledge graph entities by framing the task as a sequence-to-sequence problem. Specifically, given the encoded state of an input text, our decoder directly predicts paths in the knowledge graph, starting from the root and ending at the the target node following hypernym-hyponym relationships. In this way, and in contrast to other text-to-entity mapping systems, our model outputs hierarchically structured predictions that are fully interpretable in the context of the underlying ontology, in an end-to-end manner. We present a proof-of-concept experiment with encouraging results, comparable to those of state-of-the-art systems.},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Prokhorov, Victor and Pilehvar, Mohammad Taher and Collier, Nigel},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {1968--1976},
}

@inproceedings{lockard_openceres_2019,
	address = {Minneapolis, Minnesota},
	title = {{OpenCeres}: {When} {Open} {Information} {Extraction} {Meets} the {Semi}-{Structured} {Web}},
	url = {https://aclanthology.org/N19-1309/},
	doi = {10.18653/v1/N19-1309},
	abstract = {Open Information Extraction (OpenIE), the problem of harvesting triples from natural language text whose predicate relations are not aligned to any pre-defined ontology, has been a popular subject of research for the last decade. However, this research has largely ignored the vast quantity of facts available in semi-structured webpages. In this paper, we define the problem of OpenIE from semi-structured websites to extract such facts, and present an approach for solving it. We also introduce a labeled evaluation dataset to motivate research in this area. Given a semi-structured website and a set of seed facts for some relations existing on its pages, we employ a semi-supervised label propagation technique to automatically create training data for the relations present on the site. We then use this training data to learn a classifier for relation extraction. Experimental results of this method on our new benchmark dataset obtained a precision of over 70\%. A larger scale extraction experiment on 31 websites in the movie vertical resulted in the extraction of over 2 million triples.},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lockard, Colin and Shiralkar, Prashant and Dong, Xin Luna},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {3047--3056},
}

@inproceedings{murty_hierarchical_2018,
	address = {Melbourne, Australia},
	title = {Hierarchical {Losses} and {New} {Resources} for {Fine}-grained {Entity} {Typing} and {Linking}},
	url = {https://aclanthology.org/P18-1010/},
	doi = {10.18653/v1/P18-1010},
	abstract = {Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: \textit{MedMentions}, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and \textit{TypeNet}, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Murty, Shikhar and Verga, Patrick and Vilnis, Luke and Radovanovic, Irena and McCallum, Andrew},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	pages = {97--109},
}

@inproceedings{huang_zero-shot_2018,
	address = {Melbourne, Australia},
	title = {Zero-{Shot} {Transfer} {Learning} for {Event} {Extraction}},
	url = {https://aclanthology.org/P18-1201/},
	doi = {10.18653/v1/P18-1201},
	abstract = {Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Lifu and Ji, Heng and Cho, Kyunghyun and Dagan, Ido and Riedel, Sebastian and Voss, Clare},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	pages = {2160--2170},
}

@inproceedings{lu_object-oriented_2018,
	address = {Melbourne, Australia},
	title = {Object-oriented {Neural} {Programming} ({OONP}) for {Document} {Understanding}},
	url = {https://aclanthology.org/P18-1253/},
	doi = {10.18653/v1/P18-1253},
	abstract = {We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and builds and updates an intermediate ontology during the process to summarize its partial understanding of the text. OONP supports a big variety of forms (both symbolic and differentiable) for representing the state and the document, and a rich family of operations to compose the representation. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL), reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lu, Zhengdong and Liu, Xianggen and Cui, Haotian and Yan, Yukun and Zheng, Daqi},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	pages = {2717--2726},
}

@inproceedings{ramadan_large-scale_2018,
	address = {Melbourne, Australia},
	title = {Large-{Scale} {Multi}-{Domain} {Belief} {Tracking} with {Knowledge} {Sharing}},
	url = {https://aclanthology.org/P18-2069/},
	doi = {10.18653/v1/P18-2069},
	abstract = {Robust dialogue belief tracking is a key component in maintaining good quality dialogue systems. The tasks that dialogue systems are trying to solve are becoming increasingly complex, requiring scalability to multi-domain, semantically rich dialogues. However, most current approaches have difficulty scaling up with domains because of the dependency of the model parameters on the dialogue ontology. In this paper, a novel approach is introduced that fully utilizes semantic similarity between dialogue utterances and the ontology terms, allowing the information to be shared across domains. The evaluation is performed on a recently collected multi-domain dialogues dataset, one order of magnitude larger than currently available corpora. Our model demonstrates great capability in handling multi-domain dialogues, simultaneously outperforming existing state-of-the-art models in single-domain dialogue tracking tasks.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ramadan, Osman and Budzianowski, Paweł and Gašić, Milica},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	pages = {432--437},
}

@inproceedings{hartung_santo_2018,
	address = {Melbourne, Australia},
	title = {{SANTO}: {A} {Web}-based {Annotation} {Tool} for {Ontology}-driven {Slot} {Filling}},
	url = {https://aclanthology.org/P18-4012/},
	doi = {10.18653/v1/P18-4012},
	abstract = {Supervised machine learning algorithms require training data whose generation for complex relation extraction tasks tends to be difficult. Being optimized for relation extraction at sentence level, many annotation tools lack in facilitating the annotation of relational structures that are widely spread across the text. This leads to non-intuitive and cumbersome visualizations, making the annotation process unnecessarily time-consuming. We propose SANTO, an easy-to-use, domain-adaptive annotation tool specialized for complex slot filling tasks which may involve problems of cardinality and referential grounding. The web-based architecture enables fast and clearly structured annotation for multiple users in parallel. Relational structures are formulated as templates following the conceptualization of an underlying ontology. Further, import and export procedures of standard formats enable interoperability with external sources and tools.},
	booktitle = {Proceedings of {ACL} 2018, {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Hartung, Matthias and ter Horst, Hendrik and Grimm, Frank and Diekmann, Tim and Klinger, Roman and Cimiano, Philipp},
	editor = {Liu, Fei and Solorio, Thamar},
	month = jul,
	year = {2018},
	pages = {68--73},
}

@inproceedings{shen_web-scale_2018,
	address = {Melbourne, Australia},
	title = {A {Web}-scale system for scientific knowledge exploration},
	url = {https://aclanthology.org/P18-4015/},
	doi = {10.18653/v1/P18-4015},
	abstract = {To enable efficient exploration of Web-scale scientific knowledge, it is necessary to organize scientific publications into a hierarchical concept structure. In this work, we present a large-scale system to (1) identify hundreds of thousands of scientific concepts, (2) tag these identified concepts to hundreds of millions of scientific publications by leveraging both text and graph structure, and (3) build a six-level concept hierarchy with a subsumption-based model. The system builds the most comprehensive cross-domain scientific concept ontology published to date, with more than 200 thousand concepts and over one million relationships.},
	booktitle = {Proceedings of {ACL} 2018, {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Zhihong and Ma, Hao and Wang, Kuansan},
	editor = {Liu, Fei and Solorio, Thamar},
	month = jul,
	year = {2018},
	pages = {87--92},
}

@inproceedings{wu_transferable_2019,
	address = {Florence, Italy},
	title = {Transferable {Multi}-{Domain} {State} {Generator} for {Task}-{Oriented} {Dialogue} {Systems}},
	url = {https://aclanthology.org/P19-1078/},
	doi = {10.18653/v1/P19-1078},
	abstract = {Over-dependence on domain ontology and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short when tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art 48.62\% joint goal accuracy for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58\% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Chien-Sheng and Madotto, Andrea and Hosseini-Asl, Ehsan and Xiong, Caiming and Socher, Richard and Fung, Pascale},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {808--819},
}

@inproceedings{lee_sumbt_2019,
	address = {Florence, Italy},
	title = {{SUMBT}: {Slot}-{Utterance} {Matching} for {Universal} and {Scalable} {Belief} {Tracking}},
	url = {https://aclanthology.org/P19-1546/},
	doi = {10.18653/v1/P19-1546},
	abstract = {In goal-oriented dialog systems, belief trackers estimate the probability distribution of slot-values at every dialog turn. Previous neural approaches have modeled domain- and slot-dependent belief trackers, and have difficulty in adding new slot-values, resulting in lack of flexibility of domain ontology configurations. In this paper, we propose a new approach to universal and scalable belief tracker, called slot-utterance matching belief tracker (SUMBT). The model learns the relations between domain-slot-types and slot-values appearing in utterances through attention mechanisms based on contextual semantic vectors. Furthermore, the model predicts slot-value labels in a non-parametric way. From our experiments on two dialog corpora, WOZ 2.0 and MultiWOZ, the proposed model showed performance improvement in comparison with slot-dependent methods and achieved the state-of-the-art joint accuracy.},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Hwaran and Lee, Jinsik and Kim, Tae-Yoon},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {5478--5483},
}

@inproceedings{bebeshina-clairet_using_2019,
	address = {Varna, Bulgaria},
	title = {Using a {Lexical} {Semantic} {Network} for the {Ontology} {Building}},
	url = {https://aclanthology.org/R19-1012/},
	doi = {10.26615/978-954-452-056-4_012},
	abstract = {Building multilingual ontologies is a hard task as ontologies are often data-rich resources. We introduce an approach which allows exploiting structured lexical semantic knowledge for the ontology building. Given a multilingual lexical semantic (non ontological) resource and an ontology model, it allows mining relevant semantic knowledge and make the ontology building and enhancement process faster.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2019)},
	publisher = {INCOMA Ltd.},
	author = {Bebeshina-Clairet, Nadia and Despres, Sylvie and Lafourcade, Mathieu},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2019},
	pages = {92--101},
}

@inproceedings{estevez-velarde_demo_2019,
	address = {Varna, Bulgaria},
	title = {Demo {Application} for {LETO}: {Learning} {Engine} {Through} {Ontologies}},
	url = {https://aclanthology.org/R19-1032/},
	doi = {10.26615/978-954-452-056-4_032},
	abstract = {The massive amount of multi-formatted information available on the Web necessitates the design of software systems that leverage this information to obtain knowledge that is valid and useful. The main challenge is to discover relevant information and continuously update, enrich and integrate knowledge from various sources of structured and unstructured data. This paper presents the Learning Engine Through Ontologies(LETO) framework, an architecture for the continuous and incremental discovery of knowledge from multiple sources of unstructured and structured data. We justify the main design decision behind LETO's architecture and evaluate the framework's feasibility using the Internet Movie Data Base(IMDB) and Twitter as a practical application.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2019)},
	publisher = {INCOMA Ltd.},
	author = {Estevez-Velarde, Suilan and Montoyo, Andrés and Almeida-Cruz, Yudivian and Gutiérrez, Yoan and Piad-Morffis, Alejandro and Muñoz, Rafael},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2019},
	pages = {276--284},
}

@inproceedings{galitsky_discourse-based_2019,
	address = {Varna, Bulgaria},
	title = {Discourse-{Based} {Approach} to {Involvement} of {Background} {Knowledge} for {Question} {Answering}},
	url = {https://aclanthology.org/R19-1044/},
	doi = {10.26615/978-954-452-056-4_044},
	abstract = {We introduce a concept of a virtual discourse tree to improve question answering (Q/A) recall for complex, multi-sentence questions. Augmenting the discourse tree of an answer with tree fragments obtained from text corpora playing the role of ontology, we obtain on the fly a canonical discourse representation of this answer that is independent of the thought structure of a given author. This mechanism is critical for finding an answer that is not only relevant in terms of questions entities but also in terms of inter-relations between these entities in an answer and its style. We evaluate the Q/A system enabled with virtual discourse trees and observe a substantial increase of performance answering complex questions such as Yahoo! Answers and www.2carpros.com.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2019)},
	publisher = {INCOMA Ltd.},
	author = {Galitsky, Boris and Ilvovsky, Dmitry},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2019},
	pages = {373--381},
}

@inproceedings{nicula_building_2019,
	address = {Varna, Bulgaria},
	title = {Building a {Comprehensive} {Romanian} {Knowledge} {Base} for {Drug} {Administration}},
	url = {https://aclanthology.org/R19-1096/},
	doi = {10.26615/978-954-452-056-4_096},
	abstract = {Information on drug administration is obtained traditionally from doctors and pharmacists, as well as leaflets which provide in most cases cumbersome and hard-to-follow details. Thus, the need for medical knowledge bases emerges to provide access to concrete and well-structured information which can play an important role in informing patients. This paper introduces a Romanian medical knowledge base focused on drug-drug interactions, on representing relevant drug information, and on symptom-disease relations. The knowledge base was created by extracting and transforming information using Natural Language Processing techniques from both structured and unstructured sources, together with manual annotations. The resulting Romanian ontologies are aligned with larger English medical ontologies. Our knowledge base supports queries regarding drugs (e.g., active ingredients, concentration, expiration date), drug-drug interaction, symptom-disease relations, as well as drug-symptom relations.},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2019)},
	publisher = {INCOMA Ltd.},
	author = {Nicula, Bogdan and Dascalu, Mihai and Sîrbu, Maria-Dorinela and Trăușan-Matu, Ștefan and Nuță, Alexandru},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2019},
	pages = {829--836},
}

@inproceedings{poostchi_cluster_2018,
	address = {Dunedin, New Zealand},
	title = {Cluster {Labeling} by {Word} {Embeddings} and {WordNet}'s {Hypernymy}},
	url = {https://aclanthology.org/U18-1008/},
	abstract = {Cluster labeling is the assignment of representative labels to clusters obtained from the organization of a document collection. Once assigned, the labels can play an important role in applications such as navigation, search and document classification. However, finding appropriately descriptive labels is still a challenging task. In this paper, we propose various approaches for assigning labels to word clusters by leveraging word embeddings and the synonymity and hypernymy relations in the WordNet lexical ontology. Experiments carried out using the WebAP document dataset have shown that one of the approaches stand out in the comparison and is capable of selecting labels that are reasonably aligned with those chosen by a pool of four human annotators.},
	booktitle = {Proceedings of the {Australasian} {Language} {Technology} {Association} {Workshop} 2018},
	author = {Poostchi, Hanieh and Piccardi, Massimo},
	editor = {Kim, Sunghwan Mac and Zhang, Xiuzhen (Jenny)},
	month = dec,
	year = {2018},
	pages = {66--70},
}

@inproceedings{wu_text_2003,
	address = {Sapporo, Japan},
	title = {Text {Categorization} {Using} {Automatically} {Acquired} {Domain} {Ontology}},
	url = {https://aclanthology.org/W03-1118/},
	doi = {10.3115/1118935.1118953},
	booktitle = {Proceedings of the {Sixth} {International} {Workshop} on {Information} {Retrieval} with {Asian} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Shih-Hung and Tsai, Tzong-Han and Hsu, Wen-Lian},
	month = jul,
	year = {2003},
	pages = {138--145},
}

@inproceedings{mani_automatically_2004,
	address = {Geneva, Switzerland},
	title = {Automatically {Inducing} {Ontologies} from {Corpora}},
	url = {https://aclanthology.org/W04-1806/},
	booktitle = {Proceedings of {CompuTerm} 2004: 3rd {International} {Workshop} on {Computational} {Terminology}},
	publisher = {COLING},
	author = {Mani, Inderjeet and Samuel, Ken and Concepcion, Kris and Vogel, David},
	month = aug,
	year = {2004},
	pages = {47--54},
}

@inproceedings{fyshe_term_2006,
	address = {New York, New York},
	title = {Term {Generalization} and {Synonym} {Resolution} for {Biological} {Abstracts}: {Using} the {Gene} {Ontology} for {Subcellular} {Localization} {Prediction}},
	url = {https://aclanthology.org/W06-3303/},
	booktitle = {Proceedings of the {HLT}-{NAACL} {BioNLP} {Workshop} on {Linking} {Natural} {Language} and {Biology}},
	publisher = {Association for Computational Linguistics},
	author = {Fyshe, Alona and Szafron, Duane},
	editor = {Verspoor, Karin and Cohen, Kevin Bretonnel and Goertzel, Ben and Mani, Inderjeet},
	month = jun,
	year = {2006},
	pages = {17--24},
}

@inproceedings{galanis_generating_2007,
	address = {Saarbrücken, Germany},
	title = {Generating {Multilingual} {Descriptions} from {Linguistically} {Annotated} {OWL} {Ontologies}: the {NaturalOWL} {System}},
	url = {https://aclanthology.org/W07-2322/},
	booktitle = {Proceedings of the {Eleventh} {European} {Workshop} on {Natural} {Language} {Generation} ({ENLG} 07)},
	publisher = {DFKI GmbH},
	author = {Galanis, Dimitrios and Androutsopoulos, Ion},
	editor = {Busemann, Stephan},
	month = jun,
	year = {2007},
	pages = {143--146},
}

@inproceedings{conway_developing_2010,
	address = {Beijing, China},
	title = {Developing a {Biosurveillance} {Application} {Ontology} for {Influenza}-{Like}-{Illness}},
	url = {https://aclanthology.org/W10-3307/},
	booktitle = {Proceedings of the 6th {Workshop} on {Ontologies} and {Lexical} {Resources}},
	publisher = {Coling 2010 Organizing Committee},
	author = {Conway, Mike and Dowling, John and Chapman, Wendy},
	editor = {Oltramari, Alessandro and Vossen, Piek and Lu, Qin},
	month = aug,
	year = {2010},
	pages = {58--66},
}

@inproceedings{bouayad-agha_content_2011,
	address = {Nancy, France},
	title = {Content selection from an ontology-based knowledge base for the generation of football summaries},
	url = {https://aclanthology.org/W11-2810/},
	booktitle = {Proceedings of the 13th {European} {Workshop} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Bouayad-Agha, Nadjet and Casamayor, Gerard and Wanner, Leo},
	editor = {Gardent, Claire and Striegnitz, Kristina},
	month = sep,
	year = {2011},
	pages = {72--81},
}

@inproceedings{movshovitz-attias_bootstrapping_2012,
	address = {Montréal, Canada},
	title = {Bootstrapping {Biomedical} {Ontologies} for {Scientific} {Text} using {NELL}},
	url = {https://aclanthology.org/W12-2402/},
	booktitle = {{BioNLP}: {Proceedings} of the 2012 {Workshop} on {Biomedical} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Movshovitz-Attias, Dana and Cohen, William W.},
	editor = {Cohen, Kevin B. and Demner-Fushman, Dina and Ananiadou, Sophia and Webber, Bonnie and Tsujii, Jun'ichi and Pestian, John},
	month = jun,
	year = {2012},
	pages = {11--19},
}

@inproceedings{allen_automatically_2013,
	address = {Potsdam, Germany},
	title = {Automatically {Deriving} {Event} {Ontologies} for a {CommonSense} {Knowledge} {Base}},
	url = {https://aclanthology.org/W13-0103/},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Computational} {Semantics} ({IWCS} 2013) – {Long} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Allen, James and de Beaumont, Will and Galescu, Lucian and Orfan, Jansen and Swift, Mary and Teng, Choh Man},
	editor = {Koller, Alexander and Erk, Katrin},
	month = mar,
	year = {2013},
	pages = {23--34},
}

@inproceedings{genc_building_2013,
	address = {Sofia, Bulgaria},
	title = {Building {Ontologies} from {Collaborative} {Knowledge} {Bases} to {Search} and {Interpret} {Multilingual} {Corpora}},
	url = {https://aclanthology.org/W13-2511/},
	booktitle = {Proceedings of the {Sixth} {Workshop} on {Building} and {Using} {Comparable} {Corpora}},
	publisher = {Association for Computational Linguistics},
	author = {Genc, Yegin and Lennon, Elizabeth and Mason, Winter and Nickerson, Jeffrey},
	editor = {Sharoff, Serge and Zweigenbaum, Pierre and Rapp, Reinhard},
	month = aug,
	year = {2013},
	pages = {87--94},
}

@inproceedings{mccrae_modelling_2014,
	address = {Dublin, Ireland},
	title = {Modelling the {Semantics} of {Adjectives} in the {Ontology}-{Lexicon} {Interface}},
	url = {https://aclanthology.org/W14-4724/},
	doi = {10.3115/v1/W14-4724},
	booktitle = {Proceedings of the 4th {Workshop} on {Cognitive} {Aspects} of the {Lexicon} ({CogALex})},
	publisher = {Association for Computational Linguistics and Dublin City University},
	author = {McCrae, John P. and Quattri, Francesca and Unger, Christina and Cimiano, Philipp},
	editor = {Zock, Michael and Rapp, Reinhard and Huang, Chu-Ren},
	month = aug,
	year = {2014},
	pages = {198--209},
}

@inproceedings{behera_imagact4all_2016,
	address = {Osaka, Japan},
	title = {The {IMAGACT4ALL} {Ontology} of {Animated} {Images}: {Implications} for {Theoretical} and {Machine} {Translation} of {Action} {Verbs} from {English}-{Indian} {Languages}},
	url = {https://aclanthology.org/W16-3707/},
	abstract = {Action verbs are one of the frequently occurring linguistic elements in any given natural language as the speakers use them during every linguistic intercourse. However, each language expresses action verbs in its own inherently unique manner by categorization. One verb can refer to several interpretations of actions and one action can be expressed by more than one verb. The inter-language and intra-language variations create ambiguity for the translation of languages from the source language to target language with respect to action verbs. IMAGACT is a corpus-based ontological platform of action verbs translated from prototypic animated images explained in English and Italian as meta-languages. In this paper, we are presenting the issues and challenges in translating action verbs of Indian languages as target and English as source language by observing the animated images. Among the ten Indian languages which have been annotated so far on the platform are Sanskrit, Hindi, Urdu, Odia (Oriya), Bengali, Manipuri, Tamil, Assamese, Magahi and Marathi. Out of them, Manipuri belongs to the Sino-Tibetan, Tamil comes off the Dravidian and the rest owe their genesis to the Indo-Aryan language family. One of the issues is that the one-word morphological English verbs are translated into most of the Indian languages as verbs having more than one-word form; for instance as in the case of conjunct, compound, serial verbs and so on. We are further presenting a cross-lingual comparison of action verbs among Indian languages. In addition, we are also dealing with the issues in disambiguating animated images by the L1 native speakers using competence-based judgements and the theoretical and machine translation implications they bear.},
	booktitle = {Proceedings of the 6th {Workshop} on {South} and {Southeast} {Asian} {Natural} {Language} {Processing} ({WSSANLP2016})},
	publisher = {The COLING 2016 Organizing Committee},
	author = {Behera, Pitambar and Muzaffar, Sharmin and Ojha, Atul Ku. and Jha, Girish},
	editor = {Wu, Dekai and Bhattacharyya, Pushpak},
	month = dec,
	year = {2016},
	pages = {64--73},
}

@inproceedings{bellandi_developing_2017,
	address = {Montpellier, France},
	title = {Developing {LexO}: a {Collaborative} {Editor} of {Multilingual} {Lexica} and {Termino}-{Ontological} {Resources} in the {Humanities}},
	url = {https://aclanthology.org/W17-7010/},
	booktitle = {Proceedings of {Language}, {Ontology}, {Terminology} and {Knowledge} {Structures} {Workshop} ({LOTKS} 2017)},
	publisher = {Association for Computational Linguistics},
	author = {Bellandi, Andrea and Giovannetti, Emiliano and Piccini, Silvia and Weingart, Anja},
	editor = {Frontini, Francesca and Grčić Simeunović, Larisa and Vintar, Špela and Khan, Anas Fahad and Parvisi, Artemis},
	month = sep,
	year = {2017},
}

@inproceedings{liang_distractor_2018,
	address = {New Orleans, Louisiana},
	title = {Distractor {Generation} for {Multiple} {Choice} {Questions} {Using} {Learning} to {Rank}},
	url = {https://aclanthology.org/W18-0533/},
	doi = {10.18653/v1/W18-0533},
	abstract = {We investigate how machine learning models, specifically ranking models, can be used to select useful distractors for multiple choice questions. Our proposed models can learn to select distractors that resemble those in actual exam questions, which is different from most existing unsupervised ontology-based and similarity-based methods. We empirically study feature-based and neural net (NN) based ranking models with experiments on the recently released SciQ dataset and our MCQL dataset. Experimental results show that feature-based ensemble learning methods (random forest and LambdaMART) outperform both the NN-based method and unsupervised baselines. These two datasets can also be used as benchmarks for distractor generation.},
	booktitle = {Proceedings of the {Thirteenth} {Workshop} on {Innovative} {Use} of {NLP} for {Building} {Educational} {Applications}},
	publisher = {Association for Computational Linguistics},
	author = {Liang, Chen and Yang, Xiao and Dave, Neisarg and Wham, Drew and Pursel, Bart and Giles, C. Lee},
	editor = {Tetreault, Joel and Burstein, Jill and Kochmar, Ekaterina and Leacock, Claudia and Yannakoudakis, Helen},
	month = jun,
	year = {2018},
	pages = {284--290},
}

@inproceedings{wang_ontology_2018,
	address = {Melbourne, Australia},
	title = {Ontology alignment in the biomedical domain using entity definitions and context},
	url = {https://aclanthology.org/W18-2306/},
	doi = {10.18653/v1/W18-2306},
	abstract = {Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system.},
	booktitle = {Proceedings of the {BioNLP} 2018 workshop},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Lucy Lu and Bhagavatula, Chandra and Neumann, Mark and Lo, Kyle and Wilhelm, Chris and Ammar, Waleed},
	editor = {Demner-Fushman, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
	month = jul,
	year = {2018},
	pages = {47--55},
}

@inproceedings{pustejovsky_every_2018,
	address = {Santa Fe, New Mexico, U.S.A},
	title = {Every {Object} {Tells} a {Story}},
	url = {https://aclanthology.org/W18-4301/},
	abstract = {Most work within the computational event modeling community has tended to focus on the interpretation and ordering of events that are associated with verbs and event nominals in linguistic expressions. What is often overlooked in the construction of a global interpretation of a narrative is the role contributed by the objects participating in these structures, and the latent events and activities conventionally associated with them. Recently, the analysis of visual images has also enriched the scope of how events can be identified, by anchoring both linguistic expressions and ontological labels to segments, subregions, and properties of images. By semantically grounding event descriptions in their visualization, the importance of object-based attributes becomes more apparent. In this position paper, we look at the narrative structure of objects: that is, how objects reference events through their intrinsic attributes, such as affordances, purposes, and functions. We argue that, not only do objects encode conventionalized events, but that when they are composed within specific habitats, the ensemble can be viewed as modeling coherent event sequences, thereby enriching the global interpretation of the evolving narrative being constructed.},
	booktitle = {Proceedings of the {Workshop} {Events} and {Stories} in the {News} 2018},
	publisher = {Association for Computational Linguistics},
	author = {Pustejovsky, James and Krishnaswamy, Nikhil},
	editor = {Caselli, Tommaso and Miller, Ben and van Erp, Marieke and Vossen, Piek and Palmer, Martha and Hovy, Eduard and Mitamura, Teruko and Caswell, David and Brown, Susan W. and Bonial, Claire},
	month = aug,
	year = {2018},
	pages = {1--6},
}

@inproceedings{sharma_degree_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Degree based {Classification} of {Harmful} {Speech} using {Twitter} {Data}},
	url = {https://aclanthology.org/W18-4413/},
	abstract = {Harmful speech has various forms and it has been plaguing the social media in different ways. If we need to crackdown different degrees of hate speech and abusive behavior amongst it, the classification needs to be based on complex ramifications which needs to be defined and hold accountable for, other than racist, sexist or against some particular group and community. This paper primarily describes how we created an ontological classification of harmful speech based on degree of hateful intent and used it to annotate twitter data accordingly. The key contribution of this paper is the new dataset of tweets we created based on ontological classes and degrees of harmful speech found in the text. We also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. This serves as a preliminary work to lay down foundation on defining different classes of harmful speech and subsequent work will be done in making it's automatic detection more robust and efficient.},
	booktitle = {Proceedings of the {First} {Workshop} on {Trolling}, {Aggression} and {Cyberbullying} ({TRAC}-2018)},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Sanjana and Agrawal, Saksham and Shrivastava, Manish},
	editor = {Kumar, Ritesh and Ojha, Atul Kr. and Zampieri, Marcos and Malmasi, Shervin},
	month = aug,
	year = {2018},
	pages = {106--112},
}

@inproceedings{naresh_kumar_ontology-based_2018,
	address = {Brussels, Belgium},
	title = {Ontology-{Based} {Retrieval} \& {Neural} {Approaches} for {BioASQ} {Ideal} {Answer} {Generation}},
	url = {https://aclanthology.org/W18-5310/},
	doi = {10.18653/v1/W18-5310},
	abstract = {The ever-increasing magnitude of biomedical information sources makes it difficult and time-consuming for a human researcher to find the most relevant documents and pinpointed answers for a specific question or topic when using only a traditional search engine. Biomedical Question Answering systems automatically identify the most relevant documents and pinpointed answers, given an information need expressed as a natural language question. Generating a non-redundant, human-readable summary that satisfies the information need of a given biomedical question is the focus of the Ideal Answer Generation task, part of the BioASQ challenge. This paper presents a system for ideal answer generation (using ontology-based retrieval and a neural learning-to-rank approach, combined with extractive and abstractive summarization techniques) which achieved the highest ROUGE score of 0.659 on the BioASQ 5b batch 2 test.},
	booktitle = {Proceedings of the 6th {BioASQ} {Workshop} {A} challenge on large-scale biomedical semantic indexing and question answering},
	publisher = {Association for Computational Linguistics},
	author = {Naresh Kumar, Ashwin and Kesavamoorthy, Harini and Das, Madhura and Kalwad, Pramati and Chandu, Khyathi and Mitamura, Teruko and Nyberg, Eric},
	editor = {Kakadiaris, Ioannis A. and Paliouras, George and Krithara, Anastasia},
	month = nov,
	year = {2018},
	pages = {79--89},
}

@inproceedings{rojas-barahona_deep_2018,
	address = {Brussels, Belgium},
	title = {Deep learning for language understanding of mental health concepts derived from {Cognitive} {Behavioural} {Therapy}},
	url = {https://aclanthology.org/W18-5606/},
	doi = {10.18653/v1/W18-5606},
	abstract = {In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.},
	booktitle = {Proceedings of the {Ninth} {International} {Workshop} on {Health} {Text} {Mining} and {Information} {Analysis}},
	publisher = {Association for Computational Linguistics},
	author = {Rojas-Barahona, Lina M. and Tseng, Bo-Hsiang and Dai, Yinpei and Mansfield, Clare and Ramadan, Osman and Ultes, Stefan and Crawford, Michael and Gašić, Milica},
	editor = {Lavelli, Alberto and Minard, Anne-Lyse and Rinaldi, Fabio},
	month = oct,
	year = {2018},
	pages = {44--54},
}

@inproceedings{balachandran_learning_2018,
	address = {Brussels, Belgium},
	title = {Learning to {Define} {Terms} in the {Software} {Domain}},
	url = {https://aclanthology.org/W18-6122/},
	doi = {10.18653/v1/W18-6122},
	abstract = {One way to test a person's knowledge of a domain is to ask them to define domain-specific terms. Here, we investigate the task of automatically generating definitions of technical terms by reading text from the technical domain. Specifically, we learn definitions of software entities from a large corpus built from the user forum Stack Overflow. To model definitions, we train a language model and incorporate additional domain-specific information like word co-occurrence, and ontological category information. Our approach improves previous baselines by 2 BLEU points for the definition generation task. Our experiments also show the additional challenges associated with the task and the short-comings of language-model based architectures for definition generation.},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {W}-{NUT}: {The} 4th {Workshop} on {Noisy} {User}-generated {Text}},
	publisher = {Association for Computational Linguistics},
	author = {Balachandran, Vidhisha and Rajagopal, Dheeraj and Kanjirathinkal, Rose Catherine and Cohen, William},
	editor = {Xu, Wei and Ritter, Alan and Baldwin, Tim and Rahimi, Afshin},
	month = nov,
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Language model, Computational linguistics, Definition generation, Domain specific, Learn+, Large corpora, Domain-specific information, Software domains, Software entities, Stack overflow, Technical terms},
	pages = {164--172},
	annote = {Cited by: 3},
}

@inproceedings{iter_frameit_2018,
	address = {Brussels, Belgium},
	title = {{FrameIt}: {Ontology} {Discovery} for {Noisy} {User}-{Generated} {Text}},
	url = {https://aclanthology.org/W18-6123/},
	doi = {10.18653/v1/W18-6123},
	abstract = {A common need of NLP applications is to extract structured data from text corpora in order to perform analytics or trigger an appropriate action. The ontology defining the structure is typically application dependent and in many cases it is not known a priori. We describe the FrameIt System that provides a workflow for (1) quickly discovering an ontology to model a text corpus and (2) learning an SRL model that extracts the instances of the ontology from sentences in the corpus. FrameIt exploits data that is obtained in the ontology discovery phase as weak supervision data to bootstrap the SRL model and then enables the user to refine the model with active learning. We present empirical results and qualitative analysis of the performance of FrameIt on three corpora of noisy user-generated text.},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {W}-{NUT}: {The} 4th {Workshop} on {Noisy} {User}-generated {Text}},
	publisher = {Association for Computational Linguistics},
	author = {Iter, Dan and Halevy, Alon and Tan, Wang-Chiew},
	editor = {Xu, Wei and Ritter, Alan and Baldwin, Tim and Rahimi, Afshin},
	month = nov,
	year = {2018},
	pages = {173--183},
}

@inproceedings{shvets_sentence_2018,
	address = {Tilburg University, The Netherlands},
	title = {Sentence {Packaging} in {Text} {Generation} from {Semantic} {Graphs} as a {Community} {Detection} {Problem}},
	url = {https://aclanthology.org/W18-6542/},
	doi = {10.18653/v1/W18-6542},
	abstract = {An increasing amount of research tackles the challenge of text generation from abstract ontological or semantic structures, which are in their very nature potentially large connected graphs. These graphs must be “packaged” into sentence-wise subgraphs. We interpret the problem of sentence packaging as a community detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1-score of 0.738.},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Shvets, Alexander and Mille, Simon and Wanner, Leo},
	editor = {Krahmer, Emiel and Gatt, Albert and Goudbeek, Martijn},
	month = nov,
	year = {2018},
	pages = {350--359},
}

@inproceedings{funke_interactive_2018,
	address = {Tilburg University, The Netherlands},
	title = {Interactive health insight miner: an adaptive, semantic-based approach},
	url = {https://aclanthology.org/W18-6559/},
	doi = {10.18653/v1/W18-6559},
	abstract = {E-health applications aim to support the user in adopting healthy habits. An important feature is to provide insights into the user's lifestyle. To actively engage the user in the insight mining process, we propose an ontology-based framework with a Controlled Natural Language interface, which enables the user to ask for specific insights and to customize personal information.},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Funke, Isabel and Helaoui, Rim and Härmä, Aki},
	editor = {Krahmer, Emiel and Gatt, Albert and Goudbeek, Martijn},
	month = nov,
	year = {2018},
	pages = {478--479},
}

@inproceedings{pomarlan_sensors_2018,
	address = {Tilburg, The Netherlands},
	title = {From sensors to sense: {Integrated} heterogeneous ontologies for {Natural} {Language} {Generation}},
	url = {https://aclanthology.org/W18-6904/},
	doi = {10.18653/v1/W18-6904},
	abstract = {We propose the combination of a robotics ontology (KnowRob) with a linguistically motivated one (GUM) under the upper ontology DUL. We use the DUL Event, Situation, Description pattern to formalize reasoning techniques to convert between a robot's beliefstate and its linguistic utterances. We plan to employ these techniques to equip robots with a reason-aloud ability, through which they can explain their actions as they perform them, in natural language, at a level of granularity appropriate to the user, their query and the context at hand.},
	booktitle = {Proceedings of the {Workshop} on {NLG} for {Human}–{Robot} {Interaction}},
	publisher = {Association for Computational Linguistics},
	author = {Pomarlan, Mihai and Porzel, Robert and Bateman, John and Malaka, Rainer},
	editor = {Foster, Mary Ellen and Buschmeier, Hendrik and Gkatzia, Dimitra},
	month = nov,
	year = {2018},
	pages = {17--21},
}

@inproceedings{lawley_towards_2019,
	address = {Gothenburg, Sweden},
	title = {Towards {Natural} {Language} {Story} {Understanding} with {Rich} {Logical} {Schemas}},
	url = {https://aclanthology.org/W19-1102/},
	doi = {10.18653/v1/W19-1102},
	abstract = {Generating “commonsense” knowledge for intelligent understanding and reasoning is a difficult, long-standing problem, whose scale challenges the capacity of any approach driven primarily by human input. Furthermore, approaches based on mining statistically repetitive patterns fail to produce the rich representations humans acquire, and fall far short of human efficiency in inducing knowledge from text. The idea of our approach to this problem is to provide a learning system with a “head start” consisting of a semantic parser, some basic ontological knowledge, and most importantly, a small set of very general schemas about the kinds of patterns of events (often purposive, causal, or socially conventional) that even a one- or two-year-old could reasonably be presumed to possess. We match these initial schemas to simple children's stories, obtaining concrete instances, and combining and abstracting these into new candidate schemas. Both the initial and generated schemas are specified using a rich, expressive logical form. While modern approaches to schema reasoning often only use slot-and-filler structures, this logical form allows us to specify complex relations and constraints over the slots. Though formal, the representations are language-like, and as such readily relatable to NL text. The agents, objects, and other roles in the schemas are represented by typed variables, and the event variables can be related through partial temporal ordering and causal relations. To match natural language stories with existing schemas, we first parse the stories into an underspecified variant of the logical form used by the schemas, which is suitable for most concrete stories. We include a walkthrough of matching a children's story to these schemas and generating inferences from these matches.},
	booktitle = {Proceedings of the {Sixth} {Workshop} on {Natural} {Language} and {Computer} {Science}},
	publisher = {Association for Computational Linguistics},
	author = {Lawley, Lane and Kim, Gene Louis and Schubert, Lenhart},
	editor = {Cooper, Robin and de Paiva, Valeria and Moss, Lawrence S.},
	month = may,
	year = {2019},
	pages = {11--22},
}

@inproceedings{sarthou_semantic_2019,
	address = {Minneapolis, Minnesota},
	title = {Semantic {Spatial} {Representation}: a unique representation of an environment based on an ontology for robotic applications},
	url = {https://aclanthology.org/W19-1606/},
	doi = {10.18653/v1/W19-1606},
	abstract = {It is important, for human-robot interaction, to endow the robot with the knowledge necessary to understand human needs and to be able to respond to them. We present a formalized and unified representation for indoor environments using an ontology devised for a route description task in which a robot must provide explanations to a person. We show that this representation can be used to choose a route to explain to a human as well as to verbalize it using a route perspective. Based on ontology, this representation has a strong possibility of evolution to adapt to many other applications. With it, we get the semantics of the environment elements while keeping a description of the known connectivity of the environment. This representation and the illustration algorithms, to find and verbalize a route, have been tested in two environments of different scales.},
	booktitle = {Proceedings of the {Combined} {Workshop} on {Spatial} {Language} {Understanding} ({SpLU}) and {Grounded} {Communication} for {Robotics} ({RoboNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Sarthou, Guillaume and Clodic, Aurélie and Alami, Rachid},
	editor = {Bhatia, Archna and Bisk, Yonatan and Kordjamshidi, Parisa and Thomason, Jesse},
	month = jun,
	year = {2019},
	pages = {50--60},
}

@inproceedings{tao_effective_2019,
	address = {Minneapolis, Minnesota, USA},
	title = {Effective {Feature} {Representation} for {Clinical} {Text} {Concept} {Extraction}},
	url = {https://aclanthology.org/W19-1901/},
	doi = {10.18653/v1/W19-1901},
	abstract = {Crucial information about the practice of healthcare is recorded only in free-form text, which creates an enormous opportunity for high-impact NLP. However, annotated healthcare datasets tend to be small and expensive to obtain, which raises the question of how to make maximally efficient uses of the available data. To this end, we develop an LSTM-CRF model for combining unsupervised word representations and hand-built feature representations derived from publicly available healthcare ontologies. We show that this combined model yields superior performance on five datasets of diverse kinds of healthcare text (clinical, social, scientific, commercial). Each involves the labeling of complex, multi-word spans that pick out different healthcare concepts. We also introduce a new labeled dataset for identifying the treatment relations between drugs and diseases.},
	booktitle = {Proceedings of the 2nd {Clinical} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Tao, Yifeng and Godefroy, Bruno and Genthial, Guillaume and Potts, Christopher},
	editor = {Rumshisky, Anna and Roberts, Kirk and Bethard, Steven and Naumann, Tristan},
	month = jun,
	year = {2019},
	pages = {1--14},
}

@inproceedings{mondal_medical_2019,
	address = {Minneapolis, Minnesota, USA},
	title = {Medical {Entity} {Linking} using {Triplet} {Network}},
	url = {https://aclanthology.org/W19-1912/},
	doi = {10.18653/v1/W19-1912},
	abstract = {Entity linking (or Normalization) is an essential task in text mining that maps the entity mentions in the medical text to standard entities in a given Knowledge Base (KB). This task is of great importance in the medical domain. It can also be used for merging different medical and clinical ontologies. In this paper, we center around the problem of disease linking or normalization. This task is executed in two phases: candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for candidate ranking. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin.},
	booktitle = {Proceedings of the 2nd {Clinical} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Mondal, Ishani and Purkayastha, Sukannya and Sarkar, Sudeshna and Goyal, Pawan and Pillai, Jitesh and Bhattacharyya, Amitava and Gattu, Mahanandeeshwar},
	editor = {Rumshisky, Anna and Roberts, Kirk and Bethard, Steven and Naumann, Tristan},
	month = jun,
	year = {2019},
	pages = {95--100},
}

@inproceedings{jahan_character_2019,
	address = {Minneapolis, Minnesota},
	title = {Character {Identification} {Refined}: {A} {Proposal}},
	url = {https://aclanthology.org/W19-2402/},
	doi = {10.18653/v1/W19-2402},
	abstract = {Characters are a key element of narrative and so character identification plays an important role in automatic narrative understanding. Unfortunately, most prior work that incorporates character identification is not built upon a clear, theoretically grounded concept of character. They either take character identification for granted (e.g., using simple heuristics on referring expressions), or rely on simplified definitions that do not capture important distinctions between characters and other referents in the story. Prior approaches have also been rather complicated, relying, for example, on predefined case bases or ontologies. In this paper we propose a narratologically grounded definition of character for discussion at the workshop, and also demonstrate a preliminary yet straightforward supervised machine learning model with a small set of features that performs well on two corpora. The most important of the two corpora is a set of 46 Russian folktales, on which the model achieves an F1 of 0.81. Error analysis suggests that features relevant to the plot will be necessary for further improvements in performance.},
	booktitle = {Proceedings of the {First} {Workshop} on {Narrative} {Understanding}},
	publisher = {Association for Computational Linguistics},
	author = {Jahan, Labiba and Finlayson, Mark},
	editor = {Bamman, David and Chaturvedi, Snigdha and Clark, Elizabeth and Fiterau, Madalina and Iyyer, Mohit},
	month = jun,
	year = {2019},
	pages = {12--18},
}

@inproceedings{manjavacas_feasibility_2019,
	address = {Minneapolis, USA},
	title = {On the {Feasibility} of {Automated} {Detection} of {Allusive} {Text} {Reuse}},
	url = {https://aclanthology.org/W19-2514/},
	doi = {10.18653/v1/W19-2514},
	abstract = {The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely — commonly based on none or very few shared words. Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and ontologies can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a windowing approach, and that (ii) retrieval performance can be moderately boosted with distributional semantics.},
	booktitle = {Proceedings of the 3rd {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	publisher = {Association for Computational Linguistics},
	author = {Manjavacas, Enrique and Long, Brian and Kestemont, Mike},
	editor = {Alex, Beatrice and Degaetano-Ortlieb, Stefania and Kazantseva, Anna and Reiter, Nils and Szpakowicz, Stan},
	month = jun,
	year = {2019},
	pages = {104--114},
}

@inproceedings{prabhumoye_principled_2019,
	address = {Florence, Italy},
	title = {Principled {Frameworks} for {Evaluating} {Ethics} in {NLP} {Systems}},
	url = {https://aclanthology.org/W19-3637/},
	abstract = {We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each.},
	booktitle = {Proceedings of the 2019 {Workshop} on {Widening} {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Prabhumoye, Shrimai and Mayfield, Elijah and Black, Alan W},
	editor = {Axelrod, Amittai and Yang, Diyi and Cunha, Rossana and Shaikh, Samira and Waseem, Zeerak},
	month = aug,
	year = {2019},
	pages = {118--121},
}

@inproceedings{lopez_fine-grained_2019,
	address = {Florence, Italy},
	title = {Fine-{Grained} {Entity} {Typing} in {Hyperbolic} {Space}},
	url = {https://aclanthology.org/W19-4319/},
	doi = {10.18653/v1/W19-4319},
	abstract = {How can we represent hierarchical information present in large type inventories for entity typing? We study the suitability of hyperbolic embeddings to capture hierarchical relations between mentions in context and their target types in a shared vector space. We evaluate on two datasets and propose two different techniques to extract hierarchical information from the type inventory: from an expert-generated ontology and by automatically mining the dataset. The hyperbolic model shows improvements in some but not all cases over its Euclidean counterpart. Our analysis suggests that the adequacy of this geometry depends on the granularity of the type inventory and the representation of its distribution.},
	booktitle = {Proceedings of the 4th {Workshop} on {Representation} {Learning} for {NLP} ({RepL4NLP}-2019)},
	publisher = {Association for Computational Linguistics},
	author = {López, Federico and Heinzerling, Benjamin and Strube, Michael},
	editor = {Augenstein, Isabelle and Gella, Spandana and Ruder, Sebastian and Kann, Katharina and Can, Burcu and Welbl, Johannes and Conneau, Alexis and Ren, Xiang and Rei, Marek},
	month = aug,
	year = {2019},
	pages = {169--180},
}

@inproceedings{joshi_comparison_2019,
	address = {Florence, Italy},
	title = {A {Comparison} of {Word}-based and {Context}-based {Representations} for {Classification} {Problems} in {Health} {Informatics}},
	url = {https://aclanthology.org/W19-5015/},
	doi = {10.18653/v1/W19-5015},
	abstract = {Distributed representations of text can be used as features when training a statistical classifier. These representations may be created as a composition of word vectors or as context-based sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontology. There is an improvement of 2-4\% in the accuracy when these context-based representations are used instead of word-based representations.},
	booktitle = {Proceedings of the 18th {BioNLP} {Workshop} and {Shared} {Task}},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Aditya and Karimi, Sarvnaz and Sparks, Ross and Paris, Cecile and MacIntyre, C Raina},
	editor = {Demner-Fushman, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
	month = aug,
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Language model, Health, Medical informatics, Health informatics, Computational linguistics, Text processing, Ontology's, Classification (of information), Personal health, Word vectors, Context-based, Distributed representation, Statistical classifier},
	pages = {135--141},
	annote = {Cited by: 12},
}

@inproceedings{wiegreffe_clinical_2019,
	address = {Florence, Italy},
	title = {Clinical {Concept} {Extraction} for {Document}-{Level} {Coding}},
	url = {https://aclanthology.org/W19-5028/},
	doi = {10.18653/v1/W19-5028},
	abstract = {The text of clinical notes can be a valuable source of patient information and clinical assessments. Historically, the primary approach for exploiting clinical notes has been information extraction: linking spans of text to concepts in a detailed domain ontology. However, recent work has demonstrated the potential of supervised machine learning to extract document-level codes directly from the raw text of clinical notes. We propose to bridge the gap between the two approaches with two novel syntheses: (1) treating extracted concepts as features, which are used to supplement or replace the text of the note; (2) treating extracted concepts as labels, which are used to learn a better representation of the text. Unfortunately, the resulting concepts do not yield performance gains on the document-level clinical coding task. We explore possible explanations and future research directions.},
	booktitle = {Proceedings of the 18th {BioNLP} {Workshop} and {Shared} {Task}},
	publisher = {Association for Computational Linguistics},
	author = {Wiegreffe, Sarah and Choi, Edward and Yan, Sherry and Sun, Jimeng and Eisenstein, Jacob},
	editor = {Demner-Fushman, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
	month = aug,
	year = {2019},
	pages = {261--272},
}

@inproceedings{gao_dialog_2019,
	address = {Stockholm, Sweden},
	title = {Dialog {State} {Tracking}: {A} {Neural} {Reading} {Comprehension} {Approach}},
	url = {https://aclanthology.org/W19-5932/},
	doi = {10.18653/v1/W19-5932},
	abstract = {Dialog state tracking is used to estimate the current belief state of a dialog given all the preceding conversation. Machine reading comprehension, on the other hand, focuses on building systems that read passages of text and answer questions that require some understanding of passages. We formulate dialog state tracking as a reading comprehension task to answer the question what is the state of the current dialog? after reading conversational context. In contrast to traditional state tracking methods where the dialog state is often predicted as a distribution over a closed set of all the possible slot values within an ontology, our method uses a simple attention-based neural network to point to the slot values within the conversation. Experiments on MultiWOZ-2.0 cross-domain dialog dataset show that our simple system can obtain similar accuracies compared to the previous more complex methods. By exploiting recent advances in contextual word embeddings, adding a model that explicitly tracks whether a slot value should be carried over to the next turn, and combining our method with a traditional joint state tracking method that relies on closed set vocabulary, we can obtain a joint-goal accuracy of 47.33\% on the standard test split, exceeding current state-of-the-art by 11.75\%**.},
	booktitle = {Proceedings of the 20th {Annual} {SIGdial} {Meeting} on {Discourse} and {Dialogue}},
	publisher = {Association for Computational Linguistics},
	author = {Gao, Shuyang and Sethi, Abhishek and Agarwal, Sanchit and Chung, Tagyoung and Hakkani-Tur, Dilek},
	editor = {Nakamura, Satoshi and Gasic, Milica and Zukerman, Ingrid and Skantze, Gabriel and Nakano, Mikio and Papangelis, Alexandros and Ultes, Stefan and Yoshino, Koichiro},
	month = sep,
	year = {2019},
	pages = {264--273},
}

@inproceedings{cervone_natural_2019,
	address = {Tokyo, Japan},
	title = {Natural {Language} {Generation} at {Scale}: {A} {Case} {Study} for {Open} {Domain} {Question} {Answering}},
	url = {https://aclanthology.org/W19-8657/},
	doi = {10.18653/v1/W19-8657},
	abstract = {Current approaches to Natural Language Generation (NLG) for dialog mainly focus on domain-specific, task-oriented applications (e.g. restaurant booking) using limited ontologies (up to 20 slot types), usually without considering the previous conversation context. Furthermore, these approaches require large amounts of data for each domain, and do not benefit from examples that may be available for other domains. This work explores the feasibility of applying statistical NLG to scenarios requiring larger ontologies, such as multi-domain dialog applications or open-domain question answering (QA) based on knowledge graphs. We model NLG through an Encoder-Decoder framework using a large dataset of interactions between real-world users and a conversational agent for open-domain QA. First, we investigate the impact of increasing the number of slot types on the generation quality and experiment with different partitions of the QA data with progressively larger ontologies (up to 369 slot types). Second, we perform multi-task learning experiments between open-domain QA and task-oriented dialog, and benchmark our model on a popular NLG dataset. Moreover, we experiment with using the conversational context as an additional input to improve response generation quality. Our experiments show the feasibility of learning statistical NLG models for open-domain QA with larger ontologies.},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Cervone, Alessandra and Khatri, Chandra and Goel, Rahul and Hedayatnia, Behnam and Venkatesh, Anu and Hakkani-Tur, Dilek and Gabriel, Raefer},
	editor = {van Deemter, Kees and Lin, Chenghua and Takamura, Hiroya},
	month = oct,
	year = {2019},
	pages = {453--462},
}
